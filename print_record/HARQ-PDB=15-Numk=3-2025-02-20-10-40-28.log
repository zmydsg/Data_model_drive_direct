
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): -0.1705528199672699 
model_pd.l_d.mean(): -19.24897003173828 
model_pd.lagr.mean(): -19.419523239135742 
model_pd.lambdas: dict_items([('pout', tensor([1.0001], device='cuda:0')), ('power', tensor([0.9999], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3984], device='cuda:0')), ('power', tensor([-20.6474], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:-0.1705528199672699
epoch£º0	 i:1 	 global-step:1	 l-p:-0.20870685577392578
epoch£º0	 i:2 	 global-step:2	 l-p:-0.27698108553886414
epoch£º0	 i:3 	 global-step:3	 l-p:-0.4038802981376648
epoch£º0	 i:4 	 global-step:4	 l-p:-0.7105808854103088
epoch£º0	 i:5 	 global-step:5	 l-p:-2.530949592590332
epoch£º0	 i:6 	 global-step:6	 l-p:1.7505215406417847
epoch£º0	 i:7 	 global-step:7	 l-p:0.49168118834495544
epoch£º0	 i:8 	 global-step:8	 l-p:0.6606582403182983
epoch£º0	 i:9 	 global-step:9	 l-p:0.05680999532341957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5667, 3.5671, 3.5667],
        [3.5667, 3.7813, 3.7378],
        [3.5667, 3.6280, 3.5888],
        [3.5667, 4.5730, 5.2699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.3772571384906769 
model_pd.l_d.mean(): -18.863636016845703 
model_pd.lagr.mean(): -18.486379623413086 
model_pd.lambdas: dict_items([('pout', tensor([1.0058], device='cuda:0')), ('power', tensor([0.9952], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9113], device='cuda:0')), ('power', tensor([-19.8624], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.3772571384906769
epoch£º1	 i:1 	 global-step:21	 l-p:0.2680797576904297
epoch£º1	 i:2 	 global-step:22	 l-p:0.1878843605518341
epoch£º1	 i:3 	 global-step:23	 l-p:0.24289998412132263
epoch£º1	 i:4 	 global-step:24	 l-p:0.13980881869792938
epoch£º1	 i:5 	 global-step:25	 l-p:0.18688207864761353
epoch£º1	 i:6 	 global-step:26	 l-p:0.17738257348537445
epoch£º1	 i:7 	 global-step:27	 l-p:0.21875016391277313
epoch£º1	 i:8 	 global-step:28	 l-p:0.22573958337306976
epoch£º1	 i:9 	 global-step:29	 l-p:0.11670548468828201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9151, 5.9885, 6.5145],
        [4.9151, 5.0129, 4.9528],
        [4.9151, 5.7268, 5.9879],
        [4.9151, 4.9970, 4.9433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.13362069427967072 
model_pd.l_d.mean(): -20.28553581237793 
model_pd.lagr.mean(): -20.151914596557617 
model_pd.lambdas: dict_items([('pout', tensor([1.0098], device='cuda:0')), ('power', tensor([0.9906], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4447], device='cuda:0')), ('power', tensor([-20.9246], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.13362069427967072
epoch£º2	 i:1 	 global-step:41	 l-p:0.10827075690031052
epoch£º2	 i:2 	 global-step:42	 l-p:0.10365581512451172
epoch£º2	 i:3 	 global-step:43	 l-p:0.10691769421100616
epoch£º2	 i:4 	 global-step:44	 l-p:0.11889182776212692
epoch£º2	 i:5 	 global-step:45	 l-p:0.10603252053260803
epoch£º2	 i:6 	 global-step:46	 l-p:0.11256217956542969
epoch£º2	 i:7 	 global-step:47	 l-p:-0.5149641633033752
epoch£º2	 i:8 	 global-step:48	 l-p:0.10819238424301147
epoch£º2	 i:9 	 global-step:49	 l-p:0.11758995056152344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3948, 6.4842, 6.9566],
        [5.3948, 5.4458, 5.4069],
        [5.3948, 5.5008, 5.4351],
        [5.3948, 6.9666, 8.0014]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.08163347095251083 
model_pd.l_d.mean(): -18.719200134277344 
model_pd.lagr.mean(): -18.6375675201416 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9893], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4201], device='cuda:0')), ('power', tensor([-19.3489], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.08163347095251083
epoch£º3	 i:1 	 global-step:61	 l-p:0.08723416179418564
epoch£º3	 i:2 	 global-step:62	 l-p:0.00024626255617477
epoch£º3	 i:3 	 global-step:63	 l-p:0.12233921885490417
epoch£º3	 i:4 	 global-step:64	 l-p:0.11886759847402573
epoch£º3	 i:5 	 global-step:65	 l-p:0.1169566884636879
epoch£º3	 i:6 	 global-step:66	 l-p:0.11346925795078278
epoch£º3	 i:7 	 global-step:67	 l-p:0.1185336485505104
epoch£º3	 i:8 	 global-step:68	 l-p:0.12619811296463013
epoch£º3	 i:9 	 global-step:69	 l-p:0.12450944632291794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9813, 5.7049, 5.8823],
        [4.9813, 5.8650, 6.1866],
        [4.9813, 6.5221, 7.6085],
        [4.9813, 5.0697, 5.0130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.11540032923221588 
model_pd.l_d.mean(): -19.056425094604492 
model_pd.lagr.mean(): -18.941024780273438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4842], device='cuda:0')), ('power', tensor([-19.7591], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.11540032923221588
epoch£º4	 i:1 	 global-step:81	 l-p:0.1313871145248413
epoch£º4	 i:2 	 global-step:82	 l-p:0.16932250559329987
epoch£º4	 i:3 	 global-step:83	 l-p:0.13243122398853302
epoch£º4	 i:4 	 global-step:84	 l-p:0.15471574664115906
epoch£º4	 i:5 	 global-step:85	 l-p:0.1268613338470459
epoch£º4	 i:6 	 global-step:86	 l-p:0.1303672343492508
epoch£º4	 i:7 	 global-step:87	 l-p:0.13439372181892395
epoch£º4	 i:8 	 global-step:88	 l-p:0.14058850705623627
epoch£º4	 i:9 	 global-step:89	 l-p:0.16173121333122253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6544, 4.6546, 4.6544],
        [4.6544, 4.6605, 4.6548],
        [4.6544, 4.7478, 4.6909],
        [4.6544, 4.7884, 4.7204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.18696843087673187 
model_pd.l_d.mean(): -20.106107711791992 
model_pd.lagr.mean(): -19.919139862060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5405], device='cuda:0')), ('power', tensor([-20.8780], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.18696843087673187
epoch£º5	 i:1 	 global-step:101	 l-p:0.15168224275112152
epoch£º5	 i:2 	 global-step:102	 l-p:0.16870616376399994
epoch£º5	 i:3 	 global-step:103	 l-p:0.08483701199293137
epoch£º5	 i:4 	 global-step:104	 l-p:0.1332511156797409
epoch£º5	 i:5 	 global-step:105	 l-p:0.15382403135299683
epoch£º5	 i:6 	 global-step:106	 l-p:0.12070299685001373
epoch£º5	 i:7 	 global-step:107	 l-p:0.13906322419643402
epoch£º5	 i:8 	 global-step:108	 l-p:0.1369633972644806
epoch£º5	 i:9 	 global-step:109	 l-p:0.13650409877300262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8654, 4.8931, 4.8703],
        [4.8654, 4.8654, 4.8654],
        [4.8654, 6.2734, 7.2129],
        [4.8654, 4.8702, 4.8657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.13271106779575348 
model_pd.l_d.mean(): -20.582923889160156 
model_pd.lagr.mean(): -20.450212478637695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4267], device='cuda:0')), ('power', tensor([-21.2438], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.13271106779575348
epoch£º6	 i:1 	 global-step:121	 l-p:0.132117360830307
epoch£º6	 i:2 	 global-step:122	 l-p:0.13903439044952393
epoch£º6	 i:3 	 global-step:123	 l-p:0.15074068307876587
epoch£º6	 i:4 	 global-step:124	 l-p:0.12953133881092072
epoch£º6	 i:5 	 global-step:125	 l-p:0.15238352119922638
epoch£º6	 i:6 	 global-step:126	 l-p:0.1375342160463333
epoch£º6	 i:7 	 global-step:127	 l-p:0.12962380051612854
epoch£º6	 i:8 	 global-step:128	 l-p:0.13109979033470154
epoch£º6	 i:9 	 global-step:129	 l-p:0.12418151646852493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8894, 5.5407, 5.6706],
        [4.8894, 4.9519, 4.9077],
        [4.8894, 4.8894, 4.8894],
        [4.8894, 4.9105, 4.8925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.1277899444103241 
model_pd.l_d.mean(): -20.45469093322754 
model_pd.lagr.mean(): -20.326900482177734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4353], device='cuda:0')), ('power', tensor([-21.1229], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.1277899444103241
epoch£º7	 i:1 	 global-step:141	 l-p:0.11821334809064865
epoch£º7	 i:2 	 global-step:142	 l-p:0.13609935343265533
epoch£º7	 i:3 	 global-step:143	 l-p:0.13799819350242615
epoch£º7	 i:4 	 global-step:144	 l-p:0.14886146783828735
epoch£º7	 i:5 	 global-step:145	 l-p:-0.24089446663856506
epoch£º7	 i:6 	 global-step:146	 l-p:0.12930406630039215
epoch£º7	 i:7 	 global-step:147	 l-p:0.13897576928138733
epoch£º7	 i:8 	 global-step:148	 l-p:0.166330024600029
epoch£º7	 i:9 	 global-step:149	 l-p:0.14020608365535736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6980, 4.6980, 4.6980],
        [4.6980, 4.6981, 4.6980],
        [4.6980, 5.4962, 5.7742],
        [4.6980, 4.8410, 4.7713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.1339498609304428 
model_pd.l_d.mean(): -18.755619049072266 
model_pd.lagr.mean(): -18.62166976928711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5704], device='cuda:0')), ('power', tensor([-19.5433], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.1339498609304428
epoch£º8	 i:1 	 global-step:161	 l-p:0.10937044024467468
epoch£º8	 i:2 	 global-step:162	 l-p:0.13943378627300262
epoch£º8	 i:3 	 global-step:163	 l-p:0.1424514502286911
epoch£º8	 i:4 	 global-step:164	 l-p:0.12771056592464447
epoch£º8	 i:5 	 global-step:165	 l-p:0.14805708825588226
epoch£º8	 i:6 	 global-step:166	 l-p:0.12971781194210052
epoch£º8	 i:7 	 global-step:167	 l-p:0.13172124326229095
epoch£º8	 i:8 	 global-step:168	 l-p:0.14077118039131165
epoch£º8	 i:9 	 global-step:169	 l-p:0.1406712681055069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8656, 4.8674, 4.8657],
        [4.8656, 4.8676, 4.8657],
        [4.8656, 5.0135, 4.9413],
        [4.8656, 4.8864, 4.8687]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.1353025734424591 
model_pd.l_d.mean(): -17.660247802734375 
model_pd.lagr.mean(): -17.524944305419922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5548], device='cuda:0')), ('power', tensor([-18.4200], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.1353025734424591
epoch£º9	 i:1 	 global-step:181	 l-p:0.12824122607707977
epoch£º9	 i:2 	 global-step:182	 l-p:0.13838037848472595
epoch£º9	 i:3 	 global-step:183	 l-p:0.13025379180908203
epoch£º9	 i:4 	 global-step:184	 l-p:0.12077470868825912
epoch£º9	 i:5 	 global-step:185	 l-p:0.14665238559246063
epoch£º9	 i:6 	 global-step:186	 l-p:0.16524089872837067
epoch£º9	 i:7 	 global-step:187	 l-p:0.1290958970785141
epoch£º9	 i:8 	 global-step:188	 l-p:0.12787111103534698
epoch£º9	 i:9 	 global-step:189	 l-p:0.10969629138708115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8094, 6.0339, 6.7596],
        [4.8094, 5.6447, 5.9466],
        [4.8094, 4.8145, 4.8098],
        [4.8094, 4.8094, 4.8094]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.15206654369831085 
model_pd.l_d.mean(): -20.22431755065918 
model_pd.lagr.mean(): -20.072250366210938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4822], device='cuda:0')), ('power', tensor([-20.9380], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.15206654369831085
epoch£º10	 i:1 	 global-step:201	 l-p:0.1391996592283249
epoch£º10	 i:2 	 global-step:202	 l-p:0.11491081863641739
epoch£º10	 i:3 	 global-step:203	 l-p:0.13253657519817352
epoch£º10	 i:4 	 global-step:204	 l-p:0.13277117908000946
epoch£º10	 i:5 	 global-step:205	 l-p:0.13973131775856018
epoch£º10	 i:6 	 global-step:206	 l-p:0.1499173790216446
epoch£º10	 i:7 	 global-step:207	 l-p:0.1646251231431961
epoch£º10	 i:8 	 global-step:208	 l-p:-0.057261694222688675
epoch£º10	 i:9 	 global-step:209	 l-p:0.14185252785682678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7934, 5.3128, 5.3590],
        [4.7934, 6.1000, 6.9314],
        [4.7934, 5.5043, 5.6960],
        [4.7934, 4.8165, 4.7971]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.11840072274208069 
model_pd.l_d.mean(): -20.09127426147461 
model_pd.lagr.mean(): -19.97287368774414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4935], device='cuda:0')), ('power', tensor([-20.8149], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.11840072274208069
epoch£º11	 i:1 	 global-step:221	 l-p:0.14984552562236786
epoch£º11	 i:2 	 global-step:222	 l-p:0.12453457713127136
epoch£º11	 i:3 	 global-step:223	 l-p:0.1274409145116806
epoch£º11	 i:4 	 global-step:224	 l-p:0.14308910071849823
epoch£º11	 i:5 	 global-step:225	 l-p:0.13841389119625092
epoch£º11	 i:6 	 global-step:226	 l-p:0.12569931149482727
epoch£º11	 i:7 	 global-step:227	 l-p:0.20616722106933594
epoch£º11	 i:8 	 global-step:228	 l-p:0.12325731664896011
epoch£º11	 i:9 	 global-step:229	 l-p:0.15525996685028076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9188, 5.3214, 5.2957],
        [4.9188, 4.9555, 4.9266],
        [4.9188, 4.9500, 4.9248],
        [4.9188, 5.0892, 5.0141]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.12307573109865189 
model_pd.l_d.mean(): -19.340229034423828 
model_pd.lagr.mean(): -19.217153549194336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5181], device='cuda:0')), ('power', tensor([-20.0808], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.12307573109865189
epoch£º12	 i:1 	 global-step:241	 l-p:0.1360074281692505
epoch£º12	 i:2 	 global-step:242	 l-p:0.1200442835688591
epoch£º12	 i:3 	 global-step:243	 l-p:0.13655416667461395
epoch£º12	 i:4 	 global-step:244	 l-p:0.13907766342163086
epoch£º12	 i:5 	 global-step:245	 l-p:0.13875256478786469
epoch£º12	 i:6 	 global-step:246	 l-p:0.13550536334514618
epoch£º12	 i:7 	 global-step:247	 l-p:0.13369177281856537
epoch£º12	 i:8 	 global-step:248	 l-p:0.1390436291694641
epoch£º12	 i:9 	 global-step:249	 l-p:0.09760922938585281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8531, 5.9949, 6.6194],
        [4.8531, 4.8531, 4.8531],
        [4.8531, 4.8564, 4.8532],
        [4.8531, 5.2476, 5.2222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.12329307198524475 
model_pd.l_d.mean(): -18.70691680908203 
model_pd.lagr.mean(): -18.5836238861084 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5255], device='cuda:0')), ('power', tensor([-19.4482], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.12329307198524475
epoch£º13	 i:1 	 global-step:261	 l-p:0.18996219336986542
epoch£º13	 i:2 	 global-step:262	 l-p:0.12834058701992035
epoch£º13	 i:3 	 global-step:263	 l-p:0.13055801391601562
epoch£º13	 i:4 	 global-step:264	 l-p:0.13821376860141754
epoch£º13	 i:5 	 global-step:265	 l-p:0.14281058311462402
epoch£º13	 i:6 	 global-step:266	 l-p:0.11233247816562653
epoch£º13	 i:7 	 global-step:267	 l-p:0.13484710454940796
epoch£º13	 i:8 	 global-step:268	 l-p:0.1386806219816208
epoch£º13	 i:9 	 global-step:269	 l-p:0.12354065477848053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8310, 4.8310, 4.8310],
        [4.8310, 5.7266, 6.0897],
        [4.8310, 4.8650, 4.8380],
        [4.8310, 5.6608, 5.9595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.15094657242298126 
model_pd.l_d.mean(): -18.576107025146484 
model_pd.lagr.mean(): -18.425161361694336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5205], device='cuda:0')), ('power', tensor([-19.3109], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.15094657242298126
epoch£º14	 i:1 	 global-step:281	 l-p:0.12489094585180283
epoch£º14	 i:2 	 global-step:282	 l-p:0.16706731915473938
epoch£º14	 i:3 	 global-step:283	 l-p:0.14096157252788544
epoch£º14	 i:4 	 global-step:284	 l-p:0.1355593204498291
epoch£º14	 i:5 	 global-step:285	 l-p:0.12305637449026108
epoch£º14	 i:6 	 global-step:286	 l-p:0.1179165467619896
epoch£º14	 i:7 	 global-step:287	 l-p:0.13151219487190247
epoch£º14	 i:8 	 global-step:288	 l-p:0.13156644999980927
epoch£º14	 i:9 	 global-step:289	 l-p:0.015598921105265617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7478, 4.7787, 4.7538],
        [4.7478, 4.7514, 4.7480],
        [4.7478, 4.9186, 4.8468],
        [4.7478, 4.7583, 4.7488]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.15577492117881775 
model_pd.l_d.mean(): -20.549571990966797 
model_pd.lagr.mean(): -20.393796920776367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4671], device='cuda:0')), ('power', tensor([-21.2513], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.15577492117881775
epoch£º15	 i:1 	 global-step:301	 l-p:0.16217118501663208
epoch£º15	 i:2 	 global-step:302	 l-p:0.12419745326042175
epoch£º15	 i:3 	 global-step:303	 l-p:0.14429180324077606
epoch£º15	 i:4 	 global-step:304	 l-p:0.158433735370636
epoch£º15	 i:5 	 global-step:305	 l-p:0.14607131481170654
epoch£º15	 i:6 	 global-step:306	 l-p:0.11911477893590927
epoch£º15	 i:7 	 global-step:307	 l-p:0.14245356619358063
epoch£º15	 i:8 	 global-step:308	 l-p:0.13309893012046814
epoch£º15	 i:9 	 global-step:309	 l-p:0.2072703093290329
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8623, 4.8642, 4.8624],
        [4.8623, 4.8884, 4.8668],
        [4.8623, 4.8627, 4.8623],
        [4.8623, 5.2191, 5.1793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.12061784416437149 
model_pd.l_d.mean(): -18.991748809814453 
model_pd.lagr.mean(): -18.871131896972656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4860], device='cuda:0')), ('power', tensor([-19.6958], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.12061784416437149
epoch£º16	 i:1 	 global-step:321	 l-p:0.11633116751909256
epoch£º16	 i:2 	 global-step:322	 l-p:0.1511351615190506
epoch£º16	 i:3 	 global-step:323	 l-p:0.13550671935081482
epoch£º16	 i:4 	 global-step:324	 l-p:0.12454651296138763
epoch£º16	 i:5 	 global-step:325	 l-p:0.11478567868471146
epoch£º16	 i:6 	 global-step:326	 l-p:0.1339390128850937
epoch£º16	 i:7 	 global-step:327	 l-p:0.1547188013792038
epoch£º16	 i:8 	 global-step:328	 l-p:0.1300259232521057
epoch£º16	 i:9 	 global-step:329	 l-p:0.12865795195102692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9091, 5.8365, 6.2249],
        [4.9091, 5.2229, 5.1669],
        [4.9091, 4.9091, 4.9090],
        [4.9091, 5.8243, 6.2004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.1605757474899292 
model_pd.l_d.mean(): -20.177288055419922 
model_pd.lagr.mean(): -20.016712188720703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-20.8588], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.1605757474899292
epoch£º17	 i:1 	 global-step:341	 l-p:0.12124791741371155
epoch£º17	 i:2 	 global-step:342	 l-p:0.1369282454252243
epoch£º17	 i:3 	 global-step:343	 l-p:0.1387721449136734
epoch£º17	 i:4 	 global-step:344	 l-p:0.13086798787117004
epoch£º17	 i:5 	 global-step:345	 l-p:0.12558548152446747
epoch£º17	 i:6 	 global-step:346	 l-p:0.13737840950489044
epoch£º17	 i:7 	 global-step:347	 l-p:0.14227284491062164
epoch£º17	 i:8 	 global-step:348	 l-p:0.1304900199174881
epoch£º17	 i:9 	 global-step:349	 l-p:0.13335967063903809
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7509, 4.7988, 4.7634],
        [4.7509, 4.7511, 4.7509],
        [4.7509, 4.8621, 4.8004],
        [4.7509, 4.7632, 4.7523]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.14244747161865234 
model_pd.l_d.mean(): -20.058181762695312 
model_pd.lagr.mean(): -19.915733337402344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5130], device='cuda:0')), ('power', tensor([-20.8015], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.14244747161865234
epoch£º18	 i:1 	 global-step:361	 l-p:0.1964106261730194
epoch£º18	 i:2 	 global-step:362	 l-p:0.13524913787841797
epoch£º18	 i:3 	 global-step:363	 l-p:0.17518547177314758
epoch£º18	 i:4 	 global-step:364	 l-p:0.13196007907390594
epoch£º18	 i:5 	 global-step:365	 l-p:0.13758912682533264
epoch£º18	 i:6 	 global-step:366	 l-p:0.13374997675418854
epoch£º18	 i:7 	 global-step:367	 l-p:0.05584566295146942
epoch£º18	 i:8 	 global-step:368	 l-p:0.1406228393316269
epoch£º18	 i:9 	 global-step:369	 l-p:0.14228110015392303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7707, 4.9428, 4.8715],
        [4.7707, 4.8001, 4.7764],
        [4.7707, 4.7707, 4.7707],
        [4.7707, 4.9752, 4.9040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.14762257039546967 
model_pd.l_d.mean(): -20.52219581604004 
model_pd.lagr.mean(): -20.37457275390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4538], device='cuda:0')), ('power', tensor([-21.2100], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.14762257039546967
epoch£º19	 i:1 	 global-step:381	 l-p:0.1467691957950592
epoch£º19	 i:2 	 global-step:382	 l-p:0.1333903670310974
epoch£º19	 i:3 	 global-step:383	 l-p:0.13146843016147614
epoch£º19	 i:4 	 global-step:384	 l-p:0.12496519088745117
epoch£º19	 i:5 	 global-step:385	 l-p:0.145356684923172
epoch£º19	 i:6 	 global-step:386	 l-p:0.12978971004486084
epoch£º19	 i:7 	 global-step:387	 l-p:0.12460929900407791
epoch£º19	 i:8 	 global-step:388	 l-p:0.10972106456756592
epoch£º19	 i:9 	 global-step:389	 l-p:0.17068132758140564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9244, 4.9271, 4.9245],
        [4.9244, 5.3463, 5.3350],
        [4.9244, 4.9666, 4.9344],
        [4.9244, 4.9853, 4.9425]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.1243649274110794 
model_pd.l_d.mean(): -20.474220275878906 
model_pd.lagr.mean(): -20.349855422973633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4229], device='cuda:0')), ('power', tensor([-21.1300], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.1243649274110794
epoch£º20	 i:1 	 global-step:401	 l-p:0.12764763832092285
epoch£º20	 i:2 	 global-step:402	 l-p:0.12204498797655106
epoch£º20	 i:3 	 global-step:403	 l-p:0.19467642903327942
epoch£º20	 i:4 	 global-step:404	 l-p:0.14041240513324738
epoch£º20	 i:5 	 global-step:405	 l-p:0.13922305405139923
epoch£º20	 i:6 	 global-step:406	 l-p:0.12473925203084946
epoch£º20	 i:7 	 global-step:407	 l-p:0.1312161535024643
epoch£º20	 i:8 	 global-step:408	 l-p:0.125510111451149
epoch£º20	 i:9 	 global-step:409	 l-p:0.12827453017234802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9206, 6.3323, 7.2835],
        [4.9206, 4.9821, 4.9391],
        [4.9206, 4.9206, 4.9206],
        [4.9206, 5.0206, 4.9615]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.13278593122959137 
model_pd.l_d.mean(): -20.315526962280273 
model_pd.lagr.mean(): -20.182741165161133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4409], device='cuda:0')), ('power', tensor([-20.9880], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.13278593122959137
epoch£º21	 i:1 	 global-step:421	 l-p:0.12889425456523895
epoch£º21	 i:2 	 global-step:422	 l-p:0.1364850550889969
epoch£º21	 i:3 	 global-step:423	 l-p:0.130698099732399
epoch£º21	 i:4 	 global-step:424	 l-p:0.1629711091518402
epoch£º21	 i:5 	 global-step:425	 l-p:0.13519862294197083
epoch£º21	 i:6 	 global-step:426	 l-p:0.15799154341220856
epoch£º21	 i:7 	 global-step:427	 l-p:0.16641129553318024
epoch£º21	 i:8 	 global-step:428	 l-p:0.18876037001609802
epoch£º21	 i:9 	 global-step:429	 l-p:0.13467781245708466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7039, 4.8001, 4.7437],
        [4.7039, 4.9843, 4.9283],
        [4.7039, 4.7060, 4.7040],
        [4.7039, 4.7340, 4.7099]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.18139030039310455 
model_pd.l_d.mean(): -20.529512405395508 
model_pd.lagr.mean(): -20.348121643066406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4821], device='cuda:0')), ('power', tensor([-21.2463], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.18139030039310455
epoch£º22	 i:1 	 global-step:441	 l-p:0.13949239253997803
epoch£º22	 i:2 	 global-step:442	 l-p:0.13412989675998688
epoch£º22	 i:3 	 global-step:443	 l-p:0.13338464498519897
epoch£º22	 i:4 	 global-step:444	 l-p:0.11404124647378922
epoch£º22	 i:5 	 global-step:445	 l-p:0.14604125916957855
epoch£º22	 i:6 	 global-step:446	 l-p:0.1198136955499649
epoch£º22	 i:7 	 global-step:447	 l-p:0.1396036446094513
epoch£º22	 i:8 	 global-step:448	 l-p:0.13568784296512604
epoch£º22	 i:9 	 global-step:449	 l-p:0.12129489332437515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0005, 6.4325, 7.3950],
        [5.0005, 5.5550, 5.6192],
        [5.0005, 5.0011, 5.0005],
        [5.0005, 5.9316, 6.3177]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.10506940633058548 
model_pd.l_d.mean(): -19.999509811401367 
model_pd.lagr.mean(): -19.894439697265625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4431], device='cuda:0')), ('power', tensor([-20.6707], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.10506940633058548
epoch£º23	 i:1 	 global-step:461	 l-p:0.13177190721035004
epoch£º23	 i:2 	 global-step:462	 l-p:0.13836048543453217
epoch£º23	 i:3 	 global-step:463	 l-p:0.11578115075826645
epoch£º23	 i:4 	 global-step:464	 l-p:0.13819649815559387
epoch£º23	 i:5 	 global-step:465	 l-p:0.13182738423347473
epoch£º23	 i:6 	 global-step:466	 l-p:0.15629075467586517
epoch£º23	 i:7 	 global-step:467	 l-p:0.12824773788452148
epoch£º23	 i:8 	 global-step:468	 l-p:0.14313864707946777
epoch£º23	 i:9 	 global-step:469	 l-p:0.14557309448719025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8297, 5.2774, 5.2876],
        [4.8297, 5.1131, 5.0544],
        [4.8297, 4.9099, 4.8588],
        [4.8297, 4.8297, 4.8297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.13809721171855927 
model_pd.l_d.mean(): -20.957128524780273 
model_pd.lagr.mean(): -20.81903076171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4010], device='cuda:0')), ('power', tensor([-21.5958], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.13809721171855927
epoch£º24	 i:1 	 global-step:481	 l-p:0.14638172090053558
epoch£º24	 i:2 	 global-step:482	 l-p:0.12133727967739105
epoch£º24	 i:3 	 global-step:483	 l-p:0.12021403759717941
epoch£º24	 i:4 	 global-step:484	 l-p:0.14195190370082855
epoch£º24	 i:5 	 global-step:485	 l-p:0.11674381047487259
epoch£º24	 i:6 	 global-step:486	 l-p:0.14352476596832275
epoch£º24	 i:7 	 global-step:487	 l-p:0.1408405303955078
epoch£º24	 i:8 	 global-step:488	 l-p:0.1871071606874466
epoch£º24	 i:9 	 global-step:489	 l-p:0.12445072084665298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8869, 4.8869, 4.8869],
        [4.8869, 4.8869, 4.8869],
        [4.8869, 4.8869, 4.8869],
        [4.8869, 6.0146, 6.6315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.13400283455848694 
model_pd.l_d.mean(): -18.35012435913086 
model_pd.lagr.mean(): -18.216121673583984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5760], device='cuda:0')), ('power', tensor([-19.1391], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.13400283455848694
epoch£º25	 i:1 	 global-step:501	 l-p:0.1348123401403427
epoch£º25	 i:2 	 global-step:502	 l-p:0.1424340158700943
epoch£º25	 i:3 	 global-step:503	 l-p:0.12410889565944672
epoch£º25	 i:4 	 global-step:504	 l-p:0.38650745153427124
epoch£º25	 i:5 	 global-step:505	 l-p:0.14746809005737305
epoch£º25	 i:6 	 global-step:506	 l-p:0.13004587590694427
epoch£º25	 i:7 	 global-step:507	 l-p:0.128732368350029
epoch£º25	 i:8 	 global-step:508	 l-p:0.14900240302085876
epoch£º25	 i:9 	 global-step:509	 l-p:0.12858295440673828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8317, 5.0213, 4.9503],
        [4.8317, 5.4288, 5.5392],
        [4.8317, 4.8906, 4.8494],
        [4.8317, 5.3466, 5.3990]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.15572033822536469 
model_pd.l_d.mean(): -20.57039451599121 
model_pd.lagr.mean(): -20.414674758911133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4454], device='cuda:0')), ('power', tensor([-21.2502], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.15572033822536469
epoch£º26	 i:1 	 global-step:521	 l-p:0.13857898116111755
epoch£º26	 i:2 	 global-step:522	 l-p:0.13958783447742462
epoch£º26	 i:3 	 global-step:523	 l-p:0.12689097225666046
epoch£º26	 i:4 	 global-step:524	 l-p:0.005108680576086044
epoch£º26	 i:5 	 global-step:525	 l-p:0.1563279628753662
epoch£º26	 i:6 	 global-step:526	 l-p:0.1412874311208725
epoch£º26	 i:7 	 global-step:527	 l-p:0.14351704716682434
epoch£º26	 i:8 	 global-step:528	 l-p:0.14050118625164032
epoch£º26	 i:9 	 global-step:529	 l-p:0.12953172624111176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8516, 4.9671, 4.9046],
        [4.8516, 5.1809, 5.1367],
        [4.8516, 4.8516, 4.8516],
        [4.8516, 4.8541, 4.8517]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.1248527318239212 
model_pd.l_d.mean(): -18.258005142211914 
model_pd.lagr.mean(): -18.13315200805664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5421], device='cuda:0')), ('power', tensor([-19.0114], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.1248527318239212
epoch£º27	 i:1 	 global-step:541	 l-p:0.13130123913288116
epoch£º27	 i:2 	 global-step:542	 l-p:0.11955446749925613
epoch£º27	 i:3 	 global-step:543	 l-p:0.14030654728412628
epoch£º27	 i:4 	 global-step:544	 l-p:0.13030587136745453
epoch£º27	 i:5 	 global-step:545	 l-p:0.13880915939807892
epoch£º27	 i:6 	 global-step:546	 l-p:0.13480256497859955
epoch£º27	 i:7 	 global-step:547	 l-p:0.14729155600070953
epoch£º27	 i:8 	 global-step:548	 l-p:0.3794190585613251
epoch£º27	 i:9 	 global-step:549	 l-p:0.1356358379125595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8318, 5.0297, 4.9594],
        [4.8318, 5.4246, 5.5333],
        [4.8318, 4.8797, 4.8445],
        [4.8318, 6.1341, 6.9727]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.13328687846660614 
model_pd.l_d.mean(): -19.90778350830078 
model_pd.lagr.mean(): -19.77449607849121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4996], device='cuda:0')), ('power', tensor([-20.6357], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.13328687846660614
epoch£º28	 i:1 	 global-step:561	 l-p:0.16079841554164886
epoch£º28	 i:2 	 global-step:562	 l-p:0.12945257127285004
epoch£º28	 i:3 	 global-step:563	 l-p:0.12827639281749725
epoch£º28	 i:4 	 global-step:564	 l-p:0.1363905966281891
epoch£º28	 i:5 	 global-step:565	 l-p:0.16052936017513275
epoch£º28	 i:6 	 global-step:566	 l-p:0.12322255969047546
epoch£º28	 i:7 	 global-step:567	 l-p:0.12306928634643555
epoch£º28	 i:8 	 global-step:568	 l-p:0.12756717205047607
epoch£º28	 i:9 	 global-step:569	 l-p:0.13800181448459625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0150, 5.0150, 5.0150],
        [5.0150, 5.2199, 5.1464],
        [5.0150, 5.6643, 5.8007],
        [5.0150, 5.0193, 5.0153]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.13617298007011414 
model_pd.l_d.mean(): -19.668298721313477 
model_pd.lagr.mean(): -19.53212547302246 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4471], device='cuda:0')), ('power', tensor([-20.3400], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.13617298007011414
epoch£º29	 i:1 	 global-step:581	 l-p:0.10736896097660065
epoch£º29	 i:2 	 global-step:582	 l-p:0.11962665617465973
epoch£º29	 i:3 	 global-step:583	 l-p:0.13835927844047546
epoch£º29	 i:4 	 global-step:584	 l-p:0.1320590078830719
epoch£º29	 i:5 	 global-step:585	 l-p:0.13992439210414886
epoch£º29	 i:6 	 global-step:586	 l-p:0.2121214121580124
epoch£º29	 i:7 	 global-step:587	 l-p:0.1248338371515274
epoch£º29	 i:8 	 global-step:588	 l-p:0.1286928504705429
epoch£º29	 i:9 	 global-step:589	 l-p:0.13239063322544098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8334, 4.8350, 4.8335],
        [4.8334, 6.1193, 6.9392],
        [4.8334, 5.0449, 4.9758],
        [4.8334, 6.0008, 6.6780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.15086640417575836 
model_pd.l_d.mean(): -18.951662063598633 
model_pd.lagr.mean(): -18.800796508789062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5613], device='cuda:0')), ('power', tensor([-19.7322], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.15086640417575836
epoch£º30	 i:1 	 global-step:601	 l-p:0.1221606582403183
epoch£º30	 i:2 	 global-step:602	 l-p:0.12749309837818146
epoch£º30	 i:3 	 global-step:603	 l-p:0.01176495011895895
epoch£º30	 i:4 	 global-step:604	 l-p:0.13113868236541748
epoch£º30	 i:5 	 global-step:605	 l-p:0.16878890991210938
epoch£º30	 i:6 	 global-step:606	 l-p:0.13794544339179993
epoch£º30	 i:7 	 global-step:607	 l-p:0.13072313368320465
epoch£º30	 i:8 	 global-step:608	 l-p:0.1297627091407776
epoch£º30	 i:9 	 global-step:609	 l-p:0.15519964694976807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7230, 4.7230, 4.7230],
        [4.7230, 4.7310, 4.7237],
        [4.7230, 6.0393, 6.9206],
        [4.7230, 4.7768, 4.7387]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.13403412699699402 
model_pd.l_d.mean(): -18.06913948059082 
model_pd.lagr.mean(): -17.93510627746582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6331], device='cuda:0')), ('power', tensor([-18.9134], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.13403412699699402
epoch£º31	 i:1 	 global-step:621	 l-p:0.17097526788711548
epoch£º31	 i:2 	 global-step:622	 l-p:0.12065774947404861
epoch£º31	 i:3 	 global-step:623	 l-p:0.13284063339233398
epoch£º31	 i:4 	 global-step:624	 l-p:0.13170890510082245
epoch£º31	 i:5 	 global-step:625	 l-p:0.12651528418064117
epoch£º31	 i:6 	 global-step:626	 l-p:0.14885224401950836
epoch£º31	 i:7 	 global-step:627	 l-p:0.20637066662311554
epoch£º31	 i:8 	 global-step:628	 l-p:0.14710265398025513
epoch£º31	 i:9 	 global-step:629	 l-p:0.16361552476882935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8148, 6.2454, 7.2552],
        [4.8148, 5.7434, 6.1608],
        [4.8148, 4.8712, 4.8316],
        [4.8148, 4.8162, 4.8148]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.1348629593849182 
model_pd.l_d.mean(): -19.91737937927246 
model_pd.lagr.mean(): -19.782516479492188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4778], device='cuda:0')), ('power', tensor([-20.6232], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.1348629593849182
epoch£º32	 i:1 	 global-step:641	 l-p:0.14056627452373505
epoch£º32	 i:2 	 global-step:642	 l-p:0.15270274877548218
epoch£º32	 i:3 	 global-step:643	 l-p:0.1226506233215332
epoch£º32	 i:4 	 global-step:644	 l-p:0.17839185893535614
epoch£º32	 i:5 	 global-step:645	 l-p:0.1390889436006546
epoch£º32	 i:6 	 global-step:646	 l-p:0.15400078892707825
epoch£º32	 i:7 	 global-step:647	 l-p:0.28045305609703064
epoch£º32	 i:8 	 global-step:648	 l-p:0.11922266334295273
epoch£º32	 i:9 	 global-step:649	 l-p:0.1317528486251831
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9064, 4.9066, 4.9064],
        [4.9064, 5.1508, 5.0847],
        [4.9064, 4.9071, 4.9064],
        [4.9064, 5.1511, 5.0849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.1178995743393898 
model_pd.l_d.mean(): -20.350543975830078 
model_pd.lagr.mean(): -20.23264503479004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4456], device='cuda:0')), ('power', tensor([-21.0282], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.1178995743393898
epoch£º33	 i:1 	 global-step:661	 l-p:0.2017344981431961
epoch£º33	 i:2 	 global-step:662	 l-p:0.13895784318447113
epoch£º33	 i:3 	 global-step:663	 l-p:0.11759907752275467
epoch£º33	 i:4 	 global-step:664	 l-p:0.1404702365398407
epoch£º33	 i:5 	 global-step:665	 l-p:0.13330426812171936
epoch£º33	 i:6 	 global-step:666	 l-p:0.1486963927745819
epoch£º33	 i:7 	 global-step:667	 l-p:0.15925201773643494
epoch£º33	 i:8 	 global-step:668	 l-p:0.13533394038677216
epoch£º33	 i:9 	 global-step:669	 l-p:0.19958792626857758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7523, 5.3300, 5.4378],
        [4.7523, 4.7525, 4.7523],
        [4.7523, 5.0221, 4.9660],
        [4.7523, 4.8058, 4.7679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.15000683069229126 
model_pd.l_d.mean(): -20.41020965576172 
model_pd.lagr.mean(): -20.260202407836914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4844], device='cuda:0')), ('power', tensor([-21.1281], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.15000683069229126
epoch£º34	 i:1 	 global-step:681	 l-p:0.14427055418491364
epoch£º34	 i:2 	 global-step:682	 l-p:0.13362863659858704
epoch£º34	 i:3 	 global-step:683	 l-p:0.12760525941848755
epoch£º34	 i:4 	 global-step:684	 l-p:0.15791381895542145
epoch£º34	 i:5 	 global-step:685	 l-p:0.14213170111179352
epoch£º34	 i:6 	 global-step:686	 l-p:0.14003965258598328
epoch£º34	 i:7 	 global-step:687	 l-p:0.136570543050766
epoch£º34	 i:8 	 global-step:688	 l-p:0.12412753701210022
epoch£º34	 i:9 	 global-step:689	 l-p:0.17167051136493683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9326, 5.3872, 5.4008],
        [4.9326, 4.9326, 4.9326],
        [4.9326, 5.0646, 4.9986],
        [4.9326, 6.2882, 7.1788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.119378961622715 
model_pd.l_d.mean(): -19.90288543701172 
model_pd.lagr.mean(): -19.783506393432617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4777], device='cuda:0')), ('power', tensor([-20.6084], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.119378961622715
epoch£º35	 i:1 	 global-step:701	 l-p:0.13346023857593536
epoch£º35	 i:2 	 global-step:702	 l-p:0.15160039067268372
epoch£º35	 i:3 	 global-step:703	 l-p:0.1281459480524063
epoch£º35	 i:4 	 global-step:704	 l-p:0.1159839779138565
epoch£º35	 i:5 	 global-step:705	 l-p:0.1319238245487213
epoch£º35	 i:6 	 global-step:706	 l-p:0.14844465255737305
epoch£º35	 i:7 	 global-step:707	 l-p:0.1274728626012802
epoch£º35	 i:8 	 global-step:708	 l-p:0.10740445554256439
epoch£º35	 i:9 	 global-step:709	 l-p:0.12258648872375488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8829, 4.8829, 4.8829],
        [4.8829, 4.8829, 4.8829],
        [4.8829, 5.5178, 5.6593],
        [4.8829, 4.9286, 4.8948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.13556872308254242 
model_pd.l_d.mean(): -19.032873153686523 
model_pd.lagr.mean(): -18.89730453491211 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5131], device='cuda:0')), ('power', tensor([-19.7650], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.13556872308254242
epoch£º36	 i:1 	 global-step:721	 l-p:0.11874748021364212
epoch£º36	 i:2 	 global-step:722	 l-p:0.14395715296268463
epoch£º36	 i:3 	 global-step:723	 l-p:0.1194489449262619
epoch£º36	 i:4 	 global-step:724	 l-p:0.13529376685619354
epoch£º36	 i:5 	 global-step:725	 l-p:0.15216578543186188
epoch£º36	 i:6 	 global-step:726	 l-p:0.15417824685573578
epoch£º36	 i:7 	 global-step:727	 l-p:0.13981586694717407
epoch£º36	 i:8 	 global-step:728	 l-p:-0.2512494623661041
epoch£º36	 i:9 	 global-step:729	 l-p:0.1316712200641632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8199, 4.8259, 4.8204],
        [4.8199, 6.0991, 6.9193],
        [4.8199, 5.2085, 5.1949],
        [4.8199, 5.3073, 5.3491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.12874582409858704 
model_pd.l_d.mean(): -18.53508186340332 
model_pd.lagr.mean(): -18.406335830688477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5719], device='cuda:0')), ('power', tensor([-19.3218], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.12874582409858704
epoch£º37	 i:1 	 global-step:741	 l-p:0.12683430314064026
epoch£º37	 i:2 	 global-step:742	 l-p:0.23317618668079376
epoch£º37	 i:3 	 global-step:743	 l-p:0.12618312239646912
epoch£º37	 i:4 	 global-step:744	 l-p:0.1402868777513504
epoch£º37	 i:5 	 global-step:745	 l-p:0.1320992410182953
epoch£º37	 i:6 	 global-step:746	 l-p:0.12678919732570648
epoch£º37	 i:7 	 global-step:747	 l-p:0.1179887056350708
epoch£º37	 i:8 	 global-step:748	 l-p:0.12296472489833832
epoch£º37	 i:9 	 global-step:749	 l-p:0.1188793033361435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9075, 5.0171, 4.9567],
        [4.9075, 4.9725, 4.9286],
        [4.9075, 4.9326, 4.9120],
        [4.9075, 4.9696, 4.9270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.1629534810781479 
model_pd.l_d.mean(): -19.890724182128906 
model_pd.lagr.mean(): -19.72776985168457 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4836], device='cuda:0')), ('power', tensor([-20.6022], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.1629534810781479
epoch£º38	 i:1 	 global-step:761	 l-p:0.14156821370124817
epoch£º38	 i:2 	 global-step:762	 l-p:0.1292097419500351
epoch£º38	 i:3 	 global-step:763	 l-p:0.12469333410263062
epoch£º38	 i:4 	 global-step:764	 l-p:0.1333230435848236
epoch£º38	 i:5 	 global-step:765	 l-p:0.12368366122245789
epoch£º38	 i:6 	 global-step:766	 l-p:0.17688822746276855
epoch£º38	 i:7 	 global-step:767	 l-p:0.13974910974502563
epoch£º38	 i:8 	 global-step:768	 l-p:0.1474287509918213
epoch£º38	 i:9 	 global-step:769	 l-p:0.12311303615570068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7669, 4.7669, 4.7669],
        [4.7669, 5.8098, 6.3631],
        [4.7669, 4.7669, 4.7669],
        [4.7669, 4.7689, 4.7670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.13791495561599731 
model_pd.l_d.mean(): -20.780826568603516 
model_pd.lagr.mean(): -20.642911911010742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4421], device='cuda:0')), ('power', tensor([-21.4596], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.13791495561599731
epoch£º39	 i:1 	 global-step:781	 l-p:0.13355515897274017
epoch£º39	 i:2 	 global-step:782	 l-p:0.10830505192279816
epoch£º39	 i:3 	 global-step:783	 l-p:0.13879545032978058
epoch£º39	 i:4 	 global-step:784	 l-p:0.16920572519302368
epoch£º39	 i:5 	 global-step:785	 l-p:0.19250859320163727
epoch£º39	 i:6 	 global-step:786	 l-p:0.1455109417438507
epoch£º39	 i:7 	 global-step:787	 l-p:0.14439333975315094
epoch£º39	 i:8 	 global-step:788	 l-p:0.13691245019435883
epoch£º39	 i:9 	 global-step:789	 l-p:0.12353097647428513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9624, 4.9626, 4.9624],
        [4.9624, 4.9809, 4.9652],
        [4.9624, 5.0748, 5.0134],
        [4.9624, 4.9699, 4.9631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.11194060742855072 
model_pd.l_d.mean(): -20.54973030090332 
model_pd.lagr.mean(): -20.437789916992188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4112], device='cuda:0')), ('power', tensor([-21.1944], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.11194060742855072
epoch£º40	 i:1 	 global-step:801	 l-p:0.13013167679309845
epoch£º40	 i:2 	 global-step:802	 l-p:0.11617660522460938
epoch£º40	 i:3 	 global-step:803	 l-p:0.15148191154003143
epoch£º40	 i:4 	 global-step:804	 l-p:0.12384332716464996
epoch£º40	 i:5 	 global-step:805	 l-p:0.11678904294967651
epoch£º40	 i:6 	 global-step:806	 l-p:0.12566936016082764
epoch£º40	 i:7 	 global-step:807	 l-p:0.19719886779785156
epoch£º40	 i:8 	 global-step:808	 l-p:0.12911376357078552
epoch£º40	 i:9 	 global-step:809	 l-p:0.1451345682144165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9346, 4.9372, 4.9348],
        [4.9346, 4.9350, 4.9346],
        [4.9346, 5.1178, 5.0478],
        [4.9346, 5.0490, 4.9875]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.14238078892230988 
model_pd.l_d.mean(): -20.475343704223633 
model_pd.lagr.mean(): -20.332962036132812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4205], device='cuda:0')), ('power', tensor([-21.1286], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.14238078892230988
epoch£º41	 i:1 	 global-step:821	 l-p:0.12187550961971283
epoch£º41	 i:2 	 global-step:822	 l-p:0.11777671426534653
epoch£º41	 i:3 	 global-step:823	 l-p:0.1327102780342102
epoch£º41	 i:4 	 global-step:824	 l-p:0.22756117582321167
epoch£º41	 i:5 	 global-step:825	 l-p:0.12361916899681091
epoch£º41	 i:6 	 global-step:826	 l-p:0.13086546957492828
epoch£º41	 i:7 	 global-step:827	 l-p:0.12329093366861343
epoch£º41	 i:8 	 global-step:828	 l-p:0.1310649961233139
epoch£º41	 i:9 	 global-step:829	 l-p:0.16321763396263123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8334, 5.1117, 5.0574],
        [4.8334, 4.8334, 4.8334],
        [4.8334, 6.1941, 7.1180],
        [4.8334, 4.8490, 4.8355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.12238797545433044 
model_pd.l_d.mean(): -18.99122428894043 
model_pd.lagr.mean(): -18.86883544921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5236], device='cuda:0')), ('power', tensor([-19.7336], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.12238797545433044
epoch£º42	 i:1 	 global-step:841	 l-p:0.13512763381004333
epoch£º42	 i:2 	 global-step:842	 l-p:0.13941630721092224
epoch£º42	 i:3 	 global-step:843	 l-p:0.2128729522228241
epoch£º42	 i:4 	 global-step:844	 l-p:0.15254870057106018
epoch£º42	 i:5 	 global-step:845	 l-p:0.14720894396305084
epoch£º42	 i:6 	 global-step:846	 l-p:0.13498574495315552
epoch£º42	 i:7 	 global-step:847	 l-p:0.09509290754795074
epoch£º42	 i:8 	 global-step:848	 l-p:0.13840322196483612
epoch£º42	 i:9 	 global-step:849	 l-p:0.161254420876503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4331,  0.3277,  1.0000,  0.2480,
          1.0000,  0.7566, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[4.7679, 5.9701, 6.7115],
        [4.7679, 5.3090, 5.3940],
        [4.7679, 5.0661, 5.0204],
        [4.7679, 5.9103, 6.5806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.06236201152205467 
model_pd.l_d.mean(): -19.928688049316406 
model_pd.lagr.mean(): -19.86632537841797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5186], device='cuda:0')), ('power', tensor([-20.6763], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.06236201152205467
epoch£º43	 i:1 	 global-step:861	 l-p:0.14900276064872742
epoch£º43	 i:2 	 global-step:862	 l-p:0.1346110850572586
epoch£º43	 i:3 	 global-step:863	 l-p:0.13241025805473328
epoch£º43	 i:4 	 global-step:864	 l-p:0.1403082013130188
epoch£º43	 i:5 	 global-step:865	 l-p:0.12816762924194336
epoch£º43	 i:6 	 global-step:866	 l-p:0.1303037703037262
epoch£º43	 i:7 	 global-step:867	 l-p:0.142071932554245
epoch£º43	 i:8 	 global-step:868	 l-p:0.12005418539047241
epoch£º43	 i:9 	 global-step:869	 l-p:0.14093871414661407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8878, 5.0136, 4.9502],
        [4.8878, 4.8948, 4.8884],
        [4.8878, 4.8902, 4.8880],
        [4.8878, 4.8879, 4.8878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.13652120530605316 
model_pd.l_d.mean(): -20.439321517944336 
model_pd.lagr.mean(): -20.30280113220215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4325], device='cuda:0')), ('power', tensor([-21.1045], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.13652120530605316
epoch£º44	 i:1 	 global-step:881	 l-p:0.13807760179042816
epoch£º44	 i:2 	 global-step:882	 l-p:-0.03221452608704567
epoch£º44	 i:3 	 global-step:883	 l-p:0.1403733342885971
epoch£º44	 i:4 	 global-step:884	 l-p:0.13725218176841736
epoch£º44	 i:5 	 global-step:885	 l-p:0.12938448786735535
epoch£º44	 i:6 	 global-step:886	 l-p:0.1125863790512085
epoch£º44	 i:7 	 global-step:887	 l-p:0.15188005566596985
epoch£º44	 i:8 	 global-step:888	 l-p:0.16582362353801727
epoch£º44	 i:9 	 global-step:889	 l-p:0.12697510421276093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8619, 4.8619, 4.8619],
        [4.8619, 4.8629, 4.8619],
        [4.8619, 5.0938, 5.0297],
        [4.8619, 4.8619, 4.8619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.13258913159370422 
model_pd.l_d.mean(): -20.82813262939453 
model_pd.lagr.mean(): -20.69554328918457 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4113], device='cuda:0')), ('power', tensor([-21.4759], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.13258913159370422
epoch£º45	 i:1 	 global-step:901	 l-p:0.13080714643001556
epoch£º45	 i:2 	 global-step:902	 l-p:0.11158214509487152
epoch£º45	 i:3 	 global-step:903	 l-p:0.12573997676372528
epoch£º45	 i:4 	 global-step:904	 l-p:0.14953848719596863
epoch£º45	 i:5 	 global-step:905	 l-p:-0.0076849935576319695
epoch£º45	 i:6 	 global-step:906	 l-p:0.14163838326931
epoch£º45	 i:7 	 global-step:907	 l-p:0.1417524665594101
epoch£º45	 i:8 	 global-step:908	 l-p:0.16078774631023407
epoch£º45	 i:9 	 global-step:909	 l-p:0.1286722719669342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8843, 4.8885, 4.8846],
        [4.8843, 4.9107, 4.8893],
        [4.8843, 4.8843, 4.8843],
        [4.8843, 4.9101, 4.8891]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.11818859726190567 
model_pd.l_d.mean(): -19.682449340820312 
model_pd.lagr.mean(): -19.564260482788086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4923], device='cuda:0')), ('power', tensor([-20.4005], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.11818859726190567
epoch£º46	 i:1 	 global-step:921	 l-p:0.24148528277873993
epoch£º46	 i:2 	 global-step:922	 l-p:0.12811942398548126
epoch£º46	 i:3 	 global-step:923	 l-p:0.12558238208293915
epoch£º46	 i:4 	 global-step:924	 l-p:0.14085669815540314
epoch£º46	 i:5 	 global-step:925	 l-p:0.1160445287823677
epoch£º46	 i:6 	 global-step:926	 l-p:0.14508891105651855
epoch£º46	 i:7 	 global-step:927	 l-p:0.1322476714849472
epoch£º46	 i:8 	 global-step:928	 l-p:0.12909680604934692
epoch£º46	 i:9 	 global-step:929	 l-p:0.15558670461177826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8284, 4.8840, 4.8452],
        [4.8284, 4.9346, 4.8765],
        [4.8284, 4.8301, 4.8284],
        [4.8284, 5.1694, 5.1380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.12015224248170853 
model_pd.l_d.mean(): -18.425655364990234 
model_pd.lagr.mean(): -18.305503845214844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5323], device='cuda:0')), ('power', tensor([-19.1709], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.12015224248170853
epoch£º47	 i:1 	 global-step:941	 l-p:0.14286480844020844
epoch£º47	 i:2 	 global-step:942	 l-p:0.1649903655052185
epoch£º47	 i:3 	 global-step:943	 l-p:0.1309085488319397
epoch£º47	 i:4 	 global-step:944	 l-p:0.1378817856311798
epoch£º47	 i:5 	 global-step:945	 l-p:0.11365701258182526
epoch£º47	 i:6 	 global-step:946	 l-p:0.13483518362045288
epoch£º47	 i:7 	 global-step:947	 l-p:0.1350652426481247
epoch£º47	 i:8 	 global-step:948	 l-p:0.1337072104215622
epoch£º47	 i:9 	 global-step:949	 l-p:0.1701151579618454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7862, 5.1243, 5.0940],
        [4.7862, 4.9667, 4.9006],
        [4.7862, 4.7862, 4.7862],
        [4.7862, 5.0162, 4.9545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.17270885407924652 
model_pd.l_d.mean(): -20.85230255126953 
model_pd.lagr.mean(): -20.679594039916992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4346], device='cuda:0')), ('power', tensor([-21.5241], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.17270885407924652
epoch£º48	 i:1 	 global-step:961	 l-p:0.12782037258148193
epoch£º48	 i:2 	 global-step:962	 l-p:0.13549137115478516
epoch£º48	 i:3 	 global-step:963	 l-p:0.12013398855924606
epoch£º48	 i:4 	 global-step:964	 l-p:0.13807326555252075
epoch£º48	 i:5 	 global-step:965	 l-p:0.21216601133346558
epoch£º48	 i:6 	 global-step:966	 l-p:0.12402007728815079
epoch£º48	 i:7 	 global-step:967	 l-p:0.12953077256679535
epoch£º48	 i:8 	 global-step:968	 l-p:0.11850964277982712
epoch£º48	 i:9 	 global-step:969	 l-p:0.11744974553585052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9222, 4.9565, 4.9298],
        [4.9222, 4.9235, 4.9222],
        [4.9222, 5.0805, 5.0129],
        [4.9222, 4.9283, 4.9226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.1259399801492691 
model_pd.l_d.mean(): -20.845258712768555 
model_pd.lagr.mean(): -20.719318389892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3920], device='cuda:0')), ('power', tensor([-21.4735], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.1259399801492691
epoch£º49	 i:1 	 global-step:981	 l-p:0.13945183157920837
epoch£º49	 i:2 	 global-step:982	 l-p:0.159671351313591
epoch£º49	 i:3 	 global-step:983	 l-p:0.13257470726966858
epoch£º49	 i:4 	 global-step:984	 l-p:0.14661988615989685
epoch£º49	 i:5 	 global-step:985	 l-p:0.14536651968955994
epoch£º49	 i:6 	 global-step:986	 l-p:0.1393745243549347
epoch£º49	 i:7 	 global-step:987	 l-p:0.19165420532226562
epoch£º49	 i:8 	 global-step:988	 l-p:0.11981222033500671
epoch£º49	 i:9 	 global-step:989	 l-p:0.14000070095062256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8121, 5.6421, 5.9736],
        [4.8121, 4.8122, 4.8121],
        [4.8121, 5.1292, 5.0902],
        [4.8121, 4.8138, 4.8121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.1387050449848175 
model_pd.l_d.mean(): -20.09463119506836 
model_pd.lagr.mean(): -19.9559268951416 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4945], device='cuda:0')), ('power', tensor([-20.8194], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.1387050449848175
epoch£º50	 i:1 	 global-step:1001	 l-p:0.13915686309337616
epoch£º50	 i:2 	 global-step:1002	 l-p:0.1439087986946106
epoch£º50	 i:3 	 global-step:1003	 l-p:0.14031468331813812
epoch£º50	 i:4 	 global-step:1004	 l-p:0.12623010575771332
epoch£º50	 i:5 	 global-step:1005	 l-p:0.12198721617460251
epoch£º50	 i:6 	 global-step:1006	 l-p:0.133905291557312
epoch£º50	 i:7 	 global-step:1007	 l-p:0.1457744538784027
epoch£º50	 i:8 	 global-step:1008	 l-p:0.1240728572010994
epoch£º50	 i:9 	 global-step:1009	 l-p:0.1359298676252365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9641, 4.9641, 4.9641],
        [4.9641, 5.5699, 5.6900],
        [4.9641, 6.2928, 7.1561],
        [4.9641, 4.9642, 4.9641]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.11433090269565582 
model_pd.l_d.mean(): -18.682893753051758 
model_pd.lagr.mean(): -18.56856346130371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5100], device='cuda:0')), ('power', tensor([-19.4081], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.11433090269565582
epoch£º51	 i:1 	 global-step:1021	 l-p:0.10961990803480148
epoch£º51	 i:2 	 global-step:1022	 l-p:0.11620549857616425
epoch£º51	 i:3 	 global-step:1023	 l-p:0.12692879140377045
epoch£º51	 i:4 	 global-step:1024	 l-p:0.12340952455997467
epoch£º51	 i:5 	 global-step:1025	 l-p:0.12480290979146957
epoch£º51	 i:6 	 global-step:1026	 l-p:0.1382141411304474
epoch£º51	 i:7 	 global-step:1027	 l-p:0.18905295431613922
epoch£º51	 i:8 	 global-step:1028	 l-p:0.15352876484394073
epoch£º51	 i:9 	 global-step:1029	 l-p:0.15872779488563538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7054, 5.2840, 5.4071],
        [4.7054, 5.1215, 5.1349],
        [4.7054, 4.7539, 4.7193],
        [4.7054, 4.7054, 4.7054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.14334242045879364 
model_pd.l_d.mean(): -20.545291900634766 
model_pd.lagr.mean(): -20.401948928833008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4688], device='cuda:0')), ('power', tensor([-21.2487], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.14334242045879364
epoch£º52	 i:1 	 global-step:1041	 l-p:0.1566525399684906
epoch£º52	 i:2 	 global-step:1042	 l-p:0.13932733237743378
epoch£º52	 i:3 	 global-step:1043	 l-p:0.1480974704027176
epoch£º52	 i:4 	 global-step:1044	 l-p:0.040672667324543
epoch£º52	 i:5 	 global-step:1045	 l-p:0.1460200697183609
epoch£º52	 i:6 	 global-step:1046	 l-p:0.16352491080760956
epoch£º52	 i:7 	 global-step:1047	 l-p:-0.14911627769470215
epoch£º52	 i:8 	 global-step:1048	 l-p:0.19619663059711456
epoch£º52	 i:9 	 global-step:1049	 l-p:0.10795406997203827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6464, 4.6544, 4.6472],
        [4.6464, 4.6465, 4.6464],
        [4.6464, 4.6470, 4.6464],
        [4.6464, 4.6469, 4.6464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.15336982905864716 
model_pd.l_d.mean(): -19.0693359375 
model_pd.lagr.mean(): -18.915966033935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5614], device='cuda:0')), ('power', tensor([-19.8512], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.15336982905864716
epoch£º53	 i:1 	 global-step:1061	 l-p:0.13261447846889496
epoch£º53	 i:2 	 global-step:1062	 l-p:0.13290844857692719
epoch£º53	 i:3 	 global-step:1063	 l-p:-0.6200470924377441
epoch£º53	 i:4 	 global-step:1064	 l-p:0.12235389649868011
epoch£º53	 i:5 	 global-step:1065	 l-p:0.11967462301254272
epoch£º53	 i:6 	 global-step:1066	 l-p:0.12426762282848358
epoch£º53	 i:7 	 global-step:1067	 l-p:0.12091457843780518
epoch£º53	 i:8 	 global-step:1068	 l-p:0.13446269929409027
epoch£º53	 i:9 	 global-step:1069	 l-p:0.13642306625843048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0138, 5.0138, 5.0138],
        [5.0138, 5.7298, 5.9357],
        [5.0138, 5.7818, 6.0333],
        [5.0138, 5.1210, 5.0617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.1283007562160492 
model_pd.l_d.mean(): -20.59379768371582 
model_pd.lagr.mean(): -20.465496063232422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3899], device='cuda:0')), ('power', tensor([-21.2171], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.1283007562160492
epoch£º54	 i:1 	 global-step:1081	 l-p:0.12305673211812973
epoch£º54	 i:2 	 global-step:1082	 l-p:0.12654590606689453
epoch£º54	 i:3 	 global-step:1083	 l-p:0.11654889583587646
epoch£º54	 i:4 	 global-step:1084	 l-p:0.125128835439682
epoch£º54	 i:5 	 global-step:1085	 l-p:0.1412522941827774
epoch£º54	 i:6 	 global-step:1086	 l-p:0.049731191247701645
epoch£º54	 i:7 	 global-step:1087	 l-p:0.15598686039447784
epoch£º54	 i:8 	 global-step:1088	 l-p:0.13251037895679474
epoch£º54	 i:9 	 global-step:1089	 l-p:0.14124998450279236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5530,  0.4539,  1.0000,  0.3726,
          1.0000,  0.8208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7399,  0.6692,  1.0000,  0.6053,
          1.0000,  0.9045, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5585,  0.4600,  1.0000,  0.3788,
          1.0000,  0.8235, 31.6228]], device='cuda:0')
 pt:tensor([[4.8611, 5.5963, 5.8362],
        [4.8611, 5.8973, 6.4395],
        [4.8611, 5.2947, 5.3095],
        [4.8611, 5.6054, 5.8533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.1556423157453537 
model_pd.l_d.mean(): -20.037296295166016 
model_pd.lagr.mean(): -19.881654739379883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4839], device='cuda:0')), ('power', tensor([-20.7506], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.1556423157453537
epoch£º55	 i:1 	 global-step:1101	 l-p:-0.7849821448326111
epoch£º55	 i:2 	 global-step:1102	 l-p:0.14931732416152954
epoch£º55	 i:3 	 global-step:1103	 l-p:0.15564197301864624
epoch£º55	 i:4 	 global-step:1104	 l-p:0.10965920984745026
epoch£º55	 i:5 	 global-step:1105	 l-p:0.11472263187170029
epoch£º55	 i:6 	 global-step:1106	 l-p:0.1134539395570755
epoch£º55	 i:7 	 global-step:1107	 l-p:0.12120098620653152
epoch£º55	 i:8 	 global-step:1108	 l-p:0.11333505064249039
epoch£º55	 i:9 	 global-step:1109	 l-p:0.14994147419929504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9238, 6.2318, 7.0814],
        [4.9238, 4.9238, 4.9238],
        [4.9238, 5.1621, 5.0997],
        [4.9238, 5.0004, 4.9521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.18312373757362366 
model_pd.l_d.mean(): -20.000104904174805 
model_pd.lagr.mean(): -19.816980361938477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4756], device='cuda:0')), ('power', tensor([-20.7045], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.18312373757362366
epoch£º56	 i:1 	 global-step:1121	 l-p:0.13572737574577332
epoch£º56	 i:2 	 global-step:1122	 l-p:0.12788768112659454
epoch£º56	 i:3 	 global-step:1123	 l-p:0.1244068518280983
epoch£º56	 i:4 	 global-step:1124	 l-p:0.12217757850885391
epoch£º56	 i:5 	 global-step:1125	 l-p:0.1296311616897583
epoch£º56	 i:6 	 global-step:1126	 l-p:0.1343836933374405
epoch£º56	 i:7 	 global-step:1127	 l-p:0.14420050382614136
epoch£º56	 i:8 	 global-step:1128	 l-p:0.16601382195949554
epoch£º56	 i:9 	 global-step:1129	 l-p:0.14165659248828888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8278, 5.8063, 6.2925],
        [4.8278, 5.9211, 6.5343],
        [4.8278, 4.8278, 4.8278],
        [4.8278, 4.8938, 4.8504]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.16100236773490906 
model_pd.l_d.mean(): -20.344301223754883 
model_pd.lagr.mean(): -20.183298110961914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4736], device='cuda:0')), ('power', tensor([-21.0504], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.16100236773490906
epoch£º57	 i:1 	 global-step:1141	 l-p:0.1389024555683136
epoch£º57	 i:2 	 global-step:1142	 l-p:0.163075789809227
epoch£º57	 i:3 	 global-step:1143	 l-p:0.0594819039106369
epoch£º57	 i:4 	 global-step:1144	 l-p:0.1249304711818695
epoch£º57	 i:5 	 global-step:1145	 l-p:0.14687785506248474
epoch£º57	 i:6 	 global-step:1146	 l-p:0.1473759561777115
epoch£º57	 i:7 	 global-step:1147	 l-p:0.11315196007490158
epoch£º57	 i:8 	 global-step:1148	 l-p:0.12686310708522797
epoch£º57	 i:9 	 global-step:1149	 l-p:0.11401432007551193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9294, 5.2412, 5.1980],
        [4.9294, 5.1477, 5.0827],
        [4.9294, 6.1823, 6.9652],
        [4.9294, 4.9294, 4.9294]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.130781888961792 
model_pd.l_d.mean(): -20.614870071411133 
model_pd.lagr.mean(): -20.484088897705078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4156], device='cuda:0')), ('power', tensor([-21.2647], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.130781888961792
epoch£º58	 i:1 	 global-step:1161	 l-p:0.13997012376785278
epoch£º58	 i:2 	 global-step:1162	 l-p:0.12424472719430923
epoch£º58	 i:3 	 global-step:1163	 l-p:0.13025303184986115
epoch£º58	 i:4 	 global-step:1164	 l-p:0.15094777941703796
epoch£º58	 i:5 	 global-step:1165	 l-p:0.13278721272945404
epoch£º58	 i:6 	 global-step:1166	 l-p:0.13467437028884888
epoch£º58	 i:7 	 global-step:1167	 l-p:0.14395642280578613
epoch£º58	 i:8 	 global-step:1168	 l-p:0.15477177500724792
epoch£º58	 i:9 	 global-step:1169	 l-p:0.08763572573661804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6160, 4.9343, 4.9073],
        [4.6160, 4.6180, 4.6160],
        [4.6160, 4.6160, 4.6160],
        [4.6160, 4.7686, 4.7075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.1309569776058197 
model_pd.l_d.mean(): -18.834362030029297 
model_pd.lagr.mean(): -18.703405380249023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5796], device='cuda:0')), ('power', tensor([-19.6323], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.1309569776058197
epoch£º59	 i:1 	 global-step:1181	 l-p:0.14608953893184662
epoch£º59	 i:2 	 global-step:1182	 l-p:0.1655699461698532
epoch£º59	 i:3 	 global-step:1183	 l-p:0.3569336235523224
epoch£º59	 i:4 	 global-step:1184	 l-p:0.1537742018699646
epoch£º59	 i:5 	 global-step:1185	 l-p:0.18062952160835266
epoch£º59	 i:6 	 global-step:1186	 l-p:0.13993841409683228
epoch£º59	 i:7 	 global-step:1187	 l-p:0.12406109273433685
epoch£º59	 i:8 	 global-step:1188	 l-p:0.1306029111146927
epoch£º59	 i:9 	 global-step:1189	 l-p:0.13422897458076477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0032, 5.1117, 5.0526],
        [5.0032, 5.0032, 5.0032],
        [5.0032, 5.0052, 5.0033],
        [5.0032, 5.3502, 5.3173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.15152157843112946 
model_pd.l_d.mean(): -18.204551696777344 
model_pd.lagr.mean(): -18.053030014038086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5590], device='cuda:0')), ('power', tensor([-18.9745], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.15152157843112946
epoch£º60	 i:1 	 global-step:1201	 l-p:0.13279065489768982
epoch£º60	 i:2 	 global-step:1202	 l-p:0.11343330144882202
epoch£º60	 i:3 	 global-step:1203	 l-p:0.11557818949222565
epoch£º60	 i:4 	 global-step:1204	 l-p:0.1295955330133438
epoch£º60	 i:5 	 global-step:1205	 l-p:0.12157322466373444
epoch£º60	 i:6 	 global-step:1206	 l-p:0.1259971559047699
epoch£º60	 i:7 	 global-step:1207	 l-p:0.11329430341720581
epoch£º60	 i:8 	 global-step:1208	 l-p:0.14463354647159576
epoch£º60	 i:9 	 global-step:1209	 l-p:0.1162833571434021
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8474, 4.8474, 4.8474],
        [4.8474, 5.4709, 5.6208],
        [4.8474, 4.8539, 4.8480],
        [4.8474, 5.5586, 5.7822]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.15132205188274384 
model_pd.l_d.mean(): -19.231029510498047 
model_pd.lagr.mean(): -19.079708099365234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5056], device='cuda:0')), ('power', tensor([-19.9577], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.15132205188274384
epoch£º61	 i:1 	 global-step:1221	 l-p:0.14555978775024414
epoch£º61	 i:2 	 global-step:1222	 l-p:0.12608946859836578
epoch£º61	 i:3 	 global-step:1223	 l-p:0.17638875544071198
epoch£º61	 i:4 	 global-step:1224	 l-p:0.13829435408115387
epoch£º61	 i:5 	 global-step:1225	 l-p:0.11722929775714874
epoch£º61	 i:6 	 global-step:1226	 l-p:0.135319784283638
epoch£º61	 i:7 	 global-step:1227	 l-p:0.12992562353610992
epoch£º61	 i:8 	 global-step:1228	 l-p:0.13775049149990082
epoch£º61	 i:9 	 global-step:1229	 l-p:0.1524820178747177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8237, 4.8551, 4.8306],
        [4.8237, 5.7478, 6.1806],
        [4.8237, 4.9539, 4.8923],
        [4.8237, 5.0358, 4.9734]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.1618838906288147 
model_pd.l_d.mean(): -19.955581665039062 
model_pd.lagr.mean(): -19.793697357177734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5083], device='cuda:0')), ('power', tensor([-20.6929], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.1618838906288147
epoch£º62	 i:1 	 global-step:1241	 l-p:0.131062313914299
epoch£º62	 i:2 	 global-step:1242	 l-p:0.10828813910484314
epoch£º62	 i:3 	 global-step:1243	 l-p:0.12258163094520569
epoch£º62	 i:4 	 global-step:1244	 l-p:0.17882339656352997
epoch£º62	 i:5 	 global-step:1245	 l-p:0.1262427568435669
epoch£º62	 i:6 	 global-step:1246	 l-p:0.13355733454227448
epoch£º62	 i:7 	 global-step:1247	 l-p:0.13900470733642578
epoch£º62	 i:8 	 global-step:1248	 l-p:0.13778628408908844
epoch£º62	 i:9 	 global-step:1249	 l-p:0.13403667509555817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9333, 4.9360, 4.9334],
        [4.9333, 6.2781, 7.1768],
        [4.9333, 6.2829, 7.1874],
        [4.9333, 4.9333, 4.9333]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.13273391127586365 
model_pd.l_d.mean(): -20.04271697998047 
model_pd.lagr.mean(): -19.909982681274414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4595], device='cuda:0')), ('power', tensor([-20.7311], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.13273391127586365
epoch£º63	 i:1 	 global-step:1261	 l-p:0.14918698370456696
epoch£º63	 i:2 	 global-step:1262	 l-p:0.03723624721169472
epoch£º63	 i:3 	 global-step:1263	 l-p:0.15349526703357697
epoch£º63	 i:4 	 global-step:1264	 l-p:0.11600928753614426
epoch£º63	 i:5 	 global-step:1265	 l-p:0.13936546444892883
epoch£º63	 i:6 	 global-step:1266	 l-p:0.1434883326292038
epoch£º63	 i:7 	 global-step:1267	 l-p:0.18238921463489532
epoch£º63	 i:8 	 global-step:1268	 l-p:0.11513859778642654
epoch£º63	 i:9 	 global-step:1269	 l-p:0.019870605319738388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6645, 4.6662, 4.6646],
        [4.6645, 4.6645, 4.6645],
        [4.6645, 4.6906, 4.6698],
        [4.6645, 4.6645, 4.6645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.131393700838089 
model_pd.l_d.mean(): -19.788049697875977 
model_pd.lagr.mean(): -19.65665626525879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5490], device='cuda:0')), ('power', tensor([-20.5651], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.131393700838089
epoch£º64	 i:1 	 global-step:1281	 l-p:0.14740721881389618
epoch£º64	 i:2 	 global-step:1282	 l-p:0.250814288854599
epoch£º64	 i:3 	 global-step:1283	 l-p:0.2206883579492569
epoch£º64	 i:4 	 global-step:1284	 l-p:0.11125994473695755
epoch£º64	 i:5 	 global-step:1285	 l-p:0.16891951858997345
epoch£º64	 i:6 	 global-step:1286	 l-p:0.13798083364963531
epoch£º64	 i:7 	 global-step:1287	 l-p:0.10177389532327652
epoch£º64	 i:8 	 global-step:1288	 l-p:0.1432226300239563
epoch£º64	 i:9 	 global-step:1289	 l-p:0.11565269529819489
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9464, 4.9468, 4.9464],
        [4.9464, 4.9464, 4.9464],
        [4.9464, 6.0190, 6.5946],
        [4.9464, 5.0665, 5.0056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.12497018277645111 
model_pd.l_d.mean(): -20.00855255126953 
model_pd.lagr.mean(): -19.883583068847656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4640], device='cuda:0')), ('power', tensor([-20.7012], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.12497018277645111
epoch£º65	 i:1 	 global-step:1301	 l-p:0.13927985727787018
epoch£º65	 i:2 	 global-step:1302	 l-p:0.13500738143920898
epoch£º65	 i:3 	 global-step:1303	 l-p:0.1367315798997879
epoch£º65	 i:4 	 global-step:1304	 l-p:0.12114287912845612
epoch£º65	 i:5 	 global-step:1305	 l-p:0.16686652600765228
epoch£º65	 i:6 	 global-step:1306	 l-p:0.13535253703594208
epoch£º65	 i:7 	 global-step:1307	 l-p:0.14535699784755707
epoch£º65	 i:8 	 global-step:1308	 l-p:0.14821551740169525
epoch£º65	 i:9 	 global-step:1309	 l-p:0.28142058849334717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7085, 4.7085, 4.7085],
        [4.7085, 5.6967, 6.2181],
        [4.7085, 5.2368, 5.3287],
        [4.7085, 4.7322, 4.7130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.14083659648895264 
model_pd.l_d.mean(): -20.467304229736328 
model_pd.lagr.mean(): -20.326467514038086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4910], device='cuda:0')), ('power', tensor([-21.1925], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.14083659648895264
epoch£º66	 i:1 	 global-step:1321	 l-p:0.1394413709640503
epoch£º66	 i:2 	 global-step:1322	 l-p:0.13220123946666718
epoch£º66	 i:3 	 global-step:1323	 l-p:0.1271350085735321
epoch£º66	 i:4 	 global-step:1324	 l-p:0.1659308522939682
epoch£º66	 i:5 	 global-step:1325	 l-p:0.13364893198013306
epoch£º66	 i:6 	 global-step:1326	 l-p:0.13650865852832794
epoch£º66	 i:7 	 global-step:1327	 l-p:0.18171893060207367
epoch£º66	 i:8 	 global-step:1328	 l-p:0.13006791472434998
epoch£º66	 i:9 	 global-step:1329	 l-p:0.15337467193603516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8989, 4.8989, 4.8989],
        [4.8989, 4.8989, 4.8989],
        [4.8989, 5.8760, 6.3564],
        [4.8989, 5.6693, 5.9438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.16615210473537445 
model_pd.l_d.mean(): -20.004283905029297 
model_pd.lagr.mean(): -19.838130950927734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4794], device='cuda:0')), ('power', tensor([-20.7127], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.16615210473537445
epoch£º67	 i:1 	 global-step:1341	 l-p:0.11301320791244507
epoch£º67	 i:2 	 global-step:1342	 l-p:0.1289769560098648
epoch£º67	 i:3 	 global-step:1343	 l-p:0.1295994371175766
epoch£º67	 i:4 	 global-step:1344	 l-p:0.13698533177375793
epoch£º67	 i:5 	 global-step:1345	 l-p:0.2112170159816742
epoch£º67	 i:6 	 global-step:1346	 l-p:0.13393227756023407
epoch£º67	 i:7 	 global-step:1347	 l-p:0.1287926584482193
epoch£º67	 i:8 	 global-step:1348	 l-p:0.14464171230793
epoch£º67	 i:9 	 global-step:1349	 l-p:0.10992908477783203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8884, 6.1548, 6.9688],
        [4.8884, 6.2118, 7.0960],
        [4.8884, 4.8982, 4.8895],
        [4.8884, 5.9382, 6.4999]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.12088995426893234 
model_pd.l_d.mean(): -19.09015655517578 
model_pd.lagr.mean(): -18.969266891479492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5412], device='cuda:0')), ('power', tensor([-19.8516], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.12088995426893234
epoch£º68	 i:1 	 global-step:1361	 l-p:0.3602924942970276
epoch£º68	 i:2 	 global-step:1362	 l-p:0.15068292617797852
epoch£º68	 i:3 	 global-step:1363	 l-p:0.11881648749113083
epoch£º68	 i:4 	 global-step:1364	 l-p:0.12765470147132874
epoch£º68	 i:5 	 global-step:1365	 l-p:0.12374517321586609
epoch£º68	 i:6 	 global-step:1366	 l-p:0.13096337020397186
epoch£º68	 i:7 	 global-step:1367	 l-p:0.1556166112422943
epoch£º68	 i:8 	 global-step:1368	 l-p:0.11844921857118607
epoch£º68	 i:9 	 global-step:1369	 l-p:0.16050155460834503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8355, 5.0000, 4.9361],
        [4.8355, 5.9071, 6.5025],
        [4.8355, 5.0575, 4.9975],
        [4.8355, 4.8355, 4.8355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.1523037850856781 
model_pd.l_d.mean(): -18.61500358581543 
model_pd.lagr.mean(): -18.46269989013672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5706], device='cuda:0')), ('power', tensor([-19.4014], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.1523037850856781
epoch£º69	 i:1 	 global-step:1381	 l-p:0.13182906806468964
epoch£º69	 i:2 	 global-step:1382	 l-p:-0.20030562579631805
epoch£º69	 i:3 	 global-step:1383	 l-p:0.15426036715507507
epoch£º69	 i:4 	 global-step:1384	 l-p:0.12037219852209091
epoch£º69	 i:5 	 global-step:1385	 l-p:0.12480195611715317
epoch£º69	 i:6 	 global-step:1386	 l-p:0.11934072524309158
epoch£º69	 i:7 	 global-step:1387	 l-p:0.1328185498714447
epoch£º69	 i:8 	 global-step:1388	 l-p:0.12728609144687653
epoch£º69	 i:9 	 global-step:1389	 l-p:0.12114960700273514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9306, 6.3162, 7.2715],
        [4.9306, 4.9583, 4.9362],
        [4.9306, 4.9306, 4.9306],
        [4.9306, 6.2266, 7.0700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.10399816185235977 
model_pd.l_d.mean(): -17.98577880859375 
model_pd.lagr.mean(): -17.88178062438965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5762], device='cuda:0')), ('power', tensor([-18.7710], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.10399816185235977
epoch£º70	 i:1 	 global-step:1401	 l-p:0.1265588104724884
epoch£º70	 i:2 	 global-step:1402	 l-p:0.1290423572063446
epoch£º70	 i:3 	 global-step:1403	 l-p:0.14046332240104675
epoch£º70	 i:4 	 global-step:1404	 l-p:0.15240438282489777
epoch£º70	 i:5 	 global-step:1405	 l-p:0.1852654218673706
epoch£º70	 i:6 	 global-step:1406	 l-p:0.16521550714969635
epoch£º70	 i:7 	 global-step:1407	 l-p:0.10284261405467987
epoch£º70	 i:8 	 global-step:1408	 l-p:0.1266200840473175
epoch£º70	 i:9 	 global-step:1409	 l-p:0.1500064581632614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8210, 5.0123, 4.9496],
        [4.8210, 4.8241, 4.8212],
        [4.8210, 4.8210, 4.8210],
        [4.8210, 5.4243, 5.5657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.1379421353340149 
model_pd.l_d.mean(): -20.326967239379883 
model_pd.lagr.mean(): -20.18902587890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4781], device='cuda:0')), ('power', tensor([-21.0375], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.1379421353340149
epoch£º71	 i:1 	 global-step:1421	 l-p:0.12925595045089722
epoch£º71	 i:2 	 global-step:1422	 l-p:0.09324878454208374
epoch£º71	 i:3 	 global-step:1423	 l-p:0.12162993103265762
epoch£º71	 i:4 	 global-step:1424	 l-p:0.1606615036725998
epoch£º71	 i:5 	 global-step:1425	 l-p:0.139139324426651
epoch£º71	 i:6 	 global-step:1426	 l-p:0.142048180103302
epoch£º71	 i:7 	 global-step:1427	 l-p:0.16101199388504028
epoch£º71	 i:8 	 global-step:1428	 l-p:0.14341707527637482
epoch£º71	 i:9 	 global-step:1429	 l-p:0.15501175820827484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8509, 4.8509, 4.8509],
        [4.8509, 4.8509, 4.8509],
        [4.8509, 4.8554, 4.8512],
        [4.8509, 4.8509, 4.8509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.12231538444757462 
model_pd.l_d.mean(): -19.1968936920166 
model_pd.lagr.mean(): -19.0745792388916 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4907], device='cuda:0')), ('power', tensor([-19.9080], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.12231538444757462
epoch£º72	 i:1 	 global-step:1441	 l-p:0.14672492444515228
epoch£º72	 i:2 	 global-step:1442	 l-p:0.13569682836532593
epoch£º72	 i:3 	 global-step:1443	 l-p:0.1420106738805771
epoch£º72	 i:4 	 global-step:1444	 l-p:0.14734193682670593
epoch£º72	 i:5 	 global-step:1445	 l-p:0.11704408377408981
epoch£º72	 i:6 	 global-step:1446	 l-p:0.15368448197841644
epoch£º72	 i:7 	 global-step:1447	 l-p:0.13301153481006622
epoch£º72	 i:8 	 global-step:1448	 l-p:0.1564180850982666
epoch£º72	 i:9 	 global-step:1449	 l-p:-0.04173728823661804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8868, 4.8868, 4.8868],
        [4.8868, 5.0443, 4.9804],
        [4.8868, 5.8553, 6.3325],
        [4.8868, 5.1080, 5.0473]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.13179334998130798 
model_pd.l_d.mean(): -19.311609268188477 
model_pd.lagr.mean(): -19.1798152923584 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4706], device='cuda:0')), ('power', tensor([-20.0034], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.13179334998130798
epoch£º73	 i:1 	 global-step:1461	 l-p:0.14397241175174713
epoch£º73	 i:2 	 global-step:1462	 l-p:0.22131885588169098
epoch£º73	 i:3 	 global-step:1463	 l-p:0.11549553275108337
epoch£º73	 i:4 	 global-step:1464	 l-p:0.12857049703598022
epoch£º73	 i:5 	 global-step:1465	 l-p:0.1209375262260437
epoch£º73	 i:6 	 global-step:1466	 l-p:0.13365601003170013
epoch£º73	 i:7 	 global-step:1467	 l-p:0.14521102607250214
epoch£º73	 i:8 	 global-step:1468	 l-p:0.10627195984125137
epoch£º73	 i:9 	 global-step:1469	 l-p:0.12822704017162323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9009, 4.9189, 4.9038],
        [4.9009, 5.2703, 5.2565],
        [4.9009, 5.6303, 5.8715],
        [4.9009, 5.1214, 5.0604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.13520686328411102 
model_pd.l_d.mean(): -19.755630493164062 
model_pd.lagr.mean(): -19.620424270629883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5021], device='cuda:0')), ('power', tensor([-20.4845], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.13520686328411102
epoch£º74	 i:1 	 global-step:1481	 l-p:0.11785116791725159
epoch£º74	 i:2 	 global-step:1482	 l-p:0.1447976976633072
epoch£º74	 i:3 	 global-step:1483	 l-p:0.1340370923280716
epoch£º74	 i:4 	 global-step:1484	 l-p:0.12525728344917297
epoch£º74	 i:5 	 global-step:1485	 l-p:0.18117550015449524
epoch£º74	 i:6 	 global-step:1486	 l-p:0.1846606284379959
epoch£º74	 i:7 	 global-step:1487	 l-p:0.10868524014949799
epoch£º74	 i:8 	 global-step:1488	 l-p:0.16222670674324036
epoch£º74	 i:9 	 global-step:1489	 l-p:0.13272394239902496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8920, 5.2251, 5.1962],
        [4.8920, 5.1021, 5.0402],
        [4.8920, 4.8920, 4.8920],
        [4.8920, 5.6745, 5.9660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.13642878830432892 
model_pd.l_d.mean(): -20.096839904785156 
model_pd.lagr.mean(): -19.960411071777344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4682], device='cuda:0')), ('power', tensor([-20.7948], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.13642878830432892
epoch£º75	 i:1 	 global-step:1501	 l-p:0.12808763980865479
epoch£º75	 i:2 	 global-step:1502	 l-p:0.14080145955085754
epoch£º75	 i:3 	 global-step:1503	 l-p:0.14227347075939178
epoch£º75	 i:4 	 global-step:1504	 l-p:0.13767801225185394
epoch£º75	 i:5 	 global-step:1505	 l-p:0.11957088857889175
epoch£º75	 i:6 	 global-step:1506	 l-p:0.16405288875102997
epoch£º75	 i:7 	 global-step:1507	 l-p:0.13617005944252014
epoch£º75	 i:8 	 global-step:1508	 l-p:0.09892372041940689
epoch£º75	 i:9 	 global-step:1509	 l-p:0.14535851776599884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8378, 4.8383, 4.8378],
        [4.8378, 4.8378, 4.8378],
        [4.8378, 5.5728, 5.8274],
        [4.8378, 5.0654, 5.0077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.12408151477575302 
model_pd.l_d.mean(): -19.416542053222656 
model_pd.lagr.mean(): -19.292461395263672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5277], device='cuda:0')), ('power', tensor([-20.1678], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.12408151477575302
epoch£º76	 i:1 	 global-step:1521	 l-p:0.13844993710517883
epoch£º76	 i:2 	 global-step:1522	 l-p:0.2667609453201294
epoch£º76	 i:3 	 global-step:1523	 l-p:0.1271219104528427
epoch£º76	 i:4 	 global-step:1524	 l-p:0.11962028592824936
epoch£º76	 i:5 	 global-step:1525	 l-p:0.11829332262277603
epoch£º76	 i:6 	 global-step:1526	 l-p:0.11112568527460098
epoch£º76	 i:7 	 global-step:1527	 l-p:0.11747092753648758
epoch£º76	 i:8 	 global-step:1528	 l-p:0.12500177323818207
epoch£º76	 i:9 	 global-step:1529	 l-p:0.16276463866233826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9340, 4.9511, 4.9366],
        [4.9340, 4.9343, 4.9340],
        [4.9340, 4.9758, 4.9450],
        [4.9340, 5.0350, 4.9797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.12634246051311493 
model_pd.l_d.mean(): -20.45311737060547 
model_pd.lagr.mean(): -20.32677459716797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4319], device='cuda:0')), ('power', tensor([-21.1179], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.12634246051311493
epoch£º77	 i:1 	 global-step:1541	 l-p:0.11698099225759506
epoch£º77	 i:2 	 global-step:1542	 l-p:0.1516047716140747
epoch£º77	 i:3 	 global-step:1543	 l-p:0.10697004199028015
epoch£º77	 i:4 	 global-step:1544	 l-p:0.13720113039016724
epoch£º77	 i:5 	 global-step:1545	 l-p:0.191650390625
epoch£º77	 i:6 	 global-step:1546	 l-p:0.12666630744934082
epoch£º77	 i:7 	 global-step:1547	 l-p:0.1754729002714157
epoch£º77	 i:8 	 global-step:1548	 l-p:0.2000814825296402
epoch£º77	 i:9 	 global-step:1549	 l-p:0.14020676910877228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8064, 5.3960, 5.5315],
        [4.8064, 5.6718, 6.0569],
        [4.8064, 4.9062, 4.8524],
        [4.8064, 4.8570, 4.8218]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.14392508566379547 
model_pd.l_d.mean(): -19.922985076904297 
model_pd.lagr.mean(): -19.77906036376953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5141], device='cuda:0')), ('power', tensor([-20.6659], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.14392508566379547
epoch£º78	 i:1 	 global-step:1561	 l-p:0.13526180386543274
epoch£º78	 i:2 	 global-step:1562	 l-p:0.16947108507156372
epoch£º78	 i:3 	 global-step:1563	 l-p:0.13437457382678986
epoch£º78	 i:4 	 global-step:1564	 l-p:0.1637706756591797
epoch£º78	 i:5 	 global-step:1565	 l-p:0.12506109476089478
epoch£º78	 i:6 	 global-step:1566	 l-p:0.0851726159453392
epoch£º78	 i:7 	 global-step:1567	 l-p:0.13314510881900787
epoch£º78	 i:8 	 global-step:1568	 l-p:0.14539073407649994
epoch£º78	 i:9 	 global-step:1569	 l-p:0.14050908386707306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8253, 5.4680, 5.6450],
        [4.8253, 4.8254, 4.8253],
        [4.8253, 4.8269, 4.8254],
        [4.8253, 4.8296, 4.8256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.147577702999115 
model_pd.l_d.mean(): -20.45136833190918 
model_pd.lagr.mean(): -20.303791046142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4685], device='cuda:0')), ('power', tensor([-21.1535], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.147577702999115
epoch£º79	 i:1 	 global-step:1581	 l-p:0.17190253734588623
epoch£º79	 i:2 	 global-step:1582	 l-p:0.13692979514598846
epoch£º79	 i:3 	 global-step:1583	 l-p:0.1371907740831375
epoch£º79	 i:4 	 global-step:1584	 l-p:0.12769241631031036
epoch£º79	 i:5 	 global-step:1585	 l-p:0.19376687705516815
epoch£º79	 i:6 	 global-step:1586	 l-p:0.23010782897472382
epoch£º79	 i:7 	 global-step:1587	 l-p:0.14395828545093536
epoch£º79	 i:8 	 global-step:1588	 l-p:0.1554265022277832
epoch£º79	 i:9 	 global-step:1589	 l-p:0.10195565968751907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8129, 6.1496, 7.0742],
        [4.8129, 4.8291, 4.8154],
        [4.8129, 4.8129, 4.8129],
        [4.8129, 5.7480, 6.2052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.17076775431632996 
model_pd.l_d.mean(): -19.077077865600586 
model_pd.lagr.mean(): -18.90631103515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5202], device='cuda:0')), ('power', tensor([-19.8170], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.17076775431632996
epoch£º80	 i:1 	 global-step:1601	 l-p:0.12720775604248047
epoch£º80	 i:2 	 global-step:1602	 l-p:0.13936415314674377
epoch£º80	 i:3 	 global-step:1603	 l-p:0.12070970237255096
epoch£º80	 i:4 	 global-step:1604	 l-p:0.2459450513124466
epoch£º80	 i:5 	 global-step:1605	 l-p:0.13246311247348785
epoch£º80	 i:6 	 global-step:1606	 l-p:0.1286003291606903
epoch£º80	 i:7 	 global-step:1607	 l-p:0.13222189247608185
epoch£º80	 i:8 	 global-step:1608	 l-p:0.1268490105867386
epoch£º80	 i:9 	 global-step:1609	 l-p:0.13116592168807983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9125, 4.9184, 4.9130],
        [4.9125, 4.9125, 4.9125],
        [4.9125, 5.8767, 6.3502],
        [4.9125, 4.9828, 4.9382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.12235800176858902 
model_pd.l_d.mean(): -20.220449447631836 
model_pd.lagr.mean(): -20.09809112548828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4607], device='cuda:0')), ('power', tensor([-20.9121], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.12235800176858902
epoch£º81	 i:1 	 global-step:1621	 l-p:0.14380015432834625
epoch£º81	 i:2 	 global-step:1622	 l-p:0.13110731542110443
epoch£º81	 i:3 	 global-step:1623	 l-p:0.15584640204906464
epoch£º81	 i:4 	 global-step:1624	 l-p:0.16522778570652008
epoch£º81	 i:5 	 global-step:1625	 l-p:0.5319159030914307
epoch£º81	 i:6 	 global-step:1626	 l-p:0.1328507512807846
epoch£º81	 i:7 	 global-step:1627	 l-p:-0.01242748275399208
epoch£º81	 i:8 	 global-step:1628	 l-p:-1.5088337659835815
epoch£º81	 i:9 	 global-step:1629	 l-p:0.13620223104953766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7284, 4.7941, 4.7523],
        [4.7284, 5.3704, 5.5580],
        [4.7284, 4.9282, 4.8703],
        [4.7284, 5.7550, 6.3262]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.2517271637916565 
model_pd.l_d.mean(): -20.295780181884766 
model_pd.lagr.mean(): -20.044052124023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5085], device='cuda:0')), ('power', tensor([-21.0370], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.2517271637916565
epoch£º82	 i:1 	 global-step:1641	 l-p:0.13042320311069489
epoch£º82	 i:2 	 global-step:1642	 l-p:0.15342366695404053
epoch£º82	 i:3 	 global-step:1643	 l-p:0.16691799461841583
epoch£º82	 i:4 	 global-step:1644	 l-p:0.13605406880378723
epoch£º82	 i:5 	 global-step:1645	 l-p:0.13757389783859253
epoch£º82	 i:6 	 global-step:1646	 l-p:0.15046018362045288
epoch£º82	 i:7 	 global-step:1647	 l-p:0.13736285269260406
epoch£º82	 i:8 	 global-step:1648	 l-p:0.1285059154033661
epoch£º82	 i:9 	 global-step:1649	 l-p:0.13813801109790802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9474, 5.8658, 6.2877],
        [4.9474, 5.1825, 5.1241],
        [4.9474, 4.9480, 4.9474],
        [4.9474, 5.4526, 5.5167]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.11888689547777176 
model_pd.l_d.mean(): -19.251197814941406 
model_pd.lagr.mean(): -19.13231086730957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4983], device='cuda:0')), ('power', tensor([-19.9706], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.11888689547777176
epoch£º83	 i:1 	 global-step:1661	 l-p:0.10654529184103012
epoch£º83	 i:2 	 global-step:1662	 l-p:0.13716617226600647
epoch£º83	 i:3 	 global-step:1663	 l-p:0.13136298954486847
epoch£º83	 i:4 	 global-step:1664	 l-p:0.1272410750389099
epoch£º83	 i:5 	 global-step:1665	 l-p:0.12627093493938446
epoch£º83	 i:6 	 global-step:1666	 l-p:0.1322902888059616
epoch£º83	 i:7 	 global-step:1667	 l-p:0.1546744853258133
epoch£º83	 i:8 	 global-step:1668	 l-p:-0.1961614042520523
epoch£º83	 i:9 	 global-step:1669	 l-p:0.1377706080675125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8804, 4.8806, 4.8804],
        [4.8804, 4.8888, 4.8813],
        [4.8804, 4.9535, 4.9081],
        [4.8804, 5.1099, 5.0527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.13600750267505646 
model_pd.l_d.mean(): -20.28609275817871 
model_pd.lagr.mean(): -20.15008544921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4736], device='cuda:0')), ('power', tensor([-20.9916], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.13600750267505646
epoch£º84	 i:1 	 global-step:1681	 l-p:0.12280065566301346
epoch£º84	 i:2 	 global-step:1682	 l-p:0.1397218108177185
epoch£º84	 i:3 	 global-step:1683	 l-p:0.14863763749599457
epoch£º84	 i:4 	 global-step:1684	 l-p:0.12543249130249023
epoch£º84	 i:5 	 global-step:1685	 l-p:0.1465684175491333
epoch£º84	 i:6 	 global-step:1686	 l-p:0.1846572458744049
epoch£º84	 i:7 	 global-step:1687	 l-p:0.067458875477314
epoch£º84	 i:8 	 global-step:1688	 l-p:0.13643606007099152
epoch£º84	 i:9 	 global-step:1689	 l-p:0.13473890721797943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6507,  0.5638,  1.0000,  0.4886,
          1.0000,  0.8665, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[4.8291, 5.5559, 5.8086],
        [4.8291, 5.2917, 5.3377],
        [4.8291, 5.6603, 6.0119],
        [4.8291, 5.1527, 5.1256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.16141672432422638 
model_pd.l_d.mean(): -20.01518440246582 
model_pd.lagr.mean(): -19.85376739501953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5067], device='cuda:0')), ('power', tensor([-20.7516], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.16141672432422638
epoch£º85	 i:1 	 global-step:1701	 l-p:0.1355731338262558
epoch£º85	 i:2 	 global-step:1702	 l-p:0.09140525758266449
epoch£º85	 i:3 	 global-step:1703	 l-p:0.15843580663204193
epoch£º85	 i:4 	 global-step:1704	 l-p:0.15323683619499207
epoch£º85	 i:5 	 global-step:1705	 l-p:0.16742171347141266
epoch£º85	 i:6 	 global-step:1706	 l-p:0.11873292922973633
epoch£º85	 i:7 	 global-step:1707	 l-p:0.12002982944250107
epoch£º85	 i:8 	 global-step:1708	 l-p:0.13899476826190948
epoch£º85	 i:9 	 global-step:1709	 l-p:0.12461856752634048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9069, 5.7601, 6.1237],
        [4.9069, 4.9165, 4.9080],
        [4.9069, 4.9069, 4.9069],
        [4.9069, 5.2180, 5.1828]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.5794409513473511 
model_pd.l_d.mean(): -17.3538818359375 
model_pd.lagr.mean(): -16.77444076538086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6600], device='cuda:0')), ('power', tensor([-18.2178], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.5794409513473511
epoch£º86	 i:1 	 global-step:1721	 l-p:0.1167583167552948
epoch£º86	 i:2 	 global-step:1722	 l-p:0.13129422068595886
epoch£º86	 i:3 	 global-step:1723	 l-p:0.14097461104393005
epoch£º86	 i:4 	 global-step:1724	 l-p:0.1256062537431717
epoch£º86	 i:5 	 global-step:1725	 l-p:0.1254449337720871
epoch£º86	 i:6 	 global-step:1726	 l-p:0.13080517947673798
epoch£º86	 i:7 	 global-step:1727	 l-p:0.1361178308725357
epoch£º86	 i:8 	 global-step:1728	 l-p:0.14058926701545715
epoch£º86	 i:9 	 global-step:1729	 l-p:0.19268359243869781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6877, 4.7752, 4.7263],
        [4.6877, 4.6917, 4.6880],
        [4.6877, 4.6881, 4.6877],
        [4.6877, 5.1694, 5.2391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.15296418964862823 
model_pd.l_d.mean(): -19.67074966430664 
model_pd.lagr.mean(): -19.517786026000977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5793], device='cuda:0')), ('power', tensor([-20.4775], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.15296418964862823
epoch£º87	 i:1 	 global-step:1741	 l-p:0.1362922638654709
epoch£º87	 i:2 	 global-step:1742	 l-p:0.13600538671016693
epoch£º87	 i:3 	 global-step:1743	 l-p:0.17058828473091125
epoch£º87	 i:4 	 global-step:1744	 l-p:0.056219395250082016
epoch£º87	 i:5 	 global-step:1745	 l-p:0.00994174461811781
epoch£º87	 i:6 	 global-step:1746	 l-p:0.13078343868255615
epoch£º87	 i:7 	 global-step:1747	 l-p:0.2215430736541748
epoch£º87	 i:8 	 global-step:1748	 l-p:0.149092435836792
epoch£º87	 i:9 	 global-step:1749	 l-p:0.14658984541893005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8849, 4.8850, 4.8849],
        [4.8849, 5.1778, 5.1376],
        [4.8849, 4.9856, 4.9315],
        [4.8849, 4.8902, 4.8854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.1538292020559311 
model_pd.l_d.mean(): -20.648605346679688 
model_pd.lagr.mean(): -20.494775772094727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4303], device='cuda:0')), ('power', tensor([-21.3139], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.1538292020559311
epoch£º88	 i:1 	 global-step:1761	 l-p:0.12668462097644806
epoch£º88	 i:2 	 global-step:1762	 l-p:0.1292259395122528
epoch£º88	 i:3 	 global-step:1763	 l-p:0.1392078399658203
epoch£º88	 i:4 	 global-step:1764	 l-p:0.13166570663452148
epoch£º88	 i:5 	 global-step:1765	 l-p:0.14827710390090942
epoch£º88	 i:6 	 global-step:1766	 l-p:0.14860227704048157
epoch£º88	 i:7 	 global-step:1767	 l-p:0.1481894999742508
epoch£º88	 i:8 	 global-step:1768	 l-p:0.11828427761793137
epoch£º88	 i:9 	 global-step:1769	 l-p:0.1179070845246315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7805, 5.7454, 6.2442],
        [4.7805, 4.8046, 4.7853],
        [4.7805, 4.8130, 4.7883],
        [4.7805, 4.9300, 4.8700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.11386749148368835 
model_pd.l_d.mean(): -19.359743118286133 
model_pd.lagr.mean(): -19.24587631225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5823], device='cuda:0')), ('power', tensor([-20.1662], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.11386749148368835
epoch£º89	 i:1 	 global-step:1781	 l-p:0.15337316691875458
epoch£º89	 i:2 	 global-step:1782	 l-p:0.13262100517749786
epoch£º89	 i:3 	 global-step:1783	 l-p:0.15016499161720276
epoch£º89	 i:4 	 global-step:1784	 l-p:0.21534425020217896
epoch£º89	 i:5 	 global-step:1785	 l-p:0.14111772179603577
epoch£º89	 i:6 	 global-step:1786	 l-p:0.1453944593667984
epoch£º89	 i:7 	 global-step:1787	 l-p:-0.18001863360404968
epoch£º89	 i:8 	 global-step:1788	 l-p:0.11794286221265793
epoch£º89	 i:9 	 global-step:1789	 l-p:0.12753349542617798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9759, 6.0880, 6.7187],
        [4.9759, 4.9759, 4.9759],
        [4.9759, 4.9759, 4.9759],
        [4.9759, 5.0138, 4.9854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.16866885125637054 
model_pd.l_d.mean(): -19.378896713256836 
model_pd.lagr.mean(): -19.210227966308594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4680], device='cuda:0')), ('power', tensor([-20.0688], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.16866885125637054
epoch£º90	 i:1 	 global-step:1801	 l-p:0.11207707971334457
epoch£º90	 i:2 	 global-step:1802	 l-p:0.1222267895936966
epoch£º90	 i:3 	 global-step:1803	 l-p:0.11915937811136246
epoch£º90	 i:4 	 global-step:1804	 l-p:0.12066037952899933
epoch£º90	 i:5 	 global-step:1805	 l-p:0.13652829825878143
epoch£º90	 i:6 	 global-step:1806	 l-p:0.12311704456806183
epoch£º90	 i:7 	 global-step:1807	 l-p:0.1575777530670166
epoch£º90	 i:8 	 global-step:1808	 l-p:0.14702574908733368
epoch£º90	 i:9 	 global-step:1809	 l-p:0.13849151134490967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7746, 4.7746, 4.7746],
        [4.7746, 4.7750, 4.7746],
        [4.7746, 4.7805, 4.7751],
        [4.7746, 5.5272, 5.8131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.13219673931598663 
model_pd.l_d.mean(): -19.322526931762695 
model_pd.lagr.mean(): -19.190330505371094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5022], device='cuda:0')), ('power', tensor([-20.0467], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.13219673931598663
epoch£º91	 i:1 	 global-step:1821	 l-p:0.18661005795001984
epoch£º91	 i:2 	 global-step:1822	 l-p:0.2720409035682678
epoch£º91	 i:3 	 global-step:1823	 l-p:0.13594110310077667
epoch£º91	 i:4 	 global-step:1824	 l-p:0.14104971289634705
epoch£º91	 i:5 	 global-step:1825	 l-p:0.16390269994735718
epoch£º91	 i:6 	 global-step:1826	 l-p:0.10023563355207443
epoch£º91	 i:7 	 global-step:1827	 l-p:0.12935858964920044
epoch£º91	 i:8 	 global-step:1828	 l-p:0.13096703588962555
epoch£º91	 i:9 	 global-step:1829	 l-p:0.15266235172748566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8990, 4.8990, 4.8990],
        [4.8990, 5.9394, 6.5035],
        [4.8990, 5.0452, 4.9839],
        [4.8990, 4.9581, 4.9189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.1324765980243683 
model_pd.l_d.mean(): -19.61575698852539 
model_pd.lagr.mean(): -19.483280181884766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5347], device='cuda:0')), ('power', tensor([-20.3764], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.1324765980243683
epoch£º92	 i:1 	 global-step:1841	 l-p:0.13203619420528412
epoch£º92	 i:2 	 global-step:1842	 l-p:0.11994266510009766
epoch£º92	 i:3 	 global-step:1843	 l-p:-1.6428931951522827
epoch£º92	 i:4 	 global-step:1844	 l-p:0.13775376975536346
epoch£º92	 i:5 	 global-step:1845	 l-p:0.1441325694322586
epoch£º92	 i:6 	 global-step:1846	 l-p:0.15492980182170868
epoch£º92	 i:7 	 global-step:1847	 l-p:0.14308521151542664
epoch£º92	 i:8 	 global-step:1848	 l-p:0.10595136135816574
epoch£º92	 i:9 	 global-step:1849	 l-p:0.12840475142002106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9038, 4.9038, 4.9038],
        [4.9038, 5.2691, 5.2589],
        [4.9038, 5.3030, 5.3098],
        [4.9038, 5.1776, 5.1319]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.12621036171913147 
model_pd.l_d.mean(): -19.67438507080078 
model_pd.lagr.mean(): -19.548173904418945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4846], device='cuda:0')), ('power', tensor([-20.3844], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.12621036171913147
epoch£º93	 i:1 	 global-step:1861	 l-p:0.1303080916404724
epoch£º93	 i:2 	 global-step:1862	 l-p:0.12462568283081055
epoch£º93	 i:3 	 global-step:1863	 l-p:0.14521996676921844
epoch£º93	 i:4 	 global-step:1864	 l-p:0.154922753572464
epoch£º93	 i:5 	 global-step:1865	 l-p:0.18871328234672546
epoch£º93	 i:6 	 global-step:1866	 l-p:0.14262078702449799
epoch£º93	 i:7 	 global-step:1867	 l-p:0.1495605856180191
epoch£º93	 i:8 	 global-step:1868	 l-p:-0.16266857087612152
epoch£º93	 i:9 	 global-step:1869	 l-p:0.12662577629089355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9841, 5.5304, 5.6247],
        [4.9841, 4.9841, 4.9841],
        [4.9841, 5.0286, 4.9965],
        [4.9841, 4.9986, 4.9861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.14907804131507874 
model_pd.l_d.mean(): -20.695781707763672 
model_pd.lagr.mean(): -20.546703338623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3949], device='cuda:0')), ('power', tensor([-21.3254], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.14907804131507874
epoch£º94	 i:1 	 global-step:1881	 l-p:0.11324051022529602
epoch£º94	 i:2 	 global-step:1882	 l-p:0.17027992010116577
epoch£º94	 i:3 	 global-step:1883	 l-p:0.12374496459960938
epoch£º94	 i:4 	 global-step:1884	 l-p:0.12355297058820724
epoch£º94	 i:5 	 global-step:1885	 l-p:0.11605754494667053
epoch£º94	 i:6 	 global-step:1886	 l-p:0.11329713463783264
epoch£º94	 i:7 	 global-step:1887	 l-p:0.12080415338277817
epoch£º94	 i:8 	 global-step:1888	 l-p:0.13050523400306702
epoch£º94	 i:9 	 global-step:1889	 l-p:0.15640729665756226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8952, 4.8953, 4.8952],
        [4.8952, 4.8952, 4.8952],
        [4.8952, 4.8966, 4.8953],
        [4.8952, 4.9107, 4.8976]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.1227862685918808 
model_pd.l_d.mean(): -19.26405143737793 
model_pd.lagr.mean(): -19.141265869140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5217], device='cuda:0')), ('power', tensor([-20.0075], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.1227862685918808
epoch£º95	 i:1 	 global-step:1901	 l-p:0.14851592481136322
epoch£º95	 i:2 	 global-step:1902	 l-p:0.13646700978279114
epoch£º95	 i:3 	 global-step:1903	 l-p:0.12951582670211792
epoch£º95	 i:4 	 global-step:1904	 l-p:0.12773476541042328
epoch£º95	 i:5 	 global-step:1905	 l-p:0.2341030389070511
epoch£º95	 i:6 	 global-step:1906	 l-p:0.13593195378780365
epoch£º95	 i:7 	 global-step:1907	 l-p:0.1373540163040161
epoch£º95	 i:8 	 global-step:1908	 l-p:0.13590224087238312
epoch£º95	 i:9 	 global-step:1909	 l-p:0.36928579211235046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7908, 5.9199, 6.6059],
        [4.7908, 4.7915, 4.7909],
        [4.7908, 5.5351, 5.8142],
        [4.7908, 4.7908, 4.7908]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.1040368601679802 
model_pd.l_d.mean(): -18.3614559173584 
model_pd.lagr.mean(): -18.25741958618164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6453], device='cuda:0')), ('power', tensor([-19.2214], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.1040368601679802
epoch£º96	 i:1 	 global-step:1921	 l-p:0.15209129452705383
epoch£º96	 i:2 	 global-step:1922	 l-p:0.12140179425477982
epoch£º96	 i:3 	 global-step:1923	 l-p:0.13411930203437805
epoch£º96	 i:4 	 global-step:1924	 l-p:0.13842815160751343
epoch£º96	 i:5 	 global-step:1925	 l-p:0.13623853027820587
epoch£º96	 i:6 	 global-step:1926	 l-p:0.17731259763240814
epoch£º96	 i:7 	 global-step:1927	 l-p:0.1200762540102005
epoch£º96	 i:8 	 global-step:1928	 l-p:0.13987481594085693
epoch£º96	 i:9 	 global-step:1929	 l-p:0.12689977884292603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0394, 5.5216, 5.5699],
        [5.0394, 5.9005, 6.2611],
        [5.0394, 5.0394, 5.0394],
        [5.0394, 6.3565, 7.2214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.14717115461826324 
model_pd.l_d.mean(): -20.29973793029785 
model_pd.lagr.mean(): -20.15256690979004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4151], device='cuda:0')), ('power', tensor([-20.9457], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.14717115461826324
epoch£º97	 i:1 	 global-step:1941	 l-p:0.12230131775140762
epoch£º97	 i:2 	 global-step:1942	 l-p:0.10776922106742859
epoch£º97	 i:3 	 global-step:1943	 l-p:0.14829879999160767
epoch£º97	 i:4 	 global-step:1944	 l-p:0.18553416430950165
epoch£º97	 i:5 	 global-step:1945	 l-p:0.1393761932849884
epoch£º97	 i:6 	 global-step:1946	 l-p:0.11608050763607025
epoch£º97	 i:7 	 global-step:1947	 l-p:0.1325930953025818
epoch£º97	 i:8 	 global-step:1948	 l-p:0.14327211678028107
epoch£º97	 i:9 	 global-step:1949	 l-p:0.1273166537284851
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7284, 5.0911, 5.0908],
        [4.7284, 4.7284, 4.7284],
        [4.7284, 4.7677, 4.7392],
        [4.7284, 5.3218, 5.4769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.13279227912425995 
model_pd.l_d.mean(): -20.320938110351562 
model_pd.lagr.mean(): -20.188146591186523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5115], device='cuda:0')), ('power', tensor([-21.0655], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.13279227912425995
epoch£º98	 i:1 	 global-step:1961	 l-p:0.14916136860847473
epoch£º98	 i:2 	 global-step:1962	 l-p:0.12943695485591888
epoch£º98	 i:3 	 global-step:1963	 l-p:-0.03086453303694725
epoch£º98	 i:4 	 global-step:1964	 l-p:0.17162242531776428
epoch£º98	 i:5 	 global-step:1965	 l-p:0.1813192516565323
epoch£º98	 i:6 	 global-step:1966	 l-p:0.1363522708415985
epoch£º98	 i:7 	 global-step:1967	 l-p:0.1489410698413849
epoch£º98	 i:8 	 global-step:1968	 l-p:0.12327190488576889
epoch£º98	 i:9 	 global-step:1969	 l-p:0.13717468082904816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9241, 4.9241, 4.9241],
        [4.9241, 4.9242, 4.9241],
        [4.9241, 5.3863, 5.4309],
        [4.9241, 4.9242, 4.9241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.14523757994174957 
model_pd.l_d.mean(): -19.399662017822266 
model_pd.lagr.mean(): -19.254425048828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4567], device='cuda:0')), ('power', tensor([-20.0783], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.14523757994174957
epoch£º99	 i:1 	 global-step:1981	 l-p:0.14091113209724426
epoch£º99	 i:2 	 global-step:1982	 l-p:0.11232717335224152
epoch£º99	 i:3 	 global-step:1983	 l-p:0.144032284617424
epoch£º99	 i:4 	 global-step:1984	 l-p:0.12520843744277954
epoch£º99	 i:5 	 global-step:1985	 l-p:-0.0472448356449604
epoch£º99	 i:6 	 global-step:1986	 l-p:0.12293484061956406
epoch£º99	 i:7 	 global-step:1987	 l-p:0.1597193479537964
epoch£º99	 i:8 	 global-step:1988	 l-p:0.12660063803195953
epoch£º99	 i:9 	 global-step:1989	 l-p:0.13950878381729126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8764, 4.8770, 4.8764],
        [4.8764, 5.3749, 5.4471],
        [4.8764, 4.9642, 4.9146],
        [4.8764, 4.8764, 4.8764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.16193655133247375 
model_pd.l_d.mean(): -20.743061065673828 
model_pd.lagr.mean(): -20.581125259399414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4232], device='cuda:0')), ('power', tensor([-21.4021], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.16193655133247375
epoch£º100	 i:1 	 global-step:2001	 l-p:0.13343580067157745
epoch£º100	 i:2 	 global-step:2002	 l-p:0.11211352050304413
epoch£º100	 i:3 	 global-step:2003	 l-p:0.11762823164463043
epoch£º100	 i:4 	 global-step:2004	 l-p:0.13966894149780273
epoch£º100	 i:5 	 global-step:2005	 l-p:0.14430440962314606
epoch£º100	 i:6 	 global-step:2006	 l-p:0.13814467191696167
epoch£º100	 i:7 	 global-step:2007	 l-p:2.371717691421509
epoch£º100	 i:8 	 global-step:2008	 l-p:-0.654670774936676
epoch£º100	 i:9 	 global-step:2009	 l-p:0.17071132361888885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8124, 4.8712, 4.8328],
        [4.8124, 4.8124, 4.8124],
        [4.8124, 4.8144, 4.8125],
        [4.8124, 4.8124, 4.8124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.1360611915588379 
model_pd.l_d.mean(): -20.10513687133789 
model_pd.lagr.mean(): -19.96907615661621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4843], device='cuda:0')), ('power', tensor([-20.8196], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.1360611915588379
epoch£º101	 i:1 	 global-step:2021	 l-p:0.1169670969247818
epoch£º101	 i:2 	 global-step:2022	 l-p:0.1056198850274086
epoch£º101	 i:3 	 global-step:2023	 l-p:0.04082449525594711
epoch£º101	 i:4 	 global-step:2024	 l-p:0.13701309263706207
epoch£º101	 i:5 	 global-step:2025	 l-p:0.1337142139673233
epoch£º101	 i:6 	 global-step:2026	 l-p:0.11803337931632996
epoch£º101	 i:7 	 global-step:2027	 l-p:0.12559399008750916
epoch£º101	 i:8 	 global-step:2028	 l-p:0.14421258866786957
epoch£º101	 i:9 	 global-step:2029	 l-p:0.1384107917547226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5760,  0.4793,  1.0000,  0.3988,
          1.0000,  0.8321, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5791,  0.4826,  1.0000,  0.4023,
          1.0000,  0.8335, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9034,  0.8733,  1.0000,  0.8442,
          1.0000,  0.9667, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[4.9761, 5.6923, 5.9284],
        [4.9761, 5.6972, 5.9375],
        [4.9761, 6.2050, 6.9788],
        [4.9761, 5.1470, 5.0850]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.11542236804962158 
model_pd.l_d.mean(): -20.0781307220459 
model_pd.lagr.mean(): -19.96270751953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-20.7715], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.11542236804962158
epoch£º102	 i:1 	 global-step:2041	 l-p:0.16332575678825378
epoch£º102	 i:2 	 global-step:2042	 l-p:0.015221395529806614
epoch£º102	 i:3 	 global-step:2043	 l-p:0.14635691046714783
epoch£º102	 i:4 	 global-step:2044	 l-p:0.1400952786207199
epoch£º102	 i:5 	 global-step:2045	 l-p:0.13479045033454895
epoch£º102	 i:6 	 global-step:2046	 l-p:0.1427353322505951
epoch£º102	 i:7 	 global-step:2047	 l-p:0.12749645113945007
epoch£º102	 i:8 	 global-step:2048	 l-p:0.14489059150218964
epoch£º102	 i:9 	 global-step:2049	 l-p:0.23216859996318817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6879, 4.7790, 4.7302],
        [4.6879, 5.0148, 5.0012],
        [4.6879, 4.7728, 4.7256],
        [4.6879, 4.6978, 4.6891]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.12455138564109802 
model_pd.l_d.mean(): -19.899564743041992 
model_pd.lagr.mean(): -19.775012969970703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5609], device='cuda:0')), ('power', tensor([-20.6900], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.12455138564109802
epoch£º103	 i:1 	 global-step:2061	 l-p:0.07467256486415863
epoch£º103	 i:2 	 global-step:2062	 l-p:0.1533610075712204
epoch£º103	 i:3 	 global-step:2063	 l-p:0.16106335818767548
epoch£º103	 i:4 	 global-step:2064	 l-p:0.13235993683338165
epoch£º103	 i:5 	 global-step:2065	 l-p:0.13750945031642914
epoch£º103	 i:6 	 global-step:2066	 l-p:0.1301945298910141
epoch£º103	 i:7 	 global-step:2067	 l-p:0.15221476554870605
epoch£º103	 i:8 	 global-step:2068	 l-p:0.1304803192615509
epoch£º103	 i:9 	 global-step:2069	 l-p:0.12791787087917328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9658, 4.9676, 4.9659],
        [4.9658, 5.1130, 5.0521],
        [4.9658, 5.0240, 4.9854],
        [4.9658, 5.8965, 6.3411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.12890952825546265 
model_pd.l_d.mean(): -20.7160701751709 
model_pd.lagr.mean(): -20.587160110473633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3973], device='cuda:0')), ('power', tensor([-21.3483], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.12890952825546265
epoch£º104	 i:1 	 global-step:2081	 l-p:0.38051337003707886
epoch£º104	 i:2 	 global-step:2082	 l-p:0.1530838906764984
epoch£º104	 i:3 	 global-step:2083	 l-p:0.1586426943540573
epoch£º104	 i:4 	 global-step:2084	 l-p:0.12984168529510498
epoch£º104	 i:5 	 global-step:2085	 l-p:0.12669587135314941
epoch£º104	 i:6 	 global-step:2086	 l-p:0.1257573664188385
epoch£º104	 i:7 	 global-step:2087	 l-p:0.14560434222221375
epoch£º104	 i:8 	 global-step:2088	 l-p:0.1246655061841011
epoch£º104	 i:9 	 global-step:2089	 l-p:0.13258510828018188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8395, 4.8395, 4.8395],
        [4.8395, 4.8395, 4.8395],
        [4.8395, 5.0425, 4.9862],
        [4.8395, 5.3608, 5.4540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.16076740622520447 
model_pd.l_d.mean(): -20.348073959350586 
model_pd.lagr.mean(): -20.187307357788086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-21.0497], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.16076740622520447
epoch£º105	 i:1 	 global-step:2101	 l-p:0.16111765801906586
epoch£º105	 i:2 	 global-step:2102	 l-p:0.12355226278305054
epoch£º105	 i:3 	 global-step:2103	 l-p:0.1399644911289215
epoch£º105	 i:4 	 global-step:2104	 l-p:0.14281944930553436
epoch£º105	 i:5 	 global-step:2105	 l-p:0.18189936876296997
epoch£º105	 i:6 	 global-step:2106	 l-p:0.1642855852842331
epoch£º105	 i:7 	 global-step:2107	 l-p:0.16650962829589844
epoch£º105	 i:8 	 global-step:2108	 l-p:0.07363082468509674
epoch£º105	 i:9 	 global-step:2109	 l-p:0.1287360042333603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9102, 4.9131, 4.9103],
        [4.9102, 5.6171, 5.8541],
        [4.9102, 4.9287, 4.9134],
        [4.9102, 5.8248, 6.2619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.13306353986263275 
model_pd.l_d.mean(): -18.52001190185547 
model_pd.lagr.mean(): -18.386947631835938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5637], device='cuda:0')), ('power', tensor([-19.2983], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.13306353986263275
epoch£º106	 i:1 	 global-step:2121	 l-p:0.14283248782157898
epoch£º106	 i:2 	 global-step:2122	 l-p:0.13301776349544525
epoch£º106	 i:3 	 global-step:2123	 l-p:0.13347318768501282
epoch£º106	 i:4 	 global-step:2124	 l-p:0.12378179281949997
epoch£º106	 i:5 	 global-step:2125	 l-p:0.12922562658786774
epoch£º106	 i:6 	 global-step:2126	 l-p:-0.8474906086921692
epoch£º106	 i:7 	 global-step:2127	 l-p:0.1347498744726181
epoch£º106	 i:8 	 global-step:2128	 l-p:0.13276293873786926
epoch£º106	 i:9 	 global-step:2129	 l-p:0.11549234390258789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9003, 4.9029, 4.9005],
        [4.9003, 4.9054, 4.9007],
        [4.9003, 5.9268, 6.4851],
        [4.9003, 5.7419, 6.1070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.14522868394851685 
model_pd.l_d.mean(): -19.37531089782715 
model_pd.lagr.mean(): -19.23008155822754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5056], device='cuda:0')), ('power', tensor([-20.1036], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.14522868394851685
epoch£º107	 i:1 	 global-step:2141	 l-p:0.12108005583286285
epoch£º107	 i:2 	 global-step:2142	 l-p:-0.02931811287999153
epoch£º107	 i:3 	 global-step:2143	 l-p:0.1228298619389534
epoch£º107	 i:4 	 global-step:2144	 l-p:0.12857729196548462
epoch£º107	 i:5 	 global-step:2145	 l-p:0.13160918653011322
epoch£º107	 i:6 	 global-step:2146	 l-p:0.1727304756641388
epoch£º107	 i:7 	 global-step:2147	 l-p:0.1509353369474411
epoch£º107	 i:8 	 global-step:2148	 l-p:0.12692809104919434
epoch£º107	 i:9 	 global-step:2149	 l-p:0.1382356435060501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8240, 4.8342, 4.8253],
        [4.8240, 4.8242, 4.8240],
        [4.8240, 4.9191, 4.8684],
        [4.8240, 4.8241, 4.8240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.13067962229251862 
model_pd.l_d.mean(): -18.85418701171875 
model_pd.lagr.mean(): -18.723506927490234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5408], device='cuda:0')), ('power', tensor([-19.6127], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.13067962229251862
epoch£º108	 i:1 	 global-step:2161	 l-p:0.16373465955257416
epoch£º108	 i:2 	 global-step:2162	 l-p:0.10531777143478394
epoch£º108	 i:3 	 global-step:2163	 l-p:0.13608963787555695
epoch£º108	 i:4 	 global-step:2164	 l-p:0.1389104127883911
epoch£º108	 i:5 	 global-step:2165	 l-p:0.15492033958435059
epoch£º108	 i:6 	 global-step:2166	 l-p:0.12788476049900055
epoch£º108	 i:7 	 global-step:2167	 l-p:0.12684445083141327
epoch£º108	 i:8 	 global-step:2168	 l-p:0.13363517820835114
epoch£º108	 i:9 	 global-step:2169	 l-p:0.16246291995048523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9291, 5.0128, 4.9646],
        [4.9291, 4.9306, 4.9292],
        [4.9291, 6.1867, 7.0080],
        [4.9291, 4.9291, 4.9291]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.11599602550268173 
model_pd.l_d.mean(): -20.014204025268555 
model_pd.lagr.mean(): -19.898208618164062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4745], device='cuda:0')), ('power', tensor([-20.7176], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.11599602550268173
epoch£º109	 i:1 	 global-step:2181	 l-p:0.13467222452163696
epoch£º109	 i:2 	 global-step:2182	 l-p:0.13121934235095978
epoch£º109	 i:3 	 global-step:2183	 l-p:0.1461174190044403
epoch£º109	 i:4 	 global-step:2184	 l-p:0.3928159773349762
epoch£º109	 i:5 	 global-step:2185	 l-p:0.13976259529590607
epoch£º109	 i:6 	 global-step:2186	 l-p:0.14309147000312805
epoch£º109	 i:7 	 global-step:2187	 l-p:0.12363267689943314
epoch£º109	 i:8 	 global-step:2188	 l-p:0.13457190990447998
epoch£º109	 i:9 	 global-step:2189	 l-p:0.1170627549290657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9408, 4.9412, 4.9408],
        [4.9408, 4.9408, 4.9408],
        [4.9408, 4.9417, 4.9408],
        [4.9408, 4.9613, 4.9445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.6576502919197083 
model_pd.l_d.mean(): -20.808998107910156 
model_pd.lagr.mean(): -20.151348114013672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3950], device='cuda:0')), ('power', tensor([-21.4399], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.6576502919197083
epoch£º110	 i:1 	 global-step:2201	 l-p:0.1315685510635376
epoch£º110	 i:2 	 global-step:2202	 l-p:0.09853294491767883
epoch£º110	 i:3 	 global-step:2203	 l-p:0.13467925786972046
epoch£º110	 i:4 	 global-step:2204	 l-p:0.14398527145385742
epoch£º110	 i:5 	 global-step:2205	 l-p:0.13493666052818298
epoch£º110	 i:6 	 global-step:2206	 l-p:0.13921110332012177
epoch£º110	 i:7 	 global-step:2207	 l-p:0.13349328935146332
epoch£º110	 i:8 	 global-step:2208	 l-p:0.2331242859363556
epoch£º110	 i:9 	 global-step:2209	 l-p:0.21811828017234802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8588, 4.8632, 4.8592],
        [4.8588, 5.5366, 5.7565],
        [4.8588, 5.3721, 5.4610],
        [4.8588, 5.7227, 6.1183]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.17196954786777496 
model_pd.l_d.mean(): -20.088756561279297 
model_pd.lagr.mean(): -19.916786193847656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4937], device='cuda:0')), ('power', tensor([-20.8127], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.17196954786777496
epoch£º111	 i:1 	 global-step:2221	 l-p:0.13511769473552704
epoch£º111	 i:2 	 global-step:2222	 l-p:0.43979907035827637
epoch£º111	 i:3 	 global-step:2223	 l-p:0.12737250328063965
epoch£º111	 i:4 	 global-step:2224	 l-p:0.1179569885134697
epoch£º111	 i:5 	 global-step:2225	 l-p:0.12329527735710144
epoch£º111	 i:6 	 global-step:2226	 l-p:0.11508519947528839
epoch£º111	 i:7 	 global-step:2227	 l-p:0.14042595028877258
epoch£º111	 i:8 	 global-step:2228	 l-p:0.13586412370204926
epoch£º111	 i:9 	 global-step:2229	 l-p:0.1105535700917244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9609, 5.0253, 4.9843],
        [4.9609, 5.0233, 4.9831],
        [4.9609, 4.9609, 4.9609],
        [4.9609, 4.9831, 4.9652]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.14736400544643402 
model_pd.l_d.mean(): -20.548274993896484 
model_pd.lagr.mean(): -20.400911331176758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4242], device='cuda:0')), ('power', tensor([-21.2062], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.14736400544643402
epoch£º112	 i:1 	 global-step:2241	 l-p:-0.12832054495811462
epoch£º112	 i:2 	 global-step:2242	 l-p:0.15494009852409363
epoch£º112	 i:3 	 global-step:2243	 l-p:0.1348726749420166
epoch£º112	 i:4 	 global-step:2244	 l-p:0.13465283811092377
epoch£º112	 i:5 	 global-step:2245	 l-p:0.15540634095668793
epoch£º112	 i:6 	 global-step:2246	 l-p:0.11932958662509918
epoch£º112	 i:7 	 global-step:2247	 l-p:0.1672920435667038
epoch£º112	 i:8 	 global-step:2248	 l-p:0.13128440082073212
epoch£º112	 i:9 	 global-step:2249	 l-p:0.13383544981479645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7995, 4.7996, 4.7995],
        [4.7995, 5.4675, 5.6863],
        [4.7995, 4.8596, 4.8212],
        [4.7995, 4.7996, 4.7995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.09869098663330078 
model_pd.l_d.mean(): -19.49871253967285 
model_pd.lagr.mean(): -19.400020599365234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5252], device='cuda:0')), ('power', tensor([-20.2484], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.09869098663330078
epoch£º113	 i:1 	 global-step:2261	 l-p:0.13664361834526062
epoch£º113	 i:2 	 global-step:2262	 l-p:0.13010583817958832
epoch£º113	 i:3 	 global-step:2263	 l-p:0.14399541914463043
epoch£º113	 i:4 	 global-step:2264	 l-p:0.13898921012878418
epoch£º113	 i:5 	 global-step:2265	 l-p:0.16413015127182007
epoch£º113	 i:6 	 global-step:2266	 l-p:0.530487060546875
epoch£º113	 i:7 	 global-step:2267	 l-p:0.13765840232372284
epoch£º113	 i:8 	 global-step:2268	 l-p:0.1482563614845276
epoch£º113	 i:9 	 global-step:2269	 l-p:-1.4896036386489868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8029, 4.8029, 4.8029],
        [4.8029, 4.8044, 4.8029],
        [4.8029, 4.8037, 4.8029],
        [4.8029, 5.6696, 6.0778]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.1260044425725937 
model_pd.l_d.mean(): -19.596435546875 
model_pd.lagr.mean(): -19.470430374145508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5526], device='cuda:0')), ('power', tensor([-20.3751], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.1260044425725937
epoch£º114	 i:1 	 global-step:2281	 l-p:0.14142738282680511
epoch£º114	 i:2 	 global-step:2282	 l-p:0.14235760271549225
epoch£º114	 i:3 	 global-step:2283	 l-p:0.15759176015853882
epoch£º114	 i:4 	 global-step:2284	 l-p:0.1312214732170105
epoch£º114	 i:5 	 global-step:2285	 l-p:0.13056114315986633
epoch£º114	 i:6 	 global-step:2286	 l-p:0.12767702341079712
epoch£º114	 i:7 	 global-step:2287	 l-p:0.13334107398986816
epoch£º114	 i:8 	 global-step:2288	 l-p:0.08846119791269302
epoch£º114	 i:9 	 global-step:2289	 l-p:0.1360904425382614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8590, 4.8590, 4.8590],
        [4.8590, 4.8590, 4.8590],
        [4.8590, 4.8590, 4.8590],
        [4.8590, 5.0529, 4.9967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.12731550633907318 
model_pd.l_d.mean(): -20.264554977416992 
model_pd.lagr.mean(): -20.137239456176758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4765], device='cuda:0')), ('power', tensor([-20.9728], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.12731550633907318
epoch£º115	 i:1 	 global-step:2301	 l-p:0.11605078727006912
epoch£º115	 i:2 	 global-step:2302	 l-p:0.12111835181713104
epoch£º115	 i:3 	 global-step:2303	 l-p:0.6380696892738342
epoch£º115	 i:4 	 global-step:2304	 l-p:0.17217551171779633
epoch£º115	 i:5 	 global-step:2305	 l-p:0.13116322457790375
epoch£º115	 i:6 	 global-step:2306	 l-p:0.13663756847381592
epoch£º115	 i:7 	 global-step:2307	 l-p:0.13647909462451935
epoch£º115	 i:8 	 global-step:2308	 l-p:0.44292938709259033
epoch£º115	 i:9 	 global-step:2309	 l-p:0.19257743656635284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7770, 4.8521, 4.8082],
        [4.7770, 4.8115, 4.7860],
        [4.7770, 4.7770, 4.7770],
        [4.7770, 4.9689, 4.9146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.11802409589290619 
model_pd.l_d.mean(): -19.752134323120117 
model_pd.lagr.mean(): -19.634109497070312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5378], device='cuda:0')), ('power', tensor([-20.5174], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.11802409589290619
epoch£º116	 i:1 	 global-step:2321	 l-p:0.14100971817970276
epoch£º116	 i:2 	 global-step:2322	 l-p:0.36584749817848206
epoch£º116	 i:3 	 global-step:2323	 l-p:0.15108436346054077
epoch£º116	 i:4 	 global-step:2324	 l-p:0.1030905544757843
epoch£º116	 i:5 	 global-step:2325	 l-p:0.17403629422187805
epoch£º116	 i:6 	 global-step:2326	 l-p:0.15331271290779114
epoch£º116	 i:7 	 global-step:2327	 l-p:0.13738489151000977
epoch£º116	 i:8 	 global-step:2328	 l-p:0.10913664847612381
epoch£º116	 i:9 	 global-step:2329	 l-p:0.1331188976764679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0272, 5.0282, 5.0272],
        [5.0272, 5.0892, 5.0491],
        [5.0272, 5.0272, 5.0272],
        [5.0272, 5.0272, 5.0272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.12944476306438446 
model_pd.l_d.mean(): -20.764751434326172 
model_pd.lagr.mean(): -20.63530731201172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3770], device='cuda:0')), ('power', tensor([-21.3768], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.12944476306438446
epoch£º117	 i:1 	 global-step:2341	 l-p:0.13371959328651428
epoch£º117	 i:2 	 global-step:2342	 l-p:0.13443724811077118
epoch£º117	 i:3 	 global-step:2343	 l-p:0.11467336118221283
epoch£º117	 i:4 	 global-step:2344	 l-p:0.14235876500606537
epoch£º117	 i:5 	 global-step:2345	 l-p:0.12434371560811996
epoch£º117	 i:6 	 global-step:2346	 l-p:0.2798925042152405
epoch£º117	 i:7 	 global-step:2347	 l-p:0.14010675251483917
epoch£º117	 i:8 	 global-step:2348	 l-p:-0.2807316482067108
epoch£º117	 i:9 	 global-step:2349	 l-p:0.15427857637405396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7121, 5.8615, 6.5997],
        [4.7121, 5.0061, 4.9815],
        [4.7121, 4.7312, 4.7157],
        [4.7121, 4.7121, 4.7121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.12218080461025238 
model_pd.l_d.mean(): -19.742918014526367 
model_pd.lagr.mean(): -19.620737075805664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5612], device='cuda:0')), ('power', tensor([-20.5320], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.12218080461025238
epoch£º118	 i:1 	 global-step:2361	 l-p:0.1412966102361679
epoch£º118	 i:2 	 global-step:2362	 l-p:0.14145757257938385
epoch£º118	 i:3 	 global-step:2363	 l-p:0.19654709100723267
epoch£º118	 i:4 	 global-step:2364	 l-p:0.14453358948230743
epoch£º118	 i:5 	 global-step:2365	 l-p:0.15233665704727173
epoch£º118	 i:6 	 global-step:2366	 l-p:0.15165379643440247
epoch£º118	 i:7 	 global-step:2367	 l-p:-3.8013899326324463
epoch£º118	 i:8 	 global-step:2368	 l-p:0.49016273021698
epoch£º118	 i:9 	 global-step:2369	 l-p:0.18055041134357452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8881, 5.0810, 5.0246],
        [4.8881, 4.8960, 4.8890],
        [4.8881, 5.4221, 5.5269],
        [4.8881, 5.5574, 5.7707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.16675588488578796 
model_pd.l_d.mean(): -20.724071502685547 
model_pd.lagr.mean(): -20.557315826416016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4260], device='cuda:0')), ('power', tensor([-21.3858], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.16675588488578796
epoch£º119	 i:1 	 global-step:2381	 l-p:0.13331852853298187
epoch£º119	 i:2 	 global-step:2382	 l-p:0.12292730808258057
epoch£º119	 i:3 	 global-step:2383	 l-p:0.11853937059640884
epoch£º119	 i:4 	 global-step:2384	 l-p:0.11453112959861755
epoch£º119	 i:5 	 global-step:2385	 l-p:0.11662659049034119
epoch£º119	 i:6 	 global-step:2386	 l-p:0.1251595914363861
epoch£º119	 i:7 	 global-step:2387	 l-p:0.14094850420951843
epoch£º119	 i:8 	 global-step:2388	 l-p:-0.1242419183254242
epoch£º119	 i:9 	 global-step:2389	 l-p:0.1461232602596283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9116, 4.9117, 4.9116],
        [4.9116, 4.9118, 4.9116],
        [4.9116, 4.9125, 4.9117],
        [4.9116, 5.3191, 5.3397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.1310277134180069 
model_pd.l_d.mean(): -20.30597686767578 
model_pd.lagr.mean(): -20.174949645996094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4627], device='cuda:0')), ('power', tensor([-21.0006], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.1310277134180069
epoch£º120	 i:1 	 global-step:2401	 l-p:0.12613509595394135
epoch£º120	 i:2 	 global-step:2402	 l-p:0.12777549028396606
epoch£º120	 i:3 	 global-step:2403	 l-p:0.1521228402853012
epoch£º120	 i:4 	 global-step:2404	 l-p:0.14741481840610504
epoch£º120	 i:5 	 global-step:2405	 l-p:0.12710566818714142
epoch£º120	 i:6 	 global-step:2406	 l-p:0.14593540132045746
epoch£º120	 i:7 	 global-step:2407	 l-p:0.1261429637670517
epoch£º120	 i:8 	 global-step:2408	 l-p:0.08757686614990234
epoch£º120	 i:9 	 global-step:2409	 l-p:0.04899677634239197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7389, 4.9189, 4.8650],
        [4.7389, 4.7647, 4.7447],
        [4.7389, 4.7389, 4.7389],
        [4.7389, 4.7395, 4.7390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.14720797538757324 
model_pd.l_d.mean(): -19.512258529663086 
model_pd.lagr.mean(): -19.36505126953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4978], device='cuda:0')), ('power', tensor([-20.2340], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.14720797538757324
epoch£º121	 i:1 	 global-step:2421	 l-p:0.10668914765119553
epoch£º121	 i:2 	 global-step:2422	 l-p:0.13416026532649994
epoch£º121	 i:3 	 global-step:2423	 l-p:0.13525031507015228
epoch£º121	 i:4 	 global-step:2424	 l-p:0.20093199610710144
epoch£º121	 i:5 	 global-step:2425	 l-p:0.12882670760154724
epoch£º121	 i:6 	 global-step:2426	 l-p:0.13877400755882263
epoch£º121	 i:7 	 global-step:2427	 l-p:0.16517531871795654
epoch£º121	 i:8 	 global-step:2428	 l-p:0.12525486946105957
epoch£º121	 i:9 	 global-step:2429	 l-p:0.13323885202407837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9519, 4.9785, 4.9578],
        [4.9519, 4.9616, 4.9531],
        [4.9519, 4.9519, 4.9519],
        [4.9519, 4.9587, 4.9526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.13254080712795258 
model_pd.l_d.mean(): -18.616777420043945 
model_pd.lagr.mean(): -18.484235763549805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5176], device='cuda:0')), ('power', tensor([-19.3490], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.13254080712795258
epoch£º122	 i:1 	 global-step:2441	 l-p:0.11315503716468811
epoch£º122	 i:2 	 global-step:2442	 l-p:0.14787860214710236
epoch£º122	 i:3 	 global-step:2443	 l-p:0.1347586065530777
epoch£º122	 i:4 	 global-step:2444	 l-p:0.13366122543811798
epoch£º122	 i:5 	 global-step:2445	 l-p:0.11103036999702454
epoch£º122	 i:6 	 global-step:2446	 l-p:0.14293278753757477
epoch£º122	 i:7 	 global-step:2447	 l-p:0.13065378367900848
epoch£º122	 i:8 	 global-step:2448	 l-p:0.11598474532365799
epoch£º122	 i:9 	 global-step:2449	 l-p:0.13312138617038727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7853, 4.7857, 4.7853],
        [4.7853, 4.8367, 4.8025],
        [4.7853, 4.7909, 4.7858],
        [4.7853, 4.7853, 4.7853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.1392684131860733 
model_pd.l_d.mean(): -20.52486228942871 
model_pd.lagr.mean(): -20.38559341430664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4787], device='cuda:0')), ('power', tensor([-21.2382], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.1392684131860733
epoch£º123	 i:1 	 global-step:2461	 l-p:0.13566488027572632
epoch£º123	 i:2 	 global-step:2462	 l-p:0.1760098934173584
epoch£º123	 i:3 	 global-step:2463	 l-p:0.14319750666618347
epoch£º123	 i:4 	 global-step:2464	 l-p:0.068473219871521
epoch£º123	 i:5 	 global-step:2465	 l-p:0.1453886181116104
epoch£º123	 i:6 	 global-step:2466	 l-p:0.21027906239032745
epoch£º123	 i:7 	 global-step:2467	 l-p:0.14651356637477875
epoch£º123	 i:8 	 global-step:2468	 l-p:5.5312628746032715
epoch£º123	 i:9 	 global-step:2469	 l-p:0.08913480490446091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6756, 4.6759, 4.6756],
        [4.6756, 5.3672, 5.6271],
        [4.6756, 4.7073, 4.6838],
        [4.6756, 4.7228, 4.6910]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.219879150390625 
model_pd.l_d.mean(): -20.14448356628418 
model_pd.lagr.mean(): -19.924604415893555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5301], device='cuda:0')), ('power', tensor([-20.9062], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.219879150390625
epoch£º124	 i:1 	 global-step:2481	 l-p:-0.1832006424665451
epoch£º124	 i:2 	 global-step:2482	 l-p:0.1809689998626709
epoch£º124	 i:3 	 global-step:2483	 l-p:0.13148342072963715
epoch£º124	 i:4 	 global-step:2484	 l-p:0.14064909517765045
epoch£º124	 i:5 	 global-step:2485	 l-p:0.1032804474234581
epoch£º124	 i:6 	 global-step:2486	 l-p:0.11696553975343704
epoch£º124	 i:7 	 global-step:2487	 l-p:0.10889524221420288
epoch£º124	 i:8 	 global-step:2488	 l-p:0.11589593440294266
epoch£º124	 i:9 	 global-step:2489	 l-p:0.11167103797197342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2933, 5.3084, 5.2955],
        [5.2933, 6.7117, 7.6683],
        [5.2933, 5.3124, 5.2965],
        [5.2933, 5.7491, 5.7754]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.10582859814167023 
model_pd.l_d.mean(): -19.020652770996094 
model_pd.lagr.mean(): -18.914823532104492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3972], device='cuda:0')), ('power', tensor([-19.6343], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.10582859814167023
epoch£º125	 i:1 	 global-step:2501	 l-p:0.11200494319200516
epoch£º125	 i:2 	 global-step:2502	 l-p:0.11221738904714584
epoch£º125	 i:3 	 global-step:2503	 l-p:0.12958799302577972
epoch£º125	 i:4 	 global-step:2504	 l-p:0.11510604619979858
epoch£º125	 i:5 	 global-step:2505	 l-p:0.12889905273914337
epoch£º125	 i:6 	 global-step:2506	 l-p:0.12400539964437485
epoch£º125	 i:7 	 global-step:2507	 l-p:0.19418397545814514
epoch£º125	 i:8 	 global-step:2508	 l-p:0.14392688870429993
epoch£º125	 i:9 	 global-step:2509	 l-p:0.1576327681541443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6927, 5.3561, 5.5890],
        [4.6927, 5.1113, 5.1548],
        [4.6927, 4.6927, 4.6927],
        [4.6927, 4.6977, 4.6932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.14195752143859863 
model_pd.l_d.mean(): -20.695751190185547 
model_pd.lagr.mean(): -20.55379295349121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4766], device='cuda:0')), ('power', tensor([-21.4088], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.14195752143859863
epoch£º126	 i:1 	 global-step:2521	 l-p:0.16387206315994263
epoch£º126	 i:2 	 global-step:2522	 l-p:0.16865752637386322
epoch£º126	 i:3 	 global-step:2523	 l-p:0.17569099366664886
epoch£º126	 i:4 	 global-step:2524	 l-p:0.14735250174999237
epoch£º126	 i:5 	 global-step:2525	 l-p:0.15556645393371582
epoch£º126	 i:6 	 global-step:2526	 l-p:0.11348530650138855
epoch£º126	 i:7 	 global-step:2527	 l-p:0.17040660977363586
epoch£º126	 i:8 	 global-step:2528	 l-p:-0.04627350717782974
epoch£º126	 i:9 	 global-step:2529	 l-p:-0.2055606245994568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2616,  0.1673,  1.0000,  0.1070,
          1.0000,  0.6396, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6901,  0.6098,  1.0000,  0.5389,
          1.0000,  0.8837, 31.6228]], device='cuda:0')
 pt:tensor([[4.6878, 4.7974, 4.7464],
        [4.6878, 4.7629, 4.7200],
        [4.6878, 4.8728, 4.8214],
        [4.6878, 5.4595, 5.7949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.09847179800271988 
model_pd.l_d.mean(): -19.732093811035156 
model_pd.lagr.mean(): -19.633621215820312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5375], device='cuda:0')), ('power', tensor([-20.4968], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.09847179800271988
epoch£º127	 i:1 	 global-step:2541	 l-p:0.24362993240356445
epoch£º127	 i:2 	 global-step:2542	 l-p:0.13802199065685272
epoch£º127	 i:3 	 global-step:2543	 l-p:0.14091643691062927
epoch£º127	 i:4 	 global-step:2544	 l-p:0.11357738822698593
epoch£º127	 i:5 	 global-step:2545	 l-p:0.14619827270507812
epoch£º127	 i:6 	 global-step:2546	 l-p:0.1190783679485321
epoch£º127	 i:7 	 global-step:2547	 l-p:0.11689545959234238
epoch£º127	 i:8 	 global-step:2548	 l-p:0.11896898597478867
epoch£º127	 i:9 	 global-step:2549	 l-p:0.11203303933143616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2054, 5.6830, 5.7288],
        [5.2054, 5.3938, 5.3313],
        [5.2054, 5.2084, 5.2056],
        [5.2054, 5.2062, 5.2055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.09624392539262772 
model_pd.l_d.mean(): -19.65865135192871 
model_pd.lagr.mean(): -19.562406539916992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4349], device='cuda:0')), ('power', tensor([-20.3178], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.09624392539262772
epoch£º128	 i:1 	 global-step:2561	 l-p:0.12332051992416382
epoch£º128	 i:2 	 global-step:2562	 l-p:0.12178033590316772
epoch£º128	 i:3 	 global-step:2563	 l-p:0.19876843690872192
epoch£º128	 i:4 	 global-step:2564	 l-p:0.14209702610969543
epoch£º128	 i:5 	 global-step:2565	 l-p:0.14448288083076477
epoch£º128	 i:6 	 global-step:2566	 l-p:0.15669958293437958
epoch£º128	 i:7 	 global-step:2567	 l-p:0.12646138668060303
epoch£º128	 i:8 	 global-step:2568	 l-p:0.1411818116903305
epoch£º128	 i:9 	 global-step:2569	 l-p:-0.05513293296098709
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7559, 4.7567, 4.7559],
        [4.7559, 4.7726, 4.7588],
        [4.7559, 4.7559, 4.7559],
        [4.7559, 4.8818, 4.8280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.11934978514909744 
model_pd.l_d.mean(): -19.985822677612305 
model_pd.lagr.mean(): -19.866472244262695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5479], device='cuda:0')), ('power', tensor([-20.7640], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.11934978514909744
epoch£º129	 i:1 	 global-step:2581	 l-p:0.23816464841365814
epoch£º129	 i:2 	 global-step:2582	 l-p:0.14637191593647003
epoch£º129	 i:3 	 global-step:2583	 l-p:0.15547879040241241
epoch£º129	 i:4 	 global-step:2584	 l-p:0.13004407286643982
epoch£º129	 i:5 	 global-step:2585	 l-p:0.1259792000055313
epoch£º129	 i:6 	 global-step:2586	 l-p:0.15478195250034332
epoch£º129	 i:7 	 global-step:2587	 l-p:0.15062372386455536
epoch£º129	 i:8 	 global-step:2588	 l-p:0.14669199287891388
epoch£º129	 i:9 	 global-step:2589	 l-p:0.12790746986865997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8931, 5.0317, 4.9751],
        [4.8931, 4.8931, 4.8931],
        [4.8931, 4.8931, 4.8931],
        [4.8931, 4.8942, 4.8932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.05324307829141617 
model_pd.l_d.mean(): -18.751596450805664 
model_pd.lagr.mean(): -18.698352813720703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5354], device='cuda:0')), ('power', tensor([-19.5034], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.05324307829141617
epoch£º130	 i:1 	 global-step:2601	 l-p:0.16436515748500824
epoch£º130	 i:2 	 global-step:2602	 l-p:0.13259871304035187
epoch£º130	 i:3 	 global-step:2603	 l-p:0.13835889101028442
epoch£º130	 i:4 	 global-step:2604	 l-p:0.12866687774658203
epoch£º130	 i:5 	 global-step:2605	 l-p:0.1563277393579483
epoch£º130	 i:6 	 global-step:2606	 l-p:0.12384704500436783
epoch£º130	 i:7 	 global-step:2607	 l-p:0.2065443992614746
epoch£º130	 i:8 	 global-step:2608	 l-p:0.129185289144516
epoch£º130	 i:9 	 global-step:2609	 l-p:0.13685664534568787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9149, 4.9149, 4.9149],
        [4.9149, 4.9902, 4.9460],
        [4.9149, 4.9150, 4.9149],
        [4.9149, 4.9149, 4.9149]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.13943062722682953 
model_pd.l_d.mean(): -19.717037200927734 
model_pd.lagr.mean(): -19.577606201171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4984], device='cuda:0')), ('power', tensor([-20.4417], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.13943062722682953
epoch£º131	 i:1 	 global-step:2621	 l-p:0.16039979457855225
epoch£º131	 i:2 	 global-step:2622	 l-p:0.15113243460655212
epoch£º131	 i:3 	 global-step:2623	 l-p:0.0801546722650528
epoch£º131	 i:4 	 global-step:2624	 l-p:0.12559686601161957
epoch£º131	 i:5 	 global-step:2625	 l-p:0.1487724632024765
epoch£º131	 i:6 	 global-step:2626	 l-p:0.13280443847179413
epoch£º131	 i:7 	 global-step:2627	 l-p:0.1596919596195221
epoch£º131	 i:8 	 global-step:2628	 l-p:0.1252422332763672
epoch£º131	 i:9 	 global-step:2629	 l-p:0.15524578094482422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9342, 4.9343, 4.9342],
        [4.9342, 6.0200, 6.6522],
        [4.9342, 4.9342, 4.9342],
        [4.9342, 5.3588, 5.3916]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.1171233057975769 
model_pd.l_d.mean(): -19.11957359313965 
model_pd.lagr.mean(): -19.002450942993164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5287], device='cuda:0')), ('power', tensor([-19.8686], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.1171233057975769
epoch£º132	 i:1 	 global-step:2641	 l-p:0.11805965006351471
epoch£º132	 i:2 	 global-step:2642	 l-p:0.12227495014667511
epoch£º132	 i:3 	 global-step:2643	 l-p:0.12845924496650696
epoch£º132	 i:4 	 global-step:2644	 l-p:0.13996723294258118
epoch£º132	 i:5 	 global-step:2645	 l-p:0.12983834743499756
epoch£º132	 i:6 	 global-step:2646	 l-p:0.1326560378074646
epoch£º132	 i:7 	 global-step:2647	 l-p:-0.3242800831794739
epoch£º132	 i:8 	 global-step:2648	 l-p:0.12532424926757812
epoch£º132	 i:9 	 global-step:2649	 l-p:0.16938692331314087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9633, 4.9633, 4.9633],
        [4.9633, 4.9633, 4.9633],
        [4.9633, 5.2694, 5.2421],
        [4.9633, 5.0746, 5.0208]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.14211565256118774 
model_pd.l_d.mean(): -18.585256576538086 
model_pd.lagr.mean(): -18.443140029907227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5125], device='cuda:0')), ('power', tensor([-19.3119], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.14211565256118774
epoch£º133	 i:1 	 global-step:2661	 l-p:0.12338553369045258
epoch£º133	 i:2 	 global-step:2662	 l-p:0.135629341006279
epoch£º133	 i:3 	 global-step:2663	 l-p:0.13458365201950073
epoch£º133	 i:4 	 global-step:2664	 l-p:0.13398437201976776
epoch£º133	 i:5 	 global-step:2665	 l-p:0.12387629598379135
epoch£º133	 i:6 	 global-step:2666	 l-p:0.15726177394390106
epoch£º133	 i:7 	 global-step:2667	 l-p:0.10740502178668976
epoch£º133	 i:8 	 global-step:2668	 l-p:0.15949282050132751
epoch£º133	 i:9 	 global-step:2669	 l-p:0.13608108460903168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9565, 5.2468, 5.2143],
        [4.9565, 5.0353, 4.9898],
        [4.9565, 4.9721, 4.9590],
        [4.9565, 4.9589, 4.9566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.13903044164180756 
model_pd.l_d.mean(): -20.33591651916504 
model_pd.lagr.mean(): -20.19688606262207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4413], device='cuda:0')), ('power', tensor([-21.0090], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.13903044164180756
epoch£º134	 i:1 	 global-step:2681	 l-p:0.12881341576576233
epoch£º134	 i:2 	 global-step:2682	 l-p:0.12976430356502533
epoch£º134	 i:3 	 global-step:2683	 l-p:0.19369901716709137
epoch£º134	 i:4 	 global-step:2684	 l-p:0.12775449454784393
epoch£º134	 i:5 	 global-step:2685	 l-p:0.11525478214025497
epoch£º134	 i:6 	 global-step:2686	 l-p:0.1298200488090515
epoch£º134	 i:7 	 global-step:2687	 l-p:0.1170801892876625
epoch£º134	 i:8 	 global-step:2688	 l-p:0.11491645872592926
epoch£º134	 i:9 	 global-step:2689	 l-p:0.14984619617462158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9485, 4.9485, 4.9485],
        [4.9485, 6.0586, 6.7180],
        [4.9485, 5.0731, 5.0176],
        [4.9485, 5.8683, 6.3214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.12527979910373688 
model_pd.l_d.mean(): -18.75283432006836 
model_pd.lagr.mean(): -18.627553939819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5492], device='cuda:0')), ('power', tensor([-19.5188], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.12527979910373688
epoch£º135	 i:1 	 global-step:2701	 l-p:0.09575940668582916
epoch£º135	 i:2 	 global-step:2702	 l-p:0.1251632124185562
epoch£º135	 i:3 	 global-step:2703	 l-p:0.17985183000564575
epoch£º135	 i:4 	 global-step:2704	 l-p:0.1289377510547638
epoch£º135	 i:5 	 global-step:2705	 l-p:0.15725447237491608
epoch£º135	 i:6 	 global-step:2706	 l-p:0.16906151175498962
epoch£º135	 i:7 	 global-step:2707	 l-p:0.11557210981845856
epoch£º135	 i:8 	 global-step:2708	 l-p:0.1288587599992752
epoch£º135	 i:9 	 global-step:2709	 l-p:0.14107881486415863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9326, 4.9327, 4.9326],
        [4.9326, 5.0753, 5.0186],
        [4.9326, 4.9333, 4.9326],
        [4.9326, 5.1044, 5.0476]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.1410580575466156 
model_pd.l_d.mean(): -20.93806266784668 
model_pd.lagr.mean(): -20.79700469970703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3873], device='cuda:0')), ('power', tensor([-21.5626], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.1410580575466156
epoch£º136	 i:1 	 global-step:2721	 l-p:0.14095091819763184
epoch£º136	 i:2 	 global-step:2722	 l-p:0.15685485303401947
epoch£º136	 i:3 	 global-step:2723	 l-p:0.12046729773283005
epoch£º136	 i:4 	 global-step:2724	 l-p:0.1483631134033203
epoch£º136	 i:5 	 global-step:2725	 l-p:0.11636438220739365
epoch£º136	 i:6 	 global-step:2726	 l-p:0.1849488765001297
epoch£º136	 i:7 	 global-step:2727	 l-p:0.1346040517091751
epoch£º136	 i:8 	 global-step:2728	 l-p:0.17906320095062256
epoch£º136	 i:9 	 global-step:2729	 l-p:0.11643771827220917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9626, 4.9626, 4.9626],
        [4.9626, 4.9792, 4.9655],
        [4.9626, 4.9626, 4.9626],
        [4.9626, 5.9680, 6.5105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): -0.21733130514621735 
model_pd.l_d.mean(): -20.639101028442383 
model_pd.lagr.mean(): -20.85643196105957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4171], device='cuda:0')), ('power', tensor([-21.2908], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:-0.21733130514621735
epoch£º137	 i:1 	 global-step:2741	 l-p:0.13477736711502075
epoch£º137	 i:2 	 global-step:2742	 l-p:0.11629074811935425
epoch£º137	 i:3 	 global-step:2743	 l-p:0.12511572241783142
epoch£º137	 i:4 	 global-step:2744	 l-p:0.11655975878238678
epoch£º137	 i:5 	 global-step:2745	 l-p:0.13058848679065704
epoch£º137	 i:6 	 global-step:2746	 l-p:0.11374679952859879
epoch£º137	 i:7 	 global-step:2747	 l-p:0.10907068103551865
epoch£º137	 i:8 	 global-step:2748	 l-p:0.14096106588840485
epoch£º137	 i:9 	 global-step:2749	 l-p:0.1456695795059204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9450, 5.0152, 4.9730],
        [4.9450, 4.9450, 4.9450],
        [4.9450, 4.9450, 4.9450],
        [4.9450, 4.9450, 4.9450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.13197945058345795 
model_pd.l_d.mean(): -20.644393920898438 
model_pd.lagr.mean(): -20.512414932250977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4153], device='cuda:0')), ('power', tensor([-21.2943], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.13197945058345795
epoch£º138	 i:1 	 global-step:2761	 l-p:0.12657366693019867
epoch£º138	 i:2 	 global-step:2762	 l-p:0.13945885002613068
epoch£º138	 i:3 	 global-step:2763	 l-p:0.2630811333656311
epoch£º138	 i:4 	 global-step:2764	 l-p:0.15144531428813934
epoch£º138	 i:5 	 global-step:2765	 l-p:0.13410897552967072
epoch£º138	 i:6 	 global-step:2766	 l-p:0.06284262239933014
epoch£º138	 i:7 	 global-step:2767	 l-p:-0.004456805996596813
epoch£º138	 i:8 	 global-step:2768	 l-p:0.13680942356586456
epoch£º138	 i:9 	 global-step:2769	 l-p:0.2062848061323166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8998, 5.8802, 6.4067],
        [4.8998, 6.0689, 6.8069],
        [4.8998, 4.8998, 4.8998],
        [4.8998, 4.9019, 4.8999]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.08524306863546371 
model_pd.l_d.mean(): -18.715904235839844 
model_pd.lagr.mean(): -18.630661010742188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5404], device='cuda:0')), ('power', tensor([-19.4725], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.08524306863546371
epoch£º139	 i:1 	 global-step:2781	 l-p:0.14947080612182617
epoch£º139	 i:2 	 global-step:2782	 l-p:0.13185220956802368
epoch£º139	 i:3 	 global-step:2783	 l-p:0.13532906770706177
epoch£º139	 i:4 	 global-step:2784	 l-p:0.1286405771970749
epoch£º139	 i:5 	 global-step:2785	 l-p:0.11740146577358246
epoch£º139	 i:6 	 global-step:2786	 l-p:0.13436168432235718
epoch£º139	 i:7 	 global-step:2787	 l-p:0.12016300857067108
epoch£º139	 i:8 	 global-step:2788	 l-p:0.16960102319717407
epoch£º139	 i:9 	 global-step:2789	 l-p:0.16296225786209106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8772, 5.9064, 6.4901],
        [4.8772, 4.9108, 4.8860],
        [4.8772, 5.4738, 5.6372],
        [4.8772, 5.9258, 6.5308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.14227047562599182 
model_pd.l_d.mean(): -20.090377807617188 
model_pd.lagr.mean(): -19.94810676574707 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4946], device='cuda:0')), ('power', tensor([-20.8152], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.14227047562599182
epoch£º140	 i:1 	 global-step:2801	 l-p:0.13655923306941986
epoch£º140	 i:2 	 global-step:2802	 l-p:0.13427744805812836
epoch£º140	 i:3 	 global-step:2803	 l-p:0.17216773331165314
epoch£º140	 i:4 	 global-step:2804	 l-p:0.10804177820682526
epoch£º140	 i:5 	 global-step:2805	 l-p:0.13066662847995758
epoch£º140	 i:6 	 global-step:2806	 l-p:0.13725516200065613
epoch£º140	 i:7 	 global-step:2807	 l-p:0.13687990605831146
epoch£º140	 i:8 	 global-step:2808	 l-p:0.12624011933803558
epoch£º140	 i:9 	 global-step:2809	 l-p:0.12929391860961914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0309, 5.0310, 5.0309],
        [5.0309, 5.0310, 5.0309],
        [5.0309, 5.0309, 5.0309],
        [5.0309, 5.1604, 5.1038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.1361769437789917 
model_pd.l_d.mean(): -20.141586303710938 
model_pd.lagr.mean(): -20.005409240722656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4396], device='cuda:0')), ('power', tensor([-20.8108], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.1361769437789917
epoch£º141	 i:1 	 global-step:2821	 l-p:0.11883781105279922
epoch£º141	 i:2 	 global-step:2822	 l-p:0.4114396572113037
epoch£º141	 i:3 	 global-step:2823	 l-p:0.1284993439912796
epoch£º141	 i:4 	 global-step:2824	 l-p:0.1370944231748581
epoch£º141	 i:5 	 global-step:2825	 l-p:0.12967777252197266
epoch£º141	 i:6 	 global-step:2826	 l-p:0.13021890819072723
epoch£º141	 i:7 	 global-step:2827	 l-p:0.12636256217956543
epoch£º141	 i:8 	 global-step:2828	 l-p:0.048189759254455566
epoch£º141	 i:9 	 global-step:2829	 l-p:0.19941306114196777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7062, 4.7986, 4.7517],
        [4.7062, 4.8646, 4.8123],
        [4.7062, 4.8480, 4.7954],
        [4.7062, 4.7066, 4.7062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.4735479950904846 
model_pd.l_d.mean(): -20.265653610229492 
model_pd.lagr.mean(): -19.792104721069336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5249], device='cuda:0')), ('power', tensor([-21.0234], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.4735479950904846
epoch£º142	 i:1 	 global-step:2841	 l-p:0.16609671711921692
epoch£º142	 i:2 	 global-step:2842	 l-p:0.14317354559898376
epoch£º142	 i:3 	 global-step:2843	 l-p:-0.31363755464553833
epoch£º142	 i:4 	 global-step:2844	 l-p:0.1526547521352768
epoch£º142	 i:5 	 global-step:2845	 l-p:0.15587429702281952
epoch£º142	 i:6 	 global-step:2846	 l-p:0.11304257810115814
epoch£º142	 i:7 	 global-step:2847	 l-p:0.11818141490221024
epoch£º142	 i:8 	 global-step:2848	 l-p:0.12783662974834442
epoch£º142	 i:9 	 global-step:2849	 l-p:0.1120646670460701
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1699, 5.1699, 5.1699],
        [5.1699, 5.1699, 5.1699],
        [5.1699, 5.1700, 5.1699],
        [5.1699, 5.9434, 6.2308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.1171668991446495 
model_pd.l_d.mean(): -18.952970504760742 
model_pd.lagr.mean(): -18.835803985595703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4599], device='cuda:0')), ('power', tensor([-19.6299], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.1171668991446495
epoch£º143	 i:1 	 global-step:2861	 l-p:0.11352948844432831
epoch£º143	 i:2 	 global-step:2862	 l-p:0.11672374606132507
epoch£º143	 i:3 	 global-step:2863	 l-p:0.13291004300117493
epoch£º143	 i:4 	 global-step:2864	 l-p:0.12742698192596436
epoch£º143	 i:5 	 global-step:2865	 l-p:0.1233748197555542
epoch£º143	 i:6 	 global-step:2866	 l-p:0.15727022290229797
epoch£º143	 i:7 	 global-step:2867	 l-p:0.14085868000984192
epoch£º143	 i:8 	 global-step:2868	 l-p:0.10172450542449951
epoch£º143	 i:9 	 global-step:2869	 l-p:0.17131328582763672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7389, 5.8968, 6.6536],
        [4.7389, 5.2827, 5.4187],
        [4.7389, 4.8285, 4.7820],
        [4.7389, 4.7441, 4.7394]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.13802045583724976 
model_pd.l_d.mean(): -19.54542350769043 
model_pd.lagr.mean(): -19.40740394592285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5242], device='cuda:0')), ('power', tensor([-20.2945], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.13802045583724976
epoch£º144	 i:1 	 global-step:2881	 l-p:0.13731655478477478
epoch£º144	 i:2 	 global-step:2882	 l-p:0.8976826071739197
epoch£º144	 i:3 	 global-step:2883	 l-p:0.11642498522996902
epoch£º144	 i:4 	 global-step:2884	 l-p:0.04442040994763374
epoch£º144	 i:5 	 global-step:2885	 l-p:0.17070956528186798
epoch£º144	 i:6 	 global-step:2886	 l-p:0.15110152959823608
epoch£º144	 i:7 	 global-step:2887	 l-p:0.0997985377907753
epoch£º144	 i:8 	 global-step:2888	 l-p:0.15677832067012787
epoch£º144	 i:9 	 global-step:2889	 l-p:0.12231328338384628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0241, 5.7362, 5.9854],
        [5.0241, 5.8313, 6.1681],
        [5.0241, 5.0330, 5.0252],
        [5.0241, 6.1066, 6.7282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.12291364371776581 
model_pd.l_d.mean(): -20.473590850830078 
model_pd.lagr.mean(): -20.350677490234375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4191], device='cuda:0')), ('power', tensor([-21.1254], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.12291364371776581
epoch£º145	 i:1 	 global-step:2901	 l-p:0.19273920357227325
epoch£º145	 i:2 	 global-step:2902	 l-p:0.11853478848934174
epoch£º145	 i:3 	 global-step:2903	 l-p:0.12223052978515625
epoch£º145	 i:4 	 global-step:2904	 l-p:0.1343545764684677
epoch£º145	 i:5 	 global-step:2905	 l-p:0.11446249485015869
epoch£º145	 i:6 	 global-step:2906	 l-p:0.13535848259925842
epoch£º145	 i:7 	 global-step:2907	 l-p:0.10579978674650192
epoch£º145	 i:8 	 global-step:2908	 l-p:0.14327305555343628
epoch£º145	 i:9 	 global-step:2909	 l-p:0.1431720107793808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8910, 4.8910, 4.8910],
        [4.8910, 4.9224, 4.8990],
        [4.8910, 5.1414, 5.1011],
        [4.8910, 4.9501, 4.9127]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.153000608086586 
model_pd.l_d.mean(): -19.637460708618164 
model_pd.lagr.mean(): -19.484460830688477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4967], device='cuda:0')), ('power', tensor([-20.3595], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.153000608086586
epoch£º146	 i:1 	 global-step:2921	 l-p:0.16735365986824036
epoch£º146	 i:2 	 global-step:2922	 l-p:0.13910219073295593
epoch£º146	 i:3 	 global-step:2923	 l-p:0.13617944717407227
epoch£º146	 i:4 	 global-step:2924	 l-p:-0.02159285917878151
epoch£º146	 i:5 	 global-step:2925	 l-p:0.14188022911548615
epoch£º146	 i:6 	 global-step:2926	 l-p:0.1615276336669922
epoch£º146	 i:7 	 global-step:2927	 l-p:0.14258907735347748
epoch£º146	 i:8 	 global-step:2928	 l-p:0.17253609001636505
epoch£º146	 i:9 	 global-step:2929	 l-p:0.19414469599723816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8221, 4.9906, 4.9371],
        [4.8221, 4.8222, 4.8221],
        [4.8221, 4.8660, 4.8359],
        [4.8221, 5.0702, 5.0317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.136629119515419 
model_pd.l_d.mean(): -20.78437042236328 
model_pd.lagr.mean(): -20.647741317749023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4406], device='cuda:0')), ('power', tensor([-21.4616], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.136629119515419
epoch£º147	 i:1 	 global-step:2941	 l-p:0.13038335740566254
epoch£º147	 i:2 	 global-step:2942	 l-p:0.13309499621391296
epoch£º147	 i:3 	 global-step:2943	 l-p:0.16095837950706482
epoch£º147	 i:4 	 global-step:2944	 l-p:0.13943561911582947
epoch£º147	 i:5 	 global-step:2945	 l-p:0.1471857726573944
epoch£º147	 i:6 	 global-step:2946	 l-p:0.05776146799325943
epoch£º147	 i:7 	 global-step:2947	 l-p:0.11968858540058136
epoch£º147	 i:8 	 global-step:2948	 l-p:0.14238815009593964
epoch£º147	 i:9 	 global-step:2949	 l-p:0.12378814071416855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0169, 5.0381, 5.0211],
        [5.0169, 5.0267, 5.0182],
        [5.0169, 6.1192, 6.7654],
        [5.0169, 5.0169, 5.0169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.12316320091485977 
model_pd.l_d.mean(): -20.501426696777344 
model_pd.lagr.mean(): -20.378263473510742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4118], device='cuda:0')), ('power', tensor([-21.1461], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.12316320091485977
epoch£º148	 i:1 	 global-step:2961	 l-p:0.15081074833869934
epoch£º148	 i:2 	 global-step:2962	 l-p:0.12833271920681
epoch£º148	 i:3 	 global-step:2963	 l-p:0.1693330556154251
epoch£º148	 i:4 	 global-step:2964	 l-p:0.13700413703918457
epoch£º148	 i:5 	 global-step:2965	 l-p:0.1186041608452797
epoch£º148	 i:6 	 global-step:2966	 l-p:0.14176784455776215
epoch£º148	 i:7 	 global-step:2967	 l-p:0.10500054061412811
epoch£º148	 i:8 	 global-step:2968	 l-p:0.12056592851877213
epoch£º148	 i:9 	 global-step:2969	 l-p:0.13967077434062958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8391, 4.9000, 4.8622],
        [4.8391, 4.8681, 4.8463],
        [4.8391, 5.8639, 6.4534],
        [4.8391, 5.2855, 5.3448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.19449461996555328 
model_pd.l_d.mean(): -18.755414962768555 
model_pd.lagr.mean(): -18.56092071533203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5831], device='cuda:0')), ('power', tensor([-19.5561], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.19449461996555328
epoch£º149	 i:1 	 global-step:2981	 l-p:0.14283959567546844
epoch£º149	 i:2 	 global-step:2982	 l-p:0.23028641939163208
epoch£º149	 i:3 	 global-step:2983	 l-p:0.13946904242038727
epoch£º149	 i:4 	 global-step:2984	 l-p:0.10392646491527557
epoch£º149	 i:5 	 global-step:2985	 l-p:0.14259886741638184
epoch£º149	 i:6 	 global-step:2986	 l-p:0.1373528391122818
epoch£º149	 i:7 	 global-step:2987	 l-p:0.17946287989616394
epoch£º149	 i:8 	 global-step:2988	 l-p:0.11623448133468628
epoch£º149	 i:9 	 global-step:2989	 l-p:0.14683493971824646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8550, 5.1442, 5.1191],
        [4.8550, 5.2684, 5.3060],
        [4.8550, 5.1380, 5.1105],
        [4.8550, 4.8569, 4.8552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.13420291244983673 
model_pd.l_d.mean(): -18.662220001220703 
model_pd.lagr.mean(): -18.528017044067383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5523], device='cuda:0')), ('power', tensor([-19.4304], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.13420291244983673
epoch£º150	 i:1 	 global-step:3001	 l-p:0.17181095480918884
epoch£º150	 i:2 	 global-step:3002	 l-p:0.06377919018268585
epoch£º150	 i:3 	 global-step:3003	 l-p:0.12168359756469727
epoch£º150	 i:4 	 global-step:3004	 l-p:0.12089885771274567
epoch£º150	 i:5 	 global-step:3005	 l-p:0.12665696442127228
epoch£º150	 i:6 	 global-step:3006	 l-p:0.12319706380367279
epoch£º150	 i:7 	 global-step:3007	 l-p:0.1272905170917511
epoch£º150	 i:8 	 global-step:3008	 l-p:0.1284281313419342
epoch£º150	 i:9 	 global-step:3009	 l-p:0.16400061547756195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9225, 5.2713, 5.2696],
        [4.9225, 4.9225, 4.9225],
        [4.9225, 5.6900, 6.0043],
        [4.9225, 5.3899, 5.4578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.13014452159404755 
model_pd.l_d.mean(): -20.736202239990234 
model_pd.lagr.mean(): -20.60605812072754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4178], device='cuda:0')), ('power', tensor([-21.3896], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.13014452159404755
epoch£º151	 i:1 	 global-step:3021	 l-p:0.14095193147659302
epoch£º151	 i:2 	 global-step:3022	 l-p:0.22008119523525238
epoch£º151	 i:3 	 global-step:3023	 l-p:0.019734084606170654
epoch£º151	 i:4 	 global-step:3024	 l-p:0.3467504382133484
epoch£º151	 i:5 	 global-step:3025	 l-p:0.1586190015077591
epoch£º151	 i:6 	 global-step:3026	 l-p:0.14771288633346558
epoch£º151	 i:7 	 global-step:3027	 l-p:0.06827212870121002
epoch£º151	 i:8 	 global-step:3028	 l-p:0.1447804719209671
epoch£º151	 i:9 	 global-step:3029	 l-p:0.12968119978904724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2225,  0.1348,  1.0000,  0.0817,
          1.0000,  0.6059, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228]], device='cuda:0')
 pt:tensor([[4.9094, 5.0421, 4.9876],
        [4.9094, 5.8390, 6.3168],
        [4.9094, 5.0814, 5.0270],
        [4.9094, 5.0765, 5.0217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.09888523817062378 
model_pd.l_d.mean(): -19.859479904174805 
model_pd.lagr.mean(): -19.760595321655273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5230], device='cuda:0')), ('power', tensor([-20.6108], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.09888523817062378
epoch£º152	 i:1 	 global-step:3041	 l-p:0.11933143436908722
epoch£º152	 i:2 	 global-step:3042	 l-p:0.13062678277492523
epoch£º152	 i:3 	 global-step:3043	 l-p:0.12601885199546814
epoch£º152	 i:4 	 global-step:3044	 l-p:0.13487133383750916
epoch£º152	 i:5 	 global-step:3045	 l-p:0.12301889061927795
epoch£º152	 i:6 	 global-step:3046	 l-p:0.12868452072143555
epoch£º152	 i:7 	 global-step:3047	 l-p:0.14273975789546967
epoch£º152	 i:8 	 global-step:3048	 l-p:0.16294066607952118
epoch£º152	 i:9 	 global-step:3049	 l-p:0.12502315640449524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9620, 4.9658, 4.9623],
        [4.9620, 4.9621, 4.9620],
        [4.9620, 4.9723, 4.9634],
        [4.9620, 4.9620, 4.9620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.17590846121311188 
model_pd.l_d.mean(): -19.3745174407959 
model_pd.lagr.mean(): -19.1986083984375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4619], device='cuda:0')), ('power', tensor([-20.0581], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.17590846121311188
epoch£º153	 i:1 	 global-step:3061	 l-p:0.12960614264011383
epoch£º153	 i:2 	 global-step:3062	 l-p:0.06712303310632706
epoch£º153	 i:3 	 global-step:3063	 l-p:0.141714945435524
epoch£º153	 i:4 	 global-step:3064	 l-p:0.13708563148975372
epoch£º153	 i:5 	 global-step:3065	 l-p:0.11808963865041733
epoch£º153	 i:6 	 global-step:3066	 l-p:0.14354035258293152
epoch£º153	 i:7 	 global-step:3067	 l-p:0.12174247205257416
epoch£º153	 i:8 	 global-step:3068	 l-p:0.14104878902435303
epoch£º153	 i:9 	 global-step:3069	 l-p:0.172220379114151
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7397, 5.3093, 5.4701],
        [4.7397, 4.8731, 4.8210],
        [4.7397, 4.7610, 4.7442],
        [4.7397, 4.7436, 4.7400]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.1414233148097992 
model_pd.l_d.mean(): -18.744279861450195 
model_pd.lagr.mean(): -18.602855682373047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5893], device='cuda:0')), ('power', tensor([-19.5512], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.1414233148097992
epoch£º154	 i:1 	 global-step:3081	 l-p:0.13062319159507751
epoch£º154	 i:2 	 global-step:3082	 l-p:0.2004508674144745
epoch£º154	 i:3 	 global-step:3083	 l-p:0.13997782766819
epoch£º154	 i:4 	 global-step:3084	 l-p:0.12102671712636948
epoch£º154	 i:5 	 global-step:3085	 l-p:0.1569138467311859
epoch£º154	 i:6 	 global-step:3086	 l-p:0.13411082327365875
epoch£º154	 i:7 	 global-step:3087	 l-p:0.14188164472579956
epoch£º154	 i:8 	 global-step:3088	 l-p:0.37196406722068787
epoch£º154	 i:9 	 global-step:3089	 l-p:0.13024653494358063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0544, 5.6952, 5.8845],
        [5.0544, 5.6066, 5.7260],
        [5.0544, 5.0969, 5.0671],
        [5.0544, 5.3767, 5.3575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.12937356531620026 
model_pd.l_d.mean(): -19.95016860961914 
model_pd.lagr.mean(): -19.8207950592041 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4462], device='cuda:0')), ('power', tensor([-20.6240], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.12937356531620026
epoch£º155	 i:1 	 global-step:3101	 l-p:0.24715854227542877
epoch£º155	 i:2 	 global-step:3102	 l-p:0.13077068328857422
epoch£º155	 i:3 	 global-step:3103	 l-p:0.1256963163614273
epoch£º155	 i:4 	 global-step:3104	 l-p:0.11778142303228378
epoch£º155	 i:5 	 global-step:3105	 l-p:0.15363234281539917
epoch£º155	 i:6 	 global-step:3106	 l-p:0.12744459509849548
epoch£º155	 i:7 	 global-step:3107	 l-p:0.1664089411497116
epoch£º155	 i:8 	 global-step:3108	 l-p:0.12190254777669907
epoch£º155	 i:9 	 global-step:3109	 l-p:0.1368243545293808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9160,  0.8896,  1.0000,  0.8640,
          1.0000,  0.9712, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228]], device='cuda:0')
 pt:tensor([[4.8870, 5.9777, 6.6363],
        [4.8870, 6.0009, 6.6857],
        [4.8870, 5.0347, 4.9802],
        [4.8870, 5.9744, 6.6293]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.12619447708129883 
model_pd.l_d.mean(): -19.98299789428711 
model_pd.lagr.mean(): -19.85680389404297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5019], device='cuda:0')), ('power', tensor([-20.7141], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.12619447708129883
epoch£º156	 i:1 	 global-step:3121	 l-p:0.15135420858860016
epoch£º156	 i:2 	 global-step:3122	 l-p:0.1253318190574646
epoch£º156	 i:3 	 global-step:3123	 l-p:0.07846078276634216
epoch£º156	 i:4 	 global-step:3124	 l-p:0.14663086831569672
epoch£º156	 i:5 	 global-step:3125	 l-p:0.13939516246318817
epoch£º156	 i:6 	 global-step:3126	 l-p:0.1545577496290207
epoch£º156	 i:7 	 global-step:3127	 l-p:0.13000737130641937
epoch£º156	 i:8 	 global-step:3128	 l-p:0.12198879569768906
epoch£º156	 i:9 	 global-step:3129	 l-p:0.14602838456630707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1482,  0.0784,  1.0000,  0.0415,
          1.0000,  0.5292, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228]], device='cuda:0')
 pt:tensor([[4.9445, 5.0063, 4.9679],
        [4.9445, 5.4315, 5.5134],
        [4.9445, 5.8217, 6.2433],
        [4.9445, 5.1717, 5.1253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.07944972068071365 
model_pd.l_d.mean(): -20.169662475585938 
model_pd.lagr.mean(): -20.090211868286133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4783], device='cuda:0')), ('power', tensor([-20.8787], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.07944972068071365
epoch£º157	 i:1 	 global-step:3141	 l-p:0.11509034782648087
epoch£º157	 i:2 	 global-step:3142	 l-p:0.16390813887119293
epoch£º157	 i:3 	 global-step:3143	 l-p:0.12628737092018127
epoch£º157	 i:4 	 global-step:3144	 l-p:0.14506517350673676
epoch£º157	 i:5 	 global-step:3145	 l-p:0.18913565576076508
epoch£º157	 i:6 	 global-step:3146	 l-p:0.13293150067329407
epoch£º157	 i:7 	 global-step:3147	 l-p:0.15370208024978638
epoch£º157	 i:8 	 global-step:3148	 l-p:0.14092475175857544
epoch£º157	 i:9 	 global-step:3149	 l-p:0.2076369673013687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8969, 4.8970, 4.8969],
        [4.8969, 4.9047, 4.8979],
        [4.8969, 4.8970, 4.8969],
        [4.8969, 5.2925, 5.3194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.14895284175872803 
model_pd.l_d.mean(): -19.583742141723633 
model_pd.lagr.mean(): -19.434789657592773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4825], device='cuda:0')), ('power', tensor([-20.2906], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.14895284175872803
epoch£º158	 i:1 	 global-step:3161	 l-p:0.13469301164150238
epoch£º158	 i:2 	 global-step:3162	 l-p:0.12151207774877548
epoch£º158	 i:3 	 global-step:3163	 l-p:0.12073340266942978
epoch£º158	 i:4 	 global-step:3164	 l-p:0.12663118541240692
epoch£º158	 i:5 	 global-step:3165	 l-p:0.13046810030937195
epoch£º158	 i:6 	 global-step:3166	 l-p:0.15536123514175415
epoch£º158	 i:7 	 global-step:3167	 l-p:0.14182543754577637
epoch£º158	 i:8 	 global-step:3168	 l-p:0.13796430826187134
epoch£º158	 i:9 	 global-step:3169	 l-p:0.2769440710544586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8208, 4.9538, 4.9010],
        [4.8208, 4.8786, 4.8423],
        [4.8208, 4.9203, 4.8715],
        [4.8208, 4.8525, 4.8292]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.4180757403373718 
model_pd.l_d.mean(): -20.487672805786133 
model_pd.lagr.mean(): -20.069597244262695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4828], device='cuda:0')), ('power', tensor([-21.2048], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.4180757403373718
epoch£º159	 i:1 	 global-step:3181	 l-p:0.1394852101802826
epoch£º159	 i:2 	 global-step:3182	 l-p:0.13685716688632965
epoch£º159	 i:3 	 global-step:3183	 l-p:-0.4479115605354309
epoch£º159	 i:4 	 global-step:3184	 l-p:0.15447501838207245
epoch£º159	 i:5 	 global-step:3185	 l-p:0.1324952244758606
epoch£º159	 i:6 	 global-step:3186	 l-p:0.12132975459098816
epoch£º159	 i:7 	 global-step:3187	 l-p:0.1358351707458496
epoch£º159	 i:8 	 global-step:3188	 l-p:0.12447452545166016
epoch£º159	 i:9 	 global-step:3189	 l-p:0.18452446162700653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8677, 4.9223, 4.8872],
        [4.8677, 5.5553, 5.8066],
        [4.8677, 4.8677, 4.8677],
        [4.8677, 4.8682, 4.8677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.19721265137195587 
model_pd.l_d.mean(): -20.888843536376953 
model_pd.lagr.mean(): -20.691631317138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4179], device='cuda:0')), ('power', tensor([-21.5440], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.19721265137195587
epoch£º160	 i:1 	 global-step:3201	 l-p:0.11666759848594666
epoch£º160	 i:2 	 global-step:3202	 l-p:0.12616091966629028
epoch£º160	 i:3 	 global-step:3203	 l-p:0.13175541162490845
epoch£º160	 i:4 	 global-step:3204	 l-p:0.1416354477405548
epoch£º160	 i:5 	 global-step:3205	 l-p:-0.07914365828037262
epoch£º160	 i:6 	 global-step:3206	 l-p:0.12825393676757812
epoch£º160	 i:7 	 global-step:3207	 l-p:0.1326577067375183
epoch£º160	 i:8 	 global-step:3208	 l-p:0.13389548659324646
epoch£º160	 i:9 	 global-step:3209	 l-p:0.1566382795572281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2302,  0.1411,  1.0000,  0.0865,
          1.0000,  0.6129, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1536,  0.0823,  1.0000,  0.0441,
          1.0000,  0.5356, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228]], device='cuda:0')
 pt:tensor([[4.9397, 5.0731, 5.0187],
        [4.9397, 5.0032, 4.9642],
        [4.9397, 5.5149, 5.6643],
        [4.9397, 5.5890, 5.7995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.1601996123790741 
model_pd.l_d.mean(): -20.868894577026367 
model_pd.lagr.mean(): -20.708694458007812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4005], device='cuda:0')), ('power', tensor([-21.5061], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.1601996123790741
epoch£º161	 i:1 	 global-step:3221	 l-p:0.13114707171916962
epoch£º161	 i:2 	 global-step:3222	 l-p:0.283307820558548
epoch£º161	 i:3 	 global-step:3223	 l-p:0.11708482354879379
epoch£º161	 i:4 	 global-step:3224	 l-p:0.036047838628292084
epoch£º161	 i:5 	 global-step:3225	 l-p:0.19145646691322327
epoch£º161	 i:6 	 global-step:3226	 l-p:0.1307990700006485
epoch£º161	 i:7 	 global-step:3227	 l-p:0.14571918547153473
epoch£º161	 i:8 	 global-step:3228	 l-p:0.17050029337406158
epoch£º161	 i:9 	 global-step:3229	 l-p:0.16710969805717468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6408, 5.1010, 5.1898],
        [4.6408, 4.6412, 4.6408],
        [4.6408, 4.7159, 4.6742],
        [4.6408, 5.2163, 5.3959]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.12154872715473175 
model_pd.l_d.mean(): -20.335466384887695 
model_pd.lagr.mean(): -20.213916778564453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5525], device='cuda:0')), ('power', tensor([-21.1222], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.12154872715473175
epoch£º162	 i:1 	 global-step:3241	 l-p:0.1546345055103302
epoch£º162	 i:2 	 global-step:3242	 l-p:0.16063044965267181
epoch£º162	 i:3 	 global-step:3243	 l-p:0.1591300517320633
epoch£º162	 i:4 	 global-step:3244	 l-p:0.07120045274496078
epoch£º162	 i:5 	 global-step:3245	 l-p:0.12858489155769348
epoch£º162	 i:6 	 global-step:3246	 l-p:0.1959967315196991
epoch£º162	 i:7 	 global-step:3247	 l-p:0.08599425107240677
epoch£º162	 i:8 	 global-step:3248	 l-p:0.5676352977752686
epoch£º162	 i:9 	 global-step:3249	 l-p:0.12522700428962708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9430, 5.8062, 6.2165],
        [4.9430, 4.9430, 4.9430],
        [4.9430, 5.2567, 5.2406],
        [4.9430, 4.9430, 4.9430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.14483648538589478 
model_pd.l_d.mean(): -18.544212341308594 
model_pd.lagr.mean(): -18.399375915527344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5659], device='cuda:0')), ('power', tensor([-19.3250], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.14483648538589478
epoch£º163	 i:1 	 global-step:3261	 l-p:0.11734257638454437
epoch£º163	 i:2 	 global-step:3262	 l-p:0.11954665929079056
epoch£º163	 i:3 	 global-step:3263	 l-p:0.171098992228508
epoch£º163	 i:4 	 global-step:3264	 l-p:0.13174888491630554
epoch£º163	 i:5 	 global-step:3265	 l-p:0.13821907341480255
epoch£º163	 i:6 	 global-step:3266	 l-p:0.11682458966970444
epoch£º163	 i:7 	 global-step:3267	 l-p:0.14107324182987213
epoch£º163	 i:8 	 global-step:3268	 l-p:0.12590628862380981
epoch£º163	 i:9 	 global-step:3269	 l-p:0.13194335997104645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9567, 5.1772, 5.1299],
        [4.9567, 4.9607, 4.9570],
        [4.9567, 4.9575, 4.9567],
        [4.9567, 4.9793, 4.9615]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.12746456265449524 
model_pd.l_d.mean(): -20.163738250732422 
model_pd.lagr.mean(): -20.036273956298828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4618], device='cuda:0')), ('power', tensor([-20.8559], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.12746456265449524
epoch£º164	 i:1 	 global-step:3281	 l-p:0.12813426554203033
epoch£º164	 i:2 	 global-step:3282	 l-p:0.1466173678636551
epoch£º164	 i:3 	 global-step:3283	 l-p:0.111243337392807
epoch£º164	 i:4 	 global-step:3284	 l-p:0.14925241470336914
epoch£º164	 i:5 	 global-step:3285	 l-p:0.12970535457134247
epoch£º164	 i:6 	 global-step:3286	 l-p:0.12972231209278107
epoch£º164	 i:7 	 global-step:3287	 l-p:0.1451050490140915
epoch£º164	 i:8 	 global-step:3288	 l-p:0.1501992791891098
epoch£º164	 i:9 	 global-step:3289	 l-p:0.1478876769542694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8951, 4.8951, 4.8951],
        [4.8951, 5.6363, 5.9356],
        [4.8951, 5.4144, 5.5263],
        [4.8951, 5.0772, 5.0250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.13100190460681915 
model_pd.l_d.mean(): -20.204376220703125 
model_pd.lagr.mean(): -20.073373794555664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4780], device='cuda:0')), ('power', tensor([-20.9135], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.13100190460681915
epoch£º165	 i:1 	 global-step:3301	 l-p:0.13213136792182922
epoch£º165	 i:2 	 global-step:3302	 l-p:0.16265225410461426
epoch£º165	 i:3 	 global-step:3303	 l-p:0.14334672689437866
epoch£º165	 i:4 	 global-step:3304	 l-p:-0.016626177355647087
epoch£º165	 i:5 	 global-step:3305	 l-p:0.4067019820213318
epoch£º165	 i:6 	 global-step:3306	 l-p:0.12461766600608826
epoch£º165	 i:7 	 global-step:3307	 l-p:0.14270052313804626
epoch£º165	 i:8 	 global-step:3308	 l-p:0.01495349407196045
epoch£º165	 i:9 	 global-step:3309	 l-p:0.13115334510803223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8703, 4.8719, 4.8704],
        [4.8703, 4.9373, 4.8972],
        [4.8703, 4.9465, 4.9033],
        [4.8703, 4.8703, 4.8703]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.14294900000095367 
model_pd.l_d.mean(): -18.978899002075195 
model_pd.lagr.mean(): -18.83595085144043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5456], device='cuda:0')), ('power', tensor([-19.7437], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.14294900000095367
epoch£º166	 i:1 	 global-step:3321	 l-p:0.13213302195072174
epoch£º166	 i:2 	 global-step:3322	 l-p:0.14451313018798828
epoch£º166	 i:3 	 global-step:3323	 l-p:0.14301970601081848
epoch£º166	 i:4 	 global-step:3324	 l-p:0.12642788887023926
epoch£º166	 i:5 	 global-step:3325	 l-p:0.11149520426988602
epoch£º166	 i:6 	 global-step:3326	 l-p:0.12366149574518204
epoch£º166	 i:7 	 global-step:3327	 l-p:0.13026462495326996
epoch£º166	 i:8 	 global-step:3328	 l-p:0.1189211905002594
epoch£º166	 i:9 	 global-step:3329	 l-p:0.8375544548034668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0056, 5.0056, 5.0056],
        [5.0056, 5.1581, 5.1023],
        [5.0056, 6.2000, 6.9611],
        [5.0056, 5.0175, 5.0074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.1349252462387085 
model_pd.l_d.mean(): -19.4580135345459 
model_pd.lagr.mean(): -19.323087692260742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4820], device='cuda:0')), ('power', tensor([-20.1630], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.1349252462387085
epoch£º167	 i:1 	 global-step:3341	 l-p:0.13003334403038025
epoch£º167	 i:2 	 global-step:3342	 l-p:0.1270914375782013
epoch£º167	 i:3 	 global-step:3343	 l-p:0.13898716866970062
epoch£º167	 i:4 	 global-step:3344	 l-p:0.1327374428510666
epoch£º167	 i:5 	 global-step:3345	 l-p:0.12146511673927307
epoch£º167	 i:6 	 global-step:3346	 l-p:0.37041378021240234
epoch£º167	 i:7 	 global-step:3347	 l-p:0.07089854031801224
epoch£º167	 i:8 	 global-step:3348	 l-p:0.04804828390479088
epoch£º167	 i:9 	 global-step:3349	 l-p:0.1206582635641098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8240, 4.8246, 4.8241],
        [4.8240, 5.8396, 6.4302],
        [4.8240, 4.8241, 4.8240],
        [4.8240, 4.8241, 4.8240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.1283157914876938 
model_pd.l_d.mean(): -18.658109664916992 
model_pd.lagr.mean(): -18.529794692993164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5751], device='cuda:0')), ('power', tensor([-19.4495], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.1283157914876938
epoch£º168	 i:1 	 global-step:3361	 l-p:0.12601737678050995
epoch£º168	 i:2 	 global-step:3362	 l-p:0.14647895097732544
epoch£º168	 i:3 	 global-step:3363	 l-p:0.1445416957139969
epoch£º168	 i:4 	 global-step:3364	 l-p:0.141448974609375
epoch£º168	 i:5 	 global-step:3365	 l-p:0.11996982246637344
epoch£º168	 i:6 	 global-step:3366	 l-p:0.11652900278568268
epoch£º168	 i:7 	 global-step:3367	 l-p:0.14142724871635437
epoch£º168	 i:8 	 global-step:3368	 l-p:0.15155062079429626
epoch£º168	 i:9 	 global-step:3369	 l-p:0.12819160521030426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8907, 4.8909, 4.8907],
        [4.8907, 4.9091, 4.8943],
        [4.8907, 5.0211, 4.9678],
        [4.8907, 4.9412, 4.9079]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.13750283420085907 
model_pd.l_d.mean(): -20.71607208251953 
model_pd.lagr.mean(): -20.578569412231445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4292], device='cuda:0')), ('power', tensor([-21.3809], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.13750283420085907
epoch£º169	 i:1 	 global-step:3381	 l-p:0.1419377326965332
epoch£º169	 i:2 	 global-step:3382	 l-p:0.13368959724903107
epoch£º169	 i:3 	 global-step:3383	 l-p:-0.10803461074829102
epoch£º169	 i:4 	 global-step:3384	 l-p:0.1405070424079895
epoch£º169	 i:5 	 global-step:3385	 l-p:0.09745147824287415
epoch£º169	 i:6 	 global-step:3386	 l-p:0.0516805574297905
epoch£º169	 i:7 	 global-step:3387	 l-p:0.15241681039333344
epoch£º169	 i:8 	 global-step:3388	 l-p:0.12538912892341614
epoch£º169	 i:9 	 global-step:3389	 l-p:0.14869755506515503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9488, 5.1440, 5.0929],
        [4.9488, 4.9496, 4.9488],
        [4.9488, 4.9724, 4.9540],
        [4.9488, 4.9494, 4.9488]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.15095169842243195 
model_pd.l_d.mean(): -18.792495727539062 
model_pd.lagr.mean(): -18.641544342041016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5342], device='cuda:0')), ('power', tensor([-19.5436], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.15095169842243195
epoch£º170	 i:1 	 global-step:3401	 l-p:0.14093738794326782
epoch£º170	 i:2 	 global-step:3402	 l-p:0.13808465003967285
epoch£º170	 i:3 	 global-step:3403	 l-p:0.12541283667087555
epoch£º170	 i:4 	 global-step:3404	 l-p:0.12352132797241211
epoch£º170	 i:5 	 global-step:3405	 l-p:0.4302891790866852
epoch£º170	 i:6 	 global-step:3406	 l-p:0.12052367627620697
epoch£º170	 i:7 	 global-step:3407	 l-p:0.12274771183729172
epoch£º170	 i:8 	 global-step:3408	 l-p:0.13009628653526306
epoch£º170	 i:9 	 global-step:3409	 l-p:0.1512349545955658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9569, 5.4455, 5.5327],
        [4.9569, 4.9583, 4.9570],
        [4.9569, 4.9820, 4.9626],
        [4.9569, 4.9570, 4.9569]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.11723866313695908 
model_pd.l_d.mean(): -18.685169219970703 
model_pd.lagr.mean(): -18.567930221557617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5492], device='cuda:0')), ('power', tensor([-19.4504], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.11723866313695908
epoch£º171	 i:1 	 global-step:3421	 l-p:0.12483366578817368
epoch£º171	 i:2 	 global-step:3422	 l-p:0.16539987921714783
epoch£º171	 i:3 	 global-step:3423	 l-p:0.14427408576011658
epoch£º171	 i:4 	 global-step:3424	 l-p:0.07828859239816666
epoch£º171	 i:5 	 global-step:3425	 l-p:0.1168067678809166
epoch£º171	 i:6 	 global-step:3426	 l-p:0.1346408873796463
epoch£º171	 i:7 	 global-step:3427	 l-p:0.25297823548316956
epoch£º171	 i:8 	 global-step:3428	 l-p:0.1385161578655243
epoch£º171	 i:9 	 global-step:3429	 l-p:0.15588746964931488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8075, 4.8080, 4.8075],
        [4.8075, 4.8128, 4.8080],
        [4.8075, 5.0546, 5.0202],
        [4.8075, 4.8075, 4.8075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.11587787419557571 
model_pd.l_d.mean(): -20.79187774658203 
model_pd.lagr.mean(): -20.676000595092773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4505], device='cuda:0')), ('power', tensor([-21.4793], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.11587787419557571
epoch£º172	 i:1 	 global-step:3441	 l-p:0.19615676999092102
epoch£º172	 i:2 	 global-step:3442	 l-p:0.09902402758598328
epoch£º172	 i:3 	 global-step:3443	 l-p:-0.16619785130023956
epoch£º172	 i:4 	 global-step:3444	 l-p:0.12688247859477997
epoch£º172	 i:5 	 global-step:3445	 l-p:0.1366274207830429
epoch£º172	 i:6 	 global-step:3446	 l-p:0.14091645181179047
epoch£º172	 i:7 	 global-step:3447	 l-p:0.17888309061527252
epoch£º172	 i:8 	 global-step:3448	 l-p:0.12066817283630371
epoch£º172	 i:9 	 global-step:3449	 l-p:0.12980066239833832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2106,  0.1253,  1.0000,  0.0745,
          1.0000,  0.5949, 31.6228]], device='cuda:0')
 pt:tensor([[5.1352, 6.2636, 6.9315],
        [5.1352, 5.2356, 5.1843],
        [5.1352, 5.8314, 6.0679],
        [5.1352, 5.2477, 5.1941]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.11810413002967834 
model_pd.l_d.mean(): -19.468307495117188 
model_pd.lagr.mean(): -19.350202560424805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4743], device='cuda:0')), ('power', tensor([-20.1656], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.11810413002967834
epoch£º173	 i:1 	 global-step:3461	 l-p:0.09931766241788864
epoch£º173	 i:2 	 global-step:3462	 l-p:0.12843376398086548
epoch£º173	 i:3 	 global-step:3463	 l-p:0.14313119649887085
epoch£º173	 i:4 	 global-step:3464	 l-p:0.12501589953899384
epoch£º173	 i:5 	 global-step:3465	 l-p:0.1391165554523468
epoch£º173	 i:6 	 global-step:3466	 l-p:0.13275131583213806
epoch£º173	 i:7 	 global-step:3467	 l-p:0.09164789319038391
epoch£º173	 i:8 	 global-step:3468	 l-p:0.09233192354440689
epoch£º173	 i:9 	 global-step:3469	 l-p:0.14172421395778656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6641, 5.4157, 5.7590],
        [4.6641, 4.6756, 4.6658],
        [4.6641, 4.7299, 4.6907],
        [4.6641, 4.7830, 4.7331]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.15753112733364105 
model_pd.l_d.mean(): -20.258506774902344 
model_pd.lagr.mean(): -20.100975036621094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5426], device='cuda:0')), ('power', tensor([-21.0342], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.15753112733364105
epoch£º174	 i:1 	 global-step:3481	 l-p:0.13991989195346832
epoch£º174	 i:2 	 global-step:3482	 l-p:0.10786405950784683
epoch£º174	 i:3 	 global-step:3483	 l-p:0.15757036209106445
epoch£º174	 i:4 	 global-step:3484	 l-p:0.15775451064109802
epoch£º174	 i:5 	 global-step:3485	 l-p:-0.5204098224639893
epoch£º174	 i:6 	 global-step:3486	 l-p:0.18957816064357758
epoch£º174	 i:7 	 global-step:3487	 l-p:0.13602901995182037
epoch£º174	 i:8 	 global-step:3488	 l-p:0.1296108365058899
epoch£º174	 i:9 	 global-step:3489	 l-p:0.1185542643070221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1419, 5.1446, 5.1421],
        [5.1419, 5.1534, 5.1436],
        [5.1419, 5.1960, 5.1603],
        [5.1419, 5.2255, 5.1786]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.12142471224069595 
model_pd.l_d.mean(): -20.606672286987305 
model_pd.lagr.mean(): -20.485246658325195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3579], device='cuda:0')), ('power', tensor([-21.1974], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.12142471224069595
epoch£º175	 i:1 	 global-step:3501	 l-p:0.1204502284526825
epoch£º175	 i:2 	 global-step:3502	 l-p:0.15718084573745728
epoch£º175	 i:3 	 global-step:3503	 l-p:0.1956687569618225
epoch£º175	 i:4 	 global-step:3504	 l-p:0.1315770298242569
epoch£º175	 i:5 	 global-step:3505	 l-p:0.11829814314842224
epoch£º175	 i:6 	 global-step:3506	 l-p:0.11533787101507187
epoch£º175	 i:7 	 global-step:3507	 l-p:0.12438644468784332
epoch£º175	 i:8 	 global-step:3508	 l-p:0.12497850507497787
epoch£º175	 i:9 	 global-step:3509	 l-p:0.1243501752614975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9239, 4.9239, 4.9239],
        [4.9239, 4.9423, 4.9274],
        [4.9239, 5.4518, 5.5721],
        [4.9239, 5.1304, 5.0823]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.09485835582017899 
model_pd.l_d.mean(): -19.311046600341797 
model_pd.lagr.mean(): -19.216188430786133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5082], device='cuda:0')), ('power', tensor([-20.0413], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.09485835582017899
epoch£º176	 i:1 	 global-step:3521	 l-p:0.1733630746603012
epoch£º176	 i:2 	 global-step:3522	 l-p:0.1369851678609848
epoch£º176	 i:3 	 global-step:3523	 l-p:0.13735857605934143
epoch£º176	 i:4 	 global-step:3524	 l-p:0.10091428458690643
epoch£º176	 i:5 	 global-step:3525	 l-p:0.49958884716033936
epoch£º176	 i:6 	 global-step:3526	 l-p:0.1388544738292694
epoch£º176	 i:7 	 global-step:3527	 l-p:0.1350790113210678
epoch£º176	 i:8 	 global-step:3528	 l-p:-0.31469541788101196
epoch£º176	 i:9 	 global-step:3529	 l-p:0.1576545387506485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8008, 5.8643, 6.5157],
        [4.8008, 4.8014, 4.8008],
        [4.8008, 4.8965, 4.8484],
        [4.8008, 4.8011, 4.8008]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.11438027769327164 
model_pd.l_d.mean(): -19.82997703552246 
model_pd.lagr.mean(): -19.71559715270996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5277], device='cuda:0')), ('power', tensor([-20.5858], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.11438027769327164
epoch£º177	 i:1 	 global-step:3541	 l-p:0.132944256067276
epoch£º177	 i:2 	 global-step:3542	 l-p:0.17254601418972015
epoch£º177	 i:3 	 global-step:3543	 l-p:0.11052663624286652
epoch£º177	 i:4 	 global-step:3544	 l-p:0.13880185782909393
epoch£º177	 i:5 	 global-step:3545	 l-p:0.1079491600394249
epoch£º177	 i:6 	 global-step:3546	 l-p:0.1308198869228363
epoch£º177	 i:7 	 global-step:3547	 l-p:0.12542791664600372
epoch£º177	 i:8 	 global-step:3548	 l-p:0.14899082481861115
epoch£º177	 i:9 	 global-step:3549	 l-p:0.19665446877479553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9387, 4.9389, 4.9387],
        [4.9387, 4.9387, 4.9387],
        [4.9387, 4.9387, 4.9387],
        [4.9387, 5.0076, 4.9663]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): 0.19902339577674866 
model_pd.l_d.mean(): -18.319555282592773 
model_pd.lagr.mean(): -18.12053108215332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5403], device='cuda:0')), ('power', tensor([-19.0717], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:0.19902339577674866
epoch£º178	 i:1 	 global-step:3561	 l-p:0.14015372097492218
epoch£º178	 i:2 	 global-step:3562	 l-p:0.13764123618602753
epoch£º178	 i:3 	 global-step:3563	 l-p:0.11383336782455444
epoch£º178	 i:4 	 global-step:3564	 l-p:0.1295003443956375
epoch£º178	 i:5 	 global-step:3565	 l-p:0.15325015783309937
epoch£º178	 i:6 	 global-step:3566	 l-p:0.12533922493457794
epoch£º178	 i:7 	 global-step:3567	 l-p:0.2864017188549042
epoch£º178	 i:8 	 global-step:3568	 l-p:0.12287682294845581
epoch£º178	 i:9 	 global-step:3569	 l-p:0.07877229154109955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8146, 4.8156, 4.8146],
        [4.8146, 4.8349, 4.8186],
        [4.8146, 4.8338, 4.8183],
        [4.8146, 4.8150, 4.8146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.1745615303516388 
model_pd.l_d.mean(): -20.57073211669922 
model_pd.lagr.mean(): -20.396169662475586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4693], device='cuda:0')), ('power', tensor([-21.2749], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.1745615303516388
epoch£º179	 i:1 	 global-step:3581	 l-p:0.5507935881614685
epoch£º179	 i:2 	 global-step:3582	 l-p:0.12614737451076508
epoch£º179	 i:3 	 global-step:3583	 l-p:0.12481383234262466
epoch£º179	 i:4 	 global-step:3584	 l-p:0.1499885767698288
epoch£º179	 i:5 	 global-step:3585	 l-p:0.13075833022594452
epoch£º179	 i:6 	 global-step:3586	 l-p:0.16029822826385498
epoch£º179	 i:7 	 global-step:3587	 l-p:0.1263006627559662
epoch£º179	 i:8 	 global-step:3588	 l-p:0.1053110882639885
epoch£º179	 i:9 	 global-step:3589	 l-p:0.20090588927268982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8972, 4.8979, 4.8972],
        [4.8972, 4.8977, 4.8972],
        [4.8972, 5.0858, 5.0354],
        [4.8972, 4.8972, 4.8972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.12800465524196625 
model_pd.l_d.mean(): -20.887340545654297 
model_pd.lagr.mean(): -20.759336471557617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4121], device='cuda:0')), ('power', tensor([-21.5366], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.12800465524196625
epoch£º180	 i:1 	 global-step:3601	 l-p:0.1490323394536972
epoch£º180	 i:2 	 global-step:3602	 l-p:-2.756057024002075
epoch£º180	 i:3 	 global-step:3603	 l-p:0.20642119646072388
epoch£º180	 i:4 	 global-step:3604	 l-p:-0.08389792591333389
epoch£º180	 i:5 	 global-step:3605	 l-p:0.15990416705608368
epoch£º180	 i:6 	 global-step:3606	 l-p:0.26364731788635254
epoch£º180	 i:7 	 global-step:3607	 l-p:0.1281605213880539
epoch£º180	 i:8 	 global-step:3608	 l-p:0.1351509690284729
epoch£º180	 i:9 	 global-step:3609	 l-p:0.1341998130083084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9307, 4.9372, 4.9314],
        [4.9307, 5.0031, 4.9605],
        [4.9307, 5.3229, 5.3524],
        [4.9307, 5.5853, 5.8100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.15094341337680817 
model_pd.l_d.mean(): -20.478042602539062 
model_pd.lagr.mean(): -20.327098846435547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-21.1628], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.15094341337680817
epoch£º181	 i:1 	 global-step:3621	 l-p:0.19987136125564575
epoch£º181	 i:2 	 global-step:3622	 l-p:0.12429339438676834
epoch£º181	 i:3 	 global-step:3623	 l-p:0.1285988986492157
epoch£º181	 i:4 	 global-step:3624	 l-p:0.13532637059688568
epoch£º181	 i:5 	 global-step:3625	 l-p:0.10633552074432373
epoch£º181	 i:6 	 global-step:3626	 l-p:0.13399171829223633
epoch£º181	 i:7 	 global-step:3627	 l-p:0.12670035660266876
epoch£º181	 i:8 	 global-step:3628	 l-p:0.17258408665657043
epoch£º181	 i:9 	 global-step:3629	 l-p:0.19639451801776886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9395, 4.9475, 4.9404],
        [4.9395, 4.9891, 4.9557],
        [4.9395, 5.0613, 5.0081],
        [4.9395, 4.9395, 4.9395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.12427698075771332 
model_pd.l_d.mean(): -20.257434844970703 
model_pd.lagr.mean(): -20.13315773010254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4389], device='cuda:0')), ('power', tensor([-20.9272], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.12427698075771332
epoch£º182	 i:1 	 global-step:3641	 l-p:0.1427507847547531
epoch£º182	 i:2 	 global-step:3642	 l-p:0.12839879095554352
epoch£º182	 i:3 	 global-step:3643	 l-p:0.12142867594957352
epoch£º182	 i:4 	 global-step:3644	 l-p:0.21496905386447906
epoch£º182	 i:5 	 global-step:3645	 l-p:0.0847998782992363
epoch£º182	 i:6 	 global-step:3646	 l-p:0.1809975653886795
epoch£º182	 i:7 	 global-step:3647	 l-p:0.20534202456474304
epoch£º182	 i:8 	 global-step:3648	 l-p:0.14273634552955627
epoch£º182	 i:9 	 global-step:3649	 l-p:0.11360303312540054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0609, 5.3789, 5.3634],
        [5.0609, 5.2015, 5.1456],
        [5.0609, 5.0692, 5.0619],
        [5.0609, 5.7900, 6.0675]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.12625227868556976 
model_pd.l_d.mean(): -20.340787887573242 
model_pd.lagr.mean(): -20.214534759521484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4169], device='cuda:0')), ('power', tensor([-20.9890], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.12625227868556976
epoch£º183	 i:1 	 global-step:3661	 l-p:0.12626920640468597
epoch£º183	 i:2 	 global-step:3662	 l-p:0.13033168017864227
epoch£º183	 i:3 	 global-step:3663	 l-p:0.12541474401950836
epoch£º183	 i:4 	 global-step:3664	 l-p:0.19418232142925262
epoch£º183	 i:5 	 global-step:3665	 l-p:0.41211768984794617
epoch£º183	 i:6 	 global-step:3666	 l-p:0.14067848026752472
epoch£º183	 i:7 	 global-step:3667	 l-p:-1.5958448648452759
epoch£º183	 i:8 	 global-step:3668	 l-p:0.1289014369249344
epoch£º183	 i:9 	 global-step:3669	 l-p:0.12876015901565552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9054, 4.9744, 4.9329],
        [4.9054, 5.6723, 6.0030],
        [4.9054, 4.9374, 4.9134],
        [4.9054, 4.9054, 4.9054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.14093390107154846 
model_pd.l_d.mean(): -19.021873474121094 
model_pd.lagr.mean(): -18.880939483642578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5307], device='cuda:0')), ('power', tensor([-19.7719], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.14093390107154846
epoch£º184	 i:1 	 global-step:3681	 l-p:0.16306167840957642
epoch£º184	 i:2 	 global-step:3682	 l-p:0.1296522617340088
epoch£º184	 i:3 	 global-step:3683	 l-p:0.12197954952716827
epoch£º184	 i:4 	 global-step:3684	 l-p:0.13019488751888275
epoch£º184	 i:5 	 global-step:3685	 l-p:0.10056592524051666
epoch£º184	 i:6 	 global-step:3686	 l-p:0.13618376851081848
epoch£º184	 i:7 	 global-step:3687	 l-p:0.12137655913829803
epoch£º184	 i:8 	 global-step:3688	 l-p:0.11443639546632767
epoch£º184	 i:9 	 global-step:3689	 l-p:0.09778808057308197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9300, 5.0471, 4.9943],
        [4.9300, 4.9300, 4.9300],
        [4.9300, 4.9445, 4.9323],
        [4.9300, 5.6584, 5.9506]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.12938259541988373 
model_pd.l_d.mean(): -19.371826171875 
model_pd.lagr.mean(): -19.242443084716797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5220], device='cuda:0')), ('power', tensor([-20.1168], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.12938259541988373
epoch£º185	 i:1 	 global-step:3701	 l-p:0.2400667518377304
epoch£º185	 i:2 	 global-step:3702	 l-p:0.13699136674404144
epoch£º185	 i:3 	 global-step:3703	 l-p:0.1487191766500473
epoch£º185	 i:4 	 global-step:3704	 l-p:0.1451980620622635
epoch£º185	 i:5 	 global-step:3705	 l-p:0.01418549008667469
epoch£º185	 i:6 	 global-step:3706	 l-p:0.14426444470882416
epoch£º185	 i:7 	 global-step:3707	 l-p:0.16219455003738403
epoch£º185	 i:8 	 global-step:3708	 l-p:0.11549035459756851
epoch£º185	 i:9 	 global-step:3709	 l-p:-2.596336841583252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8944, 4.8944, 4.8944],
        [4.8944, 5.2419, 5.2495],
        [4.8944, 5.1998, 5.1867],
        [4.8944, 4.9971, 4.9466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.13235394656658173 
model_pd.l_d.mean(): -20.221006393432617 
model_pd.lagr.mean(): -20.088651657104492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4862], device='cuda:0')), ('power', tensor([-20.9387], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.13235394656658173
epoch£º186	 i:1 	 global-step:3721	 l-p:0.13773450255393982
epoch£º186	 i:2 	 global-step:3722	 l-p:0.19752417504787445
epoch£º186	 i:3 	 global-step:3723	 l-p:0.18734343349933624
epoch£º186	 i:4 	 global-step:3724	 l-p:0.10520897805690765
epoch£º186	 i:5 	 global-step:3725	 l-p:0.11362019926309586
epoch£º186	 i:6 	 global-step:3726	 l-p:0.13410182297229767
epoch£º186	 i:7 	 global-step:3727	 l-p:0.13955290615558624
epoch£º186	 i:8 	 global-step:3728	 l-p:0.12080726027488708
epoch£º186	 i:9 	 global-step:3729	 l-p:0.11620623618364334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1181, 5.1182, 5.1181],
        [5.1181, 5.2981, 5.2423],
        [5.1181, 5.4358, 5.4184],
        [5.1181, 5.1181, 5.1181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.19261306524276733 
model_pd.l_d.mean(): -19.39527702331543 
model_pd.lagr.mean(): -19.20266342163086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4525], device='cuda:0')), ('power', tensor([-20.0695], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.19261306524276733
epoch£º187	 i:1 	 global-step:3741	 l-p:0.14034758508205414
epoch£º187	 i:2 	 global-step:3742	 l-p:0.11663968861103058
epoch£º187	 i:3 	 global-step:3743	 l-p:0.13576258718967438
epoch£º187	 i:4 	 global-step:3744	 l-p:0.1387120634317398
epoch£º187	 i:5 	 global-step:3745	 l-p:0.12621620297431946
epoch£º187	 i:6 	 global-step:3746	 l-p:0.05972834303975105
epoch£º187	 i:7 	 global-step:3747	 l-p:0.09835878759622574
epoch£º187	 i:8 	 global-step:3748	 l-p:0.10246704518795013
epoch£º187	 i:9 	 global-step:3749	 l-p:0.1438964456319809
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6941, 5.3894, 5.6810],
        [4.6941, 4.7561, 4.7170],
        [4.6941, 4.6941, 4.6941],
        [4.6941, 5.3783, 5.6596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.1499200463294983 
model_pd.l_d.mean(): -20.68931007385254 
model_pd.lagr.mean(): -20.539390563964844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4995], device='cuda:0')), ('power', tensor([-21.4257], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.1499200463294983
epoch£º188	 i:1 	 global-step:3761	 l-p:0.13252076506614685
epoch£º188	 i:2 	 global-step:3762	 l-p:0.12470614165067673
epoch£º188	 i:3 	 global-step:3763	 l-p:0.14235888421535492
epoch£º188	 i:4 	 global-step:3764	 l-p:0.08908885717391968
epoch£º188	 i:5 	 global-step:3765	 l-p:0.1844816952943802
epoch£º188	 i:6 	 global-step:3766	 l-p:0.16655169427394867
epoch£º188	 i:7 	 global-step:3767	 l-p:0.1096939817070961
epoch£º188	 i:8 	 global-step:3768	 l-p:0.1731683313846588
epoch£º188	 i:9 	 global-step:3769	 l-p:0.11730965226888657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0382, 6.0354, 6.5816],
        [5.0382, 6.0515, 6.6147],
        [5.0382, 5.0383, 5.0382],
        [5.0382, 5.0382, 5.0382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.10281267017126083 
model_pd.l_d.mean(): -19.673938751220703 
model_pd.lagr.mean(): -19.57112693786621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4677], device='cuda:0')), ('power', tensor([-20.3667], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.10281267017126083
epoch£º189	 i:1 	 global-step:3781	 l-p:0.2102561593055725
epoch£º189	 i:2 	 global-step:3782	 l-p:0.11469399183988571
epoch£º189	 i:3 	 global-step:3783	 l-p:0.12662623822689056
epoch£º189	 i:4 	 global-step:3784	 l-p:0.12223192304372787
epoch£º189	 i:5 	 global-step:3785	 l-p:0.10520799458026886
epoch£º189	 i:6 	 global-step:3786	 l-p:0.12324202060699463
epoch£º189	 i:7 	 global-step:3787	 l-p:0.11035659909248352
epoch£º189	 i:8 	 global-step:3788	 l-p:0.11788225173950195
epoch£º189	 i:9 	 global-step:3789	 l-p:0.13425090909004211
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0203, 5.0203, 5.0203],
        [5.0203, 5.0203, 5.0203],
        [5.0203, 5.0203, 5.0203],
        [5.0203, 5.0346, 5.0225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.1334172785282135 
model_pd.l_d.mean(): -20.22040367126465 
model_pd.lagr.mean(): -20.086986541748047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4357], device='cuda:0')), ('power', tensor([-20.8864], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.1334172785282135
epoch£º190	 i:1 	 global-step:3801	 l-p:0.1392415165901184
epoch£º190	 i:2 	 global-step:3802	 l-p:0.13302527368068695
epoch£º190	 i:3 	 global-step:3803	 l-p:0.07738739997148514
epoch£º190	 i:4 	 global-step:3804	 l-p:0.24375514686107635
epoch£º190	 i:5 	 global-step:3805	 l-p:0.13900771737098694
epoch£º190	 i:6 	 global-step:3806	 l-p:0.20351718366146088
epoch£º190	 i:7 	 global-step:3807	 l-p:0.15213605761528015
epoch£º190	 i:8 	 global-step:3808	 l-p:0.15095311403274536
epoch£º190	 i:9 	 global-step:3809	 l-p:0.15823037922382355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6779, 5.5110, 5.9398],
        [4.6779, 4.7166, 4.6880],
        [4.6779, 4.6779, 4.6779],
        [4.6779, 4.6779, 4.6779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.1651991605758667 
model_pd.l_d.mean(): -18.84339714050293 
model_pd.lagr.mean(): -18.678197860717773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6520], device='cuda:0')), ('power', tensor([-19.7154], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.1651991605758667
epoch£º191	 i:1 	 global-step:3821	 l-p:0.20701178908348083
epoch£º191	 i:2 	 global-step:3822	 l-p:0.021259259432554245
epoch£º191	 i:3 	 global-step:3823	 l-p:0.1356019377708435
epoch£º191	 i:4 	 global-step:3824	 l-p:0.160842165350914
epoch£º191	 i:5 	 global-step:3825	 l-p:0.10942994058132172
epoch£º191	 i:6 	 global-step:3826	 l-p:0.12196099758148193
epoch£º191	 i:7 	 global-step:3827	 l-p:0.14225637912750244
epoch£º191	 i:8 	 global-step:3828	 l-p:0.15518520772457123
epoch£º191	 i:9 	 global-step:3829	 l-p:0.12967263162136078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9334, 4.9493, 4.9359],
        [4.9334, 4.9430, 4.9346],
        [4.9334, 5.7662, 6.1599],
        [4.9334, 4.9561, 4.9378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.19204145669937134 
model_pd.l_d.mean(): -20.202836990356445 
model_pd.lagr.mean(): -20.01079559326172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4746], device='cuda:0')), ('power', tensor([-20.9085], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.19204145669937134
epoch£º192	 i:1 	 global-step:3841	 l-p:0.14703191816806793
epoch£º192	 i:2 	 global-step:3842	 l-p:0.4419727921485901
epoch£º192	 i:3 	 global-step:3843	 l-p:0.12885116040706635
epoch£º192	 i:4 	 global-step:3844	 l-p:0.5390805006027222
epoch£º192	 i:5 	 global-step:3845	 l-p:0.1306798756122589
epoch£º192	 i:6 	 global-step:3846	 l-p:0.14513570070266724
epoch£º192	 i:7 	 global-step:3847	 l-p:0.15570713579654694
epoch£º192	 i:8 	 global-step:3848	 l-p:0.12739647924900055
epoch£º192	 i:9 	 global-step:3849	 l-p:0.13173797726631165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9372, 5.7056, 6.0369],
        [4.9372, 4.9514, 4.9393],
        [4.9372, 4.9388, 4.9373],
        [4.9372, 4.9373, 4.9372]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.13625158369541168 
model_pd.l_d.mean(): -20.545116424560547 
model_pd.lagr.mean(): -20.408864974975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4440], device='cuda:0')), ('power', tensor([-21.2232], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.13625158369541168
epoch£º193	 i:1 	 global-step:3861	 l-p:0.1379309445619583
epoch£º193	 i:2 	 global-step:3862	 l-p:0.05909763649106026
epoch£º193	 i:3 	 global-step:3863	 l-p:-0.08911127597093582
epoch£º193	 i:4 	 global-step:3864	 l-p:0.13292571902275085
epoch£º193	 i:5 	 global-step:3865	 l-p:0.1466543823480606
epoch£º193	 i:6 	 global-step:3866	 l-p:0.13397090137004852
epoch£º193	 i:7 	 global-step:3867	 l-p:0.1034758985042572
epoch£º193	 i:8 	 global-step:3868	 l-p:0.15018188953399658
epoch£º193	 i:9 	 global-step:3869	 l-p:0.11219386756420135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7177, 4.7177, 4.7177],
        [4.7177, 5.1899, 5.2911],
        [4.7177, 4.8765, 4.8254],
        [4.7177, 4.7415, 4.7220]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.09618446230888367 
model_pd.l_d.mean(): -20.923030853271484 
model_pd.lagr.mean(): -20.826847076416016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-21.6311], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.09618446230888367
epoch£º194	 i:1 	 global-step:3881	 l-p:0.14337682723999023
epoch£º194	 i:2 	 global-step:3882	 l-p:-0.017312288284301758
epoch£º194	 i:3 	 global-step:3883	 l-p:0.13345502316951752
epoch£º194	 i:4 	 global-step:3884	 l-p:0.13586148619651794
epoch£º194	 i:5 	 global-step:3885	 l-p:0.11681105941534042
epoch£º194	 i:6 	 global-step:3886	 l-p:0.12126366049051285
epoch£º194	 i:7 	 global-step:3887	 l-p:0.1419016569852829
epoch£º194	 i:8 	 global-step:3888	 l-p:0.13041456043720245
epoch£º194	 i:9 	 global-step:3889	 l-p:1.2948805093765259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1024, 5.1715, 5.1289],
        [5.1024, 5.6085, 5.7044],
        [5.1024, 5.1048, 5.1026],
        [5.1024, 5.5111, 5.5431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.11473122239112854 
model_pd.l_d.mean(): -20.30506134033203 
model_pd.lagr.mean(): -20.190330505371094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4129], device='cuda:0')), ('power', tensor([-20.9488], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.11473122239112854
epoch£º195	 i:1 	 global-step:3901	 l-p:0.11911234259605408
epoch£º195	 i:2 	 global-step:3902	 l-p:0.12006860226392746
epoch£º195	 i:3 	 global-step:3903	 l-p:0.12040446698665619
epoch£º195	 i:4 	 global-step:3904	 l-p:0.16077375411987305
epoch£º195	 i:5 	 global-step:3905	 l-p:0.13427498936653137
epoch£º195	 i:6 	 global-step:3906	 l-p:0.14203019440174103
epoch£º195	 i:7 	 global-step:3907	 l-p:0.2242790013551712
epoch£º195	 i:8 	 global-step:3908	 l-p:0.11449432373046875
epoch£º195	 i:9 	 global-step:3909	 l-p:0.10718674212694168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7942, 4.7942, 4.7942],
        [4.7942, 4.7985, 4.7945],
        [4.7942, 4.7943, 4.7942],
        [4.7942, 4.7972, 4.7944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.13742327690124512 
model_pd.l_d.mean(): -19.62436294555664 
model_pd.lagr.mean(): -19.486940383911133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5739], device='cuda:0')), ('power', tensor([-20.4252], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.13742327690124512
epoch£º196	 i:1 	 global-step:3921	 l-p:0.09885147213935852
epoch£º196	 i:2 	 global-step:3922	 l-p:0.08998271822929382
epoch£º196	 i:3 	 global-step:3923	 l-p:0.09523887187242508
epoch£º196	 i:4 	 global-step:3924	 l-p:0.13065694272518158
epoch£º196	 i:5 	 global-step:3925	 l-p:0.1557673215866089
epoch£º196	 i:6 	 global-step:3926	 l-p:0.12408997863531113
epoch£º196	 i:7 	 global-step:3927	 l-p:0.1265478879213333
epoch£º196	 i:8 	 global-step:3928	 l-p:0.13584551215171814
epoch£º196	 i:9 	 global-step:3929	 l-p:0.12398513406515121
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9352, 4.9355, 4.9352],
        [4.9352, 5.3408, 5.3815],
        [4.9352, 4.9352, 4.9352],
        [4.9352, 4.9481, 4.9369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.185348778963089 
model_pd.l_d.mean(): -19.068532943725586 
model_pd.lagr.mean(): -18.8831844329834 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5373], device='cuda:0')), ('power', tensor([-19.8258], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.185348778963089
epoch£º197	 i:1 	 global-step:3941	 l-p:0.1937805563211441
epoch£º197	 i:2 	 global-step:3942	 l-p:0.15182408690452576
epoch£º197	 i:3 	 global-step:3943	 l-p:0.11366184800863266
epoch£º197	 i:4 	 global-step:3944	 l-p:0.0007927942206151783
epoch£º197	 i:5 	 global-step:3945	 l-p:0.13425283133983612
epoch£º197	 i:6 	 global-step:3946	 l-p:0.1288825422525406
epoch£º197	 i:7 	 global-step:3947	 l-p:0.13322897255420685
epoch£º197	 i:8 	 global-step:3948	 l-p:0.1320980191230774
epoch£º197	 i:9 	 global-step:3949	 l-p:0.12072648853063583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9714, 4.9909, 4.9746],
        [4.9714, 4.9714, 4.9714],
        [4.9714, 4.9714, 4.9714],
        [4.9714, 4.9721, 4.9714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.12789568305015564 
model_pd.l_d.mean(): -20.52582359313965 
model_pd.lagr.mean(): -20.39792823791504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4282], device='cuda:0')), ('power', tensor([-21.1875], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.12789568305015564
epoch£º198	 i:1 	 global-step:3961	 l-p:0.1378096491098404
epoch£º198	 i:2 	 global-step:3962	 l-p:0.14969317615032196
epoch£º198	 i:3 	 global-step:3963	 l-p:0.1229160875082016
epoch£º198	 i:4 	 global-step:3964	 l-p:0.1088869571685791
epoch£º198	 i:5 	 global-step:3965	 l-p:0.12790751457214355
epoch£º198	 i:6 	 global-step:3966	 l-p:0.14037755131721497
epoch£º198	 i:7 	 global-step:3967	 l-p:0.2755989134311676
epoch£º198	 i:8 	 global-step:3968	 l-p:0.14133508503437042
epoch£º198	 i:9 	 global-step:3969	 l-p:0.12410082668066025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9916, 4.9923, 4.9916],
        [4.9916, 5.0016, 4.9927],
        [4.9916, 4.9916, 4.9916],
        [4.9916, 5.1046, 5.0508]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.16565673053264618 
model_pd.l_d.mean(): -19.046627044677734 
model_pd.lagr.mean(): -18.880970001220703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4943], device='cuda:0')), ('power', tensor([-19.7598], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.16565673053264618
epoch£º199	 i:1 	 global-step:3981	 l-p:0.1175159215927124
epoch£º199	 i:2 	 global-step:3982	 l-p:0.12080241739749908
epoch£º199	 i:3 	 global-step:3983	 l-p:0.12708142399787903
epoch£º199	 i:4 	 global-step:3984	 l-p:0.14025664329528809
epoch£º199	 i:5 	 global-step:3985	 l-p:0.17369893193244934
epoch£º199	 i:6 	 global-step:3986	 l-p:0.11406558752059937
epoch£º199	 i:7 	 global-step:3987	 l-p:0.1166718453168869
epoch£º199	 i:8 	 global-step:3988	 l-p:0.112652488052845
epoch£º199	 i:9 	 global-step:3989	 l-p:0.15627212822437286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0232, 5.0232, 5.0232],
        [5.0232, 5.0236, 5.0232],
        [5.0232, 5.0349, 5.0246],
        [5.0232, 5.3584, 5.3553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.14447875320911407 
model_pd.l_d.mean(): -20.52608299255371 
model_pd.lagr.mean(): -20.38160514831543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4162], device='cuda:0')), ('power', tensor([-21.1756], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.14447875320911407
epoch£º200	 i:1 	 global-step:4001	 l-p:0.13946650922298431
epoch£º200	 i:2 	 global-step:4002	 l-p:0.12588965892791748
epoch£º200	 i:3 	 global-step:4003	 l-p:0.19321997463703156
epoch£º200	 i:4 	 global-step:4004	 l-p:0.1326303631067276
epoch£º200	 i:5 	 global-step:4005	 l-p:0.24506284296512604
epoch£º200	 i:6 	 global-step:4006	 l-p:0.15810701251029968
epoch£º200	 i:7 	 global-step:4007	 l-p:0.15053190290927887
epoch£º200	 i:8 	 global-step:4008	 l-p:0.11471275240182877
epoch£º200	 i:9 	 global-step:4009	 l-p:0.7177740931510925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9198, 4.9198, 4.9198],
        [4.9198, 5.3880, 5.4722],
        [4.9198, 4.9198, 4.9198],
        [4.9198, 5.0090, 4.9597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): 0.36109650135040283 
model_pd.l_d.mean(): -18.556884765625 
model_pd.lagr.mean(): -18.19578742980957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5605], device='cuda:0')), ('power', tensor([-19.3323], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:0.36109650135040283
epoch£º201	 i:1 	 global-step:4021	 l-p:0.10966260731220245
epoch£º201	 i:2 	 global-step:4022	 l-p:0.08151764422655106
epoch£º201	 i:3 	 global-step:4023	 l-p:0.13063472509384155
epoch£º201	 i:4 	 global-step:4024	 l-p:0.14910711348056793
epoch£º201	 i:5 	 global-step:4025	 l-p:0.13450777530670166
epoch£º201	 i:6 	 global-step:4026	 l-p:0.1225690245628357
epoch£º201	 i:7 	 global-step:4027	 l-p:0.13518968224525452
epoch£º201	 i:8 	 global-step:4028	 l-p:0.1219390481710434
epoch£º201	 i:9 	 global-step:4029	 l-p:0.14950813353061676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0117, 5.0119, 5.0117],
        [5.0117, 5.0117, 5.0117],
        [5.0117, 5.0999, 5.0507],
        [5.0117, 5.0242, 5.0132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.13338984549045563 
model_pd.l_d.mean(): -20.060264587402344 
model_pd.lagr.mean(): -19.9268741607666 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-20.7404], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.13338984549045563
epoch£º202	 i:1 	 global-step:4041	 l-p:0.1493251919746399
epoch£º202	 i:2 	 global-step:4042	 l-p:0.1312796026468277
epoch£º202	 i:3 	 global-step:4043	 l-p:0.15026478469371796
epoch£º202	 i:4 	 global-step:4044	 l-p:0.19722692668437958
epoch£º202	 i:5 	 global-step:4045	 l-p:0.14595547318458557
epoch£º202	 i:6 	 global-step:4046	 l-p:0.132430300116539
epoch£º202	 i:7 	 global-step:4047	 l-p:0.1337028443813324
epoch£º202	 i:8 	 global-step:4048	 l-p:0.0385655015707016
epoch£º202	 i:9 	 global-step:4049	 l-p:0.2104766070842743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8243, 5.3476, 5.4806],
        [4.8243, 4.9077, 4.8599],
        [4.8243, 4.8753, 4.8393],
        [4.8243, 4.8244, 4.8243]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.06952986866235733 
model_pd.l_d.mean(): -18.9290714263916 
model_pd.lagr.mean(): -18.859540939331055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5446], device='cuda:0')), ('power', tensor([-19.6923], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.06952986866235733
epoch£º203	 i:1 	 global-step:4061	 l-p:0.10468412190675735
epoch£º203	 i:2 	 global-step:4062	 l-p:0.10911469906568527
epoch£º203	 i:3 	 global-step:4063	 l-p:0.13938333094120026
epoch£º203	 i:4 	 global-step:4064	 l-p:-0.014068622142076492
epoch£º203	 i:5 	 global-step:4065	 l-p:0.18799805641174316
epoch£º203	 i:6 	 global-step:4066	 l-p:0.13480006158351898
epoch£º203	 i:7 	 global-step:4067	 l-p:0.13843834400177002
epoch£º203	 i:8 	 global-step:4068	 l-p:0.12696392834186554
epoch£º203	 i:9 	 global-step:4069	 l-p:0.13477584719657898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9546, 5.4648, 5.5770],
        [4.9546, 4.9546, 4.9546],
        [4.9546, 5.4553, 5.5605],
        [4.9546, 5.0616, 5.0085]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.12281572818756104 
model_pd.l_d.mean(): -20.372314453125 
model_pd.lagr.mean(): -20.24949836730957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4627], device='cuda:0')), ('power', tensor([-21.0676], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.12281572818756104
epoch£º204	 i:1 	 global-step:4081	 l-p:0.1524205505847931
epoch£º204	 i:2 	 global-step:4082	 l-p:0.30754053592681885
epoch£º204	 i:3 	 global-step:4083	 l-p:0.31282398104667664
epoch£º204	 i:4 	 global-step:4084	 l-p:0.14396752417087555
epoch£º204	 i:5 	 global-step:4085	 l-p:0.1036062017083168
epoch£º204	 i:6 	 global-step:4086	 l-p:0.14277943968772888
epoch£º204	 i:7 	 global-step:4087	 l-p:0.14954832196235657
epoch£º204	 i:8 	 global-step:4088	 l-p:0.1427629292011261
epoch£º204	 i:9 	 global-step:4089	 l-p:0.13359569013118744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9503, 5.0170, 4.9744],
        [4.9503, 4.9503, 4.9503],
        [4.9503, 4.9504, 4.9503],
        [4.9503, 4.9503, 4.9503]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.18841245770454407 
model_pd.l_d.mean(): -19.691621780395508 
model_pd.lagr.mean(): -19.503210067749023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5059], device='cuda:0')), ('power', tensor([-20.4236], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.18841245770454407
epoch£º205	 i:1 	 global-step:4101	 l-p:0.13660107553005219
epoch£º205	 i:2 	 global-step:4102	 l-p:0.1250513643026352
epoch£º205	 i:3 	 global-step:4103	 l-p:0.15809719264507294
epoch£º205	 i:4 	 global-step:4104	 l-p:0.15348535776138306
epoch£º205	 i:5 	 global-step:4105	 l-p:0.07311227917671204
epoch£º205	 i:6 	 global-step:4106	 l-p:0.14254088699817657
epoch£º205	 i:7 	 global-step:4107	 l-p:0.17050385475158691
epoch£º205	 i:8 	 global-step:4108	 l-p:0.11225221306085587
epoch£º205	 i:9 	 global-step:4109	 l-p:0.12313743680715561
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0280, 5.0670, 5.0377],
        [5.0280, 5.9752, 6.4747],
        [5.0280, 5.5954, 5.7456],
        [5.0280, 5.0280, 5.0280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.13938173651695251 
model_pd.l_d.mean(): -20.223487854003906 
model_pd.lagr.mean(): -20.0841064453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4633], device='cuda:0')), ('power', tensor([-20.9178], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.13938173651695251
epoch£º206	 i:1 	 global-step:4121	 l-p:0.10292519629001617
epoch£º206	 i:2 	 global-step:4122	 l-p:0.12199008464813232
epoch£º206	 i:3 	 global-step:4123	 l-p:0.15110257267951965
epoch£º206	 i:4 	 global-step:4124	 l-p:0.11495359241962433
epoch£º206	 i:5 	 global-step:4125	 l-p:0.1677960753440857
epoch£º206	 i:6 	 global-step:4126	 l-p:0.13609153032302856
epoch£º206	 i:7 	 global-step:4127	 l-p:0.1134052649140358
epoch£º206	 i:8 	 global-step:4128	 l-p:0.17040710151195526
epoch£º206	 i:9 	 global-step:4129	 l-p:0.12894831597805023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9725, 4.9938, 4.9758],
        [4.9725, 4.9725, 4.9725],
        [4.9725, 5.2571, 5.2334],
        [4.9725, 5.1429, 5.0881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.13198556005954742 
model_pd.l_d.mean(): -19.841344833374023 
model_pd.lagr.mean(): -19.709360122680664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4589], device='cuda:0')), ('power', tensor([-20.5270], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.13198556005954742
epoch£º207	 i:1 	 global-step:4141	 l-p:0.12597011029720306
epoch£º207	 i:2 	 global-step:4142	 l-p:0.20228804647922516
epoch£º207	 i:3 	 global-step:4143	 l-p:0.16058526933193207
epoch£º207	 i:4 	 global-step:4144	 l-p:0.16470468044281006
epoch£º207	 i:5 	 global-step:4145	 l-p:0.1359066367149353
epoch£º207	 i:6 	 global-step:4146	 l-p:0.12252723425626755
epoch£º207	 i:7 	 global-step:4147	 l-p:0.5730238556861877
epoch£º207	 i:8 	 global-step:4148	 l-p:0.12738575041294098
epoch£º207	 i:9 	 global-step:4149	 l-p:0.13486598432064056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8914, 4.8915, 4.8914],
        [4.8914, 5.3771, 5.4765],
        [4.8914, 5.0205, 4.9652],
        [4.8914, 4.8914, 4.8914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.13262884318828583 
model_pd.l_d.mean(): -20.86733055114746 
model_pd.lagr.mean(): -20.73470115661621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4190], device='cuda:0')), ('power', tensor([-21.5234], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:0.13262884318828583
epoch£º208	 i:1 	 global-step:4161	 l-p:-0.153291255235672
epoch£º208	 i:2 	 global-step:4162	 l-p:0.12531091272830963
epoch£º208	 i:3 	 global-step:4163	 l-p:0.12909412384033203
epoch£º208	 i:4 	 global-step:4164	 l-p:0.13654474914073944
epoch£º208	 i:5 	 global-step:4165	 l-p:0.14292052388191223
epoch£º208	 i:6 	 global-step:4166	 l-p:0.18740932643413544
epoch£º208	 i:7 	 global-step:4167	 l-p:0.09755735844373703
epoch£º208	 i:8 	 global-step:4168	 l-p:0.21028171479701996
epoch£º208	 i:9 	 global-step:4169	 l-p:0.15456998348236084
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228]], device='cuda:0')
 pt:tensor([[4.8772, 5.2160, 5.2225],
        [4.8772, 5.8037, 6.3035],
        [4.8772, 4.9276, 4.8916],
        [4.8772, 5.1284, 5.0952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): -0.08305427432060242 
model_pd.l_d.mean(): -20.242319107055664 
model_pd.lagr.mean(): -20.32537269592285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5072], device='cuda:0')), ('power', tensor([-20.9817], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:-0.08305427432060242
epoch£º209	 i:1 	 global-step:4181	 l-p:0.13659189641475677
epoch£º209	 i:2 	 global-step:4182	 l-p:0.12858957052230835
epoch£º209	 i:3 	 global-step:4183	 l-p:0.14129209518432617
epoch£º209	 i:4 	 global-step:4184	 l-p:0.14280152320861816
epoch£º209	 i:5 	 global-step:4185	 l-p:0.10170146822929382
epoch£º209	 i:6 	 global-step:4186	 l-p:0.13482628762722015
epoch£º209	 i:7 	 global-step:4187	 l-p:0.20046530663967133
epoch£º209	 i:8 	 global-step:4188	 l-p:0.1568024754524231
epoch£º209	 i:9 	 global-step:4189	 l-p:0.1242862120270729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0026, 5.0026, 5.0026],
        [5.0026, 5.0026, 5.0026],
        [5.0026, 5.1574, 5.1009],
        [5.0026, 5.0026, 5.0026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.17077507078647614 
model_pd.l_d.mean(): -20.62519073486328 
model_pd.lagr.mean(): -20.454416275024414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4188], device='cuda:0')), ('power', tensor([-21.2784], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.17077507078647614
epoch£º210	 i:1 	 global-step:4201	 l-p:0.1386323869228363
epoch£º210	 i:2 	 global-step:4202	 l-p:0.14797694981098175
epoch£º210	 i:3 	 global-step:4203	 l-p:0.12521754205226898
epoch£º210	 i:4 	 global-step:4204	 l-p:0.13508287072181702
epoch£º210	 i:5 	 global-step:4205	 l-p:0.12279406934976578
epoch£º210	 i:6 	 global-step:4206	 l-p:0.11828532814979553
epoch£º210	 i:7 	 global-step:4207	 l-p:0.12348442524671555
epoch£º210	 i:8 	 global-step:4208	 l-p:-0.3308510482311249
epoch£º210	 i:9 	 global-step:4209	 l-p:0.12624751031398773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0662, 5.4720, 5.5063],
        [5.0662, 5.0663, 5.0662],
        [5.0662, 5.2085, 5.1511],
        [5.0662, 5.0663, 5.0662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.13839121162891388 
model_pd.l_d.mean(): -19.56390380859375 
model_pd.lagr.mean(): -19.425512313842773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4469], device='cuda:0')), ('power', tensor([-20.2343], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.13839121162891388
epoch£º211	 i:1 	 global-step:4221	 l-p:0.12709461152553558
epoch£º211	 i:2 	 global-step:4222	 l-p:0.1505214124917984
epoch£º211	 i:3 	 global-step:4223	 l-p:0.171807661652565
epoch£º211	 i:4 	 global-step:4224	 l-p:0.12507615983486176
epoch£º211	 i:5 	 global-step:4225	 l-p:0.15359382331371307
epoch£º211	 i:6 	 global-step:4226	 l-p:0.10608809441328049
epoch£º211	 i:7 	 global-step:4227	 l-p:0.11786404997110367
epoch£º211	 i:8 	 global-step:4228	 l-p:0.12420087307691574
epoch£º211	 i:9 	 global-step:4229	 l-p:0.13257530331611633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9751, 5.4480, 5.5324],
        [4.9751, 4.9752, 4.9751],
        [4.9751, 4.9751, 4.9751],
        [4.9751, 5.4175, 5.4810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.13759605586528778 
model_pd.l_d.mean(): -20.943103790283203 
model_pd.lagr.mean(): -20.80550765991211 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3865], device='cuda:0')), ('power', tensor([-21.5668], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.13759605586528778
epoch£º212	 i:1 	 global-step:4241	 l-p:0.11819575726985931
epoch£º212	 i:2 	 global-step:4242	 l-p:0.14406238496303558
epoch£º212	 i:3 	 global-step:4243	 l-p:0.1205708310008049
epoch£º212	 i:4 	 global-step:4244	 l-p:0.24620461463928223
epoch£º212	 i:5 	 global-step:4245	 l-p:0.15063662827014923
epoch£º212	 i:6 	 global-step:4246	 l-p:0.11993874609470367
epoch£º212	 i:7 	 global-step:4247	 l-p:0.1851404309272766
epoch£º212	 i:8 	 global-step:4248	 l-p:-1.0002039670944214
epoch£º212	 i:9 	 global-step:4249	 l-p:0.10819840431213379
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7771, 4.7776, 4.7771],
        [4.7771, 4.7771, 4.7771],
        [4.7771, 4.9229, 4.8684],
        [4.7771, 4.7839, 4.7772]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.12231416255235672 
model_pd.l_d.mean(): -19.526752471923828 
model_pd.lagr.mean(): -19.404438018798828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5788], device='cuda:0')), ('power', tensor([-20.3314], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.12231416255235672
epoch£º213	 i:1 	 global-step:4261	 l-p:0.13258588314056396
epoch£º213	 i:2 	 global-step:4262	 l-p:0.13917788863182068
epoch£º213	 i:3 	 global-step:4263	 l-p:0.13414984941482544
epoch£º213	 i:4 	 global-step:4264	 l-p:0.020560486242175102
epoch£º213	 i:5 	 global-step:4265	 l-p:0.14378012716770172
epoch£º213	 i:6 	 global-step:4266	 l-p:0.13463252782821655
epoch£º213	 i:7 	 global-step:4267	 l-p:0.11960110068321228
epoch£º213	 i:8 	 global-step:4268	 l-p:0.44867461919784546
epoch£º213	 i:9 	 global-step:4269	 l-p:0.09651419520378113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8272, 4.8272, 4.8272],
        [4.8272, 4.8527, 4.8307],
        [4.8272, 4.9692, 4.9139],
        [4.8272, 4.8272, 4.8272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.14025366306304932 
model_pd.l_d.mean(): -20.610639572143555 
model_pd.lagr.mean(): -20.470386505126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4677], device='cuda:0')), ('power', tensor([-21.3137], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.14025366306304932
epoch£º214	 i:1 	 global-step:4281	 l-p:0.12439718842506409
epoch£º214	 i:2 	 global-step:4282	 l-p:0.09174030274152756
epoch£º214	 i:3 	 global-step:4283	 l-p:0.18617026507854462
epoch£º214	 i:4 	 global-step:4284	 l-p:0.13782455027103424
epoch£º214	 i:5 	 global-step:4285	 l-p:0.3248004913330078
epoch£º214	 i:6 	 global-step:4286	 l-p:0.17262069880962372
epoch£º214	 i:7 	 global-step:4287	 l-p:0.4605756402015686
epoch£º214	 i:8 	 global-step:4288	 l-p:0.14248985052108765
epoch£º214	 i:9 	 global-step:4289	 l-p:0.18539221584796906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9984, 5.1568, 5.1003],
        [4.9984, 4.9987, 4.9984],
        [4.9984, 5.0104, 4.9995],
        [4.9984, 4.9985, 4.9984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.15978874266147614 
model_pd.l_d.mean(): -20.523405075073242 
model_pd.lagr.mean(): -20.363616943359375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4355], device='cuda:0')), ('power', tensor([-21.1925], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.15978874266147614
epoch£º215	 i:1 	 global-step:4301	 l-p:0.1299777626991272
epoch£º215	 i:2 	 global-step:4302	 l-p:0.14428208768367767
epoch£º215	 i:3 	 global-step:4303	 l-p:0.11818298697471619
epoch£º215	 i:4 	 global-step:4304	 l-p:4.026756763458252
epoch£º215	 i:5 	 global-step:4305	 l-p:0.1254991590976715
epoch£º215	 i:6 	 global-step:4306	 l-p:0.13928058743476868
epoch£º215	 i:7 	 global-step:4307	 l-p:0.11865135282278061
epoch£º215	 i:8 	 global-step:4308	 l-p:0.1283266246318817
epoch£º215	 i:9 	 global-step:4309	 l-p:0.11150585114955902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0823, 5.0830, 5.0823],
        [5.0823, 5.4354, 5.4393],
        [5.0823, 5.0988, 5.0844],
        [5.0823, 5.0823, 5.0823]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.11852335929870605 
model_pd.l_d.mean(): -19.117692947387695 
model_pd.lagr.mean(): -18.999170303344727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4602], device='cuda:0')), ('power', tensor([-19.7967], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.11852335929870605
epoch£º216	 i:1 	 global-step:4321	 l-p:0.11841575056314468
epoch£º216	 i:2 	 global-step:4322	 l-p:0.1572607159614563
epoch£º216	 i:3 	 global-step:4323	 l-p:0.15006956458091736
epoch£º216	 i:4 	 global-step:4324	 l-p:0.15144012868404388
epoch£º216	 i:5 	 global-step:4325	 l-p:0.18878348171710968
epoch£º216	 i:6 	 global-step:4326	 l-p:0.08566877990961075
epoch£º216	 i:7 	 global-step:4327	 l-p:0.12580445408821106
epoch£º216	 i:8 	 global-step:4328	 l-p:0.12295979261398315
epoch£º216	 i:9 	 global-step:4329	 l-p:0.15655115246772766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9541, 4.9626, 4.9545],
        [4.9541, 5.3954, 5.4601],
        [4.9541, 5.1409, 5.0883],
        [4.9541, 4.9541, 4.9541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.1342940479516983 
model_pd.l_d.mean(): -20.628311157226562 
model_pd.lagr.mean(): -20.494016647338867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4214], device='cuda:0')), ('power', tensor([-21.2842], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.1342940479516983
epoch£º217	 i:1 	 global-step:4341	 l-p:0.10390496253967285
epoch£º217	 i:2 	 global-step:4342	 l-p:0.248202845454216
epoch£º217	 i:3 	 global-step:4343	 l-p:0.1598535180091858
epoch£º217	 i:4 	 global-step:4344	 l-p:0.17267419397830963
epoch£º217	 i:5 	 global-step:4345	 l-p:0.12978962063789368
epoch£º217	 i:6 	 global-step:4346	 l-p:0.13334988057613373
epoch£º217	 i:7 	 global-step:4347	 l-p:0.28741928935050964
epoch£º217	 i:8 	 global-step:4348	 l-p:0.2138487547636032
epoch£º217	 i:9 	 global-step:4349	 l-p:0.13903671503067017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9595, 4.9619, 4.9596],
        [4.9595, 4.9595, 4.9595],
        [4.9595, 4.9596, 4.9595],
        [4.9595, 4.9595, 4.9595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.12264800816774368 
model_pd.l_d.mean(): -20.660396575927734 
model_pd.lagr.mean(): -20.537748336791992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4217], device='cuda:0')), ('power', tensor([-21.3169], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.12264800816774368
epoch£º218	 i:1 	 global-step:4361	 l-p:0.13353750109672546
epoch£º218	 i:2 	 global-step:4362	 l-p:0.13659319281578064
epoch£º218	 i:3 	 global-step:4363	 l-p:0.1773918867111206
epoch£º218	 i:4 	 global-step:4364	 l-p:0.12672856450080872
epoch£º218	 i:5 	 global-step:4365	 l-p:0.2568768560886383
epoch£º218	 i:6 	 global-step:4366	 l-p:0.6218377351760864
epoch£º218	 i:7 	 global-step:4367	 l-p:0.12597425282001495
epoch£º218	 i:8 	 global-step:4368	 l-p:0.1969779133796692
epoch£º218	 i:9 	 global-step:4369	 l-p:0.08987250924110413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0179, 5.0223, 5.0181],
        [5.0179, 5.0281, 5.0187],
        [5.0179, 5.1762, 5.1192],
        [5.0179, 6.0427, 6.6260]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.11964525282382965 
model_pd.l_d.mean(): -18.85618019104004 
model_pd.lagr.mean(): -18.736534118652344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5034], device='cuda:0')), ('power', tensor([-19.5765], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.11964525282382965
epoch£º219	 i:1 	 global-step:4381	 l-p:0.04287030175328255
epoch£º219	 i:2 	 global-step:4382	 l-p:0.1376504749059677
epoch£º219	 i:3 	 global-step:4383	 l-p:0.11595046520233154
epoch£º219	 i:4 	 global-step:4384	 l-p:0.15602771937847137
epoch£º219	 i:5 	 global-step:4385	 l-p:0.11611219495534897
epoch£º219	 i:6 	 global-step:4386	 l-p:0.14328093826770782
epoch£º219	 i:7 	 global-step:4387	 l-p:0.12767799198627472
epoch£º219	 i:8 	 global-step:4388	 l-p:0.11719220131635666
epoch£º219	 i:9 	 global-step:4389	 l-p:0.1301383227109909
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0530, 5.0532, 5.0530],
        [5.0530, 5.1161, 5.0739],
        [5.0530, 5.0530, 5.0530],
        [5.0530, 5.0650, 5.0540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.1456603854894638 
model_pd.l_d.mean(): -20.346113204956055 
model_pd.lagr.mean(): -20.20045280456543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4225], device='cuda:0')), ('power', tensor([-21.0001], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.1456603854894638
epoch£º220	 i:1 	 global-step:4401	 l-p:0.12677931785583496
epoch£º220	 i:2 	 global-step:4402	 l-p:0.10777471214532852
epoch£º220	 i:3 	 global-step:4403	 l-p:0.13951601088047028
epoch£º220	 i:4 	 global-step:4404	 l-p:0.14900149405002594
epoch£º220	 i:5 	 global-step:4405	 l-p:0.15215742588043213
epoch£º220	 i:6 	 global-step:4406	 l-p:0.05568862706422806
epoch£º220	 i:7 	 global-step:4407	 l-p:0.1417483240365982
epoch£º220	 i:8 	 global-step:4408	 l-p:0.15822821855545044
epoch£º220	 i:9 	 global-step:4409	 l-p:0.16701306402683258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7710, 5.1912, 5.2546],
        [4.7710, 5.0291, 5.0021],
        [4.7710, 4.8660, 4.8135],
        [4.7710, 4.7710, 4.7710]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.1612405776977539 
model_pd.l_d.mean(): -20.192171096801758 
model_pd.lagr.mean(): -20.030929565429688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5210], device='cuda:0')), ('power', tensor([-20.9451], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.1612405776977539
epoch£º221	 i:1 	 global-step:4421	 l-p:0.11815007776021957
epoch£º221	 i:2 	 global-step:4422	 l-p:0.14224259555339813
epoch£º221	 i:3 	 global-step:4423	 l-p:0.15498028695583344
epoch£º221	 i:4 	 global-step:4424	 l-p:0.07716042548418045
epoch£º221	 i:5 	 global-step:4425	 l-p:0.06170206516981125
epoch£º221	 i:6 	 global-step:4426	 l-p:0.12762340903282166
epoch£º221	 i:7 	 global-step:4427	 l-p:0.13835477828979492
epoch£º221	 i:8 	 global-step:4428	 l-p:0.17248454689979553
epoch£º221	 i:9 	 global-step:4429	 l-p:0.1336291879415512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228]], device='cuda:0')
 pt:tensor([[4.8986, 5.8886, 6.4529],
        [4.8986, 5.4195, 5.5461],
        [4.8986, 5.8448, 6.3632],
        [4.8986, 5.3769, 5.4718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.11896192282438278 
model_pd.l_d.mean(): -19.598194122314453 
model_pd.lagr.mean(): -19.479232788085938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5572], device='cuda:0')), ('power', tensor([-20.3816], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:0.11896192282438278
epoch£º222	 i:1 	 global-step:4441	 l-p:0.11686786264181137
epoch£º222	 i:2 	 global-step:4442	 l-p:0.14638501405715942
epoch£º222	 i:3 	 global-step:4443	 l-p:0.14744935929775238
epoch£º222	 i:4 	 global-step:4444	 l-p:0.128785640001297
epoch£º222	 i:5 	 global-step:4445	 l-p:0.1354299932718277
epoch£º222	 i:6 	 global-step:4446	 l-p:0.20720790326595306
epoch£º222	 i:7 	 global-step:4447	 l-p:0.09533455967903137
epoch£º222	 i:8 	 global-step:4448	 l-p:0.05854444205760956
epoch£º222	 i:9 	 global-step:4449	 l-p:-0.10622848570346832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8277, 4.8278, 4.8277],
        [4.8277, 4.9321, 4.8779],
        [4.8277, 5.0143, 4.9635],
        [4.8277, 4.8359, 4.8275]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): 0.25728151202201843 
model_pd.l_d.mean(): -20.742082595825195 
model_pd.lagr.mean(): -20.484800338745117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-21.4427], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:0.25728151202201843
epoch£º223	 i:1 	 global-step:4461	 l-p:0.1392461508512497
epoch£º223	 i:2 	 global-step:4462	 l-p:0.1332804411649704
epoch£º223	 i:3 	 global-step:4463	 l-p:0.11807332932949066
epoch£º223	 i:4 	 global-step:4464	 l-p:0.12109113484621048
epoch£º223	 i:5 	 global-step:4465	 l-p:0.19607912003993988
epoch£º223	 i:6 	 global-step:4466	 l-p:0.28310275077819824
epoch£º223	 i:7 	 global-step:4467	 l-p:0.2346348911523819
epoch£º223	 i:8 	 global-step:4468	 l-p:0.13591685891151428
epoch£º223	 i:9 	 global-step:4469	 l-p:0.1668996810913086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0357, 5.0357, 5.0357],
        [5.0357, 5.0357, 5.0357],
        [5.0357, 5.0659, 5.0410],
        [5.0357, 5.0364, 5.0357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.12211787700653076 
model_pd.l_d.mean(): -20.573740005493164 
model_pd.lagr.mean(): -20.451622009277344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3994], device='cuda:0')), ('power', tensor([-21.2066], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.12211787700653076
epoch£º224	 i:1 	 global-step:4481	 l-p:0.12359906733036041
epoch£º224	 i:2 	 global-step:4482	 l-p:0.13439181447029114
epoch£º224	 i:3 	 global-step:4483	 l-p:0.1468375325202942
epoch£º224	 i:4 	 global-step:4484	 l-p:0.14201360940933228
epoch£º224	 i:5 	 global-step:4485	 l-p:0.07547853887081146
epoch£º224	 i:6 	 global-step:4486	 l-p:0.15510572493076324
epoch£º224	 i:7 	 global-step:4487	 l-p:0.15428033471107483
epoch£º224	 i:8 	 global-step:4488	 l-p:0.10856921970844269
epoch£º224	 i:9 	 global-step:4489	 l-p:0.12164627760648727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0849, 5.0849, 5.0849],
        [5.0849, 5.0849, 5.0849],
        [5.0849, 5.0849, 5.0849],
        [5.0849, 5.2218, 5.1634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.1165757030248642 
model_pd.l_d.mean(): -19.738494873046875 
model_pd.lagr.mean(): -19.621919631958008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4400], device='cuda:0')), ('power', tensor([-20.4037], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.1165757030248642
epoch£º225	 i:1 	 global-step:4501	 l-p:0.013080375269055367
epoch£º225	 i:2 	 global-step:4502	 l-p:0.12361599504947662
epoch£º225	 i:3 	 global-step:4503	 l-p:0.17280207574367523
epoch£º225	 i:4 	 global-step:4504	 l-p:0.12017107009887695
epoch£º225	 i:5 	 global-step:4505	 l-p:0.17262284457683563
epoch£º225	 i:6 	 global-step:4506	 l-p:0.11893822997808456
epoch£º225	 i:7 	 global-step:4507	 l-p:0.1253843754529953
epoch£º225	 i:8 	 global-step:4508	 l-p:0.1652178317308426
epoch£º225	 i:9 	 global-step:4509	 l-p:0.12730877101421356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9764, 5.1360, 5.0790],
        [4.9764, 5.5084, 5.6376],
        [4.9764, 5.0413, 4.9975],
        [4.9764, 4.9764, 4.9764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.1249006986618042 
model_pd.l_d.mean(): -20.59423065185547 
model_pd.lagr.mean(): -20.469329833984375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4284], device='cuda:0')), ('power', tensor([-21.2569], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.1249006986618042
epoch£º226	 i:1 	 global-step:4521	 l-p:0.21627986431121826
epoch£º226	 i:2 	 global-step:4522	 l-p:0.09313496202230453
epoch£º226	 i:3 	 global-step:4523	 l-p:0.23868945240974426
epoch£º226	 i:4 	 global-step:4524	 l-p:0.14849160611629486
epoch£º226	 i:5 	 global-step:4525	 l-p:0.17835132777690887
epoch£º226	 i:6 	 global-step:4526	 l-p:0.12457561492919922
epoch£º226	 i:7 	 global-step:4527	 l-p:0.11364343017339706
epoch£º226	 i:8 	 global-step:4528	 l-p:0.14238962531089783
epoch£º226	 i:9 	 global-step:4529	 l-p:0.19365493953227997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9906, 5.1062, 5.0497],
        [4.9906, 4.9916, 4.9906],
        [4.9906, 5.0125, 4.9931],
        [4.9906, 4.9907, 4.9906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.14749853312969208 
model_pd.l_d.mean(): -19.997350692749023 
model_pd.lagr.mean(): -19.849851608276367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4550], device='cuda:0')), ('power', tensor([-20.6807], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.14749853312969208
epoch£º227	 i:1 	 global-step:4541	 l-p:0.11990837752819061
epoch£º227	 i:2 	 global-step:4542	 l-p:0.12921284139156342
epoch£º227	 i:3 	 global-step:4543	 l-p:0.16551010310649872
epoch£º227	 i:4 	 global-step:4544	 l-p:0.21121108531951904
epoch£º227	 i:5 	 global-step:4545	 l-p:0.2044898122549057
epoch£º227	 i:6 	 global-step:4546	 l-p:0.1258804202079773
epoch£º227	 i:7 	 global-step:4547	 l-p:0.12425117194652557
epoch£º227	 i:8 	 global-step:4548	 l-p:0.12691958248615265
epoch£º227	 i:9 	 global-step:4549	 l-p:0.17027947306632996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0182, 5.0182, 5.0182],
        [5.0182, 5.0182, 5.0182],
        [5.0182, 5.0183, 5.0182],
        [5.0182, 5.0481, 5.0229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.131675586104393 
model_pd.l_d.mean(): -20.522876739501953 
model_pd.lagr.mean(): -20.39120101928711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4281], device='cuda:0')), ('power', tensor([-21.1845], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.131675586104393
epoch£º228	 i:1 	 global-step:4561	 l-p:0.12893587350845337
epoch£º228	 i:2 	 global-step:4562	 l-p:0.13833051919937134
epoch£º228	 i:3 	 global-step:4563	 l-p:0.1295633167028427
epoch£º228	 i:4 	 global-step:4564	 l-p:0.09395885467529297
epoch£º228	 i:5 	 global-step:4565	 l-p:0.19356141984462738
epoch£º228	 i:6 	 global-step:4566	 l-p:0.13425187766551971
epoch£º228	 i:7 	 global-step:4567	 l-p:0.1420917809009552
epoch£º228	 i:8 	 global-step:4568	 l-p:0.17280349135398865
epoch£º228	 i:9 	 global-step:4569	 l-p:0.17880946397781372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9766, 4.9767, 4.9766],
        [4.9766, 5.5341, 5.6834],
        [4.9766, 4.9894, 4.9771],
        [4.9766, 4.9847, 4.9767]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.13712549209594727 
model_pd.l_d.mean(): -18.6424503326416 
model_pd.lagr.mean(): -18.505325317382812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5444], device='cuda:0')), ('power', tensor([-19.4024], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.13712549209594727
epoch£º229	 i:1 	 global-step:4581	 l-p:0.1404632329940796
epoch£º229	 i:2 	 global-step:4582	 l-p:0.15950006246566772
epoch£º229	 i:3 	 global-step:4583	 l-p:0.17146143317222595
epoch£º229	 i:4 	 global-step:4584	 l-p:0.11762474477291107
epoch£º229	 i:5 	 global-step:4585	 l-p:0.13368217647075653
epoch£º229	 i:6 	 global-step:4586	 l-p:-0.06011299043893814
epoch£º229	 i:7 	 global-step:4587	 l-p:0.12178540974855423
epoch£º229	 i:8 	 global-step:4588	 l-p:0.11237634718418121
epoch£º229	 i:9 	 global-step:4589	 l-p:0.12861548364162445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1466,  0.0773,  1.0000,  0.0408,
          1.0000,  0.5273, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2503,  1.0000,  0.1770,
          1.0000,  0.7073, 31.6228]], device='cuda:0')
 pt:tensor([[5.0868, 5.1082, 5.0893],
        [5.0868, 6.0698, 6.6006],
        [5.0868, 5.1167, 5.0917],
        [5.0868, 5.2927, 5.2404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.1075892224907875 
model_pd.l_d.mean(): -19.039527893066406 
model_pd.lagr.mean(): -18.93193817138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4919], device='cuda:0')), ('power', tensor([-19.7501], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.1075892224907875
epoch£º230	 i:1 	 global-step:4601	 l-p:0.13219738006591797
epoch£º230	 i:2 	 global-step:4602	 l-p:0.12211848795413971
epoch£º230	 i:3 	 global-step:4603	 l-p:0.15398041903972626
epoch£º230	 i:4 	 global-step:4604	 l-p:0.16988757252693176
epoch£º230	 i:5 	 global-step:4605	 l-p:0.13059623539447784
epoch£º230	 i:6 	 global-step:4606	 l-p:0.23630791902542114
epoch£º230	 i:7 	 global-step:4607	 l-p:0.15237034857273102
epoch£º230	 i:8 	 global-step:4608	 l-p:0.14110486209392548
epoch£º230	 i:9 	 global-step:4609	 l-p:-0.10142367333173752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8814, 4.8814, 4.8814],
        [4.8814, 4.8814, 4.8814],
        [4.8814, 5.7612, 6.2150],
        [4.8814, 4.9288, 4.8918]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.12914983928203583 
model_pd.l_d.mean(): -20.139930725097656 
model_pd.lagr.mean(): -20.010780334472656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5093], device='cuda:0')), ('power', tensor([-20.8803], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.12914983928203583
epoch£º231	 i:1 	 global-step:4621	 l-p:0.14236943423748016
epoch£º231	 i:2 	 global-step:4622	 l-p:0.12565454840660095
epoch£º231	 i:3 	 global-step:4623	 l-p:0.13442739844322205
epoch£º231	 i:4 	 global-step:4624	 l-p:0.13840994238853455
epoch£º231	 i:5 	 global-step:4625	 l-p:0.27231916785240173
epoch£º231	 i:6 	 global-step:4626	 l-p:0.13639792799949646
epoch£º231	 i:7 	 global-step:4627	 l-p:0.061681851744651794
epoch£º231	 i:8 	 global-step:4628	 l-p:0.04674813151359558
epoch£º231	 i:9 	 global-step:4629	 l-p:0.1691403090953827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7763, 4.7763, 4.7763],
        [4.7763, 4.7983, 4.7768],
        [4.7763, 5.1426, 5.1713],
        [4.7763, 4.7769, 4.7761]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.12486237287521362 
model_pd.l_d.mean(): -20.680498123168945 
model_pd.lagr.mean(): -20.555635452270508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4879], device='cuda:0')), ('power', tensor([-21.4049], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.12486237287521362
epoch£º232	 i:1 	 global-step:4641	 l-p:0.18944579362869263
epoch£º232	 i:2 	 global-step:4642	 l-p:0.05647512897849083
epoch£º232	 i:3 	 global-step:4643	 l-p:0.14071209728717804
epoch£º232	 i:4 	 global-step:4644	 l-p:0.14081014692783356
epoch£º232	 i:5 	 global-step:4645	 l-p:0.13375318050384521
epoch£º232	 i:6 	 global-step:4646	 l-p:0.14836174249649048
epoch£º232	 i:7 	 global-step:4647	 l-p:0.12391425669193268
epoch£º232	 i:8 	 global-step:4648	 l-p:0.11937005072832108
epoch£º232	 i:9 	 global-step:4649	 l-p:0.12137876451015472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0523, 5.0947, 5.0614],
        [5.0523, 5.0705, 5.0538],
        [5.0523, 5.3567, 5.3387],
        [5.0523, 5.8851, 6.2702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.08481398224830627 
model_pd.l_d.mean(): -19.999101638793945 
model_pd.lagr.mean(): -19.914287567138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4777], device='cuda:0')), ('power', tensor([-20.7057], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.08481398224830627
epoch£º233	 i:1 	 global-step:4661	 l-p:0.13709694147109985
epoch£º233	 i:2 	 global-step:4662	 l-p:0.11049526184797287
epoch£º233	 i:3 	 global-step:4663	 l-p:0.13289403915405273
epoch£º233	 i:4 	 global-step:4664	 l-p:0.1381179392337799
epoch£º233	 i:5 	 global-step:4665	 l-p:0.15475013852119446
epoch£º233	 i:6 	 global-step:4666	 l-p:0.1476781815290451
epoch£º233	 i:7 	 global-step:4667	 l-p:0.1353388875722885
epoch£º233	 i:8 	 global-step:4668	 l-p:0.12057843804359436
epoch£º233	 i:9 	 global-step:4669	 l-p:0.12988705933094025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9684, 4.9703, 4.9683],
        [4.9684, 4.9763, 4.9682],
        [4.9684, 4.9815, 4.9686],
        [4.9684, 4.9684, 4.9684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.1231614351272583 
model_pd.l_d.mean(): -20.02397918701172 
model_pd.lagr.mean(): -19.90081787109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4938], device='cuda:0')), ('power', tensor([-20.7472], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.1231614351272583
epoch£º234	 i:1 	 global-step:4681	 l-p:0.54082190990448
epoch£º234	 i:2 	 global-step:4682	 l-p:0.12477060407400131
epoch£º234	 i:3 	 global-step:4683	 l-p:0.1290034055709839
epoch£º234	 i:4 	 global-step:4684	 l-p:0.1984225958585739
epoch£º234	 i:5 	 global-step:4685	 l-p:0.03697928786277771
epoch£º234	 i:6 	 global-step:4686	 l-p:0.1304442137479782
epoch£º234	 i:7 	 global-step:4687	 l-p:0.14490234851837158
epoch£º234	 i:8 	 global-step:4688	 l-p:0.13985227048397064
epoch£º234	 i:9 	 global-step:4689	 l-p:0.07263685017824173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7858, 5.4234, 5.6598],
        [4.7858, 4.7858, 4.7858],
        [4.7858, 4.7870, 4.7854],
        [4.7858, 4.7858, 4.7858]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.1379036158323288 
model_pd.l_d.mean(): -20.549694061279297 
model_pd.lagr.mean(): -20.41179084777832 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4814], device='cuda:0')), ('power', tensor([-21.2661], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.1379036158323288
epoch£º235	 i:1 	 global-step:4701	 l-p:-0.3212527632713318
epoch£º235	 i:2 	 global-step:4702	 l-p:0.13479511439800262
epoch£º235	 i:3 	 global-step:4703	 l-p:0.04911286383867264
epoch£º235	 i:4 	 global-step:4704	 l-p:0.16355395317077637
epoch£º235	 i:5 	 global-step:4705	 l-p:0.17330169677734375
epoch£º235	 i:6 	 global-step:4706	 l-p:0.13064590096473694
epoch£º235	 i:7 	 global-step:4707	 l-p:0.14047281444072723
epoch£º235	 i:8 	 global-step:4708	 l-p:0.123563252389431
epoch£º235	 i:9 	 global-step:4709	 l-p:0.12258070707321167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8974, 5.1054, 5.0573],
        [4.8974, 4.8974, 4.8974],
        [4.8974, 5.0316, 4.9734],
        [4.8974, 4.9192, 4.8983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.13090664148330688 
model_pd.l_d.mean(): -20.494335174560547 
model_pd.lagr.mean(): -20.363428115844727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4624], device='cuda:0')), ('power', tensor([-21.1906], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.13090664148330688
epoch£º236	 i:1 	 global-step:4721	 l-p:0.13499024510383606
epoch£º236	 i:2 	 global-step:4722	 l-p:0.1349843144416809
epoch£º236	 i:3 	 global-step:4723	 l-p:0.13846294581890106
epoch£º236	 i:4 	 global-step:4724	 l-p:0.33567455410957336
epoch£º236	 i:5 	 global-step:4725	 l-p:0.11200135946273804
epoch£º236	 i:6 	 global-step:4726	 l-p:0.14923697710037231
epoch£º236	 i:7 	 global-step:4727	 l-p:0.1148359552025795
epoch£º236	 i:8 	 global-step:4728	 l-p:0.15211232006549835
epoch£º236	 i:9 	 global-step:4729	 l-p:0.12765228748321533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8244, 5.5559, 5.8734],
        [4.8244, 4.9017, 4.8511],
        [4.8244, 4.8244, 4.8244],
        [4.8244, 4.9506, 4.8927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.05066172406077385 
model_pd.l_d.mean(): -19.581411361694336 
model_pd.lagr.mean(): -19.530750274658203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5423], device='cuda:0')), ('power', tensor([-20.3494], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.05066172406077385
epoch£º237	 i:1 	 global-step:4741	 l-p:0.13289140164852142
epoch£º237	 i:2 	 global-step:4742	 l-p:-0.0378880649805069
epoch£º237	 i:3 	 global-step:4743	 l-p:0.14687718451023102
epoch£º237	 i:4 	 global-step:4744	 l-p:0.1275448054075241
epoch£º237	 i:5 	 global-step:4745	 l-p:0.18749506771564484
epoch£º237	 i:6 	 global-step:4746	 l-p:0.1239243894815445
epoch£º237	 i:7 	 global-step:4747	 l-p:0.1450672447681427
epoch£º237	 i:8 	 global-step:4748	 l-p:0.13124240934848785
epoch£º237	 i:9 	 global-step:4749	 l-p:0.1202186718583107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9808, 5.8701, 6.3206],
        [4.9808, 5.0298, 4.9917],
        [4.9808, 5.4909, 5.6044],
        [4.9808, 4.9808, 4.9808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.13201972842216492 
model_pd.l_d.mean(): -17.288978576660156 
model_pd.lagr.mean(): -17.156959533691406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6337], device='cuda:0')), ('power', tensor([-18.1253], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.13201972842216492
epoch£º238	 i:1 	 global-step:4761	 l-p:0.19710154831409454
epoch£º238	 i:2 	 global-step:4762	 l-p:0.12694190442562103
epoch£º238	 i:3 	 global-step:4763	 l-p:0.133139967918396
epoch£º238	 i:4 	 global-step:4764	 l-p:0.15770968794822693
epoch£º238	 i:5 	 global-step:4765	 l-p:0.19937366247177124
epoch£º238	 i:6 	 global-step:4766	 l-p:0.09317081421613693
epoch£º238	 i:7 	 global-step:4767	 l-p:0.18404872715473175
epoch£º238	 i:8 	 global-step:4768	 l-p:0.12464641034603119
epoch£º238	 i:9 	 global-step:4769	 l-p:0.12719707190990448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[4.9628, 5.4570, 5.5601],
        [4.9628, 5.7587, 6.1201],
        [4.9628, 5.0487, 4.9961],
        [4.9628, 5.3190, 5.3318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.15832847356796265 
model_pd.l_d.mean(): -20.348207473754883 
model_pd.lagr.mean(): -20.189878463745117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4709], device='cuda:0')), ('power', tensor([-21.0516], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.15832847356796265
epoch£º239	 i:1 	 global-step:4781	 l-p:0.10930158197879791
epoch£º239	 i:2 	 global-step:4782	 l-p:0.12825092673301697
epoch£º239	 i:3 	 global-step:4783	 l-p:0.13615840673446655
epoch£º239	 i:4 	 global-step:4784	 l-p:-0.10023529082536697
epoch£º239	 i:5 	 global-step:4785	 l-p:0.1320524960756302
epoch£º239	 i:6 	 global-step:4786	 l-p:0.1219778060913086
epoch£º239	 i:7 	 global-step:4787	 l-p:0.157925084233284
epoch£º239	 i:8 	 global-step:4788	 l-p:0.11743344366550446
epoch£º239	 i:9 	 global-step:4789	 l-p:0.10356444865465164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7702, 5.0885, 5.0902],
        [4.7702, 4.8019, 4.7718],
        [4.7702, 4.8309, 4.7852],
        [4.7702, 4.7701, 4.7702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): -0.0438927561044693 
model_pd.l_d.mean(): -20.714401245117188 
model_pd.lagr.mean(): -20.75829315185547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4704], device='cuda:0')), ('power', tensor([-21.4213], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:-0.0438927561044693
epoch£º240	 i:1 	 global-step:4801	 l-p:0.16233153641223907
epoch£º240	 i:2 	 global-step:4802	 l-p:0.14044134318828583
epoch£º240	 i:3 	 global-step:4803	 l-p:0.06483198702335358
epoch£º240	 i:4 	 global-step:4804	 l-p:0.130329430103302
epoch£º240	 i:5 	 global-step:4805	 l-p:0.12709511816501617
epoch£º240	 i:6 	 global-step:4806	 l-p:0.14522519707679749
epoch£º240	 i:7 	 global-step:4807	 l-p:0.15288910269737244
epoch£º240	 i:8 	 global-step:4808	 l-p:0.2011794000864029
epoch£º240	 i:9 	 global-step:4809	 l-p:0.00805892888456583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9236, 4.9732, 4.9339],
        [4.9236, 4.9236, 4.9236],
        [4.9236, 5.4266, 5.5395],
        [4.9236, 5.6785, 6.0061]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.34426936507225037 
model_pd.l_d.mean(): -20.44830322265625 
model_pd.lagr.mean(): -20.104034423828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4604], device='cuda:0')), ('power', tensor([-21.1421], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.34426936507225037
epoch£º241	 i:1 	 global-step:4821	 l-p:0.35754311084747314
epoch£º241	 i:2 	 global-step:4822	 l-p:0.11997589468955994
epoch£º241	 i:3 	 global-step:4823	 l-p:0.15541011095046997
epoch£º241	 i:4 	 global-step:4824	 l-p:0.13406571745872498
epoch£º241	 i:5 	 global-step:4825	 l-p:0.12761825323104858
epoch£º241	 i:6 	 global-step:4826	 l-p:0.11834129691123962
epoch£º241	 i:7 	 global-step:4827	 l-p:0.11930776387453079
epoch£º241	 i:8 	 global-step:4828	 l-p:0.13367727398872375
epoch£º241	 i:9 	 global-step:4829	 l-p:0.07092786580324173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0557, 5.3535, 5.3325],
        [5.0557, 5.0731, 5.0564],
        [5.0557, 5.0557, 5.0557],
        [5.0557, 5.7604, 6.0273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.1404300183057785 
model_pd.l_d.mean(): -20.276094436645508 
model_pd.lagr.mean(): -20.135663986206055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4390], device='cuda:0')), ('power', tensor([-20.9462], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.1404300183057785
epoch£º242	 i:1 	 global-step:4841	 l-p:0.12477439641952515
epoch£º242	 i:2 	 global-step:4842	 l-p:0.15747177600860596
epoch£º242	 i:3 	 global-step:4843	 l-p:0.12659120559692383
epoch£º242	 i:4 	 global-step:4844	 l-p:0.14922791719436646
epoch£º242	 i:5 	 global-step:4845	 l-p:0.1324806809425354
epoch£º242	 i:6 	 global-step:4846	 l-p:0.09521014988422394
epoch£º242	 i:7 	 global-step:4847	 l-p:0.13411493599414825
epoch£º242	 i:8 	 global-step:4848	 l-p:0.1246308982372284
epoch£º242	 i:9 	 global-step:4849	 l-p:0.2674463093280792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9689, 4.9941, 4.9703],
        [4.9689, 4.9689, 4.9689],
        [4.9689, 4.9689, 4.9689],
        [4.9689, 5.5872, 5.7882]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.15440936386585236 
model_pd.l_d.mean(): -20.061735153198242 
model_pd.lagr.mean(): -19.907325744628906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4543], device='cuda:0')), ('power', tensor([-20.7451], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.15440936386585236
epoch£º243	 i:1 	 global-step:4861	 l-p:0.349812388420105
epoch£º243	 i:2 	 global-step:4862	 l-p:0.12944650650024414
epoch£º243	 i:3 	 global-step:4863	 l-p:0.13985855877399445
epoch£º243	 i:4 	 global-step:4864	 l-p:0.12119216471910477
epoch£º243	 i:5 	 global-step:4865	 l-p:0.11844223737716675
epoch£º243	 i:6 	 global-step:4866	 l-p:0.19584275782108307
epoch£º243	 i:7 	 global-step:4867	 l-p:-0.09636498987674713
epoch£º243	 i:8 	 global-step:4868	 l-p:0.1314084678888321
epoch£º243	 i:9 	 global-step:4869	 l-p:0.13165795803070068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1771,  0.0994,  1.0000,  0.0558,
          1.0000,  0.5615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[4.8766, 4.8888, 4.8751],
        [4.8766, 5.1567, 5.1348],
        [4.8766, 4.9078, 4.8785],
        [4.8766, 5.4180, 5.5648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.055152930319309235 
model_pd.l_d.mean(): -20.49510383605957 
model_pd.lagr.mean(): -20.439950942993164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4775], device='cuda:0')), ('power', tensor([-21.2069], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.055152930319309235
epoch£º244	 i:1 	 global-step:4881	 l-p:0.1394796222448349
epoch£º244	 i:2 	 global-step:4882	 l-p:0.12288767844438553
epoch£º244	 i:3 	 global-step:4883	 l-p:0.12926799058914185
epoch£º244	 i:4 	 global-step:4884	 l-p:0.12551340460777283
epoch£º244	 i:5 	 global-step:4885	 l-p:-0.06133380904793739
epoch£º244	 i:6 	 global-step:4886	 l-p:0.1350957602262497
epoch£º244	 i:7 	 global-step:4887	 l-p:0.2219034880399704
epoch£º244	 i:8 	 global-step:4888	 l-p:0.08168087154626846
epoch£º244	 i:9 	 global-step:4889	 l-p:0.15735897421836853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8648, 4.8648, 4.8648],
        [4.8648, 5.0081, 4.9492],
        [4.8648, 5.1495, 5.1301],
        [4.8648, 4.8648, 4.8648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.08363240957260132 
model_pd.l_d.mean(): -20.004287719726562 
model_pd.lagr.mean(): -19.920656204223633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5282], device='cuda:0')), ('power', tensor([-20.7625], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.08363240957260132
epoch£º245	 i:1 	 global-step:4901	 l-p:0.21294260025024414
epoch£º245	 i:2 	 global-step:4902	 l-p:-0.3057277202606201
epoch£º245	 i:3 	 global-step:4903	 l-p:0.14886516332626343
epoch£º245	 i:4 	 global-step:4904	 l-p:0.11590760201215744
epoch£º245	 i:5 	 global-step:4905	 l-p:0.13196061551570892
epoch£º245	 i:6 	 global-step:4906	 l-p:0.08010190725326538
epoch£º245	 i:7 	 global-step:4907	 l-p:0.1683882176876068
epoch£º245	 i:8 	 global-step:4908	 l-p:0.12428184598684311
epoch£º245	 i:9 	 global-step:4909	 l-p:0.1234099268913269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1325, 5.6221, 5.7100],
        [5.1325, 5.1326, 5.1325],
        [5.1325, 5.1848, 5.1450],
        [5.1325, 5.1325, 5.1325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.12045295536518097 
model_pd.l_d.mean(): -19.456945419311523 
model_pd.lagr.mean(): -19.33649253845215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4712], device='cuda:0')), ('power', tensor([-20.1510], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.12045295536518097
epoch£º246	 i:1 	 global-step:4921	 l-p:0.12988345324993134
epoch£º246	 i:2 	 global-step:4922	 l-p:0.12733420729637146
epoch£º246	 i:3 	 global-step:4923	 l-p:0.136764794588089
epoch£º246	 i:4 	 global-step:4924	 l-p:0.13089287281036377
epoch£º246	 i:5 	 global-step:4925	 l-p:0.12898294627666473
epoch£º246	 i:6 	 global-step:4926	 l-p:0.027833737432956696
epoch£º246	 i:7 	 global-step:4927	 l-p:0.13892529904842377
epoch£º246	 i:8 	 global-step:4928	 l-p:0.15404339134693146
epoch£º246	 i:9 	 global-step:4929	 l-p:0.12092169374227524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0493, 5.0493, 5.0493],
        [5.0493, 5.0572, 5.0487],
        [5.0493, 5.1392, 5.0847],
        [5.0493, 5.0493, 5.0493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.075923390686512 
model_pd.l_d.mean(): -19.953643798828125 
model_pd.lagr.mean(): -19.87771987915039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4412], device='cuda:0')), ('power', tensor([-20.6225], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.075923390686512
epoch£º247	 i:1 	 global-step:4941	 l-p:0.1349520981311798
epoch£º247	 i:2 	 global-step:4942	 l-p:0.1699797809123993
epoch£º247	 i:3 	 global-step:4943	 l-p:0.13954244554042816
epoch£º247	 i:4 	 global-step:4944	 l-p:0.14202867448329926
epoch£º247	 i:5 	 global-step:4945	 l-p:0.1928263008594513
epoch£º247	 i:6 	 global-step:4946	 l-p:0.12907198071479797
epoch£º247	 i:7 	 global-step:4947	 l-p:0.13003815710544586
epoch£º247	 i:8 	 global-step:4948	 l-p:0.11993007361888885
epoch£º247	 i:9 	 global-step:4949	 l-p:0.11529003083705902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0460, 5.0460, 5.0460],
        [5.0460, 5.0460, 5.0460],
        [5.0460, 5.0530, 5.0452],
        [5.0460, 5.0460, 5.0460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.10971389710903168 
model_pd.l_d.mean(): -20.614484786987305 
model_pd.lagr.mean(): -20.504770278930664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4132], device='cuda:0')), ('power', tensor([-21.2619], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.10971389710903168
epoch£º248	 i:1 	 global-step:4961	 l-p:0.14007513225078583
epoch£º248	 i:2 	 global-step:4962	 l-p:0.12368741631507874
epoch£º248	 i:3 	 global-step:4963	 l-p:0.17741893231868744
epoch£º248	 i:4 	 global-step:4964	 l-p:0.12761640548706055
epoch£º248	 i:5 	 global-step:4965	 l-p:0.13680201768875122
epoch£º248	 i:6 	 global-step:4966	 l-p:0.13287799060344696
epoch£º248	 i:7 	 global-step:4967	 l-p:0.14309780299663544
epoch£º248	 i:8 	 global-step:4968	 l-p:0.12229703366756439
epoch£º248	 i:9 	 global-step:4969	 l-p:-0.15961676836013794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2392,  0.1485,  1.0000,  0.0922,
          1.0000,  0.6208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228]], device='cuda:0')
 pt:tensor([[4.9006, 5.2959, 5.3360],
        [4.9006, 5.3443, 5.4166],
        [4.9006, 4.9669, 4.9184],
        [4.9006, 5.6388, 5.9544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): 0.006001629866659641 
model_pd.l_d.mean(): -20.677852630615234 
model_pd.lagr.mean(): -20.671850204467773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4509], device='cuda:0')), ('power', tensor([-21.3645], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:0.006001629866659641
epoch£º249	 i:1 	 global-step:4981	 l-p:0.15245769917964935
epoch£º249	 i:2 	 global-step:4982	 l-p:0.12303941696882248
epoch£º249	 i:3 	 global-step:4983	 l-p:0.1817607879638672
epoch£º249	 i:4 	 global-step:4984	 l-p:0.21216191351413727
epoch£º249	 i:5 	 global-step:4985	 l-p:0.13354401290416718
epoch£º249	 i:6 	 global-step:4986	 l-p:0.10747081786394119
epoch£º249	 i:7 	 global-step:4987	 l-p:0.12473484128713608
epoch£º249	 i:8 	 global-step:4988	 l-p:0.15053877234458923
epoch£º249	 i:9 	 global-step:4989	 l-p:0.49421554803848267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9395, 5.1167, 5.0600],
        [4.9395, 4.9395, 4.9395],
        [4.9395, 4.9498, 4.9378],
        [4.9395, 4.9395, 4.9395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.16068284213542938 
model_pd.l_d.mean(): -19.52195167541504 
model_pd.lagr.mean(): -19.361268997192383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5170], device='cuda:0')), ('power', tensor([-20.2635], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.16068284213542938
epoch£º250	 i:1 	 global-step:5001	 l-p:0.16915440559387207
epoch£º250	 i:2 	 global-step:5002	 l-p:0.1279444694519043
epoch£º250	 i:3 	 global-step:5003	 l-p:0.14026522636413574
epoch£º250	 i:4 	 global-step:5004	 l-p:0.1282796710729599
epoch£º250	 i:5 	 global-step:5005	 l-p:0.12652765214443207
epoch£º250	 i:6 	 global-step:5006	 l-p:0.14376935362815857
epoch£º250	 i:7 	 global-step:5007	 l-p:0.12437155097723007
epoch£º250	 i:8 	 global-step:5008	 l-p:0.16293388605117798
epoch£º250	 i:9 	 global-step:5009	 l-p:0.0877242460846901
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0521, 5.2101, 5.1496],
        [5.0521, 5.0521, 5.0521],
        [5.0521, 5.1302, 5.0783],
        [5.0521, 5.0521, 5.0521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.18398013710975647 
model_pd.l_d.mean(): -20.156898498535156 
model_pd.lagr.mean(): -19.972917556762695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4571], device='cuda:0')), ('power', tensor([-20.8441], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.18398013710975647
epoch£º251	 i:1 	 global-step:5021	 l-p:0.13463659584522247
epoch£º251	 i:2 	 global-step:5022	 l-p:0.12298333644866943
epoch£º251	 i:3 	 global-step:5023	 l-p:0.11959298700094223
epoch£º251	 i:4 	 global-step:5024	 l-p:0.14219459891319275
epoch£º251	 i:5 	 global-step:5025	 l-p:0.11592619866132736
epoch£º251	 i:6 	 global-step:5026	 l-p:0.07897230982780457
epoch£º251	 i:7 	 global-step:5027	 l-p:0.12587814033031464
epoch£º251	 i:8 	 global-step:5028	 l-p:0.12721996009349823
epoch£º251	 i:9 	 global-step:5029	 l-p:0.13055290281772614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0204, 5.0204, 5.0204],
        [5.0204, 5.9338, 6.4042],
        [5.0204, 5.0947, 5.0437],
        [5.0204, 5.1837, 5.1239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.12824462354183197 
model_pd.l_d.mean(): -20.094635009765625 
model_pd.lagr.mean(): -19.96639060974121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4871], device='cuda:0')), ('power', tensor([-20.8118], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.12824462354183197
epoch£º252	 i:1 	 global-step:5041	 l-p:0.21430765092372894
epoch£º252	 i:2 	 global-step:5042	 l-p:0.1272054761648178
epoch£º252	 i:3 	 global-step:5043	 l-p:0.10369440168142319
epoch£º252	 i:4 	 global-step:5044	 l-p:0.17751696705818176
epoch£º252	 i:5 	 global-step:5045	 l-p:0.1263602375984192
epoch£º252	 i:6 	 global-step:5046	 l-p:-0.5342782735824585
epoch£º252	 i:7 	 global-step:5047	 l-p:0.13442139327526093
epoch£º252	 i:8 	 global-step:5048	 l-p:0.13038553297519684
epoch£º252	 i:9 	 global-step:5049	 l-p:0.07688091695308685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228]], device='cuda:0')
 pt:tensor([[4.8350, 5.6683, 6.0837],
        [4.8350, 4.9044, 4.8533],
        [4.8350, 4.8962, 4.8482],
        [4.8350, 5.2284, 5.2715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.13382351398468018 
model_pd.l_d.mean(): -20.601703643798828 
model_pd.lagr.mean(): -20.467880249023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4734], device='cuda:0')), ('power', tensor([-21.3105], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.13382351398468018
epoch£º253	 i:1 	 global-step:5061	 l-p:0.22801807522773743
epoch£º253	 i:2 	 global-step:5062	 l-p:0.1103101298213005
epoch£º253	 i:3 	 global-step:5063	 l-p:0.1383524239063263
epoch£º253	 i:4 	 global-step:5064	 l-p:0.0784701555967331
epoch£º253	 i:5 	 global-step:5065	 l-p:0.1416517198085785
epoch£º253	 i:6 	 global-step:5066	 l-p:0.13150139153003693
epoch£º253	 i:7 	 global-step:5067	 l-p:0.23726992309093475
epoch£º253	 i:8 	 global-step:5068	 l-p:0.14332713186740875
epoch£º253	 i:9 	 global-step:5069	 l-p:0.12494531273841858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0129, 5.0221, 5.0114],
        [5.0129, 5.0534, 5.0178],
        [5.0129, 5.0129, 5.0129],
        [5.0129, 5.0129, 5.0129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.15717875957489014 
model_pd.l_d.mean(): -20.200599670410156 
model_pd.lagr.mean(): -20.043420791625977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4580], device='cuda:0')), ('power', tensor([-20.8892], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.15717875957489014
epoch£º254	 i:1 	 global-step:5081	 l-p:0.09933847188949585
epoch£º254	 i:2 	 global-step:5082	 l-p:0.14269402623176575
epoch£º254	 i:3 	 global-step:5083	 l-p:0.1317453682422638
epoch£º254	 i:4 	 global-step:5084	 l-p:0.10439654439687729
epoch£º254	 i:5 	 global-step:5085	 l-p:0.11703015118837357
epoch£º254	 i:6 	 global-step:5086	 l-p:0.11476351320743561
epoch£º254	 i:7 	 global-step:5087	 l-p:0.14846469461917877
epoch£º254	 i:8 	 global-step:5088	 l-p:0.15059389173984528
epoch£º254	 i:9 	 global-step:5089	 l-p:0.1536453366279602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9847,  0.9796,  1.0000,  0.9746,
          1.0000,  0.9949, 31.6228]], device='cuda:0')
 pt:tensor([[5.0551, 5.9604, 6.4184],
        [5.0551, 5.1890, 5.1274],
        [5.0551, 5.4995, 5.5621],
        [5.0551, 6.1158, 6.7342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.15712514519691467 
model_pd.l_d.mean(): -18.833473205566406 
model_pd.lagr.mean(): -18.676347732543945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5149], device='cuda:0')), ('power', tensor([-19.5653], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.15712514519691467
epoch£º255	 i:1 	 global-step:5101	 l-p:0.11565853655338287
epoch£º255	 i:2 	 global-step:5102	 l-p:0.09031350910663605
epoch£º255	 i:3 	 global-step:5103	 l-p:0.12296783179044724
epoch£º255	 i:4 	 global-step:5104	 l-p:0.17294633388519287
epoch£º255	 i:5 	 global-step:5105	 l-p:0.12644201517105103
epoch£º255	 i:6 	 global-step:5106	 l-p:0.11781264841556549
epoch£º255	 i:7 	 global-step:5107	 l-p:0.2261480838060379
epoch£º255	 i:8 	 global-step:5108	 l-p:0.19018365442752838
epoch£º255	 i:9 	 global-step:5109	 l-p:0.14323239028453827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9521, 4.9799, 4.9518],
        [4.9521, 4.9527, 4.9514],
        [4.9521, 4.9521, 4.9521],
        [4.9521, 5.3122, 5.3282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.1443110853433609 
model_pd.l_d.mean(): -20.630748748779297 
model_pd.lagr.mean(): -20.48643684387207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4360], device='cuda:0')), ('power', tensor([-21.3016], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.1443110853433609
epoch£º256	 i:1 	 global-step:5121	 l-p:-0.9508593678474426
epoch£º256	 i:2 	 global-step:5122	 l-p:0.13405358791351318
epoch£º256	 i:3 	 global-step:5123	 l-p:0.13181161880493164
epoch£º256	 i:4 	 global-step:5124	 l-p:0.08652067184448242
epoch£º256	 i:5 	 global-step:5125	 l-p:0.1331668645143509
epoch£º256	 i:6 	 global-step:5126	 l-p:0.13759970664978027
epoch£º256	 i:7 	 global-step:5127	 l-p:0.26137590408325195
epoch£º256	 i:8 	 global-step:5128	 l-p:0.12210515886545181
epoch£º256	 i:9 	 global-step:5129	 l-p:0.42781442403793335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8819, 4.8819, 4.8819],
        [4.8819, 4.8819, 4.8819],
        [4.8819, 5.0529, 4.9952],
        [4.8819, 4.8844, 4.8799]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.12437303364276886 
model_pd.l_d.mean(): -19.610490798950195 
model_pd.lagr.mean(): -19.48611831665039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4950], device='cuda:0')), ('power', tensor([-20.3305], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.12437303364276886
epoch£º257	 i:1 	 global-step:5141	 l-p:0.14059408009052277
epoch£º257	 i:2 	 global-step:5142	 l-p:0.12482579797506332
epoch£º257	 i:3 	 global-step:5143	 l-p:0.07231456786394119
epoch£º257	 i:4 	 global-step:5144	 l-p:0.1736532598733902
epoch£º257	 i:5 	 global-step:5145	 l-p:0.12842488288879395
epoch£º257	 i:6 	 global-step:5146	 l-p:0.13022813200950623
epoch£º257	 i:7 	 global-step:5147	 l-p:0.1706141233444214
epoch£º257	 i:8 	 global-step:5148	 l-p:0.28375622630119324
epoch£º257	 i:9 	 global-step:5149	 l-p:0.1318027526140213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0315, 5.0314, 5.0315],
        [5.0315, 5.0618, 5.0325],
        [5.0315, 5.0315, 5.0315],
        [5.0315, 5.4108, 5.4337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.12388372421264648 
model_pd.l_d.mean(): -20.088218688964844 
model_pd.lagr.mean(): -19.96433448791504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4603], device='cuda:0')), ('power', tensor([-20.7780], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.12388372421264648
epoch£º258	 i:1 	 global-step:5161	 l-p:0.12639574706554413
epoch£º258	 i:2 	 global-step:5162	 l-p:0.1824967861175537
epoch£º258	 i:3 	 global-step:5163	 l-p:0.10643136501312256
epoch£º258	 i:4 	 global-step:5164	 l-p:0.11888345330953598
epoch£º258	 i:5 	 global-step:5165	 l-p:0.16040611267089844
epoch£º258	 i:6 	 global-step:5166	 l-p:0.16648563742637634
epoch£º258	 i:7 	 global-step:5167	 l-p:0.12168437987565994
epoch£º258	 i:8 	 global-step:5168	 l-p:0.11846553534269333
epoch£º258	 i:9 	 global-step:5169	 l-p:0.13970947265625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0139, 5.3883, 5.4093],
        [5.0139, 5.0139, 5.0139],
        [5.0139, 5.0139, 5.0139],
        [5.0139, 5.0818, 5.0317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.11530490964651108 
model_pd.l_d.mean(): -18.976091384887695 
model_pd.lagr.mean(): -18.86078643798828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4974], device='cuda:0')), ('power', tensor([-19.6916], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:0.11530490964651108
epoch£º259	 i:1 	 global-step:5181	 l-p:0.10178406536579132
epoch£º259	 i:2 	 global-step:5182	 l-p:0.15411964058876038
epoch£º259	 i:3 	 global-step:5183	 l-p:0.14732606709003448
epoch£º259	 i:4 	 global-step:5184	 l-p:-1.2589161396026611
epoch£º259	 i:5 	 global-step:5185	 l-p:-0.004422187805175781
epoch£º259	 i:6 	 global-step:5186	 l-p:0.12288112193346024
epoch£º259	 i:7 	 global-step:5187	 l-p:0.1274559199810028
epoch£º259	 i:8 	 global-step:5188	 l-p:0.06414090842008591
epoch£º259	 i:9 	 global-step:5189	 l-p:0.15141066908836365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8632, 5.3762, 5.5031],
        [4.8632, 5.1153, 5.0813],
        [4.8632, 4.8632, 4.8632],
        [4.8632, 4.9315, 4.8794]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.19930370151996613 
model_pd.l_d.mean(): -20.15465545654297 
model_pd.lagr.mean(): -19.955350875854492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5214], device='cuda:0')), ('power', tensor([-20.9076], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.19930370151996613
epoch£º260	 i:1 	 global-step:5201	 l-p:0.07528664171695709
epoch£º260	 i:2 	 global-step:5202	 l-p:0.1930990219116211
epoch£º260	 i:3 	 global-step:5203	 l-p:0.11096832156181335
epoch£º260	 i:4 	 global-step:5204	 l-p:0.027425574138760567
epoch£º260	 i:5 	 global-step:5205	 l-p:0.1370236873626709
epoch£º260	 i:6 	 global-step:5206	 l-p:0.13387084007263184
epoch£º260	 i:7 	 global-step:5207	 l-p:0.15560948848724365
epoch£º260	 i:8 	 global-step:5208	 l-p:0.37720707058906555
epoch£º260	 i:9 	 global-step:5209	 l-p:0.13026654720306396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9957, 4.9957, 4.9957],
        [4.9957, 5.3220, 5.3173],
        [4.9957, 5.3642, 5.3827],
        [4.9957, 5.0006, 4.9936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): 0.18233360350131989 
model_pd.l_d.mean(): -20.067363739013672 
model_pd.lagr.mean(): -19.88503074645996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4483], device='cuda:0')), ('power', tensor([-20.7446], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:0.18233360350131989
epoch£º261	 i:1 	 global-step:5221	 l-p:0.09612607955932617
epoch£º261	 i:2 	 global-step:5222	 l-p:0.15671062469482422
epoch£º261	 i:3 	 global-step:5223	 l-p:0.11705984175205231
epoch£º261	 i:4 	 global-step:5224	 l-p:0.15277880430221558
epoch£º261	 i:5 	 global-step:5225	 l-p:0.1366536021232605
epoch£º261	 i:6 	 global-step:5226	 l-p:0.12255760282278061
epoch£º261	 i:7 	 global-step:5227	 l-p:0.13428735733032227
epoch£º261	 i:8 	 global-step:5228	 l-p:0.14279277622699738
epoch£º261	 i:9 	 global-step:5229	 l-p:0.13735657930374146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0402, 5.0404, 5.0398],
        [5.0402, 5.0503, 5.0380],
        [5.0402, 5.0400, 5.0401],
        [5.0402, 5.0400, 5.0402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.12423072755336761 
model_pd.l_d.mean(): -19.544981002807617 
model_pd.lagr.mean(): -19.42074966430664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4786], device='cuda:0')), ('power', tensor([-20.2475], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.12423072755336761
epoch£º262	 i:1 	 global-step:5241	 l-p:0.13048474490642548
epoch£º262	 i:2 	 global-step:5242	 l-p:0.1417991816997528
epoch£º262	 i:3 	 global-step:5243	 l-p:-0.08919362723827362
epoch£º262	 i:4 	 global-step:5244	 l-p:0.07224651426076889
epoch£º262	 i:5 	 global-step:5245	 l-p:0.28471896052360535
epoch£º262	 i:6 	 global-step:5246	 l-p:0.18675072491168976
epoch£º262	 i:7 	 global-step:5247	 l-p:0.1494389921426773
epoch£º262	 i:8 	 global-step:5248	 l-p:-0.015078687109053135
epoch£º262	 i:9 	 global-step:5249	 l-p:0.13478252291679382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9487, 5.4104, 5.4925],
        [4.9487, 5.2276, 5.2018],
        [4.9487, 4.9502, 4.9471],
        [4.9487, 5.3545, 5.3987]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.1324232965707779 
model_pd.l_d.mean(): -19.34600067138672 
model_pd.lagr.mean(): -19.213577270507812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5044], device='cuda:0')), ('power', tensor([-20.0727], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.1324232965707779
epoch£º263	 i:1 	 global-step:5261	 l-p:0.3321864902973175
epoch£º263	 i:2 	 global-step:5262	 l-p:0.13555778563022614
epoch£º263	 i:3 	 global-step:5263	 l-p:0.1355852335691452
epoch£º263	 i:4 	 global-step:5264	 l-p:0.18522542715072632
epoch£º263	 i:5 	 global-step:5265	 l-p:0.13995672762393951
epoch£º263	 i:6 	 global-step:5266	 l-p:0.16329802572727203
epoch£º263	 i:7 	 global-step:5267	 l-p:0.13208217918872833
epoch£º263	 i:8 	 global-step:5268	 l-p:0.08908279240131378
epoch£º263	 i:9 	 global-step:5269	 l-p:0.12507687509059906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0683, 5.0682, 5.0681],
        [5.0683, 5.5738, 5.6790],
        [5.0683, 5.8369, 6.1623],
        [5.0683, 5.4151, 5.4178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.15183177590370178 
model_pd.l_d.mean(): -20.49730682373047 
model_pd.lagr.mean(): -20.345474243164062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4213], device='cuda:0')), ('power', tensor([-21.1517], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.15183177590370178
epoch£º264	 i:1 	 global-step:5281	 l-p:0.12981638312339783
epoch£º264	 i:2 	 global-step:5282	 l-p:0.10287176817655563
epoch£º264	 i:3 	 global-step:5283	 l-p:0.12148536741733551
epoch£º264	 i:4 	 global-step:5284	 l-p:0.14277838170528412
epoch£º264	 i:5 	 global-step:5285	 l-p:0.14114047586917877
epoch£º264	 i:6 	 global-step:5286	 l-p:0.17212054133415222
epoch£º264	 i:7 	 global-step:5287	 l-p:0.21144165098667145
epoch£º264	 i:8 	 global-step:5288	 l-p:0.14571744203567505
epoch£º264	 i:9 	 global-step:5289	 l-p:0.11968286335468292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0351, 5.8771, 6.2760],
        [5.0351, 5.1887, 5.1263],
        [5.0351, 5.0351, 5.0351],
        [5.0351, 5.0351, 5.0351]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.11227680742740631 
model_pd.l_d.mean(): -19.797897338867188 
model_pd.lagr.mean(): -19.68562126159668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4976], device='cuda:0')), ('power', tensor([-20.5226], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.11227680742740631
epoch£º265	 i:1 	 global-step:5301	 l-p:0.1432933211326599
epoch£º265	 i:2 	 global-step:5302	 l-p:0.13342779874801636
epoch£º265	 i:3 	 global-step:5303	 l-p:0.20612581074237823
epoch£º265	 i:4 	 global-step:5304	 l-p:0.08753184229135513
epoch£º265	 i:5 	 global-step:5305	 l-p:0.1542305201292038
epoch£º265	 i:6 	 global-step:5306	 l-p:0.12969009578227997
epoch£º265	 i:7 	 global-step:5307	 l-p:0.13243532180786133
epoch£º265	 i:8 	 global-step:5308	 l-p:0.14634136855602264
epoch£º265	 i:9 	 global-step:5309	 l-p:0.12136586755514145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0448, 5.0445, 5.0447],
        [5.0448, 5.0447, 5.0448],
        [5.0448, 5.0462, 5.0436],
        [5.0448, 5.0448, 5.0448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.1244901791214943 
model_pd.l_d.mean(): -20.352741241455078 
model_pd.lagr.mean(): -20.22825050354004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4411], device='cuda:0')), ('power', tensor([-21.0258], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.1244901791214943
epoch£º266	 i:1 	 global-step:5321	 l-p:0.13142551481723785
epoch£º266	 i:2 	 global-step:5322	 l-p:0.1262425184249878
epoch£º266	 i:3 	 global-step:5323	 l-p:0.5476776957511902
epoch£º266	 i:4 	 global-step:5324	 l-p:0.15925870835781097
epoch£º266	 i:5 	 global-step:5325	 l-p:5.841760635375977
epoch£º266	 i:6 	 global-step:5326	 l-p:0.14155635237693787
epoch£º266	 i:7 	 global-step:5327	 l-p:0.2123069167137146
epoch£º266	 i:8 	 global-step:5328	 l-p:0.13083113729953766
epoch£º266	 i:9 	 global-step:5329	 l-p:0.13451170921325684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0247, 5.0247, 5.0247],
        [5.0247, 5.1139, 5.0557],
        [5.0247, 5.0247, 5.0247],
        [5.0247, 5.0247, 5.0239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.1321096569299698 
model_pd.l_d.mean(): -19.963077545166016 
model_pd.lagr.mean(): -19.83096694946289 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4890], device='cuda:0')), ('power', tensor([-20.6808], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.1321096569299698
epoch£º267	 i:1 	 global-step:5341	 l-p:0.14680849015712738
epoch£º267	 i:2 	 global-step:5342	 l-p:0.1176060140132904
epoch£º267	 i:3 	 global-step:5343	 l-p:0.13548772037029266
epoch£º267	 i:4 	 global-step:5344	 l-p:0.12090620398521423
epoch£º267	 i:5 	 global-step:5345	 l-p:0.2184436023235321
epoch£º267	 i:6 	 global-step:5346	 l-p:0.5476927161216736
epoch£º267	 i:7 	 global-step:5347	 l-p:0.12544304132461548
epoch£º267	 i:8 	 global-step:5348	 l-p:0.13590413331985474
epoch£º267	 i:9 	 global-step:5349	 l-p:0.18185725808143616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9703, 4.9702, 4.9703],
        [4.9703, 5.1782, 5.1258],
        [4.9703, 4.9699, 4.9694],
        [4.9703, 4.9703, 4.9703]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.15084318816661835 
model_pd.l_d.mean(): -20.239662170410156 
model_pd.lagr.mean(): -20.08881950378418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4781], device='cuda:0')), ('power', tensor([-20.9493], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.15084318816661835
epoch£º268	 i:1 	 global-step:5361	 l-p:0.1383659690618515
epoch£º268	 i:2 	 global-step:5362	 l-p:0.15183457732200623
epoch£º268	 i:3 	 global-step:5363	 l-p:0.1222304254770279
epoch£º268	 i:4 	 global-step:5364	 l-p:0.2458333969116211
epoch£º268	 i:5 	 global-step:5365	 l-p:0.27112242579460144
epoch£º268	 i:6 	 global-step:5366	 l-p:0.1788138598203659
epoch£º268	 i:7 	 global-step:5367	 l-p:0.13188889622688293
epoch£º268	 i:8 	 global-step:5368	 l-p:0.1473766416311264
epoch£º268	 i:9 	 global-step:5369	 l-p:0.12738686800003052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0322, 5.0321, 5.0322],
        [5.0322, 5.0317, 5.0320],
        [5.0322, 5.0435, 5.0287],
        [5.0322, 5.0361, 5.0298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.1821839064359665 
model_pd.l_d.mean(): -20.42032814025879 
model_pd.lagr.mean(): -20.238143920898438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4507], device='cuda:0')), ('power', tensor([-21.1040], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.1821839064359665
epoch£º269	 i:1 	 global-step:5381	 l-p:0.21506133675575256
epoch£º269	 i:2 	 global-step:5382	 l-p:0.13991311192512512
epoch£º269	 i:3 	 global-step:5383	 l-p:0.11546605080366135
epoch£º269	 i:4 	 global-step:5384	 l-p:0.12399976700544357
epoch£º269	 i:5 	 global-step:5385	 l-p:0.12340695410966873
epoch£º269	 i:6 	 global-step:5386	 l-p:0.10815723240375519
epoch£º269	 i:7 	 global-step:5387	 l-p:0.18609550595283508
epoch£º269	 i:8 	 global-step:5388	 l-p:0.1422121375799179
epoch£º269	 i:9 	 global-step:5389	 l-p:0.105341337621212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9350, 4.9342, 4.9348],
        [4.9350, 5.1878, 5.1511],
        [4.9350, 4.9348, 4.9350],
        [4.9350, 5.0433, 4.9807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.13598695397377014 
model_pd.l_d.mean(): -20.64524269104004 
model_pd.lagr.mean(): -20.50925636291504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4417], device='cuda:0')), ('power', tensor([-21.3221], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.13598695397377014
epoch£º270	 i:1 	 global-step:5401	 l-p:0.01827440783381462
epoch£º270	 i:2 	 global-step:5402	 l-p:0.05035702511668205
epoch£º270	 i:3 	 global-step:5403	 l-p:0.1585087776184082
epoch£º270	 i:4 	 global-step:5404	 l-p:0.1453879475593567
epoch£º270	 i:5 	 global-step:5405	 l-p:0.022459888830780983
epoch£º270	 i:6 	 global-step:5406	 l-p:0.2553228437900543
epoch£º270	 i:7 	 global-step:5407	 l-p:0.13487491011619568
epoch£º270	 i:8 	 global-step:5408	 l-p:0.13462549448013306
epoch£º270	 i:9 	 global-step:5409	 l-p:0.1883619874715805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8828, 4.8828, 4.8828],
        [4.8828, 4.9694, 4.9097],
        [4.8828, 5.2631, 5.2951],
        [4.8828, 5.5868, 5.8749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): 0.09879238158464432 
model_pd.l_d.mean(): -20.487001419067383 
model_pd.lagr.mean(): -20.388208389282227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4833], device='cuda:0')), ('power', tensor([-21.2046], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:0.09879238158464432
epoch£º271	 i:1 	 global-step:5421	 l-p:0.12657888233661652
epoch£º271	 i:2 	 global-step:5422	 l-p:0.16625462472438812
epoch£º271	 i:3 	 global-step:5423	 l-p:0.16865390539169312
epoch£º271	 i:4 	 global-step:5424	 l-p:0.1383553296327591
epoch£º271	 i:5 	 global-step:5425	 l-p:0.12120486795902252
epoch£º271	 i:6 	 global-step:5426	 l-p:-0.1287224292755127
epoch£º271	 i:7 	 global-step:5427	 l-p:0.12884521484375
epoch£º271	 i:8 	 global-step:5428	 l-p:0.23304881155490875
epoch£º271	 i:9 	 global-step:5429	 l-p:0.13731588423252106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9954, 4.9947, 4.9946],
        [4.9954, 5.4824, 5.5798],
        [4.9954, 4.9966, 4.9932],
        [4.9954, 4.9946, 4.9950]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): 0.2929147779941559 
model_pd.l_d.mean(): -19.212366104125977 
model_pd.lagr.mean(): -18.919450759887695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4982], device='cuda:0')), ('power', tensor([-19.9313], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:0.2929147779941559
epoch£º272	 i:1 	 global-step:5441	 l-p:0.13656434416770935
epoch£º272	 i:2 	 global-step:5442	 l-p:0.11328896880149841
epoch£º272	 i:3 	 global-step:5443	 l-p:0.08174566924571991
epoch£º272	 i:4 	 global-step:5444	 l-p:0.1354747861623764
epoch£º272	 i:5 	 global-step:5445	 l-p:0.12131921201944351
epoch£º272	 i:6 	 global-step:5446	 l-p:0.16657109558582306
epoch£º272	 i:7 	 global-step:5447	 l-p:0.12424787133932114
epoch£º272	 i:8 	 global-step:5448	 l-p:0.1505945473909378
epoch£º272	 i:9 	 global-step:5449	 l-p:0.13228534162044525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0222, 5.0894, 5.0366],
        [5.0222, 5.0218, 5.0222],
        [5.0222, 5.0252, 5.0194],
        [5.0222, 5.0215, 5.0215]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.1418841928243637 
model_pd.l_d.mean(): -18.519201278686523 
model_pd.lagr.mean(): -18.377317428588867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5395], device='cuda:0')), ('power', tensor([-19.2727], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.1418841928243637
epoch£º273	 i:1 	 global-step:5461	 l-p:0.13203775882720947
epoch£º273	 i:2 	 global-step:5462	 l-p:0.1362520307302475
epoch£º273	 i:3 	 global-step:5463	 l-p:0.34535855054855347
epoch£º273	 i:4 	 global-step:5464	 l-p:0.11953219771385193
epoch£º273	 i:5 	 global-step:5465	 l-p:0.08071306347846985
epoch£º273	 i:6 	 global-step:5466	 l-p:0.14399346709251404
epoch£º273	 i:7 	 global-step:5467	 l-p:0.13528044521808624
epoch£º273	 i:8 	 global-step:5468	 l-p:0.135725200176239
epoch£º273	 i:9 	 global-step:5469	 l-p:0.29486793279647827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8273, 5.0225, 4.9688],
        [4.8273, 5.5093, 5.7841],
        [4.8273, 4.8264, 4.8272],
        [4.8273, 5.5075, 5.7807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.054908473044633865 
model_pd.l_d.mean(): -20.694080352783203 
model_pd.lagr.mean(): -20.639171600341797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4705], device='cuda:0')), ('power', tensor([-21.4009], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.054908473044633865
epoch£º274	 i:1 	 global-step:5481	 l-p:0.138872429728508
epoch£º274	 i:2 	 global-step:5482	 l-p:0.25236496329307556
epoch£º274	 i:3 	 global-step:5483	 l-p:-0.23771657049655914
epoch£º274	 i:4 	 global-step:5484	 l-p:0.15414975583553314
epoch£º274	 i:5 	 global-step:5485	 l-p:0.1436711698770523
epoch£º274	 i:6 	 global-step:5486	 l-p:0.12708911299705505
epoch£º274	 i:7 	 global-step:5487	 l-p:0.12615804374217987
epoch£º274	 i:8 	 global-step:5488	 l-p:-0.5946559906005859
epoch£º274	 i:9 	 global-step:5489	 l-p:0.1090591624379158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9125, 5.1848, 5.1568],
        [4.9125, 5.2079, 5.1907],
        [4.9125, 4.9125, 4.9125],
        [4.9125, 4.9109, 4.9115]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.1620860993862152 
model_pd.l_d.mean(): -20.7784366607666 
model_pd.lagr.mean(): -20.616350173950195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4316], device='cuda:0')), ('power', tensor([-21.4464], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.1620860993862152
epoch£º275	 i:1 	 global-step:5501	 l-p:0.13419349491596222
epoch£º275	 i:2 	 global-step:5502	 l-p:0.12437644600868225
epoch£º275	 i:3 	 global-step:5503	 l-p:0.21656817197799683
epoch£º275	 i:4 	 global-step:5504	 l-p:0.18655377626419067
epoch£º275	 i:5 	 global-step:5505	 l-p:0.17387963831424713
epoch£º275	 i:6 	 global-step:5506	 l-p:0.11600089073181152
epoch£º275	 i:7 	 global-step:5507	 l-p:0.10470398515462875
epoch£º275	 i:8 	 global-step:5508	 l-p:0.15009890496730804
epoch£º275	 i:9 	 global-step:5509	 l-p:0.12749932706356049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1274, 5.8179, 6.0679],
        [5.1274, 5.1274, 5.1265],
        [5.1274, 5.1273, 5.1274],
        [5.1274, 5.1271, 5.1273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.12379753589630127 
model_pd.l_d.mean(): -20.774080276489258 
model_pd.lagr.mean(): -20.65028190612793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3680], device='cuda:0')), ('power', tensor([-21.3770], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.12379753589630127
epoch£º276	 i:1 	 global-step:5521	 l-p:0.14500032365322113
epoch£º276	 i:2 	 global-step:5522	 l-p:0.14397990703582764
epoch£º276	 i:3 	 global-step:5523	 l-p:0.13328415155410767
epoch£º276	 i:4 	 global-step:5524	 l-p:0.15577469766139984
epoch£º276	 i:5 	 global-step:5525	 l-p:0.17234694957733154
epoch£º276	 i:6 	 global-step:5526	 l-p:0.1378377079963684
epoch£º276	 i:7 	 global-step:5527	 l-p:0.11645818501710892
epoch£º276	 i:8 	 global-step:5528	 l-p:0.11092840135097504
epoch£º276	 i:9 	 global-step:5529	 l-p:0.15898214280605316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9987, 5.4811, 5.5750],
        [4.9987, 5.5678, 5.7282],
        [4.9987, 4.9986, 4.9986],
        [4.9987, 5.3715, 5.3925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.14513055980205536 
model_pd.l_d.mean(): -20.201475143432617 
model_pd.lagr.mean(): -20.056344985961914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4773], device='cuda:0')), ('power', tensor([-20.9099], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.14513055980205536
epoch£º277	 i:1 	 global-step:5541	 l-p:0.12422920018434525
epoch£º277	 i:2 	 global-step:5542	 l-p:0.12557542324066162
epoch£º277	 i:3 	 global-step:5543	 l-p:0.14563356339931488
epoch£º277	 i:4 	 global-step:5544	 l-p:-0.21147212386131287
epoch£º277	 i:5 	 global-step:5545	 l-p:0.10342875868082047
epoch£º277	 i:6 	 global-step:5546	 l-p:0.14578814804553986
epoch£º277	 i:7 	 global-step:5547	 l-p:0.032754503190517426
epoch£º277	 i:8 	 global-step:5548	 l-p:0.13849765062332153
epoch£º277	 i:9 	 global-step:5549	 l-p:0.143825963139534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8266, 4.8250, 4.8262],
        [4.8266, 4.8243, 4.8238],
        [4.8266, 4.8370, 4.8174],
        [4.8266, 5.3123, 5.4225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.13573111593723297 
model_pd.l_d.mean(): -19.174718856811523 
model_pd.lagr.mean(): -19.03898811340332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5407], device='cuda:0')), ('power', tensor([-19.9367], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.13573111593723297
epoch£º278	 i:1 	 global-step:5561	 l-p:0.17827436327934265
epoch£º278	 i:2 	 global-step:5562	 l-p:0.07299265265464783
epoch£º278	 i:3 	 global-step:5563	 l-p:0.12317802011966705
epoch£º278	 i:4 	 global-step:5564	 l-p:0.1779177486896515
epoch£º278	 i:5 	 global-step:5565	 l-p:0.12192830443382263
epoch£º278	 i:6 	 global-step:5566	 l-p:0.15089578926563263
epoch£º278	 i:7 	 global-step:5567	 l-p:0.12226496636867523
epoch£º278	 i:8 	 global-step:5568	 l-p:0.1843954175710678
epoch£º278	 i:9 	 global-step:5569	 l-p:0.1289331167936325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0364, 5.0797, 5.0373],
        [5.0364, 5.0377, 5.0335],
        [5.0364, 5.5246, 5.6202],
        [5.0364, 5.0704, 5.0340]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.13453033566474915 
model_pd.l_d.mean(): -20.484039306640625 
model_pd.lagr.mean(): -20.34950828552246 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4291], device='cuda:0')), ('power', tensor([-21.1463], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.13453033566474915
epoch£º279	 i:1 	 global-step:5581	 l-p:0.21260930597782135
epoch£º279	 i:2 	 global-step:5582	 l-p:0.10726787149906158
epoch£º279	 i:3 	 global-step:5583	 l-p:0.09275811910629272
epoch£º279	 i:4 	 global-step:5584	 l-p:0.13642194867134094
epoch£º279	 i:5 	 global-step:5585	 l-p:0.1530195027589798
epoch£º279	 i:6 	 global-step:5586	 l-p:0.1508021503686905
epoch£º279	 i:7 	 global-step:5587	 l-p:0.12356465309858322
epoch£º279	 i:8 	 global-step:5588	 l-p:0.13840828835964203
epoch£º279	 i:9 	 global-step:5589	 l-p:0.1360614150762558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0599, 5.0597, 5.0599],
        [5.0599, 5.0613, 5.0571],
        [5.0599, 5.0715, 5.0547],
        [5.0599, 5.0969, 5.0586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.1250198483467102 
model_pd.l_d.mean(): -20.04170799255371 
model_pd.lagr.mean(): -19.916688919067383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4316], device='cuda:0')), ('power', tensor([-20.7016], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.1250198483467102
epoch£º280	 i:1 	 global-step:5601	 l-p:0.14879421889781952
epoch£º280	 i:2 	 global-step:5602	 l-p:0.14774002134799957
epoch£º280	 i:3 	 global-step:5603	 l-p:0.13087500631809235
epoch£º280	 i:4 	 global-step:5604	 l-p:0.13279928267002106
epoch£º280	 i:5 	 global-step:5605	 l-p:0.10537414997816086
epoch£º280	 i:6 	 global-step:5606	 l-p:0.09647466242313385
epoch£º280	 i:7 	 global-step:5607	 l-p:0.12394881248474121
epoch£º280	 i:8 	 global-step:5608	 l-p:0.1327279806137085
epoch£º280	 i:9 	 global-step:5609	 l-p:0.09741929173469543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8523, 4.9435, 4.8801],
        [4.8523, 4.8522, 4.8523],
        [4.8523, 4.8500, 4.8515],
        [4.8523, 4.8559, 4.8443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.09646869450807571 
model_pd.l_d.mean(): -20.619125366210938 
model_pd.lagr.mean(): -20.52265739440918 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4758], device='cuda:0')), ('power', tensor([-21.3305], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.09646869450807571
epoch£º281	 i:1 	 global-step:5621	 l-p:0.09919807314872742
epoch£º281	 i:2 	 global-step:5622	 l-p:0.15711116790771484
epoch£º281	 i:3 	 global-step:5623	 l-p:0.33089345693588257
epoch£º281	 i:4 	 global-step:5624	 l-p:0.10352490842342377
epoch£º281	 i:5 	 global-step:5625	 l-p:0.11883235722780228
epoch£º281	 i:6 	 global-step:5626	 l-p:0.16374923288822174
epoch£º281	 i:7 	 global-step:5627	 l-p:0.1379365473985672
epoch£º281	 i:8 	 global-step:5628	 l-p:0.1326490342617035
epoch£º281	 i:9 	 global-step:5629	 l-p:0.12105193734169006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1456,  0.0766,  1.0000,  0.0403,
          1.0000,  0.5261, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228]], device='cuda:0')
 pt:tensor([[4.9772, 4.9854, 4.9703],
        [4.9772, 5.1078, 5.0416],
        [4.9772, 4.9895, 4.9698],
        [4.9772, 5.6459, 5.8925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.09970054775476456 
model_pd.l_d.mean(): -18.578083038330078 
model_pd.lagr.mean(): -18.478382110595703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5939], device='cuda:0')), ('power', tensor([-19.3879], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.09970054775476456
epoch£º282	 i:1 	 global-step:5641	 l-p:0.13557514548301697
epoch£º282	 i:2 	 global-step:5642	 l-p:0.18358729779720306
epoch£º282	 i:3 	 global-step:5643	 l-p:0.2198033332824707
epoch£º282	 i:4 	 global-step:5644	 l-p:0.1769275963306427
epoch£º282	 i:5 	 global-step:5645	 l-p:0.1483769714832306
epoch£º282	 i:6 	 global-step:5646	 l-p:0.11335490643978119
epoch£º282	 i:7 	 global-step:5647	 l-p:0.1494607925415039
epoch£º282	 i:8 	 global-step:5648	 l-p:0.13097847998142242
epoch£º282	 i:9 	 global-step:5649	 l-p:0.12771902978420258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0745, 5.3298, 5.2889],
        [5.0745, 5.7702, 6.0305],
        [5.0745, 5.3297, 5.2888],
        [5.0745, 5.0738, 5.0744]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.08424004167318344 
model_pd.l_d.mean(): -20.070207595825195 
model_pd.lagr.mean(): -19.9859676361084 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4647], device='cuda:0')), ('power', tensor([-20.7642], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.08424004167318344
epoch£º283	 i:1 	 global-step:5661	 l-p:0.12170363962650299
epoch£º283	 i:2 	 global-step:5662	 l-p:0.12636786699295044
epoch£º283	 i:3 	 global-step:5663	 l-p:0.1426946371793747
epoch£º283	 i:4 	 global-step:5664	 l-p:0.19677914679050446
epoch£º283	 i:5 	 global-step:5665	 l-p:0.1336478739976883
epoch£º283	 i:6 	 global-step:5666	 l-p:0.12399150431156158
epoch£º283	 i:7 	 global-step:5667	 l-p:0.3599197566509247
epoch£º283	 i:8 	 global-step:5668	 l-p:0.17811229825019836
epoch£º283	 i:9 	 global-step:5669	 l-p:0.3406437933444977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0049, 5.0032, 5.0041],
        [5.0049, 5.0036, 5.0045],
        [5.0049, 5.0048, 5.0049],
        [5.0049, 5.1479, 5.0817]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.18243415653705597 
model_pd.l_d.mean(): -20.75779914855957 
model_pd.lagr.mean(): -20.57536506652832 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4132], device='cuda:0')), ('power', tensor([-21.4067], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.18243415653705597
epoch£º284	 i:1 	 global-step:5681	 l-p:0.1748766452074051
epoch£º284	 i:2 	 global-step:5682	 l-p:0.13660992681980133
epoch£º284	 i:3 	 global-step:5683	 l-p:0.1529691368341446
epoch£º284	 i:4 	 global-step:5684	 l-p:0.12273342907428741
epoch£º284	 i:5 	 global-step:5685	 l-p:0.030522016808390617
epoch£º284	 i:6 	 global-step:5686	 l-p:0.12787921726703644
epoch£º284	 i:7 	 global-step:5687	 l-p:0.1386309564113617
epoch£º284	 i:8 	 global-step:5688	 l-p:0.11707020550966263
epoch£º284	 i:9 	 global-step:5689	 l-p:0.10154052823781967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1174, 5.1174, 5.1174],
        [5.1174, 5.1164, 5.1169],
        [5.1174, 5.1293, 5.1120],
        [5.1174, 5.3604, 5.3137]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.11176539957523346 
model_pd.l_d.mean(): -20.45630645751953 
model_pd.lagr.mean(): -20.344541549682617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4089], device='cuda:0')), ('power', tensor([-21.0976], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.11176539957523346
epoch£º285	 i:1 	 global-step:5701	 l-p:0.12622761726379395
epoch£º285	 i:2 	 global-step:5702	 l-p:0.13509432971477509
epoch£º285	 i:3 	 global-step:5703	 l-p:0.12202522903680801
epoch£º285	 i:4 	 global-step:5704	 l-p:0.1280958205461502
epoch£º285	 i:5 	 global-step:5705	 l-p:-0.3974463641643524
epoch£º285	 i:6 	 global-step:5706	 l-p:0.03744623064994812
epoch£º285	 i:7 	 global-step:5707	 l-p:-0.04002389684319496
epoch£º285	 i:8 	 global-step:5708	 l-p:0.1922149807214737
epoch£º285	 i:9 	 global-step:5709	 l-p:0.12695421278476715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9447, 4.9423, 4.9431],
        [4.9447, 4.9447, 4.9447],
        [4.9447, 4.9447, 4.9447],
        [4.9447, 4.9441, 4.9447]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.13077344000339508 
model_pd.l_d.mean(): -20.625232696533203 
model_pd.lagr.mean(): -20.49445915222168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4408], device='cuda:0')), ('power', tensor([-21.3009], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:0.13077344000339508
epoch£º286	 i:1 	 global-step:5721	 l-p:0.1317957043647766
epoch£º286	 i:2 	 global-step:5722	 l-p:0.25842633843421936
epoch£º286	 i:3 	 global-step:5723	 l-p:0.10360882431268692
epoch£º286	 i:4 	 global-step:5724	 l-p:0.07629067450761795
epoch£º286	 i:5 	 global-step:5725	 l-p:0.007673830725252628
epoch£º286	 i:6 	 global-step:5726	 l-p:0.1308584064245224
epoch£º286	 i:7 	 global-step:5727	 l-p:0.13494186103343964
epoch£º286	 i:8 	 global-step:5728	 l-p:0.14066505432128906
epoch£º286	 i:9 	 global-step:5729	 l-p:0.1384914368391037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0043, 5.2003, 5.1419],
        [5.0043, 5.2786, 5.2474],
        [5.0043, 5.0298, 4.9970],
        [5.0043, 5.0031, 5.0012]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.13988463580608368 
model_pd.l_d.mean(): -20.427553176879883 
model_pd.lagr.mean(): -20.287668228149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4484], device='cuda:0')), ('power', tensor([-21.1088], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:0.13988463580608368
epoch£º287	 i:1 	 global-step:5741	 l-p:0.12817028164863586
epoch£º287	 i:2 	 global-step:5742	 l-p:0.18273615837097168
epoch£º287	 i:3 	 global-step:5743	 l-p:-0.02563495561480522
epoch£º287	 i:4 	 global-step:5744	 l-p:0.13318300247192383
epoch£º287	 i:5 	 global-step:5745	 l-p:0.1453341841697693
epoch£º287	 i:6 	 global-step:5746	 l-p:0.1390707641839981
epoch£º287	 i:7 	 global-step:5747	 l-p:0.1426401138305664
epoch£º287	 i:8 	 global-step:5748	 l-p:0.14320501685142517
epoch£º287	 i:9 	 global-step:5749	 l-p:-0.039478786289691925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9344, 4.9317, 4.9329],
        [4.9344, 4.9344, 4.9344],
        [4.9344, 4.9339, 4.9343],
        [4.9344, 5.0732, 5.0062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.19521814584732056 
model_pd.l_d.mean(): -20.571823120117188 
model_pd.lagr.mean(): -20.376604080200195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4576], device='cuda:0')), ('power', tensor([-21.2642], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.19521814584732056
epoch£º288	 i:1 	 global-step:5761	 l-p:0.1452319473028183
epoch£º288	 i:2 	 global-step:5762	 l-p:0.1476486772298813
epoch£º288	 i:3 	 global-step:5763	 l-p:0.5842887759208679
epoch£º288	 i:4 	 global-step:5764	 l-p:0.14082510769367218
epoch£º288	 i:5 	 global-step:5765	 l-p:0.13252495229244232
epoch£º288	 i:6 	 global-step:5766	 l-p:0.14808614552021027
epoch£º288	 i:7 	 global-step:5767	 l-p:0.124300017952919
epoch£º288	 i:8 	 global-step:5768	 l-p:0.09988905489444733
epoch£º288	 i:9 	 global-step:5769	 l-p:0.401283323764801
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9869, 5.3254, 5.3270],
        [4.9869, 4.9866, 4.9869],
        [4.9869, 4.9867, 4.9869],
        [4.9869, 4.9846, 4.9856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.11736873537302017 
model_pd.l_d.mean(): -19.024105072021484 
model_pd.lagr.mean(): -18.906736373901367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5397], device='cuda:0')), ('power', tensor([-19.7833], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.11736873537302017
epoch£º289	 i:1 	 global-step:5781	 l-p:0.09225653111934662
epoch£º289	 i:2 	 global-step:5782	 l-p:0.8306561708450317
epoch£º289	 i:3 	 global-step:5783	 l-p:0.13661208748817444
epoch£º289	 i:4 	 global-step:5784	 l-p:0.1513485610485077
epoch£º289	 i:5 	 global-step:5785	 l-p:0.7866986393928528
epoch£º289	 i:6 	 global-step:5786	 l-p:0.17850442230701447
epoch£º289	 i:7 	 global-step:5787	 l-p:0.14371545612812042
epoch£º289	 i:8 	 global-step:5788	 l-p:0.1231670081615448
epoch£º289	 i:9 	 global-step:5789	 l-p:0.14203153550624847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9905, 4.9905, 4.9905],
        [4.9905, 4.9895, 4.9904],
        [4.9905, 4.9905, 4.9905],
        [4.9905, 5.0120, 4.9812]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.15302273631095886 
model_pd.l_d.mean(): -20.612531661987305 
model_pd.lagr.mean(): -20.459508895874023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4212], device='cuda:0')), ('power', tensor([-21.2681], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.15302273631095886
epoch£º290	 i:1 	 global-step:5801	 l-p:0.12860862910747528
epoch£º290	 i:2 	 global-step:5802	 l-p:0.12778308987617493
epoch£º290	 i:3 	 global-step:5803	 l-p:0.9725321531295776
epoch£º290	 i:4 	 global-step:5804	 l-p:0.12943556904792786
epoch£º290	 i:5 	 global-step:5805	 l-p:0.12770314514636993
epoch£º290	 i:6 	 global-step:5806	 l-p:0.14099706709384918
epoch£º290	 i:7 	 global-step:5807	 l-p:0.2173338234424591
epoch£º290	 i:8 	 global-step:5808	 l-p:0.1510779708623886
epoch£º290	 i:9 	 global-step:5809	 l-p:0.1481877714395523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8545, 4.8819, 4.8420],
        [4.8545, 5.0851, 5.0402],
        [4.8545, 4.8508, 4.8528],
        [4.8545, 4.8544, 4.8545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.09721515327692032 
model_pd.l_d.mean(): -20.91400718688965 
model_pd.lagr.mean(): -20.816791534423828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4327], device='cuda:0')), ('power', tensor([-21.5846], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.09721515327692032
epoch£º291	 i:1 	 global-step:5821	 l-p:0.11709434539079666
epoch£º291	 i:2 	 global-step:5822	 l-p:0.12095773220062256
epoch£º291	 i:3 	 global-step:5823	 l-p:0.1667349934577942
epoch£º291	 i:4 	 global-step:5824	 l-p:0.12983794510364532
epoch£º291	 i:5 	 global-step:5825	 l-p:-0.1283528059720993
epoch£º291	 i:6 	 global-step:5826	 l-p:0.17839057743549347
epoch£º291	 i:7 	 global-step:5827	 l-p:0.16360296308994293
epoch£º291	 i:8 	 global-step:5828	 l-p:0.09900205582380295
epoch£º291	 i:9 	 global-step:5829	 l-p:0.13257193565368652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1119, 5.1119, 5.1119],
        [5.1119, 5.1107, 5.1116],
        [5.1119, 5.1118, 5.1119],
        [5.1119, 5.1119, 5.1119]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.12876193225383759 
model_pd.l_d.mean(): -20.044748306274414 
model_pd.lagr.mean(): -19.915987014770508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4336], device='cuda:0')), ('power', tensor([-20.7068], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.12876193225383759
epoch£º292	 i:1 	 global-step:5841	 l-p:0.1240556612610817
epoch£º292	 i:2 	 global-step:5842	 l-p:0.11813390254974365
epoch£º292	 i:3 	 global-step:5843	 l-p:0.11646205931901932
epoch£º292	 i:4 	 global-step:5844	 l-p:0.10135940462350845
epoch£º292	 i:5 	 global-step:5845	 l-p:0.1559242159128189
epoch£º292	 i:6 	 global-step:5846	 l-p:0.11528529971837997
epoch£º292	 i:7 	 global-step:5847	 l-p:0.18492640554904938
epoch£º292	 i:8 	 global-step:5848	 l-p:0.1289423704147339
epoch£º292	 i:9 	 global-step:5849	 l-p:0.12691381573677063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2542,  0.1610,  1.0000,  0.1020,
          1.0000,  0.6334, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1464,  0.0772,  1.0000,  0.0407,
          1.0000,  0.5270, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228]], device='cuda:0')
 pt:tensor([[5.0562, 5.1143, 5.0602],
        [5.0562, 5.1116, 5.0587],
        [5.0562, 5.0630, 5.0481],
        [5.0562, 5.0588, 5.0497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.12845328450202942 
model_pd.l_d.mean(): -20.610980987548828 
model_pd.lagr.mean(): -20.482526779174805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4023], device='cuda:0')), ('power', tensor([-21.2472], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.12845328450202942
epoch£º293	 i:1 	 global-step:5861	 l-p:0.11639191955327988
epoch£º293	 i:2 	 global-step:5862	 l-p:0.28290119767189026
epoch£º293	 i:3 	 global-step:5863	 l-p:6.228943347930908
epoch£º293	 i:4 	 global-step:5864	 l-p:0.13261155784130096
epoch£º293	 i:5 	 global-step:5865	 l-p:0.12374837696552277
epoch£º293	 i:6 	 global-step:5866	 l-p:-0.17824581265449524
epoch£º293	 i:7 	 global-step:5867	 l-p:0.12820176780223846
epoch£º293	 i:8 	 global-step:5868	 l-p:0.14881044626235962
epoch£º293	 i:9 	 global-step:5869	 l-p:0.2602989375591278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228]], device='cuda:0')
 pt:tensor([[4.9393, 4.9467, 4.9276],
        [4.9393, 5.2161, 5.1877],
        [4.9393, 5.2874, 5.2966],
        [4.9393, 5.0070, 4.9466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.16249975562095642 
model_pd.l_d.mean(): -19.48507308959961 
model_pd.lagr.mean(): -19.322572708129883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4982], device='cuda:0')), ('power', tensor([-20.2070], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.16249975562095642
epoch£º294	 i:1 	 global-step:5881	 l-p:0.14373041689395905
epoch£º294	 i:2 	 global-step:5882	 l-p:4.617710113525391
epoch£º294	 i:3 	 global-step:5883	 l-p:0.12885451316833496
epoch£º294	 i:4 	 global-step:5884	 l-p:0.11597981303930283
epoch£º294	 i:5 	 global-step:5885	 l-p:0.46948590874671936
epoch£º294	 i:6 	 global-step:5886	 l-p:0.15559278428554535
epoch£º294	 i:7 	 global-step:5887	 l-p:0.11361726373434067
epoch£º294	 i:8 	 global-step:5888	 l-p:0.12945687770843506
epoch£º294	 i:9 	 global-step:5889	 l-p:-0.3092713952064514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9677, 4.9935, 4.9564],
        [4.9677, 4.9676, 4.9677],
        [4.9677, 4.9670, 4.9676],
        [4.9677, 4.9677, 4.9677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.1407042145729065 
model_pd.l_d.mean(): -20.866382598876953 
model_pd.lagr.mean(): -20.725677490234375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4113], device='cuda:0')), ('power', tensor([-21.5146], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.1407042145729065
epoch£º295	 i:1 	 global-step:5901	 l-p:0.12559060752391815
epoch£º295	 i:2 	 global-step:5902	 l-p:0.024513734504580498
epoch£º295	 i:3 	 global-step:5903	 l-p:-0.03721078485250473
epoch£º295	 i:4 	 global-step:5904	 l-p:0.04198506101965904
epoch£º295	 i:5 	 global-step:5905	 l-p:0.13862459361553192
epoch£º295	 i:6 	 global-step:5906	 l-p:0.1541149765253067
epoch£º295	 i:7 	 global-step:5907	 l-p:0.12518379092216492
epoch£º295	 i:8 	 global-step:5908	 l-p:0.13854475319385529
epoch£º295	 i:9 	 global-step:5909	 l-p:0.1402796506881714
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0527, 5.0964, 5.0487],
        [5.0527, 5.0509, 5.0523],
        [5.0527, 5.0524, 5.0527],
        [5.0527, 5.0527, 5.0527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.16245993971824646 
model_pd.l_d.mean(): -20.472518920898438 
model_pd.lagr.mean(): -20.31005859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4271], device='cuda:0')), ('power', tensor([-21.1326], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.16245993971824646
epoch£º296	 i:1 	 global-step:5921	 l-p:0.1331861913204193
epoch£º296	 i:2 	 global-step:5922	 l-p:0.1367371678352356
epoch£º296	 i:3 	 global-step:5923	 l-p:0.1989215761423111
epoch£º296	 i:4 	 global-step:5924	 l-p:0.17649824917316437
epoch£º296	 i:5 	 global-step:5925	 l-p:0.10648009181022644
epoch£º296	 i:6 	 global-step:5926	 l-p:0.12537981569766998
epoch£º296	 i:7 	 global-step:5927	 l-p:0.13702146708965302
epoch£º296	 i:8 	 global-step:5928	 l-p:0.13355611264705658
epoch£º296	 i:9 	 global-step:5929	 l-p:0.10454444587230682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9705, 5.6618, 5.9299],
        [4.9705, 4.9882, 4.9576],
        [4.9705, 4.9671, 4.9689],
        [4.9705, 4.9702, 4.9705]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.5158894062042236 
model_pd.l_d.mean(): -19.514955520629883 
model_pd.lagr.mean(): -18.999065399169922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5218], device='cuda:0')), ('power', tensor([-20.2613], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.5158894062042236
epoch£º297	 i:1 	 global-step:5941	 l-p:0.12499257922172546
epoch£º297	 i:2 	 global-step:5942	 l-p:0.13429930806159973
epoch£º297	 i:3 	 global-step:5943	 l-p:0.11079564690589905
epoch£º297	 i:4 	 global-step:5944	 l-p:0.13555842638015747
epoch£º297	 i:5 	 global-step:5945	 l-p:0.14313744008541107
epoch£º297	 i:6 	 global-step:5946	 l-p:-0.9607941508293152
epoch£º297	 i:7 	 global-step:5947	 l-p:0.09980842471122742
epoch£º297	 i:8 	 global-step:5948	 l-p:0.12279481440782547
epoch£º297	 i:9 	 global-step:5949	 l-p:0.15812720358371735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7992, 4.7992, 4.7992],
        [4.7992, 4.7934, 4.7900],
        [4.7992, 4.7991, 4.7992],
        [4.7992, 5.6078, 6.0055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.14042536914348602 
model_pd.l_d.mean(): -19.839027404785156 
model_pd.lagr.mean(): -19.6986026763916 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5305], device='cuda:0')), ('power', tensor([-20.5978], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.14042536914348602
epoch£º298	 i:1 	 global-step:5961	 l-p:0.6385947465896606
epoch£º298	 i:2 	 global-step:5962	 l-p:0.15389414131641388
epoch£º298	 i:3 	 global-step:5963	 l-p:0.1298689991235733
epoch£º298	 i:4 	 global-step:5964	 l-p:0.19730022549629211
epoch£º298	 i:5 	 global-step:5965	 l-p:0.1274019330739975
epoch£º298	 i:6 	 global-step:5966	 l-p:-0.24252772331237793
epoch£º298	 i:7 	 global-step:5967	 l-p:0.15204690396785736
epoch£º298	 i:8 	 global-step:5968	 l-p:0.13000361621379852
epoch£º298	 i:9 	 global-step:5969	 l-p:0.30000537633895874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0137, 5.0109, 5.0127],
        [5.0137, 5.0218, 5.0020],
        [5.0137, 5.0640, 5.0106],
        [5.0137, 5.1470, 5.0769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.12208127975463867 
model_pd.l_d.mean(): -20.472869873046875 
model_pd.lagr.mean(): -20.350788116455078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4450], device='cuda:0')), ('power', tensor([-21.1512], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.12208127975463867
epoch£º299	 i:1 	 global-step:5981	 l-p:0.17438438534736633
epoch£º299	 i:2 	 global-step:5982	 l-p:0.24852827191352844
epoch£º299	 i:3 	 global-step:5983	 l-p:0.1285746991634369
epoch£º299	 i:4 	 global-step:5984	 l-p:0.13283661007881165
epoch£º299	 i:5 	 global-step:5985	 l-p:0.14666812121868134
epoch£º299	 i:6 	 global-step:5986	 l-p:0.1329280287027359
epoch£º299	 i:7 	 global-step:5987	 l-p:0.2893833816051483
epoch£º299	 i:8 	 global-step:5988	 l-p:0.09502919763326645
epoch£º299	 i:9 	 global-step:5989	 l-p:0.20657461881637573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0231, 5.7164, 5.9807],
        [5.0231, 5.0200, 5.0183],
        [5.0231, 5.0231, 5.0231],
        [5.0231, 5.0198, 5.0186]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.16593295335769653 
model_pd.l_d.mean(): -20.656208038330078 
model_pd.lagr.mean(): -20.49027442932129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4191], device='cuda:0')), ('power', tensor([-21.3101], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.16593295335769653
epoch£º300	 i:1 	 global-step:6001	 l-p:0.2162931263446808
epoch£º300	 i:2 	 global-step:6002	 l-p:0.1334066092967987
epoch£º300	 i:3 	 global-step:6003	 l-p:0.13363884389400482
epoch£º300	 i:4 	 global-step:6004	 l-p:0.1341220587491989
epoch£º300	 i:5 	 global-step:6005	 l-p:0.12812495231628418
epoch£º300	 i:6 	 global-step:6006	 l-p:0.11990343034267426
epoch£º300	 i:7 	 global-step:6007	 l-p:0.3124631345272064
epoch£º300	 i:8 	 global-step:6008	 l-p:0.12939350306987762
epoch£º300	 i:9 	 global-step:6009	 l-p:0.14315994083881378
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9374, 4.9403, 4.9243],
        [4.9374, 4.9374, 4.9374],
        [4.9374, 5.0848, 5.0154],
        [4.9374, 4.9374, 4.9374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): 0.059149354696273804 
model_pd.l_d.mean(): -19.23186683654785 
model_pd.lagr.mean(): -19.172718048095703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5224], device='cuda:0')), ('power', tensor([-19.9757], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:0.059149354696273804
epoch£º301	 i:1 	 global-step:6021	 l-p:0.1273033171892166
epoch£º301	 i:2 	 global-step:6022	 l-p:0.11925889551639557
epoch£º301	 i:3 	 global-step:6023	 l-p:0.1493813693523407
epoch£º301	 i:4 	 global-step:6024	 l-p:0.10263089835643768
epoch£º301	 i:5 	 global-step:6025	 l-p:0.141477569937706
epoch£º301	 i:6 	 global-step:6026	 l-p:0.14482074975967407
epoch£º301	 i:7 	 global-step:6027	 l-p:0.018286366015672684
epoch£º301	 i:8 	 global-step:6028	 l-p:0.7183722257614136
epoch£º301	 i:9 	 global-step:6029	 l-p:0.09106548130512238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8459, 4.8511, 4.8278],
        [4.8459, 4.8453, 4.8459],
        [4.8459, 5.4193, 5.5989],
        [4.8459, 4.8454, 4.8459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.14681720733642578 
model_pd.l_d.mean(): -20.74224090576172 
model_pd.lagr.mean(): -20.59542465209961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4598], device='cuda:0')), ('power', tensor([-21.4386], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.14681720733642578
epoch£º302	 i:1 	 global-step:6041	 l-p:0.18710540235042572
epoch£º302	 i:2 	 global-step:6042	 l-p:0.12256240844726562
epoch£º302	 i:3 	 global-step:6043	 l-p:0.07788538187742233
epoch£º302	 i:4 	 global-step:6044	 l-p:0.1493598073720932
epoch£º302	 i:5 	 global-step:6045	 l-p:0.1353006511926651
epoch£º302	 i:6 	 global-step:6046	 l-p:0.24675023555755615
epoch£º302	 i:7 	 global-step:6047	 l-p:0.14638324081897736
epoch£º302	 i:8 	 global-step:6048	 l-p:0.14347368478775024
epoch£º302	 i:9 	 global-step:6049	 l-p:-0.1253197342157364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9058, 4.9058, 4.9058],
        [4.9058, 4.9096, 4.8901],
        [4.9058, 5.7126, 6.0951],
        [4.9058, 4.9387, 4.8911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.1393495351076126 
model_pd.l_d.mean(): -19.162452697753906 
model_pd.lagr.mean(): -19.023103713989258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5248], device='cuda:0')), ('power', tensor([-19.9080], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.1393495351076126
epoch£º303	 i:1 	 global-step:6061	 l-p:0.16710951924324036
epoch£º303	 i:2 	 global-step:6062	 l-p:0.1381211280822754
epoch£º303	 i:3 	 global-step:6063	 l-p:-0.6346145272254944
epoch£º303	 i:4 	 global-step:6064	 l-p:0.20269210636615753
epoch£º303	 i:5 	 global-step:6065	 l-p:0.09155631065368652
epoch£º303	 i:6 	 global-step:6066	 l-p:0.13710665702819824
epoch£º303	 i:7 	 global-step:6067	 l-p:0.131631001830101
epoch£º303	 i:8 	 global-step:6068	 l-p:0.1391429752111435
epoch£º303	 i:9 	 global-step:6069	 l-p:0.09803963452577591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1347, 5.1347, 5.1347],
        [5.1347, 5.1347, 5.1347],
        [5.1347, 5.1343, 5.1347],
        [5.1347, 5.1346, 5.1347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.12621241807937622 
model_pd.l_d.mean(): -19.068729400634766 
model_pd.lagr.mean(): -18.942516326904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4560], device='cuda:0')), ('power', tensor([-19.7430], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.12621241807937622
epoch£º304	 i:1 	 global-step:6081	 l-p:0.1114296019077301
epoch£º304	 i:2 	 global-step:6082	 l-p:0.12764115631580353
epoch£º304	 i:3 	 global-step:6083	 l-p:0.14281228184700012
epoch£º304	 i:4 	 global-step:6084	 l-p:0.15883398056030273
epoch£º304	 i:5 	 global-step:6085	 l-p:0.16871783137321472
epoch£º304	 i:6 	 global-step:6086	 l-p:0.07515791058540344
epoch£º304	 i:7 	 global-step:6087	 l-p:0.14923232793807983
epoch£º304	 i:8 	 global-step:6088	 l-p:0.1258011907339096
epoch£º304	 i:9 	 global-step:6089	 l-p:0.13141460716724396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1251, 5.1762, 5.1223],
        [5.1251, 5.1250, 5.1251],
        [5.1251, 5.8274, 6.0892],
        [5.1251, 5.1289, 5.1152]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.1335880011320114 
model_pd.l_d.mean(): -20.15101432800293 
model_pd.lagr.mean(): -20.017425537109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4453], device='cuda:0')), ('power', tensor([-20.8262], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.1335880011320114
epoch£º305	 i:1 	 global-step:6101	 l-p:0.12807266414165497
epoch£º305	 i:2 	 global-step:6102	 l-p:0.09273530542850494
epoch£º305	 i:3 	 global-step:6103	 l-p:0.18282797932624817
epoch£º305	 i:4 	 global-step:6104	 l-p:0.18348757922649384
epoch£º305	 i:5 	 global-step:6105	 l-p:0.1266823261976242
epoch£º305	 i:6 	 global-step:6106	 l-p:0.13174496591091156
epoch£º305	 i:7 	 global-step:6107	 l-p:0.17685620486736298
epoch£º305	 i:8 	 global-step:6108	 l-p:0.1126621812582016
epoch£º305	 i:9 	 global-step:6109	 l-p:0.11273950338363647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1147, 5.1147, 5.1147],
        [5.1147, 5.7868, 6.0234],
        [5.1147, 5.1147, 5.1147],
        [5.1147, 5.2206, 5.1498]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): 0.15672658383846283 
model_pd.l_d.mean(): -19.03230094909668 
model_pd.lagr.mean(): -18.875574111938477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5012], device='cuda:0')), ('power', tensor([-19.7523], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:0.15672658383846283
epoch£º306	 i:1 	 global-step:6121	 l-p:0.11830559372901917
epoch£º306	 i:2 	 global-step:6122	 l-p:0.12541347742080688
epoch£º306	 i:3 	 global-step:6123	 l-p:0.09312830120325089
epoch£º306	 i:4 	 global-step:6124	 l-p:0.1383070945739746
epoch£º306	 i:5 	 global-step:6125	 l-p:0.13376379013061523
epoch£º306	 i:6 	 global-step:6126	 l-p:0.14423657953739166
epoch£º306	 i:7 	 global-step:6127	 l-p:0.11800859123468399
epoch£º306	 i:8 	 global-step:6128	 l-p:-0.12579110264778137
epoch£º306	 i:9 	 global-step:6129	 l-p:0.03030862659215927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9557, 4.9557, 4.9557],
        [4.9557, 4.9500, 4.9479],
        [4.9557, 5.3738, 5.4257],
        [4.9557, 4.9497, 4.9491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.14324840903282166 
model_pd.l_d.mean(): -20.59811019897461 
model_pd.lagr.mean(): -20.454862594604492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4497], device='cuda:0')), ('power', tensor([-21.2827], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.14324840903282166
epoch£º307	 i:1 	 global-step:6141	 l-p:0.13445128500461578
epoch£º307	 i:2 	 global-step:6142	 l-p:0.12118152529001236
epoch£º307	 i:3 	 global-step:6143	 l-p:0.14136800169944763
epoch£º307	 i:4 	 global-step:6144	 l-p:0.14089035987854004
epoch£º307	 i:5 	 global-step:6145	 l-p:0.11191240698099136
epoch£º307	 i:6 	 global-step:6146	 l-p:-0.05350711569190025
epoch£º307	 i:7 	 global-step:6147	 l-p:0.2707919180393219
epoch£º307	 i:8 	 global-step:6148	 l-p:0.15608946979045868
epoch£º307	 i:9 	 global-step:6149	 l-p:0.12963131070137024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8488, 4.8446, 4.8477],
        [4.8488, 4.8433, 4.8469],
        [4.8488, 4.8410, 4.8376],
        [4.8488, 4.9192, 4.8502]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.13226331770420074 
model_pd.l_d.mean(): -20.99270248413086 
model_pd.lagr.mean(): -20.86043930053711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4281], device='cuda:0')), ('power', tensor([-21.6594], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.13226331770420074
epoch£º308	 i:1 	 global-step:6161	 l-p:0.0965411439538002
epoch£º308	 i:2 	 global-step:6162	 l-p:0.14251108467578888
epoch£º308	 i:3 	 global-step:6163	 l-p:0.15830792486667633
epoch£º308	 i:4 	 global-step:6164	 l-p:0.1552078127861023
epoch£º308	 i:5 	 global-step:6165	 l-p:0.8799492716789246
epoch£º308	 i:6 	 global-step:6166	 l-p:0.061997316777706146
epoch£º308	 i:7 	 global-step:6167	 l-p:0.29623278975486755
epoch£º308	 i:8 	 global-step:6168	 l-p:0.12919758260250092
epoch£º308	 i:9 	 global-step:6169	 l-p:0.13183504343032837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9672, 4.9773, 4.9482],
        [4.9672, 5.2049, 5.1569],
        [4.9672, 4.9671, 4.9672],
        [4.9672, 5.3400, 5.3617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.17623835802078247 
model_pd.l_d.mean(): -19.05769920349121 
model_pd.lagr.mean(): -18.881460189819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5268], device='cuda:0')), ('power', tensor([-19.8042], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.17623835802078247
epoch£º309	 i:1 	 global-step:6181	 l-p:0.3716807961463928
epoch£º309	 i:2 	 global-step:6182	 l-p:0.14018630981445312
epoch£º309	 i:3 	 global-step:6183	 l-p:0.12198598682880402
epoch£º309	 i:4 	 global-step:6184	 l-p:0.12018777430057526
epoch£º309	 i:5 	 global-step:6185	 l-p:0.13157367706298828
epoch£º309	 i:6 	 global-step:6186	 l-p:0.13497450947761536
epoch£º309	 i:7 	 global-step:6187	 l-p:0.1661413162946701
epoch£º309	 i:8 	 global-step:6188	 l-p:0.15399932861328125
epoch£º309	 i:9 	 global-step:6189	 l-p:0.12944072484970093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1603, 5.1578, 5.1597],
        [5.1603, 5.1602, 5.1603],
        [5.1603, 5.4167, 5.3708],
        [5.1603, 5.7033, 5.8310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.1539394110441208 
model_pd.l_d.mean(): -20.70913314819336 
model_pd.lagr.mean(): -20.555192947387695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3760], device='cuda:0')), ('power', tensor([-21.3196], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.1539394110441208
epoch£º310	 i:1 	 global-step:6201	 l-p:-0.003522195853292942
epoch£º310	 i:2 	 global-step:6202	 l-p:0.12057618796825409
epoch£º310	 i:3 	 global-step:6203	 l-p:0.1136152520775795
epoch£º310	 i:4 	 global-step:6204	 l-p:0.14479894936084747
epoch£º310	 i:5 	 global-step:6205	 l-p:0.13645125925540924
epoch£º310	 i:6 	 global-step:6206	 l-p:0.15253335237503052
epoch£º310	 i:7 	 global-step:6207	 l-p:0.13306531310081482
epoch£º310	 i:8 	 global-step:6208	 l-p:0.10513772815465927
epoch£º310	 i:9 	 global-step:6209	 l-p:0.13699807226657867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2563,  0.1628,  1.0000,  0.1034,
          1.0000,  0.6352, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[5.0432, 5.0870, 5.0324],
        [5.0432, 5.0833, 5.0309],
        [5.0432, 5.0519, 5.0264],
        [5.0432, 5.1111, 5.0458]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.11719106137752533 
model_pd.l_d.mean(): -19.06839370727539 
model_pd.lagr.mean(): -18.951202392578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4957], device='cuda:0')), ('power', tensor([-19.7832], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.11719106137752533
epoch£º311	 i:1 	 global-step:6221	 l-p:0.2871986925601959
epoch£º311	 i:2 	 global-step:6222	 l-p:0.22570155560970306
epoch£º311	 i:3 	 global-step:6223	 l-p:0.13377317786216736
epoch£º311	 i:4 	 global-step:6224	 l-p:0.12392152845859528
epoch£º311	 i:5 	 global-step:6225	 l-p:0.11511030048131943
epoch£º311	 i:6 	 global-step:6226	 l-p:0.18390873074531555
epoch£º311	 i:7 	 global-step:6227	 l-p:-0.017894048243761063
epoch£º311	 i:8 	 global-step:6228	 l-p:0.147321879863739
epoch£º311	 i:9 	 global-step:6229	 l-p:0.13764400780200958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9728, 5.0134, 4.9581],
        [4.9728, 4.9699, 4.9722],
        [4.9728, 4.9728, 4.9728],
        [4.9728, 5.4435, 5.5313]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): -0.016672419384121895 
model_pd.l_d.mean(): -20.766109466552734 
model_pd.lagr.mean(): -20.78278160095215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4260], device='cuda:0')), ('power', tensor([-21.4282], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:-0.016672419384121895
epoch£º312	 i:1 	 global-step:6241	 l-p:0.11118804663419724
epoch£º312	 i:2 	 global-step:6242	 l-p:0.1084744781255722
epoch£º312	 i:3 	 global-step:6243	 l-p:0.2746267020702362
epoch£º312	 i:4 	 global-step:6244	 l-p:0.1476864516735077
epoch£º312	 i:5 	 global-step:6245	 l-p:0.08417684584856033
epoch£º312	 i:6 	 global-step:6246	 l-p:0.16265854239463806
epoch£º312	 i:7 	 global-step:6247	 l-p:0.13305716216564178
epoch£º312	 i:8 	 global-step:6248	 l-p:0.137542724609375
epoch£º312	 i:9 	 global-step:6249	 l-p:-0.040335994213819504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9668, 4.9595, 4.9591],
        [4.9668, 5.5549, 5.7351],
        [4.9668, 5.0061, 4.9509],
        [4.9668, 4.9622, 4.9654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.1506119966506958 
model_pd.l_d.mean(): -20.678905487060547 
model_pd.lagr.mean(): -20.52829360961914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4373], device='cuda:0')), ('power', tensor([-21.3517], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.1506119966506958
epoch£º313	 i:1 	 global-step:6261	 l-p:0.13285349309444427
epoch£º313	 i:2 	 global-step:6262	 l-p:0.2541443109512329
epoch£º313	 i:3 	 global-step:6263	 l-p:0.1901199370622635
epoch£º313	 i:4 	 global-step:6264	 l-p:0.14565224945545197
epoch£º313	 i:5 	 global-step:6265	 l-p:0.11840087920427322
epoch£º313	 i:6 	 global-step:6266	 l-p:0.1697154939174652
epoch£º313	 i:7 	 global-step:6267	 l-p:0.13221600651741028
epoch£º313	 i:8 	 global-step:6268	 l-p:0.07378779351711273
epoch£º313	 i:9 	 global-step:6269	 l-p:0.017483064904808998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9396, 4.9396, 4.9396],
        [4.9396, 4.9340, 4.9375],
        [4.9396, 4.9339, 4.9253],
        [4.9396, 4.9402, 4.9198]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.03674919903278351 
model_pd.l_d.mean(): -19.99505615234375 
model_pd.lagr.mean(): -19.95830726623535 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5267], device='cuda:0')), ('power', tensor([-20.7516], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.03674919903278351
epoch£º314	 i:1 	 global-step:6281	 l-p:0.14019809663295746
epoch£º314	 i:2 	 global-step:6282	 l-p:0.1511314958333969
epoch£º314	 i:3 	 global-step:6283	 l-p:0.12576396763324738
epoch£º314	 i:4 	 global-step:6284	 l-p:1.0171982049942017
epoch£º314	 i:5 	 global-step:6285	 l-p:0.24418072402477264
epoch£º314	 i:6 	 global-step:6286	 l-p:0.1378515213727951
epoch£º314	 i:7 	 global-step:6287	 l-p:0.12835091352462769
epoch£º314	 i:8 	 global-step:6288	 l-p:0.09807439893484116
epoch£º314	 i:9 	 global-step:6289	 l-p:0.1388542652130127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1187, 5.3412, 5.2836],
        [5.1187, 5.1565, 5.1061],
        [5.1187, 5.1187, 5.1187],
        [5.1187, 5.5537, 5.6067]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.128397598862648 
model_pd.l_d.mean(): -20.201770782470703 
model_pd.lagr.mean(): -20.073373794555664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4267], device='cuda:0')), ('power', tensor([-20.8585], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.128397598862648
epoch£º315	 i:1 	 global-step:6301	 l-p:0.11594144254922867
epoch£º315	 i:2 	 global-step:6302	 l-p:0.11048413068056107
epoch£º315	 i:3 	 global-step:6303	 l-p:0.1283259093761444
epoch£º315	 i:4 	 global-step:6304	 l-p:0.13187554478645325
epoch£º315	 i:5 	 global-step:6305	 l-p:0.1896142214536667
epoch£º315	 i:6 	 global-step:6306	 l-p:0.20256038010120392
epoch£º315	 i:7 	 global-step:6307	 l-p:0.15263573825359344
epoch£º315	 i:8 	 global-step:6308	 l-p:0.11980635672807693
epoch£º315	 i:9 	 global-step:6309	 l-p:0.13605491816997528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9857, 4.9816, 4.9846],
        [4.9857, 4.9849, 4.9856],
        [4.9857, 5.0404, 4.9769],
        [4.9857, 4.9783, 4.9771]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.13898709416389465 
model_pd.l_d.mean(): -20.96759605407715 
model_pd.lagr.mean(): -20.828609466552734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3908], device='cuda:0')), ('power', tensor([-21.5959], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.13898709416389465
epoch£º316	 i:1 	 global-step:6321	 l-p:0.07729155570268631
epoch£º316	 i:2 	 global-step:6322	 l-p:0.4890303611755371
epoch£º316	 i:3 	 global-step:6323	 l-p:0.14789840579032898
epoch£º316	 i:4 	 global-step:6324	 l-p:0.12108542770147324
epoch£º316	 i:5 	 global-step:6325	 l-p:0.14794127643108368
epoch£º316	 i:6 	 global-step:6326	 l-p:0.10588947683572769
epoch£º316	 i:7 	 global-step:6327	 l-p:0.04156233370304108
epoch£º316	 i:8 	 global-step:6328	 l-p:0.1335195004940033
epoch£º316	 i:9 	 global-step:6329	 l-p:0.12000013142824173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7125, 4.6988, 4.6997],
        [4.7125, 4.7118, 4.7124],
        [4.7125, 4.7057, 4.6826],
        [4.7125, 4.7020, 4.6867]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.12896469235420227 
model_pd.l_d.mean(): -20.399324417114258 
model_pd.lagr.mean(): -20.27035903930664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5465], device='cuda:0')), ('power', tensor([-21.1806], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.12896469235420227
epoch£º317	 i:1 	 global-step:6341	 l-p:0.18048077821731567
epoch£º317	 i:2 	 global-step:6342	 l-p:0.19621668756008148
epoch£º317	 i:3 	 global-step:6343	 l-p:0.13800252974033356
epoch£º317	 i:4 	 global-step:6344	 l-p:0.24667038023471832
epoch£º317	 i:5 	 global-step:6345	 l-p:0.1364675909280777
epoch£º317	 i:6 	 global-step:6346	 l-p:0.13269327580928802
epoch£º317	 i:7 	 global-step:6347	 l-p:0.13077452778816223
epoch£º317	 i:8 	 global-step:6348	 l-p:0.15448440611362457
epoch£º317	 i:9 	 global-step:6349	 l-p:0.14096876978874207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8648, 4.8546, 4.8525],
        [4.8648, 4.8647, 4.8648],
        [4.8648, 4.8598, 4.8635],
        [4.8648, 4.8644, 4.8648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.06927940249443054 
model_pd.l_d.mean(): -20.25363540649414 
model_pd.lagr.mean(): -20.184356689453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5078], device='cuda:0')), ('power', tensor([-20.9937], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.06927940249443054
epoch£º318	 i:1 	 global-step:6361	 l-p:0.1438702642917633
epoch£º318	 i:2 	 global-step:6362	 l-p:0.12975157797336578
epoch£º318	 i:3 	 global-step:6363	 l-p:0.2014274150133133
epoch£º318	 i:4 	 global-step:6364	 l-p:0.13226431608200073
epoch£º318	 i:5 	 global-step:6365	 l-p:0.020119495689868927
epoch£º318	 i:6 	 global-step:6366	 l-p:-0.22470985352993011
epoch£º318	 i:7 	 global-step:6367	 l-p:0.13420473039150238
epoch£º318	 i:8 	 global-step:6368	 l-p:0.12127942591905594
epoch£º318	 i:9 	 global-step:6369	 l-p:0.12358249723911285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1090, 5.1149, 5.0920],
        [5.1090, 5.1399, 5.0926],
        [5.1090, 5.1090, 5.1090],
        [5.1090, 5.1090, 5.1090]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.16231606900691986 
model_pd.l_d.mean(): -20.502647399902344 
model_pd.lagr.mean(): -20.34033203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4168], device='cuda:0')), ('power', tensor([-21.1525], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.16231606900691986
epoch£º319	 i:1 	 global-step:6381	 l-p:0.11460807174444199
epoch£º319	 i:2 	 global-step:6382	 l-p:0.12513071298599243
epoch£º319	 i:3 	 global-step:6383	 l-p:0.13221919536590576
epoch£º319	 i:4 	 global-step:6384	 l-p:0.1369427591562271
epoch£º319	 i:5 	 global-step:6385	 l-p:0.12365366518497467
epoch£º319	 i:6 	 global-step:6386	 l-p:0.4574752748012543
epoch£º319	 i:7 	 global-step:6387	 l-p:-0.05425485596060753
epoch£º319	 i:8 	 global-step:6388	 l-p:0.16584250330924988
epoch£º319	 i:9 	 global-step:6389	 l-p:0.18193097412586212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9982, 5.0186, 4.9752],
        [4.9982, 5.0671, 4.9973],
        [4.9982, 5.0536, 4.9886],
        [4.9982, 4.9961, 4.9979]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.16056498885154724 
model_pd.l_d.mean(): -20.16353416442871 
model_pd.lagr.mean(): -20.00296974182129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4418], device='cuda:0')), ('power', tensor([-20.8352], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.16056498885154724
epoch£º320	 i:1 	 global-step:6401	 l-p:0.14406928420066833
epoch£º320	 i:2 	 global-step:6402	 l-p:0.10232473164796829
epoch£º320	 i:3 	 global-step:6403	 l-p:0.18478967249393463
epoch£º320	 i:4 	 global-step:6404	 l-p:0.11684864014387131
epoch£º320	 i:5 	 global-step:6405	 l-p:0.1441504955291748
epoch£º320	 i:6 	 global-step:6406	 l-p:0.1280493587255478
epoch£º320	 i:7 	 global-step:6407	 l-p:0.2085123360157013
epoch£º320	 i:8 	 global-step:6408	 l-p:0.2275710552930832
epoch£º320	 i:9 	 global-step:6409	 l-p:0.14327195286750793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0543, 5.0475, 5.0503],
        [5.0543, 5.3886, 5.3827],
        [5.0543, 5.0516, 5.0538],
        [5.0543, 5.0541, 5.0543]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.1175636276602745 
model_pd.l_d.mean(): -19.57583999633789 
model_pd.lagr.mean(): -19.458276748657227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4816], device='cuda:0')), ('power', tensor([-20.2817], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.1175636276602745
epoch£º321	 i:1 	 global-step:6421	 l-p:0.33175235986709595
epoch£º321	 i:2 	 global-step:6422	 l-p:0.11416769027709961
epoch£º321	 i:3 	 global-step:6423	 l-p:0.1324683278799057
epoch£º321	 i:4 	 global-step:6424	 l-p:0.12506327033042908
epoch£º321	 i:5 	 global-step:6425	 l-p:-0.27994248270988464
epoch£º321	 i:6 	 global-step:6426	 l-p:0.12640024721622467
epoch£º321	 i:7 	 global-step:6427	 l-p:0.23889194428920746
epoch£º321	 i:8 	 global-step:6428	 l-p:0.14198213815689087
epoch£º321	 i:9 	 global-step:6429	 l-p:0.15252910554409027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9791, 5.8111, 6.2083],
        [4.9791, 4.9790, 4.9791],
        [4.9791, 4.9788, 4.9791],
        [4.9791, 4.9791, 4.9791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.17749813199043274 
model_pd.l_d.mean(): -20.643508911132812 
model_pd.lagr.mean(): -20.46601104736328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4343], device='cuda:0')), ('power', tensor([-21.3128], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.17749813199043274
epoch£º322	 i:1 	 global-step:6441	 l-p:0.1436590701341629
epoch£º322	 i:2 	 global-step:6442	 l-p:0.11667371541261673
epoch£º322	 i:3 	 global-step:6443	 l-p:0.1871243566274643
epoch£º322	 i:4 	 global-step:6444	 l-p:0.1033100038766861
epoch£º322	 i:5 	 global-step:6445	 l-p:0.13488434255123138
epoch£º322	 i:6 	 global-step:6446	 l-p:0.12720979750156403
epoch£º322	 i:7 	 global-step:6447	 l-p:-0.002741041127592325
epoch£º322	 i:8 	 global-step:6448	 l-p:2.0086400508880615
epoch£º322	 i:9 	 global-step:6449	 l-p:1.9237618446350098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0333, 5.0333, 5.0333],
        [5.0333, 5.0305, 5.0329],
        [5.0333, 5.1923, 5.1184],
        [5.0333, 5.0254, 5.0280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.308427631855011 
model_pd.l_d.mean(): -20.37157440185547 
model_pd.lagr.mean(): -20.063146591186523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4545], device='cuda:0')), ('power', tensor([-21.0585], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.308427631855011
epoch£º323	 i:1 	 global-step:6461	 l-p:0.14125481247901917
epoch£º323	 i:2 	 global-step:6462	 l-p:0.0999688133597374
epoch£º323	 i:3 	 global-step:6463	 l-p:0.16146525740623474
epoch£º323	 i:4 	 global-step:6464	 l-p:0.1279841810464859
epoch£º323	 i:5 	 global-step:6465	 l-p:0.11992210149765015
epoch£º323	 i:6 	 global-step:6466	 l-p:0.2807638645172119
epoch£º323	 i:7 	 global-step:6467	 l-p:0.11611291766166687
epoch£º323	 i:8 	 global-step:6468	 l-p:0.11300921440124512
epoch£º323	 i:9 	 global-step:6469	 l-p:0.12554015219211578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2444, 5.2444, 5.2444],
        [5.2444, 5.2444, 5.2444],
        [5.2444, 5.2404, 5.2429],
        [5.2444, 5.2546, 5.2283]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.13153064250946045 
model_pd.l_d.mean(): -19.665142059326172 
model_pd.lagr.mean(): -19.533611297607422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4748], device='cuda:0')), ('power', tensor([-20.3651], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.13153064250946045
epoch£º324	 i:1 	 global-step:6481	 l-p:0.11898647248744965
epoch£º324	 i:2 	 global-step:6482	 l-p:0.14780251681804657
epoch£º324	 i:3 	 global-step:6483	 l-p:0.1301376074552536
epoch£º324	 i:4 	 global-step:6484	 l-p:0.0951300784945488
epoch£º324	 i:5 	 global-step:6485	 l-p:0.16151881217956543
epoch£º324	 i:6 	 global-step:6486	 l-p:0.13123592734336853
epoch£º324	 i:7 	 global-step:6487	 l-p:0.13214631378650665
epoch£º324	 i:8 	 global-step:6488	 l-p:0.09695012122392654
epoch£º324	 i:9 	 global-step:6489	 l-p:-0.20211032032966614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8608, 4.8770, 4.8279],
        [4.8608, 4.8541, 4.8588],
        [4.8608, 4.8606, 4.8608],
        [4.8608, 4.9459, 4.8674]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.13502788543701172 
model_pd.l_d.mean(): -19.285707473754883 
model_pd.lagr.mean(): -19.150680541992188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5306], device='cuda:0')), ('power', tensor([-20.0385], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.13502788543701172
epoch£º325	 i:1 	 global-step:6501	 l-p:0.11576036363840103
epoch£º325	 i:2 	 global-step:6502	 l-p:-0.33907100558280945
epoch£º325	 i:3 	 global-step:6503	 l-p:0.1401561200618744
epoch£º325	 i:4 	 global-step:6504	 l-p:0.12937068939208984
epoch£º325	 i:5 	 global-step:6505	 l-p:0.14089347422122955
epoch£º325	 i:6 	 global-step:6506	 l-p:0.128776416182518
epoch£º325	 i:7 	 global-step:6507	 l-p:0.12285958975553513
epoch£º325	 i:8 	 global-step:6508	 l-p:0.10935424268245697
epoch£º325	 i:9 	 global-step:6509	 l-p:0.12931734323501587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8950, 4.9713, 4.8943],
        [4.8950, 4.8942, 4.8950],
        [4.8950, 5.6543, 5.9928],
        [4.8950, 4.9582, 4.8844]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.13287995755672455 
model_pd.l_d.mean(): -20.320270538330078 
model_pd.lagr.mean(): -20.18739128112793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4919], device='cuda:0')), ('power', tensor([-21.0448], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.13287995755672455
epoch£º326	 i:1 	 global-step:6521	 l-p:0.1313682496547699
epoch£º326	 i:2 	 global-step:6522	 l-p:0.1617187112569809
epoch£º326	 i:3 	 global-step:6523	 l-p:0.10344215482473373
epoch£º326	 i:4 	 global-step:6524	 l-p:0.1650216281414032
epoch£º326	 i:5 	 global-step:6525	 l-p:0.10851345956325531
epoch£º326	 i:6 	 global-step:6526	 l-p:0.1257571578025818
epoch£º326	 i:7 	 global-step:6527	 l-p:0.1375434249639511
epoch£º326	 i:8 	 global-step:6528	 l-p:0.11472665518522263
epoch£º326	 i:9 	 global-step:6529	 l-p:0.13400748372077942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8873, 5.6591, 6.0101],
        [4.8873, 4.8871, 4.8873],
        [4.8873, 4.8749, 4.8690],
        [4.8873, 4.8864, 4.8872]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.1337304711341858 
model_pd.l_d.mean(): -20.160768508911133 
model_pd.lagr.mean(): -20.02703857421875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5001], device='cuda:0')), ('power', tensor([-20.8920], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.1337304711341858
epoch£º327	 i:1 	 global-step:6541	 l-p:-0.09675893932580948
epoch£º327	 i:2 	 global-step:6542	 l-p:0.11804868280887604
epoch£º327	 i:3 	 global-step:6543	 l-p:0.1152513399720192
epoch£º327	 i:4 	 global-step:6544	 l-p:0.04165701940655708
epoch£º327	 i:5 	 global-step:6545	 l-p:0.15896724164485931
epoch£º327	 i:6 	 global-step:6546	 l-p:0.15596330165863037
epoch£º327	 i:7 	 global-step:6547	 l-p:0.09345471113920212
epoch£º327	 i:8 	 global-step:6548	 l-p:0.12992718815803528
epoch£º327	 i:9 	 global-step:6549	 l-p:0.12742634117603302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0216, 5.0173, 5.0206],
        [5.0216, 5.0197, 5.0214],
        [5.0216, 5.1008, 5.0250],
        [5.0216, 5.5198, 5.6239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.2722688615322113 
model_pd.l_d.mean(): -20.098575592041016 
model_pd.lagr.mean(): -19.82630729675293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4831], device='cuda:0')), ('power', tensor([-20.8118], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.2722688615322113
epoch£º328	 i:1 	 global-step:6561	 l-p:0.12431438267230988
epoch£º328	 i:2 	 global-step:6562	 l-p:0.10290153324604034
epoch£º328	 i:3 	 global-step:6563	 l-p:0.42302948236465454
epoch£º328	 i:4 	 global-step:6564	 l-p:0.16564075648784637
epoch£º328	 i:5 	 global-step:6565	 l-p:0.11173833906650543
epoch£º328	 i:6 	 global-step:6566	 l-p:0.20861007273197174
epoch£º328	 i:7 	 global-step:6567	 l-p:0.16680164635181427
epoch£º328	 i:8 	 global-step:6568	 l-p:0.12878833711147308
epoch£º328	 i:9 	 global-step:6569	 l-p:0.1055687665939331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1020, 5.1020, 5.1020],
        [5.1020, 5.3609, 5.3150],
        [5.1020, 5.1014, 5.1020],
        [5.1020, 5.0938, 5.0902]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.1295834183692932 
model_pd.l_d.mean(): -19.033632278442383 
model_pd.lagr.mean(): -18.904048919677734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5164], device='cuda:0')), ('power', tensor([-19.7692], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.1295834183692932
epoch£º329	 i:1 	 global-step:6581	 l-p:0.11272922158241272
epoch£º329	 i:2 	 global-step:6582	 l-p:0.1162690743803978
epoch£º329	 i:3 	 global-step:6583	 l-p:0.14318206906318665
epoch£º329	 i:4 	 global-step:6584	 l-p:0.11754175275564194
epoch£º329	 i:5 	 global-step:6585	 l-p:0.1631527543067932
epoch£º329	 i:6 	 global-step:6586	 l-p:0.1262582689523697
epoch£º329	 i:7 	 global-step:6587	 l-p:0.8435277938842773
epoch£º329	 i:8 	 global-step:6588	 l-p:0.11316435784101486
epoch£º329	 i:9 	 global-step:6589	 l-p:-0.009890730492770672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9435, 4.9428, 4.9434],
        [4.9435, 4.9426, 4.9434],
        [4.9435, 4.9429, 4.9434],
        [4.9435, 4.9435, 4.9435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.13035909831523895 
model_pd.l_d.mean(): -20.54020881652832 
model_pd.lagr.mean(): -20.409849166870117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4531], device='cuda:0')), ('power', tensor([-21.2276], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.13035909831523895
epoch£º330	 i:1 	 global-step:6601	 l-p:0.16151048243045807
epoch£º330	 i:2 	 global-step:6602	 l-p:0.15142253041267395
epoch£º330	 i:3 	 global-step:6603	 l-p:0.038122642785310745
epoch£º330	 i:4 	 global-step:6604	 l-p:0.15874651074409485
epoch£º330	 i:5 	 global-step:6605	 l-p:0.026913221925497055
epoch£º330	 i:6 	 global-step:6606	 l-p:0.1373528391122818
epoch£º330	 i:7 	 global-step:6607	 l-p:0.12417661398649216
epoch£º330	 i:8 	 global-step:6608	 l-p:0.13242840766906738
epoch£º330	 i:9 	 global-step:6609	 l-p:0.15057627856731415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7921, 4.8287, 4.7584],
        [4.7921, 5.4684, 5.7427],
        [4.7921, 4.7892, 4.7917],
        [4.7921, 5.3659, 5.5512]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.09776867926120758 
model_pd.l_d.mean(): -20.292797088623047 
model_pd.lagr.mean(): -20.19502830505371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5347], device='cuda:0')), ('power', tensor([-21.0608], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.09776867926120758
epoch£º331	 i:1 	 global-step:6621	 l-p:0.1356804221868515
epoch£º331	 i:2 	 global-step:6622	 l-p:0.156231090426445
epoch£º331	 i:3 	 global-step:6623	 l-p:0.12339995801448822
epoch£º331	 i:4 	 global-step:6624	 l-p:0.15760254859924316
epoch£º331	 i:5 	 global-step:6625	 l-p:0.17391479015350342
epoch£º331	 i:6 	 global-step:6626	 l-p:0.13267628848552704
epoch£º331	 i:7 	 global-step:6627	 l-p:-0.4693398177623749
epoch£º331	 i:8 	 global-step:6628	 l-p:0.12676046788692474
epoch£º331	 i:9 	 global-step:6629	 l-p:0.11863737553358078
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0915, 5.1092, 5.0641],
        [5.0915, 5.2662, 5.1921],
        [5.0915, 5.1571, 5.0844],
        [5.0915, 5.0895, 5.0913]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.12668456137180328 
model_pd.l_d.mean(): -20.287546157836914 
model_pd.lagr.mean(): -20.16086196899414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4532], device='cuda:0')), ('power', tensor([-20.9722], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.12668456137180328
epoch£º332	 i:1 	 global-step:6641	 l-p:0.11128873378038406
epoch£º332	 i:2 	 global-step:6642	 l-p:0.19991667568683624
epoch£º332	 i:3 	 global-step:6643	 l-p:0.10738241672515869
epoch£º332	 i:4 	 global-step:6644	 l-p:0.18301624059677124
epoch£º332	 i:5 	 global-step:6645	 l-p:0.13775862753391266
epoch£º332	 i:6 	 global-step:6646	 l-p:0.12967948615550995
epoch£º332	 i:7 	 global-step:6647	 l-p:0.14193859696388245
epoch£º332	 i:8 	 global-step:6648	 l-p:0.1582609862089157
epoch£º332	 i:9 	 global-step:6649	 l-p:0.11840757727622986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0872, 5.0872, 5.0872],
        [5.0872, 5.3328, 5.2811],
        [5.0872, 5.0872, 5.0872],
        [5.0872, 5.0872, 5.0872]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.1488911211490631 
model_pd.l_d.mean(): -20.366046905517578 
model_pd.lagr.mean(): -20.21715545654297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4301], device='cuda:0')), ('power', tensor([-21.0280], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.1488911211490631
epoch£º333	 i:1 	 global-step:6661	 l-p:0.11794079840183258
epoch£º333	 i:2 	 global-step:6662	 l-p:0.12035094201564789
epoch£º333	 i:3 	 global-step:6663	 l-p:0.26364633440971375
epoch£º333	 i:4 	 global-step:6664	 l-p:0.17107467353343964
epoch£º333	 i:5 	 global-step:6665	 l-p:0.1423659324645996
epoch£º333	 i:6 	 global-step:6666	 l-p:0.12368081510066986
epoch£º333	 i:7 	 global-step:6667	 l-p:-0.06869220733642578
epoch£º333	 i:8 	 global-step:6668	 l-p:0.13905392587184906
epoch£º333	 i:9 	 global-step:6669	 l-p:0.01849130541086197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9977, 4.9974, 4.9977],
        [4.9977, 4.9957, 4.9975],
        [4.9977, 5.0291, 4.9680],
        [4.9977, 4.9943, 4.9681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.14925888180732727 
model_pd.l_d.mean(): -20.888748168945312 
model_pd.lagr.mean(): -20.73948860168457 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4028], device='cuda:0')), ('power', tensor([-21.5285], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.14925888180732727
epoch£º334	 i:1 	 global-step:6681	 l-p:0.19684725999832153
epoch£º334	 i:2 	 global-step:6682	 l-p:0.1441064029932022
epoch£º334	 i:3 	 global-step:6683	 l-p:0.030801167711615562
epoch£º334	 i:4 	 global-step:6684	 l-p:0.1482001394033432
epoch£º334	 i:5 	 global-step:6685	 l-p:0.4138554036617279
epoch£º334	 i:6 	 global-step:6686	 l-p:0.1340281218290329
epoch£º334	 i:7 	 global-step:6687	 l-p:0.20994608104228973
epoch£º334	 i:8 	 global-step:6688	 l-p:0.12614907324314117
epoch£º334	 i:9 	 global-step:6689	 l-p:0.11140763759613037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1317, 5.1232, 5.1153],
        [5.1317, 5.8975, 6.2150],
        [5.1317, 5.1317, 5.1317],
        [5.1317, 5.9787, 6.3714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.1010633036494255 
model_pd.l_d.mean(): -19.245012283325195 
model_pd.lagr.mean(): -19.143949508666992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5014], device='cuda:0')), ('power', tensor([-19.9676], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.1010633036494255
epoch£º335	 i:1 	 global-step:6701	 l-p:0.12995491921901703
epoch£º335	 i:2 	 global-step:6702	 l-p:0.16093826293945312
epoch£º335	 i:3 	 global-step:6703	 l-p:0.15628285706043243
epoch£º335	 i:4 	 global-step:6704	 l-p:0.13313354551792145
epoch£º335	 i:5 	 global-step:6705	 l-p:0.13677901029586792
epoch£º335	 i:6 	 global-step:6706	 l-p:0.2997289001941681
epoch£º335	 i:7 	 global-step:6707	 l-p:0.11991357803344727
epoch£º335	 i:8 	 global-step:6708	 l-p:0.1374683827161789
epoch£º335	 i:9 	 global-step:6709	 l-p:0.12447038292884827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2039,  0.1200,  1.0000,  0.0706,
          1.0000,  0.5886, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1532,  0.0820,  1.0000,  0.0439,
          1.0000,  0.5351, 31.6228]], device='cuda:0')
 pt:tensor([[5.0107, 4.9977, 4.9935],
        [5.0107, 5.0091, 4.9790],
        [5.0107, 5.2444, 5.1890],
        [5.0107, 4.9987, 4.9899]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.11928611248731613 
model_pd.l_d.mean(): -19.383010864257812 
model_pd.lagr.mean(): -19.26372528076172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4680], device='cuda:0')), ('power', tensor([-20.0730], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.11928611248731613
epoch£º336	 i:1 	 global-step:6721	 l-p:0.07377086579799652
epoch£º336	 i:2 	 global-step:6722	 l-p:0.132987380027771
epoch£º336	 i:3 	 global-step:6723	 l-p:0.11570566147565842
epoch£º336	 i:4 	 global-step:6724	 l-p:0.14401203393936157
epoch£º336	 i:5 	 global-step:6725	 l-p:0.1349901705980301
epoch£º336	 i:6 	 global-step:6726	 l-p:-0.09219404309988022
epoch£º336	 i:7 	 global-step:6727	 l-p:0.1232949048280716
epoch£º336	 i:8 	 global-step:6728	 l-p:0.14489030838012695
epoch£º336	 i:9 	 global-step:6729	 l-p:0.011928443796932697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8487, 5.7064, 6.1428],
        [4.8487, 4.8314, 4.8348],
        [4.8487, 4.9208, 4.8373],
        [4.8487, 5.2690, 5.3290]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.13899879157543182 
model_pd.l_d.mean(): -18.459867477416992 
model_pd.lagr.mean(): -18.32086944580078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5763], device='cuda:0')), ('power', tensor([-19.2504], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.13899879157543182
epoch£º337	 i:1 	 global-step:6741	 l-p:-1.31647789478302
epoch£º337	 i:2 	 global-step:6742	 l-p:0.06598957628011703
epoch£º337	 i:3 	 global-step:6743	 l-p:0.13319145143032074
epoch£º337	 i:4 	 global-step:6744	 l-p:0.11428944021463394
epoch£º337	 i:5 	 global-step:6745	 l-p:0.13348785042762756
epoch£º337	 i:6 	 global-step:6746	 l-p:0.1323910653591156
epoch£º337	 i:7 	 global-step:6747	 l-p:0.15227167308330536
epoch£º337	 i:8 	 global-step:6748	 l-p:0.14920909702777863
epoch£º337	 i:9 	 global-step:6749	 l-p:0.31147292256355286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9095, 4.8937, 4.8829],
        [4.9095, 4.8955, 4.9014],
        [4.9095, 4.9057, 4.9090],
        [4.9095, 4.9095, 4.9095]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.15185488760471344 
model_pd.l_d.mean(): -20.285118103027344 
model_pd.lagr.mean(): -20.133262634277344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4978], device='cuda:0')), ('power', tensor([-21.0153], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.15185488760471344
epoch£º338	 i:1 	 global-step:6761	 l-p:0.12362360954284668
epoch£º338	 i:2 	 global-step:6762	 l-p:0.10222043097019196
epoch£º338	 i:3 	 global-step:6763	 l-p:0.13206207752227783
epoch£º338	 i:4 	 global-step:6764	 l-p:-0.0648769661784172
epoch£º338	 i:5 	 global-step:6765	 l-p:0.1560627967119217
epoch£º338	 i:6 	 global-step:6766	 l-p:0.10507857799530029
epoch£º338	 i:7 	 global-step:6767	 l-p:0.1741466522216797
epoch£º338	 i:8 	 global-step:6768	 l-p:0.10931292176246643
epoch£º338	 i:9 	 global-step:6769	 l-p:0.14946496486663818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0410, 5.6084, 5.7643],
        [5.0410, 5.0271, 5.0258],
        [5.0410, 5.0286, 5.0330],
        [5.0410, 5.0410, 5.0410]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.12530116736888885 
model_pd.l_d.mean(): -20.41271209716797 
model_pd.lagr.mean(): -20.287410736083984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4501], device='cuda:0')), ('power', tensor([-21.0956], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.12530116736888885
epoch£º339	 i:1 	 global-step:6781	 l-p:0.13687318563461304
epoch£º339	 i:2 	 global-step:6782	 l-p:0.12966662645339966
epoch£º339	 i:3 	 global-step:6783	 l-p:0.624984860420227
epoch£º339	 i:4 	 global-step:6784	 l-p:0.15545545518398285
epoch£º339	 i:5 	 global-step:6785	 l-p:-0.04354488104581833
epoch£º339	 i:6 	 global-step:6786	 l-p:0.14628860354423523
epoch£º339	 i:7 	 global-step:6787	 l-p:0.1267794817686081
epoch£º339	 i:8 	 global-step:6788	 l-p:0.15623392164707184
epoch£º339	 i:9 	 global-step:6789	 l-p:-0.04423300549387932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0463, 5.4268, 5.4467],
        [5.0463, 5.0445, 5.0461],
        [5.0463, 5.0462, 5.0463],
        [5.0463, 5.0338, 5.0231]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.1229880079627037 
model_pd.l_d.mean(): -19.941699981689453 
model_pd.lagr.mean(): -19.81871223449707 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4855], device='cuda:0')), ('power', tensor([-20.6557], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.1229880079627037
epoch£º340	 i:1 	 global-step:6801	 l-p:0.1356291025876999
epoch£º340	 i:2 	 global-step:6802	 l-p:0.3706977665424347
epoch£º340	 i:3 	 global-step:6803	 l-p:0.14159293472766876
epoch£º340	 i:4 	 global-step:6804	 l-p:0.12771323323249817
epoch£º340	 i:5 	 global-step:6805	 l-p:0.115513376891613
epoch£º340	 i:6 	 global-step:6806	 l-p:0.24223770201206207
epoch£º340	 i:7 	 global-step:6807	 l-p:0.12406076490879059
epoch£º340	 i:8 	 global-step:6808	 l-p:0.18346336483955383
epoch£º340	 i:9 	 global-step:6809	 l-p:0.117594413459301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1151, 5.1145, 5.1151],
        [5.1151, 5.1139, 5.0841],
        [5.1151, 5.5961, 5.6799],
        [5.1151, 5.6137, 5.7104]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.12960250675678253 
model_pd.l_d.mean(): -20.101703643798828 
model_pd.lagr.mean(): -19.97210121154785 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4634], device='cuda:0')), ('power', tensor([-20.7947], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.12960250675678253
epoch£º341	 i:1 	 global-step:6821	 l-p:0.14533764123916626
epoch£º341	 i:2 	 global-step:6822	 l-p:0.1369968205690384
epoch£º341	 i:3 	 global-step:6823	 l-p:0.1192738339304924
epoch£º341	 i:4 	 global-step:6824	 l-p:0.12949489057064056
epoch£º341	 i:5 	 global-step:6825	 l-p:0.12075282633304596
epoch£º341	 i:6 	 global-step:6826	 l-p:-0.06194531172513962
epoch£º341	 i:7 	 global-step:6827	 l-p:0.1462080031633377
epoch£º341	 i:8 	 global-step:6828	 l-p:0.08713103085756302
epoch£º341	 i:9 	 global-step:6829	 l-p:0.26412856578826904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9446, 4.9446, 4.9446],
        [4.9446, 5.4777, 5.6147],
        [4.9446, 5.6737, 5.9778],
        [4.9446, 5.4786, 5.6163]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.14679773151874542 
model_pd.l_d.mean(): -20.719470977783203 
model_pd.lagr.mean(): -20.572673797607422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4448], device='cuda:0')), ('power', tensor([-21.4003], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.14679773151874542
epoch£º342	 i:1 	 global-step:6841	 l-p:0.22124755382537842
epoch£º342	 i:2 	 global-step:6842	 l-p:0.14108461141586304
epoch£º342	 i:3 	 global-step:6843	 l-p:0.1248578429222107
epoch£º342	 i:4 	 global-step:6844	 l-p:0.13088814914226532
epoch£º342	 i:5 	 global-step:6845	 l-p:0.25093016028404236
epoch£º342	 i:6 	 global-step:6846	 l-p:-0.00919743999838829
epoch£º342	 i:7 	 global-step:6847	 l-p:0.07367601990699768
epoch£º342	 i:8 	 global-step:6848	 l-p:0.05433136969804764
epoch£º342	 i:9 	 global-step:6849	 l-p:0.13675501942634583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0592, 5.6245, 5.7769],
        [5.0592, 5.0592, 5.0592],
        [5.0592, 5.8150, 6.1307],
        [5.0592, 5.0476, 5.0534]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.13556937873363495 
model_pd.l_d.mean(): -20.74853515625 
model_pd.lagr.mean(): -20.612966537475586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4022], device='cuda:0')), ('power', tensor([-21.3862], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.13556937873363495
epoch£º343	 i:1 	 global-step:6861	 l-p:0.13164372742176056
epoch£º343	 i:2 	 global-step:6862	 l-p:0.13260744512081146
epoch£º343	 i:3 	 global-step:6863	 l-p:0.11892540007829666
epoch£º343	 i:4 	 global-step:6864	 l-p:0.13207337260246277
epoch£º343	 i:5 	 global-step:6865	 l-p:-1.0168522596359253
epoch£º343	 i:6 	 global-step:6866	 l-p:0.13001859188079834
epoch£º343	 i:7 	 global-step:6867	 l-p:-0.002220964292064309
epoch£º343	 i:8 	 global-step:6868	 l-p:0.12694436311721802
epoch£º343	 i:9 	 global-step:6869	 l-p:0.12958620488643646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228]], device='cuda:0')
 pt:tensor([[4.9986, 5.7331, 6.0360],
        [4.9986, 5.5646, 5.7230],
        [4.9986, 4.9876, 4.9645],
        [4.9986, 4.9860, 4.9663]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.14429856836795807 
model_pd.l_d.mean(): -20.77669334411621 
model_pd.lagr.mean(): -20.632394790649414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4171], device='cuda:0')), ('power', tensor([-21.4298], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.14429856836795807
epoch£º344	 i:1 	 global-step:6881	 l-p:0.09961892664432526
epoch£º344	 i:2 	 global-step:6882	 l-p:0.13202396035194397
epoch£º344	 i:3 	 global-step:6883	 l-p:0.12736888229846954
epoch£º344	 i:4 	 global-step:6884	 l-p:0.09991661459207535
epoch£º344	 i:5 	 global-step:6885	 l-p:0.15047423541545868
epoch£º344	 i:6 	 global-step:6886	 l-p:0.13322101533412933
epoch£º344	 i:7 	 global-step:6887	 l-p:0.08945643901824951
epoch£º344	 i:8 	 global-step:6888	 l-p:0.14843206107616425
epoch£º344	 i:9 	 global-step:6889	 l-p:-0.14490661025047302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8868, 4.8763, 4.8835],
        [4.8868, 5.5930, 5.8825],
        [4.8868, 4.8741, 4.8818],
        [4.8868, 4.9495, 4.8642]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.12447626888751984 
model_pd.l_d.mean(): -19.504487991333008 
model_pd.lagr.mean(): -19.38001251220703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5000], device='cuda:0')), ('power', tensor([-20.2284], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.12447626888751984
epoch£º345	 i:1 	 global-step:6901	 l-p:0.14045988023281097
epoch£º345	 i:2 	 global-step:6902	 l-p:0.12164599448442459
epoch£º345	 i:3 	 global-step:6903	 l-p:0.14630518853664398
epoch£º345	 i:4 	 global-step:6904	 l-p:0.1205194815993309
epoch£º345	 i:5 	 global-step:6905	 l-p:-0.4703902304172516
epoch£º345	 i:6 	 global-step:6906	 l-p:0.08647473901510239
epoch£º345	 i:7 	 global-step:6907	 l-p:0.1868588924407959
epoch£º345	 i:8 	 global-step:6908	 l-p:0.13316987454891205
epoch£º345	 i:9 	 global-step:6909	 l-p:0.12438348680734634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0357, 5.0296, 5.0344],
        [5.0357, 5.0356, 5.0357],
        [5.0357, 5.0218, 5.0278],
        [5.0357, 5.1566, 5.0696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.14727655053138733 
model_pd.l_d.mean(): -19.89808464050293 
model_pd.lagr.mean(): -19.750808715820312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5110], device='cuda:0')), ('power', tensor([-20.6375], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.14727655053138733
epoch£º346	 i:1 	 global-step:6921	 l-p:0.13151055574417114
epoch£º346	 i:2 	 global-step:6922	 l-p:0.14118923246860504
epoch£º346	 i:3 	 global-step:6923	 l-p:0.2179068773984909
epoch£º346	 i:4 	 global-step:6924	 l-p:0.13033820688724518
epoch£º346	 i:5 	 global-step:6925	 l-p:0.10478272289037704
epoch£º346	 i:6 	 global-step:6926	 l-p:0.12169820815324783
epoch£º346	 i:7 	 global-step:6927	 l-p:0.1287759691476822
epoch£º346	 i:8 	 global-step:6928	 l-p:0.13502317667007446
epoch£º346	 i:9 	 global-step:6929	 l-p:-0.25613778829574585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0072, 5.0072, 5.0072],
        [5.0072, 5.0855, 4.9990],
        [5.0072, 5.8020, 6.1587],
        [5.0072, 5.0043, 5.0069]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.12051793187856674 
model_pd.l_d.mean(): -19.96599769592285 
model_pd.lagr.mean(): -19.84547996520996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5060], device='cuda:0')), ('power', tensor([-20.7011], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:0.12051793187856674
epoch£º347	 i:1 	 global-step:6941	 l-p:0.07436883449554443
epoch£º347	 i:2 	 global-step:6942	 l-p:0.08209110051393509
epoch£º347	 i:3 	 global-step:6943	 l-p:0.07257545739412308
epoch£º347	 i:4 	 global-step:6944	 l-p:0.12686340510845184
epoch£º347	 i:5 	 global-step:6945	 l-p:0.16785697638988495
epoch£º347	 i:6 	 global-step:6946	 l-p:0.24729442596435547
epoch£º347	 i:7 	 global-step:6947	 l-p:0.1254015862941742
epoch£º347	 i:8 	 global-step:6948	 l-p:0.1191478967666626
epoch£º347	 i:9 	 global-step:6949	 l-p:0.13631170988082886
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0376, 5.0200, 5.0190],
        [5.0376, 5.0238, 5.0303],
        [5.0376, 5.0203, 5.0214],
        [5.0376, 5.0376, 5.0376]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.1363769918680191 
model_pd.l_d.mean(): -20.422988891601562 
model_pd.lagr.mean(): -20.286611557006836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4358], device='cuda:0')), ('power', tensor([-21.0914], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.1363769918680191
epoch£º348	 i:1 	 global-step:6961	 l-p:0.01323466282337904
epoch£º348	 i:2 	 global-step:6962	 l-p:0.16256290674209595
epoch£º348	 i:3 	 global-step:6963	 l-p:0.19360625743865967
epoch£º348	 i:4 	 global-step:6964	 l-p:0.1285417079925537
epoch£º348	 i:5 	 global-step:6965	 l-p:0.11962665617465973
epoch£º348	 i:6 	 global-step:6966	 l-p:0.14684852957725525
epoch£º348	 i:7 	 global-step:6967	 l-p:-0.0830613300204277
epoch£º348	 i:8 	 global-step:6968	 l-p:0.13897643983364105
epoch£º348	 i:9 	 global-step:6969	 l-p:0.05217697098851204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9754, 4.9754, 4.9754],
        [4.9754, 4.9744, 4.9753],
        [4.9754, 4.9741, 4.9753],
        [4.9754, 4.9606, 4.9679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.13897204399108887 
model_pd.l_d.mean(): -18.9002685546875 
model_pd.lagr.mean(): -18.76129722595215 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5457], device='cuda:0')), ('power', tensor([-19.6643], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.13897204399108887
epoch£º349	 i:1 	 global-step:6981	 l-p:0.12765219807624817
epoch£º349	 i:2 	 global-step:6982	 l-p:0.1343279778957367
epoch£º349	 i:3 	 global-step:6983	 l-p:0.15725940465927124
epoch£º349	 i:4 	 global-step:6984	 l-p:0.12436617910861969
epoch£º349	 i:5 	 global-step:6985	 l-p:0.14536701142787933
epoch£º349	 i:6 	 global-step:6986	 l-p:0.1583820879459381
epoch£º349	 i:7 	 global-step:6987	 l-p:0.0012597274035215378
epoch£º349	 i:8 	 global-step:6988	 l-p:0.13861249387264252
epoch£º349	 i:9 	 global-step:6989	 l-p:0.08466792106628418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8094, 4.8080, 4.8093],
        [4.8094, 5.2526, 5.3315],
        [4.8094, 5.1138, 5.0987],
        [4.8094, 5.2613, 5.3467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.22066277265548706 
model_pd.l_d.mean(): -20.495580673217773 
model_pd.lagr.mean(): -20.274917602539062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5149], device='cuda:0')), ('power', tensor([-21.2456], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.22066277265548706
epoch£º350	 i:1 	 global-step:7001	 l-p:0.1332194060087204
epoch£º350	 i:2 	 global-step:7002	 l-p:0.06897927820682526
epoch£º350	 i:3 	 global-step:7003	 l-p:0.11950469762086868
epoch£º350	 i:4 	 global-step:7004	 l-p:0.32638609409332275
epoch£º350	 i:5 	 global-step:7005	 l-p:0.13407979905605316
epoch£º350	 i:6 	 global-step:7006	 l-p:0.12223105132579803
epoch£º350	 i:7 	 global-step:7007	 l-p:0.06494151800870895
epoch£º350	 i:8 	 global-step:7008	 l-p:0.136912003159523
epoch£º350	 i:9 	 global-step:7009	 l-p:0.12378976494073868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0190, 5.0005, 5.0034],
        [5.0190, 4.9998, 4.9940],
        [5.0190, 5.1752, 5.0913],
        [5.0190, 5.1940, 5.1146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.001306829391978681 
model_pd.l_d.mean(): -20.17508888244629 
model_pd.lagr.mean(): -20.173782348632812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4881], device='cuda:0')), ('power', tensor([-20.8942], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:0.001306829391978681
epoch£º351	 i:1 	 global-step:7021	 l-p:-3.2398228645324707
epoch£º351	 i:2 	 global-step:7022	 l-p:0.11065483093261719
epoch£º351	 i:3 	 global-step:7023	 l-p:0.1401258111000061
epoch£º351	 i:4 	 global-step:7024	 l-p:0.12228526175022125
epoch£º351	 i:5 	 global-step:7025	 l-p:0.13286559283733368
epoch£º351	 i:6 	 global-step:7026	 l-p:0.15363892912864685
epoch£º351	 i:7 	 global-step:7027	 l-p:0.12016770988702774
epoch£º351	 i:8 	 global-step:7028	 l-p:0.07315652072429657
epoch£º351	 i:9 	 global-step:7029	 l-p:0.13423070311546326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9605, 4.9411, 4.9467],
        [4.9605, 5.0394, 4.9493],
        [4.9605, 4.9398, 4.9268],
        [4.9605, 5.1648, 5.0952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): 0.09330277889966965 
model_pd.l_d.mean(): -20.985002517700195 
model_pd.lagr.mean(): -20.891698837280273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4028], device='cuda:0')), ('power', tensor([-21.6258], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:0.09330277889966965
epoch£º352	 i:1 	 global-step:7041	 l-p:0.19038698077201843
epoch£º352	 i:2 	 global-step:7042	 l-p:0.14198602735996246
epoch£º352	 i:3 	 global-step:7043	 l-p:0.19432812929153442
epoch£º352	 i:4 	 global-step:7044	 l-p:0.1457679122686386
epoch£º352	 i:5 	 global-step:7045	 l-p:0.8809641599655151
epoch£º352	 i:6 	 global-step:7046	 l-p:0.5287237167358398
epoch£º352	 i:7 	 global-step:7047	 l-p:0.11346074938774109
epoch£º352	 i:8 	 global-step:7048	 l-p:0.13726496696472168
epoch£º352	 i:9 	 global-step:7049	 l-p:0.11856824904680252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1264, 5.1215, 5.1256],
        [5.1264, 5.1246, 5.1263],
        [5.1264, 5.1247, 5.1263],
        [5.1264, 5.1264, 5.1264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.12594211101531982 
model_pd.l_d.mean(): -20.453121185302734 
model_pd.lagr.mean(): -20.327178955078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4146], device='cuda:0')), ('power', tensor([-21.1001], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.12594211101531982
epoch£º353	 i:1 	 global-step:7061	 l-p:0.13180749118328094
epoch£º353	 i:2 	 global-step:7062	 l-p:0.17411361634731293
epoch£º353	 i:3 	 global-step:7063	 l-p:0.13923190534114838
epoch£º353	 i:4 	 global-step:7064	 l-p:0.1124710813164711
epoch£º353	 i:5 	 global-step:7065	 l-p:0.16560742259025574
epoch£º353	 i:6 	 global-step:7066	 l-p:0.1476992517709732
epoch£º353	 i:7 	 global-step:7067	 l-p:0.14113874733448029
epoch£º353	 i:8 	 global-step:7068	 l-p:-0.2365766316652298
epoch£º353	 i:9 	 global-step:7069	 l-p:0.11850068718194962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0104, 4.9931, 4.9999],
        [5.0104, 5.2778, 5.2335],
        [5.0104, 5.8324, 6.2133],
        [5.0104, 5.0052, 5.0095]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.12719261646270752 
model_pd.l_d.mean(): -20.749370574951172 
model_pd.lagr.mean(): -20.622177124023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4033], device='cuda:0')), ('power', tensor([-21.3882], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.12719261646270752
epoch£º354	 i:1 	 global-step:7081	 l-p:0.27100956439971924
epoch£º354	 i:2 	 global-step:7082	 l-p:0.07948670536279678
epoch£º354	 i:3 	 global-step:7083	 l-p:0.18174628913402557
epoch£º354	 i:4 	 global-step:7084	 l-p:0.1411455273628235
epoch£º354	 i:5 	 global-step:7085	 l-p:0.1352645754814148
epoch£º354	 i:6 	 global-step:7086	 l-p:0.14326411485671997
epoch£º354	 i:7 	 global-step:7087	 l-p:0.13853931427001953
epoch£º354	 i:8 	 global-step:7088	 l-p:0.12456890940666199
epoch£º354	 i:9 	 global-step:7089	 l-p:-0.04144681990146637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9873, 5.8080, 6.1902],
        [4.9873, 4.9745, 4.9825],
        [4.9873, 4.9843, 4.9870],
        [4.9873, 4.9862, 4.9872]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.13088449835777283 
model_pd.l_d.mean(): -20.143861770629883 
model_pd.lagr.mean(): -20.012977600097656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4742], device='cuda:0')), ('power', tensor([-20.8484], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.13088449835777283
epoch£º355	 i:1 	 global-step:7101	 l-p:0.12816105782985687
epoch£º355	 i:2 	 global-step:7102	 l-p:0.16048656404018402
epoch£º355	 i:3 	 global-step:7103	 l-p:0.1243227869272232
epoch£º355	 i:4 	 global-step:7104	 l-p:0.14757099747657776
epoch£º355	 i:5 	 global-step:7105	 l-p:0.16770526766777039
epoch£º355	 i:6 	 global-step:7106	 l-p:0.1499667465686798
epoch£º355	 i:7 	 global-step:7107	 l-p:0.08290542662143707
epoch£º355	 i:8 	 global-step:7108	 l-p:0.14037632942199707
epoch£º355	 i:9 	 global-step:7109	 l-p:0.08458925038576126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8209, 4.7923, 4.7907],
        [4.8209, 5.2296, 5.2816],
        [4.8209, 4.8209, 4.8209],
        [4.8209, 4.7921, 4.7884]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.16318842768669128 
model_pd.l_d.mean(): -19.890230178833008 
model_pd.lagr.mean(): -19.727041244506836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5186], device='cuda:0')), ('power', tensor([-20.6374], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.16318842768669128
epoch£º356	 i:1 	 global-step:7121	 l-p:0.1508585661649704
epoch£º356	 i:2 	 global-step:7122	 l-p:0.14803552627563477
epoch£º356	 i:3 	 global-step:7123	 l-p:0.19850561022758484
epoch£º356	 i:4 	 global-step:7124	 l-p:0.08032859861850739
epoch£º356	 i:5 	 global-step:7125	 l-p:0.10926871746778488
epoch£º356	 i:6 	 global-step:7126	 l-p:0.18251651525497437
epoch£º356	 i:7 	 global-step:7127	 l-p:0.06968478858470917
epoch£º356	 i:8 	 global-step:7128	 l-p:0.08855145424604416
epoch£º356	 i:9 	 global-step:7129	 l-p:0.1482565999031067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9211, 5.6688, 5.9903],
        [4.9211, 4.9194, 4.9210],
        [4.9211, 4.9132, 4.9195],
        [4.9211, 4.8996, 4.8737]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 10.56118392944336 
model_pd.l_d.mean(): -20.010089874267578 
model_pd.lagr.mean(): -9.448905944824219 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5415], device='cuda:0')), ('power', tensor([-20.7820], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:10.56118392944336
epoch£º357	 i:1 	 global-step:7141	 l-p:0.1545025259256363
epoch£º357	 i:2 	 global-step:7142	 l-p:0.1372346729040146
epoch£º357	 i:3 	 global-step:7143	 l-p:1.1660794019699097
epoch£º357	 i:4 	 global-step:7144	 l-p:0.0673835352063179
epoch£º357	 i:5 	 global-step:7145	 l-p:0.1378336399793625
epoch£º357	 i:6 	 global-step:7146	 l-p:0.13203997910022736
epoch£º357	 i:7 	 global-step:7147	 l-p:0.12692533433437347
epoch£º357	 i:8 	 global-step:7148	 l-p:0.11663611978292465
epoch£º357	 i:9 	 global-step:7149	 l-p:0.12827561795711517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9396, 4.9388, 4.9396],
        [4.9396, 4.9396, 4.9396],
        [4.9396, 4.9420, 4.8804],
        [4.9396, 5.5219, 5.6977]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.079487644135952 
model_pd.l_d.mean(): -20.62091636657715 
model_pd.lagr.mean(): -20.54142951965332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4559], device='cuda:0')), ('power', tensor([-21.3120], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.079487644135952
epoch£º358	 i:1 	 global-step:7161	 l-p:0.03274962306022644
epoch£º358	 i:2 	 global-step:7162	 l-p:0.15525905787944794
epoch£º358	 i:3 	 global-step:7163	 l-p:0.13316689431667328
epoch£º358	 i:4 	 global-step:7164	 l-p:0.09348681569099426
epoch£º358	 i:5 	 global-step:7165	 l-p:0.12957490980625153
epoch£º358	 i:6 	 global-step:7166	 l-p:0.14744436740875244
epoch£º358	 i:7 	 global-step:7167	 l-p:0.13386838138103485
epoch£º358	 i:8 	 global-step:7168	 l-p:0.27129533886909485
epoch£º358	 i:9 	 global-step:7169	 l-p:0.190238818526268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9714, 4.9543, 4.9631],
        [4.9714, 4.9714, 4.9714],
        [4.9714, 4.9605, 4.9159],
        [4.9714, 4.9714, 4.9714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.1409544199705124 
model_pd.l_d.mean(): -20.287439346313477 
model_pd.lagr.mean(): -20.146484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4766], device='cuda:0')), ('power', tensor([-20.9960], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.1409544199705124
epoch£º359	 i:1 	 global-step:7181	 l-p:0.12728455662727356
epoch£º359	 i:2 	 global-step:7182	 l-p:0.1319090574979782
epoch£º359	 i:3 	 global-step:7183	 l-p:0.12423454970121384
epoch£º359	 i:4 	 global-step:7184	 l-p:0.4787868857383728
epoch£º359	 i:5 	 global-step:7185	 l-p:0.14463669061660767
epoch£º359	 i:6 	 global-step:7186	 l-p:0.239109069108963
epoch£º359	 i:7 	 global-step:7187	 l-p:0.09534935653209686
epoch£º359	 i:8 	 global-step:7188	 l-p:0.17043481767177582
epoch£º359	 i:9 	 global-step:7189	 l-p:0.14180180430412292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9023, 4.8765, 4.8535],
        [4.9023, 4.8772, 4.8519],
        [4.9023, 4.8740, 4.8692],
        [4.9023, 4.9022, 4.9023]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.13115181028842926 
model_pd.l_d.mean(): -20.130943298339844 
model_pd.lagr.mean(): -19.999792098999023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5237], device='cuda:0')), ('power', tensor([-20.8860], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.13115181028842926
epoch£º360	 i:1 	 global-step:7201	 l-p:0.11310369521379471
epoch£º360	 i:2 	 global-step:7202	 l-p:0.18676914274692535
epoch£º360	 i:3 	 global-step:7203	 l-p:0.015692973509430885
epoch£º360	 i:4 	 global-step:7204	 l-p:0.16189128160476685
epoch£º360	 i:5 	 global-step:7205	 l-p:0.14986532926559448
epoch£º360	 i:6 	 global-step:7206	 l-p:0.14660057425498962
epoch£º360	 i:7 	 global-step:7207	 l-p:0.0814402624964714
epoch£º360	 i:8 	 global-step:7208	 l-p:0.13105688989162445
epoch£º360	 i:9 	 global-step:7209	 l-p:0.14099276065826416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8488, 4.8488, 4.8488],
        [4.8488, 4.8488, 4.8488],
        [4.8488, 4.8487, 4.8488],
        [4.8488, 4.8473, 4.8487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.1771438866853714 
model_pd.l_d.mean(): -20.64250373840332 
model_pd.lagr.mean(): -20.465360641479492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4919], device='cuda:0')), ('power', tensor([-21.3706], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.1771438866853714
epoch£º361	 i:1 	 global-step:7221	 l-p:0.19022910296916962
epoch£º361	 i:2 	 global-step:7222	 l-p:-0.14027199149131775
epoch£º361	 i:3 	 global-step:7223	 l-p:0.0932302474975586
epoch£º361	 i:4 	 global-step:7224	 l-p:0.11247917264699936
epoch£º361	 i:5 	 global-step:7225	 l-p:0.11557848006486893
epoch£º361	 i:6 	 global-step:7226	 l-p:0.1288747638463974
epoch£º361	 i:7 	 global-step:7227	 l-p:0.1330443173646927
epoch£º361	 i:8 	 global-step:7228	 l-p:-0.1988501250743866
epoch£º361	 i:9 	 global-step:7229	 l-p:0.35805949568748474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0858, 5.0721, 5.0376],
        [5.0858, 5.0858, 5.0858],
        [5.0858, 5.0858, 5.0858],
        [5.0858, 5.5284, 5.5847]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.13107885420322418 
model_pd.l_d.mean(): -20.359325408935547 
model_pd.lagr.mean(): -20.228246688842773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4434], device='cuda:0')), ('power', tensor([-21.0348], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.13107885420322418
epoch£º362	 i:1 	 global-step:7241	 l-p:0.19712162017822266
epoch£º362	 i:2 	 global-step:7242	 l-p:0.13606581091880798
epoch£º362	 i:3 	 global-step:7243	 l-p:0.13803178071975708
epoch£º362	 i:4 	 global-step:7244	 l-p:0.18068315088748932
epoch£º362	 i:5 	 global-step:7245	 l-p:0.13203902542591095
epoch£º362	 i:6 	 global-step:7246	 l-p:0.14051410555839539
epoch£º362	 i:7 	 global-step:7247	 l-p:0.1336398422718048
epoch£º362	 i:8 	 global-step:7248	 l-p:0.12312671542167664
epoch£º362	 i:9 	 global-step:7249	 l-p:0.13787217438220978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1221, 5.1000, 5.0976],
        [5.1221, 5.1216, 5.1221],
        [5.1221, 5.1208, 5.1220],
        [5.1221, 5.1222, 5.0696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.19525213539600372 
model_pd.l_d.mean(): -20.695043563842773 
model_pd.lagr.mean(): -20.499792098999023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3927], device='cuda:0')), ('power', tensor([-21.3224], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.19525213539600372
epoch£º363	 i:1 	 global-step:7261	 l-p:0.1503245085477829
epoch£º363	 i:2 	 global-step:7262	 l-p:0.08308997750282288
epoch£º363	 i:3 	 global-step:7263	 l-p:0.11465302854776382
epoch£º363	 i:4 	 global-step:7264	 l-p:0.1273464858531952
epoch£º363	 i:5 	 global-step:7265	 l-p:0.14186258614063263
epoch£º363	 i:6 	 global-step:7266	 l-p:0.11655372381210327
epoch£º363	 i:7 	 global-step:7267	 l-p:0.15534579753875732
epoch£º363	 i:8 	 global-step:7268	 l-p:0.12029444426298141
epoch£º363	 i:9 	 global-step:7269	 l-p:0.12345965206623077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1273, 6.0423, 6.4966],
        [5.1273, 5.1049, 5.1025],
        [5.1273, 5.1272, 5.1273],
        [5.1273, 5.1380, 5.0742]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.14763623476028442 
model_pd.l_d.mean(): -19.332658767700195 
model_pd.lagr.mean(): -19.185022354125977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4663], device='cuda:0')), ('power', tensor([-20.0203], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.14763623476028442
epoch£º364	 i:1 	 global-step:7281	 l-p:0.12147051841020584
epoch£º364	 i:2 	 global-step:7282	 l-p:-1.1708457469940186
epoch£º364	 i:3 	 global-step:7283	 l-p:0.11935017257928848
epoch£º364	 i:4 	 global-step:7284	 l-p:0.13846240937709808
epoch£º364	 i:5 	 global-step:7285	 l-p:0.08939308673143387
epoch£º364	 i:6 	 global-step:7286	 l-p:0.05460464209318161
epoch£º364	 i:7 	 global-step:7287	 l-p:0.17633666098117828
epoch£º364	 i:8 	 global-step:7288	 l-p:0.1329660266637802
epoch£º364	 i:9 	 global-step:7289	 l-p:0.044817257672548294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8606, 4.8606, 4.8606],
        [4.8606, 4.8395, 4.7911],
        [4.8606, 4.8293, 4.8057],
        [4.8606, 4.8544, 4.8597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.09835682809352875 
model_pd.l_d.mean(): -20.775630950927734 
model_pd.lagr.mean(): -20.677274703979492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4648], device='cuda:0')), ('power', tensor([-21.4775], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.09835682809352875
epoch£º365	 i:1 	 global-step:7301	 l-p:0.1763957142829895
epoch£º365	 i:2 	 global-step:7302	 l-p:0.15020428597927094
epoch£º365	 i:3 	 global-step:7303	 l-p:0.11096808314323425
epoch£º365	 i:4 	 global-step:7304	 l-p:0.14583347737789154
epoch£º365	 i:5 	 global-step:7305	 l-p:0.13737422227859497
epoch£º365	 i:6 	 global-step:7306	 l-p:-0.37935203313827515
epoch£º365	 i:7 	 global-step:7307	 l-p:0.113532155752182
epoch£º365	 i:8 	 global-step:7308	 l-p:0.1262608915567398
epoch£º365	 i:9 	 global-step:7309	 l-p:0.134588822722435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9387, 4.9350, 4.9383],
        [4.9387, 5.6240, 5.8866],
        [4.9387, 5.7121, 6.0537],
        [4.9387, 4.9357, 4.9384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.11651928722858429 
model_pd.l_d.mean(): -18.692138671875 
model_pd.lagr.mean(): -18.575618743896484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6126], device='cuda:0')), ('power', tensor([-19.5223], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.11651928722858429
epoch£º366	 i:1 	 global-step:7321	 l-p:0.13704675436019897
epoch£º366	 i:2 	 global-step:7322	 l-p:0.13708913326263428
epoch£º366	 i:3 	 global-step:7323	 l-p:0.13775470852851868
epoch£º366	 i:4 	 global-step:7324	 l-p:-0.6143121719360352
epoch£º366	 i:5 	 global-step:7325	 l-p:0.13272304832935333
epoch£º366	 i:6 	 global-step:7326	 l-p:0.0952354148030281
epoch£º366	 i:7 	 global-step:7327	 l-p:0.14096689224243164
epoch£º366	 i:8 	 global-step:7328	 l-p:0.06943309307098389
epoch£º366	 i:9 	 global-step:7329	 l-p:0.12541763484477997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228]], device='cuda:0')
 pt:tensor([[5.0804, 5.8281, 6.1312],
        [5.0804, 5.0676, 5.0239],
        [5.0804, 5.0560, 5.0393],
        [5.0804, 5.1689, 5.0715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.22983042895793915 
model_pd.l_d.mean(): -19.335235595703125 
model_pd.lagr.mean(): -19.105405807495117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5019], device='cuda:0')), ('power', tensor([-20.0593], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.22983042895793915
epoch£º367	 i:1 	 global-step:7341	 l-p:0.14309446513652802
epoch£º367	 i:2 	 global-step:7342	 l-p:0.142441064119339
epoch£º367	 i:3 	 global-step:7343	 l-p:0.14376996457576752
epoch£º367	 i:4 	 global-step:7344	 l-p:0.13071605563163757
epoch£º367	 i:5 	 global-step:7345	 l-p:0.13002942502498627
epoch£º367	 i:6 	 global-step:7346	 l-p:0.14303115010261536
epoch£º367	 i:7 	 global-step:7347	 l-p:0.12053368240594864
epoch£º367	 i:8 	 global-step:7348	 l-p:0.10060989111661911
epoch£º367	 i:9 	 global-step:7349	 l-p:0.12951025366783142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2097, 5.2097, 5.2097],
        [5.2097, 5.2097, 5.2097],
        [5.2097, 5.2075, 5.2095],
        [5.2097, 5.2612, 5.1737]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.15622788667678833 
model_pd.l_d.mean(): -20.297607421875 
model_pd.lagr.mean(): -20.141380310058594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4217], device='cuda:0')), ('power', tensor([-20.9502], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.15622788667678833
epoch£º368	 i:1 	 global-step:7361	 l-p:0.08484145253896713
epoch£º368	 i:2 	 global-step:7362	 l-p:0.12413938343524933
epoch£º368	 i:3 	 global-step:7363	 l-p:0.2029232531785965
epoch£º368	 i:4 	 global-step:7364	 l-p:0.11636590957641602
epoch£º368	 i:5 	 global-step:7365	 l-p:0.1356782466173172
epoch£º368	 i:6 	 global-step:7366	 l-p:0.1576336771249771
epoch£º368	 i:7 	 global-step:7367	 l-p:0.11756458133459091
epoch£º368	 i:8 	 global-step:7368	 l-p:0.109035424888134
epoch£º368	 i:9 	 global-step:7369	 l-p:0.18283846974372864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0187, 5.1307, 5.0313],
        [5.0187, 5.0169, 5.0186],
        [5.0187, 5.0187, 5.0187],
        [5.0187, 5.7471, 6.0389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.14587946236133575 
model_pd.l_d.mean(): -19.733905792236328 
model_pd.lagr.mean(): -19.58802604675293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5403], device='cuda:0')), ('power', tensor([-20.5015], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.14587946236133575
epoch£º369	 i:1 	 global-step:7381	 l-p:0.1236111968755722
epoch£º369	 i:2 	 global-step:7382	 l-p:0.08320187032222748
epoch£º369	 i:3 	 global-step:7383	 l-p:0.17474646866321564
epoch£º369	 i:4 	 global-step:7384	 l-p:0.1276528239250183
epoch£º369	 i:5 	 global-step:7385	 l-p:0.12349610030651093
epoch£º369	 i:6 	 global-step:7386	 l-p:0.16878727078437805
epoch£º369	 i:7 	 global-step:7387	 l-p:0.09311693906784058
epoch£º369	 i:8 	 global-step:7388	 l-p:-3.332221031188965
epoch£º369	 i:9 	 global-step:7389	 l-p:0.12787896394729614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0185, 5.0605, 4.9668],
        [5.0185, 5.0161, 5.0183],
        [5.0185, 5.5358, 5.6511],
        [5.0185, 4.9885, 4.9871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.1319153606891632 
model_pd.l_d.mean(): -17.956472396850586 
model_pd.lagr.mean(): -17.824556350708008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5623], device='cuda:0')), ('power', tensor([-18.7272], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.1319153606891632
epoch£º370	 i:1 	 global-step:7401	 l-p:0.136655792593956
epoch£º370	 i:2 	 global-step:7402	 l-p:0.07092902809381485
epoch£º370	 i:3 	 global-step:7403	 l-p:0.13786481320858002
epoch£º370	 i:4 	 global-step:7404	 l-p:0.13357096910476685
epoch£º370	 i:5 	 global-step:7405	 l-p:0.13489730656147003
epoch£º370	 i:6 	 global-step:7406	 l-p:0.15222454071044922
epoch£º370	 i:7 	 global-step:7407	 l-p:0.11683990806341171
epoch£º370	 i:8 	 global-step:7408	 l-p:-5.9689788818359375
epoch£º370	 i:9 	 global-step:7409	 l-p:0.12219968438148499
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0346, 5.0346, 5.0346],
        [5.0346, 5.0334, 5.0345],
        [5.0346, 5.7565, 6.0407],
        [5.0346, 5.0236, 5.0318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): -0.09822794049978256 
model_pd.l_d.mean(): -20.284130096435547 
model_pd.lagr.mean(): -20.38235855102539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-20.9851], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:-0.09822794049978256
epoch£º371	 i:1 	 global-step:7421	 l-p:0.12920889258384705
epoch£º371	 i:2 	 global-step:7422	 l-p:0.12136779725551605
epoch£º371	 i:3 	 global-step:7423	 l-p:0.14552123844623566
epoch£º371	 i:4 	 global-step:7424	 l-p:0.11168878525495529
epoch£º371	 i:5 	 global-step:7425	 l-p:-0.0919174775481224
epoch£º371	 i:6 	 global-step:7426	 l-p:0.11761238425970078
epoch£º371	 i:7 	 global-step:7427	 l-p:0.1987946629524231
epoch£º371	 i:8 	 global-step:7428	 l-p:0.1298416554927826
epoch£º371	 i:9 	 global-step:7429	 l-p:0.12190479040145874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8475, 5.6346, 5.9970],
        [4.8475, 4.8475, 4.8475],
        [4.8475, 4.8461, 4.8474],
        [4.8475, 4.8540, 4.7659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.130162313580513 
model_pd.l_d.mean(): -19.42588996887207 
model_pd.lagr.mean(): -19.295726776123047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5541], device='cuda:0')), ('power', tensor([-20.2043], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.130162313580513
epoch£º372	 i:1 	 global-step:7441	 l-p:0.06889308989048004
epoch£º372	 i:2 	 global-step:7442	 l-p:0.13453997671604156
epoch£º372	 i:3 	 global-step:7443	 l-p:0.15795928239822388
epoch£º372	 i:4 	 global-step:7444	 l-p:0.18108871579170227
epoch£º372	 i:5 	 global-step:7445	 l-p:0.13206292688846588
epoch£º372	 i:6 	 global-step:7446	 l-p:0.2049425095319748
epoch£º372	 i:7 	 global-step:7447	 l-p:0.1646079421043396
epoch£º372	 i:8 	 global-step:7448	 l-p:0.1418546438217163
epoch£º372	 i:9 	 global-step:7449	 l-p:0.0855761393904686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8498, 5.5047, 5.7481],
        [4.8498, 4.8310, 4.8431],
        [4.8498, 4.8498, 4.8498],
        [4.8498, 4.8896, 4.7865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.14398322999477386 
model_pd.l_d.mean(): -19.02362823486328 
model_pd.lagr.mean(): -18.8796443939209 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5751], device='cuda:0')), ('power', tensor([-19.8191], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.14398322999477386
epoch£º373	 i:1 	 global-step:7461	 l-p:-0.042628392577171326
epoch£º373	 i:2 	 global-step:7462	 l-p:0.0807863250374794
epoch£º373	 i:3 	 global-step:7463	 l-p:0.15704263746738434
epoch£º373	 i:4 	 global-step:7464	 l-p:0.1183767020702362
epoch£º373	 i:5 	 global-step:7465	 l-p:0.13327309489250183
epoch£º373	 i:6 	 global-step:7466	 l-p:0.14464379847049713
epoch£º373	 i:7 	 global-step:7467	 l-p:0.15973339974880219
epoch£º373	 i:8 	 global-step:7468	 l-p:0.14679782092571259
epoch£º373	 i:9 	 global-step:7469	 l-p:0.10243958979845047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9573, 4.9543, 4.8781],
        [4.9573, 4.9326, 4.9441],
        [4.9573, 4.9381, 4.9497],
        [4.9573, 5.7144, 6.0372]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.09906873852014542 
model_pd.l_d.mean(): -20.609960556030273 
model_pd.lagr.mean(): -20.51089096069336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4523], device='cuda:0')), ('power', tensor([-21.2972], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.09906873852014542
epoch£º374	 i:1 	 global-step:7481	 l-p:0.23398932814598083
epoch£º374	 i:2 	 global-step:7482	 l-p:0.06191770359873772
epoch£º374	 i:3 	 global-step:7483	 l-p:0.12594936788082123
epoch£º374	 i:4 	 global-step:7484	 l-p:0.13861773908138275
epoch£º374	 i:5 	 global-step:7485	 l-p:0.13526268303394318
epoch£º374	 i:6 	 global-step:7486	 l-p:0.09788578003644943
epoch£º374	 i:7 	 global-step:7487	 l-p:0.10802794247865677
epoch£º374	 i:8 	 global-step:7488	 l-p:0.14086385071277618
epoch£º374	 i:9 	 global-step:7489	 l-p:0.06715752184391022
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9056, 4.9017, 4.9052],
        [4.9056, 4.9056, 4.9056],
        [4.9056, 4.9010, 4.9051],
        [4.9056, 4.9055, 4.9056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.07005136460065842 
model_pd.l_d.mean(): -20.46990203857422 
model_pd.lagr.mean(): -20.399850845336914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4631], device='cuda:0')), ('power', tensor([-21.1667], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:0.07005136460065842
epoch£º375	 i:1 	 global-step:7501	 l-p:0.37070608139038086
epoch£º375	 i:2 	 global-step:7502	 l-p:0.14867402613162994
epoch£º375	 i:3 	 global-step:7503	 l-p:0.12173190712928772
epoch£º375	 i:4 	 global-step:7504	 l-p:0.12726794183254242
epoch£º375	 i:5 	 global-step:7505	 l-p:0.1332586407661438
epoch£º375	 i:6 	 global-step:7506	 l-p:0.18709345161914825
epoch£º375	 i:7 	 global-step:7507	 l-p:0.13227377831935883
epoch£º375	 i:8 	 global-step:7508	 l-p:0.14610740542411804
epoch£º375	 i:9 	 global-step:7509	 l-p:0.11689258366823196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0618, 5.0332, 5.0401],
        [5.0618, 5.1066, 5.0087],
        [5.0618, 5.0617, 5.0618],
        [5.0618, 5.0404, 4.9941]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.11039013415575027 
model_pd.l_d.mean(): -19.675962448120117 
model_pd.lagr.mean(): -19.56557273864746 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4979], device='cuda:0')), ('power', tensor([-20.3996], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.11039013415575027
epoch£º376	 i:1 	 global-step:7521	 l-p:0.16780641674995422
epoch£º376	 i:2 	 global-step:7522	 l-p:0.16701588034629822
epoch£º376	 i:3 	 global-step:7523	 l-p:0.13313733041286469
epoch£º376	 i:4 	 global-step:7524	 l-p:0.32726019620895386
epoch£º376	 i:5 	 global-step:7525	 l-p:0.13619670271873474
epoch£º376	 i:6 	 global-step:7526	 l-p:0.1349981427192688
epoch£º376	 i:7 	 global-step:7527	 l-p:0.11021336913108826
epoch£º376	 i:8 	 global-step:7528	 l-p:0.13255183398723602
epoch£º376	 i:9 	 global-step:7529	 l-p:-0.09093200415372849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0543, 5.0372, 4.9815],
        [5.0543, 5.0255, 5.0333],
        [5.0543, 5.0287, 5.0390],
        [5.0543, 5.7430, 5.9949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): -0.01648051291704178 
model_pd.l_d.mean(): -20.280054092407227 
model_pd.lagr.mean(): -20.29653549194336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4462], device='cuda:0')), ('power', tensor([-20.9575], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:-0.01648051291704178
epoch£º377	 i:1 	 global-step:7541	 l-p:0.025537818670272827
epoch£º377	 i:2 	 global-step:7542	 l-p:0.1661912500858307
epoch£º377	 i:3 	 global-step:7543	 l-p:0.2352653443813324
epoch£º377	 i:4 	 global-step:7544	 l-p:0.11495911329984665
epoch£º377	 i:5 	 global-step:7545	 l-p:0.12913818657398224
epoch£º377	 i:6 	 global-step:7546	 l-p:0.14149123430252075
epoch£º377	 i:7 	 global-step:7547	 l-p:0.13148368895053864
epoch£º377	 i:8 	 global-step:7548	 l-p:0.07340298593044281
epoch£º377	 i:9 	 global-step:7549	 l-p:0.12134192883968353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2202, 5.2533, 5.1655],
        [5.2202, 5.2202, 5.2202],
        [5.2202, 5.1927, 5.1843],
        [5.2202, 5.1982, 5.2069]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.13632941246032715 
model_pd.l_d.mean(): -19.896894454956055 
model_pd.lagr.mean(): -19.76056480407715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4589], device='cuda:0')), ('power', tensor([-20.5832], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.13632941246032715
epoch£º378	 i:1 	 global-step:7561	 l-p:0.11177536100149155
epoch£º378	 i:2 	 global-step:7562	 l-p:0.13184534013271332
epoch£º378	 i:3 	 global-step:7563	 l-p:0.13733458518981934
epoch£º378	 i:4 	 global-step:7564	 l-p:0.10580471903085709
epoch£º378	 i:5 	 global-step:7565	 l-p:0.15556560456752777
epoch£º378	 i:6 	 global-step:7566	 l-p:0.14390884339809418
epoch£º378	 i:7 	 global-step:7567	 l-p:0.12419184297323227
epoch£º378	 i:8 	 global-step:7568	 l-p:0.12448497861623764
epoch£º378	 i:9 	 global-step:7569	 l-p:0.13485823571681976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9992, 4.9663, 4.9322],
        [4.9992, 4.9992, 4.9992],
        [4.9992, 4.9900, 4.9974],
        [4.9992, 4.9761, 4.9212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.14658163487911224 
model_pd.l_d.mean(): -19.108694076538086 
model_pd.lagr.mean(): -18.962112426757812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5347], device='cuda:0')), ('power', tensor([-19.8638], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.14658163487911224
epoch£º379	 i:1 	 global-step:7581	 l-p:0.09324870258569717
epoch£º379	 i:2 	 global-step:7582	 l-p:0.1244860365986824
epoch£º379	 i:3 	 global-step:7583	 l-p:0.09140878170728683
epoch£º379	 i:4 	 global-step:7584	 l-p:0.14162202179431915
epoch£º379	 i:5 	 global-step:7585	 l-p:0.14036628603935242
epoch£º379	 i:6 	 global-step:7586	 l-p:0.07653012126684189
epoch£º379	 i:7 	 global-step:7587	 l-p:0.13409070670604706
epoch£º379	 i:8 	 global-step:7588	 l-p:0.14801010489463806
epoch£º379	 i:9 	 global-step:7589	 l-p:0.12383312731981277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7714, 4.9186, 4.8205],
        [4.7714, 4.7573, 4.7680],
        [4.7714, 4.7711, 4.7714],
        [4.7714, 4.7684, 4.7711]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.15761883556842804 
model_pd.l_d.mean(): -20.415058135986328 
model_pd.lagr.mean(): -20.25743865966797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5206], device='cuda:0')), ('power', tensor([-21.1700], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.15761883556842804
epoch£º380	 i:1 	 global-step:7601	 l-p:0.18592773377895355
epoch£º380	 i:2 	 global-step:7602	 l-p:0.1298229694366455
epoch£º380	 i:3 	 global-step:7603	 l-p:0.12950967252254486
epoch£º380	 i:4 	 global-step:7604	 l-p:0.12223894149065018
epoch£º380	 i:5 	 global-step:7605	 l-p:0.1614774912595749
epoch£º380	 i:6 	 global-step:7606	 l-p:0.10793345421552658
epoch£º380	 i:7 	 global-step:7607	 l-p:0.028407497331500053
epoch£º380	 i:8 	 global-step:7608	 l-p:-0.06226600334048271
epoch£º380	 i:9 	 global-step:7609	 l-p:0.7243369817733765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7286, 4.7285, 4.7286],
        [4.7286, 5.1254, 5.1714],
        [4.7286, 4.6917, 4.7061],
        [4.7286, 4.6759, 4.6575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.35552504658699036 
model_pd.l_d.mean(): -20.108680725097656 
model_pd.lagr.mean(): -19.753154754638672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5972], device='cuda:0')), ('power', tensor([-20.9385], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.35552504658699036
epoch£º381	 i:1 	 global-step:7621	 l-p:0.2141840159893036
epoch£º381	 i:2 	 global-step:7622	 l-p:0.10454245656728745
epoch£º381	 i:3 	 global-step:7623	 l-p:0.08701438456773758
epoch£º381	 i:4 	 global-step:7624	 l-p:0.3192328214645386
epoch£º381	 i:5 	 global-step:7625	 l-p:0.12678097188472748
epoch£º381	 i:6 	 global-step:7626	 l-p:0.12903641164302826
epoch£º381	 i:7 	 global-step:7627	 l-p:0.17995865643024445
epoch£º381	 i:8 	 global-step:7628	 l-p:0.11091455072164536
epoch£º381	 i:9 	 global-step:7629	 l-p:0.13890567421913147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1340, 5.1003, 5.0965],
        [5.1340, 5.1338, 5.1340],
        [5.1340, 5.1333, 5.1340],
        [5.1340, 5.1018, 5.0784]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.12883344292640686 
model_pd.l_d.mean(): -20.783708572387695 
model_pd.lagr.mean(): -20.654874801635742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3701], device='cuda:0')), ('power', tensor([-21.3889], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.12883344292640686
epoch£º382	 i:1 	 global-step:7641	 l-p:0.14465934038162231
epoch£º382	 i:2 	 global-step:7642	 l-p:0.15217378735542297
epoch£º382	 i:3 	 global-step:7643	 l-p:0.13495904207229614
epoch£º382	 i:4 	 global-step:7644	 l-p:-0.0892820730805397
epoch£º382	 i:5 	 global-step:7645	 l-p:0.11537223309278488
epoch£º382	 i:6 	 global-step:7646	 l-p:0.06119420751929283
epoch£º382	 i:7 	 global-step:7647	 l-p:0.18012495338916779
epoch£º382	 i:8 	 global-step:7648	 l-p:-0.0029259109869599342
epoch£º382	 i:9 	 global-step:7649	 l-p:0.10905228555202484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1053, 5.1025, 5.1050],
        [5.1053, 5.1050, 5.0260],
        [5.1053, 5.1053, 5.1053],
        [5.1053, 5.0761, 5.0856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.12736882269382477 
model_pd.l_d.mean(): -20.190759658813477 
model_pd.lagr.mean(): -20.063390731811523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4499], device='cuda:0')), ('power', tensor([-20.8710], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.12736882269382477
epoch£º383	 i:1 	 global-step:7661	 l-p:0.305972158908844
epoch£º383	 i:2 	 global-step:7662	 l-p:0.12966345250606537
epoch£º383	 i:3 	 global-step:7663	 l-p:0.12156786024570465
epoch£º383	 i:4 	 global-step:7664	 l-p:0.08435388654470444
epoch£º383	 i:5 	 global-step:7665	 l-p:0.13175801932811737
epoch£º383	 i:6 	 global-step:7666	 l-p:0.14211715757846832
epoch£º383	 i:7 	 global-step:7667	 l-p:0.13485054671764374
epoch£º383	 i:8 	 global-step:7668	 l-p:0.16591215133666992
epoch£º383	 i:9 	 global-step:7669	 l-p:-1.6731305122375488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9920, 5.5179, 5.6395],
        [4.9920, 4.9560, 4.9150],
        [4.9920, 5.2422, 5.1814],
        [4.9920, 4.9683, 4.9815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.2763061821460724 
model_pd.l_d.mean(): -20.764375686645508 
model_pd.lagr.mean(): -20.488069534301758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4372], device='cuda:0')), ('power', tensor([-21.4379], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.2763061821460724
epoch£º384	 i:1 	 global-step:7681	 l-p:0.15599824488162994
epoch£º384	 i:2 	 global-step:7682	 l-p:0.13314393162727356
epoch£º384	 i:3 	 global-step:7683	 l-p:0.15828357636928558
epoch£º384	 i:4 	 global-step:7684	 l-p:0.13882605731487274
epoch£º384	 i:5 	 global-step:7685	 l-p:0.10829661786556244
epoch£º384	 i:6 	 global-step:7686	 l-p:0.1092081218957901
epoch£º384	 i:7 	 global-step:7687	 l-p:0.017730560153722763
epoch£º384	 i:8 	 global-step:7688	 l-p:0.11029651761054993
epoch£º384	 i:9 	 global-step:7689	 l-p:0.1321278214454651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0007, 4.9932, 4.9101],
        [5.0007, 4.9987, 5.0006],
        [5.0007, 5.0007, 5.0007],
        [5.0007, 5.4891, 5.5809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.13818907737731934 
model_pd.l_d.mean(): -20.232891082763672 
model_pd.lagr.mean(): -20.094701766967773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4857], device='cuda:0')), ('power', tensor([-20.9502], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.13818907737731934
epoch£º385	 i:1 	 global-step:7701	 l-p:0.123838871717453
epoch£º385	 i:2 	 global-step:7702	 l-p:0.14540238678455353
epoch£º385	 i:3 	 global-step:7703	 l-p:0.11701236665248871
epoch£º385	 i:4 	 global-step:7704	 l-p:0.12196125835180283
epoch£º385	 i:5 	 global-step:7705	 l-p:0.14826317131519318
epoch£º385	 i:6 	 global-step:7706	 l-p:0.10669494420289993
epoch£º385	 i:7 	 global-step:7707	 l-p:0.12989309430122375
epoch£º385	 i:8 	 global-step:7708	 l-p:0.25737088918685913
epoch£º385	 i:9 	 global-step:7709	 l-p:0.3682684600353241
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8089, 4.7693, 4.7824],
        [4.8089, 4.9912, 4.9027],
        [4.8089, 4.8089, 4.8089],
        [4.8089, 4.7775, 4.7932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.17111892998218536 
model_pd.l_d.mean(): -20.180011749267578 
model_pd.lagr.mean(): -20.008892059326172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5625], device='cuda:0')), ('power', tensor([-20.9752], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.17111892998218536
epoch£º386	 i:1 	 global-step:7721	 l-p:0.15999074280261993
epoch£º386	 i:2 	 global-step:7722	 l-p:0.16108161211013794
epoch£º386	 i:3 	 global-step:7723	 l-p:0.9377363324165344
epoch£º386	 i:4 	 global-step:7724	 l-p:0.0470283217728138
epoch£º386	 i:5 	 global-step:7725	 l-p:0.08579327166080475
epoch£º386	 i:6 	 global-step:7726	 l-p:0.11211880296468735
epoch£º386	 i:7 	 global-step:7727	 l-p:0.1218639463186264
epoch£º386	 i:8 	 global-step:7728	 l-p:0.1631191223859787
epoch£º386	 i:9 	 global-step:7729	 l-p:0.1268061250448227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2642, 5.2447, 5.2558],
        [5.2642, 5.7641, 5.8457],
        [5.2642, 5.2638, 5.2642],
        [5.2642, 5.2350, 5.2409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.12543557584285736 
model_pd.l_d.mean(): -20.59577751159668 
model_pd.lagr.mean(): -20.4703426361084 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3671], device='cuda:0')), ('power', tensor([-21.1959], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.12543557584285736
epoch£º387	 i:1 	 global-step:7741	 l-p:0.12653730809688568
epoch£º387	 i:2 	 global-step:7742	 l-p:0.12461835891008377
epoch£º387	 i:3 	 global-step:7743	 l-p:-0.009907855652272701
epoch£º387	 i:4 	 global-step:7744	 l-p:0.11787518858909607
epoch£º387	 i:5 	 global-step:7745	 l-p:0.1339789628982544
epoch£º387	 i:6 	 global-step:7746	 l-p:0.10906356573104858
epoch£º387	 i:7 	 global-step:7747	 l-p:0.147821843624115
epoch£º387	 i:8 	 global-step:7748	 l-p:0.17186832427978516
epoch£º387	 i:9 	 global-step:7749	 l-p:0.11790021508932114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1175, 5.1146, 5.1172],
        [5.1175, 5.0952, 5.1080],
        [5.1175, 5.1140, 5.1171],
        [5.1175, 5.1053, 5.0309]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.11427659541368484 
model_pd.l_d.mean(): -19.436559677124023 
model_pd.lagr.mean(): -19.322282791137695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5491], device='cuda:0')), ('power', tensor([-20.2099], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.11427659541368484
epoch£º388	 i:1 	 global-step:7761	 l-p:-3.562927304301411e-05
epoch£º388	 i:2 	 global-step:7762	 l-p:0.037773583084344864
epoch£º388	 i:3 	 global-step:7763	 l-p:0.19325046241283417
epoch£º388	 i:4 	 global-step:7764	 l-p:0.1342199146747589
epoch£º388	 i:5 	 global-step:7765	 l-p:0.15930715203285217
epoch£º388	 i:6 	 global-step:7766	 l-p:0.32107624411582947
epoch£º388	 i:7 	 global-step:7767	 l-p:0.1494733989238739
epoch£º388	 i:8 	 global-step:7768	 l-p:0.12635014951229095
epoch£º388	 i:9 	 global-step:7769	 l-p:0.12714360654354095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9856, 5.0479, 4.9325],
        [4.9856, 4.9537, 4.8899],
        [4.9856, 4.9854, 4.9856],
        [4.9856, 4.9494, 4.9611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.10455428808927536 
model_pd.l_d.mean(): -19.395401000976562 
model_pd.lagr.mean(): -19.29084587097168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5308], device='cuda:0')), ('power', tensor([-20.1496], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.10455428808927536
epoch£º389	 i:1 	 global-step:7781	 l-p:0.11217496544122696
epoch£º389	 i:2 	 global-step:7782	 l-p:0.14685645699501038
epoch£º389	 i:3 	 global-step:7783	 l-p:0.1359473615884781
epoch£º389	 i:4 	 global-step:7784	 l-p:0.2163805514574051
epoch£º389	 i:5 	 global-step:7785	 l-p:0.13677291572093964
epoch£º389	 i:6 	 global-step:7786	 l-p:0.13022945821285248
epoch£º389	 i:7 	 global-step:7787	 l-p:0.08399589359760284
epoch£º389	 i:8 	 global-step:7788	 l-p:-1.3228422403335571
epoch£º389	 i:9 	 global-step:7789	 l-p:0.0679844543337822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0184, 5.0181, 5.0184],
        [5.0184, 4.9727, 4.9527],
        [5.0184, 5.5545, 5.6804],
        [5.0184, 5.0181, 5.0184]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.05591343343257904 
model_pd.l_d.mean(): -20.669267654418945 
model_pd.lagr.mean(): -20.613353729248047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4366], device='cuda:0')), ('power', tensor([-21.3412], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.05591343343257904
epoch£º390	 i:1 	 global-step:7801	 l-p:0.118931345641613
epoch£º390	 i:2 	 global-step:7802	 l-p:0.12586861848831177
epoch£º390	 i:3 	 global-step:7803	 l-p:0.16570086777210236
epoch£º390	 i:4 	 global-step:7804	 l-p:0.12640967965126038
epoch£º390	 i:5 	 global-step:7805	 l-p:0.2466452717781067
epoch£º390	 i:6 	 global-step:7806	 l-p:0.12168176472187042
epoch£º390	 i:7 	 global-step:7807	 l-p:0.1338907778263092
epoch£º390	 i:8 	 global-step:7808	 l-p:0.15348756313323975
epoch£º390	 i:9 	 global-step:7809	 l-p:0.15280160307884216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0829, 5.0750, 5.0817],
        [5.0829, 5.0742, 4.9890],
        [5.0829, 5.0565, 4.9915],
        [5.0829, 5.0793, 5.0826]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.16884107887744904 
model_pd.l_d.mean(): -20.689674377441406 
model_pd.lagr.mean(): -20.52083396911621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4108], device='cuda:0')), ('power', tensor([-21.3354], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.16884107887744904
epoch£º391	 i:1 	 global-step:7821	 l-p:-0.9107860922813416
epoch£º391	 i:2 	 global-step:7822	 l-p:0.13007360696792603
epoch£º391	 i:3 	 global-step:7823	 l-p:0.11803295463323593
epoch£º391	 i:4 	 global-step:7824	 l-p:0.10242787003517151
epoch£º391	 i:5 	 global-step:7825	 l-p:0.052689675241708755
epoch£º391	 i:6 	 global-step:7826	 l-p:0.17128798365592957
epoch£º391	 i:7 	 global-step:7827	 l-p:0.10950592160224915
epoch£º391	 i:8 	 global-step:7828	 l-p:0.11701308935880661
epoch£º391	 i:9 	 global-step:7829	 l-p:0.11152265220880508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0307, 4.9967, 5.0098],
        [5.0307, 5.0304, 5.0307],
        [5.0307, 5.8375, 6.1928],
        [5.0307, 5.0307, 5.0307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.0677296593785286 
model_pd.l_d.mean(): -19.888437271118164 
model_pd.lagr.mean(): -19.820707321166992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4614], device='cuda:0')), ('power', tensor([-20.5771], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.0677296593785286
epoch£º392	 i:1 	 global-step:7841	 l-p:0.2520734369754791
epoch£º392	 i:2 	 global-step:7842	 l-p:0.13580366969108582
epoch£º392	 i:3 	 global-step:7843	 l-p:-0.028543466702103615
epoch£º392	 i:4 	 global-step:7844	 l-p:0.14505521953105927
epoch£º392	 i:5 	 global-step:7845	 l-p:0.12514249980449677
epoch£º392	 i:6 	 global-step:7846	 l-p:0.13333183526992798
epoch£º392	 i:7 	 global-step:7847	 l-p:-0.4432297348976135
epoch£º392	 i:8 	 global-step:7848	 l-p:0.11373840272426605
epoch£º392	 i:9 	 global-step:7849	 l-p:0.11251182109117508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0425, 5.6795, 5.8847],
        [5.0425, 5.0425, 5.0425],
        [5.0425, 5.2616, 5.1816],
        [5.0425, 5.0425, 5.0425]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): 0.16657012701034546 
model_pd.l_d.mean(): -20.31499481201172 
model_pd.lagr.mean(): -20.14842414855957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4788], device='cuda:0')), ('power', tensor([-21.0262], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:0.16657012701034546
epoch£º393	 i:1 	 global-step:7861	 l-p:0.11795338988304138
epoch£º393	 i:2 	 global-step:7862	 l-p:0.15733078122138977
epoch£º393	 i:3 	 global-step:7863	 l-p:0.11966453492641449
epoch£º393	 i:4 	 global-step:7864	 l-p:1.53622305393219
epoch£º393	 i:5 	 global-step:7865	 l-p:0.12781377136707306
epoch£º393	 i:6 	 global-step:7866	 l-p:-0.009262418374419212
epoch£º393	 i:7 	 global-step:7867	 l-p:0.1456102728843689
epoch£º393	 i:8 	 global-step:7868	 l-p:0.1387791931629181
epoch£º393	 i:9 	 global-step:7869	 l-p:0.13310660421848297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0377, 5.0013, 5.0136],
        [5.0377, 5.0373, 5.0377],
        [5.0377, 5.0010, 4.9434],
        [5.0377, 5.7503, 6.0204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.3439979553222656 
model_pd.l_d.mean(): -20.570884704589844 
model_pd.lagr.mean(): -20.226886749267578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4366], device='cuda:0')), ('power', tensor([-21.2418], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.3439979553222656
epoch£º394	 i:1 	 global-step:7881	 l-p:0.12069833278656006
epoch£º394	 i:2 	 global-step:7882	 l-p:0.11575322598218918
epoch£º394	 i:3 	 global-step:7883	 l-p:0.12085476517677307
epoch£º394	 i:4 	 global-step:7884	 l-p:0.1136554479598999
epoch£º394	 i:5 	 global-step:7885	 l-p:0.26734259724617004
epoch£º394	 i:6 	 global-step:7886	 l-p:0.16361618041992188
epoch£º394	 i:7 	 global-step:7887	 l-p:0.14457961916923523
epoch£º394	 i:8 	 global-step:7888	 l-p:0.045263566076755524
epoch£º394	 i:9 	 global-step:7889	 l-p:0.16306723654270172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9510, 4.9495, 4.9509],
        [4.9510, 4.9039, 4.9091],
        [4.9510, 4.9510, 4.9510],
        [4.9510, 4.9127, 4.9268]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.12447382509708405 
model_pd.l_d.mean(): -20.12398338317871 
model_pd.lagr.mean(): -19.999509811401367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5168], device='cuda:0')), ('power', tensor([-20.8718], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.12447382509708405
epoch£º395	 i:1 	 global-step:7901	 l-p:0.12531037628650665
epoch£º395	 i:2 	 global-step:7902	 l-p:0.14175717532634735
epoch£º395	 i:3 	 global-step:7903	 l-p:0.16087231040000916
epoch£º395	 i:4 	 global-step:7904	 l-p:0.09704532474279404
epoch£º395	 i:5 	 global-step:7905	 l-p:0.10214345902204514
epoch£º395	 i:6 	 global-step:7906	 l-p:0.16037839651107788
epoch£º395	 i:7 	 global-step:7907	 l-p:0.11937349289655685
epoch£º395	 i:8 	 global-step:7908	 l-p:0.18784506618976593
epoch£º395	 i:9 	 global-step:7909	 l-p:0.17221851646900177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8901, 5.0707, 4.9760],
        [4.8901, 5.1297, 5.0625],
        [4.8901, 4.8715, 4.8849],
        [4.8901, 4.8786, 4.8880]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.14023375511169434 
model_pd.l_d.mean(): -21.067882537841797 
model_pd.lagr.mean(): -20.927648544311523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4209], device='cuda:0')), ('power', tensor([-21.7281], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.14023375511169434
epoch£º396	 i:1 	 global-step:7921	 l-p:0.07514379918575287
epoch£º396	 i:2 	 global-step:7922	 l-p:0.12608617544174194
epoch£º396	 i:3 	 global-step:7923	 l-p:0.1422872245311737
epoch£º396	 i:4 	 global-step:7924	 l-p:0.09326285123825073
epoch£º396	 i:5 	 global-step:7925	 l-p:0.16503790020942688
epoch£º396	 i:6 	 global-step:7926	 l-p:0.12093409895896912
epoch£º396	 i:7 	 global-step:7927	 l-p:0.040063075721263885
epoch£º396	 i:8 	 global-step:7928	 l-p:0.15487483143806458
epoch£º396	 i:9 	 global-step:7929	 l-p:0.10703325271606445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9543, 5.6315, 5.8778],
        [4.9543, 4.9488, 4.9537],
        [4.9543, 4.9079, 4.9163],
        [4.9543, 4.9108, 4.9223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.11521083861589432 
model_pd.l_d.mean(): -19.30066680908203 
model_pd.lagr.mean(): -19.185455322265625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5946], device='cuda:0')), ('power', tensor([-20.1190], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.11521083861589432
epoch£º397	 i:1 	 global-step:7941	 l-p:0.010270919650793076
epoch£º397	 i:2 	 global-step:7942	 l-p:0.168016716837883
epoch£º397	 i:3 	 global-step:7943	 l-p:0.12907886505126953
epoch£º397	 i:4 	 global-step:7944	 l-p:0.06678567081689835
epoch£º397	 i:5 	 global-step:7945	 l-p:0.21699492633342743
epoch£º397	 i:6 	 global-step:7946	 l-p:0.0566520094871521
epoch£º397	 i:7 	 global-step:7947	 l-p:0.13846629858016968
epoch£º397	 i:8 	 global-step:7948	 l-p:0.130426824092865
epoch£º397	 i:9 	 global-step:7949	 l-p:-0.016256075352430344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1172, 5.3717, 5.3048],
        [5.1172, 5.1172, 5.1172],
        [5.1172, 5.1157, 5.1171],
        [5.1172, 5.3016, 5.2046]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.9299238920211792 
model_pd.l_d.mean(): -19.312246322631836 
model_pd.lagr.mean(): -18.382322311401367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5060], device='cuda:0')), ('power', tensor([-20.0402], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:0.9299238920211792
epoch£º398	 i:1 	 global-step:7961	 l-p:0.10770823061466217
epoch£º398	 i:2 	 global-step:7962	 l-p:0.1981983482837677
epoch£º398	 i:3 	 global-step:7963	 l-p:0.14152057468891144
epoch£º398	 i:4 	 global-step:7964	 l-p:0.14068225026130676
epoch£º398	 i:5 	 global-step:7965	 l-p:0.11573369055986404
epoch£º398	 i:6 	 global-step:7966	 l-p:0.12955011427402496
epoch£º398	 i:7 	 global-step:7967	 l-p:0.12409783154726028
epoch£º398	 i:8 	 global-step:7968	 l-p:0.12336612492799759
epoch£º398	 i:9 	 global-step:7969	 l-p:0.11044980585575104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1079, 5.1075, 5.1079],
        [5.1079, 5.0669, 5.0757],
        [5.1079, 5.2052, 5.0867],
        [5.1079, 5.0825, 5.0057]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.11399690061807632 
model_pd.l_d.mean(): -20.029090881347656 
model_pd.lagr.mean(): -19.91509437561035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4667], device='cuda:0')), ('power', tensor([-20.7248], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.11399690061807632
epoch£º399	 i:1 	 global-step:7981	 l-p:0.059905026108026505
epoch£º399	 i:2 	 global-step:7982	 l-p:0.11942341178655624
epoch£º399	 i:3 	 global-step:7983	 l-p:0.24902047216892242
epoch£º399	 i:4 	 global-step:7984	 l-p:0.1297832578420639
epoch£º399	 i:5 	 global-step:7985	 l-p:0.14119887351989746
epoch£º399	 i:6 	 global-step:7986	 l-p:0.13341879844665527
epoch£º399	 i:7 	 global-step:7987	 l-p:0.143145352602005
epoch£º399	 i:8 	 global-step:7988	 l-p:0.1604846715927124
epoch£º399	 i:9 	 global-step:7989	 l-p:-0.38258805871009827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9528, 4.9422, 4.8360],
        [4.9528, 5.0680, 4.9496],
        [4.9528, 4.9365, 4.9488],
        [4.9528, 4.9326, 4.8335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.14343582093715668 
model_pd.l_d.mean(): -19.653467178344727 
model_pd.lagr.mean(): -19.51003074645996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5031], device='cuda:0')), ('power', tensor([-20.3822], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.14343582093715668
epoch£º400	 i:1 	 global-step:8001	 l-p:0.10482563823461533
epoch£º400	 i:2 	 global-step:8002	 l-p:0.10726947337388992
epoch£º400	 i:3 	 global-step:8003	 l-p:0.14090251922607422
epoch£º400	 i:4 	 global-step:8004	 l-p:0.1429888755083084
epoch£º400	 i:5 	 global-step:8005	 l-p:0.5726860165596008
epoch£º400	 i:6 	 global-step:8006	 l-p:0.14353524148464203
epoch£º400	 i:7 	 global-step:8007	 l-p:-0.010597877204418182
epoch£º400	 i:8 	 global-step:8008	 l-p:0.1107991561293602
epoch£º400	 i:9 	 global-step:8009	 l-p:0.13211195170879364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9765, 5.1408, 5.0364],
        [4.9765, 4.9763, 4.9765],
        [4.9765, 5.0881, 4.9687],
        [4.9765, 5.2814, 5.2455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.1493017077445984 
model_pd.l_d.mean(): -19.491535186767578 
model_pd.lagr.mean(): -19.342233657836914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5850], device='cuda:0')), ('power', tensor([-20.3022], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.1493017077445984
epoch£º401	 i:1 	 global-step:8021	 l-p:0.15270137786865234
epoch£º401	 i:2 	 global-step:8022	 l-p:0.14903853833675385
epoch£º401	 i:3 	 global-step:8023	 l-p:1.1269680261611938
epoch£º401	 i:4 	 global-step:8024	 l-p:0.1255621463060379
epoch£º401	 i:5 	 global-step:8025	 l-p:0.03961683809757233
epoch£º401	 i:6 	 global-step:8026	 l-p:0.14915864169597626
epoch£º401	 i:7 	 global-step:8027	 l-p:0.11670390516519547
epoch£º401	 i:8 	 global-step:8028	 l-p:0.09786755591630936
epoch£º401	 i:9 	 global-step:8029	 l-p:0.15116526186466217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9690, 4.9690, 4.9690],
        [4.9690, 4.9685, 4.9689],
        [4.9690, 4.9223, 4.9336],
        [4.9690, 4.9690, 4.9690]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.13849186897277832 
model_pd.l_d.mean(): -20.757030487060547 
model_pd.lagr.mean(): -20.61853790283203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4290], device='cuda:0')), ('power', tensor([-21.4222], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.13849186897277832
epoch£º402	 i:1 	 global-step:8041	 l-p:0.14074835181236267
epoch£º402	 i:2 	 global-step:8042	 l-p:0.13791005313396454
epoch£º402	 i:3 	 global-step:8043	 l-p:0.09932314604520798
epoch£º402	 i:4 	 global-step:8044	 l-p:0.12162265181541443
epoch£º402	 i:5 	 global-step:8045	 l-p:0.08484991639852524
epoch£º402	 i:6 	 global-step:8046	 l-p:0.1442607343196869
epoch£º402	 i:7 	 global-step:8047	 l-p:0.21106933057308197
epoch£º402	 i:8 	 global-step:8048	 l-p:-3.6170566082000732
epoch£º402	 i:9 	 global-step:8049	 l-p:0.1628471165895462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0038, 4.9636, 4.9786],
        [5.0038, 5.0038, 5.0038],
        [5.0038, 4.9665, 4.9824],
        [5.0038, 4.9882, 5.0000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.1859632432460785 
model_pd.l_d.mean(): -20.52585220336914 
model_pd.lagr.mean(): -20.339889526367188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4424], device='cuda:0')), ('power', tensor([-21.2021], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.1859632432460785
epoch£º403	 i:1 	 global-step:8061	 l-p:0.1302439421415329
epoch£º403	 i:2 	 global-step:8062	 l-p:0.13839812576770782
epoch£º403	 i:3 	 global-step:8063	 l-p:0.12936820089817047
epoch£º403	 i:4 	 global-step:8064	 l-p:0.08523781597614288
epoch£º403	 i:5 	 global-step:8065	 l-p:0.13971737027168274
epoch£º403	 i:6 	 global-step:8066	 l-p:0.12508763372898102
epoch£º403	 i:7 	 global-step:8067	 l-p:0.36572203040122986
epoch£º403	 i:8 	 global-step:8068	 l-p:0.12472381442785263
epoch£º403	 i:9 	 global-step:8069	 l-p:0.08391322940587997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0660, 5.1907, 5.0739],
        [5.0660, 5.5395, 5.6113],
        [5.0660, 5.0381, 5.0541],
        [5.0660, 5.0612, 5.0655]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.1553991585969925 
model_pd.l_d.mean(): -19.17428207397461 
model_pd.lagr.mean(): -19.018882751464844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5374], device='cuda:0')), ('power', tensor([-19.9329], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.1553991585969925
epoch£º404	 i:1 	 global-step:8081	 l-p:0.11439914256334305
epoch£º404	 i:2 	 global-step:8082	 l-p:0.13046540319919586
epoch£º404	 i:3 	 global-step:8083	 l-p:0.12924781441688538
epoch£º404	 i:4 	 global-step:8084	 l-p:0.12104886770248413
epoch£º404	 i:5 	 global-step:8085	 l-p:0.03622768819332123
epoch£º404	 i:6 	 global-step:8086	 l-p:0.16043594479560852
epoch£º404	 i:7 	 global-step:8087	 l-p:0.12108699232339859
epoch£º404	 i:8 	 global-step:8088	 l-p:-0.2561205327510834
epoch£º404	 i:9 	 global-step:8089	 l-p:0.14764411747455597
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1051, 5.0945, 4.9965],
        [5.1051, 5.1051, 5.1051],
        [5.1051, 5.1051, 5.1051],
        [5.1051, 5.0562, 5.0168]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.09991836547851562 
model_pd.l_d.mean(): -20.36443519592285 
model_pd.lagr.mean(): -20.264516830444336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4499], device='cuda:0')), ('power', tensor([-21.0466], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.09991836547851562
epoch£º405	 i:1 	 global-step:8101	 l-p:0.16695785522460938
epoch£º405	 i:2 	 global-step:8102	 l-p:0.1049402579665184
epoch£º405	 i:3 	 global-step:8103	 l-p:-0.017211321741342545
epoch£º405	 i:4 	 global-step:8104	 l-p:-0.20681613683700562
epoch£º405	 i:5 	 global-step:8105	 l-p:0.002598176011815667
epoch£º405	 i:6 	 global-step:8106	 l-p:0.15622404217720032
epoch£º405	 i:7 	 global-step:8107	 l-p:0.14942975342273712
epoch£º405	 i:8 	 global-step:8108	 l-p:0.1253029853105545
epoch£º405	 i:9 	 global-step:8109	 l-p:0.1323164701461792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1043, 5.0602, 5.0696],
        [5.1043, 5.0862, 5.0991],
        [5.1043, 5.0547, 5.0166],
        [5.1043, 5.2758, 5.1718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): -0.025779012590646744 
model_pd.l_d.mean(): -19.681930541992188 
model_pd.lagr.mean(): -19.70771026611328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4878], device='cuda:0')), ('power', tensor([-20.3954], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:-0.025779012590646744
epoch£º406	 i:1 	 global-step:8121	 l-p:0.09787458181381226
epoch£º406	 i:2 	 global-step:8122	 l-p:0.1296166479587555
epoch£º406	 i:3 	 global-step:8123	 l-p:0.1628672331571579
epoch£º406	 i:4 	 global-step:8124	 l-p:0.1496300995349884
epoch£º406	 i:5 	 global-step:8125	 l-p:0.3418331444263458
epoch£º406	 i:6 	 global-step:8126	 l-p:0.09576291590929031
epoch£º406	 i:7 	 global-step:8127	 l-p:0.06927203387022018
epoch£º406	 i:8 	 global-step:8128	 l-p:0.12663394212722778
epoch£º406	 i:9 	 global-step:8129	 l-p:0.12835727632045746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0659, 5.0599, 5.0652],
        [5.0659, 5.0606, 5.0653],
        [5.0659, 5.1568, 5.0335],
        [5.0659, 5.0563, 4.9534]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.15942522883415222 
model_pd.l_d.mean(): -20.642061233520508 
model_pd.lagr.mean(): -20.482635498046875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4217], device='cuda:0')), ('power', tensor([-21.2984], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.15942522883415222
epoch£º407	 i:1 	 global-step:8141	 l-p:0.12592610716819763
epoch£º407	 i:2 	 global-step:8142	 l-p:0.15080977976322174
epoch£º407	 i:3 	 global-step:8143	 l-p:0.10464321076869965
epoch£º407	 i:4 	 global-step:8144	 l-p:-0.6585111618041992
epoch£º407	 i:5 	 global-step:8145	 l-p:0.14608216285705566
epoch£º407	 i:6 	 global-step:8146	 l-p:0.14035895466804504
epoch£º407	 i:7 	 global-step:8147	 l-p:0.3380955755710602
epoch£º407	 i:8 	 global-step:8148	 l-p:0.11562972515821457
epoch£º407	 i:9 	 global-step:8149	 l-p:0.12218361347913742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1480, 5.1193, 5.1348],
        [5.1480, 5.1226, 5.1378],
        [5.1480, 5.1007, 5.0607],
        [5.1480, 5.2310, 5.1085]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.35364675521850586 
model_pd.l_d.mean(): -20.58029556274414 
model_pd.lagr.mean(): -20.226648330688477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4180], device='cuda:0')), ('power', tensor([-21.2323], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.35364675521850586
epoch£º408	 i:1 	 global-step:8161	 l-p:0.13956092298030853
epoch£º408	 i:2 	 global-step:8162	 l-p:0.14583832025527954
epoch£º408	 i:3 	 global-step:8163	 l-p:0.13910089433193207
epoch£º408	 i:4 	 global-step:8164	 l-p:0.26681819558143616
epoch£º408	 i:5 	 global-step:8165	 l-p:0.2056303322315216
epoch£º408	 i:6 	 global-step:8166	 l-p:0.12020057439804077
epoch£º408	 i:7 	 global-step:8167	 l-p:0.12182091176509857
epoch£º408	 i:8 	 global-step:8168	 l-p:0.1291479617357254
epoch£º408	 i:9 	 global-step:8169	 l-p:0.09403260052204132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1542, 5.1349, 5.1482],
        [5.1542, 5.1329, 5.1470],
        [5.1542, 5.1525, 5.1541],
        [5.1542, 5.1530, 5.1541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.12839555740356445 
model_pd.l_d.mean(): -20.60204315185547 
model_pd.lagr.mean(): -20.473648071289062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3873], device='cuda:0')), ('power', tensor([-21.2229], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.12839555740356445
epoch£º409	 i:1 	 global-step:8181	 l-p:0.165660098195076
epoch£º409	 i:2 	 global-step:8182	 l-p:0.3243463337421417
epoch£º409	 i:3 	 global-step:8183	 l-p:0.14500266313552856
epoch£º409	 i:4 	 global-step:8184	 l-p:0.14072249829769135
epoch£º409	 i:5 	 global-step:8185	 l-p:0.12398963421583176
epoch£º409	 i:6 	 global-step:8186	 l-p:-0.014265584759414196
epoch£º409	 i:7 	 global-step:8187	 l-p:-0.1247863918542862
epoch£º409	 i:8 	 global-step:8188	 l-p:0.1432737559080124
epoch£º409	 i:9 	 global-step:8189	 l-p:0.11681409925222397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1111, 5.0812, 5.0973],
        [5.1111, 5.0628, 5.0674],
        [5.1111, 5.1054, 5.1104],
        [5.1111, 5.1111, 5.1111]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.15432612597942352 
model_pd.l_d.mean(): -20.465883255004883 
model_pd.lagr.mean(): -20.31155776977539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4369], device='cuda:0')), ('power', tensor([-21.1359], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.15432612597942352
epoch£º410	 i:1 	 global-step:8201	 l-p:0.13318420946598053
epoch£º410	 i:2 	 global-step:8202	 l-p:0.09679567813873291
epoch£º410	 i:3 	 global-step:8203	 l-p:0.12892790138721466
epoch£º410	 i:4 	 global-step:8204	 l-p:0.341945081949234
epoch£º410	 i:5 	 global-step:8205	 l-p:0.13177762925624847
epoch£º410	 i:6 	 global-step:8206	 l-p:0.08960885554552078
epoch£º410	 i:7 	 global-step:8207	 l-p:0.12995602190494537
epoch£º410	 i:8 	 global-step:8208	 l-p:0.14590425789356232
epoch£º410	 i:9 	 global-step:8209	 l-p:0.08396841585636139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0032, 5.0032, 5.0032],
        [5.0032, 5.0026, 5.0032],
        [5.0032, 5.0031, 5.0032],
        [5.0032, 5.0020, 5.0032]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.5701494216918945 
model_pd.l_d.mean(): -20.36771583557129 
model_pd.lagr.mean(): -19.797565460205078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4793], device='cuda:0')), ('power', tensor([-21.0800], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.5701494216918945
epoch£º411	 i:1 	 global-step:8221	 l-p:0.13561223447322845
epoch£º411	 i:2 	 global-step:8222	 l-p:0.1515192687511444
epoch£º411	 i:3 	 global-step:8223	 l-p:0.1735183745622635
epoch£º411	 i:4 	 global-step:8224	 l-p:0.08851604163646698
epoch£º411	 i:5 	 global-step:8225	 l-p:0.3735401928424835
epoch£º411	 i:6 	 global-step:8226	 l-p:0.07608360797166824
epoch£º411	 i:7 	 global-step:8227	 l-p:0.12249823659658432
epoch£º411	 i:8 	 global-step:8228	 l-p:0.1353176236152649
epoch£º411	 i:9 	 global-step:8229	 l-p:0.14210952818393707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9786, 4.9728, 4.9780],
        [4.9786, 5.7493, 6.0721],
        [4.9786, 4.9786, 4.9786],
        [4.9786, 4.9259, 4.9337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.27979394793510437 
model_pd.l_d.mean(): -19.183059692382812 
model_pd.lagr.mean(): -18.90326499938965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5362], device='cuda:0')), ('power', tensor([-19.9405], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.27979394793510437
epoch£º412	 i:1 	 global-step:8241	 l-p:0.10813917219638824
epoch£º412	 i:2 	 global-step:8242	 l-p:0.13677825033664703
epoch£º412	 i:3 	 global-step:8243	 l-p:0.14613080024719238
epoch£º412	 i:4 	 global-step:8244	 l-p:0.12233433127403259
epoch£º412	 i:5 	 global-step:8245	 l-p:0.1567138135433197
epoch£º412	 i:6 	 global-step:8246	 l-p:0.09091508388519287
epoch£º412	 i:7 	 global-step:8247	 l-p:0.0773000717163086
epoch£º412	 i:8 	 global-step:8248	 l-p:0.16367679834365845
epoch£º412	 i:9 	 global-step:8249	 l-p:0.11866607517004013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9227, 4.8774, 4.8941],
        [4.9227, 4.9227, 4.9227],
        [4.9227, 5.6132, 5.8707],
        [4.9227, 4.9227, 4.9227]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.14960028231143951 
model_pd.l_d.mean(): -19.94041633605957 
model_pd.lagr.mean(): -19.790815353393555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5249], device='cuda:0')), ('power', tensor([-20.6946], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.14960028231143951
epoch£º413	 i:1 	 global-step:8261	 l-p:0.14739391207695007
epoch£º413	 i:2 	 global-step:8262	 l-p:0.13529816269874573
epoch£º413	 i:3 	 global-step:8263	 l-p:0.07488231360912323
epoch£º413	 i:4 	 global-step:8264	 l-p:0.11959312856197357
epoch£º413	 i:5 	 global-step:8265	 l-p:0.18171809613704681
epoch£º413	 i:6 	 global-step:8266	 l-p:0.16964289546012878
epoch£º413	 i:7 	 global-step:8267	 l-p:0.15493154525756836
epoch£º413	 i:8 	 global-step:8268	 l-p:0.024915112182497978
epoch£º413	 i:9 	 global-step:8269	 l-p:0.13063184916973114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8869, 4.8832, 4.8866],
        [4.8869, 5.0003, 4.8769],
        [4.8869, 4.8869, 4.8869],
        [4.8869, 4.8625, 4.8790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.08883395791053772 
model_pd.l_d.mean(): -19.560546875 
model_pd.lagr.mean(): -19.471712112426758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5490], device='cuda:0')), ('power', tensor([-20.3351], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.08883395791053772
epoch£º414	 i:1 	 global-step:8281	 l-p:0.14458291232585907
epoch£º414	 i:2 	 global-step:8282	 l-p:0.17318406701087952
epoch£º414	 i:3 	 global-step:8283	 l-p:0.12575268745422363
epoch£º414	 i:4 	 global-step:8284	 l-p:0.15120385587215424
epoch£º414	 i:5 	 global-step:8285	 l-p:0.1745138168334961
epoch£º414	 i:6 	 global-step:8286	 l-p:0.0912698283791542
epoch£º414	 i:7 	 global-step:8287	 l-p:0.10496661812067032
epoch£º414	 i:8 	 global-step:8288	 l-p:0.13437634706497192
epoch£º414	 i:9 	 global-step:8289	 l-p:0.16054360568523407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9314, 5.0196, 4.8911],
        [4.9314, 5.3958, 5.4692],
        [4.9314, 4.9685, 4.8376],
        [4.9314, 4.9175, 4.9286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.1073213741183281 
model_pd.l_d.mean(): -19.901287078857422 
model_pd.lagr.mean(): -19.79396629333496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5679], device='cuda:0')), ('power', tensor([-20.6989], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.1073213741183281
epoch£º415	 i:1 	 global-step:8301	 l-p:0.12475627660751343
epoch£º415	 i:2 	 global-step:8302	 l-p:0.1587299406528473
epoch£º415	 i:3 	 global-step:8303	 l-p:0.12982867658138275
epoch£º415	 i:4 	 global-step:8304	 l-p:0.14163552224636078
epoch£º415	 i:5 	 global-step:8305	 l-p:-0.15029075741767883
epoch£º415	 i:6 	 global-step:8306	 l-p:0.13409733772277832
epoch£º415	 i:7 	 global-step:8307	 l-p:0.13314631581306458
epoch£º415	 i:8 	 global-step:8308	 l-p:0.05106441304087639
epoch£º415	 i:9 	 global-step:8309	 l-p:0.12807774543762207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9590, 4.9283, 4.9463],
        [4.9590, 4.9590, 4.9590],
        [4.9590, 5.0353, 4.9054],
        [4.9590, 4.9590, 4.9590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.1694815456867218 
model_pd.l_d.mean(): -20.659637451171875 
model_pd.lagr.mean(): -20.490156173706055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4655], device='cuda:0')), ('power', tensor([-21.3609], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.1694815456867218
epoch£º416	 i:1 	 global-step:8321	 l-p:-0.8551343083381653
epoch£º416	 i:2 	 global-step:8322	 l-p:0.07100064307451248
epoch£º416	 i:3 	 global-step:8323	 l-p:0.11387860774993896
epoch£º416	 i:4 	 global-step:8324	 l-p:0.12408699840307236
epoch£º416	 i:5 	 global-step:8325	 l-p:0.08186960965394974
epoch£º416	 i:6 	 global-step:8326	 l-p:0.10995185375213623
epoch£º416	 i:7 	 global-step:8327	 l-p:0.1245192140340805
epoch£º416	 i:8 	 global-step:8328	 l-p:0.11927039921283722
epoch£º416	 i:9 	 global-step:8329	 l-p:0.12749521434307098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0263, 5.0181, 5.0251],
        [5.0263, 5.0094, 5.0221],
        [5.0263, 4.9826, 4.9982],
        [5.0263, 5.0263, 5.0263]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.3191634714603424 
model_pd.l_d.mean(): -20.42774772644043 
model_pd.lagr.mean(): -20.108583450317383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4613], device='cuda:0')), ('power', tensor([-21.1223], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.3191634714603424
epoch£º417	 i:1 	 global-step:8341	 l-p:0.1404096931219101
epoch£º417	 i:2 	 global-step:8342	 l-p:0.12887747585773468
epoch£º417	 i:3 	 global-step:8343	 l-p:0.12024690210819244
epoch£º417	 i:4 	 global-step:8344	 l-p:0.12434615194797516
epoch£º417	 i:5 	 global-step:8345	 l-p:0.14413152635097504
epoch£º417	 i:6 	 global-step:8346	 l-p:0.04074201360344887
epoch£º417	 i:7 	 global-step:8347	 l-p:0.10451073199510574
epoch£º417	 i:8 	 global-step:8348	 l-p:0.13981467485427856
epoch£º417	 i:9 	 global-step:8349	 l-p:0.14464861154556274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9447, 4.9445, 4.9447],
        [4.9447, 4.9446, 4.9447],
        [4.9447, 5.1360, 5.0388],
        [4.9447, 4.8820, 4.8760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): -0.21299102902412415 
model_pd.l_d.mean(): -19.640968322753906 
model_pd.lagr.mean(): -19.853960037231445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5191], device='cuda:0')), ('power', tensor([-20.3860], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:-0.21299102902412415
epoch£º418	 i:1 	 global-step:8361	 l-p:0.15049824118614197
epoch£º418	 i:2 	 global-step:8362	 l-p:0.10797318816184998
epoch£º418	 i:3 	 global-step:8363	 l-p:0.13674543797969818
epoch£º418	 i:4 	 global-step:8364	 l-p:0.1476336568593979
epoch£º418	 i:5 	 global-step:8365	 l-p:0.1186741441488266
epoch£º418	 i:6 	 global-step:8366	 l-p:0.12053312361240387
epoch£º418	 i:7 	 global-step:8367	 l-p:0.1803702563047409
epoch£º418	 i:8 	 global-step:8368	 l-p:0.11590154469013214
epoch£º418	 i:9 	 global-step:8369	 l-p:0.13045768439769745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9491, 4.8863, 4.8808],
        [4.9491, 4.9819, 4.8505],
        [4.9491, 4.9187, 4.8138],
        [4.9491, 4.9401, 4.9478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.1544889658689499 
model_pd.l_d.mean(): -20.318405151367188 
model_pd.lagr.mean(): -20.163915634155273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5073], device='cuda:0')), ('power', tensor([-21.0587], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.1544889658689499
epoch£º419	 i:1 	 global-step:8381	 l-p:0.13206876814365387
epoch£º419	 i:2 	 global-step:8382	 l-p:0.11619707196950912
epoch£º419	 i:3 	 global-step:8383	 l-p:0.13319750130176544
epoch£º419	 i:4 	 global-step:8384	 l-p:0.1407114714384079
epoch£º419	 i:5 	 global-step:8385	 l-p:0.08513985574245453
epoch£º419	 i:6 	 global-step:8386	 l-p:-0.01285110879689455
epoch£º419	 i:7 	 global-step:8387	 l-p:0.12816177308559418
epoch£º419	 i:8 	 global-step:8388	 l-p:0.1399177759885788
epoch£º419	 i:9 	 global-step:8389	 l-p:0.32746440172195435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9769, 5.1242, 5.0093],
        [4.9769, 4.9185, 4.9219],
        [4.9769, 4.9130, 4.8765],
        [4.9769, 4.9765, 4.9769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.07413292676210403 
model_pd.l_d.mean(): -19.638626098632812 
model_pd.lagr.mean(): -19.56449317932129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5062], device='cuda:0')), ('power', tensor([-20.3704], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.07413292676210403
epoch£º420	 i:1 	 global-step:8401	 l-p:0.129640132188797
epoch£º420	 i:2 	 global-step:8402	 l-p:0.13522395491600037
epoch£º420	 i:3 	 global-step:8403	 l-p:0.1341877281665802
epoch£º420	 i:4 	 global-step:8404	 l-p:0.13083937764167786
epoch£º420	 i:5 	 global-step:8405	 l-p:0.2541710138320923
epoch£º420	 i:6 	 global-step:8406	 l-p:0.07762842625379562
epoch£º420	 i:7 	 global-step:8407	 l-p:0.12534606456756592
epoch£º420	 i:8 	 global-step:8408	 l-p:0.11032186448574066
epoch£º420	 i:9 	 global-step:8409	 l-p:0.2382764369249344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0532, 4.9973, 4.9442],
        [5.0532, 5.6716, 5.8554],
        [5.0532, 5.0190, 5.0368],
        [5.0532, 5.0472, 5.0525]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.09914460778236389 
model_pd.l_d.mean(): -20.31427764892578 
model_pd.lagr.mean(): -20.215133666992188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4638], device='cuda:0')), ('power', tensor([-21.0101], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.09914460778236389
epoch£º421	 i:1 	 global-step:8421	 l-p:0.12589500844478607
epoch£º421	 i:2 	 global-step:8422	 l-p:0.11576411873102188
epoch£º421	 i:3 	 global-step:8423	 l-p:0.13728198409080505
epoch£º421	 i:4 	 global-step:8424	 l-p:0.17649565637111664
epoch£º421	 i:5 	 global-step:8425	 l-p:0.06709689646959305
epoch£º421	 i:6 	 global-step:8426	 l-p:1.0685651302337646
epoch£º421	 i:7 	 global-step:8427	 l-p:0.1421775221824646
epoch£º421	 i:8 	 global-step:8428	 l-p:0.1349521428346634
epoch£º421	 i:9 	 global-step:8429	 l-p:0.12644778192043304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0732, 5.0587, 5.0700],
        [5.0732, 5.0732, 5.0732],
        [5.0732, 5.0725, 5.0732],
        [5.0732, 5.0983, 4.9724]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.26166459918022156 
model_pd.l_d.mean(): -20.27570343017578 
model_pd.lagr.mean(): -20.0140380859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4695], device='cuda:0')), ('power', tensor([-20.9769], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.26166459918022156
epoch£º422	 i:1 	 global-step:8441	 l-p:0.12947534024715424
epoch£º422	 i:2 	 global-step:8442	 l-p:0.11794523149728775
epoch£º422	 i:3 	 global-step:8443	 l-p:0.1229717954993248
epoch£º422	 i:4 	 global-step:8444	 l-p:0.16116993129253387
epoch£º422	 i:5 	 global-step:8445	 l-p:0.09010053426027298
epoch£º422	 i:6 	 global-step:8446	 l-p:0.08393281698226929
epoch£º422	 i:7 	 global-step:8447	 l-p:0.003529734443873167
epoch£º422	 i:8 	 global-step:8448	 l-p:0.15803366899490356
epoch£º422	 i:9 	 global-step:8449	 l-p:0.1240425631403923
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1379, 5.1379, 5.1379],
        [5.1379, 5.2209, 5.0931],
        [5.1379, 5.1213, 5.1338],
        [5.1379, 5.1200, 5.0188]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.10787654668092728 
model_pd.l_d.mean(): -20.103487014770508 
model_pd.lagr.mean(): -19.9956111907959 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4671], device='cuda:0')), ('power', tensor([-20.8004], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.10787654668092728
epoch£º423	 i:1 	 global-step:8461	 l-p:6.425268173217773
epoch£º423	 i:2 	 global-step:8462	 l-p:0.2980155944824219
epoch£º423	 i:3 	 global-step:8463	 l-p:0.1411256492137909
epoch£º423	 i:4 	 global-step:8464	 l-p:0.19051997363567352
epoch£º423	 i:5 	 global-step:8465	 l-p:0.11395524442195892
epoch£º423	 i:6 	 global-step:8466	 l-p:0.13442373275756836
epoch£º423	 i:7 	 global-step:8467	 l-p:0.13189688324928284
epoch£º423	 i:8 	 global-step:8468	 l-p:0.12387675791978836
epoch£º423	 i:9 	 global-step:8469	 l-p:0.1209830716252327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1757, 5.1221, 5.1165],
        [5.1757, 5.1736, 5.1756],
        [5.1757, 5.4587, 5.3997],
        [5.1757, 5.1757, 5.1757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.1288606822490692 
model_pd.l_d.mean(): -20.14900016784668 
model_pd.lagr.mean(): -20.020139694213867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4628], device='cuda:0')), ('power', tensor([-20.8420], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.1288606822490692
epoch£º424	 i:1 	 global-step:8481	 l-p:0.13359926640987396
epoch£º424	 i:2 	 global-step:8482	 l-p:-1.2184804677963257
epoch£º424	 i:3 	 global-step:8483	 l-p:0.10305292904376984
epoch£º424	 i:4 	 global-step:8484	 l-p:0.0457322858273983
epoch£º424	 i:5 	 global-step:8485	 l-p:0.14393451809883118
epoch£º424	 i:6 	 global-step:8486	 l-p:0.13346174359321594
epoch£º424	 i:7 	 global-step:8487	 l-p:0.08908531814813614
epoch£º424	 i:8 	 global-step:8488	 l-p:-0.040092047303915024
epoch£º424	 i:9 	 global-step:8489	 l-p:0.2719131410121918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0295, 4.9731, 4.9788],
        [5.0295, 4.9714, 4.9134],
        [5.0295, 5.1828, 5.0686],
        [5.0295, 5.0295, 5.0295]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.16886791586875916 
model_pd.l_d.mean(): -19.516023635864258 
model_pd.lagr.mean(): -19.347156524658203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4853], device='cuda:0')), ('power', tensor([-20.2251], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.16886791586875916
epoch£º425	 i:1 	 global-step:8501	 l-p:0.14258833229541779
epoch£º425	 i:2 	 global-step:8502	 l-p:0.24830617010593414
epoch£º425	 i:3 	 global-step:8503	 l-p:-0.3778120279312134
epoch£º425	 i:4 	 global-step:8504	 l-p:0.09022460877895355
epoch£º425	 i:5 	 global-step:8505	 l-p:0.11855294555425644
epoch£º425	 i:6 	 global-step:8506	 l-p:0.11910969763994217
epoch£º425	 i:7 	 global-step:8507	 l-p:0.013482792302966118
epoch£º425	 i:8 	 global-step:8508	 l-p:0.07514572888612747
epoch£º425	 i:9 	 global-step:8509	 l-p:0.1230689212679863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1134, 5.1054, 5.1123],
        [5.1134, 5.1128, 5.1134],
        [5.1134, 5.1110, 5.1133],
        [5.1134, 5.4756, 5.4651]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): -0.6556476354598999 
model_pd.l_d.mean(): -19.04435920715332 
model_pd.lagr.mean(): -19.70000648498535 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5302], device='cuda:0')), ('power', tensor([-19.7942], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:-0.6556476354598999
epoch£º426	 i:1 	 global-step:8521	 l-p:0.1868342012166977
epoch£º426	 i:2 	 global-step:8522	 l-p:0.13615335524082184
epoch£º426	 i:3 	 global-step:8523	 l-p:0.14565634727478027
epoch£º426	 i:4 	 global-step:8524	 l-p:0.12306936830282211
epoch£º426	 i:5 	 global-step:8525	 l-p:0.12669306993484497
epoch£º426	 i:6 	 global-step:8526	 l-p:0.1246882975101471
epoch£º426	 i:7 	 global-step:8527	 l-p:0.09954167902469635
epoch£º426	 i:8 	 global-step:8528	 l-p:0.15288013219833374
epoch£º426	 i:9 	 global-step:8529	 l-p:0.11403320729732513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0576, 5.0519, 5.0570],
        [5.0576, 5.0551, 5.0575],
        [5.0576, 5.0127, 5.0288],
        [5.0576, 5.0619, 4.9395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.1620238870382309 
model_pd.l_d.mean(): -19.812973022460938 
model_pd.lagr.mean(): -19.650949478149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4490], device='cuda:0')), ('power', tensor([-20.4882], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.1620238870382309
epoch£º427	 i:1 	 global-step:8541	 l-p:0.10347340255975723
epoch£º427	 i:2 	 global-step:8542	 l-p:0.26602649688720703
epoch£º427	 i:3 	 global-step:8543	 l-p:0.11590774357318878
epoch£º427	 i:4 	 global-step:8544	 l-p:0.12027762830257416
epoch£º427	 i:5 	 global-step:8545	 l-p:0.005345096345990896
epoch£º427	 i:6 	 global-step:8546	 l-p:0.17395831644535065
epoch£º427	 i:7 	 global-step:8547	 l-p:0.07559029012918472
epoch£º427	 i:8 	 global-step:8548	 l-p:0.1545405387878418
epoch£º427	 i:9 	 global-step:8549	 l-p:0.14119970798492432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6811,  0.5993,  1.0000,  0.5273,
          1.0000,  0.8799, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[4.9930, 5.4511, 5.5136],
        [4.9930, 4.9305, 4.8733],
        [4.9930, 5.3174, 5.2884],
        [4.9930, 4.9389, 4.8612]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.2572583556175232 
model_pd.l_d.mean(): -20.60622787475586 
model_pd.lagr.mean(): -20.348970413208008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4551], device='cuda:0')), ('power', tensor([-21.2963], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.2572583556175232
epoch£º428	 i:1 	 global-step:8561	 l-p:0.1195644810795784
epoch£º428	 i:2 	 global-step:8562	 l-p:0.10661416500806808
epoch£º428	 i:3 	 global-step:8563	 l-p:0.13410057127475739
epoch£º428	 i:4 	 global-step:8564	 l-p:0.12284364551305771
epoch£º428	 i:5 	 global-step:8565	 l-p:0.005495166871696711
epoch£º428	 i:6 	 global-step:8566	 l-p:0.1603393852710724
epoch£º428	 i:7 	 global-step:8567	 l-p:0.1319054216146469
epoch£º428	 i:8 	 global-step:8568	 l-p:0.1720246523618698
epoch£º428	 i:9 	 global-step:8569	 l-p:0.11793335527181625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9484, 4.9050, 4.8043],
        [4.9484, 4.9483, 4.9484],
        [4.9484, 5.1262, 5.0207],
        [4.9484, 5.6242, 5.8638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.11053094267845154 
model_pd.l_d.mean(): -19.405506134033203 
model_pd.lagr.mean(): -19.29497528076172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5920], device='cuda:0')), ('power', tensor([-20.2224], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.11053094267845154
epoch£º429	 i:1 	 global-step:8581	 l-p:0.10583217442035675
epoch£º429	 i:2 	 global-step:8582	 l-p:0.1504831314086914
epoch£º429	 i:3 	 global-step:8583	 l-p:0.15699449181556702
epoch£º429	 i:4 	 global-step:8584	 l-p:0.16846923530101776
epoch£º429	 i:5 	 global-step:8585	 l-p:-0.005361013114452362
epoch£º429	 i:6 	 global-step:8586	 l-p:0.11073654890060425
epoch£º429	 i:7 	 global-step:8587	 l-p:0.11250690370798111
epoch£º429	 i:8 	 global-step:8588	 l-p:0.07945065200328827
epoch£º429	 i:9 	 global-step:8589	 l-p:0.11283382028341293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9798, 4.9495, 4.9682],
        [4.9798, 5.6611, 5.9024],
        [4.9798, 4.9216, 4.9311],
        [4.9798, 4.9785, 4.9798]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.3163439631462097 
model_pd.l_d.mean(): -19.348676681518555 
model_pd.lagr.mean(): -19.032333374023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5955], device='cuda:0')), ('power', tensor([-20.1685], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.3163439631462097
epoch£º430	 i:1 	 global-step:8601	 l-p:0.14902363717556
epoch£º430	 i:2 	 global-step:8602	 l-p:0.13909152150154114
epoch£º430	 i:3 	 global-step:8603	 l-p:0.14962424337863922
epoch£º430	 i:4 	 global-step:8604	 l-p:0.13185586035251617
epoch£º430	 i:5 	 global-step:8605	 l-p:0.12206408381462097
epoch£º430	 i:6 	 global-step:8606	 l-p:0.12025739997625351
epoch£º430	 i:7 	 global-step:8607	 l-p:0.12363563477993011
epoch£º430	 i:8 	 global-step:8608	 l-p:0.11309313774108887
epoch£º430	 i:9 	 global-step:8609	 l-p:0.1490560621023178
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9216, 5.0307, 4.9012],
        [4.9216, 5.5928, 5.8303],
        [4.9216, 4.9096, 4.9195],
        [4.9216, 4.8653, 4.7758]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.040695056319236755 
model_pd.l_d.mean(): -20.118501663208008 
model_pd.lagr.mean(): -20.07780647277832 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5267], device='cuda:0')), ('power', tensor([-20.8764], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.040695056319236755
epoch£º431	 i:1 	 global-step:8621	 l-p:0.10840655863285065
epoch£º431	 i:2 	 global-step:8622	 l-p:0.13779063522815704
epoch£º431	 i:3 	 global-step:8623	 l-p:0.132839635014534
epoch£º431	 i:4 	 global-step:8624	 l-p:0.1654748171567917
epoch£º431	 i:5 	 global-step:8625	 l-p:0.08512245118618011
epoch£º431	 i:6 	 global-step:8626	 l-p:0.12772725522518158
epoch£º431	 i:7 	 global-step:8627	 l-p:0.13929502665996552
epoch£º431	 i:8 	 global-step:8628	 l-p:0.1414969265460968
epoch£º431	 i:9 	 global-step:8629	 l-p:0.17460647225379944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9274, 5.5274, 5.7048],
        [4.9274, 4.9273, 4.9274],
        [4.9274, 4.9094, 4.9232],
        [4.9274, 4.8948, 4.9145]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.1453578919172287 
model_pd.l_d.mean(): -20.18343162536621 
model_pd.lagr.mean(): -20.038074493408203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5216], device='cuda:0')), ('power', tensor([-20.9369], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.1453578919172287
epoch£º432	 i:1 	 global-step:8641	 l-p:0.15429048240184784
epoch£º432	 i:2 	 global-step:8642	 l-p:0.09826591610908508
epoch£º432	 i:3 	 global-step:8643	 l-p:0.12297818809747696
epoch£º432	 i:4 	 global-step:8644	 l-p:0.11274169385433197
epoch£º432	 i:5 	 global-step:8645	 l-p:0.13253189623355865
epoch£º432	 i:6 	 global-step:8646	 l-p:0.13742424547672272
epoch£º432	 i:7 	 global-step:8647	 l-p:0.04361267387866974
epoch£º432	 i:8 	 global-step:8648	 l-p:0.2780317962169647
epoch£º432	 i:9 	 global-step:8649	 l-p:0.14599604904651642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0200, 4.9550, 4.9024],
        [5.0200, 5.2122, 5.1109],
        [5.0200, 5.0200, 5.0200],
        [5.0200, 4.9951, 5.0119]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.13997459411621094 
model_pd.l_d.mean(): -20.100431442260742 
model_pd.lagr.mean(): -19.96045684814453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5007], device='cuda:0')), ('power', tensor([-20.8316], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.13997459411621094
epoch£º433	 i:1 	 global-step:8661	 l-p:0.14511315524578094
epoch£º433	 i:2 	 global-step:8662	 l-p:0.13192375004291534
epoch£º433	 i:3 	 global-step:8663	 l-p:0.3609828054904938
epoch£º433	 i:4 	 global-step:8664	 l-p:0.11369863152503967
epoch£º433	 i:5 	 global-step:8665	 l-p:0.04959358647465706
epoch£º433	 i:6 	 global-step:8666	 l-p:-0.033396728336811066
epoch£º433	 i:7 	 global-step:8667	 l-p:0.18722058832645416
epoch£º433	 i:8 	 global-step:8668	 l-p:0.14608481526374817
epoch£º433	 i:9 	 global-step:8669	 l-p:0.1390991359949112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6197,  0.5284,  1.0000,  0.4505,
          1.0000,  0.8526, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[5.0596, 5.0348, 4.9238],
        [5.0596, 5.3119, 5.2381],
        [5.0596, 4.9946, 4.9821],
        [5.0596, 4.9963, 4.9460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.1454859972000122 
model_pd.l_d.mean(): -20.889707565307617 
model_pd.lagr.mean(): -20.744220733642578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4036], device='cuda:0')), ('power', tensor([-21.5303], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.1454859972000122
epoch£º434	 i:1 	 global-step:8681	 l-p:0.14656496047973633
epoch£º434	 i:2 	 global-step:8682	 l-p:0.11811433732509613
epoch£º434	 i:3 	 global-step:8683	 l-p:0.4601641297340393
epoch£º434	 i:4 	 global-step:8684	 l-p:0.04355756565928459
epoch£º434	 i:5 	 global-step:8685	 l-p:0.19370272755622864
epoch£º434	 i:6 	 global-step:8686	 l-p:0.12205473333597183
epoch£º434	 i:7 	 global-step:8687	 l-p:0.15122613310813904
epoch£º434	 i:8 	 global-step:8688	 l-p:0.14029395580291748
epoch£º434	 i:9 	 global-step:8689	 l-p:0.09845242649316788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9989, 4.9988, 4.9989],
        [4.9989, 4.9989, 4.9989],
        [4.9989, 4.9989, 4.9989],
        [4.9989, 5.5966, 5.7658]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.15368008613586426 
model_pd.l_d.mean(): -20.822301864624023 
model_pd.lagr.mean(): -20.668621063232422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4213], device='cuda:0')), ('power', tensor([-21.4803], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.15368008613586426
epoch£º435	 i:1 	 global-step:8701	 l-p:0.09151545912027359
epoch£º435	 i:2 	 global-step:8702	 l-p:0.10868629068136215
epoch£º435	 i:3 	 global-step:8703	 l-p:0.1259864717721939
epoch£º435	 i:4 	 global-step:8704	 l-p:0.13621175289154053
epoch£º435	 i:5 	 global-step:8705	 l-p:0.09140229970216751
epoch£º435	 i:6 	 global-step:8706	 l-p:0.07936201244592667
epoch£º435	 i:7 	 global-step:8707	 l-p:0.17746245861053467
epoch£º435	 i:8 	 global-step:8708	 l-p:0.16697825491428375
epoch£º435	 i:9 	 global-step:8709	 l-p:0.13046474754810333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9225, 5.4722, 5.6087],
        [4.9225, 5.2241, 5.1831],
        [4.9225, 4.9145, 4.9215],
        [4.9225, 4.9999, 4.8621]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.10476865619421005 
model_pd.l_d.mean(): -20.485824584960938 
model_pd.lagr.mean(): -20.38105583190918 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5112], device='cuda:0')), ('power', tensor([-21.2319], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.10476865619421005
epoch£º436	 i:1 	 global-step:8721	 l-p:0.09906003624200821
epoch£º436	 i:2 	 global-step:8722	 l-p:0.09313379228115082
epoch£º436	 i:3 	 global-step:8723	 l-p:-1.5809714794158936
epoch£º436	 i:4 	 global-step:8724	 l-p:0.12144320458173752
epoch£º436	 i:5 	 global-step:8725	 l-p:0.13835793733596802
epoch£º436	 i:6 	 global-step:8726	 l-p:0.1476997584104538
epoch£º436	 i:7 	 global-step:8727	 l-p:0.1607375293970108
epoch£º436	 i:8 	 global-step:8728	 l-p:0.14202982187271118
epoch£º436	 i:9 	 global-step:8729	 l-p:0.1122974306344986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0221, 5.0183, 5.0219],
        [5.0221, 5.7844, 6.0911],
        [5.0221, 5.2156, 5.1138],
        [5.0221, 5.2472, 5.1600]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.04029417037963867 
model_pd.l_d.mean(): -19.47159767150879 
model_pd.lagr.mean(): -19.431303024291992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4856], device='cuda:0')), ('power', tensor([-20.1804], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.04029417037963867
epoch£º437	 i:1 	 global-step:8741	 l-p:0.3026142120361328
epoch£º437	 i:2 	 global-step:8742	 l-p:0.17054763436317444
epoch£º437	 i:3 	 global-step:8743	 l-p:0.07642175257205963
epoch£º437	 i:4 	 global-step:8744	 l-p:0.0441671758890152
epoch£º437	 i:5 	 global-step:8745	 l-p:0.10599520802497864
epoch£º437	 i:6 	 global-step:8746	 l-p:0.13791754841804504
epoch£º437	 i:7 	 global-step:8747	 l-p:0.1260373294353485
epoch£º437	 i:8 	 global-step:8748	 l-p:0.16499365866184235
epoch£º437	 i:9 	 global-step:8749	 l-p:0.11035501211881638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2465, 5.1950, 5.1418],
        [5.2465, 5.3205, 5.1896],
        [5.2465, 5.2235, 5.2389],
        [5.2465, 5.2465, 5.2465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.1233060359954834 
model_pd.l_d.mean(): -19.37672996520996 
model_pd.lagr.mean(): -19.2534236907959 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4203], device='cuda:0')), ('power', tensor([-20.0179], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.1233060359954834
epoch£º438	 i:1 	 global-step:8761	 l-p:0.13087518513202667
epoch£º438	 i:2 	 global-step:8762	 l-p:0.17888712882995605
epoch£º438	 i:3 	 global-step:8763	 l-p:0.1052708774805069
epoch£º438	 i:4 	 global-step:8764	 l-p:0.07865563035011292
epoch£º438	 i:5 	 global-step:8765	 l-p:0.132019504904747
epoch£º438	 i:6 	 global-step:8766	 l-p:0.1744607836008072
epoch£º438	 i:7 	 global-step:8767	 l-p:0.11758123338222504
epoch£º438	 i:8 	 global-step:8768	 l-p:0.1395963877439499
epoch£º438	 i:9 	 global-step:8769	 l-p:0.11793958395719528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2359, 5.2131, 5.1135],
        [5.2359, 5.2343, 5.1193],
        [5.2359, 5.2381, 5.1211],
        [5.2359, 5.2357, 5.2359]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.08643671125173569 
model_pd.l_d.mean(): -20.754058837890625 
model_pd.lagr.mean(): -20.667621612548828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3582], device='cuda:0')), ('power', tensor([-21.3467], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.08643671125173569
epoch£º439	 i:1 	 global-step:8781	 l-p:0.11780818551778793
epoch£º439	 i:2 	 global-step:8782	 l-p:0.201402947306633
epoch£º439	 i:3 	 global-step:8783	 l-p:0.11663354933261871
epoch£º439	 i:4 	 global-step:8784	 l-p:0.12844257056713104
epoch£º439	 i:5 	 global-step:8785	 l-p:0.46828514337539673
epoch£º439	 i:6 	 global-step:8786	 l-p:0.13216520845890045
epoch£º439	 i:7 	 global-step:8787	 l-p:0.1795024275779724
epoch£º439	 i:8 	 global-step:8788	 l-p:0.14129053056240082
epoch£º439	 i:9 	 global-step:8789	 l-p:0.12423727661371231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1191, 5.0946, 5.1112],
        [5.1191, 5.0994, 5.1138],
        [5.1191, 5.0546, 5.0377],
        [5.1191, 5.3025, 5.1950]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.04062081128358841 
model_pd.l_d.mean(): -19.373525619506836 
model_pd.lagr.mean(): -19.332904815673828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4730], device='cuda:0')), ('power', tensor([-20.0684], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.04062081128358841
epoch£º440	 i:1 	 global-step:8801	 l-p:0.12603630125522614
epoch£º440	 i:2 	 global-step:8802	 l-p:0.10874980688095093
epoch£º440	 i:3 	 global-step:8803	 l-p:0.15004488825798035
epoch£º440	 i:4 	 global-step:8804	 l-p:0.14156892895698547
epoch£º440	 i:5 	 global-step:8805	 l-p:0.13270993530750275
epoch£º440	 i:6 	 global-step:8806	 l-p:0.5244424939155579
epoch£º440	 i:7 	 global-step:8807	 l-p:0.08247844874858856
epoch£º440	 i:8 	 global-step:8808	 l-p:0.1462857723236084
epoch£º440	 i:9 	 global-step:8809	 l-p:0.12918254733085632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0552, 5.0475, 5.0543],
        [5.0552, 5.2238, 5.1109],
        [5.0552, 5.0544, 5.0552],
        [5.0552, 5.0006, 4.9180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.040611881762742996 
model_pd.l_d.mean(): -19.159610748291016 
model_pd.lagr.mean(): -19.118999481201172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5392], device='cuda:0')), ('power', tensor([-19.9199], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.040611881762742996
epoch£º441	 i:1 	 global-step:8821	 l-p:0.1363009214401245
epoch£º441	 i:2 	 global-step:8822	 l-p:0.4037851095199585
epoch£º441	 i:3 	 global-step:8823	 l-p:0.14503273367881775
epoch£º441	 i:4 	 global-step:8824	 l-p:-0.07468000054359436
epoch£º441	 i:5 	 global-step:8825	 l-p:0.1323542594909668
epoch£º441	 i:6 	 global-step:8826	 l-p:0.12554416060447693
epoch£º441	 i:7 	 global-step:8827	 l-p:0.15979252755641937
epoch£º441	 i:8 	 global-step:8828	 l-p:0.11406964808702469
epoch£º441	 i:9 	 global-step:8829	 l-p:0.12181057035923004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1736, 5.1722, 5.1736],
        [5.1736, 5.6050, 5.6350],
        [5.1736, 5.2262, 5.0921],
        [5.1736, 5.1550, 5.1688]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.12335552275180817 
model_pd.l_d.mean(): -20.277299880981445 
model_pd.lagr.mean(): -20.15394401550293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4136], device='cuda:0')), ('power', tensor([-20.9214], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.12335552275180817
epoch£º442	 i:1 	 global-step:8841	 l-p:0.13034524023532867
epoch£º442	 i:2 	 global-step:8842	 l-p:0.43267616629600525
epoch£º442	 i:3 	 global-step:8843	 l-p:0.15042079985141754
epoch£º442	 i:4 	 global-step:8844	 l-p:0.149520605802536
epoch£º442	 i:5 	 global-step:8845	 l-p:-0.07818982005119324
epoch£º442	 i:6 	 global-step:8846	 l-p:0.13757452368736267
epoch£º442	 i:7 	 global-step:8847	 l-p:0.13852500915527344
epoch£º442	 i:8 	 global-step:8848	 l-p:0.11869495362043381
epoch£º442	 i:9 	 global-step:8849	 l-p:-0.33855825662612915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1586, 5.1241, 5.1428],
        [5.1586, 5.1579, 5.1586],
        [5.1586, 5.1577, 5.1586],
        [5.1586, 5.1140, 5.1310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.11317839473485947 
model_pd.l_d.mean(): -20.046892166137695 
model_pd.lagr.mean(): -19.933713912963867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4250], device='cuda:0')), ('power', tensor([-20.7001], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.11317839473485947
epoch£º443	 i:1 	 global-step:8861	 l-p:0.1334223747253418
epoch£º443	 i:2 	 global-step:8862	 l-p:0.12838365137577057
epoch£º443	 i:3 	 global-step:8863	 l-p:-1.6046864986419678
epoch£º443	 i:4 	 global-step:8864	 l-p:0.157845601439476
epoch£º443	 i:5 	 global-step:8865	 l-p:0.13729336857795715
epoch£º443	 i:6 	 global-step:8866	 l-p:0.11653812974691391
epoch£º443	 i:7 	 global-step:8867	 l-p:0.023350343108177185
epoch£º443	 i:8 	 global-step:8868	 l-p:0.12772105634212494
epoch£º443	 i:9 	 global-step:8869	 l-p:0.4463547170162201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0653, 5.2195, 5.1006],
        [5.0653, 5.0192, 5.0380],
        [5.0653, 4.9963, 4.9539],
        [5.0653, 5.1624, 5.0280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 2.678340435028076 
model_pd.l_d.mean(): -18.971601486206055 
model_pd.lagr.mean(): -16.29326057434082 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5351], device='cuda:0')), ('power', tensor([-19.7255], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:2.678340435028076
epoch£º444	 i:1 	 global-step:8881	 l-p:0.14176782965660095
epoch£º444	 i:2 	 global-step:8882	 l-p:0.1339072585105896
epoch£º444	 i:3 	 global-step:8883	 l-p:0.1272844672203064
epoch£º444	 i:4 	 global-step:8884	 l-p:0.7146897912025452
epoch£º444	 i:5 	 global-step:8885	 l-p:0.12848754227161407
epoch£º444	 i:6 	 global-step:8886	 l-p:0.14749988913536072
epoch£º444	 i:7 	 global-step:8887	 l-p:0.15731722116470337
epoch£º444	 i:8 	 global-step:8888	 l-p:0.16607607901096344
epoch£º444	 i:9 	 global-step:8889	 l-p:-0.18320536613464355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9674, 4.9670, 4.9674],
        [4.9674, 4.8984, 4.8255],
        [4.9674, 4.9571, 4.9659],
        [4.9674, 4.8984, 4.9002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): -0.24998436868190765 
model_pd.l_d.mean(): -20.63991355895996 
model_pd.lagr.mean(): -20.8898983001709 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4648], device='cuda:0')), ('power', tensor([-21.3403], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:-0.24998436868190765
epoch£º445	 i:1 	 global-step:8901	 l-p:0.14081379771232605
epoch£º445	 i:2 	 global-step:8902	 l-p:0.10280347615480423
epoch£º445	 i:3 	 global-step:8903	 l-p:0.00784599781036377
epoch£º445	 i:4 	 global-step:8904	 l-p:0.1363927125930786
epoch£º445	 i:5 	 global-step:8905	 l-p:0.16715243458747864
epoch£º445	 i:6 	 global-step:8906	 l-p:0.10905284434556961
epoch£º445	 i:7 	 global-step:8907	 l-p:0.11659155786037445
epoch£º445	 i:8 	 global-step:8908	 l-p:0.1331886649131775
epoch£º445	 i:9 	 global-step:8909	 l-p:0.1288786083459854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[4.9930, 4.9254, 4.8529],
        [4.9930, 5.4551, 5.5178],
        [4.9930, 4.9576, 4.8403],
        [4.9930, 5.4810, 5.5631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.11228670179843903 
model_pd.l_d.mean(): -20.20298194885254 
model_pd.lagr.mean(): -20.090694427490234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5009], device='cuda:0')), ('power', tensor([-20.9354], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.11228670179843903
epoch£º446	 i:1 	 global-step:8921	 l-p:0.14107809960842133
epoch£º446	 i:2 	 global-step:8922	 l-p:0.13149000704288483
epoch£º446	 i:3 	 global-step:8923	 l-p:0.10015137493610382
epoch£º446	 i:4 	 global-step:8924	 l-p:0.14143361151218414
epoch£º446	 i:5 	 global-step:8925	 l-p:0.11877156049013138
epoch£º446	 i:6 	 global-step:8926	 l-p:0.14235861599445343
epoch£º446	 i:7 	 global-step:8927	 l-p:0.14918360114097595
epoch£º446	 i:8 	 global-step:8928	 l-p:0.052207767963409424
epoch£º446	 i:9 	 global-step:8929	 l-p:0.07870656996965408
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9593, 4.9544, 4.9589],
        [4.9593, 4.9212, 4.9425],
        [4.9593, 4.8969, 4.8054],
        [4.9593, 4.8993, 4.8039]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.1041947603225708 
model_pd.l_d.mean(): -20.392541885375977 
model_pd.lagr.mean(): -20.288347244262695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4935], device='cuda:0')), ('power', tensor([-21.1195], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.1041947603225708
epoch£º447	 i:1 	 global-step:8941	 l-p:0.14499957859516144
epoch£º447	 i:2 	 global-step:8942	 l-p:0.15058383345603943
epoch£º447	 i:3 	 global-step:8943	 l-p:0.1372394859790802
epoch£º447	 i:4 	 global-step:8944	 l-p:0.135090172290802
epoch£º447	 i:5 	 global-step:8945	 l-p:0.002225894946604967
epoch£º447	 i:6 	 global-step:8946	 l-p:0.148329958319664
epoch£º447	 i:7 	 global-step:8947	 l-p:0.10221147537231445
epoch£º447	 i:8 	 global-step:8948	 l-p:0.09941200166940689
epoch£º447	 i:9 	 global-step:8949	 l-p:0.11950944364070892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9689, 5.5441, 5.6955],
        [4.9689, 5.3618, 5.3762],
        [4.9689, 4.9405, 4.9595],
        [4.9689, 4.9688, 4.9689]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.13721615076065063 
model_pd.l_d.mean(): -19.82135009765625 
model_pd.lagr.mean(): -19.684133529663086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5347], device='cuda:0')), ('power', tensor([-20.5843], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.13721615076065063
epoch£º448	 i:1 	 global-step:8961	 l-p:0.10001708567142487
epoch£º448	 i:2 	 global-step:8962	 l-p:0.14822734892368317
epoch£º448	 i:3 	 global-step:8963	 l-p:0.02145351842045784
epoch£º448	 i:4 	 global-step:8964	 l-p:0.0970890000462532
epoch£º448	 i:5 	 global-step:8965	 l-p:0.09307773411273956
epoch£º448	 i:6 	 global-step:8966	 l-p:0.1370355635881424
epoch£º448	 i:7 	 global-step:8967	 l-p:0.13386593759059906
epoch£º448	 i:8 	 global-step:8968	 l-p:0.2221127301454544
epoch£º448	 i:9 	 global-step:8969	 l-p:0.13792328536510468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9207, 4.9186, 4.9206],
        [4.9207, 4.8411, 4.8256],
        [4.9207, 4.8811, 4.9032],
        [4.9207, 4.8726, 4.8946]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.118593730032444 
model_pd.l_d.mean(): -20.77440071105957 
model_pd.lagr.mean(): -20.655807495117188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4669], device='cuda:0')), ('power', tensor([-21.4784], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.118593730032444
epoch£º449	 i:1 	 global-step:8981	 l-p:0.10988403111696243
epoch£º449	 i:2 	 global-step:8982	 l-p:0.17105364799499512
epoch£º449	 i:3 	 global-step:8983	 l-p:0.17524398863315582
epoch£º449	 i:4 	 global-step:8984	 l-p:0.16150672733783722
epoch£º449	 i:5 	 global-step:8985	 l-p:0.06690583378076553
epoch£º449	 i:6 	 global-step:8986	 l-p:0.14247636497020721
epoch£º449	 i:7 	 global-step:8987	 l-p:0.12291204929351807
epoch£º449	 i:8 	 global-step:8988	 l-p:0.1800851821899414
epoch£º449	 i:9 	 global-step:8989	 l-p:0.11160705238580704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9554, 4.9550, 4.9554],
        [4.9554, 4.9553, 4.9554],
        [4.9554, 4.9523, 4.9552],
        [4.9554, 5.2072, 5.1330]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.09593841433525085 
model_pd.l_d.mean(): -20.684410095214844 
model_pd.lagr.mean(): -20.588472366333008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4684], device='cuda:0')), ('power', tensor([-21.3890], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.09593841433525085
epoch£º450	 i:1 	 global-step:9001	 l-p:0.15839967131614685
epoch£º450	 i:2 	 global-step:9002	 l-p:0.12063460797071457
epoch£º450	 i:3 	 global-step:9003	 l-p:0.128152534365654
epoch£º450	 i:4 	 global-step:9004	 l-p:0.08190242946147919
epoch£º450	 i:5 	 global-step:9005	 l-p:-0.11833284050226212
epoch£º450	 i:6 	 global-step:9006	 l-p:0.1781443953514099
epoch£º450	 i:7 	 global-step:9007	 l-p:0.1496971994638443
epoch£º450	 i:8 	 global-step:9008	 l-p:0.09638930857181549
epoch£º450	 i:9 	 global-step:9009	 l-p:0.12161146849393845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0001, 4.9777, 4.9941],
        [5.0001, 4.9482, 4.9681],
        [5.0001, 5.0000, 5.0001],
        [5.0001, 4.9999, 5.0001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.13542599976062775 
model_pd.l_d.mean(): -18.771961212158203 
model_pd.lagr.mean(): -18.63653564453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5881], device='cuda:0')), ('power', tensor([-19.5779], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.13542599976062775
epoch£º451	 i:1 	 global-step:9021	 l-p:0.10901772975921631
epoch£º451	 i:2 	 global-step:9022	 l-p:0.14266157150268555
epoch£º451	 i:3 	 global-step:9023	 l-p:0.15958857536315918
epoch£º451	 i:4 	 global-step:9024	 l-p:0.07882130891084671
epoch£º451	 i:5 	 global-step:9025	 l-p:0.12009517103433609
epoch£º451	 i:6 	 global-step:9026	 l-p:-0.019278334453701973
epoch£º451	 i:7 	 global-step:9027	 l-p:0.1511225700378418
epoch£º451	 i:8 	 global-step:9028	 l-p:0.13574492931365967
epoch£º451	 i:9 	 global-step:9029	 l-p:0.05919117480516434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0072, 4.9451, 4.9589],
        [5.0072, 5.0058, 5.0071],
        [5.0072, 5.0045, 5.0070],
        [5.0072, 5.0072, 5.0072]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.28516727685928345 
model_pd.l_d.mean(): -20.097400665283203 
model_pd.lagr.mean(): -19.812232971191406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5070], device='cuda:0')), ('power', tensor([-20.8350], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.28516727685928345
epoch£º452	 i:1 	 global-step:9041	 l-p:-0.11475706100463867
epoch£º452	 i:2 	 global-step:9042	 l-p:0.10921233892440796
epoch£º452	 i:3 	 global-step:9043	 l-p:0.12672209739685059
epoch£º452	 i:4 	 global-step:9044	 l-p:0.12875579297542572
epoch£º452	 i:5 	 global-step:9045	 l-p:0.0765993595123291
epoch£º452	 i:6 	 global-step:9046	 l-p:0.12259288877248764
epoch£º452	 i:7 	 global-step:9047	 l-p:0.06305605173110962
epoch£º452	 i:8 	 global-step:9048	 l-p:0.13456645607948303
epoch£º452	 i:9 	 global-step:9049	 l-p:0.13721996545791626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0562, 5.0548, 5.0561],
        [5.0562, 5.0562, 5.0562],
        [5.0562, 5.0541, 5.0561],
        [5.0562, 5.0195, 5.0404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.27040985226631165 
model_pd.l_d.mean(): -20.695104598999023 
model_pd.lagr.mean(): -20.424694061279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4328], device='cuda:0')), ('power', tensor([-21.3634], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.27040985226631165
epoch£º453	 i:1 	 global-step:9061	 l-p:0.1438576877117157
epoch£º453	 i:2 	 global-step:9062	 l-p:0.06997878849506378
epoch£º453	 i:3 	 global-step:9063	 l-p:0.08095833659172058
epoch£º453	 i:4 	 global-step:9064	 l-p:0.1269584745168686
epoch£º453	 i:5 	 global-step:9065	 l-p:0.1301591396331787
epoch£º453	 i:6 	 global-step:9066	 l-p:0.2769865095615387
epoch£º453	 i:7 	 global-step:9067	 l-p:0.07825679332017899
epoch£º453	 i:8 	 global-step:9068	 l-p:0.11063947528600693
epoch£º453	 i:9 	 global-step:9069	 l-p:0.13955944776535034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1220, 5.0551, 5.0547],
        [5.1220, 5.2650, 5.1390],
        [5.1220, 5.1193, 5.1219],
        [5.1220, 5.1104, 5.1201]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.14336182177066803 
model_pd.l_d.mean(): -20.39954376220703 
model_pd.lagr.mean(): -20.256181716918945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4156], device='cuda:0')), ('power', tensor([-21.0471], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.14336182177066803
epoch£º454	 i:1 	 global-step:9081	 l-p:-0.1398916095495224
epoch£º454	 i:2 	 global-step:9082	 l-p:0.17050155997276306
epoch£º454	 i:3 	 global-step:9083	 l-p:0.14706538617610931
epoch£º454	 i:4 	 global-step:9084	 l-p:0.15691308677196503
epoch£º454	 i:5 	 global-step:9085	 l-p:0.12277854979038239
epoch£º454	 i:6 	 global-step:9086	 l-p:0.11661656945943832
epoch£º454	 i:7 	 global-step:9087	 l-p:0.09898509085178375
epoch£º454	 i:8 	 global-step:9088	 l-p:-0.03098219260573387
epoch£º454	 i:9 	 global-step:9089	 l-p:0.13332203030586243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0211, 4.9498, 4.8738],
        [5.0211, 5.0206, 5.0210],
        [5.0211, 5.0209, 5.0211],
        [5.0211, 5.0211, 5.0211]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.13014338910579681 
model_pd.l_d.mean(): -20.21233558654785 
model_pd.lagr.mean(): -20.082191467285156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4918], device='cuda:0')), ('power', tensor([-20.9356], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.13014338910579681
epoch£º455	 i:1 	 global-step:9101	 l-p:0.12092632055282593
epoch£º455	 i:2 	 global-step:9102	 l-p:0.15164248645305634
epoch£º455	 i:3 	 global-step:9103	 l-p:0.12648533284664154
epoch£º455	 i:4 	 global-step:9104	 l-p:0.09758403897285461
epoch£º455	 i:5 	 global-step:9105	 l-p:0.11059749126434326
epoch£º455	 i:6 	 global-step:9106	 l-p:0.12698984146118164
epoch£º455	 i:7 	 global-step:9107	 l-p:0.26235079765319824
epoch£º455	 i:8 	 global-step:9108	 l-p:0.1694769561290741
epoch£º455	 i:9 	 global-step:9109	 l-p:0.12969250977039337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8448, 4.8448, 4.8448],
        [4.8448, 4.7769, 4.7945],
        [4.8448, 4.8442, 4.8447],
        [4.8448, 4.8418, 4.8446]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.11496030539274216 
model_pd.l_d.mean(): -20.446266174316406 
model_pd.lagr.mean(): -20.33130645751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5088], device='cuda:0')), ('power', tensor([-21.1895], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.11496030539274216
epoch£º456	 i:1 	 global-step:9121	 l-p:0.16323043406009674
epoch£º456	 i:2 	 global-step:9122	 l-p:0.152659073472023
epoch£º456	 i:3 	 global-step:9123	 l-p:0.06934069097042084
epoch£º456	 i:4 	 global-step:9124	 l-p:0.13545769453048706
epoch£º456	 i:5 	 global-step:9125	 l-p:0.18153619766235352
epoch£º456	 i:6 	 global-step:9126	 l-p:0.141148179769516
epoch£º456	 i:7 	 global-step:9127	 l-p:0.21477092802524567
epoch£º456	 i:8 	 global-step:9128	 l-p:0.14696821570396423
epoch£º456	 i:9 	 global-step:9129	 l-p:0.0966411754488945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9065, 5.0788, 4.9642],
        [4.9065, 4.8351, 4.8469],
        [4.9065, 4.9111, 4.7615],
        [4.9065, 4.9065, 4.9065]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.20223015546798706 
model_pd.l_d.mean(): -20.885398864746094 
model_pd.lagr.mean(): -20.683168411254883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4569], device='cuda:0')), ('power', tensor([-21.5804], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.20223015546798706
epoch£º457	 i:1 	 global-step:9141	 l-p:0.08589042723178864
epoch£º457	 i:2 	 global-step:9142	 l-p:0.146482452750206
epoch£º457	 i:3 	 global-step:9143	 l-p:0.14075224101543427
epoch£º457	 i:4 	 global-step:9144	 l-p:0.10785672813653946
epoch£º457	 i:5 	 global-step:9145	 l-p:0.1547756791114807
epoch£º457	 i:6 	 global-step:9146	 l-p:0.12602929770946503
epoch£º457	 i:7 	 global-step:9147	 l-p:0.1180572658777237
epoch£º457	 i:8 	 global-step:9148	 l-p:0.045867323875427246
epoch£º457	 i:9 	 global-step:9149	 l-p:0.1318768709897995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0620, 5.0619, 5.0620],
        [5.0620, 5.0207, 5.0423],
        [5.0620, 5.0620, 5.0620],
        [5.0620, 5.0619, 5.0620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.15190166234970093 
model_pd.l_d.mean(): -20.72648811340332 
model_pd.lagr.mean(): -20.574586868286133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4272], device='cuda:0')), ('power', tensor([-21.3894], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.15190166234970093
epoch£º458	 i:1 	 global-step:9161	 l-p:0.11887302249670029
epoch£º458	 i:2 	 global-step:9162	 l-p:0.13297617435455322
epoch£º458	 i:3 	 global-step:9163	 l-p:0.19078876078128815
epoch£º458	 i:4 	 global-step:9164	 l-p:0.115218885242939
epoch£º458	 i:5 	 global-step:9165	 l-p:0.12638157606124878
epoch£º458	 i:6 	 global-step:9166	 l-p:-0.003458433086052537
epoch£º458	 i:7 	 global-step:9167	 l-p:0.1331253945827484
epoch£º458	 i:8 	 global-step:9168	 l-p:0.12362644821405411
epoch£º458	 i:9 	 global-step:9169	 l-p:0.24943821132183075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1060, 5.1060, 5.1060],
        [5.1060, 5.2429, 5.1135],
        [5.1060, 5.8591, 6.1457],
        [5.1060, 5.1060, 5.1060]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.13972331583499908 
model_pd.l_d.mean(): -20.221439361572266 
model_pd.lagr.mean(): -20.081716537475586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4229], device='cuda:0')), ('power', tensor([-20.8744], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.13972331583499908
epoch£º459	 i:1 	 global-step:9181	 l-p:0.1441441923379898
epoch£º459	 i:2 	 global-step:9182	 l-p:0.11607763916254044
epoch£º459	 i:3 	 global-step:9183	 l-p:0.25636357069015503
epoch£º459	 i:4 	 global-step:9184	 l-p:0.07500870525836945
epoch£º459	 i:5 	 global-step:9185	 l-p:0.05984523519873619
epoch£º459	 i:6 	 global-step:9186	 l-p:0.13635358214378357
epoch£º459	 i:7 	 global-step:9187	 l-p:0.11337961256504059
epoch£º459	 i:8 	 global-step:9188	 l-p:0.151861771941185
epoch£º459	 i:9 	 global-step:9189	 l-p:0.04590782895684242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0351, 5.0257, 5.0339],
        [5.0351, 4.9616, 4.8869],
        [5.0351, 5.7050, 5.9261],
        [5.0351, 5.0682, 4.9207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.1235136017203331 
model_pd.l_d.mean(): -18.784326553344727 
model_pd.lagr.mean(): -18.660812377929688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5695], device='cuda:0')), ('power', tensor([-19.5715], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.1235136017203331
epoch£º460	 i:1 	 global-step:9201	 l-p:0.13083386421203613
epoch£º460	 i:2 	 global-step:9202	 l-p:0.12623298168182373
epoch£º460	 i:3 	 global-step:9203	 l-p:0.2526572346687317
epoch£º460	 i:4 	 global-step:9204	 l-p:0.10878860950469971
epoch£º460	 i:5 	 global-step:9205	 l-p:0.14562268555164337
epoch£º460	 i:6 	 global-step:9206	 l-p:0.14427737891674042
epoch£º460	 i:7 	 global-step:9207	 l-p:0.13826678693294525
epoch£º460	 i:8 	 global-step:9208	 l-p:0.1014234870672226
epoch£º460	 i:9 	 global-step:9209	 l-p:0.11047373712062836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9417, 4.9397, 4.9416],
        [4.9417, 4.8573, 4.7913],
        [4.9417, 4.8959, 4.9193],
        [4.9417, 5.0256, 4.8800]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.16857734322547913 
model_pd.l_d.mean(): -18.578075408935547 
model_pd.lagr.mean(): -18.40949821472168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5820], device='cuda:0')), ('power', tensor([-19.3757], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.16857734322547913
epoch£º461	 i:1 	 global-step:9221	 l-p:0.03852418437600136
epoch£º461	 i:2 	 global-step:9222	 l-p:0.11184951663017273
epoch£º461	 i:3 	 global-step:9223	 l-p:0.15963596105575562
epoch£º461	 i:4 	 global-step:9224	 l-p:0.13899965584278107
epoch£º461	 i:5 	 global-step:9225	 l-p:0.12978079915046692
epoch£º461	 i:6 	 global-step:9226	 l-p:0.15967577695846558
epoch£º461	 i:7 	 global-step:9227	 l-p:0.07770399004220963
epoch£º461	 i:8 	 global-step:9228	 l-p:0.09767671674489975
epoch£º461	 i:9 	 global-step:9229	 l-p:0.11695852130651474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0072, 4.9676, 4.8399],
        [5.0072, 4.9415, 4.8418],
        [5.0072, 4.9667, 4.9892],
        [5.0072, 4.9673, 4.9897]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.09229443222284317 
model_pd.l_d.mean(): -20.467147827148438 
model_pd.lagr.mean(): -20.374853134155273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4731], device='cuda:0')), ('power', tensor([-21.1742], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.09229443222284317
epoch£º462	 i:1 	 global-step:9241	 l-p:0.2956402897834778
epoch£º462	 i:2 	 global-step:9242	 l-p:0.14596545696258545
epoch£º462	 i:3 	 global-step:9243	 l-p:0.0969996452331543
epoch£º462	 i:4 	 global-step:9244	 l-p:-0.18143703043460846
epoch£º462	 i:5 	 global-step:9245	 l-p:0.1196906641125679
epoch£º462	 i:6 	 global-step:9246	 l-p:0.15149015188217163
epoch£º462	 i:7 	 global-step:9247	 l-p:0.05916814133524895
epoch£º462	 i:8 	 global-step:9248	 l-p:0.09247134625911713
epoch£º462	 i:9 	 global-step:9249	 l-p:0.14089225232601166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0680, 5.0664, 5.0679],
        [5.0680, 5.0572, 5.0664],
        [5.0680, 5.0584, 5.0667],
        [5.0680, 5.0678, 5.0680]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.1985720545053482 
model_pd.l_d.mean(): -18.224905014038086 
model_pd.lagr.mean(): -18.02633285522461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5978], device='cuda:0')), ('power', tensor([-19.0348], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.1985720545053482
epoch£º463	 i:1 	 global-step:9261	 l-p:2.1454930305480957
epoch£º463	 i:2 	 global-step:9262	 l-p:0.14558863639831543
epoch£º463	 i:3 	 global-step:9263	 l-p:0.04939877986907959
epoch£º463	 i:4 	 global-step:9264	 l-p:0.12656410038471222
epoch£º463	 i:5 	 global-step:9265	 l-p:0.11819124221801758
epoch£º463	 i:6 	 global-step:9266	 l-p:0.123306043446064
epoch£º463	 i:7 	 global-step:9267	 l-p:0.13010062277317047
epoch£º463	 i:8 	 global-step:9268	 l-p:0.07293693721294403
epoch£º463	 i:9 	 global-step:9269	 l-p:0.11821877956390381
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1050, 5.1015, 5.1048],
        [5.1050, 5.1944, 5.0515],
        [5.1050, 5.3091, 5.2042],
        [5.1050, 5.1050, 5.1050]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): -0.011898565106093884 
model_pd.l_d.mean(): -20.584484100341797 
model_pd.lagr.mean(): -20.59638214111328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4274], device='cuda:0')), ('power', tensor([-21.2461], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:-0.011898565106093884
epoch£º464	 i:1 	 global-step:9281	 l-p:0.18515737354755402
epoch£º464	 i:2 	 global-step:9282	 l-p:0.06589288264513016
epoch£º464	 i:3 	 global-step:9283	 l-p:0.13300839066505432
epoch£º464	 i:4 	 global-step:9284	 l-p:0.11296382546424866
epoch£º464	 i:5 	 global-step:9285	 l-p:0.12168493121862411
epoch£º464	 i:6 	 global-step:9286	 l-p:0.2644922137260437
epoch£º464	 i:7 	 global-step:9287	 l-p:0.1440003514289856
epoch£º464	 i:8 	 global-step:9288	 l-p:0.17491096258163452
epoch£º464	 i:9 	 global-step:9289	 l-p:0.13281142711639404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0986, 5.0986, 5.0986],
        [5.0986, 5.0972, 5.0985],
        [5.0986, 5.0787, 5.0938],
        [5.0986, 5.0202, 4.9758]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.12929080426692963 
model_pd.l_d.mean(): -20.124752044677734 
model_pd.lagr.mean(): -19.995460510253906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4889], device='cuda:0')), ('power', tensor([-20.8442], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.12929080426692963
epoch£º465	 i:1 	 global-step:9301	 l-p:0.35663121938705444
epoch£º465	 i:2 	 global-step:9302	 l-p:0.12326321005821228
epoch£º465	 i:3 	 global-step:9303	 l-p:0.13368694484233856
epoch£º465	 i:4 	 global-step:9304	 l-p:0.12015523761510849
epoch£º465	 i:5 	 global-step:9305	 l-p:0.12923334538936615
epoch£º465	 i:6 	 global-step:9306	 l-p:0.14329303801059723
epoch£º465	 i:7 	 global-step:9307	 l-p:0.15688230097293854
epoch£º465	 i:8 	 global-step:9308	 l-p:0.1424349993467331
epoch£º465	 i:9 	 global-step:9309	 l-p:0.09584386646747589
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0032, 4.9214, 4.9108],
        [5.0032, 4.9177, 4.8749],
        [5.0032, 4.9877, 5.0003],
        [5.0032, 4.9998, 4.8526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.13845452666282654 
model_pd.l_d.mean(): -18.976581573486328 
model_pd.lagr.mean(): -18.83812713623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5467], device='cuda:0')), ('power', tensor([-19.7425], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.13845452666282654
epoch£º466	 i:1 	 global-step:9321	 l-p:0.12594464421272278
epoch£º466	 i:2 	 global-step:9322	 l-p:0.13723482191562653
epoch£º466	 i:3 	 global-step:9323	 l-p:0.1297382265329361
epoch£º466	 i:4 	 global-step:9324	 l-p:0.7971574068069458
epoch£º466	 i:5 	 global-step:9325	 l-p:0.05369938164949417
epoch£º466	 i:6 	 global-step:9326	 l-p:0.11376004666090012
epoch£º466	 i:7 	 global-step:9327	 l-p:0.07981833070516586
epoch£º466	 i:8 	 global-step:9328	 l-p:0.12615475058555603
epoch£º466	 i:9 	 global-step:9329	 l-p:0.1110236793756485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0511, 5.0511, 5.0511],
        [5.0511, 5.1851, 5.0522],
        [5.0511, 5.0279, 5.0449],
        [5.0511, 4.9683, 4.9396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.12532314658164978 
model_pd.l_d.mean(): -20.327716827392578 
model_pd.lagr.mean(): -20.202394485473633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4670], device='cuda:0')), ('power', tensor([-21.0270], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.12532314658164978
epoch£º467	 i:1 	 global-step:9341	 l-p:0.18871156871318817
epoch£º467	 i:2 	 global-step:9342	 l-p:0.32101520895957947
epoch£º467	 i:3 	 global-step:9343	 l-p:0.12141930311918259
epoch£º467	 i:4 	 global-step:9344	 l-p:0.1438879370689392
epoch£º467	 i:5 	 global-step:9345	 l-p:0.7133194804191589
epoch£º467	 i:6 	 global-step:9346	 l-p:0.052643146365880966
epoch£º467	 i:7 	 global-step:9347	 l-p:0.05256533995270729
epoch£º467	 i:8 	 global-step:9348	 l-p:0.11673682183027267
epoch£º467	 i:9 	 global-step:9349	 l-p:0.11387025564908981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1818, 5.1491, 5.1693],
        [5.1818, 5.3484, 5.2265],
        [5.1818, 5.1304, 5.1497],
        [5.1818, 5.8654, 6.0848]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.43222203850746155 
model_pd.l_d.mean(): -20.51917266845703 
model_pd.lagr.mean(): -20.086950302124023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4175], device='cuda:0')), ('power', tensor([-21.1699], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.43222203850746155
epoch£º468	 i:1 	 global-step:9361	 l-p:0.10659770667552948
epoch£º468	 i:2 	 global-step:9362	 l-p:0.12663310766220093
epoch£º468	 i:3 	 global-step:9363	 l-p:0.20752345025539398
epoch£º468	 i:4 	 global-step:9364	 l-p:0.14546020328998566
epoch£º468	 i:5 	 global-step:9365	 l-p:0.14423206448554993
epoch£º468	 i:6 	 global-step:9366	 l-p:0.12972106039524078
epoch£º468	 i:7 	 global-step:9367	 l-p:0.11154656857252121
epoch£º468	 i:8 	 global-step:9368	 l-p:-0.15615519881248474
epoch£º468	 i:9 	 global-step:9369	 l-p:0.11342586576938629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1380, 5.0962, 5.1181],
        [5.1380, 5.1380, 5.1380],
        [5.1380, 5.1352, 5.1379],
        [5.1380, 5.1380, 5.1380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.09384520351886749 
model_pd.l_d.mean(): -20.473892211914062 
model_pd.lagr.mean(): -20.380046844482422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4125], device='cuda:0')), ('power', tensor([-21.1190], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.09384520351886749
epoch£º469	 i:1 	 global-step:9381	 l-p:0.08526021987199783
epoch£º469	 i:2 	 global-step:9382	 l-p:0.12925969064235687
epoch£º469	 i:3 	 global-step:9383	 l-p:0.13550294935703278
epoch£º469	 i:4 	 global-step:9384	 l-p:-0.014565281569957733
epoch£º469	 i:5 	 global-step:9385	 l-p:0.6046438217163086
epoch£º469	 i:6 	 global-step:9386	 l-p:0.1836780309677124
epoch£º469	 i:7 	 global-step:9387	 l-p:0.13888059556484222
epoch£º469	 i:8 	 global-step:9388	 l-p:0.10716956108808517
epoch£º469	 i:9 	 global-step:9389	 l-p:0.11715703457593918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0369, 5.0366, 5.0369],
        [5.0369, 4.9540, 4.9391],
        [5.0369, 4.9832, 5.0053],
        [5.0369, 5.0367, 5.0369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.12240812927484512 
model_pd.l_d.mean(): -20.592073440551758 
model_pd.lagr.mean(): -20.46966552734375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4397], device='cuda:0')), ('power', tensor([-21.2663], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.12240812927484512
epoch£º470	 i:1 	 global-step:9401	 l-p:0.1043572723865509
epoch£º470	 i:2 	 global-step:9402	 l-p:0.13301633298397064
epoch£º470	 i:3 	 global-step:9403	 l-p:0.06126507744193077
epoch£º470	 i:4 	 global-step:9404	 l-p:0.15292321145534515
epoch£º470	 i:5 	 global-step:9405	 l-p:0.09379732608795166
epoch£º470	 i:6 	 global-step:9406	 l-p:0.16540254652500153
epoch£º470	 i:7 	 global-step:9407	 l-p:0.15128938853740692
epoch£º470	 i:8 	 global-step:9408	 l-p:0.0904538482427597
epoch£º470	 i:9 	 global-step:9409	 l-p:-0.9767969846725464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0093, 5.0093, 5.0093],
        [5.0093, 5.0093, 5.0093],
        [5.0093, 5.0093, 5.0093],
        [5.0093, 5.0093, 5.0093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.4469079077243805 
model_pd.l_d.mean(): -20.064632415771484 
model_pd.lagr.mean(): -19.617725372314453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5136], device='cuda:0')), ('power', tensor([-20.8086], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.4469079077243805
epoch£º471	 i:1 	 global-step:9421	 l-p:0.14287573099136353
epoch£º471	 i:2 	 global-step:9422	 l-p:-1.1701600551605225
epoch£º471	 i:3 	 global-step:9423	 l-p:0.1393413245677948
epoch£º471	 i:4 	 global-step:9424	 l-p:-0.07712230831384659
epoch£º471	 i:5 	 global-step:9425	 l-p:0.13867343962192535
epoch£º471	 i:6 	 global-step:9426	 l-p:0.127202570438385
epoch£º471	 i:7 	 global-step:9427	 l-p:0.13330009579658508
epoch£º471	 i:8 	 global-step:9428	 l-p:0.06880318373441696
epoch£º471	 i:9 	 global-step:9429	 l-p:0.13764294981956482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0717, 5.0081, 4.9042],
        [5.0717, 5.0511, 5.0667],
        [5.0717, 5.4213, 5.3966],
        [5.0717, 5.0717, 5.0717]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.12865114212036133 
model_pd.l_d.mean(): -20.61882972717285 
model_pd.lagr.mean(): -20.49017906188965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4320], device='cuda:0')), ('power', tensor([-21.2855], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.12865114212036133
epoch£º472	 i:1 	 global-step:9441	 l-p:0.2980669140815735
epoch£º472	 i:2 	 global-step:9442	 l-p:0.12694452702999115
epoch£º472	 i:3 	 global-step:9443	 l-p:0.13717253506183624
epoch£º472	 i:4 	 global-step:9444	 l-p:0.14322710037231445
epoch£º472	 i:5 	 global-step:9445	 l-p:0.12308420240879059
epoch£º472	 i:6 	 global-step:9446	 l-p:0.08273769170045853
epoch£º472	 i:7 	 global-step:9447	 l-p:0.11405513435602188
epoch£º472	 i:8 	 global-step:9448	 l-p:0.19411920011043549
epoch£º472	 i:9 	 global-step:9449	 l-p:0.12090970575809479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9241, 4.8298, 4.7788],
        [4.9241, 4.9240, 4.9241],
        [4.9241, 4.9240, 4.9241],
        [4.9241, 4.9013, 4.9184]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.12448634952306747 
model_pd.l_d.mean(): -19.676525115966797 
model_pd.lagr.mean(): -19.552038192749023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5989], device='cuda:0')), ('power', tensor([-20.5034], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.12448634952306747
epoch£º473	 i:1 	 global-step:9461	 l-p:0.12691296637058258
epoch£º473	 i:2 	 global-step:9462	 l-p:0.18424135446548462
epoch£º473	 i:3 	 global-step:9463	 l-p:0.11712749302387238
epoch£º473	 i:4 	 global-step:9464	 l-p:0.13386930525302887
epoch£º473	 i:5 	 global-step:9465	 l-p:0.14625973999500275
epoch£º473	 i:6 	 global-step:9466	 l-p:0.17335720360279083
epoch£º473	 i:7 	 global-step:9467	 l-p:0.1479610651731491
epoch£º473	 i:8 	 global-step:9468	 l-p:0.1163327544927597
epoch£º473	 i:9 	 global-step:9469	 l-p:0.1558125615119934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9010, 4.8989, 4.9009],
        [4.9010, 4.9010, 4.9010],
        [4.9010, 4.9010, 4.9010],
        [4.9010, 4.8379, 4.8608]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.12730994820594788 
model_pd.l_d.mean(): -20.271772384643555 
model_pd.lagr.mean(): -20.14446258544922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5272], device='cuda:0')), ('power', tensor([-21.0318], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.12730994820594788
epoch£º474	 i:1 	 global-step:9481	 l-p:0.15008136630058289
epoch£º474	 i:2 	 global-step:9482	 l-p:0.151975616812706
epoch£º474	 i:3 	 global-step:9483	 l-p:0.16485413908958435
epoch£º474	 i:4 	 global-step:9484	 l-p:0.14942502975463867
epoch£º474	 i:5 	 global-step:9485	 l-p:0.12196893244981766
epoch£º474	 i:6 	 global-step:9486	 l-p:0.12879768013954163
epoch£º474	 i:7 	 global-step:9487	 l-p:0.17335109412670135
epoch£º474	 i:8 	 global-step:9488	 l-p:0.11160772293806076
epoch£º474	 i:9 	 global-step:9489	 l-p:0.020075030624866486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9943, 4.9205, 4.9328],
        [4.9943, 4.9068, 4.8921],
        [4.9943, 4.9943, 4.9943],
        [4.9943, 4.9943, 4.9943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.1423606127500534 
model_pd.l_d.mean(): -20.04755210876465 
model_pd.lagr.mean(): -19.90519142150879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5255], device='cuda:0')), ('power', tensor([-20.8035], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.1423606127500534
epoch£º475	 i:1 	 global-step:9501	 l-p:0.15165600180625916
epoch£º475	 i:2 	 global-step:9502	 l-p:0.1371019184589386
epoch£º475	 i:3 	 global-step:9503	 l-p:0.09285791218280792
epoch£º475	 i:4 	 global-step:9504	 l-p:0.10501420497894287
epoch£º475	 i:5 	 global-step:9505	 l-p:0.18451698124408722
epoch£º475	 i:6 	 global-step:9506	 l-p:0.1497810035943985
epoch£º475	 i:7 	 global-step:9507	 l-p:0.19521011412143707
epoch£º475	 i:8 	 global-step:9508	 l-p:0.30325496196746826
epoch£º475	 i:9 	 global-step:9509	 l-p:0.09897228330373764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1499, 5.5374, 5.5328],
        [5.1499, 5.1499, 5.1499],
        [5.1499, 5.4692, 5.4216],
        [5.1499, 5.1011, 5.1231]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.12028974294662476 
model_pd.l_d.mean(): -20.457040786743164 
model_pd.lagr.mean(): -20.33675193786621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4253], device='cuda:0')), ('power', tensor([-21.1151], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.12028974294662476
epoch£º476	 i:1 	 global-step:9521	 l-p:0.1287270188331604
epoch£º476	 i:2 	 global-step:9522	 l-p:0.21336162090301514
epoch£º476	 i:3 	 global-step:9523	 l-p:0.1340639293193817
epoch£º476	 i:4 	 global-step:9524	 l-p:0.14829540252685547
epoch£º476	 i:5 	 global-step:9525	 l-p:0.13768121600151062
epoch£º476	 i:6 	 global-step:9526	 l-p:-0.1572716236114502
epoch£º476	 i:7 	 global-step:9527	 l-p:0.12252666056156158
epoch£º476	 i:8 	 global-step:9528	 l-p:0.06984260678291321
epoch£º476	 i:9 	 global-step:9529	 l-p:0.12459360063076019
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1122, 5.1113, 5.1122],
        [5.1122, 5.2184, 5.0747],
        [5.1122, 5.0688, 5.0918],
        [5.1122, 5.0288, 4.9779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.12929502129554749 
model_pd.l_d.mean(): -19.33446502685547 
model_pd.lagr.mean(): -19.205169677734375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5040], device='cuda:0')), ('power', tensor([-20.0607], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.12929502129554749
epoch£º477	 i:1 	 global-step:9541	 l-p:0.1350240558385849
epoch£º477	 i:2 	 global-step:9542	 l-p:0.11059217154979706
epoch£º477	 i:3 	 global-step:9543	 l-p:0.13546538352966309
epoch£º477	 i:4 	 global-step:9544	 l-p:0.11205041408538818
epoch£º477	 i:5 	 global-step:9545	 l-p:0.035750046372413635
epoch£º477	 i:6 	 global-step:9546	 l-p:-0.04882819205522537
epoch£º477	 i:7 	 global-step:9547	 l-p:0.13141101598739624
epoch£º477	 i:8 	 global-step:9548	 l-p:0.16014452278614044
epoch£º477	 i:9 	 global-step:9549	 l-p:0.16223590075969696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9397, 4.9397, 4.9397],
        [4.9397, 5.3494, 5.3715],
        [4.9397, 4.9397, 4.9397],
        [4.9397, 4.8643, 4.7484]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.0938429981470108 
model_pd.l_d.mean(): -18.75128173828125 
model_pd.lagr.mean(): -18.657438278198242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5956], device='cuda:0')), ('power', tensor([-19.5647], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.0938429981470108
epoch£º478	 i:1 	 global-step:9561	 l-p:0.16578206419944763
epoch£º478	 i:2 	 global-step:9562	 l-p:0.11549991369247437
epoch£º478	 i:3 	 global-step:9563	 l-p:0.18802452087402344
epoch£º478	 i:4 	 global-step:9564	 l-p:0.13454870879650116
epoch£º478	 i:5 	 global-step:9565	 l-p:0.13983705639839172
epoch£º478	 i:6 	 global-step:9566	 l-p:0.1322202980518341
epoch£º478	 i:7 	 global-step:9567	 l-p:0.16048751771450043
epoch£º478	 i:8 	 global-step:9568	 l-p:0.12928180396556854
epoch£º478	 i:9 	 global-step:9569	 l-p:0.15437257289886475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8934, 5.2786, 5.2861],
        [4.8934, 4.8765, 4.8902],
        [4.8934, 4.8113, 4.6957],
        [4.8934, 4.8216, 4.8421]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.12544608116149902 
model_pd.l_d.mean(): -19.89212989807129 
model_pd.lagr.mean(): -19.76668357849121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5562], device='cuda:0')), ('power', tensor([-20.6777], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.12544608116149902
epoch£º479	 i:1 	 global-step:9581	 l-p:0.12326676398515701
epoch£º479	 i:2 	 global-step:9582	 l-p:0.15564192831516266
epoch£º479	 i:3 	 global-step:9583	 l-p:0.1018933579325676
epoch£º479	 i:4 	 global-step:9584	 l-p:0.15664173662662506
epoch£º479	 i:5 	 global-step:9585	 l-p:0.19592256844043732
epoch£º479	 i:6 	 global-step:9586	 l-p:0.174173042178154
epoch£º479	 i:7 	 global-step:9587	 l-p:0.20622390508651733
epoch£º479	 i:8 	 global-step:9588	 l-p:-0.031767476350069046
epoch£º479	 i:9 	 global-step:9589	 l-p:0.23822329938411713
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8587, 4.8587, 4.8587],
        [4.8587, 4.7627, 4.7551],
        [4.8587, 4.8587, 4.8587],
        [4.8587, 4.7952, 4.8204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.1270880252122879 
model_pd.l_d.mean(): -19.346479415893555 
model_pd.lagr.mean(): -19.219390869140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5470], device='cuda:0')), ('power', tensor([-20.1168], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.1270880252122879
epoch£º480	 i:1 	 global-step:9601	 l-p:0.13587099313735962
epoch£º480	 i:2 	 global-step:9602	 l-p:0.17752347886562347
epoch£º480	 i:3 	 global-step:9603	 l-p:0.15373733639717102
epoch£º480	 i:4 	 global-step:9604	 l-p:0.0896303802728653
epoch£º480	 i:5 	 global-step:9605	 l-p:0.1757870763540268
epoch£º480	 i:6 	 global-step:9606	 l-p:0.15413495898246765
epoch£º480	 i:7 	 global-step:9607	 l-p:0.14523078501224518
epoch£º480	 i:8 	 global-step:9608	 l-p:0.10972828418016434
epoch£º480	 i:9 	 global-step:9609	 l-p:0.09925846010446548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0307, 4.9962, 5.0185],
        [5.0307, 5.0307, 5.0307],
        [5.0307, 5.0265, 5.0304],
        [5.0307, 5.0307, 5.0307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.13669101893901825 
model_pd.l_d.mean(): -20.824996948242188 
model_pd.lagr.mean(): -20.68830680847168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4289], device='cuda:0')), ('power', tensor([-21.4907], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.13669101893901825
epoch£º481	 i:1 	 global-step:9621	 l-p:0.13684335350990295
epoch£º481	 i:2 	 global-step:9622	 l-p:0.1414169818162918
epoch£º481	 i:3 	 global-step:9623	 l-p:0.16162189841270447
epoch£º481	 i:4 	 global-step:9624	 l-p:-0.08087573200464249
epoch£º481	 i:5 	 global-step:9625	 l-p:0.1411290317773819
epoch£º481	 i:6 	 global-step:9626	 l-p:0.14082710444927216
epoch£º481	 i:7 	 global-step:9627	 l-p:0.135196253657341
epoch£º481	 i:8 	 global-step:9628	 l-p:0.06902849674224854
epoch£º481	 i:9 	 global-step:9629	 l-p:0.13901621103286743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9659, 4.9432, 4.9604],
        [4.9659, 4.9167, 4.9421],
        [4.9659, 4.8782, 4.7835],
        [4.9659, 5.0338, 4.8773]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.07530045509338379 
model_pd.l_d.mean(): -20.90886116027832 
model_pd.lagr.mean(): -20.833560943603516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4306], device='cuda:0')), ('power', tensor([-21.5773], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.07530045509338379
epoch£º482	 i:1 	 global-step:9641	 l-p:0.11334703117609024
epoch£º482	 i:2 	 global-step:9642	 l-p:0.09824571758508682
epoch£º482	 i:3 	 global-step:9643	 l-p:0.08835303038358688
epoch£º482	 i:4 	 global-step:9644	 l-p:0.1777210235595703
epoch£º482	 i:5 	 global-step:9645	 l-p:0.11729123443365097
epoch£º482	 i:6 	 global-step:9646	 l-p:0.14081968367099762
epoch£º482	 i:7 	 global-step:9647	 l-p:0.12872391939163208
epoch£º482	 i:8 	 global-step:9648	 l-p:0.12212038040161133
epoch£º482	 i:9 	 global-step:9649	 l-p:0.08988659828901291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0783, 5.0783, 5.0783],
        [5.0783, 5.1675, 5.0171],
        [5.0783, 5.0779, 5.0783],
        [5.0783, 5.0783, 5.0783]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): -0.07942353188991547 
model_pd.l_d.mean(): -18.72098159790039 
model_pd.lagr.mean(): -18.800405502319336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5585], device='cuda:0')), ('power', tensor([-19.4962], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:-0.07942353188991547
epoch£º483	 i:1 	 global-step:9661	 l-p:0.21352311968803406
epoch£º483	 i:2 	 global-step:9662	 l-p:0.15004071593284607
epoch£º483	 i:3 	 global-step:9663	 l-p:0.12074794620275497
epoch£º483	 i:4 	 global-step:9664	 l-p:0.13592176139354706
epoch£º483	 i:5 	 global-step:9665	 l-p:0.001408000010997057
epoch£º483	 i:6 	 global-step:9666	 l-p:0.12179845571517944
epoch£º483	 i:7 	 global-step:9667	 l-p:0.1092226505279541
epoch£º483	 i:8 	 global-step:9668	 l-p:0.15028053522109985
epoch£º483	 i:9 	 global-step:9669	 l-p:0.12554505467414856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[5.0870, 5.3139, 5.2152],
        [5.0870, 5.0075, 4.9177],
        [5.0870, 5.0484, 4.9101],
        [5.0870, 5.3393, 5.2537]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.1302861124277115 
model_pd.l_d.mean(): -19.636079788208008 
model_pd.lagr.mean(): -19.505794525146484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5456], device='cuda:0')), ('power', tensor([-20.4081], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.1302861124277115
epoch£º484	 i:1 	 global-step:9681	 l-p:0.1399814635515213
epoch£º484	 i:2 	 global-step:9682	 l-p:0.21971257030963898
epoch£º484	 i:3 	 global-step:9683	 l-p:0.04510189965367317
epoch£º484	 i:4 	 global-step:9684	 l-p:0.07403965294361115
epoch£º484	 i:5 	 global-step:9685	 l-p:-0.35858798027038574
epoch£º484	 i:6 	 global-step:9686	 l-p:0.1331489533185959
epoch£º484	 i:7 	 global-step:9687	 l-p:0.12844322621822357
epoch£º484	 i:8 	 global-step:9688	 l-p:0.14903317391872406
epoch£º484	 i:9 	 global-step:9689	 l-p:0.12464961409568787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0291, 4.9971, 5.0186],
        [5.0291, 4.9510, 4.9612],
        [5.0291, 5.0291, 5.0291],
        [5.0291, 5.0291, 5.0291]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.13019418716430664 
model_pd.l_d.mean(): -21.118709564208984 
model_pd.lagr.mean(): -20.988515853881836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3719], device='cuda:0')), ('power', tensor([-21.7295], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.13019418716430664
epoch£º485	 i:1 	 global-step:9701	 l-p:0.0872800350189209
epoch£º485	 i:2 	 global-step:9702	 l-p:0.11977674812078476
epoch£º485	 i:3 	 global-step:9703	 l-p:0.12787365913391113
epoch£º485	 i:4 	 global-step:9704	 l-p:0.050670087337493896
epoch£º485	 i:5 	 global-step:9705	 l-p:0.12696214020252228
epoch£º485	 i:6 	 global-step:9706	 l-p:0.16334092617034912
epoch£º485	 i:7 	 global-step:9707	 l-p:0.23159493505954742
epoch£º485	 i:8 	 global-step:9708	 l-p:0.1628393828868866
epoch£º485	 i:9 	 global-step:9709	 l-p:0.3431842029094696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8736, 4.8173, 4.6682],
        [4.8736, 4.7697, 4.7378],
        [4.8736, 5.0301, 4.9015],
        [4.8736, 4.8136, 4.8400]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.16251422464847565 
model_pd.l_d.mean(): -20.034866333007812 
model_pd.lagr.mean(): -19.872352600097656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5662], device='cuda:0')), ('power', tensor([-20.8323], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.16251422464847565
epoch£º486	 i:1 	 global-step:9721	 l-p:0.12398673593997955
epoch£º486	 i:2 	 global-step:9722	 l-p:0.14424988627433777
epoch£º486	 i:3 	 global-step:9723	 l-p:0.21382130682468414
epoch£º486	 i:4 	 global-step:9724	 l-p:0.12941154837608337
epoch£º486	 i:5 	 global-step:9725	 l-p:0.12292648106813431
epoch£º486	 i:6 	 global-step:9726	 l-p:0.16779480874538422
epoch£º486	 i:7 	 global-step:9727	 l-p:0.23321574926376343
epoch£º486	 i:8 	 global-step:9728	 l-p:0.09822922199964523
epoch£º486	 i:9 	 global-step:9729	 l-p:0.1335056573152542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9175, 4.8160, 4.7827],
        [4.9175, 4.9173, 4.9175],
        [4.9175, 4.9175, 4.9175],
        [4.9175, 4.9375, 4.7728]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.11897356063127518 
model_pd.l_d.mean(): -20.160181045532227 
model_pd.lagr.mean(): -20.041208267211914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5203], device='cuda:0')), ('power', tensor([-20.9120], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.11897356063127518
epoch£º487	 i:1 	 global-step:9741	 l-p:0.1839427500963211
epoch£º487	 i:2 	 global-step:9742	 l-p:0.12456419318914413
epoch£º487	 i:3 	 global-step:9743	 l-p:0.013144703581929207
epoch£º487	 i:4 	 global-step:9744	 l-p:0.1663299947977066
epoch£º487	 i:5 	 global-step:9745	 l-p:0.11189362406730652
epoch£º487	 i:6 	 global-step:9746	 l-p:0.138456791639328
epoch£º487	 i:7 	 global-step:9747	 l-p:0.14526961743831635
epoch£º487	 i:8 	 global-step:9748	 l-p:0.13182596862316132
epoch£º487	 i:9 	 global-step:9749	 l-p:0.13131795823574066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0370, 5.0358, 5.0369],
        [5.0370, 4.9769, 4.8474],
        [5.0370, 5.0150, 5.0318],
        [5.0370, 5.3924, 5.3701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.13618041574954987 
model_pd.l_d.mean(): -20.3996524810791 
model_pd.lagr.mean(): -20.263471603393555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4767], device='cuda:0')), ('power', tensor([-21.1096], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.13618041574954987
epoch£º488	 i:1 	 global-step:9761	 l-p:0.08433042466640472
epoch£º488	 i:2 	 global-step:9762	 l-p:0.15838299691677094
epoch£º488	 i:3 	 global-step:9763	 l-p:0.1144719123840332
epoch£º488	 i:4 	 global-step:9764	 l-p:0.1525295376777649
epoch£º488	 i:5 	 global-step:9765	 l-p:0.010429763235151768
epoch£º488	 i:6 	 global-step:9766	 l-p:0.09723330289125443
epoch£º488	 i:7 	 global-step:9767	 l-p:0.10094504803419113
epoch£º488	 i:8 	 global-step:9768	 l-p:0.12321688234806061
epoch£º488	 i:9 	 global-step:9769	 l-p:0.16648538410663605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9672, 4.9342, 4.9565],
        [4.9672, 4.9669, 4.9672],
        [4.9672, 4.9312, 4.9545],
        [4.9672, 4.9672, 4.9672]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.11857423186302185 
model_pd.l_d.mean(): -20.743717193603516 
model_pd.lagr.mean(): -20.62514305114746 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4431], device='cuda:0')), ('power', tensor([-21.4230], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.11857423186302185
epoch£º489	 i:1 	 global-step:9781	 l-p:0.12354521453380585
epoch£º489	 i:2 	 global-step:9782	 l-p:0.050815001130104065
epoch£º489	 i:3 	 global-step:9783	 l-p:0.1320810317993164
epoch£º489	 i:4 	 global-step:9784	 l-p:0.1889861673116684
epoch£º489	 i:5 	 global-step:9785	 l-p:0.1898123174905777
epoch£º489	 i:6 	 global-step:9786	 l-p:0.1431262344121933
epoch£º489	 i:7 	 global-step:9787	 l-p:0.14420448243618011
epoch£º489	 i:8 	 global-step:9788	 l-p:0.13946104049682617
epoch£º489	 i:9 	 global-step:9789	 l-p:0.13979339599609375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9706, 4.9706, 4.9706],
        [4.9706, 4.9701, 4.9706],
        [4.9706, 4.9699, 4.9706],
        [4.9706, 4.9706, 4.9706]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.14234191179275513 
model_pd.l_d.mean(): -20.852577209472656 
model_pd.lagr.mean(): -20.710235595703125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4357], device='cuda:0')), ('power', tensor([-21.5255], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.14234191179275513
epoch£º490	 i:1 	 global-step:9801	 l-p:0.11683274805545807
epoch£º490	 i:2 	 global-step:9802	 l-p:0.1866832673549652
epoch£º490	 i:3 	 global-step:9803	 l-p:0.12108809500932693
epoch£º490	 i:4 	 global-step:9804	 l-p:0.12444473803043365
epoch£º490	 i:5 	 global-step:9805	 l-p:0.12764310836791992
epoch£º490	 i:6 	 global-step:9806	 l-p:0.10788404196500778
epoch£º490	 i:7 	 global-step:9807	 l-p:0.12677405774593353
epoch£º490	 i:8 	 global-step:9808	 l-p:-0.01711178757250309
epoch£º490	 i:9 	 global-step:9809	 l-p:0.13440021872520447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0230, 5.4302, 5.4431],
        [5.0230, 4.9278, 4.9047],
        [5.0230, 4.9848, 5.0086],
        [5.0230, 5.1938, 5.0682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.0018765878630802035 
model_pd.l_d.mean(): -19.431766510009766 
model_pd.lagr.mean(): -19.429889678955078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5048], device='cuda:0')), ('power', tensor([-20.1598], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.0018765878630802035
epoch£º491	 i:1 	 global-step:9821	 l-p:0.12986989319324493
epoch£º491	 i:2 	 global-step:9822	 l-p:0.1410278081893921
epoch£º491	 i:3 	 global-step:9823	 l-p:0.14422371983528137
epoch£º491	 i:4 	 global-step:9824	 l-p:0.10885640978813171
epoch£º491	 i:5 	 global-step:9825	 l-p:0.18472932279109955
epoch£º491	 i:6 	 global-step:9826	 l-p:0.14450114965438843
epoch£º491	 i:7 	 global-step:9827	 l-p:0.08184558898210526
epoch£º491	 i:8 	 global-step:9828	 l-p:0.12795212864875793
epoch£º491	 i:9 	 global-step:9829	 l-p:0.13095667958259583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0363, 5.0363, 5.0363],
        [5.0363, 4.9654, 4.8427],
        [5.0363, 5.0287, 5.0355],
        [5.0363, 5.0307, 5.0358]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.13932813704013824 
model_pd.l_d.mean(): -20.706802368164062 
model_pd.lagr.mean(): -20.567474365234375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4498], device='cuda:0')), ('power', tensor([-21.3926], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.13932813704013824
epoch£º492	 i:1 	 global-step:9841	 l-p:0.1427973210811615
epoch£º492	 i:2 	 global-step:9842	 l-p:0.11241184175014496
epoch£º492	 i:3 	 global-step:9843	 l-p:0.030914196744561195
epoch£º492	 i:4 	 global-step:9844	 l-p:0.156962588429451
epoch£º492	 i:5 	 global-step:9845	 l-p:0.11440202593803406
epoch£º492	 i:6 	 global-step:9846	 l-p:0.21094554662704468
epoch£º492	 i:7 	 global-step:9847	 l-p:0.16150496900081635
epoch£º492	 i:8 	 global-step:9848	 l-p:0.1585637778043747
epoch£º492	 i:9 	 global-step:9849	 l-p:0.11888515949249268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9041, 5.5582, 5.7688],
        [4.9041, 4.9038, 4.9041],
        [4.9041, 4.8046, 4.7963],
        [4.9041, 5.3968, 5.4792]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.1789609044790268 
model_pd.l_d.mean(): -17.567562103271484 
model_pd.lagr.mean(): -17.388601303100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6881], device='cuda:0')), ('power', tensor([-18.4626], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.1789609044790268
epoch£º493	 i:1 	 global-step:9861	 l-p:0.12594909965991974
epoch£º493	 i:2 	 global-step:9862	 l-p:0.15115167200565338
epoch£º493	 i:3 	 global-step:9863	 l-p:0.16898097097873688
epoch£º493	 i:4 	 global-step:9864	 l-p:0.1616533249616623
epoch£º493	 i:5 	 global-step:9865	 l-p:0.1646926999092102
epoch£º493	 i:6 	 global-step:9866	 l-p:0.12607870995998383
epoch£º493	 i:7 	 global-step:9867	 l-p:0.14969190955162048
epoch£º493	 i:8 	 global-step:9868	 l-p:0.121619313955307
epoch£º493	 i:9 	 global-step:9869	 l-p:0.1162402331829071
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9135, 4.8261, 4.7025],
        [4.9135, 5.5034, 5.6606],
        [4.9135, 4.8276, 4.8411],
        [4.9135, 4.8953, 4.9100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.13132642209529877 
model_pd.l_d.mean(): -18.214134216308594 
model_pd.lagr.mean(): -18.082807540893555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6521], device='cuda:0')), ('power', tensor([-19.0794], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.13132642209529877
epoch£º494	 i:1 	 global-step:9881	 l-p:0.20003654062747955
epoch£º494	 i:2 	 global-step:9882	 l-p:0.06513354927301407
epoch£º494	 i:3 	 global-step:9883	 l-p:0.14746661484241486
epoch£º494	 i:4 	 global-step:9884	 l-p:0.16592808067798615
epoch£º494	 i:5 	 global-step:9885	 l-p:0.08697929978370667
epoch£º494	 i:6 	 global-step:9886	 l-p:0.09508047997951508
epoch£º494	 i:7 	 global-step:9887	 l-p:0.14949913322925568
epoch£º494	 i:8 	 global-step:9888	 l-p:0.10555389523506165
epoch£º494	 i:9 	 global-step:9889	 l-p:0.15500254929065704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0500, 4.9595, 4.9548],
        [5.0500, 5.0494, 5.0500],
        [5.0500, 5.3609, 5.3079],
        [5.0500, 5.0387, 5.0484]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.13610759377479553 
model_pd.l_d.mean(): -20.455570220947266 
model_pd.lagr.mean(): -20.319461822509766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4538], device='cuda:0')), ('power', tensor([-21.1427], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.13610759377479553
epoch£º495	 i:1 	 global-step:9901	 l-p:0.13547061383724213
epoch£º495	 i:2 	 global-step:9902	 l-p:0.14549539983272552
epoch£º495	 i:3 	 global-step:9903	 l-p:0.13078834116458893
epoch£º495	 i:4 	 global-step:9904	 l-p:-0.12325658649206161
epoch£º495	 i:5 	 global-step:9905	 l-p:0.14799706637859344
epoch£º495	 i:6 	 global-step:9906	 l-p:0.08871699869632721
epoch£º495	 i:7 	 global-step:9907	 l-p:0.12749281525611877
epoch£º495	 i:8 	 global-step:9908	 l-p:0.1484345942735672
epoch£º495	 i:9 	 global-step:9909	 l-p:0.13138315081596375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9653, 4.9653, 4.9653],
        [4.9653, 4.9031, 4.9292],
        [4.9653, 4.9653, 4.9653],
        [4.9653, 5.2520, 5.1877]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.1456320732831955 
model_pd.l_d.mean(): -20.617752075195312 
model_pd.lagr.mean(): -20.47212028503418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4668], device='cuda:0')), ('power', tensor([-21.3199], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.1456320732831955
epoch£º496	 i:1 	 global-step:9921	 l-p:0.1316298395395279
epoch£º496	 i:2 	 global-step:9922	 l-p:0.11047108471393585
epoch£º496	 i:3 	 global-step:9923	 l-p:0.11537569016218185
epoch£º496	 i:4 	 global-step:9924	 l-p:0.1519562155008316
epoch£º496	 i:5 	 global-step:9925	 l-p:0.13920916616916656
epoch£º496	 i:6 	 global-step:9926	 l-p:0.13673381507396698
epoch£º496	 i:7 	 global-step:9927	 l-p:0.13130414485931396
epoch£º496	 i:8 	 global-step:9928	 l-p:0.31019094586372375
epoch£º496	 i:9 	 global-step:9929	 l-p:0.14243975281715393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8974, 4.7966, 4.6894],
        [4.8974, 4.8974, 4.8974],
        [4.8974, 4.8974, 4.8974],
        [4.8974, 4.8958, 4.8974]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.16862784326076508 
model_pd.l_d.mean(): -20.347332000732422 
model_pd.lagr.mean(): -20.17870330810547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5295], device='cuda:0')), ('power', tensor([-21.1106], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.16862784326076508
epoch£º497	 i:1 	 global-step:9941	 l-p:0.16088666021823883
epoch£º497	 i:2 	 global-step:9942	 l-p:0.1287141889333725
epoch£º497	 i:3 	 global-step:9943	 l-p:0.12092750519514084
epoch£º497	 i:4 	 global-step:9944	 l-p:0.13276247680187225
epoch£º497	 i:5 	 global-step:9945	 l-p:0.18529735505580902
epoch£º497	 i:6 	 global-step:9946	 l-p:0.15222811698913574
epoch£º497	 i:7 	 global-step:9947	 l-p:0.14859123528003693
epoch£º497	 i:8 	 global-step:9948	 l-p:0.1094975620508194
epoch£º497	 i:9 	 global-step:9949	 l-p:0.30638420581817627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8914, 4.8443, 4.8716],
        [4.8914, 5.2502, 5.2358],
        [4.8914, 5.0125, 4.8650],
        [4.8914, 4.8837, 4.8906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.17407427728176117 
model_pd.l_d.mean(): -20.40473747253418 
model_pd.lagr.mean(): -20.230663299560547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5168], device='cuda:0')), ('power', tensor([-21.1557], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.17407427728176117
epoch£º498	 i:1 	 global-step:9961	 l-p:0.15605074167251587
epoch£º498	 i:2 	 global-step:9962	 l-p:0.12421401590108871
epoch£º498	 i:3 	 global-step:9963	 l-p:0.16311530768871307
epoch£º498	 i:4 	 global-step:9964	 l-p:0.16914485394954681
epoch£º498	 i:5 	 global-step:9965	 l-p:0.13309405744075775
epoch£º498	 i:6 	 global-step:9966	 l-p:0.16288411617279053
epoch£º498	 i:7 	 global-step:9967	 l-p:0.10889234393835068
epoch£º498	 i:8 	 global-step:9968	 l-p:0.09160599857568741
epoch£º498	 i:9 	 global-step:9969	 l-p:0.11324776709079742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9973, 4.9966, 4.9973],
        [4.9973, 5.3564, 5.3360],
        [4.9973, 4.9973, 4.9973],
        [4.9973, 4.9973, 4.9973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.1652202606201172 
model_pd.l_d.mean(): -21.005830764770508 
model_pd.lagr.mean(): -20.84061050415039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4019], device='cuda:0')), ('power', tensor([-21.6459], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.1652202606201172
epoch£º499	 i:1 	 global-step:9981	 l-p:0.1101987436413765
epoch£º499	 i:2 	 global-step:9982	 l-p:0.1194123700261116
epoch£º499	 i:3 	 global-step:9983	 l-p:0.1488138884305954
epoch£º499	 i:4 	 global-step:9984	 l-p:0.05298418924212456
epoch£º499	 i:5 	 global-step:9985	 l-p:0.3428713083267212
epoch£º499	 i:6 	 global-step:9986	 l-p:0.1257554441690445
epoch£º499	 i:7 	 global-step:9987	 l-p:0.10659331828355789
epoch£º499	 i:8 	 global-step:9988	 l-p:0.1382519155740738
epoch£º499	 i:9 	 global-step:9989	 l-p:0.12901203334331512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0455, 4.9681, 4.9854],
        [5.0455, 4.9565, 4.8512],
        [5.0455, 5.0005, 5.0264],
        [5.0455, 5.1383, 4.9818]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): 0.12320774793624878 
model_pd.l_d.mean(): -20.827007293701172 
model_pd.lagr.mean(): -20.703800201416016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4276], device='cuda:0')), ('power', tensor([-21.4915], device='cuda:0'))])
epoch£º500	 i:0 	 global-step:10000	 l-p:0.12320774793624878
epoch£º500	 i:1 	 global-step:10001	 l-p:0.14030933380126953
epoch£º500	 i:2 	 global-step:10002	 l-p:-0.04577041417360306
epoch£º500	 i:3 	 global-step:10003	 l-p:0.15149249136447906
epoch£º500	 i:4 	 global-step:10004	 l-p:0.12317514419555664
epoch£º500	 i:5 	 global-step:10005	 l-p:0.1347656548023224
epoch£º500	 i:6 	 global-step:10006	 l-p:0.12492971122264862
epoch£º500	 i:7 	 global-step:10007	 l-p:0.18530048429965973
epoch£º500	 i:8 	 global-step:10008	 l-p:0.07322360575199127
epoch£º500	 i:9 	 global-step:10009	 l-p:0.14861787855625153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9500, 4.8548, 4.8580],
        [4.9500, 4.8537, 4.7412],
        [4.9500, 4.8674, 4.7338],
        [4.9500, 4.9228, 4.9429]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): 0.08269807696342468 
model_pd.l_d.mean(): -20.00534439086914 
model_pd.lagr.mean(): -19.922645568847656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5292], device='cuda:0')), ('power', tensor([-20.7646], device='cuda:0'))])
epoch£º501	 i:0 	 global-step:10020	 l-p:0.08269807696342468
epoch£º501	 i:1 	 global-step:10021	 l-p:0.2072976678609848
epoch£º501	 i:2 	 global-step:10022	 l-p:0.1751270592212677
epoch£º501	 i:3 	 global-step:10023	 l-p:0.11358431726694107
epoch£º501	 i:4 	 global-step:10024	 l-p:0.14100594818592072
epoch£º501	 i:5 	 global-step:10025	 l-p:-0.07758825272321701
epoch£º501	 i:6 	 global-step:10026	 l-p:0.11720477044582367
epoch£º501	 i:7 	 global-step:10027	 l-p:0.13885115087032318
epoch£º501	 i:8 	 global-step:10028	 l-p:0.09445169568061829
epoch£º501	 i:9 	 global-step:10029	 l-p:0.1342112272977829
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0189, 4.9150, 4.8465],
        [5.0189, 5.0189, 5.0189],
        [5.0189, 5.3706, 5.3434],
        [5.0189, 5.0187, 5.0189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.1440402865409851 
model_pd.l_d.mean(): -19.59906005859375 
model_pd.lagr.mean(): -19.455018997192383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5225], device='cuda:0')), ('power', tensor([-20.3470], device='cuda:0'))])
epoch£º502	 i:0 	 global-step:10040	 l-p:0.1440402865409851
epoch£º502	 i:1 	 global-step:10041	 l-p:0.003370952559635043
epoch£º502	 i:2 	 global-step:10042	 l-p:0.16964980959892273
epoch£º502	 i:3 	 global-step:10043	 l-p:0.1477016657590866
epoch£º502	 i:4 	 global-step:10044	 l-p:0.11939185857772827
epoch£º502	 i:5 	 global-step:10045	 l-p:0.14886753261089325
epoch£º502	 i:6 	 global-step:10046	 l-p:-0.027332978323101997
epoch£º502	 i:7 	 global-step:10047	 l-p:0.1316375732421875
epoch£º502	 i:8 	 global-step:10048	 l-p:0.1342095136642456
epoch£º502	 i:9 	 global-step:10049	 l-p:0.1198839321732521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9620, 4.9619, 4.9620],
        [4.9620, 5.2772, 5.2289],
        [4.9620, 4.9620, 4.9620],
        [4.9620, 4.9381, 4.9564]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.15649114549160004 
model_pd.l_d.mean(): -19.81466293334961 
model_pd.lagr.mean(): -19.658172607421875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5099], device='cuda:0')), ('power', tensor([-20.5521], device='cuda:0'))])
epoch£º503	 i:0 	 global-step:10060	 l-p:0.15649114549160004
epoch£º503	 i:1 	 global-step:10061	 l-p:0.11487012356519699
epoch£º503	 i:2 	 global-step:10062	 l-p:0.11941487342119217
epoch£º503	 i:3 	 global-step:10063	 l-p:0.13659484684467316
epoch£º503	 i:4 	 global-step:10064	 l-p:0.09817668795585632
epoch£º503	 i:5 	 global-step:10065	 l-p:0.20229509472846985
epoch£º503	 i:6 	 global-step:10066	 l-p:0.20448286831378937
epoch£º503	 i:7 	 global-step:10067	 l-p:0.349691778421402
epoch£º503	 i:8 	 global-step:10068	 l-p:0.1317884922027588
epoch£º503	 i:9 	 global-step:10069	 l-p:0.0960463136434555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8956, 4.8841, 4.8940],
        [4.8956, 4.7903, 4.6773],
        [4.8956, 4.8648, 4.8869],
        [4.8956, 4.7974, 4.8037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): 0.10147019475698471 
model_pd.l_d.mean(): -20.451139450073242 
model_pd.lagr.mean(): -20.349668502807617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5217], device='cuda:0')), ('power', tensor([-21.2076], device='cuda:0'))])
epoch£º504	 i:0 	 global-step:10080	 l-p:0.10147019475698471
epoch£º504	 i:1 	 global-step:10081	 l-p:0.11576902121305466
epoch£º504	 i:2 	 global-step:10082	 l-p:0.19262605905532837
epoch£º504	 i:3 	 global-step:10083	 l-p:0.3608698546886444
epoch£º504	 i:4 	 global-step:10084	 l-p:0.16487348079681396
epoch£º504	 i:5 	 global-step:10085	 l-p:0.13093532621860504
epoch£º504	 i:6 	 global-step:10086	 l-p:0.13267682492733002
epoch£º504	 i:7 	 global-step:10087	 l-p:0.15464577078819275
epoch£º504	 i:8 	 global-step:10088	 l-p:0.16362722218036652
epoch£º504	 i:9 	 global-step:10089	 l-p:0.1589832454919815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9515, 4.8454, 4.8301],
        [4.9515, 4.9012, 4.9292],
        [4.9515, 4.9469, 4.9512],
        [4.9515, 4.8836, 4.9107]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.19205443561077118 
model_pd.l_d.mean(): -20.643260955810547 
model_pd.lagr.mean(): -20.45120620727539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4736], device='cuda:0')), ('power', tensor([-21.3527], device='cuda:0'))])
epoch£º505	 i:0 	 global-step:10100	 l-p:0.19205443561077118
epoch£º505	 i:1 	 global-step:10101	 l-p:0.15821188688278198
epoch£º505	 i:2 	 global-step:10102	 l-p:0.10743841528892517
epoch£º505	 i:3 	 global-step:10103	 l-p:0.14925464987754822
epoch£º505	 i:4 	 global-step:10104	 l-p:0.0825299471616745
epoch£º505	 i:5 	 global-step:10105	 l-p:0.1326640099287033
epoch£º505	 i:6 	 global-step:10106	 l-p:-0.13481946289539337
epoch£º505	 i:7 	 global-step:10107	 l-p:0.14204704761505127
epoch£º505	 i:8 	 global-step:10108	 l-p:0.11198541522026062
epoch£º505	 i:9 	 global-step:10109	 l-p:0.07942713797092438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1250, 5.1142, 5.1236],
        [5.1250, 5.8133, 6.0322],
        [5.1250, 5.6656, 5.7664],
        [5.1250, 5.0560, 4.9250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.07431492209434509 
model_pd.l_d.mean(): -19.712949752807617 
model_pd.lagr.mean(): -19.638635635375977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5085], device='cuda:0')), ('power', tensor([-20.4478], device='cuda:0'))])
epoch£º506	 i:0 	 global-step:10120	 l-p:0.07431492209434509
epoch£º506	 i:1 	 global-step:10121	 l-p:0.06516259163618088
epoch£º506	 i:2 	 global-step:10122	 l-p:0.17632535099983215
epoch£º506	 i:3 	 global-step:10123	 l-p:0.10977568477392197
epoch£º506	 i:4 	 global-step:10124	 l-p:0.22373764216899872
epoch£º506	 i:5 	 global-step:10125	 l-p:0.1123414933681488
epoch£º506	 i:6 	 global-step:10126	 l-p:0.14201731979846954
epoch£º506	 i:7 	 global-step:10127	 l-p:0.10379724204540253
epoch£º506	 i:8 	 global-step:10128	 l-p:0.11619724333286285
epoch£º506	 i:9 	 global-step:10129	 l-p:0.13010555505752563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2615, 5.2198, 5.2439],
        [5.2615, 5.1707, 5.1240],
        [5.2615, 5.1783, 5.0933],
        [5.2615, 5.9306, 6.1236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.13626298308372498 
model_pd.l_d.mean(): -20.342134475708008 
model_pd.lagr.mean(): -20.20587158203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4203], device='cuda:0')), ('power', tensor([-20.9938], device='cuda:0'))])
epoch£º507	 i:0 	 global-step:10140	 l-p:0.13626298308372498
epoch£º507	 i:1 	 global-step:10141	 l-p:0.12352944910526276
epoch£º507	 i:2 	 global-step:10142	 l-p:0.11967965960502625
epoch£º507	 i:3 	 global-step:10143	 l-p:0.1715317815542221
epoch£º507	 i:4 	 global-step:10144	 l-p:0.12195556610822678
epoch£º507	 i:5 	 global-step:10145	 l-p:0.06604044139385223
epoch£º507	 i:6 	 global-step:10146	 l-p:0.05806899070739746
epoch£º507	 i:7 	 global-step:10147	 l-p:0.17843572795391083
epoch£º507	 i:8 	 global-step:10148	 l-p:0.014044911600649357
epoch£º507	 i:9 	 global-step:10149	 l-p:0.12127894908189774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1747, 5.1519, 5.1693],
        [5.1747, 5.1726, 5.1746],
        [5.1747, 5.1742, 5.1747],
        [5.1747, 5.1916, 5.0261]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.09817511588335037 
model_pd.l_d.mean(): -19.46470069885254 
model_pd.lagr.mean(): -19.366525650024414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4946], device='cuda:0')), ('power', tensor([-20.1827], device='cuda:0'))])
epoch£º508	 i:0 	 global-step:10160	 l-p:0.09817511588335037
epoch£º508	 i:1 	 global-step:10161	 l-p:0.132919579744339
epoch£º508	 i:2 	 global-step:10162	 l-p:0.2073431760072708
epoch£º508	 i:3 	 global-step:10163	 l-p:-0.011840958148241043
epoch£º508	 i:4 	 global-step:10164	 l-p:0.01675025373697281
epoch£º508	 i:5 	 global-step:10165	 l-p:0.14605796337127686
epoch£º508	 i:6 	 global-step:10166	 l-p:-0.03783643990755081
epoch£º508	 i:7 	 global-step:10167	 l-p:0.1099633276462555
epoch£º508	 i:8 	 global-step:10168	 l-p:0.14626669883728027
epoch£º508	 i:9 	 global-step:10169	 l-p:0.13764241337776184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2074, 5.4580, 5.3624],
        [5.2074, 5.2069, 5.2074],
        [5.2074, 5.9448, 6.1984],
        [5.2074, 5.1694, 5.1932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): 0.339880108833313 
model_pd.l_d.mean(): -19.881559371948242 
model_pd.lagr.mean(): -19.54167938232422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4508], device='cuda:0')), ('power', tensor([-20.5593], device='cuda:0'))])
epoch£º509	 i:0 	 global-step:10180	 l-p:0.339880108833313
epoch£º509	 i:1 	 global-step:10181	 l-p:0.3782255947589874
epoch£º509	 i:2 	 global-step:10182	 l-p:0.11868198215961456
epoch£º509	 i:3 	 global-step:10183	 l-p:0.13824015855789185
epoch£º509	 i:4 	 global-step:10184	 l-p:0.09476945549249649
epoch£º509	 i:5 	 global-step:10185	 l-p:0.12349876016378403
epoch£º509	 i:6 	 global-step:10186	 l-p:0.11477628350257874
epoch£º509	 i:7 	 global-step:10187	 l-p:-0.059453971683979034
epoch£º509	 i:8 	 global-step:10188	 l-p:0.10828390717506409
epoch£º509	 i:9 	 global-step:10189	 l-p:0.14085721969604492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1451, 5.1437, 5.1450],
        [5.1451, 5.2824, 5.1370],
        [5.1451, 5.3194, 5.1883],
        [5.1451, 5.1631, 4.9954]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.11660369485616684 
model_pd.l_d.mean(): -18.698471069335938 
model_pd.lagr.mean(): -18.581867218017578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4701], device='cuda:0')), ('power', tensor([-19.3831], device='cuda:0'))])
epoch£º510	 i:0 	 global-step:10200	 l-p:0.11660369485616684
epoch£º510	 i:1 	 global-step:10201	 l-p:0.10169629752635956
epoch£º510	 i:2 	 global-step:10202	 l-p:0.13050679862499237
epoch£º510	 i:3 	 global-step:10203	 l-p:-0.023268233984708786
epoch£º510	 i:4 	 global-step:10204	 l-p:0.12673768401145935
epoch£º510	 i:5 	 global-step:10205	 l-p:0.1350507289171219
epoch£º510	 i:6 	 global-step:10206	 l-p:0.14503084123134613
epoch£º510	 i:7 	 global-step:10207	 l-p:0.1286136955022812
epoch£º510	 i:8 	 global-step:10208	 l-p:0.15564164519309998
epoch£º510	 i:9 	 global-step:10209	 l-p:0.12343315035104752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9594, 4.8872, 4.9137],
        [4.9594, 5.0144, 4.8430],
        [4.9594, 4.9255, 4.9490],
        [4.9594, 4.9594, 4.9594]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): 0.16342340409755707 
model_pd.l_d.mean(): -20.090206146240234 
model_pd.lagr.mean(): -19.926782608032227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5410], device='cuda:0')), ('power', tensor([-20.8625], device='cuda:0'))])
epoch£º511	 i:0 	 global-step:10220	 l-p:0.16342340409755707
epoch£º511	 i:1 	 global-step:10221	 l-p:0.13329650461673737
epoch£º511	 i:2 	 global-step:10222	 l-p:0.12466456741094589
epoch£º511	 i:3 	 global-step:10223	 l-p:0.16144689917564392
epoch£º511	 i:4 	 global-step:10224	 l-p:0.13325278460979462
epoch£º511	 i:5 	 global-step:10225	 l-p:0.1714143007993698
epoch£º511	 i:6 	 global-step:10226	 l-p:0.12703298032283783
epoch£º511	 i:7 	 global-step:10227	 l-p:0.15769757330417633
epoch£º511	 i:8 	 global-step:10228	 l-p:0.06364797800779343
epoch£º511	 i:9 	 global-step:10229	 l-p:0.14772793650627136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9456, 4.9339, 4.9440],
        [4.9456, 4.9038, 4.9303],
        [4.9456, 4.9796, 4.8039],
        [4.9456, 4.8460, 4.8511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.1635449379682541 
model_pd.l_d.mean(): -20.654590606689453 
model_pd.lagr.mean(): -20.491044998168945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4618], device='cuda:0')), ('power', tensor([-21.3521], device='cuda:0'))])
epoch£º512	 i:0 	 global-step:10240	 l-p:0.1635449379682541
epoch£º512	 i:1 	 global-step:10241	 l-p:0.10190920531749725
epoch£º512	 i:2 	 global-step:10242	 l-p:0.20632727444171906
epoch£º512	 i:3 	 global-step:10243	 l-p:0.1410125195980072
epoch£º512	 i:4 	 global-step:10244	 l-p:0.11429137736558914
epoch£º512	 i:5 	 global-step:10245	 l-p:0.11402522772550583
epoch£º512	 i:6 	 global-step:10246	 l-p:0.13070164620876312
epoch£º512	 i:7 	 global-step:10247	 l-p:0.14580006897449493
epoch£º512	 i:8 	 global-step:10248	 l-p:0.162263885140419
epoch£º512	 i:9 	 global-step:10249	 l-p:0.2365613877773285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9214, 4.8045, 4.7699],
        [4.9214, 5.2349, 5.1852],
        [4.9214, 4.8101, 4.7960],
        [4.9214, 4.9192, 4.9213]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.15235529839992523 
model_pd.l_d.mean(): -20.48650550842285 
model_pd.lagr.mean(): -20.334150314331055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5073], device='cuda:0')), ('power', tensor([-21.2286], device='cuda:0'))])
epoch£º513	 i:0 	 global-step:10260	 l-p:0.15235529839992523
epoch£º513	 i:1 	 global-step:10261	 l-p:0.16526702046394348
epoch£º513	 i:2 	 global-step:10262	 l-p:0.13805706799030304
epoch£º513	 i:3 	 global-step:10263	 l-p:0.13938453793525696
epoch£º513	 i:4 	 global-step:10264	 l-p:0.17779035866260529
epoch£º513	 i:5 	 global-step:10265	 l-p:0.12671799957752228
epoch£º513	 i:6 	 global-step:10266	 l-p:0.1244644895195961
epoch£º513	 i:7 	 global-step:10267	 l-p:0.12719346582889557
epoch£º513	 i:8 	 global-step:10268	 l-p:0.048359692096710205
epoch£º513	 i:9 	 global-step:10269	 l-p:0.08926398307085037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0580, 5.6093, 5.7211],
        [5.0580, 5.3889, 5.3437],
        [5.0580, 4.9594, 4.9565],
        [5.0580, 5.0578, 5.0580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.14297056198120117 
model_pd.l_d.mean(): -20.446455001831055 
model_pd.lagr.mean(): -20.303483963012695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4542], device='cuda:0')), ('power', tensor([-21.1339], device='cuda:0'))])
epoch£º514	 i:0 	 global-step:10280	 l-p:0.14297056198120117
epoch£º514	 i:1 	 global-step:10281	 l-p:0.14509190618991852
epoch£º514	 i:2 	 global-step:10282	 l-p:0.15907706320285797
epoch£º514	 i:3 	 global-step:10283	 l-p:0.026427630335092545
epoch£º514	 i:4 	 global-step:10284	 l-p:0.0663069412112236
epoch£º514	 i:5 	 global-step:10285	 l-p:0.1301259845495224
epoch£º514	 i:6 	 global-step:10286	 l-p:0.11482036113739014
epoch£º514	 i:7 	 global-step:10287	 l-p:0.13956640660762787
epoch£º514	 i:8 	 global-step:10288	 l-p:0.12079405784606934
epoch£º514	 i:9 	 global-step:10289	 l-p:0.13352499902248383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0383, 5.3309, 5.2625],
        [5.0383, 5.2813, 5.1842],
        [5.0383, 4.9869, 5.0150],
        [5.0383, 5.0109, 4.8403]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): 0.008884022943675518 
model_pd.l_d.mean(): -19.05951499938965 
model_pd.lagr.mean(): -19.050630569458008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5593], device='cuda:0')), ('power', tensor([-19.8392], device='cuda:0'))])
epoch£º515	 i:0 	 global-step:10300	 l-p:0.008884022943675518
epoch£º515	 i:1 	 global-step:10301	 l-p:0.10355667024850845
epoch£º515	 i:2 	 global-step:10302	 l-p:0.11589060723781586
epoch£º515	 i:3 	 global-step:10303	 l-p:0.15706320106983185
epoch£º515	 i:4 	 global-step:10304	 l-p:0.13304589688777924
epoch£º515	 i:5 	 global-step:10305	 l-p:0.1338018923997879
epoch£º515	 i:6 	 global-step:10306	 l-p:0.09542475640773773
epoch£º515	 i:7 	 global-step:10307	 l-p:0.24358466267585754
epoch£º515	 i:8 	 global-step:10308	 l-p:0.24185283482074738
epoch£º515	 i:9 	 global-step:10309	 l-p:0.14859159290790558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9072, 4.8977, 4.9061],
        [4.9072, 4.7954, 4.7859],
        [4.9072, 5.1596, 5.0722],
        [4.9072, 4.8897, 4.9041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 0.3097810745239258 
model_pd.l_d.mean(): -20.007858276367188 
model_pd.lagr.mean(): -19.698078155517578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5623], device='cuda:0')), ('power', tensor([-20.8010], device='cuda:0'))])
epoch£º516	 i:0 	 global-step:10320	 l-p:0.3097810745239258
epoch£º516	 i:1 	 global-step:10321	 l-p:0.19929736852645874
epoch£º516	 i:2 	 global-step:10322	 l-p:0.14018458127975464
epoch£º516	 i:3 	 global-step:10323	 l-p:0.13052082061767578
epoch£º516	 i:4 	 global-step:10324	 l-p:0.09787452220916748
epoch£º516	 i:5 	 global-step:10325	 l-p:0.18083053827285767
epoch£º516	 i:6 	 global-step:10326	 l-p:0.11050355434417725
epoch£º516	 i:7 	 global-step:10327	 l-p:0.15608935058116913
epoch£º516	 i:8 	 global-step:10328	 l-p:0.13277091085910797
epoch£º516	 i:9 	 global-step:10329	 l-p:0.14191991090774536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9443, 4.9369, 4.9436],
        [4.9443, 4.8369, 4.7172],
        [4.9443, 4.8296, 4.7295],
        [4.9443, 4.9442, 4.9443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.10840561985969543 
model_pd.l_d.mean(): -20.055665969848633 
model_pd.lagr.mean(): -19.9472599029541 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5050], device='cuda:0')), ('power', tensor([-20.7908], device='cuda:0'))])
epoch£º517	 i:0 	 global-step:10340	 l-p:0.10840561985969543
epoch£º517	 i:1 	 global-step:10341	 l-p:0.13166436553001404
epoch£º517	 i:2 	 global-step:10342	 l-p:0.18361011147499084
epoch£º517	 i:3 	 global-step:10343	 l-p:0.15376392006874084
epoch£º517	 i:4 	 global-step:10344	 l-p:0.12770090997219086
epoch£º517	 i:5 	 global-step:10345	 l-p:0.13041017949581146
epoch£º517	 i:6 	 global-step:10346	 l-p:0.10343725234270096
epoch£º517	 i:7 	 global-step:10347	 l-p:-0.15484195947647095
epoch£º517	 i:8 	 global-step:10348	 l-p:0.08340292423963547
epoch£º517	 i:9 	 global-step:10349	 l-p:0.1393037736415863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6054,  0.5121,  1.0000,  0.4332,
          1.0000,  0.8459, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4016,  0.2963,  1.0000,  0.2186,
          1.0000,  0.7378, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1448,  0.0760,  1.0000,  0.0399,
          1.0000,  0.5251, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228]], device='cuda:0')
 pt:tensor([[5.1497, 5.3163, 5.1793],
        [5.1497, 5.1035, 4.9468],
        [5.1497, 5.0740, 5.0949],
        [5.1497, 5.0450, 5.0010]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.11556629836559296 
model_pd.l_d.mean(): -20.299575805664062 
model_pd.lagr.mean(): -20.184009552001953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4619], device='cuda:0')), ('power', tensor([-20.9933], device='cuda:0'))])
epoch£º518	 i:0 	 global-step:10360	 l-p:0.11556629836559296
epoch£º518	 i:1 	 global-step:10361	 l-p:0.043005432933568954
epoch£º518	 i:2 	 global-step:10362	 l-p:0.02936657890677452
epoch£º518	 i:3 	 global-step:10363	 l-p:0.12192506343126297
epoch£º518	 i:4 	 global-step:10364	 l-p:0.19151370227336884
epoch£º518	 i:5 	 global-step:10365	 l-p:0.13559196889400482
epoch£º518	 i:6 	 global-step:10366	 l-p:0.09128738939762115
epoch£º518	 i:7 	 global-step:10367	 l-p:0.1250021755695343
epoch£º518	 i:8 	 global-step:10368	 l-p:0.12035989761352539
epoch£º518	 i:9 	 global-step:10369	 l-p:0.22989897429943085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2350, 5.2323, 5.2349],
        [5.2350, 5.9175, 6.1208],
        [5.2350, 5.2321, 5.2348],
        [5.2350, 5.1726, 5.1975]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.12645213305950165 
model_pd.l_d.mean(): -20.412424087524414 
model_pd.lagr.mean(): -20.285972595214844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4057], device='cuda:0')), ('power', tensor([-21.0499], device='cuda:0'))])
epoch£º519	 i:0 	 global-step:10380	 l-p:0.12645213305950165
epoch£º519	 i:1 	 global-step:10381	 l-p:0.17120575904846191
epoch£º519	 i:2 	 global-step:10382	 l-p:0.11057168990373611
epoch£º519	 i:3 	 global-step:10383	 l-p:0.14919473230838776
epoch£º519	 i:4 	 global-step:10384	 l-p:-0.049461305141448975
epoch£º519	 i:5 	 global-step:10385	 l-p:-0.1379265934228897
epoch£º519	 i:6 	 global-step:10386	 l-p:0.12000200897455215
epoch£º519	 i:7 	 global-step:10387	 l-p:0.07705309242010117
epoch£º519	 i:8 	 global-step:10388	 l-p:0.11369706690311432
epoch£º519	 i:9 	 global-step:10389	 l-p:0.11694711446762085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1557, 5.1557, 5.1557],
        [5.1557, 5.6885, 5.7785],
        [5.1557, 5.1557, 5.1557],
        [5.1557, 5.1486, 5.1550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.13444450497627258 
model_pd.l_d.mean(): -19.56688117980957 
model_pd.lagr.mean(): -19.432435989379883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4464], device='cuda:0')), ('power', tensor([-20.2368], device='cuda:0'))])
epoch£º520	 i:0 	 global-step:10400	 l-p:0.13444450497627258
epoch£º520	 i:1 	 global-step:10401	 l-p:0.07269026339054108
epoch£º520	 i:2 	 global-step:10402	 l-p:0.12152545899152756
epoch£º520	 i:3 	 global-step:10403	 l-p:-0.08666948229074478
epoch£º520	 i:4 	 global-step:10404	 l-p:0.16179615259170532
epoch£º520	 i:5 	 global-step:10405	 l-p:0.23130853474140167
epoch£º520	 i:6 	 global-step:10406	 l-p:0.1331491470336914
epoch£º520	 i:7 	 global-step:10407	 l-p:0.12970910966396332
epoch£º520	 i:8 	 global-step:10408	 l-p:0.1403874009847641
epoch£º520	 i:9 	 global-step:10409	 l-p:0.1676662564277649
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0699, 4.9587, 4.8879],
        [5.0699, 5.0429, 5.0630],
        [5.0699, 5.6573, 5.7951],
        [5.0699, 4.9597, 4.9273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.1352922022342682 
model_pd.l_d.mean(): -20.731698989868164 
model_pd.lagr.mean(): -20.596406936645508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4195], device='cuda:0')), ('power', tensor([-21.3868], device='cuda:0'))])
epoch£º521	 i:0 	 global-step:10420	 l-p:0.1352922022342682
epoch£º521	 i:1 	 global-step:10421	 l-p:0.12606891989707947
epoch£º521	 i:2 	 global-step:10422	 l-p:-0.02614402212202549
epoch£º521	 i:3 	 global-step:10423	 l-p:0.15491193532943726
epoch£º521	 i:4 	 global-step:10424	 l-p:0.11574433743953705
epoch£º521	 i:5 	 global-step:10425	 l-p:0.15734413266181946
epoch£º521	 i:6 	 global-step:10426	 l-p:0.1504286378622055
epoch£º521	 i:7 	 global-step:10427	 l-p:0.14748939871788025
epoch£º521	 i:8 	 global-step:10428	 l-p:0.11903684586286545
epoch£º521	 i:9 	 global-step:10429	 l-p:0.2956547141075134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9050, 4.9013, 4.9048],
        [4.9050, 4.9050, 4.9050],
        [4.9050, 4.8964, 4.9041],
        [4.9050, 4.8954, 4.9039]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): 0.11875292658805847 
model_pd.l_d.mean(): -20.757417678833008 
model_pd.lagr.mean(): -20.63866424560547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4827], device='cuda:0')), ('power', tensor([-21.4774], device='cuda:0'))])
epoch£º522	 i:0 	 global-step:10440	 l-p:0.11875292658805847
epoch£º522	 i:1 	 global-step:10441	 l-p:0.21662230789661407
epoch£º522	 i:2 	 global-step:10442	 l-p:0.3077223002910614
epoch£º522	 i:3 	 global-step:10443	 l-p:0.1520383059978485
epoch£º522	 i:4 	 global-step:10444	 l-p:0.1865919828414917
epoch£º522	 i:5 	 global-step:10445	 l-p:0.1696282923221588
epoch£º522	 i:6 	 global-step:10446	 l-p:0.08143104612827301
epoch£º522	 i:7 	 global-step:10447	 l-p:0.11030614376068115
epoch£º522	 i:8 	 global-step:10448	 l-p:0.13306471705436707
epoch£º522	 i:9 	 global-step:10449	 l-p:0.11099888384342194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0397, 5.0221, 5.0366],
        [5.0397, 5.0397, 5.0397],
        [5.0397, 5.6142, 5.7432],
        [5.0397, 5.0396, 5.0397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.10473007708787918 
model_pd.l_d.mean(): -18.286067962646484 
model_pd.lagr.mean(): -18.181337356567383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6154], device='cuda:0')), ('power', tensor([-19.1146], device='cuda:0'))])
epoch£º523	 i:0 	 global-step:10460	 l-p:0.10473007708787918
epoch£º523	 i:1 	 global-step:10461	 l-p:0.09236310422420502
epoch£º523	 i:2 	 global-step:10462	 l-p:0.4555092751979828
epoch£º523	 i:3 	 global-step:10463	 l-p:0.14866222441196442
epoch£º523	 i:4 	 global-step:10464	 l-p:0.13474136590957642
epoch£º523	 i:5 	 global-step:10465	 l-p:-0.02696809731423855
epoch£º523	 i:6 	 global-step:10466	 l-p:0.16039691865444183
epoch£º523	 i:7 	 global-step:10467	 l-p:0.1478133350610733
epoch£º523	 i:8 	 global-step:10468	 l-p:0.14180925488471985
epoch£º523	 i:9 	 global-step:10469	 l-p:0.13877445459365845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0080, 4.9971, 5.0067],
        [5.0080, 5.2800, 5.1983],
        [5.0080, 5.0077, 5.0080],
        [5.0080, 4.9977, 5.0068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.19169263541698456 
model_pd.l_d.mean(): -20.321868896484375 
model_pd.lagr.mean(): -20.130176544189453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4832], device='cuda:0')), ('power', tensor([-21.0375], device='cuda:0'))])
epoch£º524	 i:0 	 global-step:10480	 l-p:0.19169263541698456
epoch£º524	 i:1 	 global-step:10481	 l-p:0.12700170278549194
epoch£º524	 i:2 	 global-step:10482	 l-p:0.020452136173844337
epoch£º524	 i:3 	 global-step:10483	 l-p:0.10827542841434479
epoch£º524	 i:4 	 global-step:10484	 l-p:0.134401336312294
epoch£º524	 i:5 	 global-step:10485	 l-p:0.1323963850736618
epoch£º524	 i:6 	 global-step:10486	 l-p:0.13259325921535492
epoch£º524	 i:7 	 global-step:10487	 l-p:0.1246606633067131
epoch£º524	 i:8 	 global-step:10488	 l-p:0.15002626180648804
epoch£º524	 i:9 	 global-step:10489	 l-p:0.11380830407142639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9851, 4.9828, 4.9850],
        [4.9851, 4.9851, 4.9851],
        [4.9851, 4.9820, 4.9850],
        [4.9851, 4.8642, 4.8157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.16473335027694702 
model_pd.l_d.mean(): -19.7249755859375 
model_pd.lagr.mean(): -19.56024169921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5186], device='cuda:0')), ('power', tensor([-20.4703], device='cuda:0'))])
epoch£º525	 i:0 	 global-step:10500	 l-p:0.16473335027694702
epoch£º525	 i:1 	 global-step:10501	 l-p:0.08025922626256943
epoch£º525	 i:2 	 global-step:10502	 l-p:0.12371131032705307
epoch£º525	 i:3 	 global-step:10503	 l-p:0.12701654434204102
epoch£º525	 i:4 	 global-step:10504	 l-p:0.12379077821969986
epoch£º525	 i:5 	 global-step:10505	 l-p:0.12496768683195114
epoch£º525	 i:6 	 global-step:10506	 l-p:0.18822619318962097
epoch£º525	 i:7 	 global-step:10507	 l-p:0.1739872694015503
epoch£º525	 i:8 	 global-step:10508	 l-p:0.1360543966293335
epoch£º525	 i:9 	 global-step:10509	 l-p:0.10687689483165741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9918, 4.9809, 4.9904],
        [4.9918, 4.8802, 4.8711],
        [4.9918, 5.0594, 4.8850],
        [4.9918, 5.1312, 4.9809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.1532820165157318 
model_pd.l_d.mean(): -20.66183853149414 
model_pd.lagr.mean(): -20.508556365966797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4726], device='cuda:0')), ('power', tensor([-21.3704], device='cuda:0'))])
epoch£º526	 i:0 	 global-step:10520	 l-p:0.1532820165157318
epoch£º526	 i:1 	 global-step:10521	 l-p:0.14486590027809143
epoch£º526	 i:2 	 global-step:10522	 l-p:0.06273727118968964
epoch£º526	 i:3 	 global-step:10523	 l-p:0.12644197046756744
epoch£º526	 i:4 	 global-step:10524	 l-p:0.12470301240682602
epoch£º526	 i:5 	 global-step:10525	 l-p:0.13747678697109222
epoch£º526	 i:6 	 global-step:10526	 l-p:0.18948882818222046
epoch£º526	 i:7 	 global-step:10527	 l-p:0.1382695585489273
epoch£º526	 i:8 	 global-step:10528	 l-p:0.6737019419670105
epoch£º526	 i:9 	 global-step:10529	 l-p:0.13429802656173706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1642, 5.1642, 5.1642],
        [5.1642, 5.0683, 4.9554],
        [5.1642, 5.1642, 5.1642],
        [5.1642, 5.1636, 5.1642]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.1275254338979721 
model_pd.l_d.mean(): -20.502452850341797 
model_pd.lagr.mean(): -20.374927520751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4161], device='cuda:0')), ('power', tensor([-21.1515], device='cuda:0'))])
epoch£º527	 i:0 	 global-step:10540	 l-p:0.1275254338979721
epoch£º527	 i:1 	 global-step:10541	 l-p:0.13407191634178162
epoch£º527	 i:2 	 global-step:10542	 l-p:0.1534498929977417
epoch£º527	 i:3 	 global-step:10543	 l-p:0.14059185981750488
epoch£º527	 i:4 	 global-step:10544	 l-p:-0.21623502671718597
epoch£º527	 i:5 	 global-step:10545	 l-p:0.13311927020549774
epoch£º527	 i:6 	 global-step:10546	 l-p:0.13545119762420654
epoch£º527	 i:7 	 global-step:10547	 l-p:-0.1652781069278717
epoch£º527	 i:8 	 global-step:10548	 l-p:0.12807054817676544
epoch£º527	 i:9 	 global-step:10549	 l-p:0.14105510711669922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0287, 4.9847, 5.0123],
        [5.0287, 5.0222, 5.0281],
        [5.0287, 5.3353, 5.2728],
        [5.0287, 5.3355, 5.2730]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): 0.05755172669887543 
model_pd.l_d.mean(): -19.785476684570312 
model_pd.lagr.mean(): -19.727924346923828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5286], device='cuda:0')), ('power', tensor([-20.5417], device='cuda:0'))])
epoch£º528	 i:0 	 global-step:10560	 l-p:0.05755172669887543
epoch£º528	 i:1 	 global-step:10561	 l-p:0.12322379648685455
epoch£º528	 i:2 	 global-step:10562	 l-p:0.1323624700307846
epoch£º528	 i:3 	 global-step:10563	 l-p:0.16283515095710754
epoch£º528	 i:4 	 global-step:10564	 l-p:0.12519408762454987
epoch£º528	 i:5 	 global-step:10565	 l-p:0.1332339197397232
epoch£º528	 i:6 	 global-step:10566	 l-p:0.1649736762046814
epoch£º528	 i:7 	 global-step:10567	 l-p:0.15805000066757202
epoch£º528	 i:8 	 global-step:10568	 l-p:0.13569340109825134
epoch£º528	 i:9 	 global-step:10569	 l-p:0.16166271269321442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3405,  0.2378,  1.0000,  0.1660,
          1.0000,  0.6983, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6844,  0.6031,  1.0000,  0.5315,
          1.0000,  0.8812, 31.6228]], device='cuda:0')
 pt:tensor([[4.9424, 4.8269, 4.7009],
        [4.9424, 4.8947, 4.7119],
        [4.9424, 4.8656, 4.8951],
        [4.9424, 5.1486, 5.0304]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): 0.10934312641620636 
model_pd.l_d.mean(): -20.695505142211914 
model_pd.lagr.mean(): -20.586162567138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4823], device='cuda:0')), ('power', tensor([-21.4144], device='cuda:0'))])
epoch£º529	 i:0 	 global-step:10580	 l-p:0.10934312641620636
epoch£º529	 i:1 	 global-step:10581	 l-p:0.10122045129537582
epoch£º529	 i:2 	 global-step:10582	 l-p:0.14472505450248718
epoch£º529	 i:3 	 global-step:10583	 l-p:0.14232835173606873
epoch£º529	 i:4 	 global-step:10584	 l-p:0.20275947451591492
epoch£º529	 i:5 	 global-step:10585	 l-p:0.07998541742563248
epoch£º529	 i:6 	 global-step:10586	 l-p:0.15891195833683014
epoch£º529	 i:7 	 global-step:10587	 l-p:0.1566459983587265
epoch£º529	 i:8 	 global-step:10588	 l-p:0.15673847496509552
epoch£º529	 i:9 	 global-step:10589	 l-p:0.14293193817138672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0031, 5.0031, 5.0031],
        [5.0031, 4.8794, 4.8090],
        [5.0031, 5.0031, 5.0031],
        [5.0031, 5.1158, 4.9536]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.13556574285030365 
model_pd.l_d.mean(): -20.335790634155273 
model_pd.lagr.mean(): -20.200225830078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4951], device='cuda:0')), ('power', tensor([-21.0638], device='cuda:0'))])
epoch£º530	 i:0 	 global-step:10600	 l-p:0.13556574285030365
epoch£º530	 i:1 	 global-step:10601	 l-p:0.12931451201438904
epoch£º530	 i:2 	 global-step:10602	 l-p:0.1241995245218277
epoch£º530	 i:3 	 global-step:10603	 l-p:0.09526954591274261
epoch£º530	 i:4 	 global-step:10604	 l-p:-0.1010124534368515
epoch£º530	 i:5 	 global-step:10605	 l-p:0.13417574763298035
epoch£º530	 i:6 	 global-step:10606	 l-p:0.1664808690547943
epoch£º530	 i:7 	 global-step:10607	 l-p:0.10945173352956772
epoch£º530	 i:8 	 global-step:10608	 l-p:0.1350679248571396
epoch£º530	 i:9 	 global-step:10609	 l-p:0.0727553516626358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0905, 5.0856, 5.0902],
        [5.0905, 5.0905, 5.0905],
        [5.0905, 5.0906, 5.0905],
        [5.0905, 5.0906, 5.0905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.14369355142116547 
model_pd.l_d.mean(): -19.420246124267578 
model_pd.lagr.mean(): -19.276552200317383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4778], device='cuda:0')), ('power', tensor([-20.1206], device='cuda:0'))])
epoch£º531	 i:0 	 global-step:10620	 l-p:0.14369355142116547
epoch£º531	 i:1 	 global-step:10621	 l-p:0.127309188246727
epoch£º531	 i:2 	 global-step:10622	 l-p:0.15478883683681488
epoch£º531	 i:3 	 global-step:10623	 l-p:0.13601218163967133
epoch£º531	 i:4 	 global-step:10624	 l-p:0.08020906150341034
epoch£º531	 i:5 	 global-step:10625	 l-p:0.2298824042081833
epoch£º531	 i:6 	 global-step:10626	 l-p:-0.45095619559288025
epoch£º531	 i:7 	 global-step:10627	 l-p:0.13477899134159088
epoch£º531	 i:8 	 global-step:10628	 l-p:0.13641783595085144
epoch£º531	 i:9 	 global-step:10629	 l-p:0.13361886143684387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1608, 5.1797, 4.9999],
        [5.1608, 5.1162, 5.1435],
        [5.1608, 5.2487, 5.0794],
        [5.1608, 5.1581, 5.1607]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): 0.05959245562553406 
model_pd.l_d.mean(): -19.41823959350586 
model_pd.lagr.mean(): -19.358646392822266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4769], device='cuda:0')), ('power', tensor([-20.1177], device='cuda:0'))])
epoch£º532	 i:0 	 global-step:10640	 l-p:0.05959245562553406
epoch£º532	 i:1 	 global-step:10641	 l-p:0.13193677365779877
epoch£º532	 i:2 	 global-step:10642	 l-p:0.2180146723985672
epoch£º532	 i:3 	 global-step:10643	 l-p:0.125499427318573
epoch£º532	 i:4 	 global-step:10644	 l-p:0.015101706609129906
epoch£º532	 i:5 	 global-step:10645	 l-p:0.15716242790222168
epoch£º532	 i:6 	 global-step:10646	 l-p:0.11802302300930023
epoch£º532	 i:7 	 global-step:10647	 l-p:0.11525600403547287
epoch£º532	 i:8 	 global-step:10648	 l-p:0.10882017016410828
epoch£º532	 i:9 	 global-step:10649	 l-p:0.33037108182907104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2433, 5.5671, 5.5053],
        [5.2433, 5.2402, 5.2431],
        [5.2433, 5.2200, 5.2379],
        [5.2433, 5.1356, 5.0910]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.12404830753803253 
model_pd.l_d.mean(): -19.717227935791016 
model_pd.lagr.mean(): -19.59317970275879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4307], device='cuda:0')), ('power', tensor([-20.3727], device='cuda:0'))])
epoch£º533	 i:0 	 global-step:10660	 l-p:0.12404830753803253
epoch£º533	 i:1 	 global-step:10661	 l-p:0.11621345579624176
epoch£º533	 i:2 	 global-step:10662	 l-p:0.11667583137750626
epoch£º533	 i:3 	 global-step:10663	 l-p:0.14360052347183228
epoch£º533	 i:4 	 global-step:10664	 l-p:0.01083525363355875
epoch£º533	 i:5 	 global-step:10665	 l-p:0.1374131590127945
epoch£º533	 i:6 	 global-step:10666	 l-p:0.10530929267406464
epoch£º533	 i:7 	 global-step:10667	 l-p:0.1003803238272667
epoch£º533	 i:8 	 global-step:10668	 l-p:0.1394825428724289
epoch£º533	 i:9 	 global-step:10669	 l-p:-0.1522664576768875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1267, 5.0117, 4.9776],
        [5.1267, 5.0546, 4.8966],
        [5.1267, 5.1188, 5.1259],
        [5.1267, 5.0111, 4.9318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): 0.08845709264278412 
model_pd.l_d.mean(): -19.962793350219727 
model_pd.lagr.mean(): -19.87433624267578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5044], device='cuda:0')), ('power', tensor([-20.6962], device='cuda:0'))])
epoch£º534	 i:0 	 global-step:10680	 l-p:0.08845709264278412
epoch£º534	 i:1 	 global-step:10681	 l-p:0.13876502215862274
epoch£º534	 i:2 	 global-step:10682	 l-p:0.1269179731607437
epoch£º534	 i:3 	 global-step:10683	 l-p:0.12896254658699036
epoch£º534	 i:4 	 global-step:10684	 l-p:0.11288036406040192
epoch£º534	 i:5 	 global-step:10685	 l-p:-0.2665359377861023
epoch£º534	 i:6 	 global-step:10686	 l-p:0.22058093547821045
epoch£º534	 i:7 	 global-step:10687	 l-p:0.11738868802785873
epoch£º534	 i:8 	 global-step:10688	 l-p:0.14906468987464905
epoch£º534	 i:9 	 global-step:10689	 l-p:0.15808284282684326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0375, 5.0373, 5.0375],
        [5.0375, 5.0171, 5.0336],
        [5.0375, 5.0375, 5.0375],
        [5.0375, 4.9936, 5.0216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): -0.10074320435523987 
model_pd.l_d.mean(): -20.330839157104492 
model_pd.lagr.mean(): -20.431581497192383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4647], device='cuda:0')), ('power', tensor([-21.0277], device='cuda:0'))])
epoch£º535	 i:0 	 global-step:10700	 l-p:-0.10074320435523987
epoch£º535	 i:1 	 global-step:10701	 l-p:0.14397093653678894
epoch£º535	 i:2 	 global-step:10702	 l-p:0.15479768812656403
epoch£º535	 i:3 	 global-step:10703	 l-p:0.16174788773059845
epoch£º535	 i:4 	 global-step:10704	 l-p:0.12405281513929367
epoch£º535	 i:5 	 global-step:10705	 l-p:0.15071389079093933
epoch£º535	 i:6 	 global-step:10706	 l-p:0.12849055230617523
epoch£º535	 i:7 	 global-step:10707	 l-p:0.11199858784675598
epoch£º535	 i:8 	 global-step:10708	 l-p:0.10123080015182495
epoch£º535	 i:9 	 global-step:10709	 l-p:0.13834622502326965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9815, 4.9707, 4.9802],
        [4.9815, 4.9762, 4.9811],
        [4.9815, 4.9716, 4.9804],
        [4.9815, 5.0805, 4.9109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.15815958380699158 
model_pd.l_d.mean(): -19.571430206298828 
model_pd.lagr.mean(): -19.413270950317383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5352], device='cuda:0')), ('power', tensor([-20.3321], device='cuda:0'))])
epoch£º536	 i:0 	 global-step:10720	 l-p:0.15815958380699158
epoch£º536	 i:1 	 global-step:10721	 l-p:0.08012606203556061
epoch£º536	 i:2 	 global-step:10722	 l-p:0.05292550474405289
epoch£º536	 i:3 	 global-step:10723	 l-p:0.10671384632587433
epoch£º536	 i:4 	 global-step:10724	 l-p:0.11275818943977356
epoch£º536	 i:5 	 global-step:10725	 l-p:0.10967245697975159
epoch£º536	 i:6 	 global-step:10726	 l-p:0.14002937078475952
epoch£º536	 i:7 	 global-step:10727	 l-p:0.1546943634748459
epoch£º536	 i:8 	 global-step:10728	 l-p:0.13090088963508606
epoch£º536	 i:9 	 global-step:10729	 l-p:0.10903570055961609
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1112, 5.1112, 5.1112],
        [5.1112, 4.9911, 4.9373],
        [5.1112, 4.9920, 4.9485],
        [5.1112, 5.1112, 5.1112]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): 0.10405310988426208 
model_pd.l_d.mean(): -20.83281707763672 
model_pd.lagr.mean(): -20.728763580322266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3887], device='cuda:0')), ('power', tensor([-21.4575], device='cuda:0'))])
epoch£º537	 i:0 	 global-step:10740	 l-p:0.10405310988426208
epoch£º537	 i:1 	 global-step:10741	 l-p:0.12469936162233353
epoch£º537	 i:2 	 global-step:10742	 l-p:0.19789765775203705
epoch£º537	 i:3 	 global-step:10743	 l-p:0.07263991236686707
epoch£º537	 i:4 	 global-step:10744	 l-p:0.13465076684951782
epoch£º537	 i:5 	 global-step:10745	 l-p:0.06702180951833725
epoch£º537	 i:6 	 global-step:10746	 l-p:0.12538796663284302
epoch£º537	 i:7 	 global-step:10747	 l-p:0.10296075791120529
epoch£º537	 i:8 	 global-step:10748	 l-p:0.12915875017642975
epoch£º537	 i:9 	 global-step:10749	 l-p:0.16805849969387054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2101, 5.1788, 5.2012],
        [5.2101, 5.2096, 5.2101],
        [5.2101, 5.2101, 5.2101],
        [5.2101, 5.1603, 5.1887]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.12947820127010345 
model_pd.l_d.mean(): -20.619354248046875 
model_pd.lagr.mean(): -20.48987579345703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3970], device='cuda:0')), ('power', tensor([-21.2502], device='cuda:0'))])
epoch£º538	 i:0 	 global-step:10760	 l-p:0.12947820127010345
epoch£º538	 i:1 	 global-step:10761	 l-p:0.003354809246957302
epoch£º538	 i:2 	 global-step:10762	 l-p:0.13378877937793732
epoch£º538	 i:3 	 global-step:10763	 l-p:0.11173634231090546
epoch£º538	 i:4 	 global-step:10764	 l-p:0.005390441510826349
epoch£º538	 i:5 	 global-step:10765	 l-p:0.16614386439323425
epoch£º538	 i:6 	 global-step:10766	 l-p:0.13069148361682892
epoch£º538	 i:7 	 global-step:10767	 l-p:0.235728919506073
epoch£º538	 i:8 	 global-step:10768	 l-p:0.133627787232399
epoch£º538	 i:9 	 global-step:10769	 l-p:-0.006996040232479572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2158, 5.1359, 5.1589],
        [5.2158, 5.2158, 5.2158],
        [5.2158, 5.2033, 5.2141],
        [5.2158, 5.2157, 5.2158]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.11488496512174606 
model_pd.l_d.mean(): -20.48424530029297 
model_pd.lagr.mean(): -20.369359970092773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4190], device='cuda:0')), ('power', tensor([-21.1362], device='cuda:0'))])
epoch£º539	 i:0 	 global-step:10780	 l-p:0.11488496512174606
epoch£º539	 i:1 	 global-step:10781	 l-p:-1.0583525896072388
epoch£º539	 i:2 	 global-step:10782	 l-p:0.12796683609485626
epoch£º539	 i:3 	 global-step:10783	 l-p:0.15192461013793945
epoch£º539	 i:4 	 global-step:10784	 l-p:0.12337777763605118
epoch£º539	 i:5 	 global-step:10785	 l-p:0.12353722006082535
epoch£º539	 i:6 	 global-step:10786	 l-p:0.11530819535255432
epoch£º539	 i:7 	 global-step:10787	 l-p:0.11703896522521973
epoch£º539	 i:8 	 global-step:10788	 l-p:0.17787085473537445
epoch£º539	 i:9 	 global-step:10789	 l-p:-0.055683963000774384
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1920, 5.1049, 4.9640],
        [5.1920, 5.1920, 5.1920],
        [5.1920, 5.0913, 5.0943],
        [5.1920, 5.3368, 5.1837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.15343908965587616 
model_pd.l_d.mean(): -19.28717613220215 
model_pd.lagr.mean(): -19.133737564086914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5379], device='cuda:0')), ('power', tensor([-20.0474], device='cuda:0'))])
epoch£º540	 i:0 	 global-step:10800	 l-p:0.15343908965587616
epoch£º540	 i:1 	 global-step:10801	 l-p:-0.03457999974489212
epoch£º540	 i:2 	 global-step:10802	 l-p:0.13242097198963165
epoch£º540	 i:3 	 global-step:10803	 l-p:0.12287414073944092
epoch£º540	 i:4 	 global-step:10804	 l-p:-0.004852938465774059
epoch£º540	 i:5 	 global-step:10805	 l-p:0.11979503184556961
epoch£º540	 i:6 	 global-step:10806	 l-p:0.12362563610076904
epoch£º540	 i:7 	 global-step:10807	 l-p:0.1288643777370453
epoch£º540	 i:8 	 global-step:10808	 l-p:0.13212570548057556
epoch£º540	 i:9 	 global-step:10809	 l-p:0.13826113939285278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1439, 5.1084, 5.1330],
        [5.1439, 5.1156, 5.1367],
        [5.1439, 5.6520, 5.7190],
        [5.1439, 5.1439, 5.1439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.11340872198343277 
model_pd.l_d.mean(): -19.215856552124023 
model_pd.lagr.mean(): -19.102447509765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4787], device='cuda:0')), ('power', tensor([-19.9149], device='cuda:0'))])
epoch£º541	 i:0 	 global-step:10820	 l-p:0.11340872198343277
epoch£º541	 i:1 	 global-step:10821	 l-p:0.13344573974609375
epoch£º541	 i:2 	 global-step:10822	 l-p:0.2370210736989975
epoch£º541	 i:3 	 global-step:10823	 l-p:-0.07163791358470917
epoch£º541	 i:4 	 global-step:10824	 l-p:0.1444908082485199
epoch£º541	 i:5 	 global-step:10825	 l-p:0.12453080713748932
epoch£º541	 i:6 	 global-step:10826	 l-p:0.11126253008842468
epoch£º541	 i:7 	 global-step:10827	 l-p:0.09601812809705734
epoch£º541	 i:8 	 global-step:10828	 l-p:0.11932273954153061
epoch£º541	 i:9 	 global-step:10829	 l-p:0.13881228864192963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9883, 4.9883, 4.9883],
        [4.9883, 4.8640, 4.7443],
        [4.9883, 5.6241, 5.8000],
        [4.9883, 4.8911, 4.7287]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.14183102548122406 
model_pd.l_d.mean(): -18.58171272277832 
model_pd.lagr.mean(): -18.439882278442383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5762], device='cuda:0')), ('power', tensor([-19.3734], device='cuda:0'))])
epoch£º542	 i:0 	 global-step:10840	 l-p:0.14183102548122406
epoch£º542	 i:1 	 global-step:10841	 l-p:0.1652653068304062
epoch£º542	 i:2 	 global-step:10842	 l-p:0.13353237509727478
epoch£º542	 i:3 	 global-step:10843	 l-p:0.26713839173316956
epoch£º542	 i:4 	 global-step:10844	 l-p:0.10056401044130325
epoch£º542	 i:5 	 global-step:10845	 l-p:0.14364807307720184
epoch£º542	 i:6 	 global-step:10846	 l-p:0.14433786273002625
epoch£º542	 i:7 	 global-step:10847	 l-p:0.5610461235046387
epoch£º542	 i:8 	 global-step:10848	 l-p:0.30277952551841736
epoch£º542	 i:9 	 global-step:10849	 l-p:0.13766206800937653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8686, 4.7498, 4.5909],
        [4.8686, 5.3992, 5.4994],
        [4.8686, 4.7891, 4.8222],
        [4.8686, 4.8672, 4.8685]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.14364036917686462 
model_pd.l_d.mean(): -20.11041831970215 
model_pd.lagr.mean(): -19.966777801513672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5389], device='cuda:0')), ('power', tensor([-20.8807], device='cuda:0'))])
epoch£º543	 i:0 	 global-step:10860	 l-p:0.14364036917686462
epoch£º543	 i:1 	 global-step:10861	 l-p:0.10329202562570572
epoch£º543	 i:2 	 global-step:10862	 l-p:-0.10551530867815018
epoch£º543	 i:3 	 global-step:10863	 l-p:-0.3810061812400818
epoch£º543	 i:4 	 global-step:10864	 l-p:0.1369650959968567
epoch£º543	 i:5 	 global-step:10865	 l-p:0.12881530821323395
epoch£º543	 i:6 	 global-step:10866	 l-p:0.05309716612100601
epoch£º543	 i:7 	 global-step:10867	 l-p:-0.329248309135437
epoch£º543	 i:8 	 global-step:10868	 l-p:0.09501229971647263
epoch£º543	 i:9 	 global-step:10869	 l-p:0.09572816640138626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8625, 4.8614, 4.8625],
        [4.8625, 4.7624, 4.5831],
        [4.8625, 4.8625, 4.8625],
        [4.8625, 5.1769, 5.1229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.12171018123626709 
model_pd.l_d.mean(): -20.626352310180664 
model_pd.lagr.mean(): -20.504642486572266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4925], device='cuda:0')), ('power', tensor([-21.3549], device='cuda:0'))])
epoch£º544	 i:0 	 global-step:10880	 l-p:0.12171018123626709
epoch£º544	 i:1 	 global-step:10881	 l-p:0.14467273652553558
epoch£º544	 i:2 	 global-step:10882	 l-p:0.12188830226659775
epoch£º544	 i:3 	 global-step:10883	 l-p:-0.10341233015060425
epoch£º544	 i:4 	 global-step:10884	 l-p:0.15255846083164215
epoch£º544	 i:5 	 global-step:10885	 l-p:0.1809970885515213
epoch£º544	 i:6 	 global-step:10886	 l-p:0.1178242564201355
epoch£º544	 i:7 	 global-step:10887	 l-p:0.23988093435764313
epoch£º544	 i:8 	 global-step:10888	 l-p:0.17281821370124817
epoch£º544	 i:9 	 global-step:10889	 l-p:0.13859769701957703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9857, 4.9739, 4.9842],
        [4.9857, 4.9397, 4.9691],
        [4.9857, 4.8639, 4.8556],
        [4.9857, 4.8662, 4.8623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.12329653650522232 
model_pd.l_d.mean(): -20.78207015991211 
model_pd.lagr.mean(): -20.65877342224121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4559], device='cuda:0')), ('power', tensor([-21.4749], device='cuda:0'))])
epoch£º545	 i:0 	 global-step:10900	 l-p:0.12329653650522232
epoch£º545	 i:1 	 global-step:10901	 l-p:0.1557314097881317
epoch£º545	 i:2 	 global-step:10902	 l-p:0.1388641744852066
epoch£º545	 i:3 	 global-step:10903	 l-p:0.12399619817733765
epoch£º545	 i:4 	 global-step:10904	 l-p:0.09832015633583069
epoch£º545	 i:5 	 global-step:10905	 l-p:0.13763581216335297
epoch£º545	 i:6 	 global-step:10906	 l-p:0.17472593486309052
epoch£º545	 i:7 	 global-step:10907	 l-p:0.12853001058101654
epoch£º545	 i:8 	 global-step:10908	 l-p:0.015302104875445366
epoch£º545	 i:9 	 global-step:10909	 l-p:0.12094677984714508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0455, 5.0188, 5.0393],
        [5.0455, 5.0199, 5.0398],
        [5.0455, 5.0453, 5.0455],
        [5.0455, 4.9209, 4.8086]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.13386806845664978 
model_pd.l_d.mean(): -19.83211898803711 
model_pd.lagr.mean(): -19.698251724243164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4380], device='cuda:0')), ('power', tensor([-20.4964], device='cuda:0'))])
epoch£º546	 i:0 	 global-step:10920	 l-p:0.13386806845664978
epoch£º546	 i:1 	 global-step:10921	 l-p:0.15344461798667908
epoch£º546	 i:2 	 global-step:10922	 l-p:0.14406172931194305
epoch£º546	 i:3 	 global-step:10923	 l-p:0.13410153985023499
epoch£º546	 i:4 	 global-step:10924	 l-p:0.09008491039276123
epoch£º546	 i:5 	 global-step:10925	 l-p:0.07950188964605331
epoch£º546	 i:6 	 global-step:10926	 l-p:0.16024582087993622
epoch£º546	 i:7 	 global-step:10927	 l-p:0.17300720512866974
epoch£º546	 i:8 	 global-step:10928	 l-p:0.14190851151943207
epoch£º546	 i:9 	 global-step:10929	 l-p:0.07451462745666504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9875, 4.8879, 4.9098],
        [4.9875, 5.4561, 5.5012],
        [4.9875, 4.9865, 4.9875],
        [4.9875, 4.9875, 4.9875]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.1659105271100998 
model_pd.l_d.mean(): -20.55292510986328 
model_pd.lagr.mean(): -20.387014389038086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4763], device='cuda:0')), ('power', tensor([-21.2641], device='cuda:0'))])
epoch£º547	 i:0 	 global-step:10940	 l-p:0.1659105271100998
epoch£º547	 i:1 	 global-step:10941	 l-p:0.10031411796808243
epoch£º547	 i:2 	 global-step:10942	 l-p:0.12158425152301788
epoch£º547	 i:3 	 global-step:10943	 l-p:0.12433397769927979
epoch£º547	 i:4 	 global-step:10944	 l-p:0.0058438777923583984
epoch£º547	 i:5 	 global-step:10945	 l-p:0.16682177782058716
epoch£º547	 i:6 	 global-step:10946	 l-p:0.145223468542099
epoch£º547	 i:7 	 global-step:10947	 l-p:0.07717539370059967
epoch£º547	 i:8 	 global-step:10948	 l-p:0.13609078526496887
epoch£º547	 i:9 	 global-step:10949	 l-p:0.11381115019321442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0967, 5.0964, 5.0967],
        [5.0967, 5.0094, 5.0355],
        [5.0967, 5.0656, 5.0885],
        [5.0967, 5.0514, 5.0803]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.11791308969259262 
model_pd.l_d.mean(): -19.429424285888672 
model_pd.lagr.mean(): -19.311511993408203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5127], device='cuda:0')), ('power', tensor([-20.1656], device='cuda:0'))])
epoch£º548	 i:0 	 global-step:10960	 l-p:0.11791308969259262
epoch£º548	 i:1 	 global-step:10961	 l-p:0.12622390687465668
epoch£º548	 i:2 	 global-step:10962	 l-p:0.14369674026966095
epoch£º548	 i:3 	 global-step:10963	 l-p:0.12195901572704315
epoch£º548	 i:4 	 global-step:10964	 l-p:0.1571217030286789
epoch£º548	 i:5 	 global-step:10965	 l-p:0.1157507672905922
epoch£º548	 i:6 	 global-step:10966	 l-p:0.12953414022922516
epoch£º548	 i:7 	 global-step:10967	 l-p:0.053860679268836975
epoch£º548	 i:8 	 global-step:10968	 l-p:0.11801566183567047
epoch£º548	 i:9 	 global-step:10969	 l-p:0.020650191232562065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0411, 5.3839, 5.3379],
        [5.0411, 5.0361, 5.0408],
        [5.0411, 5.0332, 5.0404],
        [5.0411, 4.9097, 4.8710]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): 0.08869924396276474 
model_pd.l_d.mean(): -20.569318771362305 
model_pd.lagr.mean(): -20.480619430541992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4440], device='cuda:0')), ('power', tensor([-21.2476], device='cuda:0'))])
epoch£º549	 i:0 	 global-step:10980	 l-p:0.08869924396276474
epoch£º549	 i:1 	 global-step:10981	 l-p:0.1347605437040329
epoch£º549	 i:2 	 global-step:10982	 l-p:0.1485726535320282
epoch£º549	 i:3 	 global-step:10983	 l-p:0.12481135874986649
epoch£º549	 i:4 	 global-step:10984	 l-p:0.0951513722538948
epoch£º549	 i:5 	 global-step:10985	 l-p:0.11323998868465424
epoch£º549	 i:6 	 global-step:10986	 l-p:0.13502882421016693
epoch£º549	 i:7 	 global-step:10987	 l-p:0.1418571025133133
epoch£º549	 i:8 	 global-step:10988	 l-p:0.13359996676445007
epoch£º549	 i:9 	 global-step:10989	 l-p:0.10322616994380951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9868, 4.9529, 4.9776],
        [4.9868, 4.8542, 4.7309],
        [4.9868, 4.8466, 4.7765],
        [4.9868, 4.9316, 4.9639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.14026296138763428 
model_pd.l_d.mean(): -18.53935432434082 
model_pd.lagr.mean(): -18.399091720581055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5882], device='cuda:0')), ('power', tensor([-19.3428], device='cuda:0'))])
epoch£º550	 i:0 	 global-step:11000	 l-p:0.14026296138763428
epoch£º550	 i:1 	 global-step:11001	 l-p:0.08511757850646973
epoch£º550	 i:2 	 global-step:11002	 l-p:0.20051437616348267
epoch£º550	 i:3 	 global-step:11003	 l-p:0.08786425739526749
epoch£º550	 i:4 	 global-step:11004	 l-p:0.12016709893941879
epoch£º550	 i:5 	 global-step:11005	 l-p:0.13609836995601654
epoch£º550	 i:6 	 global-step:11006	 l-p:0.15731996297836304
epoch£º550	 i:7 	 global-step:11007	 l-p:0.1329631209373474
epoch£º550	 i:8 	 global-step:11008	 l-p:0.13560029864311218
epoch£º550	 i:9 	 global-step:11009	 l-p:0.1186256930232048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0223, 4.9122, 4.9253],
        [5.0223, 5.1131, 4.9341],
        [5.0223, 5.5881, 5.7029],
        [5.0223, 5.0223, 5.0223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.13344554603099823 
model_pd.l_d.mean(): -18.184417724609375 
model_pd.lagr.mean(): -18.05097198486328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5918], device='cuda:0')), ('power', tensor([-18.9878], device='cuda:0'))])
epoch£º551	 i:0 	 global-step:11020	 l-p:0.13344554603099823
epoch£º551	 i:1 	 global-step:11021	 l-p:0.14419786632061005
epoch£º551	 i:2 	 global-step:11022	 l-p:0.10732490569353104
epoch£º551	 i:3 	 global-step:11023	 l-p:0.1634492427110672
epoch£º551	 i:4 	 global-step:11024	 l-p:0.12024150788784027
epoch£º551	 i:5 	 global-step:11025	 l-p:0.15888376533985138
epoch£º551	 i:6 	 global-step:11026	 l-p:0.142058327794075
epoch£º551	 i:7 	 global-step:11027	 l-p:0.12873224914073944
epoch£º551	 i:8 	 global-step:11028	 l-p:0.12780630588531494
epoch£º551	 i:9 	 global-step:11029	 l-p:0.1385849416255951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9559, 4.8191, 4.6921],
        [4.9559, 4.9559, 4.9559],
        [4.9559, 4.9559, 4.9559],
        [4.9559, 4.9559, 4.9559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.11595221608877182 
model_pd.l_d.mean(): -19.956283569335938 
model_pd.lagr.mean(): -19.84033203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5306], device='cuda:0')), ('power', tensor([-20.7164], device='cuda:0'))])
epoch£º552	 i:0 	 global-step:11040	 l-p:0.11595221608877182
epoch£º552	 i:1 	 global-step:11041	 l-p:0.20462262630462646
epoch£º552	 i:2 	 global-step:11042	 l-p:0.23174650967121124
epoch£º552	 i:3 	 global-step:11043	 l-p:0.15725962817668915
epoch£º552	 i:4 	 global-step:11044	 l-p:0.13890740275382996
epoch£º552	 i:5 	 global-step:11045	 l-p:0.12206421792507172
epoch£º552	 i:6 	 global-step:11046	 l-p:0.13024066388607025
epoch£º552	 i:7 	 global-step:11047	 l-p:0.17318479716777802
epoch£º552	 i:8 	 global-step:11048	 l-p:0.18531060218811035
epoch£º552	 i:9 	 global-step:11049	 l-p:0.21418523788452148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9275, 4.9274, 4.9275],
        [4.9275, 4.7878, 4.6569],
        [4.9275, 5.4199, 5.4838],
        [4.9275, 4.8817, 4.9119]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.4315817058086395 
model_pd.l_d.mean(): -20.2767391204834 
model_pd.lagr.mean(): -19.845157623291016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5451], device='cuda:0')), ('power', tensor([-21.0552], device='cuda:0'))])
epoch£º553	 i:0 	 global-step:11060	 l-p:0.4315817058086395
epoch£º553	 i:1 	 global-step:11061	 l-p:0.1203087568283081
epoch£º553	 i:2 	 global-step:11062	 l-p:0.11621083319187164
epoch£º553	 i:3 	 global-step:11063	 l-p:0.18331527709960938
epoch£º553	 i:4 	 global-step:11064	 l-p:0.14311207830905914
epoch£º553	 i:5 	 global-step:11065	 l-p:0.08799251168966293
epoch£º553	 i:6 	 global-step:11066	 l-p:0.13482092320919037
epoch£º553	 i:7 	 global-step:11067	 l-p:0.13503894209861755
epoch£º553	 i:8 	 global-step:11068	 l-p:0.10734057426452637
epoch£º553	 i:9 	 global-step:11069	 l-p:0.1818426251411438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0441, 5.0226, 5.0400],
        [5.0441, 5.0432, 5.0441],
        [5.0441, 5.0175, 5.0381],
        [5.0441, 4.9850, 4.7919]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): 0.07789991796016693 
model_pd.l_d.mean(): -20.024166107177734 
model_pd.lagr.mean(): -19.946266174316406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4604], device='cuda:0')), ('power', tensor([-20.7133], device='cuda:0'))])
epoch£º554	 i:0 	 global-step:11080	 l-p:0.07789991796016693
epoch£º554	 i:1 	 global-step:11081	 l-p:0.1294519007205963
epoch£º554	 i:2 	 global-step:11082	 l-p:0.09290681779384613
epoch£º554	 i:3 	 global-step:11083	 l-p:0.10858695954084396
epoch£º554	 i:4 	 global-step:11084	 l-p:0.1404552012681961
epoch£º554	 i:5 	 global-step:11085	 l-p:0.29468071460723877
epoch£º554	 i:6 	 global-step:11086	 l-p:0.12234460562467575
epoch£º554	 i:7 	 global-step:11087	 l-p:0.13004516065120697
epoch£º554	 i:8 	 global-step:11088	 l-p:0.12334416806697845
epoch£º554	 i:9 	 global-step:11089	 l-p:0.137820303440094
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1365, 5.1365, 5.1365],
        [5.1365, 5.0616, 5.0932],
        [5.1365, 5.1237, 5.1348],
        [5.1365, 5.0471, 5.0735]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.14037291705608368 
model_pd.l_d.mean(): -20.424652099609375 
model_pd.lagr.mean(): -20.284278869628906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4345], device='cuda:0')), ('power', tensor([-21.0917], device='cuda:0'))])
epoch£º555	 i:0 	 global-step:11100	 l-p:0.14037291705608368
epoch£º555	 i:1 	 global-step:11101	 l-p:0.09782300889492035
epoch£º555	 i:2 	 global-step:11102	 l-p:0.10754243284463882
epoch£º555	 i:3 	 global-step:11103	 l-p:0.1992272138595581
epoch£º555	 i:4 	 global-step:11104	 l-p:0.2904244065284729
epoch£º555	 i:5 	 global-step:11105	 l-p:0.038023341447114944
epoch£º555	 i:6 	 global-step:11106	 l-p:0.14221350848674774
epoch£º555	 i:7 	 global-step:11107	 l-p:0.11590766906738281
epoch£º555	 i:8 	 global-step:11108	 l-p:0.11219024658203125
epoch£º555	 i:9 	 global-step:11109	 l-p:0.1296384632587433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2717, 5.1537, 5.0582],
        [5.2717, 5.2717, 5.2717],
        [5.2717, 5.3465, 5.1646],
        [5.2717, 5.2699, 5.2717]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): 0.1626046746969223 
model_pd.l_d.mean(): -20.562313079833984 
model_pd.lagr.mean(): -20.399707794189453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3986], device='cuda:0')), ('power', tensor([-21.1942], device='cuda:0'))])
epoch£º556	 i:0 	 global-step:11120	 l-p:0.1626046746969223
epoch£º556	 i:1 	 global-step:11121	 l-p:0.36145609617233276
epoch£º556	 i:2 	 global-step:11122	 l-p:0.248371422290802
epoch£º556	 i:3 	 global-step:11123	 l-p:0.11936754733324051
epoch£º556	 i:4 	 global-step:11124	 l-p:0.13507935404777527
epoch£º556	 i:5 	 global-step:11125	 l-p:0.14768752455711365
epoch£º556	 i:6 	 global-step:11126	 l-p:-0.03013046458363533
epoch£º556	 i:7 	 global-step:11127	 l-p:0.1258787214756012
epoch£º556	 i:8 	 global-step:11128	 l-p:0.11407124251127243
epoch£º556	 i:9 	 global-step:11129	 l-p:0.13514862954616547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1655, 5.1553, 5.1644],
        [5.1655, 5.0750, 4.9087],
        [5.1655, 5.1073, 5.1393],
        [5.1655, 5.2277, 5.0401]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.1410897970199585 
model_pd.l_d.mean(): -19.960697174072266 
model_pd.lagr.mean(): -19.81960678100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4965], device='cuda:0')), ('power', tensor([-20.6860], device='cuda:0'))])
epoch£º557	 i:0 	 global-step:11140	 l-p:0.1410897970199585
epoch£º557	 i:1 	 global-step:11141	 l-p:0.12961828708648682
epoch£º557	 i:2 	 global-step:11142	 l-p:0.04031037166714668
epoch£º557	 i:3 	 global-step:11143	 l-p:0.13036416471004486
epoch£º557	 i:4 	 global-step:11144	 l-p:0.13141407072544098
epoch£º557	 i:5 	 global-step:11145	 l-p:0.14241471886634827
epoch£º557	 i:6 	 global-step:11146	 l-p:-0.13388456404209137
epoch£º557	 i:7 	 global-step:11147	 l-p:0.14386431872844696
epoch£º557	 i:8 	 global-step:11148	 l-p:0.11769800633192062
epoch£º557	 i:9 	 global-step:11149	 l-p:0.12179999053478241
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0472, 4.9059, 4.8248],
        [5.0472, 5.0497, 4.8482],
        [5.0472, 5.0178, 5.0401],
        [5.0472, 5.0403, 5.0466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): 0.08807694166898727 
model_pd.l_d.mean(): -19.91998291015625 
model_pd.lagr.mean(): -19.831905364990234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4857], device='cuda:0')), ('power', tensor([-20.6339], device='cuda:0'))])
epoch£º558	 i:0 	 global-step:11160	 l-p:0.08807694166898727
epoch£º558	 i:1 	 global-step:11161	 l-p:0.06734483689069748
epoch£º558	 i:2 	 global-step:11162	 l-p:0.12860837578773499
epoch£º558	 i:3 	 global-step:11163	 l-p:0.1365857571363449
epoch£º558	 i:4 	 global-step:11164	 l-p:0.12009591609239578
epoch£º558	 i:5 	 global-step:11165	 l-p:0.14073581993579865
epoch£º558	 i:6 	 global-step:11166	 l-p:0.1263858824968338
epoch£º558	 i:7 	 global-step:11167	 l-p:0.1692592054605484
epoch£º558	 i:8 	 global-step:11168	 l-p:0.18547679483890533
epoch£º558	 i:9 	 global-step:11169	 l-p:0.15992867946624756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9821, 4.9047, 4.7070],
        [4.9821, 4.9727, 4.9811],
        [4.9821, 4.9820, 4.9820],
        [4.9821, 4.9813, 4.9820]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.14861781895160675 
model_pd.l_d.mean(): -19.896692276000977 
model_pd.lagr.mean(): -19.74807357788086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5698], device='cuda:0')), ('power', tensor([-20.6962], device='cuda:0'))])
epoch£º559	 i:0 	 global-step:11180	 l-p:0.14861781895160675
epoch£º559	 i:1 	 global-step:11181	 l-p:0.15188640356063843
epoch£º559	 i:2 	 global-step:11182	 l-p:0.10156901180744171
epoch£º559	 i:3 	 global-step:11183	 l-p:0.1597464382648468
epoch£º559	 i:4 	 global-step:11184	 l-p:0.1658322513103485
epoch£º559	 i:5 	 global-step:11185	 l-p:0.13380880653858185
epoch£º559	 i:6 	 global-step:11186	 l-p:0.15518733859062195
epoch£º559	 i:7 	 global-step:11187	 l-p:0.15105758607387543
epoch£º559	 i:8 	 global-step:11188	 l-p:0.04613722860813141
epoch£º559	 i:9 	 global-step:11189	 l-p:0.11896532028913498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9770,  0.9695,  1.0000,  0.9620,
          1.0000,  0.9923, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9814,  0.9752,  1.0000,  0.9691,
          1.0000,  0.9938, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228]], device='cuda:0')
 pt:tensor([[5.0795, 5.7116, 5.8717],
        [5.0795, 4.9525, 4.8134],
        [5.0795, 5.7183, 5.8837],
        [5.0795, 5.1085, 4.9099]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): 0.11861779540777206 
model_pd.l_d.mean(): -18.705516815185547 
model_pd.lagr.mean(): -18.586898803710938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5458], device='cuda:0')), ('power', tensor([-19.4675], device='cuda:0'))])
epoch£º560	 i:0 	 global-step:11200	 l-p:0.11861779540777206
epoch£º560	 i:1 	 global-step:11201	 l-p:0.28663378953933716
epoch£º560	 i:2 	 global-step:11202	 l-p:0.08261936157941818
epoch£º560	 i:3 	 global-step:11203	 l-p:-0.2536431550979614
epoch£º560	 i:4 	 global-step:11204	 l-p:0.17599113285541534
epoch£º560	 i:5 	 global-step:11205	 l-p:0.11258472502231598
epoch£º560	 i:6 	 global-step:11206	 l-p:0.13338613510131836
epoch£º560	 i:7 	 global-step:11207	 l-p:0.26827606558799744
epoch£º560	 i:8 	 global-step:11208	 l-p:0.1455966681241989
epoch£º560	 i:9 	 global-step:11209	 l-p:0.1201760470867157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3153, 5.3153, 5.3153],
        [5.3153, 5.2061, 5.0827],
        [5.3153, 5.3064, 5.3144],
        [5.3153, 5.4060, 5.2262]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): 0.1754511445760727 
model_pd.l_d.mean(): -19.67778778076172 
model_pd.lagr.mean(): -19.502336502075195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4130], device='cuda:0')), ('power', tensor([-20.3147], device='cuda:0'))])
epoch£º561	 i:0 	 global-step:11220	 l-p:0.1754511445760727
epoch£º561	 i:1 	 global-step:11221	 l-p:0.15389566123485565
epoch£º561	 i:2 	 global-step:11222	 l-p:0.13836029171943665
epoch£º561	 i:3 	 global-step:11223	 l-p:0.1137770339846611
epoch£º561	 i:4 	 global-step:11224	 l-p:0.12486480176448822
epoch£º561	 i:5 	 global-step:11225	 l-p:0.11904335767030716
epoch£º561	 i:6 	 global-step:11226	 l-p:0.08497641980648041
epoch£º561	 i:7 	 global-step:11227	 l-p:0.1313006430864334
epoch£º561	 i:8 	 global-step:11228	 l-p:0.2342558056116104
epoch£º561	 i:9 	 global-step:11229	 l-p:0.1602979302406311
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2600, 5.2226, 5.2485],
        [5.2600, 5.2594, 5.2600],
        [5.2600, 5.2599, 5.2600],
        [5.2600, 5.2045, 5.0207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.13229285180568695 
model_pd.l_d.mean(): -19.9161319732666 
model_pd.lagr.mean(): -19.783838272094727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4124], device='cuda:0')), ('power', tensor([-20.5550], device='cuda:0'))])
epoch£º562	 i:0 	 global-step:11240	 l-p:0.13229285180568695
epoch£º562	 i:1 	 global-step:11241	 l-p:0.1253174990415573
epoch£º562	 i:2 	 global-step:11242	 l-p:0.1457560658454895
epoch£º562	 i:3 	 global-step:11243	 l-p:0.029197892174124718
epoch£º562	 i:4 	 global-step:11244	 l-p:0.11226784437894821
epoch£º562	 i:5 	 global-step:11245	 l-p:0.13432477414608002
epoch£º562	 i:6 	 global-step:11246	 l-p:0.1415673941373825
epoch£º562	 i:7 	 global-step:11247	 l-p:0.13838694989681244
epoch£º562	 i:8 	 global-step:11248	 l-p:0.050647225230932236
epoch£º562	 i:9 	 global-step:11249	 l-p:0.09193690866231918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0253, 5.0180, 5.0247],
        [5.0253, 4.9237, 4.9501],
        [5.0253, 4.9533, 4.9884],
        [5.0253, 4.9917, 5.0165]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): 0.06894833594560623 
model_pd.l_d.mean(): -20.59576988220215 
model_pd.lagr.mean(): -20.52682113647461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4687], device='cuda:0')), ('power', tensor([-21.2996], device='cuda:0'))])
epoch£º563	 i:0 	 global-step:11260	 l-p:0.06894833594560623
epoch£º563	 i:1 	 global-step:11261	 l-p:0.1493452787399292
epoch£º563	 i:2 	 global-step:11262	 l-p:0.1277390867471695
epoch£º563	 i:3 	 global-step:11263	 l-p:0.17980161309242249
epoch£º563	 i:4 	 global-step:11264	 l-p:0.09797372668981552
epoch£º563	 i:5 	 global-step:11265	 l-p:0.15686893463134766
epoch£º563	 i:6 	 global-step:11266	 l-p:0.14057588577270508
epoch£º563	 i:7 	 global-step:11267	 l-p:0.14923705160617828
epoch£º563	 i:8 	 global-step:11268	 l-p:0.11316154897212982
epoch£º563	 i:9 	 global-step:11269	 l-p:0.127546027302742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1715,  0.0953,  1.0000,  0.0530,
          1.0000,  0.5556, 31.6228]], device='cuda:0')
 pt:tensor([[5.0264, 4.9151, 4.7387],
        [5.0264, 5.0406, 4.8359],
        [5.0264, 4.8784, 4.7971],
        [5.0264, 4.9128, 4.9301]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.0658637285232544 
model_pd.l_d.mean(): -19.695594787597656 
model_pd.lagr.mean(): -19.629730224609375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4872], device='cuda:0')), ('power', tensor([-20.4085], device='cuda:0'))])
epoch£º564	 i:0 	 global-step:11280	 l-p:0.0658637285232544
epoch£º564	 i:1 	 global-step:11281	 l-p:0.21407049894332886
epoch£º564	 i:2 	 global-step:11282	 l-p:0.1340155452489853
epoch£º564	 i:3 	 global-step:11283	 l-p:0.13939513266086578
epoch£º564	 i:4 	 global-step:11284	 l-p:0.18427249789237976
epoch£º564	 i:5 	 global-step:11285	 l-p:0.10282067954540253
epoch£º564	 i:6 	 global-step:11286	 l-p:0.12862129509449005
epoch£º564	 i:7 	 global-step:11287	 l-p:0.12661857903003693
epoch£º564	 i:8 	 global-step:11288	 l-p:0.12491750717163086
epoch£º564	 i:9 	 global-step:11289	 l-p:0.11200974881649017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9471, 4.9420, 4.9468],
        [4.9471, 4.8425, 4.8707],
        [4.9471, 5.4581, 5.5305],
        [4.9471, 5.4633, 5.5397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.29207393527030945 
model_pd.l_d.mean(): -20.157838821411133 
model_pd.lagr.mean(): -19.865764617919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5331], device='cuda:0')), ('power', tensor([-20.9227], device='cuda:0'))])
epoch£º565	 i:0 	 global-step:11300	 l-p:0.29207393527030945
epoch£º565	 i:1 	 global-step:11301	 l-p:0.14680910110473633
epoch£º565	 i:2 	 global-step:11302	 l-p:0.19952760636806488
epoch£º565	 i:3 	 global-step:11303	 l-p:0.1446518748998642
epoch£º565	 i:4 	 global-step:11304	 l-p:0.1561054289340973
epoch£º565	 i:5 	 global-step:11305	 l-p:0.1589343547821045
epoch£º565	 i:6 	 global-step:11306	 l-p:0.14939866960048676
epoch£º565	 i:7 	 global-step:11307	 l-p:0.15419284999370575
epoch£º565	 i:8 	 global-step:11308	 l-p:0.05886445939540863
epoch£º565	 i:9 	 global-step:11309	 l-p:0.08763853460550308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0716, 5.0584, 5.0699],
        [5.0716, 5.0700, 5.0716],
        [5.0716, 4.9745, 5.0027],
        [5.0716, 5.0222, 5.0536]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.034325774759054184 
model_pd.l_d.mean(): -20.341196060180664 
model_pd.lagr.mean(): -20.306869506835938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4725], device='cuda:0')), ('power', tensor([-21.0462], device='cuda:0'))])
epoch£º566	 i:0 	 global-step:11320	 l-p:0.034325774759054184
epoch£º566	 i:1 	 global-step:11321	 l-p:0.11712350696325302
epoch£º566	 i:2 	 global-step:11322	 l-p:0.09382765740156174
epoch£º566	 i:3 	 global-step:11323	 l-p:0.15181156992912292
epoch£º566	 i:4 	 global-step:11324	 l-p:0.15459555387496948
epoch£º566	 i:5 	 global-step:11325	 l-p:0.11567667871713638
epoch£º566	 i:6 	 global-step:11326	 l-p:0.20527654886245728
epoch£º566	 i:7 	 global-step:11327	 l-p:0.06781301647424698
epoch£º566	 i:8 	 global-step:11328	 l-p:0.09859912842512131
epoch£º566	 i:9 	 global-step:11329	 l-p:0.1324574053287506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228]], device='cuda:0')
 pt:tensor([[5.1256, 5.7165, 5.8394],
        [5.1256, 5.6502, 5.7224],
        [5.1256, 5.0078, 5.0151],
        [5.1256, 5.4635, 5.4052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.20081637799739838 
model_pd.l_d.mean(): -18.53196144104004 
model_pd.lagr.mean(): -18.331144332885742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5660], device='cuda:0')), ('power', tensor([-19.3127], device='cuda:0'))])
epoch£º567	 i:0 	 global-step:11340	 l-p:0.20081637799739838
epoch£º567	 i:1 	 global-step:11341	 l-p:0.14460675418376923
epoch£º567	 i:2 	 global-step:11342	 l-p:-0.1837555319070816
epoch£º567	 i:3 	 global-step:11343	 l-p:0.1400507539510727
epoch£º567	 i:4 	 global-step:11344	 l-p:0.09004279971122742
epoch£º567	 i:5 	 global-step:11345	 l-p:0.13556919991970062
epoch£º567	 i:6 	 global-step:11346	 l-p:0.06051352992653847
epoch£º567	 i:7 	 global-step:11347	 l-p:0.1590283215045929
epoch£º567	 i:8 	 global-step:11348	 l-p:0.1425962746143341
epoch£º567	 i:9 	 global-step:11349	 l-p:0.09633470326662064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0513, 4.9124, 4.7773],
        [5.0513, 5.0420, 5.0503],
        [5.0513, 5.2209, 5.0680],
        [5.0513, 4.9201, 4.7685]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): 0.14414191246032715 
model_pd.l_d.mean(): -20.095256805419922 
model_pd.lagr.mean(): -19.951114654541016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5108], device='cuda:0')), ('power', tensor([-20.8366], device='cuda:0'))])
epoch£º568	 i:0 	 global-step:11360	 l-p:0.14414191246032715
epoch£º568	 i:1 	 global-step:11361	 l-p:0.17648950219154358
epoch£º568	 i:2 	 global-step:11362	 l-p:0.12277959287166595
epoch£º568	 i:3 	 global-step:11363	 l-p:0.12263133376836777
epoch£º568	 i:4 	 global-step:11364	 l-p:0.22360004484653473
epoch£º568	 i:5 	 global-step:11365	 l-p:0.13243773579597473
epoch£º568	 i:6 	 global-step:11366	 l-p:0.20167891681194305
epoch£º568	 i:7 	 global-step:11367	 l-p:0.13553082942962646
epoch£º568	 i:8 	 global-step:11368	 l-p:0.16727076470851898
epoch£º568	 i:9 	 global-step:11369	 l-p:0.11799547076225281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9153, 4.9152, 4.9153],
        [4.9153, 4.8818, 4.9069],
        [4.9153, 5.4100, 5.4707],
        [4.9153, 4.9153, 4.9153]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.13506357371807098 
model_pd.l_d.mean(): -19.60651397705078 
model_pd.lagr.mean(): -19.471450805664062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5467], device='cuda:0')), ('power', tensor([-20.3793], device='cuda:0'))])
epoch£º569	 i:0 	 global-step:11380	 l-p:0.13506357371807098
epoch£º569	 i:1 	 global-step:11381	 l-p:-0.009416112676262856
epoch£º569	 i:2 	 global-step:11382	 l-p:0.16580018401145935
epoch£º569	 i:3 	 global-step:11383	 l-p:0.1521371603012085
epoch£º569	 i:4 	 global-step:11384	 l-p:0.09315814822912216
epoch£º569	 i:5 	 global-step:11385	 l-p:0.15950815379619598
epoch£º569	 i:6 	 global-step:11386	 l-p:0.20502609014511108
epoch£º569	 i:7 	 global-step:11387	 l-p:-0.8633344173431396
epoch£º569	 i:8 	 global-step:11388	 l-p:0.24254535138607025
epoch£º569	 i:9 	 global-step:11389	 l-p:0.16439291834831238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9652, 4.9545, 4.9640],
        [4.9652, 4.9652, 4.9652],
        [4.9652, 4.8104, 4.6903],
        [4.9652, 5.2147, 5.1070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.16096900403499603 
model_pd.l_d.mean(): -20.12617301940918 
model_pd.lagr.mean(): -19.9652042388916 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5043], device='cuda:0')), ('power', tensor([-20.8613], device='cuda:0'))])
epoch£º570	 i:0 	 global-step:11400	 l-p:0.16096900403499603
epoch£º570	 i:1 	 global-step:11401	 l-p:0.15411429107189178
epoch£º570	 i:2 	 global-step:11402	 l-p:0.09164619445800781
epoch£º570	 i:3 	 global-step:11403	 l-p:0.050327736884355545
epoch£º570	 i:4 	 global-step:11404	 l-p:0.1330549567937851
epoch£º570	 i:5 	 global-step:11405	 l-p:0.15268056094646454
epoch£º570	 i:6 	 global-step:11406	 l-p:0.13459071516990662
epoch£º570	 i:7 	 global-step:11407	 l-p:0.1300591230392456
epoch£º570	 i:8 	 global-step:11408	 l-p:0.08720234036445618
epoch£º570	 i:9 	 global-step:11409	 l-p:0.14485082030296326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1142, 4.9708, 4.9210],
        [5.1142, 5.1142, 5.1142],
        [5.1142, 5.0840, 5.1070],
        [5.1142, 5.1089, 5.1138]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.1599947065114975 
model_pd.l_d.mean(): -19.360149383544922 
model_pd.lagr.mean(): -19.20015525817871 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4883], device='cuda:0')), ('power', tensor([-20.0706], device='cuda:0'))])
epoch£º571	 i:0 	 global-step:11420	 l-p:0.1599947065114975
epoch£º571	 i:1 	 global-step:11421	 l-p:-0.15834179520606995
epoch£º571	 i:2 	 global-step:11422	 l-p:0.11135886609554291
epoch£º571	 i:3 	 global-step:11423	 l-p:0.20934052765369415
epoch£º571	 i:4 	 global-step:11424	 l-p:0.0811956599354744
epoch£º571	 i:5 	 global-step:11425	 l-p:0.12091266363859177
epoch£º571	 i:6 	 global-step:11426	 l-p:0.14285577833652496
epoch£º571	 i:7 	 global-step:11427	 l-p:-0.3001290261745453
epoch£º571	 i:8 	 global-step:11428	 l-p:0.13508565723896027
epoch£º571	 i:9 	 global-step:11429	 l-p:0.11675485223531723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1662, 5.1652, 5.1661],
        [5.1662, 5.8161, 5.9811],
        [5.1662, 5.7173, 5.8054],
        [5.1662, 5.1537, 5.1646]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.12050925195217133 
model_pd.l_d.mean(): -19.55486488342285 
model_pd.lagr.mean(): -19.434354782104492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4969], device='cuda:0')), ('power', tensor([-20.2762], device='cuda:0'))])
epoch£º572	 i:0 	 global-step:11440	 l-p:0.12050925195217133
epoch£º572	 i:1 	 global-step:11441	 l-p:0.08617334812879562
epoch£º572	 i:2 	 global-step:11442	 l-p:0.7446632981300354
epoch£º572	 i:3 	 global-step:11443	 l-p:0.09766736626625061
epoch£º572	 i:4 	 global-step:11444	 l-p:0.12291780859231949
epoch£º572	 i:5 	 global-step:11445	 l-p:0.12430835515260696
epoch£º572	 i:6 	 global-step:11446	 l-p:0.1457284539937973
epoch£º572	 i:7 	 global-step:11447	 l-p:0.28834766149520874
epoch£º572	 i:8 	 global-step:11448	 l-p:0.09166652709245682
epoch£º572	 i:9 	 global-step:11449	 l-p:0.14979924261569977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0977, 5.0416, 5.0752],
        [5.0977, 4.9611, 4.9452],
        [5.0977, 5.0204, 5.0558],
        [5.0977, 5.0975, 5.0977]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): 0.14562185108661652 
model_pd.l_d.mean(): -19.595060348510742 
model_pd.lagr.mean(): -19.449438095092773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4649], device='cuda:0')), ('power', tensor([-20.2842], device='cuda:0'))])
epoch£º573	 i:0 	 global-step:11460	 l-p:0.14562185108661652
epoch£º573	 i:1 	 global-step:11461	 l-p:0.14428184926509857
epoch£º573	 i:2 	 global-step:11462	 l-p:0.0930200144648552
epoch£º573	 i:3 	 global-step:11463	 l-p:0.09998737275600433
epoch£º573	 i:4 	 global-step:11464	 l-p:0.11554048210382462
epoch£º573	 i:5 	 global-step:11465	 l-p:0.0991363525390625
epoch£º573	 i:6 	 global-step:11466	 l-p:0.142247274518013
epoch£º573	 i:7 	 global-step:11467	 l-p:0.134420245885849
epoch£º573	 i:8 	 global-step:11468	 l-p:0.2749815285205841
epoch£º573	 i:9 	 global-step:11469	 l-p:0.11634942889213562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9758, 4.8155, 4.7084],
        [4.9758, 5.0055, 4.7959],
        [4.9758, 4.9758, 4.9758],
        [4.9758, 4.9718, 4.9756]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.14887437224388123 
model_pd.l_d.mean(): -20.368772506713867 
model_pd.lagr.mean(): -20.219898223876953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4861], device='cuda:0')), ('power', tensor([-21.0879], device='cuda:0'))])
epoch£º574	 i:0 	 global-step:11480	 l-p:0.14887437224388123
epoch£º574	 i:1 	 global-step:11481	 l-p:0.1505502611398697
epoch£º574	 i:2 	 global-step:11482	 l-p:0.10843954980373383
epoch£º574	 i:3 	 global-step:11483	 l-p:0.301988810300827
epoch£º574	 i:4 	 global-step:11484	 l-p:0.3821960985660553
epoch£º574	 i:5 	 global-step:11485	 l-p:0.3207325041294098
epoch£º574	 i:6 	 global-step:11486	 l-p:0.07436350733041763
epoch£º574	 i:7 	 global-step:11487	 l-p:-0.06533222645521164
epoch£º574	 i:8 	 global-step:11488	 l-p:0.23124197125434875
epoch£º574	 i:9 	 global-step:11489	 l-p:0.12623339891433716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9465, 4.9341, 4.9450],
        [4.9465, 4.8044, 4.8005],
        [4.9465, 4.9465, 4.9465],
        [4.9465, 4.9324, 4.9447]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): 0.5179539918899536 
model_pd.l_d.mean(): -20.406349182128906 
model_pd.lagr.mean(): -19.888395309448242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5304], device='cuda:0')), ('power', tensor([-21.1712], device='cuda:0'))])
epoch£º575	 i:0 	 global-step:11500	 l-p:0.5179539918899536
epoch£º575	 i:1 	 global-step:11501	 l-p:0.1429542750120163
epoch£º575	 i:2 	 global-step:11502	 l-p:0.14712654054164886
epoch£º575	 i:3 	 global-step:11503	 l-p:0.140253946185112
epoch£º575	 i:4 	 global-step:11504	 l-p:0.11382647603750229
epoch£º575	 i:5 	 global-step:11505	 l-p:0.13185645639896393
epoch£º575	 i:6 	 global-step:11506	 l-p:0.13168774545192719
epoch£º575	 i:7 	 global-step:11507	 l-p:0.09445960819721222
epoch£º575	 i:8 	 global-step:11508	 l-p:0.11308781057596207
epoch£º575	 i:9 	 global-step:11509	 l-p:0.154171884059906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0945, 5.0942, 5.0945],
        [5.0945, 5.0910, 5.0944],
        [5.0945, 5.5792, 5.6203],
        [5.0945, 5.0232, 4.8173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): 0.11008168756961823 
model_pd.l_d.mean(): -20.317161560058594 
model_pd.lagr.mean(): -20.20707893371582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4758], device='cuda:0')), ('power', tensor([-21.0253], device='cuda:0'))])
epoch£º576	 i:0 	 global-step:11520	 l-p:0.11008168756961823
epoch£º576	 i:1 	 global-step:11521	 l-p:0.14178064465522766
epoch£º576	 i:2 	 global-step:11522	 l-p:0.13509619235992432
epoch£º576	 i:3 	 global-step:11523	 l-p:0.08885501325130463
epoch£º576	 i:4 	 global-step:11524	 l-p:0.12676137685775757
epoch£º576	 i:5 	 global-step:11525	 l-p:0.13544006645679474
epoch£º576	 i:6 	 global-step:11526	 l-p:0.1425812691450119
epoch£º576	 i:7 	 global-step:11527	 l-p:0.06060711294412613
epoch£º576	 i:8 	 global-step:11528	 l-p:0.10406308621168137
epoch£º576	 i:9 	 global-step:11529	 l-p:0.18095189332962036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0226, 4.9207, 4.9523],
        [5.0226, 5.0226, 5.0226],
        [5.0226, 5.0802, 4.8777],
        [5.0226, 5.0226, 5.0226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): 0.10459624230861664 
model_pd.l_d.mean(): -20.711793899536133 
model_pd.lagr.mean(): -20.607196807861328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4587], device='cuda:0')), ('power', tensor([-21.4068], device='cuda:0'))])
epoch£º577	 i:0 	 global-step:11540	 l-p:0.10459624230861664
epoch£º577	 i:1 	 global-step:11541	 l-p:0.1976485401391983
epoch£º577	 i:2 	 global-step:11542	 l-p:0.1682652235031128
epoch£º577	 i:3 	 global-step:11543	 l-p:0.12266083806753159
epoch£º577	 i:4 	 global-step:11544	 l-p:0.09990808367729187
epoch£º577	 i:5 	 global-step:11545	 l-p:0.14553648233413696
epoch£º577	 i:6 	 global-step:11546	 l-p:0.14089366793632507
epoch£º577	 i:7 	 global-step:11547	 l-p:0.13218675553798676
epoch£º577	 i:8 	 global-step:11548	 l-p:0.09574971348047256
epoch£º577	 i:9 	 global-step:11549	 l-p:-0.504914402961731
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9295, 4.9295, 4.9295],
        [4.9295, 4.9446, 4.7281],
        [4.9295, 4.7722, 4.7431],
        [4.9295, 4.7698, 4.6202]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.13625916838645935 
model_pd.l_d.mean(): -20.583934783935547 
model_pd.lagr.mean(): -20.447675704956055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4897], device='cuda:0')), ('power', tensor([-21.3091], device='cuda:0'))])
epoch£º578	 i:0 	 global-step:11560	 l-p:0.13625916838645935
epoch£º578	 i:1 	 global-step:11561	 l-p:0.1737704575061798
epoch£º578	 i:2 	 global-step:11562	 l-p:0.09815335273742676
epoch£º578	 i:3 	 global-step:11563	 l-p:0.11972293257713318
epoch£º578	 i:4 	 global-step:11564	 l-p:0.0009563016938045621
epoch£º578	 i:5 	 global-step:11565	 l-p:0.21922044456005096
epoch£º578	 i:6 	 global-step:11566	 l-p:0.2117055505514145
epoch£º578	 i:7 	 global-step:11567	 l-p:0.1493833363056183
epoch£º578	 i:8 	 global-step:11568	 l-p:0.26053184270858765
epoch£º578	 i:9 	 global-step:11569	 l-p:0.1620200127363205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0072, 5.5389, 5.6183],
        [5.0072, 4.9306, 4.9683],
        [5.0072, 4.8517, 4.7136],
        [5.0072, 4.9514, 4.9860]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.14809569716453552 
model_pd.l_d.mean(): -20.479568481445312 
model_pd.lagr.mean(): -20.331472396850586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4767], device='cuda:0')), ('power', tensor([-21.1904], device='cuda:0'))])
epoch£º579	 i:0 	 global-step:11580	 l-p:0.14809569716453552
epoch£º579	 i:1 	 global-step:11581	 l-p:0.06837455928325653
epoch£º579	 i:2 	 global-step:11582	 l-p:0.1474737524986267
epoch£º579	 i:3 	 global-step:11583	 l-p:0.10268308222293854
epoch£º579	 i:4 	 global-step:11584	 l-p:0.12049093842506409
epoch£º579	 i:5 	 global-step:11585	 l-p:0.15621665120124817
epoch£º579	 i:6 	 global-step:11586	 l-p:0.0916498675942421
epoch£º579	 i:7 	 global-step:11587	 l-p:0.11463272571563721
epoch£º579	 i:8 	 global-step:11588	 l-p:0.1539939045906067
epoch£º579	 i:9 	 global-step:11589	 l-p:0.06573918461799622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1004, 5.0798, 5.0968],
        [5.1004, 5.0946, 5.1000],
        [5.1004, 5.0345, 4.8237],
        [5.1004, 5.0742, 5.0949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.1115182638168335 
model_pd.l_d.mean(): -20.946016311645508 
model_pd.lagr.mean(): -20.834497451782227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3929], device='cuda:0')), ('power', tensor([-21.5763], device='cuda:0'))])
epoch£º580	 i:0 	 global-step:11600	 l-p:0.1115182638168335
epoch£º580	 i:1 	 global-step:11601	 l-p:0.1430528163909912
epoch£º580	 i:2 	 global-step:11602	 l-p:0.13096441328525543
epoch£º580	 i:3 	 global-step:11603	 l-p:0.11635804921388626
epoch£º580	 i:4 	 global-step:11604	 l-p:0.0824478343129158
epoch£º580	 i:5 	 global-step:11605	 l-p:0.20629408955574036
epoch£º580	 i:6 	 global-step:11606	 l-p:0.19891506433486938
epoch£º580	 i:7 	 global-step:11607	 l-p:0.0814882442355156
epoch£º580	 i:8 	 global-step:11608	 l-p:0.14702561497688293
epoch£º580	 i:9 	 global-step:11609	 l-p:0.12393876910209656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0455, 5.0455, 5.0455],
        [5.0455, 5.2325, 5.0832],
        [5.0455, 4.8897, 4.7613],
        [5.0455, 4.9485, 4.9825]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.17445914447307587 
model_pd.l_d.mean(): -20.292142868041992 
model_pd.lagr.mean(): -20.11768341064453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4988], device='cuda:0')), ('power', tensor([-21.0235], device='cuda:0'))])
epoch£º581	 i:0 	 global-step:11620	 l-p:0.17445914447307587
epoch£º581	 i:1 	 global-step:11621	 l-p:0.10475577414035797
epoch£º581	 i:2 	 global-step:11622	 l-p:0.12474486976861954
epoch£º581	 i:3 	 global-step:11623	 l-p:0.13204798102378845
epoch£º581	 i:4 	 global-step:11624	 l-p:0.11413418501615524
epoch£º581	 i:5 	 global-step:11625	 l-p:0.0486685112118721
epoch£º581	 i:6 	 global-step:11626	 l-p:0.08282594382762909
epoch£º581	 i:7 	 global-step:11627	 l-p:0.15065035223960876
epoch£º581	 i:8 	 global-step:11628	 l-p:0.1290336549282074
epoch£º581	 i:9 	 global-step:11629	 l-p:0.1345575600862503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1214, 5.0712, 5.1036],
        [5.1214, 5.6467, 5.7144],
        [5.1214, 5.1210, 5.1214],
        [5.1214, 5.1106, 5.1202]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.13745972514152527 
model_pd.l_d.mean(): -18.503089904785156 
model_pd.lagr.mean(): -18.365631103515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5479], device='cuda:0')), ('power', tensor([-19.2650], device='cuda:0'))])
epoch£º582	 i:0 	 global-step:11640	 l-p:0.13745972514152527
epoch£º582	 i:1 	 global-step:11641	 l-p:0.12052683532238007
epoch£º582	 i:2 	 global-step:11642	 l-p:0.11741594225168228
epoch£º582	 i:3 	 global-step:11643	 l-p:0.12899719178676605
epoch£º582	 i:4 	 global-step:11644	 l-p:0.12841926515102386
epoch£º582	 i:5 	 global-step:11645	 l-p:0.1789681613445282
epoch£º582	 i:6 	 global-step:11646	 l-p:0.8694785237312317
epoch£º582	 i:7 	 global-step:11647	 l-p:0.09826070815324783
epoch£º582	 i:8 	 global-step:11648	 l-p:-0.02009815163910389
epoch£º582	 i:9 	 global-step:11649	 l-p:0.13123592734336853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1865, 5.1530, 5.1779],
        [5.1865, 5.3161, 5.1379],
        [5.1865, 5.3389, 5.1702],
        [5.1865, 5.0589, 4.8978]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.12513945996761322 
model_pd.l_d.mean(): -20.409738540649414 
model_pd.lagr.mean(): -20.28459930419922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4513], device='cuda:0')), ('power', tensor([-21.0938], device='cuda:0'))])
epoch£º583	 i:0 	 global-step:11660	 l-p:0.12513945996761322
epoch£º583	 i:1 	 global-step:11661	 l-p:0.12738700211048126
epoch£º583	 i:2 	 global-step:11662	 l-p:0.2585499584674835
epoch£º583	 i:3 	 global-step:11663	 l-p:-0.06369023025035858
epoch£º583	 i:4 	 global-step:11664	 l-p:0.10187000036239624
epoch£º583	 i:5 	 global-step:11665	 l-p:0.13560466468334198
epoch£º583	 i:6 	 global-step:11666	 l-p:0.14157263934612274
epoch£º583	 i:7 	 global-step:11667	 l-p:0.07514267414808273
epoch£º583	 i:8 	 global-step:11668	 l-p:0.06043628603219986
epoch£º583	 i:9 	 global-step:11669	 l-p:0.1254720836877823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2675, 5.2474, 5.2640],
        [5.2675, 5.2675, 5.2675],
        [5.2675, 5.2151, 5.2475],
        [5.2675, 5.2051, 5.2395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): 0.13569480180740356 
model_pd.l_d.mean(): -20.602811813354492 
model_pd.lagr.mean(): -20.467117309570312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3940], device='cuda:0')), ('power', tensor([-21.2304], device='cuda:0'))])
epoch£º584	 i:0 	 global-step:11680	 l-p:0.13569480180740356
epoch£º584	 i:1 	 global-step:11681	 l-p:-2.2533209323883057
epoch£º584	 i:2 	 global-step:11682	 l-p:0.1723681539297104
epoch£º584	 i:3 	 global-step:11683	 l-p:0.2876218855381012
epoch£º584	 i:4 	 global-step:11684	 l-p:0.10406366735696793
epoch£º584	 i:5 	 global-step:11685	 l-p:0.14276406168937683
epoch£º584	 i:6 	 global-step:11686	 l-p:0.12231980264186859
epoch£º584	 i:7 	 global-step:11687	 l-p:0.11660506576299667
epoch£º584	 i:8 	 global-step:11688	 l-p:0.16056136786937714
epoch£º584	 i:9 	 global-step:11689	 l-p:0.10158791393041611
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3267, 5.2564, 5.2906],
        [5.3267, 5.3196, 5.3261],
        [5.3267, 5.3252, 5.3266],
        [5.3267, 5.3174, 5.3257]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.1398734301328659 
model_pd.l_d.mean(): -20.38580322265625 
model_pd.lagr.mean(): -20.245929718017578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3784], device='cuda:0')), ('power', tensor([-20.9952], device='cuda:0'))])
epoch£º585	 i:0 	 global-step:11700	 l-p:0.1398734301328659
epoch£º585	 i:1 	 global-step:11701	 l-p:0.11495546251535416
epoch£º585	 i:2 	 global-step:11702	 l-p:0.16995792090892792
epoch£º585	 i:3 	 global-step:11703	 l-p:1.4709380865097046
epoch£º585	 i:4 	 global-step:11704	 l-p:0.11331840604543686
epoch£º585	 i:5 	 global-step:11705	 l-p:0.12367774546146393
epoch£º585	 i:6 	 global-step:11706	 l-p:0.14402621984481812
epoch£º585	 i:7 	 global-step:11707	 l-p:0.11856786161661148
epoch£º585	 i:8 	 global-step:11708	 l-p:0.14370109140872955
epoch£º585	 i:9 	 global-step:11709	 l-p:0.10219910740852356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0686, 5.0679, 5.0686],
        [5.0686, 5.5681, 5.6182],
        [5.0686, 4.9343, 4.9396],
        [5.0686, 5.0363, 5.0608]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.15991172194480896 
model_pd.l_d.mean(): -20.250810623168945 
model_pd.lagr.mean(): -20.090898513793945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4786], device='cuda:0')), ('power', tensor([-20.9610], device='cuda:0'))])
epoch£º586	 i:0 	 global-step:11720	 l-p:0.15991172194480896
epoch£º586	 i:1 	 global-step:11721	 l-p:0.18347232043743134
epoch£º586	 i:2 	 global-step:11722	 l-p:0.11506324261426926
epoch£º586	 i:3 	 global-step:11723	 l-p:0.12245287001132965
epoch£º586	 i:4 	 global-step:11724	 l-p:0.11271832138299942
epoch£º586	 i:5 	 global-step:11725	 l-p:0.12550200521945953
epoch£º586	 i:6 	 global-step:11726	 l-p:0.12718604505062103
epoch£º586	 i:7 	 global-step:11727	 l-p:0.2993465065956116
epoch£º586	 i:8 	 global-step:11728	 l-p:0.15374451875686646
epoch£º586	 i:9 	 global-step:11729	 l-p:0.24770113825798035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9100, 4.7354, 4.6046],
        [4.9100, 4.9390, 4.7216],
        [4.9100, 4.8824, 4.6532],
        [4.9100, 4.8532, 4.8892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.1469407081604004 
model_pd.l_d.mean(): -20.23297691345215 
model_pd.lagr.mean(): -20.086036682128906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5460], device='cuda:0')), ('power', tensor([-21.0119], device='cuda:0'))])
epoch£º587	 i:0 	 global-step:11740	 l-p:0.1469407081604004
epoch£º587	 i:1 	 global-step:11741	 l-p:0.10019336640834808
epoch£º587	 i:2 	 global-step:11742	 l-p:0.08896079659461975
epoch£º587	 i:3 	 global-step:11743	 l-p:0.1447363942861557
epoch£º587	 i:4 	 global-step:11744	 l-p:0.1447337567806244
epoch£º587	 i:5 	 global-step:11745	 l-p:-0.038834333419799805
epoch£º587	 i:6 	 global-step:11746	 l-p:0.2691875696182251
epoch£º587	 i:7 	 global-step:11747	 l-p:-0.06522122770547867
epoch£º587	 i:8 	 global-step:11748	 l-p:0.13807904720306396
epoch£º587	 i:9 	 global-step:11749	 l-p:0.04685036465525627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8628, 4.8628, 4.8628],
        [4.8628, 4.7734, 4.8141],
        [4.8628, 4.6858, 4.6279],
        [4.8628, 4.7862, 4.8267]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.16121713817119598 
model_pd.l_d.mean(): -20.524211883544922 
model_pd.lagr.mean(): -20.362995147705078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5288], device='cuda:0')), ('power', tensor([-21.2887], device='cuda:0'))])
epoch£º588	 i:0 	 global-step:11760	 l-p:0.16121713817119598
epoch£º588	 i:1 	 global-step:11761	 l-p:0.1289609968662262
epoch£º588	 i:2 	 global-step:11762	 l-p:0.125295490026474
epoch£º588	 i:3 	 global-step:11763	 l-p:0.12570104002952576
epoch£º588	 i:4 	 global-step:11764	 l-p:0.06980736553668976
epoch£º588	 i:5 	 global-step:11765	 l-p:0.12449691444635391
epoch£º588	 i:6 	 global-step:11766	 l-p:0.6384767889976501
epoch£º588	 i:7 	 global-step:11767	 l-p:0.405842661857605
epoch£º588	 i:8 	 global-step:11768	 l-p:0.14655514061450958
epoch£º588	 i:9 	 global-step:11769	 l-p:0.14138345420360565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9381, 4.9233, 4.9362],
        [4.9381, 4.9373, 4.9381],
        [4.9381, 4.8940, 4.9250],
        [4.9381, 4.9381, 4.9381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 0.22139720618724823 
model_pd.l_d.mean(): -20.249277114868164 
model_pd.lagr.mean(): -20.02787971496582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5378], device='cuda:0')), ('power', tensor([-21.0200], device='cuda:0'))])
epoch£º589	 i:0 	 global-step:11780	 l-p:0.22139720618724823
epoch£º589	 i:1 	 global-step:11781	 l-p:0.12846983969211578
epoch£º589	 i:2 	 global-step:11782	 l-p:0.14844262599945068
epoch£º589	 i:3 	 global-step:11783	 l-p:0.08008115738630295
epoch£º589	 i:4 	 global-step:11784	 l-p:0.17319545149803162
epoch£º589	 i:5 	 global-step:11785	 l-p:0.16310366988182068
epoch£º589	 i:6 	 global-step:11786	 l-p:0.13736166059970856
epoch£º589	 i:7 	 global-step:11787	 l-p:0.13251090049743652
epoch£º589	 i:8 	 global-step:11788	 l-p:0.13408927619457245
epoch£º589	 i:9 	 global-step:11789	 l-p:0.09705058485269547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1521, 5.1186, 5.1437],
        [5.1521, 5.1311, 4.9130],
        [5.1521, 5.0969, 5.1311],
        [5.1521, 5.1424, 5.1511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.14638200402259827 
model_pd.l_d.mean(): -20.330429077148438 
model_pd.lagr.mean(): -20.18404769897461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4589], device='cuda:0')), ('power', tensor([-21.0214], device='cuda:0'))])
epoch£º590	 i:0 	 global-step:11800	 l-p:0.14638200402259827
epoch£º590	 i:1 	 global-step:11801	 l-p:0.0554639957845211
epoch£º590	 i:2 	 global-step:11802	 l-p:0.11992432922124863
epoch£º590	 i:3 	 global-step:11803	 l-p:0.12925872206687927
epoch£º590	 i:4 	 global-step:11804	 l-p:0.13139939308166504
epoch£º590	 i:5 	 global-step:11805	 l-p:0.13039536774158478
epoch£º590	 i:6 	 global-step:11806	 l-p:0.12503747642040253
epoch£º590	 i:7 	 global-step:11807	 l-p:0.048329491168260574
epoch£º590	 i:8 	 global-step:11808	 l-p:-0.08789076656103134
epoch£º590	 i:9 	 global-step:11809	 l-p:0.1367940604686737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0689, 5.1454, 4.9437],
        [5.0689, 4.9079, 4.7796],
        [5.0689, 5.0688, 5.0689],
        [5.0689, 5.0685, 5.0689]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.12409330904483795 
model_pd.l_d.mean(): -18.83990478515625 
model_pd.lagr.mean(): -18.715810775756836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5286], device='cuda:0')), ('power', tensor([-19.5858], device='cuda:0'))])
epoch£º591	 i:0 	 global-step:11820	 l-p:0.12409330904483795
epoch£º591	 i:1 	 global-step:11821	 l-p:0.172840878367424
epoch£º591	 i:2 	 global-step:11822	 l-p:0.12084317207336426
epoch£º591	 i:3 	 global-step:11823	 l-p:0.1277758628129959
epoch£º591	 i:4 	 global-step:11824	 l-p:0.08171205967664719
epoch£º591	 i:5 	 global-step:11825	 l-p:0.1200922355055809
epoch£º591	 i:6 	 global-step:11826	 l-p:0.1063685268163681
epoch£º591	 i:7 	 global-step:11827	 l-p:0.1898004710674286
epoch£º591	 i:8 	 global-step:11828	 l-p:0.09873604774475098
epoch£º591	 i:9 	 global-step:11829	 l-p:0.13569898903369904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9731,  0.9643,  1.0000,  0.9556,
          1.0000,  0.9910, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228]], device='cuda:0')
 pt:tensor([[5.0759, 4.9195, 4.8858],
        [5.0759, 4.9194, 4.8855],
        [5.0759, 5.6618, 5.7749],
        [5.0759, 4.9130, 4.8521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.1289244294166565 
model_pd.l_d.mean(): -20.170881271362305 
model_pd.lagr.mean(): -20.041955947875977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4566], device='cuda:0')), ('power', tensor([-20.8577], device='cuda:0'))])
epoch£º592	 i:0 	 global-step:11840	 l-p:0.1289244294166565
epoch£º592	 i:1 	 global-step:11841	 l-p:0.10319279134273529
epoch£º592	 i:2 	 global-step:11842	 l-p:0.05020598694682121
epoch£º592	 i:3 	 global-step:11843	 l-p:0.15188710391521454
epoch£º592	 i:4 	 global-step:11844	 l-p:0.06617245078086853
epoch£º592	 i:5 	 global-step:11845	 l-p:0.12883377075195312
epoch£º592	 i:6 	 global-step:11846	 l-p:0.14052714407444
epoch£º592	 i:7 	 global-step:11847	 l-p:0.13119882345199585
epoch£º592	 i:8 	 global-step:11848	 l-p:0.17149299383163452
epoch£º592	 i:9 	 global-step:11849	 l-p:0.11983475089073181
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0957, 5.0254, 5.0633],
        [5.0957, 4.9882, 5.0195],
        [5.0957, 5.0956, 5.0957],
        [5.0957, 5.0957, 5.0957]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): -0.10345099866390228 
model_pd.l_d.mean(): -20.76696014404297 
model_pd.lagr.mean(): -20.870410919189453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4256], device='cuda:0')), ('power', tensor([-21.4287], device='cuda:0'))])
epoch£º593	 i:0 	 global-step:11860	 l-p:-0.10345099866390228
epoch£º593	 i:1 	 global-step:11861	 l-p:0.10466402024030685
epoch£º593	 i:2 	 global-step:11862	 l-p:0.10011903196573257
epoch£º593	 i:3 	 global-step:11863	 l-p:0.17068196833133698
epoch£º593	 i:4 	 global-step:11864	 l-p:0.1362968534231186
epoch£º593	 i:5 	 global-step:11865	 l-p:0.10705089569091797
epoch£º593	 i:6 	 global-step:11866	 l-p:0.12237242609262466
epoch£º593	 i:7 	 global-step:11867	 l-p:0.1588241308927536
epoch£º593	 i:8 	 global-step:11868	 l-p:0.1468805968761444
epoch£º593	 i:9 	 global-step:11869	 l-p:0.0980505645275116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0714, 4.9705, 5.0056],
        [5.0714, 5.0714, 5.0714],
        [5.0714, 5.0106, 5.0471],
        [5.0714, 5.3194, 5.1991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.10362246632575989 
model_pd.l_d.mean(): -20.33138656616211 
model_pd.lagr.mean(): -20.227764129638672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5025], device='cuda:0')), ('power', tensor([-21.0669], device='cuda:0'))])
epoch£º594	 i:0 	 global-step:11880	 l-p:0.10362246632575989
epoch£º594	 i:1 	 global-step:11881	 l-p:0.13060946762561798
epoch£º594	 i:2 	 global-step:11882	 l-p:0.11601778119802475
epoch£º594	 i:3 	 global-step:11883	 l-p:0.1331968903541565
epoch£º594	 i:4 	 global-step:11884	 l-p:0.13083268702030182
epoch£º594	 i:5 	 global-step:11885	 l-p:0.06013736501336098
epoch£º594	 i:6 	 global-step:11886	 l-p:0.14002026617527008
epoch£º594	 i:7 	 global-step:11887	 l-p:0.19642533361911774
epoch£º594	 i:8 	 global-step:11888	 l-p:0.12790988385677338
epoch£º594	 i:9 	 global-step:11889	 l-p:0.14459918439388275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0287, 5.2673, 5.1424],
        [5.0287, 4.8669, 4.8339],
        [5.0287, 5.0286, 5.0287],
        [5.0287, 5.0270, 5.0286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.1530895084142685 
model_pd.l_d.mean(): -20.632244110107422 
model_pd.lagr.mean(): -20.479154586791992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4578], device='cuda:0')), ('power', tensor([-21.3254], device='cuda:0'))])
epoch£º595	 i:0 	 global-step:11900	 l-p:0.1530895084142685
epoch£º595	 i:1 	 global-step:11901	 l-p:0.14460130035877228
epoch£º595	 i:2 	 global-step:11902	 l-p:0.17469508945941925
epoch£º595	 i:3 	 global-step:11903	 l-p:0.27006787061691284
epoch£º595	 i:4 	 global-step:11904	 l-p:0.1276846081018448
epoch£º595	 i:5 	 global-step:11905	 l-p:0.1297696828842163
epoch£º595	 i:6 	 global-step:11906	 l-p:0.12694279849529266
epoch£º595	 i:7 	 global-step:11907	 l-p:0.11873991042375565
epoch£º595	 i:8 	 global-step:11908	 l-p:0.12055737525224686
epoch£º595	 i:9 	 global-step:11909	 l-p:0.11751839518547058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9967, 4.9592, 4.9870],
        [4.9967, 4.9959, 4.9967],
        [4.9967, 4.8874, 4.6671],
        [4.9967, 4.9967, 4.9967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): 0.11281339079141617 
model_pd.l_d.mean(): -20.572383880615234 
model_pd.lagr.mean(): -20.459569931030273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4676], device='cuda:0')), ('power', tensor([-21.2749], device='cuda:0'))])
epoch£º596	 i:0 	 global-step:11920	 l-p:0.11281339079141617
epoch£º596	 i:1 	 global-step:11921	 l-p:0.14391063153743744
epoch£º596	 i:2 	 global-step:11922	 l-p:0.1516256034374237
epoch£º596	 i:3 	 global-step:11923	 l-p:0.29637283086776733
epoch£º596	 i:4 	 global-step:11924	 l-p:0.11957328766584396
epoch£º596	 i:5 	 global-step:11925	 l-p:0.304201602935791
epoch£º596	 i:6 	 global-step:11926	 l-p:0.2838170528411865
epoch£º596	 i:7 	 global-step:11927	 l-p:0.24477341771125793
epoch£º596	 i:8 	 global-step:11928	 l-p:-0.06744997948408127
epoch£º596	 i:9 	 global-step:11929	 l-p:0.13011512160301208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9723, 4.8194, 4.8167],
        [4.9723, 4.9473, 4.9676],
        [4.9723, 4.9722, 4.9723],
        [4.9723, 4.9706, 4.9722]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): 0.1418711096048355 
model_pd.l_d.mean(): -20.473846435546875 
model_pd.lagr.mean(): -20.33197593688965 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4844], device='cuda:0')), ('power', tensor([-21.1925], device='cuda:0'))])
epoch£º597	 i:0 	 global-step:11940	 l-p:0.1418711096048355
epoch£º597	 i:1 	 global-step:11941	 l-p:0.14129485189914703
epoch£º597	 i:2 	 global-step:11942	 l-p:0.12970256805419922
epoch£º597	 i:3 	 global-step:11943	 l-p:-0.16875597834587097
epoch£º597	 i:4 	 global-step:11944	 l-p:0.20292018353939056
epoch£º597	 i:5 	 global-step:11945	 l-p:0.5236340761184692
epoch£º597	 i:6 	 global-step:11946	 l-p:0.08841672539710999
epoch£º597	 i:7 	 global-step:11947	 l-p:0.17738498747348785
epoch£º597	 i:8 	 global-step:11948	 l-p:0.13357719779014587
epoch£º597	 i:9 	 global-step:11949	 l-p:0.14993040263652802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0201, 5.0193, 5.0200],
        [5.0201, 5.1369, 4.9476],
        [5.0201, 4.8890, 4.9095],
        [5.0201, 5.4357, 5.4238]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.11611875146627426 
model_pd.l_d.mean(): -19.299161911010742 
model_pd.lagr.mean(): -19.183042526245117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5412], device='cuda:0')), ('power', tensor([-20.0630], device='cuda:0'))])
epoch£º598	 i:0 	 global-step:11960	 l-p:0.11611875146627426
epoch£º598	 i:1 	 global-step:11961	 l-p:0.12876823544502258
epoch£º598	 i:2 	 global-step:11962	 l-p:0.11149515956640244
epoch£º598	 i:3 	 global-step:11963	 l-p:0.13233819603919983
epoch£º598	 i:4 	 global-step:11964	 l-p:0.12298592925071716
epoch£º598	 i:5 	 global-step:11965	 l-p:0.13496486842632294
epoch£º598	 i:6 	 global-step:11966	 l-p:0.15098591148853302
epoch£º598	 i:7 	 global-step:11967	 l-p:0.13944444060325623
epoch£º598	 i:8 	 global-step:11968	 l-p:0.15451373159885406
epoch£º598	 i:9 	 global-step:11969	 l-p:0.13053779304027557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1099, 5.1099, 5.1099],
        [5.1099, 4.9763, 4.9895],
        [5.1099, 5.3657, 5.2467],
        [5.1099, 5.1062, 5.1097]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): 0.13197509944438934 
model_pd.l_d.mean(): -19.834897994995117 
model_pd.lagr.mean(): -19.702922821044922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5416], device='cuda:0')), ('power', tensor([-20.6050], device='cuda:0'))])
epoch£º599	 i:0 	 global-step:11980	 l-p:0.13197509944438934
epoch£º599	 i:1 	 global-step:11981	 l-p:0.15235741436481476
epoch£º599	 i:2 	 global-step:11982	 l-p:0.12023452669382095
epoch£º599	 i:3 	 global-step:11983	 l-p:0.14942869544029236
epoch£º599	 i:4 	 global-step:11984	 l-p:0.06770997494459152
epoch£º599	 i:5 	 global-step:11985	 l-p:0.06040549650788307
epoch£º599	 i:6 	 global-step:11986	 l-p:0.1316172331571579
epoch£º599	 i:7 	 global-step:11987	 l-p:0.129303440451622
epoch£º599	 i:8 	 global-step:11988	 l-p:0.49727949500083923
epoch£º599	 i:9 	 global-step:11989	 l-p:0.11921896040439606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1305, 5.2014, 4.9934],
        [5.1305, 5.1305, 5.1305],
        [5.1305, 5.1305, 5.1305],
        [5.1305, 5.1305, 5.1305]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.12728959321975708 
model_pd.l_d.mean(): -19.623497009277344 
model_pd.lagr.mean(): -19.49620819091797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4565], device='cuda:0')), ('power', tensor([-20.3043], device='cuda:0'))])
epoch£º600	 i:0 	 global-step:12000	 l-p:0.12728959321975708
epoch£º600	 i:1 	 global-step:12001	 l-p:0.1290295571088791
epoch£º600	 i:2 	 global-step:12002	 l-p:0.13398312032222748
epoch£º600	 i:3 	 global-step:12003	 l-p:0.01961885578930378
epoch£º600	 i:4 	 global-step:12004	 l-p:-0.2857791781425476
epoch£º600	 i:5 	 global-step:12005	 l-p:0.1092904806137085
epoch£º600	 i:6 	 global-step:12006	 l-p:0.18409690260887146
epoch£º600	 i:7 	 global-step:12007	 l-p:0.10709633678197861
epoch£º600	 i:8 	 global-step:12008	 l-p:0.12255097925662994
epoch£º600	 i:9 	 global-step:12009	 l-p:0.12574952840805054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0967, 5.0723, 4.8428],
        [5.0967, 4.9317, 4.7886],
        [5.0967, 5.0766, 5.0934],
        [5.0967, 5.0961, 5.0967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): -0.012078714556992054 
model_pd.l_d.mean(): -20.63680076599121 
model_pd.lagr.mean(): -20.648880004882812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4226], device='cuda:0')), ('power', tensor([-21.2941], device='cuda:0'))])
epoch£º601	 i:0 	 global-step:12020	 l-p:-0.012078714556992054
epoch£º601	 i:1 	 global-step:12021	 l-p:0.12361320853233337
epoch£º601	 i:2 	 global-step:12022	 l-p:0.16039779782295227
epoch£º601	 i:3 	 global-step:12023	 l-p:0.14484602212905884
epoch£º601	 i:4 	 global-step:12024	 l-p:0.12858682870864868
epoch£º601	 i:5 	 global-step:12025	 l-p:0.09676922857761383
epoch£º601	 i:6 	 global-step:12026	 l-p:0.18164889514446259
epoch£º601	 i:7 	 global-step:12027	 l-p:0.10506743937730789
epoch£º601	 i:8 	 global-step:12028	 l-p:0.09839235246181488
epoch£º601	 i:9 	 global-step:12029	 l-p:0.12687821686267853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0566, 5.0566, 5.0566],
        [5.0566, 5.0895, 4.8683],
        [5.0566, 5.0325, 5.0522],
        [5.0566, 5.0456, 4.8154]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): 0.13041430711746216 
model_pd.l_d.mean(): -20.40760040283203 
model_pd.lagr.mean(): -20.277185440063477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4786], device='cuda:0')), ('power', tensor([-21.1195], device='cuda:0'))])
epoch£º602	 i:0 	 global-step:12040	 l-p:0.13041430711746216
epoch£º602	 i:1 	 global-step:12041	 l-p:0.06715703010559082
epoch£º602	 i:2 	 global-step:12042	 l-p:0.11817397177219391
epoch£º602	 i:3 	 global-step:12043	 l-p:0.11221103370189667
epoch£º602	 i:4 	 global-step:12044	 l-p:0.11016390472650528
epoch£º602	 i:5 	 global-step:12045	 l-p:0.19942910969257355
epoch£º602	 i:6 	 global-step:12046	 l-p:0.12754382193088531
epoch£º602	 i:7 	 global-step:12047	 l-p:0.0967864990234375
epoch£º602	 i:8 	 global-step:12048	 l-p:0.15081797540187836
epoch£º602	 i:9 	 global-step:12049	 l-p:0.16787388920783997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0768, 4.9574, 4.9862],
        [5.0768, 5.0658, 5.0756],
        [5.0768, 5.0767, 5.0768],
        [5.0768, 4.9650, 4.9981]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.10289108008146286 
model_pd.l_d.mean(): -20.76947021484375 
model_pd.lagr.mean(): -20.66657829284668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4208], device='cuda:0')), ('power', tensor([-21.4264], device='cuda:0'))])
epoch£º603	 i:0 	 global-step:12060	 l-p:0.10289108008146286
epoch£º603	 i:1 	 global-step:12061	 l-p:0.1764216423034668
epoch£º603	 i:2 	 global-step:12062	 l-p:0.13496017456054688
epoch£º603	 i:3 	 global-step:12063	 l-p:0.1285666823387146
epoch£º603	 i:4 	 global-step:12064	 l-p:0.09794126451015472
epoch£º603	 i:5 	 global-step:12065	 l-p:0.13135555386543274
epoch£º603	 i:6 	 global-step:12066	 l-p:0.14085757732391357
epoch£º603	 i:7 	 global-step:12067	 l-p:-0.14238400757312775
epoch£º603	 i:8 	 global-step:12068	 l-p:0.11475060880184174
epoch£º603	 i:9 	 global-step:12069	 l-p:0.12172881513834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1255, 5.4166, 5.3170],
        [5.1255, 5.1255, 5.1255],
        [5.1255, 5.0613, 5.0990],
        [5.1255, 4.9605, 4.9101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): 0.774542510509491 
model_pd.l_d.mean(): -20.530954360961914 
model_pd.lagr.mean(): -19.756412506103516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4394], device='cuda:0')), ('power', tensor([-21.2042], device='cuda:0'))])
epoch£º604	 i:0 	 global-step:12080	 l-p:0.774542510509491
epoch£º604	 i:1 	 global-step:12081	 l-p:0.15349791944026947
epoch£º604	 i:2 	 global-step:12082	 l-p:0.10860391706228256
epoch£º604	 i:3 	 global-step:12083	 l-p:0.08250247687101364
epoch£º604	 i:4 	 global-step:12084	 l-p:0.12492551654577255
epoch£º604	 i:5 	 global-step:12085	 l-p:0.11180004477500916
epoch£º604	 i:6 	 global-step:12086	 l-p:0.7035476565361023
epoch£º604	 i:7 	 global-step:12087	 l-p:0.10716307163238525
epoch£º604	 i:8 	 global-step:12088	 l-p:0.139320969581604
epoch£º604	 i:9 	 global-step:12089	 l-p:0.1191011294722557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1845, 5.1599, 4.9337],
        [5.1845, 5.0805, 5.1140],
        [5.1845, 5.1311, 5.1654],
        [5.1845, 5.1409, 5.1714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.10957466065883636 
model_pd.l_d.mean(): -20.42348289489746 
model_pd.lagr.mean(): -20.313907623291016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4271], device='cuda:0')), ('power', tensor([-21.0830], device='cuda:0'))])
epoch£º605	 i:0 	 global-step:12100	 l-p:0.10957466065883636
epoch£º605	 i:1 	 global-step:12101	 l-p:0.11360646039247513
epoch£º605	 i:2 	 global-step:12102	 l-p:0.13542474806308746
epoch£º605	 i:3 	 global-step:12103	 l-p:0.12625454366207123
epoch£º605	 i:4 	 global-step:12104	 l-p:0.04485359042882919
epoch£º605	 i:5 	 global-step:12105	 l-p:0.14068134129047394
epoch£º605	 i:6 	 global-step:12106	 l-p:0.11165262758731842
epoch£º605	 i:7 	 global-step:12107	 l-p:0.27543583512306213
epoch£º605	 i:8 	 global-step:12108	 l-p:0.12469026446342468
epoch£º605	 i:9 	 global-step:12109	 l-p:-0.030302753672003746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1323, 5.1323, 5.1323],
        [5.1323, 5.3950, 5.2779],
        [5.1323, 4.9674, 4.9177],
        [5.1323, 5.1320, 5.1323]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.10154398530721664 
model_pd.l_d.mean(): -20.687326431274414 
model_pd.lagr.mean(): -20.585783004760742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4199], device='cuda:0')), ('power', tensor([-21.3423], device='cuda:0'))])
epoch£º606	 i:0 	 global-step:12120	 l-p:0.10154398530721664
epoch£º606	 i:1 	 global-step:12121	 l-p:0.1354537010192871
epoch£º606	 i:2 	 global-step:12122	 l-p:0.12762440741062164
epoch£º606	 i:3 	 global-step:12123	 l-p:0.08861599117517471
epoch£º606	 i:4 	 global-step:12124	 l-p:0.13766951858997345
epoch£º606	 i:5 	 global-step:12125	 l-p:0.0605248399078846
epoch£º606	 i:6 	 global-step:12126	 l-p:0.14226655662059784
epoch£º606	 i:7 	 global-step:12127	 l-p:0.14361949265003204
epoch£º606	 i:8 	 global-step:12128	 l-p:0.13390231132507324
epoch£º606	 i:9 	 global-step:12129	 l-p:0.07289273291826248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0654, 5.0561, 5.0646],
        [5.0654, 4.8907, 4.8171],
        [5.0654, 5.0641, 5.0654],
        [5.0654, 4.9003, 4.8675]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.14672817289829254 
model_pd.l_d.mean(): -20.48204803466797 
model_pd.lagr.mean(): -20.33531951904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4683], device='cuda:0')), ('power', tensor([-21.1843], device='cuda:0'))])
epoch£º607	 i:0 	 global-step:12140	 l-p:0.14672817289829254
epoch£º607	 i:1 	 global-step:12141	 l-p:0.13014093041419983
epoch£º607	 i:2 	 global-step:12142	 l-p:0.10885713249444962
epoch£º607	 i:3 	 global-step:12143	 l-p:0.14879387617111206
epoch£º607	 i:4 	 global-step:12144	 l-p:0.16442175209522247
epoch£º607	 i:5 	 global-step:12145	 l-p:0.11456285417079926
epoch£º607	 i:6 	 global-step:12146	 l-p:0.09639091044664383
epoch£º607	 i:7 	 global-step:12147	 l-p:0.1252441555261612
epoch£º607	 i:8 	 global-step:12148	 l-p:0.13926711678504944
epoch£º607	 i:9 	 global-step:12149	 l-p:0.18435104191303253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0229, 5.0214, 5.0228],
        [5.0229, 5.0173, 5.0225],
        [5.0229, 4.9597, 4.9980],
        [5.0229, 5.2288, 5.0820]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.18599775433540344 
model_pd.l_d.mean(): -19.59148406982422 
model_pd.lagr.mean(): -19.405487060546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5561], device='cuda:0')), ('power', tensor([-20.3737], device='cuda:0'))])
epoch£º608	 i:0 	 global-step:12160	 l-p:0.18599775433540344
epoch£º608	 i:1 	 global-step:12161	 l-p:0.1461387574672699
epoch£º608	 i:2 	 global-step:12162	 l-p:0.1444278061389923
epoch£º608	 i:3 	 global-step:12163	 l-p:0.18626655638217926
epoch£º608	 i:4 	 global-step:12164	 l-p:0.07788622379302979
epoch£º608	 i:5 	 global-step:12165	 l-p:0.11923469603061676
epoch£º608	 i:6 	 global-step:12166	 l-p:0.13523215055465698
epoch£º608	 i:7 	 global-step:12167	 l-p:0.11772225797176361
epoch£º608	 i:8 	 global-step:12168	 l-p:0.15644599497318268
epoch£º608	 i:9 	 global-step:12169	 l-p:0.13429562747478485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0410, 5.0410, 5.0410],
        [5.0410, 5.0235, 5.0385],
        [5.0410, 5.2514, 5.1065],
        [5.0410, 4.9300, 4.9654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): 0.14538522064685822 
model_pd.l_d.mean(): -19.25473976135254 
model_pd.lagr.mean(): -19.10935401916504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5649], device='cuda:0')), ('power', tensor([-20.0423], device='cuda:0'))])
epoch£º609	 i:0 	 global-step:12180	 l-p:0.14538522064685822
epoch£º609	 i:1 	 global-step:12181	 l-p:0.1369112879037857
epoch£º609	 i:2 	 global-step:12182	 l-p:0.15807507932186127
epoch£º609	 i:3 	 global-step:12183	 l-p:0.13252975046634674
epoch£º609	 i:4 	 global-step:12184	 l-p:0.1152724027633667
epoch£º609	 i:5 	 global-step:12185	 l-p:0.14286498725414276
epoch£º609	 i:6 	 global-step:12186	 l-p:0.16957762837409973
epoch£º609	 i:7 	 global-step:12187	 l-p:0.13722772896289825
epoch£º609	 i:8 	 global-step:12188	 l-p:0.10144244879484177
epoch£º609	 i:9 	 global-step:12189	 l-p:0.10700307786464691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0449, 4.9338, 4.9692],
        [5.0449, 5.0446, 5.0449],
        [5.0449, 5.0449, 5.0449],
        [5.0449, 5.0108, 5.0368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.08118586987257004 
model_pd.l_d.mean(): -20.413776397705078 
model_pd.lagr.mean(): -20.332590103149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4736], device='cuda:0')), ('power', tensor([-21.1206], device='cuda:0'))])
epoch£º610	 i:0 	 global-step:12200	 l-p:0.08118586987257004
epoch£º610	 i:1 	 global-step:12201	 l-p:0.14317937195301056
epoch£º610	 i:2 	 global-step:12202	 l-p:0.12841889262199402
epoch£º610	 i:3 	 global-step:12203	 l-p:0.12462837994098663
epoch£º610	 i:4 	 global-step:12204	 l-p:0.12846329808235168
epoch£º610	 i:5 	 global-step:12205	 l-p:0.12933917343616486
epoch£º610	 i:6 	 global-step:12206	 l-p:0.045389361679553986
epoch£º610	 i:7 	 global-step:12207	 l-p:0.1872761994600296
epoch£º610	 i:8 	 global-step:12208	 l-p:0.1541803479194641
epoch£º610	 i:9 	 global-step:12209	 l-p:0.14446203410625458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0959, 4.9250, 4.8691],
        [5.0959, 5.3864, 5.2865],
        [5.0959, 5.6177, 5.6759],
        [5.0959, 5.0948, 5.0959]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.09703058749437332 
model_pd.l_d.mean(): -20.591421127319336 
model_pd.lagr.mean(): -20.4943904876709 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4188], device='cuda:0')), ('power', tensor([-21.2443], device='cuda:0'))])
epoch£º611	 i:0 	 global-step:12220	 l-p:0.09703058749437332
epoch£º611	 i:1 	 global-step:12221	 l-p:0.11942891776561737
epoch£º611	 i:2 	 global-step:12222	 l-p:0.08919402211904526
epoch£º611	 i:3 	 global-step:12223	 l-p:0.16063930094242096
epoch£º611	 i:4 	 global-step:12224	 l-p:0.14018477499485016
epoch£º611	 i:5 	 global-step:12225	 l-p:0.11290884017944336
epoch£º611	 i:6 	 global-step:12226	 l-p:0.1277497112751007
epoch£º611	 i:7 	 global-step:12227	 l-p:0.12940600514411926
epoch£º611	 i:8 	 global-step:12228	 l-p:-0.10042668133974075
epoch£º611	 i:9 	 global-step:12229	 l-p:0.14912419021129608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1073, 5.0633, 4.8300],
        [5.1073, 5.1073, 5.1073],
        [5.1073, 4.9449, 4.9164],
        [5.1073, 5.1064, 5.1073]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.14480233192443848 
model_pd.l_d.mean(): -19.5056095123291 
model_pd.lagr.mean(): -19.360807418823242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4874], device='cuda:0')), ('power', tensor([-20.2167], device='cuda:0'))])
epoch£º612	 i:0 	 global-step:12240	 l-p:0.14480233192443848
epoch£º612	 i:1 	 global-step:12241	 l-p:0.12985189259052277
epoch£º612	 i:2 	 global-step:12242	 l-p:0.09813894331455231
epoch£º612	 i:3 	 global-step:12243	 l-p:0.1301671415567398
epoch£º612	 i:4 	 global-step:12244	 l-p:0.11300332844257355
epoch£º612	 i:5 	 global-step:12245	 l-p:0.13506680727005005
epoch£º612	 i:6 	 global-step:12246	 l-p:0.14863571524620056
epoch£º612	 i:7 	 global-step:12247	 l-p:0.14901982247829437
epoch£º612	 i:8 	 global-step:12248	 l-p:0.1950758695602417
epoch£º612	 i:9 	 global-step:12249	 l-p:0.06663457304239273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0491, 5.0489, 5.0491],
        [5.0491, 5.0246, 5.0446],
        [5.0491, 5.0459, 5.0489],
        [5.0491, 4.9923, 5.0287]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 0.20699122548103333 
model_pd.l_d.mean(): -20.59003448486328 
model_pd.lagr.mean(): -20.38304328918457 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4609], device='cuda:0')), ('power', tensor([-21.2859], device='cuda:0'))])
epoch£º613	 i:0 	 global-step:12260	 l-p:0.20699122548103333
epoch£º613	 i:1 	 global-step:12261	 l-p:0.1672544777393341
epoch£º613	 i:2 	 global-step:12262	 l-p:0.12031745910644531
epoch£º613	 i:3 	 global-step:12263	 l-p:0.05184179171919823
epoch£º613	 i:4 	 global-step:12264	 l-p:0.11432898044586182
epoch£º613	 i:5 	 global-step:12265	 l-p:0.12229902297258377
epoch£º613	 i:6 	 global-step:12266	 l-p:0.1408221572637558
epoch£º613	 i:7 	 global-step:12267	 l-p:0.09290678799152374
epoch£º613	 i:8 	 global-step:12268	 l-p:0.10687576979398727
epoch£º613	 i:9 	 global-step:12269	 l-p:0.13106802105903625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0925, 4.9928, 4.7665],
        [5.0925, 4.9156, 4.8236],
        [5.0925, 5.0892, 5.0924],
        [5.0925, 5.0908, 5.0925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.13651639223098755 
model_pd.l_d.mean(): -20.441844940185547 
model_pd.lagr.mean(): -20.305328369140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4685], device='cuda:0')), ('power', tensor([-21.1438], device='cuda:0'))])
epoch£º614	 i:0 	 global-step:12280	 l-p:0.13651639223098755
epoch£º614	 i:1 	 global-step:12281	 l-p:0.1481170356273651
epoch£º614	 i:2 	 global-step:12282	 l-p:0.024515457451343536
epoch£º614	 i:3 	 global-step:12283	 l-p:0.1276310533285141
epoch£º614	 i:4 	 global-step:12284	 l-p:0.12851159274578094
epoch£º614	 i:5 	 global-step:12285	 l-p:0.13974660634994507
epoch£º614	 i:6 	 global-step:12286	 l-p:0.1791551113128662
epoch£º614	 i:7 	 global-step:12287	 l-p:0.10954740643501282
epoch£º614	 i:8 	 global-step:12288	 l-p:0.15161138772964478
epoch£º614	 i:9 	 global-step:12289	 l-p:0.11864135414361954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0593, 4.8789, 4.7673],
        [5.0593, 4.8794, 4.7945],
        [5.0593, 5.1017, 4.8798],
        [5.0593, 5.0593, 5.0593]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.13331566751003265 
model_pd.l_d.mean(): -19.141714096069336 
model_pd.lagr.mean(): -19.008398056030273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5190], device='cuda:0')), ('power', tensor([-19.8811], device='cuda:0'))])
epoch£º615	 i:0 	 global-step:12300	 l-p:0.13331566751003265
epoch£º615	 i:1 	 global-step:12301	 l-p:0.09325972199440002
epoch£º615	 i:2 	 global-step:12302	 l-p:0.13720494508743286
epoch£º615	 i:3 	 global-step:12303	 l-p:0.1042490154504776
epoch£º615	 i:4 	 global-step:12304	 l-p:0.13135358691215515
epoch£º615	 i:5 	 global-step:12305	 l-p:0.130219966173172
epoch£º615	 i:6 	 global-step:12306	 l-p:0.14755798876285553
epoch£º615	 i:7 	 global-step:12307	 l-p:0.17416098713874817
epoch£º615	 i:8 	 global-step:12308	 l-p:0.1226300373673439
epoch£º615	 i:9 	 global-step:12309	 l-p:0.117000050842762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0619, 5.0599, 5.0618],
        [5.0619, 5.0292, 5.0544],
        [5.0619, 5.2188, 5.0436],
        [5.0619, 4.9174, 4.7140]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.1326490342617035 
model_pd.l_d.mean(): -18.742876052856445 
model_pd.lagr.mean(): -18.610227584838867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6002], device='cuda:0')), ('power', tensor([-19.5609], device='cuda:0'))])
epoch£º616	 i:0 	 global-step:12320	 l-p:0.1326490342617035
epoch£º616	 i:1 	 global-step:12321	 l-p:0.155029758810997
epoch£º616	 i:2 	 global-step:12322	 l-p:0.15435706079006195
epoch£º616	 i:3 	 global-step:12323	 l-p:0.14367260038852692
epoch£º616	 i:4 	 global-step:12324	 l-p:0.06772032380104065
epoch£º616	 i:5 	 global-step:12325	 l-p:0.10586351156234741
epoch£º616	 i:6 	 global-step:12326	 l-p:0.1312040239572525
epoch£º616	 i:7 	 global-step:12327	 l-p:0.12342355400323868
epoch£º616	 i:8 	 global-step:12328	 l-p:0.1368233859539032
epoch£º616	 i:9 	 global-step:12329	 l-p:0.12952929735183716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0592, 4.9996, 5.0370],
        [5.0592, 4.9174, 4.9315],
        [5.0592, 5.5953, 5.6646],
        [5.0592, 5.0006, 5.0377]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.14492715895175934 
model_pd.l_d.mean(): -20.256681442260742 
model_pd.lagr.mean(): -20.111753463745117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4786], device='cuda:0')), ('power', tensor([-20.9669], device='cuda:0'))])
epoch£º617	 i:0 	 global-step:12340	 l-p:0.14492715895175934
epoch£º617	 i:1 	 global-step:12341	 l-p:0.11105936020612717
epoch£º617	 i:2 	 global-step:12342	 l-p:0.16822831332683563
epoch£º617	 i:3 	 global-step:12343	 l-p:0.11656436324119568
epoch£º617	 i:4 	 global-step:12344	 l-p:0.1300477832555771
epoch£º617	 i:5 	 global-step:12345	 l-p:0.15180988609790802
epoch£º617	 i:6 	 global-step:12346	 l-p:0.12808473408222198
epoch£º617	 i:7 	 global-step:12347	 l-p:0.2129516899585724
epoch£º617	 i:8 	 global-step:12348	 l-p:0.11990348249673843
epoch£º617	 i:9 	 global-step:12349	 l-p:0.14707952737808228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0209, 4.8352, 4.7315],
        [5.0209, 4.9593, 4.9975],
        [5.0209, 5.5193, 5.5623],
        [5.0209, 4.8501, 4.8217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.12953849136829376 
model_pd.l_d.mean(): -20.303085327148438 
model_pd.lagr.mean(): -20.173547744750977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4902], device='cuda:0')), ('power', tensor([-21.0257], device='cuda:0'))])
epoch£º618	 i:0 	 global-step:12360	 l-p:0.12953849136829376
epoch£º618	 i:1 	 global-step:12361	 l-p:0.17224933207035065
epoch£º618	 i:2 	 global-step:12362	 l-p:0.10471269488334656
epoch£º618	 i:3 	 global-step:12363	 l-p:0.28673818707466125
epoch£º618	 i:4 	 global-step:12364	 l-p:0.12395280599594116
epoch£º618	 i:5 	 global-step:12365	 l-p:0.16402168571949005
epoch£º618	 i:6 	 global-step:12366	 l-p:0.13811008632183075
epoch£º618	 i:7 	 global-step:12367	 l-p:0.16071860492229462
epoch£º618	 i:8 	 global-step:12368	 l-p:0.08986923098564148
epoch£º618	 i:9 	 global-step:12369	 l-p:0.16369657218456268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0089, 4.9495, 4.9872],
        [5.0089, 4.8933, 4.9294],
        [5.0089, 4.8893, 4.9235],
        [5.0089, 5.0877, 4.8770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): 0.1781214475631714 
model_pd.l_d.mean(): -20.364803314208984 
model_pd.lagr.mean(): -20.186681747436523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5124], device='cuda:0')), ('power', tensor([-21.1108], device='cuda:0'))])
epoch£º619	 i:0 	 global-step:12380	 l-p:0.1781214475631714
epoch£º619	 i:1 	 global-step:12381	 l-p:0.15675786137580872
epoch£º619	 i:2 	 global-step:12382	 l-p:0.12087597697973251
epoch£º619	 i:3 	 global-step:12383	 l-p:0.0940934345126152
epoch£º619	 i:4 	 global-step:12384	 l-p:0.18105565011501312
epoch£º619	 i:5 	 global-step:12385	 l-p:0.10951042920351028
epoch£º619	 i:6 	 global-step:12386	 l-p:0.14535176753997803
epoch£º619	 i:7 	 global-step:12387	 l-p:0.11725424230098724
epoch£º619	 i:8 	 global-step:12388	 l-p:0.12715989351272583
epoch£º619	 i:9 	 global-step:12389	 l-p:0.17409354448318481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0495, 5.0267, 5.0455],
        [5.0495, 4.9090, 4.9260],
        [5.0495, 5.0495, 5.0495],
        [5.0495, 5.0117, 4.7725]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.1324760913848877 
model_pd.l_d.mean(): -20.874975204467773 
model_pd.lagr.mean(): -20.74249839782715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4359], device='cuda:0')), ('power', tensor([-21.5484], device='cuda:0'))])
epoch£º620	 i:0 	 global-step:12400	 l-p:0.1324760913848877
epoch£º620	 i:1 	 global-step:12401	 l-p:0.19012819230556488
epoch£º620	 i:2 	 global-step:12402	 l-p:0.12672793865203857
epoch£º620	 i:3 	 global-step:12403	 l-p:0.10947640240192413
epoch£º620	 i:4 	 global-step:12404	 l-p:0.1239953488111496
epoch£º620	 i:5 	 global-step:12405	 l-p:0.10439317673444748
epoch£º620	 i:6 	 global-step:12406	 l-p:0.1553318053483963
epoch£º620	 i:7 	 global-step:12407	 l-p:0.14067384600639343
epoch£º620	 i:8 	 global-step:12408	 l-p:0.14257267117500305
epoch£º620	 i:9 	 global-step:12409	 l-p:0.13100965321063995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0496, 5.0952, 4.8726],
        [5.0496, 5.0487, 5.0496],
        [5.0496, 5.0496, 5.0496],
        [5.0496, 5.0496, 5.0496]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.15191465616226196 
model_pd.l_d.mean(): -20.880029678344727 
model_pd.lagr.mean(): -20.72811508178711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4149], device='cuda:0')), ('power', tensor([-21.5321], device='cuda:0'))])
epoch£º621	 i:0 	 global-step:12420	 l-p:0.15191465616226196
epoch£º621	 i:1 	 global-step:12421	 l-p:0.13214610517024994
epoch£º621	 i:2 	 global-step:12422	 l-p:0.13056239485740662
epoch£º621	 i:3 	 global-step:12423	 l-p:0.19477836787700653
epoch£º621	 i:4 	 global-step:12424	 l-p:0.19380341470241547
epoch£º621	 i:5 	 global-step:12425	 l-p:0.08080870658159256
epoch£º621	 i:6 	 global-step:12426	 l-p:0.11306074261665344
epoch£º621	 i:7 	 global-step:12427	 l-p:0.1277018040418625
epoch£º621	 i:8 	 global-step:12428	 l-p:0.1489340364933014
epoch£º621	 i:9 	 global-step:12429	 l-p:0.06239037960767746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0642, 5.0642, 5.0642],
        [5.0642, 4.8886, 4.8392],
        [5.0642, 4.9554, 4.7258],
        [5.0642, 4.9119, 4.9140]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.16560222208499908 
model_pd.l_d.mean(): -20.77556037902832 
model_pd.lagr.mean(): -20.60995864868164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4332], device='cuda:0')), ('power', tensor([-21.4451], device='cuda:0'))])
epoch£º622	 i:0 	 global-step:12440	 l-p:0.16560222208499908
epoch£º622	 i:1 	 global-step:12441	 l-p:0.1341007947921753
epoch£º622	 i:2 	 global-step:12442	 l-p:0.13192979991436005
epoch£º622	 i:3 	 global-step:12443	 l-p:0.10809094458818436
epoch£º622	 i:4 	 global-step:12444	 l-p:0.12228570878505707
epoch£º622	 i:5 	 global-step:12445	 l-p:0.1142730861902237
epoch£º622	 i:6 	 global-step:12446	 l-p:0.17396190762519836
epoch£º622	 i:7 	 global-step:12447	 l-p:0.1500604748725891
epoch£º622	 i:8 	 global-step:12448	 l-p:0.12907646596431732
epoch£º622	 i:9 	 global-step:12449	 l-p:0.12530741095542908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0279, 4.9939, 5.0200],
        [5.0279, 4.9140, 4.9506],
        [5.0279, 5.0215, 5.0274],
        [5.0279, 5.0278, 5.0279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.1574993133544922 
model_pd.l_d.mean(): -20.308732986450195 
model_pd.lagr.mean(): -20.151233673095703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4495], device='cuda:0')), ('power', tensor([-20.9899], device='cuda:0'))])
epoch£º623	 i:0 	 global-step:12460	 l-p:0.1574993133544922
epoch£º623	 i:1 	 global-step:12461	 l-p:0.13044483959674835
epoch£º623	 i:2 	 global-step:12462	 l-p:0.17914573848247528
epoch£º623	 i:3 	 global-step:12463	 l-p:0.2628176808357239
epoch£º623	 i:4 	 global-step:12464	 l-p:0.12143810838460922
epoch£º623	 i:5 	 global-step:12465	 l-p:0.15069623291492462
epoch£º623	 i:6 	 global-step:12466	 l-p:0.1311541646718979
epoch£º623	 i:7 	 global-step:12467	 l-p:0.13039974868297577
epoch£º623	 i:8 	 global-step:12468	 l-p:0.14864884316921234
epoch£º623	 i:9 	 global-step:12469	 l-p:0.12251630425453186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0068, 4.9984, 5.0061],
        [5.0068, 5.3555, 5.2939],
        [5.0068, 5.0051, 5.0068],
        [5.0068, 5.0068, 5.0068]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.19742685556411743 
model_pd.l_d.mean(): -20.5125675201416 
model_pd.lagr.mean(): -20.315139770507812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4808], device='cuda:0')), ('power', tensor([-21.2280], device='cuda:0'))])
epoch£º624	 i:0 	 global-step:12480	 l-p:0.19742685556411743
epoch£º624	 i:1 	 global-step:12481	 l-p:0.13104604184627533
epoch£º624	 i:2 	 global-step:12482	 l-p:0.15484891831874847
epoch£º624	 i:3 	 global-step:12483	 l-p:0.11959826946258545
epoch£º624	 i:4 	 global-step:12484	 l-p:0.21342647075653076
epoch£º624	 i:5 	 global-step:12485	 l-p:0.1504715532064438
epoch£º624	 i:6 	 global-step:12486	 l-p:0.1493186205625534
epoch£º624	 i:7 	 global-step:12487	 l-p:0.09760822355747223
epoch£º624	 i:8 	 global-step:12488	 l-p:0.11103422194719315
epoch£º624	 i:9 	 global-step:12489	 l-p:0.157151460647583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0580, 5.0580, 5.0580],
        [5.0580, 5.0457, 5.0567],
        [5.0580, 5.0568, 5.0580],
        [5.0580, 4.8798, 4.7273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.14976657927036285 
model_pd.l_d.mean(): -20.329601287841797 
model_pd.lagr.mean(): -20.179834365844727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4867], device='cuda:0')), ('power', tensor([-21.0489], device='cuda:0'))])
epoch£º625	 i:0 	 global-step:12500	 l-p:0.14976657927036285
epoch£º625	 i:1 	 global-step:12501	 l-p:0.09749805182218552
epoch£º625	 i:2 	 global-step:12502	 l-p:0.08273231238126755
epoch£º625	 i:3 	 global-step:12503	 l-p:0.1354200541973114
epoch£º625	 i:4 	 global-step:12504	 l-p:0.14332745969295502
epoch£º625	 i:5 	 global-step:12505	 l-p:0.1522393524646759
epoch£º625	 i:6 	 global-step:12506	 l-p:0.09999644011259079
epoch£º625	 i:7 	 global-step:12507	 l-p:0.12729579210281372
epoch£º625	 i:8 	 global-step:12508	 l-p:0.12865008413791656
epoch£º625	 i:9 	 global-step:12509	 l-p:0.13021570444107056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0908, 5.0493, 5.0794],
        [5.0908, 4.9148, 4.8614],
        [5.0908, 5.0908, 5.0908],
        [5.0908, 5.4853, 5.4505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.044592879712581635 
model_pd.l_d.mean(): -20.83466339111328 
model_pd.lagr.mean(): -20.790069580078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4120], device='cuda:0')), ('power', tensor([-21.4832], device='cuda:0'))])
epoch£º626	 i:0 	 global-step:12520	 l-p:0.044592879712581635
epoch£º626	 i:1 	 global-step:12521	 l-p:0.1540268510580063
epoch£º626	 i:2 	 global-step:12522	 l-p:0.12326381355524063
epoch£º626	 i:3 	 global-step:12523	 l-p:0.13640926778316498
epoch£º626	 i:4 	 global-step:12524	 l-p:0.10608275979757309
epoch£º626	 i:5 	 global-step:12525	 l-p:0.11277680844068527
epoch£º626	 i:6 	 global-step:12526	 l-p:0.10176339745521545
epoch£º626	 i:7 	 global-step:12527	 l-p:0.15845942497253418
epoch£º626	 i:8 	 global-step:12528	 l-p:0.11931347846984863
epoch£º626	 i:9 	 global-step:12529	 l-p:0.13123385608196259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1025, 5.0077, 5.0483],
        [5.1025, 4.9293, 4.7733],
        [5.1025, 5.0149, 5.0562],
        [5.1025, 5.0710, 5.0955]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.11067920178174973 
model_pd.l_d.mean(): -20.75241470336914 
model_pd.lagr.mean(): -20.641735076904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4113], device='cuda:0')), ('power', tensor([-21.3994], device='cuda:0'))])
epoch£º627	 i:0 	 global-step:12540	 l-p:0.11067920178174973
epoch£º627	 i:1 	 global-step:12541	 l-p:0.11118333786725998
epoch£º627	 i:2 	 global-step:12542	 l-p:0.1551344394683838
epoch£º627	 i:3 	 global-step:12543	 l-p:0.11640260368585587
epoch£º627	 i:4 	 global-step:12544	 l-p:0.12961900234222412
epoch£º627	 i:5 	 global-step:12545	 l-p:0.1306713968515396
epoch£º627	 i:6 	 global-step:12546	 l-p:0.1574377715587616
epoch£º627	 i:7 	 global-step:12547	 l-p:0.05209547281265259
epoch£º627	 i:8 	 global-step:12548	 l-p:0.1590239554643631
epoch£º627	 i:9 	 global-step:12549	 l-p:0.15129931271076202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0683, 4.8835, 4.7890],
        [5.0683, 5.0680, 5.0683],
        [5.0683, 5.0674, 5.0683],
        [5.0683, 5.0034, 5.0426]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.07405112683773041 
model_pd.l_d.mean(): -20.784446716308594 
model_pd.lagr.mean(): -20.71039581298828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4254], device='cuda:0')), ('power', tensor([-21.4462], device='cuda:0'))])
epoch£º628	 i:0 	 global-step:12560	 l-p:0.07405112683773041
epoch£º628	 i:1 	 global-step:12561	 l-p:0.1456497758626938
epoch£º628	 i:2 	 global-step:12562	 l-p:0.17438466846942902
epoch£º628	 i:3 	 global-step:12563	 l-p:0.1676788032054901
epoch£º628	 i:4 	 global-step:12564	 l-p:0.10113391280174255
epoch£º628	 i:5 	 global-step:12565	 l-p:0.1402561217546463
epoch£º628	 i:6 	 global-step:12566	 l-p:0.129317045211792
epoch£º628	 i:7 	 global-step:12567	 l-p:0.12159410119056702
epoch£º628	 i:8 	 global-step:12568	 l-p:0.15176300704479218
epoch£º628	 i:9 	 global-step:12569	 l-p:0.107623390853405
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0501, 5.0492, 5.0501],
        [5.0501, 4.8708, 4.8222],
        [5.0501, 5.0456, 5.0499],
        [5.0501, 5.2495, 5.0943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.14491474628448486 
model_pd.l_d.mean(): -21.04953384399414 
model_pd.lagr.mean(): -20.904619216918945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3929], device='cuda:0')), ('power', tensor([-21.6810], device='cuda:0'))])
epoch£º629	 i:0 	 global-step:12580	 l-p:0.14491474628448486
epoch£º629	 i:1 	 global-step:12581	 l-p:0.12951666116714478
epoch£º629	 i:2 	 global-step:12582	 l-p:0.13080677390098572
epoch£º629	 i:3 	 global-step:12583	 l-p:0.1276664137840271
epoch£º629	 i:4 	 global-step:12584	 l-p:0.18569983541965485
epoch£º629	 i:5 	 global-step:12585	 l-p:0.09461709856987
epoch£º629	 i:6 	 global-step:12586	 l-p:0.10575244575738907
epoch£º629	 i:7 	 global-step:12587	 l-p:0.12024158239364624
epoch£º629	 i:8 	 global-step:12588	 l-p:0.21180926263332367
epoch£º629	 i:9 	 global-step:12589	 l-p:0.14897780120372772
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0382, 5.0381, 5.0382],
        [5.0382, 4.8576, 4.8094],
        [5.0382, 4.8490, 4.7364],
        [5.0382, 5.0382, 5.0382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.1604529768228531 
model_pd.l_d.mean(): -20.06463050842285 
model_pd.lagr.mean(): -19.904176712036133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4951], device='cuda:0')), ('power', tensor([-20.7897], device='cuda:0'))])
epoch£º630	 i:0 	 global-step:12600	 l-p:0.1604529768228531
epoch£º630	 i:1 	 global-step:12601	 l-p:0.15108564496040344
epoch£º630	 i:2 	 global-step:12602	 l-p:0.20746952295303345
epoch£º630	 i:3 	 global-step:12603	 l-p:0.10695388913154602
epoch£º630	 i:4 	 global-step:12604	 l-p:0.11088576912879944
epoch£º630	 i:5 	 global-step:12605	 l-p:0.13924448192119598
epoch£º630	 i:6 	 global-step:12606	 l-p:0.1016838327050209
epoch£º630	 i:7 	 global-step:12607	 l-p:0.1214074119925499
epoch£º630	 i:8 	 global-step:12608	 l-p:0.13857592642307281
epoch£º630	 i:9 	 global-step:12609	 l-p:0.12250859290361404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0520, 5.0355, 5.0498],
        [5.0520, 4.9852, 4.7414],
        [5.0520, 5.0520, 5.0520],
        [5.0520, 4.9259, 4.9573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.1184326708316803 
model_pd.l_d.mean(): -20.99176025390625 
model_pd.lagr.mean(): -20.873327255249023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3975], device='cuda:0')), ('power', tensor([-21.6272], device='cuda:0'))])
epoch£º631	 i:0 	 global-step:12620	 l-p:0.1184326708316803
epoch£º631	 i:1 	 global-step:12621	 l-p:0.11636575311422348
epoch£º631	 i:2 	 global-step:12622	 l-p:0.15130695700645447
epoch£º631	 i:3 	 global-step:12623	 l-p:0.1567552387714386
epoch£º631	 i:4 	 global-step:12624	 l-p:0.21293728053569794
epoch£º631	 i:5 	 global-step:12625	 l-p:0.18707720935344696
epoch£º631	 i:6 	 global-step:12626	 l-p:0.09461899846792221
epoch£º631	 i:7 	 global-step:12627	 l-p:0.12594863772392273
epoch£º631	 i:8 	 global-step:12628	 l-p:0.13019153475761414
epoch£º631	 i:9 	 global-step:12629	 l-p:0.12717412412166595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0333, 4.9789, 5.0151],
        [5.0333, 5.0165, 5.0310],
        [5.0333, 5.0282, 5.0330],
        [5.0333, 5.0292, 5.0331]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): 0.13140341639518738 
model_pd.l_d.mean(): -19.508453369140625 
model_pd.lagr.mean(): -19.377050399780273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5063], device='cuda:0')), ('power', tensor([-20.2389], device='cuda:0'))])
epoch£º632	 i:0 	 global-step:12640	 l-p:0.13140341639518738
epoch£º632	 i:1 	 global-step:12641	 l-p:0.1592579483985901
epoch£º632	 i:2 	 global-step:12642	 l-p:0.13172630965709686
epoch£º632	 i:3 	 global-step:12643	 l-p:0.15067066252231598
epoch£º632	 i:4 	 global-step:12644	 l-p:0.1432759016752243
epoch£º632	 i:5 	 global-step:12645	 l-p:0.1363288313150406
epoch£º632	 i:6 	 global-step:12646	 l-p:0.12630482017993927
epoch£º632	 i:7 	 global-step:12647	 l-p:0.22262965142726898
epoch£º632	 i:8 	 global-step:12648	 l-p:0.11945386976003647
epoch£º632	 i:9 	 global-step:12649	 l-p:0.22622846066951752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9953, 4.9809, 4.9935],
        [4.9953, 4.8023, 4.6659],
        [4.9953, 4.8839, 4.9239],
        [4.9953, 5.5241, 5.5879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.1379774957895279 
model_pd.l_d.mean(): -19.876712799072266 
model_pd.lagr.mean(): -19.73873519897461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5179], device='cuda:0')), ('power', tensor([-20.6230], device='cuda:0'))])
epoch£º633	 i:0 	 global-step:12660	 l-p:0.1379774957895279
epoch£º633	 i:1 	 global-step:12661	 l-p:0.1793101280927658
epoch£º633	 i:2 	 global-step:12662	 l-p:0.13451653718948364
epoch£º633	 i:3 	 global-step:12663	 l-p:0.21004356443881989
epoch£º633	 i:4 	 global-step:12664	 l-p:0.12726452946662903
epoch£º633	 i:5 	 global-step:12665	 l-p:0.3209013342857361
epoch£º633	 i:6 	 global-step:12666	 l-p:0.15660995244979858
epoch£º633	 i:7 	 global-step:12667	 l-p:0.20776081085205078
epoch£º633	 i:8 	 global-step:12668	 l-p:0.15441542863845825
epoch£º633	 i:9 	 global-step:12669	 l-p:0.1017124354839325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0216, 5.0216, 5.0216],
        [5.0216, 5.0178, 5.0214],
        [5.0216, 4.9195, 4.9612],
        [5.0216, 4.9292, 4.9721]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.12646131217479706 
model_pd.l_d.mean(): -20.084762573242188 
model_pd.lagr.mean(): -19.958301544189453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5091], device='cuda:0')), ('power', tensor([-20.8243], device='cuda:0'))])
epoch£º634	 i:0 	 global-step:12680	 l-p:0.12646131217479706
epoch£º634	 i:1 	 global-step:12681	 l-p:0.20465418696403503
epoch£º634	 i:2 	 global-step:12682	 l-p:0.1791212111711502
epoch£º634	 i:3 	 global-step:12683	 l-p:0.16877445578575134
epoch£º634	 i:4 	 global-step:12684	 l-p:0.06352858245372772
epoch£º634	 i:5 	 global-step:12685	 l-p:0.1503014862537384
epoch£º634	 i:6 	 global-step:12686	 l-p:0.13601285219192505
epoch£º634	 i:7 	 global-step:12687	 l-p:0.07393427938222885
epoch£º634	 i:8 	 global-step:12688	 l-p:0.1130455955862999
epoch£º634	 i:9 	 global-step:12689	 l-p:0.14106303453445435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1055, 4.9616, 4.9759],
        [5.1055, 4.9711, 4.9945],
        [5.1055, 5.1046, 5.1054],
        [5.1055, 4.9764, 4.7547]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.14332734048366547 
model_pd.l_d.mean(): -20.61983871459961 
model_pd.lagr.mean(): -20.476511001586914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4500], device='cuda:0')), ('power', tensor([-21.3049], device='cuda:0'))])
epoch£º635	 i:0 	 global-step:12700	 l-p:0.14332734048366547
epoch£º635	 i:1 	 global-step:12701	 l-p:0.13361160457134247
epoch£º635	 i:2 	 global-step:12702	 l-p:0.11351283639669418
epoch£º635	 i:3 	 global-step:12703	 l-p:0.1411609947681427
epoch£º635	 i:4 	 global-step:12704	 l-p:0.062092408537864685
epoch£º635	 i:5 	 global-step:12705	 l-p:0.1032860279083252
epoch£º635	 i:6 	 global-step:12706	 l-p:0.14496634900569916
epoch£º635	 i:7 	 global-step:12707	 l-p:0.12376844137907028
epoch£º635	 i:8 	 global-step:12708	 l-p:0.11040119081735611
epoch£º635	 i:9 	 global-step:12709	 l-p:0.7499995231628418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1450, 5.0861, 5.1233],
        [5.1450, 5.1277, 5.1425],
        [5.1450, 5.0949, 4.8555],
        [5.1450, 5.1449, 5.1450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.11552859842777252 
model_pd.l_d.mean(): -20.49762535095215 
model_pd.lagr.mean(): -20.382097244262695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4208], device='cuda:0')), ('power', tensor([-21.1515], device='cuda:0'))])
epoch£º636	 i:0 	 global-step:12720	 l-p:0.11552859842777252
epoch£º636	 i:1 	 global-step:12721	 l-p:0.12152618169784546
epoch£º636	 i:2 	 global-step:12722	 l-p:0.10327442735433578
epoch£º636	 i:3 	 global-step:12723	 l-p:0.12158454954624176
epoch£º636	 i:4 	 global-step:12724	 l-p:0.05716227367520332
epoch£º636	 i:5 	 global-step:12725	 l-p:0.0703895092010498
epoch£º636	 i:6 	 global-step:12726	 l-p:0.13321219384670258
epoch£º636	 i:7 	 global-step:12727	 l-p:-0.040444087237119675
epoch£º636	 i:8 	 global-step:12728	 l-p:0.18036264181137085
epoch£º636	 i:9 	 global-step:12729	 l-p:0.15931156277656555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1193, 4.9495, 4.9196],
        [5.1193, 5.6873, 5.7745],
        [5.1193, 4.9937, 5.0233],
        [5.1193, 5.1027, 5.1170]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.15587608516216278 
model_pd.l_d.mean(): -19.46881103515625 
model_pd.lagr.mean(): -19.31293487548828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4844], device='cuda:0')), ('power', tensor([-20.1764], device='cuda:0'))])
epoch£º637	 i:0 	 global-step:12740	 l-p:0.15587608516216278
epoch£º637	 i:1 	 global-step:12741	 l-p:0.12611830234527588
epoch£º637	 i:2 	 global-step:12742	 l-p:0.09816884994506836
epoch£º637	 i:3 	 global-step:12743	 l-p:0.14391843974590302
epoch£º637	 i:4 	 global-step:12744	 l-p:0.11771082878112793
epoch£º637	 i:5 	 global-step:12745	 l-p:0.12464360892772675
epoch£º637	 i:6 	 global-step:12746	 l-p:-0.0033745907712727785
epoch£º637	 i:7 	 global-step:12747	 l-p:0.12183608114719391
epoch£º637	 i:8 	 global-step:12748	 l-p:0.16935628652572632
epoch£º637	 i:9 	 global-step:12749	 l-p:0.09593721479177475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0940, 5.0937, 5.0940],
        [5.0940, 5.0940, 5.0940],
        [5.0940, 5.3011, 5.1477],
        [5.0940, 5.5048, 5.4791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.14052248001098633 
model_pd.l_d.mean(): -20.42304039001465 
model_pd.lagr.mean(): -20.28251838684082 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5021], device='cuda:0')), ('power', tensor([-21.1591], device='cuda:0'))])
epoch£º638	 i:0 	 global-step:12760	 l-p:0.14052248001098633
epoch£º638	 i:1 	 global-step:12761	 l-p:0.08837701380252838
epoch£º638	 i:2 	 global-step:12762	 l-p:0.051137570291757584
epoch£º638	 i:3 	 global-step:12763	 l-p:0.1226712316274643
epoch£º638	 i:4 	 global-step:12764	 l-p:0.14230409264564514
epoch£º638	 i:5 	 global-step:12765	 l-p:0.10766326636075974
epoch£º638	 i:6 	 global-step:12766	 l-p:0.1582888513803482
epoch£º638	 i:7 	 global-step:12767	 l-p:0.14742404222488403
epoch£º638	 i:8 	 global-step:12768	 l-p:0.14292870461940765
epoch£º638	 i:9 	 global-step:12769	 l-p:0.1307285726070404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0914, 5.5363, 5.5343],
        [5.0914, 5.0877, 5.0912],
        [5.0914, 4.9093, 4.8475],
        [5.0914, 4.9162, 4.8792]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.10387814044952393 
model_pd.l_d.mean(): -18.349708557128906 
model_pd.lagr.mean(): -18.245830535888672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6320], device='cuda:0')), ('power', tensor([-19.1959], device='cuda:0'))])
epoch£º639	 i:0 	 global-step:12780	 l-p:0.10387814044952393
epoch£º639	 i:1 	 global-step:12781	 l-p:0.18313860893249512
epoch£º639	 i:2 	 global-step:12782	 l-p:0.12682677805423737
epoch£º639	 i:3 	 global-step:12783	 l-p:0.08578447252511978
epoch£º639	 i:4 	 global-step:12784	 l-p:0.1392022967338562
epoch£º639	 i:5 	 global-step:12785	 l-p:0.06846480071544647
epoch£º639	 i:6 	 global-step:12786	 l-p:0.13967543840408325
epoch£º639	 i:7 	 global-step:12787	 l-p:0.1400814652442932
epoch£º639	 i:8 	 global-step:12788	 l-p:4.943779945373535
epoch£º639	 i:9 	 global-step:12789	 l-p:0.13582965731620789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1424, 4.9665, 4.9159],
        [5.1424, 4.9627, 4.8264],
        [5.1424, 5.1424, 5.1424],
        [5.1424, 5.1250, 5.1399]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 0.09712111949920654 
model_pd.l_d.mean(): -20.042505264282227 
model_pd.lagr.mean(): -19.945384979248047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5193], device='cuda:0')), ('power', tensor([-20.7920], device='cuda:0'))])
epoch£º640	 i:0 	 global-step:12800	 l-p:0.09712111949920654
epoch£º640	 i:1 	 global-step:12801	 l-p:0.1404433250427246
epoch£º640	 i:2 	 global-step:12802	 l-p:-0.7177914381027222
epoch£º640	 i:3 	 global-step:12803	 l-p:0.08877742290496826
epoch£º640	 i:4 	 global-step:12804	 l-p:0.1653713583946228
epoch£º640	 i:5 	 global-step:12805	 l-p:0.11779745668172836
epoch£º640	 i:6 	 global-step:12806	 l-p:0.125368133187294
epoch£º640	 i:7 	 global-step:12807	 l-p:0.12061529606580734
epoch£º640	 i:8 	 global-step:12808	 l-p:0.12608571350574493
epoch£º640	 i:9 	 global-step:12809	 l-p:0.1379086971282959
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1350, 5.1336, 5.1350],
        [5.1350, 5.0999, 4.8586],
        [5.1350, 5.1350, 5.1350],
        [5.1350, 5.0755, 5.1131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): 0.0037589643616229296 
model_pd.l_d.mean(): -20.635093688964844 
model_pd.lagr.mean(): -20.63133430480957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4450], device='cuda:0')), ('power', tensor([-21.3152], device='cuda:0'))])
epoch£º641	 i:0 	 global-step:12820	 l-p:0.0037589643616229296
epoch£º641	 i:1 	 global-step:12821	 l-p:0.09035148471593857
epoch£º641	 i:2 	 global-step:12822	 l-p:0.10605257004499435
epoch£º641	 i:3 	 global-step:12823	 l-p:0.10803183168172836
epoch£º641	 i:4 	 global-step:12824	 l-p:0.1494978815317154
epoch£º641	 i:5 	 global-step:12825	 l-p:0.1754147708415985
epoch£º641	 i:6 	 global-step:12826	 l-p:0.13361725211143494
epoch£º641	 i:7 	 global-step:12827	 l-p:-0.04683440923690796
epoch£º641	 i:8 	 global-step:12828	 l-p:0.13314102590084076
epoch£º641	 i:9 	 global-step:12829	 l-p:0.13566197454929352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1041, 5.1039, 5.1041],
        [5.1041, 4.9285, 4.8913],
        [5.1041, 5.1041, 5.1041],
        [5.1041, 5.5743, 5.5890]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): 0.005201623309403658 
model_pd.l_d.mean(): -19.197221755981445 
model_pd.lagr.mean(): -19.192020416259766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5664], device='cuda:0')), ('power', tensor([-19.9856], device='cuda:0'))])
epoch£º642	 i:0 	 global-step:12840	 l-p:0.005201623309403658
epoch£º642	 i:1 	 global-step:12841	 l-p:0.1579660177230835
epoch£º642	 i:2 	 global-step:12842	 l-p:0.1873519867658615
epoch£º642	 i:3 	 global-step:12843	 l-p:0.1306523233652115
epoch£º642	 i:4 	 global-step:12844	 l-p:0.0966467633843422
epoch£º642	 i:5 	 global-step:12845	 l-p:0.14696219563484192
epoch£º642	 i:6 	 global-step:12846	 l-p:0.12922002375125885
epoch£º642	 i:7 	 global-step:12847	 l-p:0.11428101360797882
epoch£º642	 i:8 	 global-step:12848	 l-p:0.09804786741733551
epoch£º642	 i:9 	 global-step:12849	 l-p:0.18538224697113037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0410, 4.9743, 5.0147],
        [5.0410, 5.0410, 5.0410],
        [5.0410, 4.9102, 4.6765],
        [5.0410, 5.0410, 5.0410]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.10397513210773468 
model_pd.l_d.mean(): -19.043392181396484 
model_pd.lagr.mean(): -18.939416885375977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5623], device='cuda:0')), ('power', tensor([-19.8260], device='cuda:0'))])
epoch£º643	 i:0 	 global-step:12860	 l-p:0.10397513210773468
epoch£º643	 i:1 	 global-step:12861	 l-p:0.12235856801271439
epoch£º643	 i:2 	 global-step:12862	 l-p:0.13251914083957672
epoch£º643	 i:3 	 global-step:12863	 l-p:0.1094888374209404
epoch£º643	 i:4 	 global-step:12864	 l-p:0.23546603322029114
epoch£º643	 i:5 	 global-step:12865	 l-p:0.10302125662565231
epoch£º643	 i:6 	 global-step:12866	 l-p:0.18421053886413574
epoch£º643	 i:7 	 global-step:12867	 l-p:0.12331289798021317
epoch£º643	 i:8 	 global-step:12868	 l-p:0.13720692694187164
epoch£º643	 i:9 	 global-step:12869	 l-p:0.1563863307237625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4607,  0.3558,  1.0000,  0.2748,
          1.0000,  0.7724, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1271,  0.0639,  1.0000,  0.0321,
          1.0000,  0.5028, 31.6228]], device='cuda:0')
 pt:tensor([[5.0630, 4.8716, 4.7785],
        [5.0630, 4.9352, 4.7020],
        [5.0630, 4.8825, 4.8422],
        [5.0630, 4.9645, 5.0070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): 0.14980170130729675 
model_pd.l_d.mean(): -20.281831741333008 
model_pd.lagr.mean(): -20.132030487060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4996], device='cuda:0')), ('power', tensor([-21.0139], device='cuda:0'))])
epoch£º644	 i:0 	 global-step:12880	 l-p:0.14980170130729675
epoch£º644	 i:1 	 global-step:12881	 l-p:0.1808495670557022
epoch£º644	 i:2 	 global-step:12882	 l-p:0.14582319557666779
epoch£º644	 i:3 	 global-step:12883	 l-p:0.12776371836662292
epoch£º644	 i:4 	 global-step:12884	 l-p:0.1594659835100174
epoch£º644	 i:5 	 global-step:12885	 l-p:0.14711043238639832
epoch£º644	 i:6 	 global-step:12886	 l-p:0.1330810934305191
epoch£º644	 i:7 	 global-step:12887	 l-p:0.03661859408020973
epoch£º644	 i:8 	 global-step:12888	 l-p:0.06872784346342087
epoch£º644	 i:9 	 global-step:12889	 l-p:0.06515924632549286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1177, 5.0603, 5.0975],
        [5.1177, 4.9520, 4.9380],
        [5.1177, 5.0935, 4.8507],
        [5.1177, 5.1145, 5.1176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.14511264860630035 
model_pd.l_d.mean(): -20.062829971313477 
model_pd.lagr.mean(): -19.91771697998047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4988], device='cuda:0')), ('power', tensor([-20.7916], device='cuda:0'))])
epoch£º645	 i:0 	 global-step:12900	 l-p:0.14511264860630035
epoch£º645	 i:1 	 global-step:12901	 l-p:-0.05628171190619469
epoch£º645	 i:2 	 global-step:12902	 l-p:0.11753617972135544
epoch£º645	 i:3 	 global-step:12903	 l-p:0.11244470626115799
epoch£º645	 i:4 	 global-step:12904	 l-p:0.12231581658124924
epoch£º645	 i:5 	 global-step:12905	 l-p:0.02681531384587288
epoch£º645	 i:6 	 global-step:12906	 l-p:0.14243392646312714
epoch£º645	 i:7 	 global-step:12907	 l-p:0.08350624144077301
epoch£º645	 i:8 	 global-step:12908	 l-p:0.10137630254030228
epoch£º645	 i:9 	 global-step:12909	 l-p:0.1258309930562973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9387,  0.9192,  1.0000,  0.9000,
          1.0000,  0.9792, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228]], device='cuda:0')
 pt:tensor([[5.1854, 5.1134, 4.8741],
        [5.1854, 5.6769, 5.7025],
        [5.1854, 5.7171, 5.7721],
        [5.1854, 5.0166, 4.9860]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): 0.10250810533761978 
model_pd.l_d.mean(): -19.39670753479004 
model_pd.lagr.mean(): -19.294198989868164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5252], device='cuda:0')), ('power', tensor([-20.1452], device='cuda:0'))])
epoch£º646	 i:0 	 global-step:12920	 l-p:0.10250810533761978
epoch£º646	 i:1 	 global-step:12921	 l-p:0.15905804932117462
epoch£º646	 i:2 	 global-step:12922	 l-p:0.12464804202318192
epoch£º646	 i:3 	 global-step:12923	 l-p:0.13564348220825195
epoch£º646	 i:4 	 global-step:12924	 l-p:-1.0147385597229004
epoch£º646	 i:5 	 global-step:12925	 l-p:0.04808691516518593
epoch£º646	 i:6 	 global-step:12926	 l-p:0.12064182758331299
epoch£º646	 i:7 	 global-step:12927	 l-p:0.12397737056016922
epoch£º646	 i:8 	 global-step:12928	 l-p:0.13430428504943848
epoch£º646	 i:9 	 global-step:12929	 l-p:0.22015979886054993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1619, 5.1619, 5.1619],
        [5.1619, 5.1616, 5.1619],
        [5.1619, 5.0436, 4.8156],
        [5.1619, 5.1616, 5.1619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): 0.13664232194423676 
model_pd.l_d.mean(): -20.069576263427734 
model_pd.lagr.mean(): -19.932933807373047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5030], device='cuda:0')), ('power', tensor([-20.8028], device='cuda:0'))])
epoch£º647	 i:0 	 global-step:12940	 l-p:0.13664232194423676
epoch£º647	 i:1 	 global-step:12941	 l-p:0.13118447363376617
epoch£º647	 i:2 	 global-step:12942	 l-p:0.12405084073543549
epoch£º647	 i:3 	 global-step:12943	 l-p:0.12073193490505219
epoch£º647	 i:4 	 global-step:12944	 l-p:0.11064343899488449
epoch£º647	 i:5 	 global-step:12945	 l-p:0.12884680926799774
epoch£º647	 i:6 	 global-step:12946	 l-p:-0.0039964341558516026
epoch£º647	 i:7 	 global-step:12947	 l-p:0.09965436905622482
epoch£º647	 i:8 	 global-step:12948	 l-p:0.14775770902633667
epoch£º647	 i:9 	 global-step:12949	 l-p:0.09312692284584045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0819, 5.0535, 4.8076],
        [5.0819, 5.0818, 5.0819],
        [5.0819, 5.0442, 5.0726],
        [5.0819, 5.0773, 5.0816]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): 0.11918710172176361 
model_pd.l_d.mean(): -20.786922454833984 
model_pd.lagr.mean(): -20.667736053466797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4379], device='cuda:0')), ('power', tensor([-21.4615], device='cuda:0'))])
epoch£º648	 i:0 	 global-step:12960	 l-p:0.11918710172176361
epoch£º648	 i:1 	 global-step:12961	 l-p:0.12779346108436584
epoch£º648	 i:2 	 global-step:12962	 l-p:0.17541027069091797
epoch£º648	 i:3 	 global-step:12963	 l-p:0.14428722858428955
epoch£º648	 i:4 	 global-step:12964	 l-p:0.18270474672317505
epoch£º648	 i:5 	 global-step:12965	 l-p:0.14819513261318207
epoch£º648	 i:6 	 global-step:12966	 l-p:0.1321987509727478
epoch£º648	 i:7 	 global-step:12967	 l-p:0.10258811712265015
epoch£º648	 i:8 	 global-step:12968	 l-p:0.12083134055137634
epoch£º648	 i:9 	 global-step:12969	 l-p:0.09906572103500366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0674, 5.0303, 5.0584],
        [5.0674, 5.0644, 5.0672],
        [5.0674, 5.0651, 5.0673],
        [5.0674, 4.8856, 4.8452]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.12290952354669571 
model_pd.l_d.mean(): -20.08905601501465 
model_pd.lagr.mean(): -19.96614646911621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4766], device='cuda:0')), ('power', tensor([-20.7955], device='cuda:0'))])
epoch£º649	 i:0 	 global-step:12980	 l-p:0.12290952354669571
epoch£º649	 i:1 	 global-step:12981	 l-p:0.1219746470451355
epoch£º649	 i:2 	 global-step:12982	 l-p:0.13074122369289398
epoch£º649	 i:3 	 global-step:12983	 l-p:0.15210318565368652
epoch£º649	 i:4 	 global-step:12984	 l-p:0.18740913271903992
epoch£º649	 i:5 	 global-step:12985	 l-p:0.13449522852897644
epoch£º649	 i:6 	 global-step:12986	 l-p:0.13339194655418396
epoch£º649	 i:7 	 global-step:12987	 l-p:0.15581050515174866
epoch£º649	 i:8 	 global-step:12988	 l-p:0.06586100906133652
epoch£º649	 i:9 	 global-step:12989	 l-p:0.22840508818626404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0340, 4.9398, 4.9836],
        [5.0340, 4.9859, 5.0198],
        [5.0340, 4.8680, 4.6566],
        [5.0340, 4.8838, 4.9010]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 0.11101038008928299 
model_pd.l_d.mean(): -20.732101440429688 
model_pd.lagr.mean(): -20.621091842651367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4542], device='cuda:0')), ('power', tensor([-21.4227], device='cuda:0'))])
epoch£º650	 i:0 	 global-step:13000	 l-p:0.11101038008928299
epoch£º650	 i:1 	 global-step:13001	 l-p:0.11354417353868484
epoch£º650	 i:2 	 global-step:13002	 l-p:0.1682075411081314
epoch£º650	 i:3 	 global-step:13003	 l-p:0.06792061030864716
epoch£º650	 i:4 	 global-step:13004	 l-p:0.14011935889720917
epoch£º650	 i:5 	 global-step:13005	 l-p:0.15492947399616241
epoch£º650	 i:6 	 global-step:13006	 l-p:0.1544262170791626
epoch£º650	 i:7 	 global-step:13007	 l-p:0.15451355278491974
epoch£º650	 i:8 	 global-step:13008	 l-p:0.14917021989822388
epoch£º650	 i:9 	 global-step:13009	 l-p:0.1545599102973938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0725, 5.0725, 5.0725],
        [5.0725, 5.5066, 5.4954],
        [5.0725, 5.0725, 5.0725],
        [5.0725, 4.9024, 4.8894]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.15924251079559326 
model_pd.l_d.mean(): -20.30360984802246 
model_pd.lagr.mean(): -20.144367218017578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4966], device='cuda:0')), ('power', tensor([-21.0328], device='cuda:0'))])
epoch£º651	 i:0 	 global-step:13020	 l-p:0.15924251079559326
epoch£º651	 i:1 	 global-step:13021	 l-p:0.11849115043878555
epoch£º651	 i:2 	 global-step:13022	 l-p:0.08451137691736221
epoch£º651	 i:3 	 global-step:13023	 l-p:0.11810433864593506
epoch£º651	 i:4 	 global-step:13024	 l-p:0.12826725840568542
epoch£º651	 i:5 	 global-step:13025	 l-p:0.13671663403511047
epoch£º651	 i:6 	 global-step:13026	 l-p:0.16048450767993927
epoch£º651	 i:7 	 global-step:13027	 l-p:0.1824672967195511
epoch£º651	 i:8 	 global-step:13028	 l-p:0.2369443029165268
epoch£º651	 i:9 	 global-step:13029	 l-p:0.09952960908412933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0383, 4.9260, 4.9670],
        [5.0383, 5.0266, 5.0371],
        [5.0383, 5.0383, 5.0383],
        [5.0383, 5.0370, 5.0383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.10597842931747437 
model_pd.l_d.mean(): -20.56391143798828 
model_pd.lagr.mean(): -20.45793342590332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4513], device='cuda:0')), ('power', tensor([-21.2497], device='cuda:0'))])
epoch£º652	 i:0 	 global-step:13040	 l-p:0.10597842931747437
epoch£º652	 i:1 	 global-step:13041	 l-p:0.1594119369983673
epoch£º652	 i:2 	 global-step:13042	 l-p:0.12666143476963043
epoch£º652	 i:3 	 global-step:13043	 l-p:0.15659542381763458
epoch£º652	 i:4 	 global-step:13044	 l-p:0.08208636939525604
epoch£º652	 i:5 	 global-step:13045	 l-p:0.1812465488910675
epoch£º652	 i:6 	 global-step:13046	 l-p:0.1528034657239914
epoch£º652	 i:7 	 global-step:13047	 l-p:0.15829645097255707
epoch£º652	 i:8 	 global-step:13048	 l-p:0.1261710226535797
epoch£º652	 i:9 	 global-step:13049	 l-p:0.1031704694032669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1157, 5.2411, 5.0428],
        [5.1157, 5.0165, 4.7736],
        [5.1157, 5.2489, 5.0543],
        [5.1157, 5.0492, 5.0894]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.1678113490343094 
model_pd.l_d.mean(): -20.18355941772461 
model_pd.lagr.mean(): -20.015748977661133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4779], device='cuda:0')), ('power', tensor([-20.8923], device='cuda:0'))])
epoch£º653	 i:0 	 global-step:13060	 l-p:0.1678113490343094
epoch£º653	 i:1 	 global-step:13061	 l-p:0.11964646726846695
epoch£º653	 i:2 	 global-step:13062	 l-p:0.14808210730552673
epoch£º653	 i:3 	 global-step:13063	 l-p:0.12540367245674133
epoch£º653	 i:4 	 global-step:13064	 l-p:0.14127318561077118
epoch£º653	 i:5 	 global-step:13065	 l-p:0.023379316553473473
epoch£º653	 i:6 	 global-step:13066	 l-p:0.08553548902273178
epoch£º653	 i:7 	 global-step:13067	 l-p:0.13114315271377563
epoch£º653	 i:8 	 global-step:13068	 l-p:0.11863912642002106
epoch£º653	 i:9 	 global-step:13069	 l-p:0.06994451582431793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1348, 5.2434, 5.0375],
        [5.1348, 4.9784, 4.9823],
        [5.1348, 5.1347, 5.1348],
        [5.1348, 4.9609, 4.9332]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): 0.12427791208028793 
model_pd.l_d.mean(): -19.6640682220459 
model_pd.lagr.mean(): -19.539791107177734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4902], device='cuda:0')), ('power', tensor([-20.3798], device='cuda:0'))])
epoch£º654	 i:0 	 global-step:13080	 l-p:0.12427791208028793
epoch£º654	 i:1 	 global-step:13081	 l-p:0.05865491181612015
epoch£º654	 i:2 	 global-step:13082	 l-p:0.14234556257724762
epoch£º654	 i:3 	 global-step:13083	 l-p:0.0873885527253151
epoch£º654	 i:4 	 global-step:13084	 l-p:0.13821732997894287
epoch£º654	 i:5 	 global-step:13085	 l-p:0.07577405124902725
epoch£º654	 i:6 	 global-step:13086	 l-p:0.12894834578037262
epoch£º654	 i:7 	 global-step:13087	 l-p:0.1136469841003418
epoch£º654	 i:8 	 global-step:13088	 l-p:0.33813661336898804
epoch£º654	 i:9 	 global-step:13089	 l-p:0.1374947428703308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4043,  0.2990,  1.0000,  0.2211,
          1.0000,  0.7394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]], device='cuda:0')
 pt:tensor([[5.1573, 4.9724, 4.8324],
        [5.1573, 5.0002, 4.7969],
        [5.1573, 4.9798, 4.9389],
        [5.1573, 5.0304, 5.0616]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.11674722284078598 
model_pd.l_d.mean(): -20.546354293823242 
model_pd.lagr.mean(): -20.429607391357422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4328], device='cuda:0')), ('power', tensor([-21.2130], device='cuda:0'))])
epoch£º655	 i:0 	 global-step:13100	 l-p:0.11674722284078598
epoch£º655	 i:1 	 global-step:13101	 l-p:0.0668146163225174
epoch£º655	 i:2 	 global-step:13102	 l-p:0.14377841353416443
epoch£º655	 i:3 	 global-step:13103	 l-p:0.178066685795784
epoch£º655	 i:4 	 global-step:13104	 l-p:0.05540042743086815
epoch£º655	 i:5 	 global-step:13105	 l-p:0.13025452196598053
epoch£º655	 i:6 	 global-step:13106	 l-p:0.13124877214431763
epoch£º655	 i:7 	 global-step:13107	 l-p:0.10707652568817139
epoch£º655	 i:8 	 global-step:13108	 l-p:-0.061212681233882904
epoch£º655	 i:9 	 global-step:13109	 l-p:0.12367981672286987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1167, 5.0962, 5.1135],
        [5.1167, 5.0842, 5.1096],
        [5.1167, 5.1019, 5.1149],
        [5.1167, 5.0060, 4.7651]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.11834035813808441 
model_pd.l_d.mean(): -20.59989356994629 
model_pd.lagr.mean(): -20.48155403137207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4340], device='cuda:0')), ('power', tensor([-21.2684], device='cuda:0'))])
epoch£º656	 i:0 	 global-step:13120	 l-p:0.11834035813808441
epoch£º656	 i:1 	 global-step:13121	 l-p:0.1310344636440277
epoch£º656	 i:2 	 global-step:13122	 l-p:0.05877251923084259
epoch£º656	 i:3 	 global-step:13123	 l-p:0.1365644931793213
epoch£º656	 i:4 	 global-step:13124	 l-p:0.14009545743465424
epoch£º656	 i:5 	 global-step:13125	 l-p:0.2251037359237671
epoch£º656	 i:6 	 global-step:13126	 l-p:0.14769084751605988
epoch£º656	 i:7 	 global-step:13127	 l-p:0.0628734603524208
epoch£º656	 i:8 	 global-step:13128	 l-p:0.1452527642250061
epoch£º656	 i:9 	 global-step:13129	 l-p:0.12189658731222153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3156,  0.2149,  1.0000,  0.1463,
          1.0000,  0.6809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[5.0779, 4.8988, 4.8703],
        [5.0779, 4.8821, 4.7588],
        [5.0779, 5.0592, 4.8116],
        [5.0779, 4.9765, 4.7289]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.1419719159603119 
model_pd.l_d.mean(): -20.441946029663086 
model_pd.lagr.mean(): -20.29997444152832 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4816], device='cuda:0')), ('power', tensor([-21.1573], device='cuda:0'))])
epoch£º657	 i:0 	 global-step:13140	 l-p:0.1419719159603119
epoch£º657	 i:1 	 global-step:13141	 l-p:0.09338053315877914
epoch£º657	 i:2 	 global-step:13142	 l-p:0.1752815544605255
epoch£º657	 i:3 	 global-step:13143	 l-p:0.14334264397621155
epoch£º657	 i:4 	 global-step:13144	 l-p:0.12069501727819443
epoch£º657	 i:5 	 global-step:13145	 l-p:0.10219969600439072
epoch£º657	 i:6 	 global-step:13146	 l-p:0.140081986784935
epoch£º657	 i:7 	 global-step:13147	 l-p:0.125398650765419
epoch£º657	 i:8 	 global-step:13148	 l-p:0.14379970729351044
epoch£º657	 i:9 	 global-step:13149	 l-p:0.030749408528208733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1140, 5.1140, 5.1140],
        [5.1140, 4.9248, 4.7760],
        [5.1140, 4.9278, 4.8716],
        [5.1140, 4.9239, 4.8485]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): 0.07791624963283539 
model_pd.l_d.mean(): -20.22674560546875 
model_pd.lagr.mean(): -20.148828506469727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4838], device='cuda:0')), ('power', tensor([-20.9420], device='cuda:0'))])
epoch£º658	 i:0 	 global-step:13160	 l-p:0.07791624963283539
epoch£º658	 i:1 	 global-step:13161	 l-p:0.1505713015794754
epoch£º658	 i:2 	 global-step:13162	 l-p:0.1643005758523941
epoch£º658	 i:3 	 global-step:13163	 l-p:0.10687617212533951
epoch£º658	 i:4 	 global-step:13164	 l-p:0.14029648900032043
epoch£º658	 i:5 	 global-step:13165	 l-p:0.10864150524139404
epoch£º658	 i:6 	 global-step:13166	 l-p:0.008937780745327473
epoch£º658	 i:7 	 global-step:13167	 l-p:0.1277751475572586
epoch£º658	 i:8 	 global-step:13168	 l-p:0.14910651743412018
epoch£º658	 i:9 	 global-step:13169	 l-p:0.12434615939855576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1219, 5.0954, 4.8481],
        [5.1219, 5.1216, 5.1219],
        [5.1219, 4.9294, 4.8133],
        [5.1219, 5.1209, 5.1219]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.12203933298587799 
model_pd.l_d.mean(): -20.481258392333984 
model_pd.lagr.mean(): -20.35921859741211 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4221], device='cuda:0')), ('power', tensor([-21.1363], device='cuda:0'))])
epoch£º659	 i:0 	 global-step:13180	 l-p:0.12203933298587799
epoch£º659	 i:1 	 global-step:13181	 l-p:0.10996831953525543
epoch£º659	 i:2 	 global-step:13182	 l-p:0.16193737089633942
epoch£º659	 i:3 	 global-step:13183	 l-p:0.13292768597602844
epoch£º659	 i:4 	 global-step:13184	 l-p:0.11773839592933655
epoch£º659	 i:5 	 global-step:13185	 l-p:0.13173922896385193
epoch£º659	 i:6 	 global-step:13186	 l-p:0.10861581563949585
epoch£º659	 i:7 	 global-step:13187	 l-p:0.1284485161304474
epoch£º659	 i:8 	 global-step:13188	 l-p:0.19577300548553467
epoch£º659	 i:9 	 global-step:13189	 l-p:0.13246287405490875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0433, 5.2204, 5.0478],
        [5.0433, 4.9239, 4.9634],
        [5.0433, 5.0192, 4.7683],
        [5.0433, 5.0432, 5.0433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.14465449750423431 
model_pd.l_d.mean(): -19.68463134765625 
model_pd.lagr.mean(): -19.539976119995117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5659], device='cuda:0')), ('power', tensor([-20.4778], device='cuda:0'))])
epoch£º660	 i:0 	 global-step:13200	 l-p:0.14465449750423431
epoch£º660	 i:1 	 global-step:13201	 l-p:0.16049045324325562
epoch£º660	 i:2 	 global-step:13202	 l-p:0.1376262903213501
epoch£º660	 i:3 	 global-step:13203	 l-p:0.1266772300004959
epoch£º660	 i:4 	 global-step:13204	 l-p:0.16658073663711548
epoch£º660	 i:5 	 global-step:13205	 l-p:0.13128893077373505
epoch£º660	 i:6 	 global-step:13206	 l-p:0.11454133689403534
epoch£º660	 i:7 	 global-step:13207	 l-p:0.1242063120007515
epoch£º660	 i:8 	 global-step:13208	 l-p:0.12477273494005203
epoch£º660	 i:9 	 global-step:13209	 l-p:0.20931632816791534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0511, 5.0502, 5.0511],
        [5.0511, 4.9496, 4.9935],
        [5.0511, 5.0242, 5.0461],
        [5.0511, 4.9041, 4.9267]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.13777317106723785 
model_pd.l_d.mean(): -19.972930908203125 
model_pd.lagr.mean(): -19.83515739440918 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4896], device='cuda:0')), ('power', tensor([-20.6913], device='cuda:0'))])
epoch£º661	 i:0 	 global-step:13220	 l-p:0.13777317106723785
epoch£º661	 i:1 	 global-step:13221	 l-p:0.13262605667114258
epoch£º661	 i:2 	 global-step:13222	 l-p:0.15384060144424438
epoch£º661	 i:3 	 global-step:13223	 l-p:0.1927991509437561
epoch£º661	 i:4 	 global-step:13224	 l-p:0.11384525150060654
epoch£º661	 i:5 	 global-step:13225	 l-p:0.11024701595306396
epoch£º661	 i:6 	 global-step:13226	 l-p:0.1349552571773529
epoch£º661	 i:7 	 global-step:13227	 l-p:0.16967159509658813
epoch£º661	 i:8 	 global-step:13228	 l-p:0.10834154486656189
epoch£º661	 i:9 	 global-step:13229	 l-p:0.18644674122333527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0520, 5.0520, 5.0520],
        [5.0520, 5.0520, 5.0520],
        [5.0520, 5.0299, 5.0484],
        [5.0520, 4.8772, 4.8651]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.13597872853279114 
model_pd.l_d.mean(): -20.905906677246094 
model_pd.lagr.mean(): -20.769927978515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4064], device='cuda:0')), ('power', tensor([-21.5495], device='cuda:0'))])
epoch£º662	 i:0 	 global-step:13240	 l-p:0.13597872853279114
epoch£º662	 i:1 	 global-step:13241	 l-p:0.1033715009689331
epoch£º662	 i:2 	 global-step:13242	 l-p:0.14980588853359222
epoch£º662	 i:3 	 global-step:13243	 l-p:0.09954851865768433
epoch£º662	 i:4 	 global-step:13244	 l-p:0.10740956664085388
epoch£º662	 i:5 	 global-step:13245	 l-p:0.12831391394138336
epoch£º662	 i:6 	 global-step:13246	 l-p:0.15626758337020874
epoch£º662	 i:7 	 global-step:13247	 l-p:0.1529655009508133
epoch£º662	 i:8 	 global-step:13248	 l-p:0.14420472085475922
epoch£º662	 i:9 	 global-step:13249	 l-p:0.14750763773918152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0924, 5.2129, 5.0104],
        [5.0924, 5.0321, 5.0709],
        [5.0924, 4.8953, 4.7708],
        [5.0924, 5.0795, 5.0910]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.12807297706604004 
model_pd.l_d.mean(): -20.588600158691406 
model_pd.lagr.mean(): -20.460527420043945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4287], device='cuda:0')), ('power', tensor([-21.2516], device='cuda:0'))])
epoch£º663	 i:0 	 global-step:13260	 l-p:0.12807297706604004
epoch£º663	 i:1 	 global-step:13261	 l-p:0.11562825739383698
epoch£º663	 i:2 	 global-step:13262	 l-p:0.08219103515148163
epoch£º663	 i:3 	 global-step:13263	 l-p:0.10773931443691254
epoch£º663	 i:4 	 global-step:13264	 l-p:0.13529439270496368
epoch£º663	 i:5 	 global-step:13265	 l-p:0.19737981259822845
epoch£º663	 i:6 	 global-step:13266	 l-p:0.169921413064003
epoch£º663	 i:7 	 global-step:13267	 l-p:0.10136878490447998
epoch£º663	 i:8 	 global-step:13268	 l-p:0.10450810939073563
epoch£º663	 i:9 	 global-step:13269	 l-p:0.12346887588500977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1025, 5.1025, 5.1026],
        [5.1025, 4.9092, 4.7597],
        [5.1025, 5.1008, 5.1025],
        [5.1025, 4.9375, 4.9363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.1304151713848114 
model_pd.l_d.mean(): -20.381986618041992 
model_pd.lagr.mean(): -20.251571655273438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4707], device='cuda:0')), ('power', tensor([-21.0856], device='cuda:0'))])
epoch£º664	 i:0 	 global-step:13280	 l-p:0.1304151713848114
epoch£º664	 i:1 	 global-step:13281	 l-p:0.15885069966316223
epoch£º664	 i:2 	 global-step:13282	 l-p:0.12536226212978363
epoch£º664	 i:3 	 global-step:13283	 l-p:0.11180585622787476
epoch£º664	 i:4 	 global-step:13284	 l-p:0.12900708615779877
epoch£º664	 i:5 	 global-step:13285	 l-p:0.12735915184020996
epoch£º664	 i:6 	 global-step:13286	 l-p:0.16459406912326813
epoch£º664	 i:7 	 global-step:13287	 l-p:0.12594571709632874
epoch£º664	 i:8 	 global-step:13288	 l-p:0.17039795219898224
epoch£º664	 i:9 	 global-step:13289	 l-p:0.05591357499361038
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0618, 5.0595, 5.0617],
        [5.0618, 5.0594, 5.0617],
        [5.0618, 4.8619, 4.7211],
        [5.0618, 4.9876, 5.0305]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.10161472111940384 
model_pd.l_d.mean(): -20.409425735473633 
model_pd.lagr.mean(): -20.307811737060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4965], device='cuda:0')), ('power', tensor([-21.1396], device='cuda:0'))])
epoch£º665	 i:0 	 global-step:13300	 l-p:0.10161472111940384
epoch£º665	 i:1 	 global-step:13301	 l-p:0.13809135556221008
epoch£º665	 i:2 	 global-step:13302	 l-p:0.18040628731250763
epoch£º665	 i:3 	 global-step:13303	 l-p:0.13025544583797455
epoch£º665	 i:4 	 global-step:13304	 l-p:0.1253357082605362
epoch£º665	 i:5 	 global-step:13305	 l-p:0.13668197393417358
epoch£º665	 i:6 	 global-step:13306	 l-p:0.13712774217128754
epoch£º665	 i:7 	 global-step:13307	 l-p:0.17922809720039368
epoch£º665	 i:8 	 global-step:13308	 l-p:0.10153604298830032
epoch£º665	 i:9 	 global-step:13309	 l-p:0.14454424381256104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0569, 5.0569, 5.0569],
        [5.0569, 4.9688, 4.7130],
        [5.0569, 4.9851, 4.7284],
        [5.0569, 4.8948, 4.9031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.12458880990743637 
model_pd.l_d.mean(): -18.880338668823242 
model_pd.lagr.mean(): -18.75575065612793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5546], device='cuda:0')), ('power', tensor([-19.6532], device='cuda:0'))])
epoch£º666	 i:0 	 global-step:13320	 l-p:0.12458880990743637
epoch£º666	 i:1 	 global-step:13321	 l-p:0.09377643465995789
epoch£º666	 i:2 	 global-step:13322	 l-p:0.13264775276184082
epoch£º666	 i:3 	 global-step:13323	 l-p:0.15549926459789276
epoch£º666	 i:4 	 global-step:13324	 l-p:0.10696139931678772
epoch£º666	 i:5 	 global-step:13325	 l-p:0.16103820502758026
epoch£º666	 i:6 	 global-step:13326	 l-p:0.15979483723640442
epoch£º666	 i:7 	 global-step:13327	 l-p:0.13793736696243286
epoch£º666	 i:8 	 global-step:13328	 l-p:0.1427777111530304
epoch£º666	 i:9 	 global-step:13329	 l-p:0.2040894776582718
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2653,  0.1705,  1.0000,  0.1095,
          1.0000,  0.6426, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]], device='cuda:0')
 pt:tensor([[5.0537, 5.5326, 5.5508],
        [5.0537, 4.8563, 4.7905],
        [5.0537, 5.2944, 5.1557],
        [5.0537, 4.8543, 4.6961]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.12915988266468048 
model_pd.l_d.mean(): -20.273998260498047 
model_pd.lagr.mean(): -20.144838333129883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5121], device='cuda:0')), ('power', tensor([-21.0187], device='cuda:0'))])
epoch£º667	 i:0 	 global-step:13340	 l-p:0.12915988266468048
epoch£º667	 i:1 	 global-step:13341	 l-p:0.11503557115793228
epoch£º667	 i:2 	 global-step:13342	 l-p:0.14075802266597748
epoch£º667	 i:3 	 global-step:13343	 l-p:0.1753711700439453
epoch£º667	 i:4 	 global-step:13344	 l-p:0.16386525332927704
epoch£º667	 i:5 	 global-step:13345	 l-p:0.17763099074363708
epoch£º667	 i:6 	 global-step:13346	 l-p:0.11832007765769958
epoch£º667	 i:7 	 global-step:13347	 l-p:0.11107492446899414
epoch£º667	 i:8 	 global-step:13348	 l-p:0.13720612227916718
epoch£º667	 i:9 	 global-step:13349	 l-p:0.11367560178041458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0664, 5.0367, 5.0605],
        [5.0664, 5.0664, 5.0664],
        [5.0664, 4.8808, 4.8493],
        [5.0664, 4.9586, 4.7054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.11666294932365417 
model_pd.l_d.mean(): -19.97018814086914 
model_pd.lagr.mean(): -19.853525161743164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5179], device='cuda:0')), ('power', tensor([-20.7175], device='cuda:0'))])
epoch£º668	 i:0 	 global-step:13360	 l-p:0.11666294932365417
epoch£º668	 i:1 	 global-step:13361	 l-p:0.14087441563606262
epoch£º668	 i:2 	 global-step:13362	 l-p:0.12844687700271606
epoch£º668	 i:3 	 global-step:13363	 l-p:0.15725935995578766
epoch£º668	 i:4 	 global-step:13364	 l-p:0.12086201459169388
epoch£º668	 i:5 	 global-step:13365	 l-p:0.1612463891506195
epoch£º668	 i:6 	 global-step:13366	 l-p:0.11909741908311844
epoch£º668	 i:7 	 global-step:13367	 l-p:0.1662684977054596
epoch£º668	 i:8 	 global-step:13368	 l-p:0.11274261027574539
epoch£º668	 i:9 	 global-step:13369	 l-p:0.25297975540161133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0344, 4.8777, 4.8952],
        [5.0344, 5.0343, 5.0344],
        [5.0344, 5.0344, 5.0344],
        [5.0344, 4.8348, 4.6628]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.13700872659683228 
model_pd.l_d.mean(): -20.1052188873291 
model_pd.lagr.mean(): -19.968210220336914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4369], device='cuda:0')), ('power', tensor([-20.7713], device='cuda:0'))])
epoch£º669	 i:0 	 global-step:13380	 l-p:0.13700872659683228
epoch£º669	 i:1 	 global-step:13381	 l-p:0.16618625819683075
epoch£º669	 i:2 	 global-step:13382	 l-p:0.134161576628685
epoch£º669	 i:3 	 global-step:13383	 l-p:0.16090846061706543
epoch£º669	 i:4 	 global-step:13384	 l-p:0.1699589490890503
epoch£º669	 i:5 	 global-step:13385	 l-p:0.11113572865724564
epoch£º669	 i:6 	 global-step:13386	 l-p:0.15328633785247803
epoch£º669	 i:7 	 global-step:13387	 l-p:0.19301220774650574
epoch£º669	 i:8 	 global-step:13388	 l-p:0.18198265135288239
epoch£º669	 i:9 	 global-step:13389	 l-p:0.11115152388811111
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0478, 5.0478, 5.0478],
        [5.0478, 4.9769, 5.0194],
        [5.0478, 5.0463, 5.0477],
        [5.0478, 5.0478, 5.0478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): 0.24403858184814453 
model_pd.l_d.mean(): -20.6199893951416 
model_pd.lagr.mean(): -20.37594985961914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4821], device='cuda:0')), ('power', tensor([-21.3378], device='cuda:0'))])
epoch£º670	 i:0 	 global-step:13400	 l-p:0.24403858184814453
epoch£º670	 i:1 	 global-step:13401	 l-p:0.11724637448787689
epoch£º670	 i:2 	 global-step:13402	 l-p:0.138483926653862
epoch£º670	 i:3 	 global-step:13403	 l-p:0.1325385868549347
epoch£º670	 i:4 	 global-step:13404	 l-p:0.1510561853647232
epoch£º670	 i:5 	 global-step:13405	 l-p:0.09981470555067062
epoch£º670	 i:6 	 global-step:13406	 l-p:0.15175700187683105
epoch£º670	 i:7 	 global-step:13407	 l-p:0.12513931095600128
epoch£º670	 i:8 	 global-step:13408	 l-p:0.14399904012680054
epoch£º670	 i:9 	 global-step:13409	 l-p:0.1551501601934433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0337, 5.0228, 5.0326],
        [5.0337, 5.0300, 5.0335],
        [5.0337, 4.8379, 4.7892],
        [5.0337, 5.0299, 5.0335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.14283791184425354 
model_pd.l_d.mean(): -20.74370002746582 
model_pd.lagr.mean(): -20.600862503051758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4361], device='cuda:0')), ('power', tensor([-21.4159], device='cuda:0'))])
epoch£º671	 i:0 	 global-step:13420	 l-p:0.14283791184425354
epoch£º671	 i:1 	 global-step:13421	 l-p:0.1725049763917923
epoch£º671	 i:2 	 global-step:13422	 l-p:0.11661320179700851
epoch£º671	 i:3 	 global-step:13423	 l-p:0.16746971011161804
epoch£º671	 i:4 	 global-step:13424	 l-p:0.29321563243865967
epoch£º671	 i:5 	 global-step:13425	 l-p:0.20727340877056122
epoch£º671	 i:6 	 global-step:13426	 l-p:0.09659982472658157
epoch£º671	 i:7 	 global-step:13427	 l-p:0.12382347881793976
epoch£º671	 i:8 	 global-step:13428	 l-p:0.18437981605529785
epoch£º671	 i:9 	 global-step:13429	 l-p:0.15039099752902985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0227, 5.0013, 5.0193],
        [5.0227, 4.8773, 4.6308],
        [5.0227, 5.0227, 5.0227],
        [5.0227, 4.8344, 4.8074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.2738136053085327 
model_pd.l_d.mean(): -20.131898880004883 
model_pd.lagr.mean(): -19.85808563232422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5167], device='cuda:0')), ('power', tensor([-20.8798], device='cuda:0'))])
epoch£º672	 i:0 	 global-step:13440	 l-p:0.2738136053085327
epoch£º672	 i:1 	 global-step:13441	 l-p:0.1361389458179474
epoch£º672	 i:2 	 global-step:13442	 l-p:0.15973901748657227
epoch£º672	 i:3 	 global-step:13443	 l-p:0.15977445244789124
epoch£º672	 i:4 	 global-step:13444	 l-p:0.10725778341293335
epoch£º672	 i:5 	 global-step:13445	 l-p:0.11824077367782593
epoch£º672	 i:6 	 global-step:13446	 l-p:0.14986594021320343
epoch£º672	 i:7 	 global-step:13447	 l-p:0.13851843774318695
epoch£º672	 i:8 	 global-step:13448	 l-p:0.1201147511601448
epoch£º672	 i:9 	 global-step:13449	 l-p:0.24747906625270844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0205, 5.0202, 5.0205],
        [5.0205, 5.0205, 5.0205],
        [5.0205, 5.0028, 4.7477],
        [5.0205, 5.0099, 5.0195]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.1345621794462204 
model_pd.l_d.mean(): -20.250268936157227 
model_pd.lagr.mean(): -20.115707397460938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4693], device='cuda:0')), ('power', tensor([-20.9510], device='cuda:0'))])
epoch£º673	 i:0 	 global-step:13460	 l-p:0.1345621794462204
epoch£º673	 i:1 	 global-step:13461	 l-p:0.16566459834575653
epoch£º673	 i:2 	 global-step:13462	 l-p:0.19872665405273438
epoch£º673	 i:3 	 global-step:13463	 l-p:0.33367079496383667
epoch£º673	 i:4 	 global-step:13464	 l-p:0.1324189007282257
epoch£º673	 i:5 	 global-step:13465	 l-p:0.1586993932723999
epoch£º673	 i:6 	 global-step:13466	 l-p:0.180337056517601
epoch£º673	 i:7 	 global-step:13467	 l-p:0.09543440490961075
epoch£º673	 i:8 	 global-step:13468	 l-p:0.16008523106575012
epoch£º673	 i:9 	 global-step:13469	 l-p:0.09801419824361801
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0323, 4.9200, 4.9638],
        [5.0323, 4.9344, 4.9802],
        [5.0323, 4.8835, 4.9093],
        [5.0323, 4.8237, 4.7053]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.1717078685760498 
model_pd.l_d.mean(): -19.56159019470215 
model_pd.lagr.mean(): -19.389883041381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5597], device='cuda:0')), ('power', tensor([-20.3472], device='cuda:0'))])
epoch£º674	 i:0 	 global-step:13480	 l-p:0.1717078685760498
epoch£º674	 i:1 	 global-step:13481	 l-p:0.13261952996253967
epoch£º674	 i:2 	 global-step:13482	 l-p:0.10846687108278275
epoch£º674	 i:3 	 global-step:13483	 l-p:0.1346098631620407
epoch£º674	 i:4 	 global-step:13484	 l-p:0.07689203321933746
epoch£º674	 i:5 	 global-step:13485	 l-p:0.13799817860126495
epoch£º674	 i:6 	 global-step:13486	 l-p:0.22411559522151947
epoch£º674	 i:7 	 global-step:13487	 l-p:0.16812025010585785
epoch£º674	 i:8 	 global-step:13488	 l-p:0.1318221539258957
epoch£º674	 i:9 	 global-step:13489	 l-p:0.13976846635341644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0932, 4.9267, 4.9300],
        [5.0932, 4.8923, 4.8059],
        [5.0932, 4.9987, 5.0437],
        [5.0932, 5.0209, 5.0636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): 0.10076981037855148 
model_pd.l_d.mean(): -20.16490364074707 
model_pd.lagr.mean(): -20.06413459777832 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4677], device='cuda:0')), ('power', tensor([-20.8630], device='cuda:0'))])
epoch£º675	 i:0 	 global-step:13500	 l-p:0.10076981037855148
epoch£º675	 i:1 	 global-step:13501	 l-p:0.13339786231517792
epoch£º675	 i:2 	 global-step:13502	 l-p:0.13691459596157074
epoch£º675	 i:3 	 global-step:13503	 l-p:0.13016831874847412
epoch£º675	 i:4 	 global-step:13504	 l-p:0.16023461520671844
epoch£º675	 i:5 	 global-step:13505	 l-p:0.15028247237205505
epoch£º675	 i:6 	 global-step:13506	 l-p:0.12131691724061966
epoch£º675	 i:7 	 global-step:13507	 l-p:0.12199069559574127
epoch£º675	 i:8 	 global-step:13508	 l-p:0.15779054164886475
epoch£º675	 i:9 	 global-step:13509	 l-p:0.04247160255908966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1038, 5.1037, 5.1038],
        [5.1038, 4.9118, 4.7315],
        [5.1038, 5.0918, 5.1025],
        [5.1038, 5.0998, 5.1036]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): 0.10134749859571457 
model_pd.l_d.mean(): -20.46139144897461 
model_pd.lagr.mean(): -20.360044479370117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4471], device='cuda:0')), ('power', tensor([-21.1417], device='cuda:0'))])
epoch£º676	 i:0 	 global-step:13520	 l-p:0.10134749859571457
epoch£º676	 i:1 	 global-step:13521	 l-p:0.14894849061965942
epoch£º676	 i:2 	 global-step:13522	 l-p:0.18529534339904785
epoch£º676	 i:3 	 global-step:13523	 l-p:0.12573866546154022
epoch£º676	 i:4 	 global-step:13524	 l-p:0.14621244370937347
epoch£º676	 i:5 	 global-step:13525	 l-p:0.07617554068565369
epoch£º676	 i:6 	 global-step:13526	 l-p:0.19336742162704468
epoch£º676	 i:7 	 global-step:13527	 l-p:0.13735181093215942
epoch£º676	 i:8 	 global-step:13528	 l-p:0.12119103968143463
epoch£º676	 i:9 	 global-step:13529	 l-p:0.12636050581932068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0594, 5.0594, 5.0594],
        [5.0594, 4.9599, 5.0055],
        [5.0594, 4.8672, 4.8289],
        [5.0594, 5.0429, 5.0573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): 0.1292261779308319 
model_pd.l_d.mean(): -20.59328842163086 
model_pd.lagr.mean(): -20.464061737060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4578], device='cuda:0')), ('power', tensor([-21.2861], device='cuda:0'))])
epoch£º677	 i:0 	 global-step:13540	 l-p:0.1292261779308319
epoch£º677	 i:1 	 global-step:13541	 l-p:0.14801083505153656
epoch£º677	 i:2 	 global-step:13542	 l-p:0.1146167740225792
epoch£º677	 i:3 	 global-step:13543	 l-p:0.1432342678308487
epoch£º677	 i:4 	 global-step:13544	 l-p:0.18450847268104553
epoch£º677	 i:5 	 global-step:13545	 l-p:0.13090892136096954
epoch£º677	 i:6 	 global-step:13546	 l-p:0.43870624899864197
epoch£º677	 i:7 	 global-step:13547	 l-p:-0.21987779438495636
epoch£º677	 i:8 	 global-step:13548	 l-p:0.16169244050979614
epoch£º677	 i:9 	 global-step:13549	 l-p:0.11236583441495895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9895, 4.9577, 4.9830],
        [4.9895, 4.9879, 4.9894],
        [4.9895, 4.9615, 4.9843],
        [4.9895, 4.9494, 4.9797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): -0.054562073200941086 
model_pd.l_d.mean(): -19.925825119018555 
model_pd.lagr.mean(): -19.98038673400879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5692], device='cuda:0')), ('power', tensor([-20.7251], device='cuda:0'))])
epoch£º678	 i:0 	 global-step:13560	 l-p:-0.054562073200941086
epoch£º678	 i:1 	 global-step:13561	 l-p:0.18034622073173523
epoch£º678	 i:2 	 global-step:13562	 l-p:0.13030797243118286
epoch£º678	 i:3 	 global-step:13563	 l-p:0.2411980926990509
epoch£º678	 i:4 	 global-step:13564	 l-p:0.12960605323314667
epoch£º678	 i:5 	 global-step:13565	 l-p:0.1349242478609085
epoch£º678	 i:6 	 global-step:13566	 l-p:0.1722506433725357
epoch£º678	 i:7 	 global-step:13567	 l-p:0.14695963263511658
epoch£º678	 i:8 	 global-step:13568	 l-p:0.14456437528133392
epoch£º678	 i:9 	 global-step:13569	 l-p:0.21818017959594727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9973, 4.9973, 4.9973],
        [4.9973, 4.9498, 4.6863],
        [4.9973, 4.9704, 4.9924],
        [4.9973, 4.9469, 4.9825]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.18470294773578644 
model_pd.l_d.mean(): -20.444713592529297 
model_pd.lagr.mean(): -20.260009765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4913], device='cuda:0')), ('power', tensor([-21.1700], device='cuda:0'))])
epoch£º679	 i:0 	 global-step:13580	 l-p:0.18470294773578644
epoch£º679	 i:1 	 global-step:13581	 l-p:0.16514001786708832
epoch£º679	 i:2 	 global-step:13582	 l-p:0.17260722815990448
epoch£º679	 i:3 	 global-step:13583	 l-p:1.7867827415466309
epoch£º679	 i:4 	 global-step:13584	 l-p:0.22978132963180542
epoch£º679	 i:5 	 global-step:13585	 l-p:0.1194833517074585
epoch£º679	 i:6 	 global-step:13586	 l-p:0.16515403985977173
epoch£º679	 i:7 	 global-step:13587	 l-p:0.11947553604841232
epoch£º679	 i:8 	 global-step:13588	 l-p:0.12310443818569183
epoch£º679	 i:9 	 global-step:13589	 l-p:0.11459800601005554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0331, 5.0263, 5.0327],
        [5.0331, 5.0307, 5.0330],
        [5.0331, 4.9954, 5.0243],
        [5.0331, 5.2183, 5.0462]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.2111300230026245 
model_pd.l_d.mean(): -20.950044631958008 
model_pd.lagr.mean(): -20.738914489746094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4261], device='cuda:0')), ('power', tensor([-21.6142], device='cuda:0'))])
epoch£º680	 i:0 	 global-step:13600	 l-p:0.2111300230026245
epoch£º680	 i:1 	 global-step:13601	 l-p:0.14175169169902802
epoch£º680	 i:2 	 global-step:13602	 l-p:0.15432019531726837
epoch£º680	 i:3 	 global-step:13603	 l-p:0.16754624247550964
epoch£º680	 i:4 	 global-step:13604	 l-p:0.21063876152038574
epoch£º680	 i:5 	 global-step:13605	 l-p:0.15334664285182953
epoch£º680	 i:6 	 global-step:13606	 l-p:0.13623996078968048
epoch£º680	 i:7 	 global-step:13607	 l-p:0.1153501644730568
epoch£º680	 i:8 	 global-step:13608	 l-p:0.12364055216312408
epoch£º680	 i:9 	 global-step:13609	 l-p:0.1176614761352539
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0897, 4.9109, 4.8981],
        [5.0897, 5.4733, 5.4214],
        [5.0897, 5.0889, 5.0897],
        [5.0897, 5.0888, 5.0897]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.10641925781965256 
model_pd.l_d.mean(): -20.376550674438477 
model_pd.lagr.mean(): -20.270132064819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4638], device='cuda:0')), ('power', tensor([-21.0731], device='cuda:0'))])
epoch£º681	 i:0 	 global-step:13620	 l-p:0.10641925781965256
epoch£º681	 i:1 	 global-step:13621	 l-p:0.13316534459590912
epoch£º681	 i:2 	 global-step:13622	 l-p:0.1740197390317917
epoch£º681	 i:3 	 global-step:13623	 l-p:0.14650025963783264
epoch£º681	 i:4 	 global-step:13624	 l-p:0.1480989158153534
epoch£º681	 i:5 	 global-step:13625	 l-p:0.0861596018075943
epoch£º681	 i:6 	 global-step:13626	 l-p:0.057835690677165985
epoch£º681	 i:7 	 global-step:13627	 l-p:0.14649631083011627
epoch£º681	 i:8 	 global-step:13628	 l-p:0.16032874584197998
epoch£º681	 i:9 	 global-step:13629	 l-p:0.11293578892946243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0786, 4.9829, 5.0286],
        [5.0786, 4.9551, 4.9955],
        [5.0786, 5.0761, 5.0785],
        [5.0786, 5.0635, 5.0768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.14283639192581177 
model_pd.l_d.mean(): -20.543807983398438 
model_pd.lagr.mean(): -20.400972366333008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4630], device='cuda:0')), ('power', tensor([-21.2413], device='cuda:0'))])
epoch£º682	 i:0 	 global-step:13640	 l-p:0.14283639192581177
epoch£º682	 i:1 	 global-step:13641	 l-p:0.12852054834365845
epoch£º682	 i:2 	 global-step:13642	 l-p:0.12534338235855103
epoch£º682	 i:3 	 global-step:13643	 l-p:0.22750085592269897
epoch£º682	 i:4 	 global-step:13644	 l-p:0.18053051829338074
epoch£º682	 i:5 	 global-step:13645	 l-p:0.12986178696155548
epoch£º682	 i:6 	 global-step:13646	 l-p:0.1372920423746109
epoch£º682	 i:7 	 global-step:13647	 l-p:0.14193621277809143
epoch£º682	 i:8 	 global-step:13648	 l-p:0.11912701278924942
epoch£º682	 i:9 	 global-step:13649	 l-p:0.11348123103380203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9641, 4.8434, 4.8877],
        [4.9641, 4.9276, 4.9559],
        [4.9641, 4.9641, 4.9641],
        [4.9641, 4.8095, 4.8363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.147928848862648 
model_pd.l_d.mean(): -20.871810913085938 
model_pd.lagr.mean(): -20.7238826751709 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4659], device='cuda:0')), ('power', tensor([-21.5758], device='cuda:0'))])
epoch£º683	 i:0 	 global-step:13660	 l-p:0.147928848862648
epoch£º683	 i:1 	 global-step:13661	 l-p:0.4672212600708008
epoch£º683	 i:2 	 global-step:13662	 l-p:0.11446313560009003
epoch£º683	 i:3 	 global-step:13663	 l-p:0.14166288077831268
epoch£º683	 i:4 	 global-step:13664	 l-p:-0.09320632368326187
epoch£º683	 i:5 	 global-step:13665	 l-p:0.12638512253761292
epoch£º683	 i:6 	 global-step:13666	 l-p:0.10894755274057388
epoch£º683	 i:7 	 global-step:13667	 l-p:0.17167136073112488
epoch£º683	 i:8 	 global-step:13668	 l-p:0.2647166848182678
epoch£º683	 i:9 	 global-step:13669	 l-p:0.11790729314088821
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9589, 4.7913, 4.5429],
        [4.9589, 4.9589, 4.9589],
        [4.9589, 5.1723, 5.0176],
        [4.9589, 4.9566, 4.9588]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): 0.12407750636339188 
model_pd.l_d.mean(): -20.36914825439453 
model_pd.lagr.mean(): -20.245071411132812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5096], device='cuda:0')), ('power', tensor([-21.1123], device='cuda:0'))])
epoch£º684	 i:0 	 global-step:13680	 l-p:0.12407750636339188
epoch£º684	 i:1 	 global-step:13681	 l-p:0.12686128914356232
epoch£º684	 i:2 	 global-step:13682	 l-p:0.13200883567333221
epoch£º684	 i:3 	 global-step:13683	 l-p:0.2784673571586609
epoch£º684	 i:4 	 global-step:13684	 l-p:0.1270376592874527
epoch£º684	 i:5 	 global-step:13685	 l-p:32.89201736450195
epoch£º684	 i:6 	 global-step:13686	 l-p:-0.2621045708656311
epoch£º684	 i:7 	 global-step:13687	 l-p:0.0028465508949011564
epoch£º684	 i:8 	 global-step:13688	 l-p:0.1326235979795456
epoch£º684	 i:9 	 global-step:13689	 l-p:0.08646253496408463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9836, 4.9038, 4.9493],
        [4.9836, 5.3257, 5.2497],
        [4.9836, 4.7662, 4.6405],
        [4.9836, 4.9836, 4.9837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.23198406398296356 
model_pd.l_d.mean(): -20.638782501220703 
model_pd.lagr.mean(): -20.40679931640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4894], device='cuda:0')), ('power', tensor([-21.3642], device='cuda:0'))])
epoch£º685	 i:0 	 global-step:13700	 l-p:0.23198406398296356
epoch£º685	 i:1 	 global-step:13701	 l-p:0.1899043172597885
epoch£º685	 i:2 	 global-step:13702	 l-p:0.12048988044261932
epoch£º685	 i:3 	 global-step:13703	 l-p:0.43560484051704407
epoch£º685	 i:4 	 global-step:13704	 l-p:0.13287168741226196
epoch£º685	 i:5 	 global-step:13705	 l-p:0.11775576323270798
epoch£º685	 i:6 	 global-step:13706	 l-p:0.09114374220371246
epoch£º685	 i:7 	 global-step:13707	 l-p:0.17652690410614014
epoch£º685	 i:8 	 global-step:13708	 l-p:0.10302184522151947
epoch£º685	 i:9 	 global-step:13709	 l-p:0.12852025032043457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0980, 5.0977, 5.0980],
        [5.0980, 5.0974, 5.0980],
        [5.0980, 5.0672, 5.0918],
        [5.0980, 5.3286, 5.1797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.09401754289865494 
model_pd.l_d.mean(): -19.0452880859375 
model_pd.lagr.mean(): -18.951271057128906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5873], device='cuda:0')), ('power', tensor([-19.8535], device='cuda:0'))])
epoch£º686	 i:0 	 global-step:13720	 l-p:0.09401754289865494
epoch£º686	 i:1 	 global-step:13721	 l-p:0.14080913364887238
epoch£º686	 i:2 	 global-step:13722	 l-p:0.10454373806715012
epoch£º686	 i:3 	 global-step:13723	 l-p:0.15020392835140228
epoch£º686	 i:4 	 global-step:13724	 l-p:0.11252816021442413
epoch£º686	 i:5 	 global-step:13725	 l-p:0.1141257956624031
epoch£º686	 i:6 	 global-step:13726	 l-p:0.09637084603309631
epoch£º686	 i:7 	 global-step:13727	 l-p:0.1416042447090149
epoch£º686	 i:8 	 global-step:13728	 l-p:0.13576294481754303
epoch£º686	 i:9 	 global-step:13729	 l-p:0.14057302474975586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4057,  0.3004,  1.0000,  0.2224,
          1.0000,  0.7403, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6689,  0.5850,  1.0000,  0.5116,
          1.0000,  0.8745, 31.6228]], device='cuda:0')
 pt:tensor([[5.1383, 5.3420, 5.1766],
        [5.1383, 4.9627, 4.7515],
        [5.1383, 4.9489, 4.9087],
        [5.1383, 5.2270, 5.0054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): 0.07380319386720657 
model_pd.l_d.mean(): -19.754270553588867 
model_pd.lagr.mean(): -19.68046760559082 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5475], device='cuda:0')), ('power', tensor([-20.5295], device='cuda:0'))])
epoch£º687	 i:0 	 global-step:13740	 l-p:0.07380319386720657
epoch£º687	 i:1 	 global-step:13741	 l-p:0.17368030548095703
epoch£º687	 i:2 	 global-step:13742	 l-p:0.1374925673007965
epoch£º687	 i:3 	 global-step:13743	 l-p:0.13082340359687805
epoch£º687	 i:4 	 global-step:13744	 l-p:0.14931663870811462
epoch£º687	 i:5 	 global-step:13745	 l-p:0.10516515374183655
epoch£º687	 i:6 	 global-step:13746	 l-p:0.08653323352336884
epoch£º687	 i:7 	 global-step:13747	 l-p:0.13745497167110443
epoch£º687	 i:8 	 global-step:13748	 l-p:0.14047342538833618
epoch£º687	 i:9 	 global-step:13749	 l-p:0.009710784070193768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1233, 5.1233, 5.1233],
        [5.1233, 4.9265, 4.7524],
        [5.1233, 4.9672, 4.9846],
        [5.1233, 5.1233, 5.1233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): 0.04298943281173706 
model_pd.l_d.mean(): -19.473247528076172 
model_pd.lagr.mean(): -19.43025779724121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5248], device='cuda:0')), ('power', tensor([-20.2222], device='cuda:0'))])
epoch£º688	 i:0 	 global-step:13760	 l-p:0.04298943281173706
epoch£º688	 i:1 	 global-step:13761	 l-p:0.13362884521484375
epoch£º688	 i:2 	 global-step:13762	 l-p:0.14953210949897766
epoch£º688	 i:3 	 global-step:13763	 l-p:0.17586497962474823
epoch£º688	 i:4 	 global-step:13764	 l-p:0.15027491748332977
epoch£º688	 i:5 	 global-step:13765	 l-p:0.119470976293087
epoch£º688	 i:6 	 global-step:13766	 l-p:0.03614354133605957
epoch£º688	 i:7 	 global-step:13767	 l-p:0.13344644010066986
epoch£º688	 i:8 	 global-step:13768	 l-p:0.12211379408836365
epoch£º688	 i:9 	 global-step:13769	 l-p:0.13067500293254852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1050, 5.0397, 5.0809],
        [5.1050, 5.0979, 5.1045],
        [5.1050, 5.1050, 5.1050],
        [5.1050, 5.1050, 5.1050]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.17561742663383484 
model_pd.l_d.mean(): -20.67066192626953 
model_pd.lagr.mean(): -20.495044708251953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4425], device='cuda:0')), ('power', tensor([-21.3486], device='cuda:0'))])
epoch£º689	 i:0 	 global-step:13780	 l-p:0.17561742663383484
epoch£º689	 i:1 	 global-step:13781	 l-p:0.13592153787612915
epoch£º689	 i:2 	 global-step:13782	 l-p:0.12102263420820236
epoch£º689	 i:3 	 global-step:13783	 l-p:0.12663568556308746
epoch£º689	 i:4 	 global-step:13784	 l-p:0.11990579217672348
epoch£º689	 i:5 	 global-step:13785	 l-p:0.12088154256343842
epoch£º689	 i:6 	 global-step:13786	 l-p:0.1889326274394989
epoch£º689	 i:7 	 global-step:13787	 l-p:0.09327606856822968
epoch£º689	 i:8 	 global-step:13788	 l-p:0.24920611083507538
epoch£º689	 i:9 	 global-step:13789	 l-p:0.12658897042274475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0325, 5.0325, 5.0325],
        [5.0325, 5.4428, 5.4098],
        [5.0325, 5.0320, 5.0325],
        [5.0325, 5.0325, 5.0325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.13908974826335907 
model_pd.l_d.mean(): -20.61335563659668 
model_pd.lagr.mean(): -20.474266052246094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4746], device='cuda:0')), ('power', tensor([-21.3234], device='cuda:0'))])
epoch£º690	 i:0 	 global-step:13800	 l-p:0.13908974826335907
epoch£º690	 i:1 	 global-step:13801	 l-p:0.07927915453910828
epoch£º690	 i:2 	 global-step:13802	 l-p:0.12472593784332275
epoch£º690	 i:3 	 global-step:13803	 l-p:0.3868357837200165
epoch£º690	 i:4 	 global-step:13804	 l-p:0.13221533596515656
epoch£º690	 i:5 	 global-step:13805	 l-p:0.18992097675800323
epoch£º690	 i:6 	 global-step:13806	 l-p:0.2280278354883194
epoch£º690	 i:7 	 global-step:13807	 l-p:0.17240864038467407
epoch£º690	 i:8 	 global-step:13808	 l-p:0.13653318583965302
epoch£º690	 i:9 	 global-step:13809	 l-p:0.1335439234972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0419, 5.0419, 5.0419],
        [5.0419, 4.9361, 4.9824],
        [5.0419, 4.9366, 4.6713],
        [5.0419, 5.0396, 5.0418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.11126554012298584 
model_pd.l_d.mean(): -19.582223892211914 
model_pd.lagr.mean(): -19.470958709716797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5206], device='cuda:0')), ('power', tensor([-20.3281], device='cuda:0'))])
epoch£º691	 i:0 	 global-step:13820	 l-p:0.11126554012298584
epoch£º691	 i:1 	 global-step:13821	 l-p:0.0983520895242691
epoch£º691	 i:2 	 global-step:13822	 l-p:0.17093190550804138
epoch£º691	 i:3 	 global-step:13823	 l-p:0.17043182253837585
epoch£º691	 i:4 	 global-step:13824	 l-p:0.1919848918914795
epoch£º691	 i:5 	 global-step:13825	 l-p:0.10073796659708023
epoch£º691	 i:6 	 global-step:13826	 l-p:0.20949439704418182
epoch£º691	 i:7 	 global-step:13827	 l-p:0.1116693839430809
epoch£º691	 i:8 	 global-step:13828	 l-p:0.11734182387590408
epoch£º691	 i:9 	 global-step:13829	 l-p:0.1184774786233902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0991, 5.0988, 5.0991],
        [5.0991, 5.0986, 5.0991],
        [5.0991, 5.0360, 5.0765],
        [5.0991, 5.0991, 5.0991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.08195414394140244 
model_pd.l_d.mean(): -20.844575881958008 
model_pd.lagr.mean(): -20.76262092590332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4063], device='cuda:0')), ('power', tensor([-21.4874], device='cuda:0'))])
epoch£º692	 i:0 	 global-step:13840	 l-p:0.08195414394140244
epoch£º692	 i:1 	 global-step:13841	 l-p:0.1255488395690918
epoch£º692	 i:2 	 global-step:13842	 l-p:0.15982356667518616
epoch£º692	 i:3 	 global-step:13843	 l-p:0.14276225864887238
epoch£º692	 i:4 	 global-step:13844	 l-p:0.10614696145057678
epoch£º692	 i:5 	 global-step:13845	 l-p:0.11991626769304276
epoch£º692	 i:6 	 global-step:13846	 l-p:0.14512082934379578
epoch£º692	 i:7 	 global-step:13847	 l-p:0.12348321825265884
epoch£º692	 i:8 	 global-step:13848	 l-p:0.2007339745759964
epoch£º692	 i:9 	 global-step:13849	 l-p:0.11324048787355423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0615, 5.0578, 5.0613],
        [5.0615, 5.1690, 4.9546],
        [5.0615, 5.4240, 5.3571],
        [5.0615, 5.4645, 5.4248]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.09665320813655853 
model_pd.l_d.mean(): -19.560575485229492 
model_pd.lagr.mean(): -19.46392250061035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5815], device='cuda:0')), ('power', tensor([-20.3684], device='cuda:0'))])
epoch£º693	 i:0 	 global-step:13860	 l-p:0.09665320813655853
epoch£º693	 i:1 	 global-step:13861	 l-p:0.07559096813201904
epoch£º693	 i:2 	 global-step:13862	 l-p:0.1577410250902176
epoch£º693	 i:3 	 global-step:13863	 l-p:0.15971356630325317
epoch£º693	 i:4 	 global-step:13864	 l-p:0.1777803748846054
epoch£º693	 i:5 	 global-step:13865	 l-p:0.13184337317943573
epoch£º693	 i:6 	 global-step:13866	 l-p:0.12274419516324997
epoch£º693	 i:7 	 global-step:13867	 l-p:0.20807567238807678
epoch£º693	 i:8 	 global-step:13868	 l-p:0.21319445967674255
epoch£º693	 i:9 	 global-step:13869	 l-p:0.11596077680587769
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0543, 4.9901, 5.0314],
        [5.0543, 4.8440, 4.6758],
        [5.0543, 5.0543, 5.0544],
        [5.0543, 5.0544, 5.0544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.10157230496406555 
model_pd.l_d.mean(): -19.199214935302734 
model_pd.lagr.mean(): -19.09764289855957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5570], device='cuda:0')), ('power', tensor([-19.9780], device='cuda:0'))])
epoch£º694	 i:0 	 global-step:13880	 l-p:0.10157230496406555
epoch£º694	 i:1 	 global-step:13881	 l-p:0.1804218292236328
epoch£º694	 i:2 	 global-step:13882	 l-p:0.13358424603939056
epoch£º694	 i:3 	 global-step:13883	 l-p:0.1284458190202713
epoch£º694	 i:4 	 global-step:13884	 l-p:0.11322503536939621
epoch£º694	 i:5 	 global-step:13885	 l-p:0.18923181295394897
epoch£º694	 i:6 	 global-step:13886	 l-p:0.14591945707798004
epoch£º694	 i:7 	 global-step:13887	 l-p:0.15117283165454865
epoch£º694	 i:8 	 global-step:13888	 l-p:0.1316162347793579
epoch£º694	 i:9 	 global-step:13889	 l-p:0.13609491288661957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0822, 4.8729, 4.7859],
        [5.0822, 4.9280, 4.6807],
        [5.0822, 4.9365, 4.6846],
        [5.0822, 5.0819, 5.0822]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.1258620172739029 
model_pd.l_d.mean(): -19.550212860107422 
model_pd.lagr.mean(): -19.42435073852539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4964], device='cuda:0')), ('power', tensor([-20.2710], device='cuda:0'))])
epoch£º695	 i:0 	 global-step:13900	 l-p:0.1258620172739029
epoch£º695	 i:1 	 global-step:13901	 l-p:0.17819908261299133
epoch£º695	 i:2 	 global-step:13902	 l-p:0.09399077296257019
epoch£º695	 i:3 	 global-step:13903	 l-p:0.13327564299106598
epoch£º695	 i:4 	 global-step:13904	 l-p:0.16656938195228577
epoch£º695	 i:5 	 global-step:13905	 l-p:0.12101169675588608
epoch£º695	 i:6 	 global-step:13906	 l-p:0.13686488568782806
epoch£º695	 i:7 	 global-step:13907	 l-p:0.09773752093315125
epoch£º695	 i:8 	 global-step:13908	 l-p:0.13149960339069366
epoch£º695	 i:9 	 global-step:13909	 l-p:0.13225474953651428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0981, 5.0893, 5.0973],
        [5.0981, 4.9459, 4.6990],
        [5.0981, 5.0537, 5.0863],
        [5.0981, 5.4494, 5.3731]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.18832702934741974 
model_pd.l_d.mean(): -20.803525924682617 
model_pd.lagr.mean(): -20.615198135375977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4261], device='cuda:0')), ('power', tensor([-21.4662], device='cuda:0'))])
epoch£º696	 i:0 	 global-step:13920	 l-p:0.18832702934741974
epoch£º696	 i:1 	 global-step:13921	 l-p:0.11832625418901443
epoch£º696	 i:2 	 global-step:13922	 l-p:0.16341865062713623
epoch£º696	 i:3 	 global-step:13923	 l-p:0.11904577165842056
epoch£º696	 i:4 	 global-step:13924	 l-p:0.14120125770568848
epoch£º696	 i:5 	 global-step:13925	 l-p:0.18101276457309723
epoch£º696	 i:6 	 global-step:13926	 l-p:0.12605607509613037
epoch£º696	 i:7 	 global-step:13927	 l-p:0.11687549203634262
epoch£º696	 i:8 	 global-step:13928	 l-p:0.13656063377857208
epoch£º696	 i:9 	 global-step:13929	 l-p:0.10969642549753189
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0296, 5.0296, 5.0296],
        [5.0296, 5.0296, 5.0296],
        [5.0296, 5.1114, 4.8847],
        [5.0296, 5.0288, 5.0296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.13197016716003418 
model_pd.l_d.mean(): -20.11646270751953 
model_pd.lagr.mean(): -19.984493255615234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4651], device='cuda:0')), ('power', tensor([-20.8114], device='cuda:0'))])
epoch£º697	 i:0 	 global-step:13940	 l-p:0.13197016716003418
epoch£º697	 i:1 	 global-step:13941	 l-p:0.16395436227321625
epoch£º697	 i:2 	 global-step:13942	 l-p:0.5497933030128479
epoch£º697	 i:3 	 global-step:13943	 l-p:0.12559625506401062
epoch£º697	 i:4 	 global-step:13944	 l-p:0.21387721598148346
epoch£º697	 i:5 	 global-step:13945	 l-p:0.1465248316526413
epoch£º697	 i:6 	 global-step:13946	 l-p:0.1761748343706131
epoch£º697	 i:7 	 global-step:13947	 l-p:0.14987127482891083
epoch£º697	 i:8 	 global-step:13948	 l-p:0.13995233178138733
epoch£º697	 i:9 	 global-step:13949	 l-p:0.16473442316055298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0418, 5.0052, 5.0336],
        [5.0418, 5.0393, 5.0417],
        [5.0418, 4.8556, 4.8451],
        [5.0418, 4.8243, 4.7042]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.14401774108409882 
model_pd.l_d.mean(): -20.581859588623047 
model_pd.lagr.mean(): -20.437841415405273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4603], device='cuda:0')), ('power', tensor([-21.2770], device='cuda:0'))])
epoch£º698	 i:0 	 global-step:13960	 l-p:0.14401774108409882
epoch£º698	 i:1 	 global-step:13961	 l-p:0.11779451370239258
epoch£º698	 i:2 	 global-step:13962	 l-p:0.20143014192581177
epoch£º698	 i:3 	 global-step:13963	 l-p:0.14772415161132812
epoch£º698	 i:4 	 global-step:13964	 l-p:0.1313786655664444
epoch£º698	 i:5 	 global-step:13965	 l-p:0.1689116209745407
epoch£º698	 i:6 	 global-step:13966	 l-p:0.2552301287651062
epoch£º698	 i:7 	 global-step:13967	 l-p:0.16424979269504547
epoch£º698	 i:8 	 global-step:13968	 l-p:0.09441523253917694
epoch£º698	 i:9 	 global-step:13969	 l-p:0.11957305669784546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0598, 5.0527, 5.0593],
        [5.0598, 4.9959, 5.0371],
        [5.0598, 4.8440, 4.7017],
        [5.0598, 5.0598, 5.0598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): 0.13959503173828125 
model_pd.l_d.mean(): -20.441518783569336 
model_pd.lagr.mean(): -20.301923751831055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4823], device='cuda:0')), ('power', tensor([-21.1576], device='cuda:0'))])
epoch£º699	 i:0 	 global-step:13980	 l-p:0.13959503173828125
epoch£º699	 i:1 	 global-step:13981	 l-p:0.12352805584669113
epoch£º699	 i:2 	 global-step:13982	 l-p:0.14942464232444763
epoch£º699	 i:3 	 global-step:13983	 l-p:0.13318665325641632
epoch£º699	 i:4 	 global-step:13984	 l-p:0.1793796718120575
epoch£º699	 i:5 	 global-step:13985	 l-p:0.06692978739738464
epoch£º699	 i:6 	 global-step:13986	 l-p:0.10362794995307922
epoch£º699	 i:7 	 global-step:13987	 l-p:0.18268975615501404
epoch£º699	 i:8 	 global-step:13988	 l-p:0.14883489906787872
epoch£º699	 i:9 	 global-step:13989	 l-p:0.1349923461675644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1181, 5.1131, 5.1178],
        [5.1181, 4.9117, 4.8351],
        [5.1181, 4.9272, 4.8989],
        [5.1181, 5.0531, 5.0945]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): 0.11061212420463562 
model_pd.l_d.mean(): -20.438322067260742 
model_pd.lagr.mean(): -20.327709197998047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4324], device='cuda:0')), ('power', tensor([-21.1034], device='cuda:0'))])
epoch£º700	 i:0 	 global-step:14000	 l-p:0.11061212420463562
epoch£º700	 i:1 	 global-step:14001	 l-p:0.12237203121185303
epoch£º700	 i:2 	 global-step:14002	 l-p:0.0828339010477066
epoch£º700	 i:3 	 global-step:14003	 l-p:0.12463375180959702
epoch£º700	 i:4 	 global-step:14004	 l-p:0.12371204793453217
epoch£º700	 i:5 	 global-step:14005	 l-p:0.15985645353794098
epoch£º700	 i:6 	 global-step:14006	 l-p:0.09055324643850327
epoch£º700	 i:7 	 global-step:14007	 l-p:0.14734281599521637
epoch£º700	 i:8 	 global-step:14008	 l-p:0.11788724362850189
epoch£º700	 i:9 	 global-step:14009	 l-p:0.15691085159778595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1249, 5.1177, 5.1244],
        [5.1249, 5.0456, 5.0906],
        [5.1249, 5.1249, 5.1249],
        [5.1249, 4.9595, 4.9716]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): 0.12448243796825409 
model_pd.l_d.mean(): -20.12603759765625 
model_pd.lagr.mean(): -20.001554489135742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4856], device='cuda:0')), ('power', tensor([-20.8421], device='cuda:0'))])
epoch£º701	 i:0 	 global-step:14020	 l-p:0.12448243796825409
epoch£º701	 i:1 	 global-step:14021	 l-p:0.14635542035102844
epoch£º701	 i:2 	 global-step:14022	 l-p:0.1353859156370163
epoch£º701	 i:3 	 global-step:14023	 l-p:0.162800133228302
epoch£º701	 i:4 	 global-step:14024	 l-p:0.09055866301059723
epoch£º701	 i:5 	 global-step:14025	 l-p:0.0935264602303505
epoch£º701	 i:6 	 global-step:14026	 l-p:0.1431787759065628
epoch£º701	 i:7 	 global-step:14027	 l-p:0.08092786371707916
epoch£º701	 i:8 	 global-step:14028	 l-p:0.1292952448129654
epoch£º701	 i:9 	 global-step:14029	 l-p:0.16097763180732727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1065, 5.0430, 5.0840],
        [5.1065, 5.2743, 5.0871],
        [5.1065, 5.0449, 5.0852],
        [5.1065, 5.1065, 5.1065]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.12799708545207977 
model_pd.l_d.mean(): -20.29868507385254 
model_pd.lagr.mean(): -20.17068862915039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4803], device='cuda:0')), ('power', tensor([-21.0112], device='cuda:0'))])
epoch£º702	 i:0 	 global-step:14040	 l-p:0.12799708545207977
epoch£º702	 i:1 	 global-step:14041	 l-p:0.19238080084323883
epoch£º702	 i:2 	 global-step:14042	 l-p:0.0445011667907238
epoch£º702	 i:3 	 global-step:14043	 l-p:0.09877489507198334
epoch£º702	 i:4 	 global-step:14044	 l-p:0.1322915107011795
epoch£º702	 i:5 	 global-step:14045	 l-p:0.1404534876346588
epoch£º702	 i:6 	 global-step:14046	 l-p:0.10578766465187073
epoch£º702	 i:7 	 global-step:14047	 l-p:0.1653209626674652
epoch£º702	 i:8 	 global-step:14048	 l-p:0.15631116926670074
epoch£º702	 i:9 	 global-step:14049	 l-p:0.12656432390213013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0783, 5.0754, 5.0782],
        [5.0783, 5.3147, 5.1662],
        [5.0783, 5.0659, 5.0770],
        [5.0783, 5.3394, 5.2055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): 0.14634200930595398 
model_pd.l_d.mean(): -20.46357536315918 
model_pd.lagr.mean(): -20.31723403930664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4903], device='cuda:0')), ('power', tensor([-21.1881], device='cuda:0'))])
epoch£º703	 i:0 	 global-step:14060	 l-p:0.14634200930595398
epoch£º703	 i:1 	 global-step:14061	 l-p:0.13124710321426392
epoch£º703	 i:2 	 global-step:14062	 l-p:0.08638089895248413
epoch£º703	 i:3 	 global-step:14063	 l-p:0.13700760900974274
epoch£º703	 i:4 	 global-step:14064	 l-p:0.20281970500946045
epoch£º703	 i:5 	 global-step:14065	 l-p:0.18619069457054138
epoch£º703	 i:6 	 global-step:14066	 l-p:0.11851365119218826
epoch£º703	 i:7 	 global-step:14067	 l-p:0.12383199483156204
epoch£º703	 i:8 	 global-step:14068	 l-p:0.06776715070009232
epoch£º703	 i:9 	 global-step:14069	 l-p:0.1522589921951294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1051, 5.1051, 5.1051],
        [5.1051, 5.0604, 5.0933],
        [5.1051, 5.0243, 5.0699],
        [5.1051, 4.9304, 4.6957]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.13208651542663574 
model_pd.l_d.mean(): -21.06289291381836 
model_pd.lagr.mean(): -20.93080711364746 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3724], device='cuda:0')), ('power', tensor([-21.6735], device='cuda:0'))])
epoch£º704	 i:0 	 global-step:14080	 l-p:0.13208651542663574
epoch£º704	 i:1 	 global-step:14081	 l-p:0.09083867073059082
epoch£º704	 i:2 	 global-step:14082	 l-p:0.16519057750701904
epoch£º704	 i:3 	 global-step:14083	 l-p:0.08723525702953339
epoch£º704	 i:4 	 global-step:14084	 l-p:0.15064111351966858
epoch£º704	 i:5 	 global-step:14085	 l-p:0.13207389414310455
epoch£º704	 i:6 	 global-step:14086	 l-p:0.10630907118320465
epoch£º704	 i:7 	 global-step:14087	 l-p:0.1416447013616562
epoch£º704	 i:8 	 global-step:14088	 l-p:0.11068178713321686
epoch£º704	 i:9 	 global-step:14089	 l-p:0.13556693494319916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1554, 5.1554, 5.1554],
        [5.1554, 5.1510, 5.1551],
        [5.1554, 4.9974, 4.7557],
        [5.1554, 5.2563, 5.0354]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): -0.12627239525318146 
model_pd.l_d.mean(): -20.143062591552734 
model_pd.lagr.mean(): -20.26933479309082 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5028], device='cuda:0')), ('power', tensor([-20.8768], device='cuda:0'))])
epoch£º705	 i:0 	 global-step:14100	 l-p:-0.12627239525318146
epoch£º705	 i:1 	 global-step:14101	 l-p:0.15132053196430206
epoch£º705	 i:2 	 global-step:14102	 l-p:0.1373259723186493
epoch£º705	 i:3 	 global-step:14103	 l-p:0.14871841669082642
epoch£º705	 i:4 	 global-step:14104	 l-p:0.11361496895551682
epoch£º705	 i:5 	 global-step:14105	 l-p:0.11346862465143204
epoch£º705	 i:6 	 global-step:14106	 l-p:0.12921851873397827
epoch£º705	 i:7 	 global-step:14107	 l-p:0.14016729593276978
epoch£º705	 i:8 	 global-step:14108	 l-p:0.08250834047794342
epoch£º705	 i:9 	 global-step:14109	 l-p:0.12308625876903534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1191, 5.0440, 5.0884],
        [5.1191, 5.3016, 5.1210],
        [5.1191, 5.0826, 5.1108],
        [5.1191, 4.9049, 4.7737]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.15581528842449188 
model_pd.l_d.mean(): -20.51284408569336 
model_pd.lagr.mean(): -20.35702896118164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4654], device='cuda:0')), ('power', tensor([-21.2124], device='cuda:0'))])
epoch£º706	 i:0 	 global-step:14120	 l-p:0.15581528842449188
epoch£º706	 i:1 	 global-step:14121	 l-p:0.12520362436771393
epoch£º706	 i:2 	 global-step:14122	 l-p:0.14597953855991364
epoch£º706	 i:3 	 global-step:14123	 l-p:0.10736220329999924
epoch£º706	 i:4 	 global-step:14124	 l-p:0.10495812445878983
epoch£º706	 i:5 	 global-step:14125	 l-p:0.2089281976222992
epoch£º706	 i:6 	 global-step:14126	 l-p:0.12356824427843094
epoch£º706	 i:7 	 global-step:14127	 l-p:0.3246251344680786
epoch£º706	 i:8 	 global-step:14128	 l-p:0.22046896815299988
epoch£º706	 i:9 	 global-step:14129	 l-p:0.13768233358860016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0196, 5.0193, 5.0196],
        [5.0196, 5.0196, 5.0196],
        [5.0196, 5.0196, 5.0196],
        [5.0196, 4.9102, 4.9583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 1.2175267934799194 
model_pd.l_d.mean(): -20.918821334838867 
model_pd.lagr.mean(): -19.7012939453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4377], device='cuda:0')), ('power', tensor([-21.5946], device='cuda:0'))])
epoch£º707	 i:0 	 global-step:14140	 l-p:1.2175267934799194
epoch£º707	 i:1 	 global-step:14141	 l-p:0.11271005123853683
epoch£º707	 i:2 	 global-step:14142	 l-p:0.2215830683708191
epoch£º707	 i:3 	 global-step:14143	 l-p:0.13648034632205963
epoch£º707	 i:4 	 global-step:14144	 l-p:0.12795120477676392
epoch£º707	 i:5 	 global-step:14145	 l-p:0.26981720328330994
epoch£º707	 i:6 	 global-step:14146	 l-p:0.19009536504745483
epoch£º707	 i:7 	 global-step:14147	 l-p:0.14565497636795044
epoch£º707	 i:8 	 global-step:14148	 l-p:0.12609173357486725
epoch£º707	 i:9 	 global-step:14149	 l-p:0.113997682929039
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0419, 4.9675, 5.0124],
        [5.0419, 4.8702, 4.8844],
        [5.0419, 4.9872, 4.7149],
        [5.0419, 4.9667, 5.0118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.13543877005577087 
model_pd.l_d.mean(): -20.378482818603516 
model_pd.lagr.mean(): -20.243043899536133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4967], device='cuda:0')), ('power', tensor([-21.1086], device='cuda:0'))])
epoch£º708	 i:0 	 global-step:14160	 l-p:0.13543877005577087
epoch£º708	 i:1 	 global-step:14161	 l-p:0.11366865038871765
epoch£º708	 i:2 	 global-step:14162	 l-p:0.16568489372730255
epoch£º708	 i:3 	 global-step:14163	 l-p:0.17318548262119293
epoch£º708	 i:4 	 global-step:14164	 l-p:0.1386115849018097
epoch£º708	 i:5 	 global-step:14165	 l-p:0.06603451818227768
epoch£º708	 i:6 	 global-step:14166	 l-p:0.31063228845596313
epoch£º708	 i:7 	 global-step:14167	 l-p:0.19062431156635284
epoch£º708	 i:8 	 global-step:14168	 l-p:0.14572319388389587
epoch£º708	 i:9 	 global-step:14169	 l-p:0.17167162895202637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0860, 4.8774, 4.8189],
        [5.0860, 5.0481, 5.0773],
        [5.0860, 5.5479, 5.5445],
        [5.0860, 4.9094, 4.6680]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): 0.10520701110363007 
model_pd.l_d.mean(): -20.18701934814453 
model_pd.lagr.mean(): -20.081811904907227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4773], device='cuda:0')), ('power', tensor([-20.8952], device='cuda:0'))])
epoch£º709	 i:0 	 global-step:14180	 l-p:0.10520701110363007
epoch£º709	 i:1 	 global-step:14181	 l-p:0.1502634733915329
epoch£º709	 i:2 	 global-step:14182	 l-p:0.06932871788740158
epoch£º709	 i:3 	 global-step:14183	 l-p:0.0964554026722908
epoch£º709	 i:4 	 global-step:14184	 l-p:0.15655893087387085
epoch£º709	 i:5 	 global-step:14185	 l-p:0.11695054918527603
epoch£º709	 i:6 	 global-step:14186	 l-p:0.1389717310667038
epoch£º709	 i:7 	 global-step:14187	 l-p:0.14880222082138062
epoch£º709	 i:8 	 global-step:14188	 l-p:0.12527480721473694
epoch£º709	 i:9 	 global-step:14189	 l-p:0.1586947739124298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1277, 5.1105, 5.1255],
        [5.1277, 4.9234, 4.8693],
        [5.1277, 4.9550, 4.9623],
        [5.1277, 5.1003, 5.1228]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.12228459864854813 
model_pd.l_d.mean(): -20.504545211791992 
model_pd.lagr.mean(): -20.382261276245117 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4310], device='cuda:0')), ('power', tensor([-21.1689], device='cuda:0'))])
epoch£º710	 i:0 	 global-step:14200	 l-p:0.12228459864854813
epoch£º710	 i:1 	 global-step:14201	 l-p:0.12678608298301697
epoch£º710	 i:2 	 global-step:14202	 l-p:0.12619449198246002
epoch£º710	 i:3 	 global-step:14203	 l-p:0.07374094426631927
epoch£º710	 i:4 	 global-step:14204	 l-p:0.1560267210006714
epoch£º710	 i:5 	 global-step:14205	 l-p:0.10031475126743317
epoch£º710	 i:6 	 global-step:14206	 l-p:0.22205457091331482
epoch£º710	 i:7 	 global-step:14207	 l-p:0.1458165943622589
epoch£º710	 i:8 	 global-step:14208	 l-p:0.1597905457019806
epoch£º710	 i:9 	 global-step:14209	 l-p:0.13912858068943024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0775, 5.0665, 5.0764],
        [5.0775, 5.1380, 4.8985],
        [5.0775, 5.0643, 4.8002],
        [5.0775, 5.0775, 5.0775]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.1522776335477829 
model_pd.l_d.mean(): -19.753801345825195 
model_pd.lagr.mean(): -19.601524353027344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5438], device='cuda:0')), ('power', tensor([-20.5252], device='cuda:0'))])
epoch£º711	 i:0 	 global-step:14220	 l-p:0.1522776335477829
epoch£º711	 i:1 	 global-step:14221	 l-p:0.19367261230945587
epoch£º711	 i:2 	 global-step:14222	 l-p:0.107340008020401
epoch£º711	 i:3 	 global-step:14223	 l-p:0.1601286679506302
epoch£º711	 i:4 	 global-step:14224	 l-p:0.1489351987838745
epoch£º711	 i:5 	 global-step:14225	 l-p:0.15421508252620697
epoch£º711	 i:6 	 global-step:14226	 l-p:0.030154937878251076
epoch£º711	 i:7 	 global-step:14227	 l-p:0.12209177762269974
epoch£º711	 i:8 	 global-step:14228	 l-p:0.08728272467851639
epoch£º711	 i:9 	 global-step:14229	 l-p:0.11873670667409897
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1673, 5.1152, 4.8480],
        [5.1673, 5.2657, 5.0419],
        [5.1673, 5.0944, 5.1382],
        [5.1673, 5.3988, 5.2431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): -0.29278501868247986 
model_pd.l_d.mean(): -20.21220588684082 
model_pd.lagr.mean(): -20.50499153137207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5092], device='cuda:0')), ('power', tensor([-20.9533], device='cuda:0'))])
epoch£º712	 i:0 	 global-step:14240	 l-p:-0.29278501868247986
epoch£º712	 i:1 	 global-step:14241	 l-p:0.1168169155716896
epoch£º712	 i:2 	 global-step:14242	 l-p:0.13260848820209503
epoch£º712	 i:3 	 global-step:14243	 l-p:0.13963072001934052
epoch£º712	 i:4 	 global-step:14244	 l-p:0.11654967814683914
epoch£º712	 i:5 	 global-step:14245	 l-p:0.12353283911943436
epoch£º712	 i:6 	 global-step:14246	 l-p:0.13814902305603027
epoch£º712	 i:7 	 global-step:14247	 l-p:0.08416762202978134
epoch£º712	 i:8 	 global-step:14248	 l-p:0.07146870344877243
epoch£º712	 i:9 	 global-step:14249	 l-p:0.1239510327577591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1592, 5.1522, 5.1587],
        [5.1592, 4.9466, 4.8001],
        [5.1592, 5.1592, 5.1592],
        [5.1592, 4.9811, 4.7508]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): 0.08622606843709946 
model_pd.l_d.mean(): -20.884140014648438 
model_pd.lagr.mean(): -20.797914505004883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3840], device='cuda:0')), ('power', tensor([-21.5047], device='cuda:0'))])
epoch£º713	 i:0 	 global-step:14260	 l-p:0.08622606843709946
epoch£º713	 i:1 	 global-step:14261	 l-p:0.03822461515665054
epoch£º713	 i:2 	 global-step:14262	 l-p:0.1354779601097107
epoch£º713	 i:3 	 global-step:14263	 l-p:0.13205479085445404
epoch£º713	 i:4 	 global-step:14264	 l-p:0.15625937283039093
epoch£º713	 i:5 	 global-step:14265	 l-p:0.1412440985441208
epoch£º713	 i:6 	 global-step:14266	 l-p:0.12034360319375992
epoch£º713	 i:7 	 global-step:14267	 l-p:0.1233479306101799
epoch£º713	 i:8 	 global-step:14268	 l-p:0.06635042279958725
epoch£º713	 i:9 	 global-step:14269	 l-p:0.18274417519569397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1049, 5.1049, 5.1049],
        [5.1049, 5.0385, 4.7661],
        [5.1049, 4.9330, 4.6869],
        [5.1049, 5.1007, 5.1047]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.15012331306934357 
model_pd.l_d.mean(): -20.602819442749023 
model_pd.lagr.mean(): -20.452695846557617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4492], device='cuda:0')), ('power', tensor([-21.2868], device='cuda:0'))])
epoch£º714	 i:0 	 global-step:14280	 l-p:0.15012331306934357
epoch£º714	 i:1 	 global-step:14281	 l-p:0.169509619474411
epoch£º714	 i:2 	 global-step:14282	 l-p:0.13799609243869781
epoch£º714	 i:3 	 global-step:14283	 l-p:0.14233112335205078
epoch£º714	 i:4 	 global-step:14284	 l-p:0.1962175965309143
epoch£º714	 i:5 	 global-step:14285	 l-p:0.19837020337581635
epoch£º714	 i:6 	 global-step:14286	 l-p:0.10948137193918228
epoch£º714	 i:7 	 global-step:14287	 l-p:0.10075660794973373
epoch£º714	 i:8 	 global-step:14288	 l-p:0.12567171454429626
epoch£º714	 i:9 	 global-step:14289	 l-p:0.09819713979959488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0427, 5.0306, 5.0415],
        [5.0427, 5.4033, 5.3309],
        [5.0427, 4.8165, 4.6743],
        [5.0427, 5.0428, 5.0427]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): 0.14021241664886475 
model_pd.l_d.mean(): -20.394092559814453 
model_pd.lagr.mean(): -20.25387954711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4871], device='cuda:0')), ('power', tensor([-21.1146], device='cuda:0'))])
epoch£º715	 i:0 	 global-step:14300	 l-p:0.14021241664886475
epoch£º715	 i:1 	 global-step:14301	 l-p:0.2146293818950653
epoch£º715	 i:2 	 global-step:14302	 l-p:0.12208472937345505
epoch£º715	 i:3 	 global-step:14303	 l-p:0.1617542952299118
epoch£º715	 i:4 	 global-step:14304	 l-p:0.19369910657405853
epoch£º715	 i:5 	 global-step:14305	 l-p:0.20050859451293945
epoch£º715	 i:6 	 global-step:14306	 l-p:0.13163045048713684
epoch£º715	 i:7 	 global-step:14307	 l-p:1.1960780620574951
epoch£º715	 i:8 	 global-step:14308	 l-p:0.13368354737758636
epoch£º715	 i:9 	 global-step:14309	 l-p:0.13181515038013458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0381, 4.9148, 4.6390],
        [5.0381, 5.0356, 5.0380],
        [5.0381, 5.0381, 5.0381],
        [5.0381, 5.3774, 5.2912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 0.21496939659118652 
model_pd.l_d.mean(): -20.361040115356445 
model_pd.lagr.mean(): -20.14607048034668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5061], device='cuda:0')), ('power', tensor([-21.1005], device='cuda:0'))])
epoch£º716	 i:0 	 global-step:14320	 l-p:0.21496939659118652
epoch£º716	 i:1 	 global-step:14321	 l-p:0.13996808230876923
epoch£º716	 i:2 	 global-step:14322	 l-p:0.14470776915550232
epoch£º716	 i:3 	 global-step:14323	 l-p:0.13157939910888672
epoch£º716	 i:4 	 global-step:14324	 l-p:0.25491222739219666
epoch£º716	 i:5 	 global-step:14325	 l-p:0.12533481419086456
epoch£º716	 i:6 	 global-step:14326	 l-p:0.13489662110805511
epoch£º716	 i:7 	 global-step:14327	 l-p:0.17277583479881287
epoch£º716	 i:8 	 global-step:14328	 l-p:0.08752813190221786
epoch£º716	 i:9 	 global-step:14329	 l-p:0.1326427310705185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0810, 4.9796, 5.0280],
        [5.0810, 4.9519, 4.9954],
        [5.0810, 5.0757, 5.0807],
        [5.0810, 5.1961, 4.9795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.14881081879138947 
model_pd.l_d.mean(): -20.863828659057617 
model_pd.lagr.mean(): -20.715017318725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4181], device='cuda:0')), ('power', tensor([-21.5190], device='cuda:0'))])
epoch£º717	 i:0 	 global-step:14340	 l-p:0.14881081879138947
epoch£º717	 i:1 	 global-step:14341	 l-p:0.10892602801322937
epoch£º717	 i:2 	 global-step:14342	 l-p:0.20047686994075775
epoch£º717	 i:3 	 global-step:14343	 l-p:0.10373640060424805
epoch£º717	 i:4 	 global-step:14344	 l-p:0.12765750288963318
epoch£º717	 i:5 	 global-step:14345	 l-p:0.06637953221797943
epoch£º717	 i:6 	 global-step:14346	 l-p:0.15705536305904388
epoch£º717	 i:7 	 global-step:14347	 l-p:0.18620267510414124
epoch£º717	 i:8 	 global-step:14348	 l-p:0.223332941532135
epoch£º717	 i:9 	 global-step:14349	 l-p:0.12030942738056183
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228]], device='cuda:0')
 pt:tensor([[5.0791, 4.8775, 4.8469],
        [5.0791, 4.9127, 4.9331],
        [5.0791, 5.4864, 5.4437],
        [5.0791, 4.9403, 4.9801]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.13012292981147766 
model_pd.l_d.mean(): -20.868181228637695 
model_pd.lagr.mean(): -20.73805809020996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4116], device='cuda:0')), ('power', tensor([-21.5167], device='cuda:0'))])
epoch£º718	 i:0 	 global-step:14360	 l-p:0.13012292981147766
epoch£º718	 i:1 	 global-step:14361	 l-p:0.10093025118112564
epoch£º718	 i:2 	 global-step:14362	 l-p:0.19368192553520203
epoch£º718	 i:3 	 global-step:14363	 l-p:0.1478756070137024
epoch£º718	 i:4 	 global-step:14364	 l-p:0.17110919952392578
epoch£º718	 i:5 	 global-step:14365	 l-p:0.10483256727457047
epoch£º718	 i:6 	 global-step:14366	 l-p:0.12324302643537521
epoch£º718	 i:7 	 global-step:14367	 l-p:0.106267049908638
epoch£º718	 i:8 	 global-step:14368	 l-p:0.24872802197933197
epoch£º718	 i:9 	 global-step:14369	 l-p:0.11877916008234024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0671, 4.9174, 4.9517],
        [5.0671, 4.9714, 4.6943],
        [5.0671, 5.1433, 4.9087],
        [5.0671, 5.0446, 5.0636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.12691305577754974 
model_pd.l_d.mean(): -18.66581153869629 
model_pd.lagr.mean(): -18.538898468017578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6021], device='cuda:0')), ('power', tensor([-19.4850], device='cuda:0'))])
epoch£º719	 i:0 	 global-step:14380	 l-p:0.12691305577754974
epoch£º719	 i:1 	 global-step:14381	 l-p:0.18844592571258545
epoch£º719	 i:2 	 global-step:14382	 l-p:0.13167715072631836
epoch£º719	 i:3 	 global-step:14383	 l-p:0.10426925122737885
epoch£º719	 i:4 	 global-step:14384	 l-p:0.14047013223171234
epoch£º719	 i:5 	 global-step:14385	 l-p:0.17369532585144043
epoch£º719	 i:6 	 global-step:14386	 l-p:0.13637913763523102
epoch£º719	 i:7 	 global-step:14387	 l-p:0.1038910448551178
epoch£º719	 i:8 	 global-step:14388	 l-p:0.15759024024009705
epoch£º719	 i:9 	 global-step:14389	 l-p:0.14519254863262177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1012, 5.0314, 5.0750],
        [5.1012, 4.9005, 4.8691],
        [5.1012, 4.9214, 4.6778],
        [5.1012, 4.8812, 4.7817]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.11450354009866714 
model_pd.l_d.mean(): -20.35941505432129 
model_pd.lagr.mean(): -20.244911193847656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4651], device='cuda:0')), ('power', tensor([-21.0570], device='cuda:0'))])
epoch£º720	 i:0 	 global-step:14400	 l-p:0.11450354009866714
epoch£º720	 i:1 	 global-step:14401	 l-p:0.12767331302165985
epoch£º720	 i:2 	 global-step:14402	 l-p:0.06415160745382309
epoch£º720	 i:3 	 global-step:14403	 l-p:0.13973546028137207
epoch£º720	 i:4 	 global-step:14404	 l-p:0.12315816432237625
epoch£º720	 i:5 	 global-step:14405	 l-p:0.1408022791147232
epoch£º720	 i:6 	 global-step:14406	 l-p:0.14282678067684174
epoch£º720	 i:7 	 global-step:14407	 l-p:0.19279463589191437
epoch£º720	 i:8 	 global-step:14408	 l-p:0.14616082608699799
epoch£º720	 i:9 	 global-step:14409	 l-p:0.12153411656618118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228]], device='cuda:0')
 pt:tensor([[5.1082, 4.9684, 4.7016],
        [5.1082, 5.5234, 5.4845],
        [5.1082, 4.8866, 4.7670],
        [5.1082, 4.8904, 4.7185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.11749694496393204 
model_pd.l_d.mean(): -19.610715866088867 
model_pd.lagr.mean(): -19.49321937561035 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5100], device='cuda:0')), ('power', tensor([-20.3460], device='cuda:0'))])
epoch£º721	 i:0 	 global-step:14420	 l-p:0.11749694496393204
epoch£º721	 i:1 	 global-step:14421	 l-p:0.0918637290596962
epoch£º721	 i:2 	 global-step:14422	 l-p:0.14492076635360718
epoch£º721	 i:3 	 global-step:14423	 l-p:0.14954277873039246
epoch£º721	 i:4 	 global-step:14424	 l-p:0.10764166712760925
epoch£º721	 i:5 	 global-step:14425	 l-p:0.13006684184074402
epoch£º721	 i:6 	 global-step:14426	 l-p:0.15767332911491394
epoch£º721	 i:7 	 global-step:14427	 l-p:0.1205369159579277
epoch£º721	 i:8 	 global-step:14428	 l-p:0.13611537218093872
epoch£º721	 i:9 	 global-step:14429	 l-p:0.17503990232944489
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0844, 5.0844, 5.0844],
        [5.0844, 5.0819, 5.0843],
        [5.0844, 5.0844, 5.0844],
        [5.0844, 5.0372, 5.0717]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.10023815929889679 
model_pd.l_d.mean(): -20.226375579833984 
model_pd.lagr.mean(): -20.126136779785156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4995], device='cuda:0')), ('power', tensor([-20.9577], device='cuda:0'))])
epoch£º722	 i:0 	 global-step:14440	 l-p:0.10023815929889679
epoch£º722	 i:1 	 global-step:14441	 l-p:0.13705897331237793
epoch£º722	 i:2 	 global-step:14442	 l-p:0.1614493876695633
epoch£º722	 i:3 	 global-step:14443	 l-p:0.13141439855098724
epoch£º722	 i:4 	 global-step:14444	 l-p:0.13723205029964447
epoch£º722	 i:5 	 global-step:14445	 l-p:0.278645783662796
epoch£º722	 i:6 	 global-step:14446	 l-p:0.12202653288841248
epoch£º722	 i:7 	 global-step:14447	 l-p:0.13400468230247498
epoch£º722	 i:8 	 global-step:14448	 l-p:0.1386304795742035
epoch£º722	 i:9 	 global-step:14449	 l-p:-0.252585768699646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0069, 4.9413, 4.9841],
        [5.0069, 4.9033, 4.6207],
        [5.0069, 5.0067, 5.0069],
        [5.0069, 5.0069, 5.0069]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.11882173269987106 
model_pd.l_d.mean(): -20.539810180664062 
model_pd.lagr.mean(): -20.420988082885742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4924], device='cuda:0')), ('power', tensor([-21.2673], device='cuda:0'))])
epoch£º723	 i:0 	 global-step:14460	 l-p:0.11882173269987106
epoch£º723	 i:1 	 global-step:14461	 l-p:0.2185598462820053
epoch£º723	 i:2 	 global-step:14462	 l-p:0.2787153124809265
epoch£º723	 i:3 	 global-step:14463	 l-p:0.13711391389369965
epoch£º723	 i:4 	 global-step:14464	 l-p:-0.29582130908966064
epoch£º723	 i:5 	 global-step:14465	 l-p:0.21963632106781006
epoch£º723	 i:6 	 global-step:14466	 l-p:0.10836122184991837
epoch£º723	 i:7 	 global-step:14467	 l-p:0.1284245252609253
epoch£º723	 i:8 	 global-step:14468	 l-p:-0.07250698655843735
epoch£º723	 i:9 	 global-step:14469	 l-p:0.17588666081428528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0208, 5.0180, 5.0207],
        [5.0208, 4.9523, 4.6721],
        [5.0208, 5.0088, 5.0196],
        [5.0208, 4.8023, 4.7447]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.12211477011442184 
model_pd.l_d.mean(): -19.830251693725586 
model_pd.lagr.mean(): -19.70813751220703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5193], device='cuda:0')), ('power', tensor([-20.5775], device='cuda:0'))])
epoch£º724	 i:0 	 global-step:14480	 l-p:0.12211477011442184
epoch£º724	 i:1 	 global-step:14481	 l-p:0.11859173327684402
epoch£º724	 i:2 	 global-step:14482	 l-p:0.27075716853141785
epoch£º724	 i:3 	 global-step:14483	 l-p:0.15678445994853973
epoch£º724	 i:4 	 global-step:14484	 l-p:0.12427838891744614
epoch£º724	 i:5 	 global-step:14485	 l-p:0.14931519329547882
epoch£º724	 i:6 	 global-step:14486	 l-p:0.13442550599575043
epoch£º724	 i:7 	 global-step:14487	 l-p:0.15878447890281677
epoch£º724	 i:8 	 global-step:14488	 l-p:0.25841963291168213
epoch£º724	 i:9 	 global-step:14489	 l-p:0.10750353336334229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0701, 5.0659, 5.0699],
        [5.0701, 4.9217, 4.6523],
        [5.0701, 4.8848, 4.6378],
        [5.0701, 4.9392, 4.9833]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.1325272023677826 
model_pd.l_d.mean(): -20.576818466186523 
model_pd.lagr.mean(): -20.444292068481445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4357], device='cuda:0')), ('power', tensor([-21.2468], device='cuda:0'))])
epoch£º725	 i:0 	 global-step:14500	 l-p:0.1325272023677826
epoch£º725	 i:1 	 global-step:14501	 l-p:0.12757335603237152
epoch£º725	 i:2 	 global-step:14502	 l-p:0.13158386945724487
epoch£º725	 i:3 	 global-step:14503	 l-p:0.17258305847644806
epoch£º725	 i:4 	 global-step:14504	 l-p:0.13229797780513763
epoch£º725	 i:5 	 global-step:14505	 l-p:0.3533475697040558
epoch£º725	 i:6 	 global-step:14506	 l-p:0.21404658257961273
epoch£º725	 i:7 	 global-step:14507	 l-p:0.10531127452850342
epoch£º725	 i:8 	 global-step:14508	 l-p:0.1141403466463089
epoch£º725	 i:9 	 global-step:14509	 l-p:0.1265724003314972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]], device='cuda:0')
 pt:tensor([[5.0324, 4.9071, 4.9540],
        [5.0324, 4.8010, 4.6685],
        [5.0324, 4.9835, 4.7063],
        [5.0324, 5.2287, 5.0540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): 0.20389774441719055 
model_pd.l_d.mean(): -19.227174758911133 
model_pd.lagr.mean(): -19.023277282714844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5743], device='cuda:0')), ('power', tensor([-20.0240], device='cuda:0'))])
epoch£º726	 i:0 	 global-step:14520	 l-p:0.20389774441719055
epoch£º726	 i:1 	 global-step:14521	 l-p:0.19639089703559875
epoch£º726	 i:2 	 global-step:14522	 l-p:0.20788002014160156
epoch£º726	 i:3 	 global-step:14523	 l-p:0.19165515899658203
epoch£º726	 i:4 	 global-step:14524	 l-p:0.15111729502677917
epoch£º726	 i:5 	 global-step:14525	 l-p:0.13123856484889984
epoch£º726	 i:6 	 global-step:14526	 l-p:0.2409018725156784
epoch£º726	 i:7 	 global-step:14527	 l-p:0.06921974569559097
epoch£º726	 i:8 	 global-step:14528	 l-p:0.15245528519153595
epoch£º726	 i:9 	 global-step:14529	 l-p:0.1227201372385025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0836, 5.0836, 5.0836],
        [5.0836, 5.2288, 5.0255],
        [5.0836, 5.4915, 5.4477],
        [5.0836, 5.0836, 5.0836]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.16005103290081024 
model_pd.l_d.mean(): -20.531517028808594 
model_pd.lagr.mean(): -20.3714656829834 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4740], device='cuda:0')), ('power', tensor([-21.2401], device='cuda:0'))])
epoch£º727	 i:0 	 global-step:14540	 l-p:0.16005103290081024
epoch£º727	 i:1 	 global-step:14541	 l-p:0.12385598570108414
epoch£º727	 i:2 	 global-step:14542	 l-p:0.1432354748249054
epoch£º727	 i:3 	 global-step:14543	 l-p:0.11555179953575134
epoch£º727	 i:4 	 global-step:14544	 l-p:0.14193910360336304
epoch£º727	 i:5 	 global-step:14545	 l-p:0.21857769787311554
epoch£º727	 i:6 	 global-step:14546	 l-p:0.12820777297019958
epoch£º727	 i:7 	 global-step:14547	 l-p:0.16621774435043335
epoch£º727	 i:8 	 global-step:14548	 l-p:0.15179398655891418
epoch£º727	 i:9 	 global-step:14549	 l-p:0.07455862313508987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1151, 5.1016, 5.1136],
        [5.1151, 5.1143, 5.1151],
        [5.1151, 4.9338, 4.6889],
        [5.1151, 5.5641, 5.5473]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.13080067932605743 
model_pd.l_d.mean(): -20.46898078918457 
model_pd.lagr.mean(): -20.338180541992188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4609], device='cuda:0')), ('power', tensor([-21.1635], device='cuda:0'))])
epoch£º728	 i:0 	 global-step:14560	 l-p:0.13080067932605743
epoch£º728	 i:1 	 global-step:14561	 l-p:0.10186179727315903
epoch£º728	 i:2 	 global-step:14562	 l-p:0.1258534938097
epoch£º728	 i:3 	 global-step:14563	 l-p:0.14479728043079376
epoch£º728	 i:4 	 global-step:14564	 l-p:0.136084645986557
epoch£º728	 i:5 	 global-step:14565	 l-p:0.13216418027877808
epoch£º728	 i:6 	 global-step:14566	 l-p:1.65675687789917
epoch£º728	 i:7 	 global-step:14567	 l-p:0.1362648606300354
epoch£º728	 i:8 	 global-step:14568	 l-p:0.11123711615800858
epoch£º728	 i:9 	 global-step:14569	 l-p:0.10053782165050507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1938, 5.0589, 5.0981],
        [5.1938, 5.1211, 5.1653],
        [5.1938, 5.0312, 5.0506],
        [5.1938, 5.1938, 5.1938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): 0.4324813485145569 
model_pd.l_d.mean(): -19.3067569732666 
model_pd.lagr.mean(): -18.87427520751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5635], device='cuda:0')), ('power', tensor([-20.0934], device='cuda:0'))])
epoch£º729	 i:0 	 global-step:14580	 l-p:0.4324813485145569
epoch£º729	 i:1 	 global-step:14581	 l-p:0.11089953780174255
epoch£º729	 i:2 	 global-step:14582	 l-p:0.1575092226266861
epoch£º729	 i:3 	 global-step:14583	 l-p:0.12688623368740082
epoch£º729	 i:4 	 global-step:14584	 l-p:0.13832511007785797
epoch£º729	 i:5 	 global-step:14585	 l-p:0.11196210980415344
epoch£º729	 i:6 	 global-step:14586	 l-p:0.12429061532020569
epoch£º729	 i:7 	 global-step:14587	 l-p:0.018636466935276985
epoch£º729	 i:8 	 global-step:14588	 l-p:0.1342962384223938
epoch£º729	 i:9 	 global-step:14589	 l-p:0.11897052824497223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5584,  0.4599,  1.0000,  0.3787,
          1.0000,  0.8235, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4602,  0.3553,  1.0000,  0.2743,
          1.0000,  0.7721, 31.6228]], device='cuda:0')
 pt:tensor([[5.1324, 5.0573, 4.7813],
        [5.1324, 5.3441, 5.1752],
        [5.1324, 4.9577, 4.7090],
        [5.1324, 4.9668, 4.7115]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.03340774402022362 
model_pd.l_d.mean(): -19.295127868652344 
model_pd.lagr.mean(): -19.261720657348633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5499], device='cuda:0')), ('power', tensor([-20.0678], device='cuda:0'))])
epoch£º730	 i:0 	 global-step:14600	 l-p:0.03340774402022362
epoch£º730	 i:1 	 global-step:14601	 l-p:0.13715766370296478
epoch£º730	 i:2 	 global-step:14602	 l-p:0.16929081082344055
epoch£º730	 i:3 	 global-step:14603	 l-p:0.1228676587343216
epoch£º730	 i:4 	 global-step:14604	 l-p:0.142097607254982
epoch£º730	 i:5 	 global-step:14605	 l-p:0.1332094520330429
epoch£º730	 i:6 	 global-step:14606	 l-p:0.1358984261751175
epoch£º730	 i:7 	 global-step:14607	 l-p:0.1385597437620163
epoch£º730	 i:8 	 global-step:14608	 l-p:0.2708018720149994
epoch£º730	 i:9 	 global-step:14609	 l-p:0.08968586474657059
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0456, 5.3541, 5.2456],
        [5.0456, 4.8627, 4.8711],
        [5.0456, 4.8140, 4.6706],
        [5.0456, 5.0347, 5.0446]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.11888208240270615 
model_pd.l_d.mean(): -20.272541046142578 
model_pd.lagr.mean(): -20.15365982055664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5152], device='cuda:0')), ('power', tensor([-21.0204], device='cuda:0'))])
epoch£º731	 i:0 	 global-step:14620	 l-p:0.11888208240270615
epoch£º731	 i:1 	 global-step:14621	 l-p:0.12893977761268616
epoch£º731	 i:2 	 global-step:14622	 l-p:0.1939879208803177
epoch£º731	 i:3 	 global-step:14623	 l-p:0.22903476655483246
epoch£º731	 i:4 	 global-step:14624	 l-p:0.13718590140342712
epoch£º731	 i:5 	 global-step:14625	 l-p:0.2283957451581955
epoch£º731	 i:6 	 global-step:14626	 l-p:-0.04983898997306824
epoch£º731	 i:7 	 global-step:14627	 l-p:0.7892674803733826
epoch£º731	 i:8 	 global-step:14628	 l-p:0.020796895027160645
epoch£º731	 i:9 	 global-step:14629	 l-p:0.11991652101278305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0000, 4.9224, 4.6381],
        [5.0000, 4.9976, 4.9999],
        [5.0000, 5.0000, 5.0000],
        [5.0000, 4.9962, 4.9998]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): 0.3291837275028229 
model_pd.l_d.mean(): -19.533382415771484 
model_pd.lagr.mean(): -19.204198837280273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5538], device='cuda:0')), ('power', tensor([-20.3126], device='cuda:0'))])
epoch£º732	 i:0 	 global-step:14640	 l-p:0.3291837275028229
epoch£º732	 i:1 	 global-step:14641	 l-p:0.28064775466918945
epoch£º732	 i:2 	 global-step:14642	 l-p:0.12513631582260132
epoch£º732	 i:3 	 global-step:14643	 l-p:0.1316756308078766
epoch£º732	 i:4 	 global-step:14644	 l-p:0.2636585235595703
epoch£º732	 i:5 	 global-step:14645	 l-p:0.11597726494073868
epoch£º732	 i:6 	 global-step:14646	 l-p:0.18704771995544434
epoch£º732	 i:7 	 global-step:14647	 l-p:0.09463519603013992
epoch£º732	 i:8 	 global-step:14648	 l-p:0.13772228360176086
epoch£º732	 i:9 	 global-step:14649	 l-p:0.23486942052841187
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0648, 5.0648, 5.0648],
        [5.0648, 5.0528, 5.0636],
        [5.0648, 4.9865, 5.0330],
        [5.0648, 5.0320, 5.0582]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.13266782462596893 
model_pd.l_d.mean(): -19.85294532775879 
model_pd.lagr.mean(): -19.720277786254883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5146], device='cuda:0')), ('power', tensor([-20.5956], device='cuda:0'))])
epoch£º733	 i:0 	 global-step:14660	 l-p:0.13266782462596893
epoch£º733	 i:1 	 global-step:14661	 l-p:0.10620725154876709
epoch£º733	 i:2 	 global-step:14662	 l-p:0.22079522907733917
epoch£º733	 i:3 	 global-step:14663	 l-p:0.1703314334154129
epoch£º733	 i:4 	 global-step:14664	 l-p:0.06993627548217773
epoch£º733	 i:5 	 global-step:14665	 l-p:0.14965412020683289
epoch£º733	 i:6 	 global-step:14666	 l-p:0.16511747241020203
epoch£º733	 i:7 	 global-step:14667	 l-p:0.10671722143888474
epoch£º733	 i:8 	 global-step:14668	 l-p:0.16477422416210175
epoch£º733	 i:9 	 global-step:14669	 l-p:0.10606225579977036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:734
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1282, 5.1269, 5.1281],
        [5.1282, 5.2480, 5.0305],
        [5.1282, 5.4665, 5.3741],
        [5.1282, 4.9269, 4.7056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:734, step:0 
model_pd.l_p.mean(): 0.15597742795944214 
model_pd.l_d.mean(): -20.437002182006836 
model_pd.lagr.mean(): -20.281024932861328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4561], device='cuda:0')), ('power', tensor([-21.1263], device='cuda:0'))])
epoch£º734	 i:0 	 global-step:14680	 l-p:0.15597742795944214
epoch£º734	 i:1 	 global-step:14681	 l-p:0.11107789725065231
epoch£º734	 i:2 	 global-step:14682	 l-p:0.10349048674106598
epoch£º734	 i:3 	 global-step:14683	 l-p:0.16211490333080292
epoch£º734	 i:4 	 global-step:14684	 l-p:0.11095238476991653
epoch£º734	 i:5 	 global-step:14685	 l-p:0.14006416499614716
epoch£º734	 i:6 	 global-step:14686	 l-p:0.07338730990886688
epoch£º734	 i:7 	 global-step:14687	 l-p:0.10609013587236404
epoch£º734	 i:8 	 global-step:14688	 l-p:0.13138198852539062
epoch£º734	 i:9 	 global-step:14689	 l-p:0.23459605872631073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:735
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0907, 4.8650, 4.7651],
        [5.0907, 5.0074, 5.0549],
        [5.0907, 4.9990, 4.7186],
        [5.0907, 4.9686, 4.6906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:735, step:0 
model_pd.l_p.mean(): 0.1459549516439438 
model_pd.l_d.mean(): -20.022872924804688 
model_pd.lagr.mean(): -19.87691879272461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4780], device='cuda:0')), ('power', tensor([-20.7300], device='cuda:0'))])
epoch£º735	 i:0 	 global-step:14700	 l-p:0.1459549516439438
epoch£º735	 i:1 	 global-step:14701	 l-p:0.09794851392507553
epoch£º735	 i:2 	 global-step:14702	 l-p:0.10567188262939453
epoch£º735	 i:3 	 global-step:14703	 l-p:0.13141795992851257
epoch£º735	 i:4 	 global-step:14704	 l-p:0.2602662146091461
epoch£º735	 i:5 	 global-step:14705	 l-p:0.13540957868099213
epoch£º735	 i:6 	 global-step:14706	 l-p:0.12757757306098938
epoch£º735	 i:7 	 global-step:14707	 l-p:0.16257987916469574
epoch£º735	 i:8 	 global-step:14708	 l-p:0.1527458131313324
epoch£º735	 i:9 	 global-step:14709	 l-p:0.2542960047721863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:736
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0679, 5.3948, 5.2965],
        [5.0679, 5.0011, 5.0444],
        [5.0679, 4.8372, 4.6843],
        [5.0679, 4.8367, 4.6928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:736, step:0 
model_pd.l_p.mean(): 0.16684798896312714 
model_pd.l_d.mean(): -20.449237823486328 
model_pd.lagr.mean(): -20.282390594482422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4920], device='cuda:0')), ('power', tensor([-21.1753], device='cuda:0'))])
epoch£º736	 i:0 	 global-step:14720	 l-p:0.16684798896312714
epoch£º736	 i:1 	 global-step:14721	 l-p:0.10973910242319107
epoch£º736	 i:2 	 global-step:14722	 l-p:0.18869085609912872
epoch£º736	 i:3 	 global-step:14723	 l-p:0.13851352035999298
epoch£º736	 i:4 	 global-step:14724	 l-p:0.09257882088422775
epoch£º736	 i:5 	 global-step:14725	 l-p:0.12875935435295105
epoch£º736	 i:6 	 global-step:14726	 l-p:0.13646173477172852
epoch£º736	 i:7 	 global-step:14727	 l-p:0.12566421926021576
epoch£º736	 i:8 	 global-step:14728	 l-p:0.09084716439247131
epoch£º736	 i:9 	 global-step:14729	 l-p:0.16041463613510132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:737
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1343, 5.1011, 5.1275],
        [5.1343, 5.5798, 5.5585],
        [5.1343, 5.0342, 4.7561],
        [5.1343, 5.5116, 5.4438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:737, step:0 
model_pd.l_p.mean(): 0.12191915512084961 
model_pd.l_d.mean(): -20.677227020263672 
model_pd.lagr.mean(): -20.555307388305664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4119], device='cuda:0')), ('power', tensor([-21.3240], device='cuda:0'))])
epoch£º737	 i:0 	 global-step:14740	 l-p:0.12191915512084961
epoch£º737	 i:1 	 global-step:14741	 l-p:0.18094806373119354
epoch£º737	 i:2 	 global-step:14742	 l-p:0.043845660984516144
epoch£º737	 i:3 	 global-step:14743	 l-p:0.10494963079690933
epoch£º737	 i:4 	 global-step:14744	 l-p:0.15600882470607758
epoch£º737	 i:5 	 global-step:14745	 l-p:0.12500256299972534
epoch£º737	 i:6 	 global-step:14746	 l-p:0.12934991717338562
epoch£º737	 i:7 	 global-step:14747	 l-p:0.12761981785297394
epoch£º737	 i:8 	 global-step:14748	 l-p:0.13771645724773407
epoch£º737	 i:9 	 global-step:14749	 l-p:0.11424847692251205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:738
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1236, 5.1114, 5.1224],
        [5.1236, 5.1211, 5.1235],
        [5.1236, 5.1229, 5.1236],
        [5.1236, 5.1236, 5.1236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:738, step:0 
model_pd.l_p.mean(): 0.10189493000507355 
model_pd.l_d.mean(): -20.268362045288086 
model_pd.lagr.mean(): -20.166467666625977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4832], device='cuda:0')), ('power', tensor([-20.9835], device='cuda:0'))])
epoch£º738	 i:0 	 global-step:14760	 l-p:0.10189493000507355
epoch£º738	 i:1 	 global-step:14761	 l-p:0.1351902037858963
epoch£º738	 i:2 	 global-step:14762	 l-p:0.08962611109018326
epoch£º738	 i:3 	 global-step:14763	 l-p:0.1447358876466751
epoch£º738	 i:4 	 global-step:14764	 l-p:0.12810789048671722
epoch£º738	 i:5 	 global-step:14765	 l-p:0.1470578908920288
epoch£º738	 i:6 	 global-step:14766	 l-p:0.14721715450286865
epoch£º738	 i:7 	 global-step:14767	 l-p:0.15415984392166138
epoch£º738	 i:8 	 global-step:14768	 l-p:0.15365096926689148
epoch£º738	 i:9 	 global-step:14769	 l-p:0.17436549067497253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:739
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0851, 5.0852, 5.0851],
        [5.0851, 5.0851, 5.0852],
        [5.0851, 5.0169, 4.7354],
        [5.0851, 4.9515, 4.9961]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:739, step:0 
model_pd.l_p.mean(): 0.1550709754228592 
model_pd.l_d.mean(): -19.724458694458008 
model_pd.lagr.mean(): -19.569387435913086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5325], device='cuda:0')), ('power', tensor([-20.4840], device='cuda:0'))])
epoch£º739	 i:0 	 global-step:14780	 l-p:0.1550709754228592
epoch£º739	 i:1 	 global-step:14781	 l-p:0.13096165657043457
epoch£º739	 i:2 	 global-step:14782	 l-p:0.18280856311321259
epoch£º739	 i:3 	 global-step:14783	 l-p:0.15888139605522156
epoch£º739	 i:4 	 global-step:14784	 l-p:0.12613512575626373
epoch£º739	 i:5 	 global-step:14785	 l-p:0.1773027926683426
epoch£º739	 i:6 	 global-step:14786	 l-p:0.08867046236991882
epoch£º739	 i:7 	 global-step:14787	 l-p:0.13144193589687347
epoch£º739	 i:8 	 global-step:14788	 l-p:0.06655043363571167
epoch£º739	 i:9 	 global-step:14789	 l-p:0.0917188823223114
====================================================================================================
====================================================================================================
====================================================================================================

epoch:740
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1471, 5.1471, 5.1471],
        [5.1471, 5.1359, 5.1460],
        [5.1471, 4.9360, 4.8857],
        [5.1471, 4.9437, 4.7222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:740, step:0 
model_pd.l_p.mean(): 0.12957637012004852 
model_pd.l_d.mean(): -19.21841049194336 
model_pd.lagr.mean(): -19.088834762573242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5253], device='cuda:0')), ('power', tensor([-19.9651], device='cuda:0'))])
epoch£º740	 i:0 	 global-step:14800	 l-p:0.12957637012004852
epoch£º740	 i:1 	 global-step:14801	 l-p:0.05165228620171547
epoch£º740	 i:2 	 global-step:14802	 l-p:0.10027721524238586
epoch£º740	 i:3 	 global-step:14803	 l-p:0.062250249087810516
epoch£º740	 i:4 	 global-step:14804	 l-p:0.13507093489170074
epoch£º740	 i:5 	 global-step:14805	 l-p:0.1283915638923645
epoch£º740	 i:6 	 global-step:14806	 l-p:0.1436701864004135
epoch£º740	 i:7 	 global-step:14807	 l-p:0.1468462496995926
epoch£º740	 i:8 	 global-step:14808	 l-p:0.13329452276229858
epoch£º740	 i:9 	 global-step:14809	 l-p:0.15157455205917358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:741
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1360, 4.9712, 4.7088],
        [5.1360, 5.5504, 5.5066],
        [5.1360, 4.9792, 4.7127],
        [5.1360, 5.1360, 5.1360]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:741, step:0 
model_pd.l_p.mean(): 0.11633912473917007 
model_pd.l_d.mean(): -20.3355770111084 
model_pd.lagr.mean(): -20.21923828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4813], device='cuda:0')), ('power', tensor([-21.0495], device='cuda:0'))])
epoch£º741	 i:0 	 global-step:14820	 l-p:0.11633912473917007
epoch£º741	 i:1 	 global-step:14821	 l-p:0.1074812039732933
epoch£º741	 i:2 	 global-step:14822	 l-p:0.12807375192642212
epoch£º741	 i:3 	 global-step:14823	 l-p:0.15815240144729614
epoch£º741	 i:4 	 global-step:14824	 l-p:0.07547995448112488
epoch£º741	 i:5 	 global-step:14825	 l-p:0.1156095489859581
epoch£º741	 i:6 	 global-step:14826	 l-p:0.14825516939163208
epoch£º741	 i:7 	 global-step:14827	 l-p:0.1490337997674942
epoch£º741	 i:8 	 global-step:14828	 l-p:0.16697926819324493
epoch£º741	 i:9 	 global-step:14829	 l-p:0.1491827517747879
====================================================================================================
====================================================================================================
====================================================================================================

epoch:742
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0803, 5.4781, 5.4247],
        [5.0803, 4.9528, 4.9998],
        [5.0803, 5.0762, 5.0801],
        [5.0803, 4.8521, 4.7621]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:742, step:0 
model_pd.l_p.mean(): 0.13867734372615814 
model_pd.l_d.mean(): -19.84881591796875 
model_pd.lagr.mean(): -19.71013832092285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5532], device='cuda:0')), ('power', tensor([-20.6308], device='cuda:0'))])
epoch£º742	 i:0 	 global-step:14840	 l-p:0.13867734372615814
epoch£º742	 i:1 	 global-step:14841	 l-p:0.12049628794193268
epoch£º742	 i:2 	 global-step:14842	 l-p:0.11581036448478699
epoch£º742	 i:3 	 global-step:14843	 l-p:0.1342707723379135
epoch£º742	 i:4 	 global-step:14844	 l-p:0.2480132281780243
epoch£º742	 i:5 	 global-step:14845	 l-p:0.14698222279548645
epoch£º742	 i:6 	 global-step:14846	 l-p:0.21820425987243652
epoch£º742	 i:7 	 global-step:14847	 l-p:0.12320973724126816
epoch£º742	 i:8 	 global-step:14848	 l-p:0.0734633207321167
epoch£º742	 i:9 	 global-step:14849	 l-p:0.12550608813762665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:743
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1039, 5.1039, 5.1039],
        [5.1039, 5.0389, 5.0816],
        [5.1039, 5.0611, 4.7827],
        [5.1039, 4.9261, 4.6659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:743, step:0 
model_pd.l_p.mean(): 0.06192794069647789 
model_pd.l_d.mean(): -19.88631248474121 
model_pd.lagr.mean(): -19.824384689331055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5227], device='cuda:0')), ('power', tensor([-20.6376], device='cuda:0'))])
epoch£º743	 i:0 	 global-step:14860	 l-p:0.06192794069647789
epoch£º743	 i:1 	 global-step:14861	 l-p:0.1294301301240921
epoch£º743	 i:2 	 global-step:14862	 l-p:0.1376597136259079
epoch£º743	 i:3 	 global-step:14863	 l-p:0.13767990469932556
epoch£º743	 i:4 	 global-step:14864	 l-p:0.13037307560443878
epoch£º743	 i:5 	 global-step:14865	 l-p:0.1261826604604721
epoch£º743	 i:6 	 global-step:14866	 l-p:0.162273570895195
epoch£º743	 i:7 	 global-step:14867	 l-p:0.1409420371055603
epoch£º743	 i:8 	 global-step:14868	 l-p:0.12340492010116577
epoch£º743	 i:9 	 global-step:14869	 l-p:0.15802648663520813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:744
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1149, 5.0300, 5.0781],
        [5.1149, 4.9797, 5.0236],
        [5.1149, 5.0314, 5.0792],
        [5.1149, 5.0766, 5.1063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:744, step:0 
model_pd.l_p.mean(): 0.1322527825832367 
model_pd.l_d.mean(): -20.913745880126953 
model_pd.lagr.mean(): -20.781492233276367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3899], device='cuda:0')), ('power', tensor([-21.5406], device='cuda:0'))])
epoch£º744	 i:0 	 global-step:14880	 l-p:0.1322527825832367
epoch£º744	 i:1 	 global-step:14881	 l-p:0.13004955649375916
epoch£º744	 i:2 	 global-step:14882	 l-p:0.10855324566364288
epoch£º744	 i:3 	 global-step:14883	 l-p:0.10956235975027084
epoch£º744	 i:4 	 global-step:14884	 l-p:0.20360159873962402
epoch£º744	 i:5 	 global-step:14885	 l-p:0.09094439446926117
epoch£º744	 i:6 	 global-step:14886	 l-p:0.1500760167837143
epoch£º744	 i:7 	 global-step:14887	 l-p:0.13300782442092896
epoch£º744	 i:8 	 global-step:14888	 l-p:3.0624022483825684
epoch£º744	 i:9 	 global-step:14889	 l-p:0.2652902901172638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:745
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0301, 4.9965, 5.0233],
        [5.0301, 5.0300, 5.0301],
        [5.0301, 5.4328, 5.3838],
        [5.0301, 4.8709, 4.5903]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:745, step:0 
model_pd.l_p.mean(): 0.26221999526023865 
model_pd.l_d.mean(): -19.694456100463867 
model_pd.lagr.mean(): -19.432235717773438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5519], device='cuda:0')), ('power', tensor([-20.4735], device='cuda:0'))])
epoch£º745	 i:0 	 global-step:14900	 l-p:0.26221999526023865
epoch£º745	 i:1 	 global-step:14901	 l-p:0.13781757652759552
epoch£º745	 i:2 	 global-step:14902	 l-p:0.26037269830703735
epoch£º745	 i:3 	 global-step:14903	 l-p:0.10934661328792572
epoch£º745	 i:4 	 global-step:14904	 l-p:0.13363558053970337
epoch£º745	 i:5 	 global-step:14905	 l-p:0.1209828332066536
epoch£º745	 i:6 	 global-step:14906	 l-p:0.31323570013046265
epoch£º745	 i:7 	 global-step:14907	 l-p:0.15606164932250977
epoch£º745	 i:8 	 global-step:14908	 l-p:0.1814027726650238
epoch£º745	 i:9 	 global-step:14909	 l-p:0.1184384673833847
====================================================================================================
====================================================================================================
====================================================================================================

epoch:746
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0811, 5.0744, 5.0807],
        [5.0811, 5.0714, 5.0803],
        [5.0811, 5.1222, 4.8675],
        [5.0811, 5.0811, 5.0811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:746, step:0 
model_pd.l_p.mean(): 0.14608058333396912 
model_pd.l_d.mean(): -19.261873245239258 
model_pd.lagr.mean(): -19.115793228149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5532], device='cuda:0')), ('power', tensor([-20.0375], device='cuda:0'))])
epoch£º746	 i:0 	 global-step:14920	 l-p:0.14608058333396912
epoch£º746	 i:1 	 global-step:14921	 l-p:0.2004266083240509
epoch£º746	 i:2 	 global-step:14922	 l-p:0.12723590433597565
epoch£º746	 i:3 	 global-step:14923	 l-p:0.13294675946235657
epoch£º746	 i:4 	 global-step:14924	 l-p:0.12407337874174118
epoch£º746	 i:5 	 global-step:14925	 l-p:0.1282188594341278
epoch£º746	 i:6 	 global-step:14926	 l-p:0.13435840606689453
epoch£º746	 i:7 	 global-step:14927	 l-p:0.10761833190917969
epoch£º746	 i:8 	 global-step:14928	 l-p:0.1102696880698204
epoch£º746	 i:9 	 global-step:14929	 l-p:0.12124359607696533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:747
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1390, 5.0856, 5.1233],
        [5.1390, 5.1006, 5.1303],
        [5.1390, 5.1442, 4.8776],
        [5.1390, 5.1649, 4.9050]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:747, step:0 
model_pd.l_p.mean(): 0.18123836815357208 
model_pd.l_d.mean(): -19.936437606811523 
model_pd.lagr.mean(): -19.755199432373047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5003], device='cuda:0')), ('power', tensor([-20.6654], device='cuda:0'))])
epoch£º747	 i:0 	 global-step:14940	 l-p:0.18123836815357208
epoch£º747	 i:1 	 global-step:14941	 l-p:0.16352100670337677
epoch£º747	 i:2 	 global-step:14942	 l-p:0.13571825623512268
epoch£º747	 i:3 	 global-step:14943	 l-p:0.1049194484949112
epoch£º747	 i:4 	 global-step:14944	 l-p:0.13265085220336914
epoch£º747	 i:5 	 global-step:14945	 l-p:0.10044977813959122
epoch£º747	 i:6 	 global-step:14946	 l-p:0.15357668697834015
epoch£º747	 i:7 	 global-step:14947	 l-p:0.08278511464595795
epoch£º747	 i:8 	 global-step:14948	 l-p:0.14953508973121643
epoch£º747	 i:9 	 global-step:14949	 l-p:0.14308452606201172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:748
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0794, 4.8849, 4.8825],
        [5.0794, 5.4729, 5.4156],
        [5.0794, 4.9684, 5.0188],
        [5.0794, 4.8702, 4.8431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:748, step:0 
model_pd.l_p.mean(): 0.16241934895515442 
model_pd.l_d.mean(): -20.736621856689453 
model_pd.lagr.mean(): -20.574201583862305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4395], device='cuda:0')), ('power', tensor([-21.4122], device='cuda:0'))])
epoch£º748	 i:0 	 global-step:14960	 l-p:0.16241934895515442
epoch£º748	 i:1 	 global-step:14961	 l-p:0.15149268507957458
epoch£º748	 i:2 	 global-step:14962	 l-p:0.1795574575662613
epoch£º748	 i:3 	 global-step:14963	 l-p:0.1306605190038681
epoch£º748	 i:4 	 global-step:14964	 l-p:0.0919567123055458
epoch£º748	 i:5 	 global-step:14965	 l-p:0.2524109482765198
epoch£º748	 i:6 	 global-step:14966	 l-p:0.10225285589694977
epoch£º748	 i:7 	 global-step:14967	 l-p:0.1671319603919983
epoch£º748	 i:8 	 global-step:14968	 l-p:0.12527069449424744
epoch£º748	 i:9 	 global-step:14969	 l-p:0.12482898682355881
====================================================================================================
====================================================================================================
====================================================================================================

epoch:749
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0799, 5.4334, 5.3493],
        [5.0799, 5.0567, 5.0763],
        [5.0799, 5.0525, 5.0751],
        [5.0799, 4.8534, 4.6491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:749, step:0 
model_pd.l_p.mean(): 0.13757874071598053 
model_pd.l_d.mean(): -20.092893600463867 
model_pd.lagr.mean(): -19.95531463623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5260], device='cuda:0')), ('power', tensor([-20.8498], device='cuda:0'))])
epoch£º749	 i:0 	 global-step:14980	 l-p:0.13757874071598053
epoch£º749	 i:1 	 global-step:14981	 l-p:0.12983666360378265
epoch£º749	 i:2 	 global-step:14982	 l-p:0.13070441782474518
epoch£º749	 i:3 	 global-step:14983	 l-p:0.13437116146087646
epoch£º749	 i:4 	 global-step:14984	 l-p:0.10977023839950562
epoch£º749	 i:5 	 global-step:14985	 l-p:0.15285763144493103
epoch£º749	 i:6 	 global-step:14986	 l-p:0.2807520031929016
epoch£º749	 i:7 	 global-step:14987	 l-p:0.2002461552619934
epoch£º749	 i:8 	 global-step:14988	 l-p:0.1432676464319229
epoch£º749	 i:9 	 global-step:14989	 l-p:0.12474872916936874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:750
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0805, 4.9825, 4.6950],
        [5.0805, 5.3918, 5.2804],
        [5.0805, 5.0760, 5.0803],
        [5.0805, 5.0750, 5.0802]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:750, step:0 
model_pd.l_p.mean(): 0.1199345737695694 
model_pd.l_d.mean(): -20.414257049560547 
model_pd.lagr.mean(): -20.294322967529297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4774], device='cuda:0')), ('power', tensor([-21.1250], device='cuda:0'))])
epoch£º750	 i:0 	 global-step:15000	 l-p:0.1199345737695694
epoch£º750	 i:1 	 global-step:15001	 l-p:0.19734685122966766
epoch£º750	 i:2 	 global-step:15002	 l-p:0.19932802021503448
epoch£º750	 i:3 	 global-step:15003	 l-p:0.2016591578722
epoch£º750	 i:4 	 global-step:15004	 l-p:0.03979237377643585
epoch£º750	 i:5 	 global-step:15005	 l-p:0.13678237795829773
epoch£º750	 i:6 	 global-step:15006	 l-p:0.12444978207349777
epoch£º750	 i:7 	 global-step:15007	 l-p:0.12244307994842529
epoch£º750	 i:8 	 global-step:15008	 l-p:0.1243688240647316
epoch£º750	 i:9 	 global-step:15009	 l-p:0.13350535929203033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:751
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1161, 5.0028, 5.0525],
        [5.1161, 5.0475, 4.7635],
        [5.1161, 5.1161, 5.1161],
        [5.1161, 4.9440, 4.9657]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:751, step:0 
model_pd.l_p.mean(): 0.15392251312732697 
model_pd.l_d.mean(): -20.67691993713379 
model_pd.lagr.mean(): -20.52299690246582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4564], device='cuda:0')), ('power', tensor([-21.3691], device='cuda:0'))])
epoch£º751	 i:0 	 global-step:15020	 l-p:0.15392251312732697
epoch£º751	 i:1 	 global-step:15021	 l-p:0.1269250512123108
epoch£º751	 i:2 	 global-step:15022	 l-p:0.16263709962368011
epoch£º751	 i:3 	 global-step:15023	 l-p:0.17302186787128448
epoch£º751	 i:4 	 global-step:15024	 l-p:0.0829072967171669
epoch£º751	 i:5 	 global-step:15025	 l-p:0.11808568984270096
epoch£º751	 i:6 	 global-step:15026	 l-p:0.14502273499965668
epoch£º751	 i:7 	 global-step:15027	 l-p:0.1312493234872818
epoch£º751	 i:8 	 global-step:15028	 l-p:0.14958199858665466
epoch£º751	 i:9 	 global-step:15029	 l-p:0.09832927584648132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:752
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1077, 5.1077, 5.1077],
        [5.1077, 5.0667, 5.0980],
        [5.1077, 4.9793, 5.0266],
        [5.1077, 5.1033, 5.1075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:752, step:0 
model_pd.l_p.mean(): 0.10726217180490494 
model_pd.l_d.mean(): -19.463417053222656 
model_pd.lagr.mean(): -19.356155395507812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5426], device='cuda:0')), ('power', tensor([-20.2304], device='cuda:0'))])
epoch£º752	 i:0 	 global-step:15040	 l-p:0.10726217180490494
epoch£º752	 i:1 	 global-step:15041	 l-p:0.12341643869876862
epoch£º752	 i:2 	 global-step:15042	 l-p:0.09343758225440979
epoch£º752	 i:3 	 global-step:15043	 l-p:0.1307157576084137
epoch£º752	 i:4 	 global-step:15044	 l-p:0.17164717614650726
epoch£º752	 i:5 	 global-step:15045	 l-p:0.1782989203929901
epoch£º752	 i:6 	 global-step:15046	 l-p:0.13062189519405365
epoch£º752	 i:7 	 global-step:15047	 l-p:0.13651926815509796
epoch£º752	 i:8 	 global-step:15048	 l-p:0.12926648557186127
epoch£º752	 i:9 	 global-step:15049	 l-p:0.12131043523550034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:753
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1324, 5.1304, 5.1323],
        [5.1324, 5.1323, 5.1324],
        [5.1324, 4.9988, 5.0440],
        [5.1324, 4.9141, 4.8602]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:753, step:0 
model_pd.l_p.mean(): 0.1352647989988327 
model_pd.l_d.mean(): -20.588104248046875 
model_pd.lagr.mean(): -20.452838897705078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4590], device='cuda:0')), ('power', tensor([-21.2820], device='cuda:0'))])
epoch£º753	 i:0 	 global-step:15060	 l-p:0.1352647989988327
epoch£º753	 i:1 	 global-step:15061	 l-p:0.14232465624809265
epoch£º753	 i:2 	 global-step:15062	 l-p:0.13080327212810516
epoch£º753	 i:3 	 global-step:15063	 l-p:0.15956149995326996
epoch£º753	 i:4 	 global-step:15064	 l-p:0.15613463521003723
epoch£º753	 i:5 	 global-step:15065	 l-p:0.06935900449752808
epoch£º753	 i:6 	 global-step:15066	 l-p:0.1991858184337616
epoch£º753	 i:7 	 global-step:15067	 l-p:0.1530439853668213
epoch£º753	 i:8 	 global-step:15068	 l-p:0.13836373388767242
epoch£º753	 i:9 	 global-step:15069	 l-p:0.12694743275642395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:754
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0942, 5.0887, 5.0939],
        [5.0942, 5.0901, 5.0940],
        [5.0942, 5.1864, 4.9516],
        [5.0942, 5.0941, 5.0943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:754, step:0 
model_pd.l_p.mean(): 0.11792022734880447 
model_pd.l_d.mean(): -20.514759063720703 
model_pd.lagr.mean(): -20.396839141845703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4554], device='cuda:0')), ('power', tensor([-21.2042], device='cuda:0'))])
epoch£º754	 i:0 	 global-step:15080	 l-p:0.11792022734880447
epoch£º754	 i:1 	 global-step:15081	 l-p:0.10629373043775558
epoch£º754	 i:2 	 global-step:15082	 l-p:0.14953924715518951
epoch£º754	 i:3 	 global-step:15083	 l-p:0.19941829144954681
epoch£º754	 i:4 	 global-step:15084	 l-p:0.1890150010585785
epoch£º754	 i:5 	 global-step:15085	 l-p:0.12274763733148575
epoch£º754	 i:6 	 global-step:15086	 l-p:0.12852300703525543
epoch£º754	 i:7 	 global-step:15087	 l-p:0.14731761813163757
epoch£º754	 i:8 	 global-step:15088	 l-p:0.14192591607570648
epoch£º754	 i:9 	 global-step:15089	 l-p:0.11093637347221375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:755
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0909, 4.9261, 4.9565],
        [5.0909, 5.0486, 5.0808],
        [5.0909, 4.9861, 4.6974],
        [5.0909, 4.8614, 4.7840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:755, step:0 
model_pd.l_p.mean(): 0.10727107524871826 
model_pd.l_d.mean(): -20.115680694580078 
model_pd.lagr.mean(): -20.00840950012207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4981], device='cuda:0')), ('power', tensor([-20.8443], device='cuda:0'))])
epoch£º755	 i:0 	 global-step:15100	 l-p:0.10727107524871826
epoch£º755	 i:1 	 global-step:15101	 l-p:0.13613437116146088
epoch£º755	 i:2 	 global-step:15102	 l-p:0.15178297460079193
epoch£º755	 i:3 	 global-step:15103	 l-p:0.15984496474266052
epoch£º755	 i:4 	 global-step:15104	 l-p:0.14775212109088898
epoch£º755	 i:5 	 global-step:15105	 l-p:0.5173285007476807
epoch£º755	 i:6 	 global-step:15106	 l-p:0.11952023953199387
epoch£º755	 i:7 	 global-step:15107	 l-p:0.1399397999048233
epoch£º755	 i:8 	 global-step:15108	 l-p:0.46502912044525146
epoch£º755	 i:9 	 global-step:15109	 l-p:0.27086201310157776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:756
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0183, 4.8727, 4.5818],
        [5.0183, 5.0183, 5.0183],
        [5.0183, 4.7753, 4.5904],
        [5.0183, 4.7795, 4.6910]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:756, step:0 
model_pd.l_p.mean(): 0.12208465486764908 
model_pd.l_d.mean(): -20.65074348449707 
model_pd.lagr.mean(): -20.528657913208008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4613], device='cuda:0')), ('power', tensor([-21.3477], device='cuda:0'))])
epoch£º756	 i:0 	 global-step:15120	 l-p:0.12208465486764908
epoch£º756	 i:1 	 global-step:15121	 l-p:-11.08554744720459
epoch£º756	 i:2 	 global-step:15122	 l-p:0.1858738213777542
epoch£º756	 i:3 	 global-step:15123	 l-p:0.18217182159423828
epoch£º756	 i:4 	 global-step:15124	 l-p:0.24674874544143677
epoch£º756	 i:5 	 global-step:15125	 l-p:0.1674315631389618
epoch£º756	 i:6 	 global-step:15126	 l-p:0.11000619828701019
epoch£º756	 i:7 	 global-step:15127	 l-p:0.13972274959087372
epoch£º756	 i:8 	 global-step:15128	 l-p:0.07662677764892578
epoch£º756	 i:9 	 global-step:15129	 l-p:0.15846562385559082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:757
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0756, 4.8727, 4.6194],
        [5.0756, 4.9668, 4.6765],
        [5.0756, 4.9397, 4.9861],
        [5.0756, 5.0756, 5.0756]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:757, step:0 
model_pd.l_p.mean(): 0.14628787338733673 
model_pd.l_d.mean(): -18.732364654541016 
model_pd.lagr.mean(): -18.586076736450195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5632], device='cuda:0')), ('power', tensor([-19.5125], device='cuda:0'))])
epoch£º757	 i:0 	 global-step:15140	 l-p:0.14628787338733673
epoch£º757	 i:1 	 global-step:15141	 l-p:0.15726405382156372
epoch£º757	 i:2 	 global-step:15142	 l-p:0.14923134446144104
epoch£º757	 i:3 	 global-step:15143	 l-p:0.12077581882476807
epoch£º757	 i:4 	 global-step:15144	 l-p:0.16461801528930664
epoch£º757	 i:5 	 global-step:15145	 l-p:0.13195672631263733
epoch£º757	 i:6 	 global-step:15146	 l-p:0.11817115545272827
epoch£º757	 i:7 	 global-step:15147	 l-p:0.08129635453224182
epoch£º757	 i:8 	 global-step:15148	 l-p:0.11896955221891403
epoch£º757	 i:9 	 global-step:15149	 l-p:0.16790546476840973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:758
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1452, 4.9556, 4.9580],
        [5.1452, 4.9821, 5.0117],
        [5.1452, 5.0738, 5.1188],
        [5.1452, 4.9147, 4.8130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:758, step:0 
model_pd.l_p.mean(): 0.06231217831373215 
model_pd.l_d.mean(): -20.764381408691406 
model_pd.lagr.mean(): -20.702068328857422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4120], device='cuda:0')), ('power', tensor([-21.4121], device='cuda:0'))])
epoch£º758	 i:0 	 global-step:15160	 l-p:0.06231217831373215
epoch£º758	 i:1 	 global-step:15161	 l-p:0.13484589755535126
epoch£º758	 i:2 	 global-step:15162	 l-p:0.15514352917671204
epoch£º758	 i:3 	 global-step:15163	 l-p:0.09989196807146072
epoch£º758	 i:4 	 global-step:15164	 l-p:0.17277288436889648
epoch£º758	 i:5 	 global-step:15165	 l-p:0.1096142828464508
epoch£º758	 i:6 	 global-step:15166	 l-p:0.1119779646396637
epoch£º758	 i:7 	 global-step:15167	 l-p:0.08870141953229904
epoch£º758	 i:8 	 global-step:15168	 l-p:0.06336408108472824
epoch£º758	 i:9 	 global-step:15169	 l-p:0.132426917552948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:759
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1815, 5.1815, 5.1815],
        [5.1815, 5.1816, 5.1816],
        [5.1815, 5.0772, 4.7936],
        [5.1815, 5.1815, 5.1816]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:759, step:0 
model_pd.l_p.mean(): 0.055474068969488144 
model_pd.l_d.mean(): -19.846288681030273 
model_pd.lagr.mean(): -19.790815353393555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4533], device='cuda:0')), ('power', tensor([-20.5262], device='cuda:0'))])
epoch£º759	 i:0 	 global-step:15180	 l-p:0.055474068969488144
epoch£º759	 i:1 	 global-step:15181	 l-p:0.14993727207183838
epoch£º759	 i:2 	 global-step:15182	 l-p:0.12720394134521484
epoch£º759	 i:3 	 global-step:15183	 l-p:0.1463981419801712
epoch£º759	 i:4 	 global-step:15184	 l-p:0.11998476088047028
epoch£º759	 i:5 	 global-step:15185	 l-p:0.1555204689502716
epoch£º759	 i:6 	 global-step:15186	 l-p:0.08359558880329132
epoch£º759	 i:7 	 global-step:15187	 l-p:0.12886111438274384
epoch£º759	 i:8 	 global-step:15188	 l-p:0.026959676295518875
epoch£º759	 i:9 	 global-step:15189	 l-p:0.11855773627758026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:760
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1468, 5.1140, 5.1403],
        [5.1468, 4.9570, 4.9599],
        [5.1468, 4.9162, 4.7370],
        [5.1468, 5.0770, 5.1216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:760, step:0 
model_pd.l_p.mean(): 0.08881774544715881 
model_pd.l_d.mean(): -19.27875328063965 
model_pd.lagr.mean(): -19.1899356842041 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5519], device='cuda:0')), ('power', tensor([-20.0533], device='cuda:0'))])
epoch£º760	 i:0 	 global-step:15200	 l-p:0.08881774544715881
epoch£º760	 i:1 	 global-step:15201	 l-p:0.13369201123714447
epoch£º760	 i:2 	 global-step:15202	 l-p:0.0817975401878357
epoch£º760	 i:3 	 global-step:15203	 l-p:0.1550399214029312
epoch£º760	 i:4 	 global-step:15204	 l-p:0.14081250131130219
epoch£º760	 i:5 	 global-step:15205	 l-p:0.1232622042298317
epoch£º760	 i:6 	 global-step:15206	 l-p:0.09591391682624817
epoch£º760	 i:7 	 global-step:15207	 l-p:0.14162418246269226
epoch£º760	 i:8 	 global-step:15208	 l-p:0.14590919017791748
epoch£º760	 i:9 	 global-step:15209	 l-p:0.11897344142198563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:761
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1198, 5.1030, 5.1177],
        [5.1198, 4.9226, 4.6678],
        [5.1198, 4.9074, 4.8768],
        [5.1198, 5.0700, 4.7853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:761, step:0 
model_pd.l_p.mean(): 0.13658088445663452 
model_pd.l_d.mean(): -18.435688018798828 
model_pd.lagr.mean(): -18.29910659790039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6091], device='cuda:0')), ('power', tensor([-19.2594], device='cuda:0'))])
epoch£º761	 i:0 	 global-step:15220	 l-p:0.13658088445663452
epoch£º761	 i:1 	 global-step:15221	 l-p:0.09211662411689758
epoch£º761	 i:2 	 global-step:15222	 l-p:0.125274658203125
epoch£º761	 i:3 	 global-step:15223	 l-p:0.13779881596565247
epoch£º761	 i:4 	 global-step:15224	 l-p:0.13385723531246185
epoch£º761	 i:5 	 global-step:15225	 l-p:0.4636003077030182
epoch£º761	 i:6 	 global-step:15226	 l-p:0.18429043889045715
epoch£º761	 i:7 	 global-step:15227	 l-p:0.14941714704036713
epoch£º761	 i:8 	 global-step:15228	 l-p:0.109138622879982
epoch£º761	 i:9 	 global-step:15229	 l-p:1.0279772281646729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:762
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0194, 4.8141, 4.8087],
        [5.0194, 4.8017, 4.7760],
        [5.0194, 4.7847, 4.5601],
        [5.0194, 5.0194, 5.0194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:762, step:0 
model_pd.l_p.mean(): 0.13567252457141876 
model_pd.l_d.mean(): -20.723209381103516 
model_pd.lagr.mean(): -20.58753776550293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4667], device='cuda:0')), ('power', tensor([-21.4264], device='cuda:0'))])
epoch£º762	 i:0 	 global-step:15240	 l-p:0.13567252457141876
epoch£º762	 i:1 	 global-step:15241	 l-p:0.3094351887702942
epoch£º762	 i:2 	 global-step:15242	 l-p:0.1315283179283142
epoch£º762	 i:3 	 global-step:15243	 l-p:0.13157223165035248
epoch£º762	 i:4 	 global-step:15244	 l-p:0.13414081931114197
epoch£º762	 i:5 	 global-step:15245	 l-p:0.13016758859157562
epoch£º762	 i:6 	 global-step:15246	 l-p:0.2892152667045593
epoch£º762	 i:7 	 global-step:15247	 l-p:-0.02097860723733902
epoch£º762	 i:8 	 global-step:15248	 l-p:0.36781805753707886
epoch£º762	 i:9 	 global-step:15249	 l-p:0.06921375542879105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:763
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9902, 4.7387, 4.6034],
        [4.9902, 4.9847, 4.9899],
        [4.9902, 4.9477, 4.9802],
        [4.9902, 5.1603, 4.9650]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:763, step:0 
model_pd.l_p.mean(): 0.15673160552978516 
model_pd.l_d.mean(): -19.469757080078125 
model_pd.lagr.mean(): -19.313026428222656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5502], device='cuda:0')), ('power', tensor([-20.2447], device='cuda:0'))])
epoch£º763	 i:0 	 global-step:15260	 l-p:0.15673160552978516
epoch£º763	 i:1 	 global-step:15261	 l-p:0.007223043125122786
epoch£º763	 i:2 	 global-step:15262	 l-p:0.11003147065639496
epoch£º763	 i:3 	 global-step:15263	 l-p:0.1270485818386078
epoch£º763	 i:4 	 global-step:15264	 l-p:0.14922283589839935
epoch£º763	 i:5 	 global-step:15265	 l-p:0.13098174333572388
epoch£º763	 i:6 	 global-step:15266	 l-p:-0.25027909874916077
epoch£º763	 i:7 	 global-step:15267	 l-p:0.17091062664985657
epoch£º763	 i:8 	 global-step:15268	 l-p:0.14876072108745575
epoch£º763	 i:9 	 global-step:15269	 l-p:0.1503707319498062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:764
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0708, 4.9935, 5.0410],
        [5.0708, 5.0670, 5.0706],
        [5.0708, 4.8443, 4.6165],
        [5.0708, 4.8545, 4.8250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:764, step:0 
model_pd.l_p.mean(): 0.18432319164276123 
model_pd.l_d.mean(): -20.809114456176758 
model_pd.lagr.mean(): -20.624792098999023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4446], device='cuda:0')), ('power', tensor([-21.4907], device='cuda:0'))])
epoch£º764	 i:0 	 global-step:15280	 l-p:0.18432319164276123
epoch£º764	 i:1 	 global-step:15281	 l-p:0.10328266024589539
epoch£º764	 i:2 	 global-step:15282	 l-p:0.14888687431812286
epoch£º764	 i:3 	 global-step:15283	 l-p:0.18604779243469238
epoch£º764	 i:4 	 global-step:15284	 l-p:0.12979047000408173
epoch£º764	 i:5 	 global-step:15285	 l-p:0.1306968480348587
epoch£º764	 i:6 	 global-step:15286	 l-p:0.13345955312252045
epoch£º764	 i:7 	 global-step:15287	 l-p:0.17810167372226715
epoch£º764	 i:8 	 global-step:15288	 l-p:0.07010597735643387
epoch£º764	 i:9 	 global-step:15289	 l-p:0.0797630101442337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:765
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1544, 5.0493, 5.1000],
        [5.1544, 5.1485, 5.1541],
        [5.1544, 5.1543, 5.1544],
        [5.1544, 5.0353, 5.0850]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:765, step:0 
model_pd.l_p.mean(): 0.11743760854005814 
model_pd.l_d.mean(): -18.58052635192871 
model_pd.lagr.mean(): -18.463088989257812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5715], device='cuda:0')), ('power', tensor([-19.3674], device='cuda:0'))])
epoch£º765	 i:0 	 global-step:15300	 l-p:0.11743760854005814
epoch£º765	 i:1 	 global-step:15301	 l-p:0.17449572682380676
epoch£º765	 i:2 	 global-step:15302	 l-p:0.08322053402662277
epoch£º765	 i:3 	 global-step:15303	 l-p:0.11875677108764648
epoch£º765	 i:4 	 global-step:15304	 l-p:0.15714000165462494
epoch£º765	 i:5 	 global-step:15305	 l-p:0.12976618111133575
epoch£º765	 i:6 	 global-step:15306	 l-p:0.22858451306819916
epoch£º765	 i:7 	 global-step:15307	 l-p:0.1267721951007843
epoch£º765	 i:8 	 global-step:15308	 l-p:0.057438261806964874
epoch£º765	 i:9 	 global-step:15309	 l-p:0.08204827457666397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:766
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2486, 5.6273, 5.5509],
        [5.2486, 5.0222, 4.8955],
        [5.2486, 5.2016, 5.2361],
        [5.2486, 5.3795, 5.1592]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:766, step:0 
model_pd.l_p.mean(): 0.121339812874794 
model_pd.l_d.mean(): -20.50690460205078 
model_pd.lagr.mean(): -20.38556480407715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4224], device='cuda:0')), ('power', tensor([-21.1625], device='cuda:0'))])
epoch£º766	 i:0 	 global-step:15320	 l-p:0.121339812874794
epoch£º766	 i:1 	 global-step:15321	 l-p:0.12094162404537201
epoch£º766	 i:2 	 global-step:15322	 l-p:0.13807392120361328
epoch£º766	 i:3 	 global-step:15323	 l-p:0.7431169748306274
epoch£º766	 i:4 	 global-step:15324	 l-p:0.12312543392181396
epoch£º766	 i:5 	 global-step:15325	 l-p:0.12075679749250412
epoch£º766	 i:6 	 global-step:15326	 l-p:0.07610955834388733
epoch£º766	 i:7 	 global-step:15327	 l-p:0.11647878587245941
epoch£º766	 i:8 	 global-step:15328	 l-p:0.006518344860523939
epoch£º766	 i:9 	 global-step:15329	 l-p:0.12531678378582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:767
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2608, 5.1987, 5.2401],
        [5.2608, 5.2608, 5.2608],
        [5.2608, 5.2608, 5.2608],
        [5.2608, 5.5381, 5.3972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:767, step:0 
model_pd.l_p.mean(): 0.11261051148176193 
model_pd.l_d.mean(): -20.76885414123535 
model_pd.lagr.mean(): -20.6562442779541 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3771], device='cuda:0')), ('power', tensor([-21.3810], device='cuda:0'))])
epoch£º767	 i:0 	 global-step:15340	 l-p:0.11261051148176193
epoch£º767	 i:1 	 global-step:15341	 l-p:0.045584551990032196
epoch£º767	 i:2 	 global-step:15342	 l-p:0.11116448789834976
epoch£º767	 i:3 	 global-step:15343	 l-p:0.5205155611038208
epoch£º767	 i:4 	 global-step:15344	 l-p:0.12838822603225708
epoch£º767	 i:5 	 global-step:15345	 l-p:0.13176560401916504
epoch£º767	 i:6 	 global-step:15346	 l-p:0.16193906962871552
epoch£º767	 i:7 	 global-step:15347	 l-p:0.13776350021362305
epoch£º767	 i:8 	 global-step:15348	 l-p:0.07544712722301483
epoch£º767	 i:9 	 global-step:15349	 l-p:0.22112533450126648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:768
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2303, 5.0012, 4.8672],
        [5.2303, 5.7564, 5.7827],
        [5.2303, 5.1830, 5.2177],
        [5.2303, 5.1759, 4.8937]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:768, step:0 
model_pd.l_p.mean(): 0.14138078689575195 
model_pd.l_d.mean(): -19.477725982666016 
model_pd.lagr.mean(): -19.336345672607422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4746], device='cuda:0')), ('power', tensor([-20.1754], device='cuda:0'))])
epoch£º768	 i:0 	 global-step:15360	 l-p:0.14138078689575195
epoch£º768	 i:1 	 global-step:15361	 l-p:0.12159904092550278
epoch£º768	 i:2 	 global-step:15362	 l-p:0.09666548669338226
epoch£º768	 i:3 	 global-step:15363	 l-p:0.08317726105451584
epoch£º768	 i:4 	 global-step:15364	 l-p:0.10303807258605957
epoch£º768	 i:5 	 global-step:15365	 l-p:0.28564995527267456
epoch£º768	 i:6 	 global-step:15366	 l-p:0.12993794679641724
epoch£º768	 i:7 	 global-step:15367	 l-p:0.1651885211467743
epoch£º768	 i:8 	 global-step:15368	 l-p:0.13247567415237427
epoch£º768	 i:9 	 global-step:15369	 l-p:0.1096302941441536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:769
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1673, 4.9380, 4.7446],
        [5.1673, 5.1471, 4.8682],
        [5.1673, 5.1386, 4.8577],
        [5.1673, 5.1533, 5.1658]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:769, step:0 
model_pd.l_p.mean(): 0.12465889006853104 
model_pd.l_d.mean(): -20.251567840576172 
model_pd.lagr.mean(): -20.126909255981445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4673], device='cuda:0')), ('power', tensor([-20.9503], device='cuda:0'))])
epoch£º769	 i:0 	 global-step:15380	 l-p:0.12465889006853104
epoch£º769	 i:1 	 global-step:15381	 l-p:0.15239612758159637
epoch£º769	 i:2 	 global-step:15382	 l-p:0.09656473249197006
epoch£º769	 i:3 	 global-step:15383	 l-p:0.2151261270046234
epoch£º769	 i:4 	 global-step:15384	 l-p:0.10961540788412094
epoch£º769	 i:5 	 global-step:15385	 l-p:0.12511137127876282
epoch£º769	 i:6 	 global-step:15386	 l-p:0.16166755557060242
epoch£º769	 i:7 	 global-step:15387	 l-p:0.12459107488393784
epoch£º769	 i:8 	 global-step:15388	 l-p:0.19482871890068054
epoch£º769	 i:9 	 global-step:15389	 l-p:0.13547283411026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:770
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0423, 5.0423, 5.0423],
        [5.0423, 4.9694, 4.6747],
        [5.0423, 4.8035, 4.5865],
        [5.0423, 4.9247, 4.9770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:770, step:0 
model_pd.l_p.mean(): -0.799365222454071 
model_pd.l_d.mean(): -18.343868255615234 
model_pd.lagr.mean(): -19.143234252929688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6304], device='cuda:0')), ('power', tensor([-19.1884], device='cuda:0'))])
epoch£º770	 i:0 	 global-step:15400	 l-p:-0.799365222454071
epoch£º770	 i:1 	 global-step:15401	 l-p:0.15583588182926178
epoch£º770	 i:2 	 global-step:15402	 l-p:0.12327034026384354
epoch£º770	 i:3 	 global-step:15403	 l-p:4.47608757019043
epoch£º770	 i:4 	 global-step:15404	 l-p:0.21446220576763153
epoch£º770	 i:5 	 global-step:15405	 l-p:0.13182145357131958
epoch£º770	 i:6 	 global-step:15406	 l-p:0.14104899764060974
epoch£º770	 i:7 	 global-step:15407	 l-p:0.15795615315437317
epoch£º770	 i:8 	 global-step:15408	 l-p:0.1100965291261673
epoch£º770	 i:9 	 global-step:15409	 l-p:-0.2488517314195633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:771
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9917, 4.9916, 4.9917],
        [4.9917, 4.9478, 4.9812],
        [4.9917, 4.9870, 4.9915],
        [4.9917, 4.7376, 4.5587]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:771, step:0 
model_pd.l_p.mean(): 0.12958402931690216 
model_pd.l_d.mean(): -20.819488525390625 
model_pd.lagr.mean(): -20.689905166625977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4411], device='cuda:0')), ('power', tensor([-21.4977], device='cuda:0'))])
epoch£º771	 i:0 	 global-step:15420	 l-p:0.12958402931690216
epoch£º771	 i:1 	 global-step:15421	 l-p:0.11802442371845245
epoch£º771	 i:2 	 global-step:15422	 l-p:0.027094433084130287
epoch£º771	 i:3 	 global-step:15423	 l-p:0.14109204709529877
epoch£º771	 i:4 	 global-step:15424	 l-p:-0.1397777497768402
epoch£º771	 i:5 	 global-step:15425	 l-p:0.17208357155323029
epoch£º771	 i:6 	 global-step:15426	 l-p:0.14934107661247253
epoch£º771	 i:7 	 global-step:15427	 l-p:0.20296314358711243
epoch£º771	 i:8 	 global-step:15428	 l-p:0.8564402461051941
epoch£º771	 i:9 	 global-step:15429	 l-p:0.26764509081840515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:772
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0015, 4.9591, 4.9916],
        [5.0015, 4.9268, 4.9741],
        [5.0015, 5.2697, 5.1295],
        [5.0015, 4.9576, 4.9910]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:772, step:0 
model_pd.l_p.mean(): 0.11050376296043396 
model_pd.l_d.mean(): -20.831697463989258 
model_pd.lagr.mean(): -20.721193313598633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4547], device='cuda:0')), ('power', tensor([-21.5239], device='cuda:0'))])
epoch£º772	 i:0 	 global-step:15440	 l-p:0.11050376296043396
epoch£º772	 i:1 	 global-step:15441	 l-p:0.15674647688865662
epoch£º772	 i:2 	 global-step:15442	 l-p:0.10408511757850647
epoch£º772	 i:3 	 global-step:15443	 l-p:0.14697466790676117
epoch£º772	 i:4 	 global-step:15444	 l-p:0.21851488947868347
epoch£º772	 i:5 	 global-step:15445	 l-p:0.11675769835710526
epoch£º772	 i:6 	 global-step:15446	 l-p:0.1160242035984993
epoch£º772	 i:7 	 global-step:15447	 l-p:0.1829577535390854
epoch£º772	 i:8 	 global-step:15448	 l-p:0.07983269542455673
epoch£º772	 i:9 	 global-step:15449	 l-p:0.16714848577976227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:773
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1284, 5.1284, 5.1284],
        [5.1284, 5.0907, 5.1202],
        [5.1284, 4.8955, 4.8170],
        [5.1284, 4.8983, 4.8300]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:773, step:0 
model_pd.l_p.mean(): 0.13520754873752594 
model_pd.l_d.mean(): -21.018348693847656 
model_pd.lagr.mean(): -20.883140563964844 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3839], device='cuda:0')), ('power', tensor([-21.6402], device='cuda:0'))])
epoch£º773	 i:0 	 global-step:15460	 l-p:0.13520754873752594
epoch£º773	 i:1 	 global-step:15461	 l-p:0.13237740099430084
epoch£º773	 i:2 	 global-step:15462	 l-p:0.09256969392299652
epoch£º773	 i:3 	 global-step:15463	 l-p:0.07762403041124344
epoch£º773	 i:4 	 global-step:15464	 l-p:0.14139831066131592
epoch£º773	 i:5 	 global-step:15465	 l-p:0.15944546461105347
epoch£º773	 i:6 	 global-step:15466	 l-p:7.584331035614014
epoch£º773	 i:7 	 global-step:15467	 l-p:0.13207948207855225
epoch£º773	 i:8 	 global-step:15468	 l-p:0.1166551262140274
epoch£º773	 i:9 	 global-step:15469	 l-p:0.10875999927520752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:774
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2303, 5.1323, 5.1824],
        [5.2303, 5.1905, 5.2211],
        [5.2303, 4.9989, 4.8719],
        [5.2303, 5.2111, 5.2277]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:774, step:0 
model_pd.l_p.mean(): 0.08237908780574799 
model_pd.l_d.mean(): -20.719253540039062 
model_pd.lagr.mean(): -20.63687515258789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4042], device='cuda:0')), ('power', tensor([-21.3586], device='cuda:0'))])
epoch£º774	 i:0 	 global-step:15480	 l-p:0.08237908780574799
epoch£º774	 i:1 	 global-step:15481	 l-p:0.11746223270893097
epoch£º774	 i:2 	 global-step:15482	 l-p:0.12762127816677094
epoch£º774	 i:3 	 global-step:15483	 l-p:0.20494742691516876
epoch£º774	 i:4 	 global-step:15484	 l-p:0.1355218142271042
epoch£º774	 i:5 	 global-step:15485	 l-p:0.06584447622299194
epoch£º774	 i:6 	 global-step:15486	 l-p:0.13643258810043335
epoch£º774	 i:7 	 global-step:15487	 l-p:0.4275919497013092
epoch£º774	 i:8 	 global-step:15488	 l-p:0.1226765587925911
epoch£º774	 i:9 	 global-step:15489	 l-p:0.1168803870677948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:775
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2745, 5.0498, 4.9533],
        [5.2745, 5.0646, 5.0234],
        [5.2745, 5.2745, 5.2745],
        [5.2745, 5.4071, 5.1856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:775, step:0 
model_pd.l_p.mean(): 0.12470249831676483 
model_pd.l_d.mean(): -18.811771392822266 
model_pd.lagr.mean(): -18.687068939208984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5956], device='cuda:0')), ('power', tensor([-19.6258], device='cuda:0'))])
epoch£º775	 i:0 	 global-step:15500	 l-p:0.12470249831676483
epoch£º775	 i:1 	 global-step:15501	 l-p:0.14133191108703613
epoch£º775	 i:2 	 global-step:15502	 l-p:0.09374124556779861
epoch£º775	 i:3 	 global-step:15503	 l-p:0.31703025102615356
epoch£º775	 i:4 	 global-step:15504	 l-p:0.09008676558732986
epoch£º775	 i:5 	 global-step:15505	 l-p:0.11267760396003723
epoch£º775	 i:6 	 global-step:15506	 l-p:0.1606152504682541
epoch£º775	 i:7 	 global-step:15507	 l-p:0.13803155720233917
epoch£º775	 i:8 	 global-step:15508	 l-p:0.13775166869163513
epoch£º775	 i:9 	 global-step:15509	 l-p:0.036828797310590744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:776
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]], device='cuda:0')
 pt:tensor([[5.2433, 5.0715, 5.0930],
        [5.2433, 5.0254, 4.9698],
        [5.2433, 5.0309, 4.9903],
        [5.2433, 5.6155, 5.5334]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:776, step:0 
model_pd.l_p.mean(): 0.17942935228347778 
model_pd.l_d.mean(): -20.606111526489258 
model_pd.lagr.mean(): -20.426681518554688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4225], device='cuda:0')), ('power', tensor([-21.2629], device='cuda:0'))])
epoch£º776	 i:0 	 global-step:15520	 l-p:0.17942935228347778
epoch£º776	 i:1 	 global-step:15521	 l-p:0.11941482126712799
epoch£º776	 i:2 	 global-step:15522	 l-p:0.13605955243110657
epoch£º776	 i:3 	 global-step:15523	 l-p:0.12674491107463837
epoch£º776	 i:4 	 global-step:15524	 l-p:0.12415141612291336
epoch£º776	 i:5 	 global-step:15525	 l-p:0.10944902151823044
epoch£º776	 i:6 	 global-step:15526	 l-p:0.1295943558216095
epoch£º776	 i:7 	 global-step:15527	 l-p:0.07517924904823303
epoch£º776	 i:8 	 global-step:15528	 l-p:0.14824160933494568
epoch£º776	 i:9 	 global-step:15529	 l-p:0.13098950684070587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:777
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1493, 4.9371, 4.9124],
        [5.1493, 5.0649, 5.1140],
        [5.1493, 4.9260, 4.8768],
        [5.1493, 4.9083, 4.7566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:777, step:0 
model_pd.l_p.mean(): 0.1185607761144638 
model_pd.l_d.mean(): -20.61060333251953 
model_pd.lagr.mean(): -20.492042541503906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4368], device='cuda:0')), ('power', tensor([-21.2821], device='cuda:0'))])
epoch£º777	 i:0 	 global-step:15540	 l-p:0.1185607761144638
epoch£º777	 i:1 	 global-step:15541	 l-p:0.14283260703086853
epoch£º777	 i:2 	 global-step:15542	 l-p:0.15233926475048065
epoch£º777	 i:3 	 global-step:15543	 l-p:0.16329118609428406
epoch£º777	 i:4 	 global-step:15544	 l-p:0.16327029466629028
epoch£º777	 i:5 	 global-step:15545	 l-p:0.092858225107193
epoch£º777	 i:6 	 global-step:15546	 l-p:0.14361901581287384
epoch£º777	 i:7 	 global-step:15547	 l-p:0.2695898413658142
epoch£º777	 i:8 	 global-step:15548	 l-p:0.05689359828829765
epoch£º777	 i:9 	 global-step:15549	 l-p:0.1767769604921341
====================================================================================================
====================================================================================================
====================================================================================================

epoch:778
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0682, 5.0672, 5.0682],
        [5.0682, 4.9887, 5.0372],
        [5.0682, 5.0510, 5.0661],
        [5.0682, 5.0613, 5.0677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:778, step:0 
model_pd.l_p.mean(): 0.2060081660747528 
model_pd.l_d.mean(): -19.054256439208984 
model_pd.lagr.mean(): -18.848247528076172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6185], device='cuda:0')), ('power', tensor([-19.8943], device='cuda:0'))])
epoch£º778	 i:0 	 global-step:15560	 l-p:0.2060081660747528
epoch£º778	 i:1 	 global-step:15561	 l-p:0.15358634293079376
epoch£º778	 i:2 	 global-step:15562	 l-p:0.09371671825647354
epoch£º778	 i:3 	 global-step:15563	 l-p:0.25698673725128174
epoch£º778	 i:4 	 global-step:15564	 l-p:0.10161241143941879
epoch£º778	 i:5 	 global-step:15565	 l-p:0.08802122622728348
epoch£º778	 i:6 	 global-step:15566	 l-p:0.19359050691127777
epoch£º778	 i:7 	 global-step:15567	 l-p:0.1521081179380417
epoch£º778	 i:8 	 global-step:15568	 l-p:0.14959992468357086
epoch£º778	 i:9 	 global-step:15569	 l-p:0.15299773216247559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:779
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1083, 4.9481, 4.9858],
        [5.1083, 4.8653, 4.6799],
        [5.1083, 5.1082, 5.1083],
        [5.1083, 5.0293, 5.0775]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:779, step:0 
model_pd.l_p.mean(): 0.12415465712547302 
model_pd.l_d.mean(): -20.506633758544922 
model_pd.lagr.mean(): -20.382478713989258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4582], device='cuda:0')), ('power', tensor([-21.1988], device='cuda:0'))])
epoch£º779	 i:0 	 global-step:15580	 l-p:0.12415465712547302
epoch£º779	 i:1 	 global-step:15581	 l-p:0.18453571200370789
epoch£º779	 i:2 	 global-step:15582	 l-p:0.13369107246398926
epoch£º779	 i:3 	 global-step:15583	 l-p:0.12305686622858047
epoch£º779	 i:4 	 global-step:15584	 l-p:0.1547955721616745
epoch£º779	 i:5 	 global-step:15585	 l-p:0.059483565390110016
epoch£º779	 i:6 	 global-step:15586	 l-p:0.1317720115184784
epoch£º779	 i:7 	 global-step:15587	 l-p:0.13607238233089447
epoch£º779	 i:8 	 global-step:15588	 l-p:0.15094894170761108
epoch£º779	 i:9 	 global-step:15589	 l-p:0.1537814438343048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:780
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1331, 5.1278, 5.1328],
        [5.1331, 5.1145, 5.1307],
        [5.1331, 5.2812, 5.0683],
        [5.1331, 5.2745, 5.0581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:780, step:0 
model_pd.l_p.mean(): 0.16901199519634247 
model_pd.l_d.mean(): -18.65096664428711 
model_pd.lagr.mean(): -18.48195457458496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5674], device='cuda:0')), ('power', tensor([-19.4344], device='cuda:0'))])
epoch£º780	 i:0 	 global-step:15600	 l-p:0.16901199519634247
epoch£º780	 i:1 	 global-step:15601	 l-p:0.050639305263757706
epoch£º780	 i:2 	 global-step:15602	 l-p:0.13923442363739014
epoch£º780	 i:3 	 global-step:15603	 l-p:0.10693056136369705
epoch£º780	 i:4 	 global-step:15604	 l-p:0.1530851125717163
epoch£º780	 i:5 	 global-step:15605	 l-p:0.11002084612846375
epoch£º780	 i:6 	 global-step:15606	 l-p:0.16438399255275726
epoch£º780	 i:7 	 global-step:15607	 l-p:0.10461089760065079
epoch£º780	 i:8 	 global-step:15608	 l-p:0.05349646508693695
epoch£º780	 i:9 	 global-step:15609	 l-p:0.12884904444217682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:781
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2095, 5.3766, 5.1721],
        [5.2095, 5.2095, 5.2095],
        [5.2095, 5.2035, 5.2091],
        [5.2095, 5.2068, 5.2094]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:781, step:0 
model_pd.l_p.mean(): 0.1596323400735855 
model_pd.l_d.mean(): -20.495323181152344 
model_pd.lagr.mean(): -20.335691452026367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4441], device='cuda:0')), ('power', tensor([-21.1730], device='cuda:0'))])
epoch£º781	 i:0 	 global-step:15620	 l-p:0.1596323400735855
epoch£º781	 i:1 	 global-step:15621	 l-p:0.11762923747301102
epoch£º781	 i:2 	 global-step:15622	 l-p:0.11316759139299393
epoch£º781	 i:3 	 global-step:15623	 l-p:0.08833763748407364
epoch£º781	 i:4 	 global-step:15624	 l-p:0.12124330550432205
epoch£º781	 i:5 	 global-step:15625	 l-p:0.02172762341797352
epoch£º781	 i:6 	 global-step:15626	 l-p:0.10928791016340256
epoch£º781	 i:7 	 global-step:15627	 l-p:0.4980182647705078
epoch£º781	 i:8 	 global-step:15628	 l-p:0.13481226563453674
epoch£º781	 i:9 	 global-step:15629	 l-p:0.1184394583106041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:782
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2155, 5.1974, 5.2132],
        [5.2155, 5.2805, 5.0279],
        [5.2155, 5.1518, 5.1944],
        [5.2155, 5.1480, 5.1920]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:782, step:0 
model_pd.l_p.mean(): -0.01759251579642296 
model_pd.l_d.mean(): -20.11860466003418 
model_pd.lagr.mean(): -20.136198043823242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4160], device='cuda:0')), ('power', tensor([-20.7634], device='cuda:0'))])
epoch£º782	 i:0 	 global-step:15640	 l-p:-0.01759251579642296
epoch£º782	 i:1 	 global-step:15641	 l-p:0.1407659649848938
epoch£º782	 i:2 	 global-step:15642	 l-p:0.12207963317632675
epoch£º782	 i:3 	 global-step:15643	 l-p:0.12970095872879028
epoch£º782	 i:4 	 global-step:15644	 l-p:0.12962472438812256
epoch£º782	 i:5 	 global-step:15645	 l-p:0.11110387742519379
epoch£º782	 i:6 	 global-step:15646	 l-p:0.05315836891531944
epoch£º782	 i:7 	 global-step:15647	 l-p:0.12725591659545898
epoch£º782	 i:8 	 global-step:15648	 l-p:0.14749202132225037
epoch£º782	 i:9 	 global-step:15649	 l-p:0.16647124290466309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:783
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1334, 5.0803, 4.7892],
        [5.1334, 4.9043, 4.8474],
        [5.1334, 5.0465, 4.7510],
        [5.1334, 5.0888, 5.1225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:783, step:0 
model_pd.l_p.mean(): 0.1592656373977661 
model_pd.l_d.mean(): -20.671428680419922 
model_pd.lagr.mean(): -20.512163162231445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4185], device='cuda:0')), ('power', tensor([-21.3249], device='cuda:0'))])
epoch£º783	 i:0 	 global-step:15660	 l-p:0.1592656373977661
epoch£º783	 i:1 	 global-step:15661	 l-p:0.1574302315711975
epoch£º783	 i:2 	 global-step:15662	 l-p:0.12540028989315033
epoch£º783	 i:3 	 global-step:15663	 l-p:0.11829740554094315
epoch£º783	 i:4 	 global-step:15664	 l-p:0.1292128562927246
epoch£º783	 i:5 	 global-step:15665	 l-p:0.1020423173904419
epoch£º783	 i:6 	 global-step:15666	 l-p:0.15859460830688477
epoch£º783	 i:7 	 global-step:15667	 l-p:0.14926524460315704
epoch£º783	 i:8 	 global-step:15668	 l-p:0.0920632928609848
epoch£º783	 i:9 	 global-step:15669	 l-p:0.18490004539489746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:784
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0975, 5.0333, 4.7384],
        [5.0975, 5.0975, 5.0975],
        [5.0975, 4.9942, 5.0468],
        [5.0975, 4.9862, 5.0389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:784, step:0 
model_pd.l_p.mean(): 0.1280040591955185 
model_pd.l_d.mean(): -20.23196029663086 
model_pd.lagr.mean(): -20.10395622253418 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5168], device='cuda:0')), ('power', tensor([-20.9810], device='cuda:0'))])
epoch£º784	 i:0 	 global-step:15680	 l-p:0.1280040591955185
epoch£º784	 i:1 	 global-step:15681	 l-p:0.13854514062404633
epoch£º784	 i:2 	 global-step:15682	 l-p:0.10742419213056564
epoch£º784	 i:3 	 global-step:15683	 l-p:0.18741798400878906
epoch£º784	 i:4 	 global-step:15684	 l-p:0.10074715316295624
epoch£º784	 i:5 	 global-step:15685	 l-p:0.1366742104291916
epoch£º784	 i:6 	 global-step:15686	 l-p:0.1749858856201172
epoch£º784	 i:7 	 global-step:15687	 l-p:0.1590932160615921
epoch£º784	 i:8 	 global-step:15688	 l-p:0.10808838903903961
epoch£º784	 i:9 	 global-step:15689	 l-p:0.1313936710357666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:785
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1303, 5.1303, 5.1303],
        [5.1303, 5.1258, 5.1300],
        [5.1303, 4.9145, 4.6643],
        [5.1303, 4.9596, 4.9902]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:785, step:0 
model_pd.l_p.mean(): 0.09709727764129639 
model_pd.l_d.mean(): -19.943340301513672 
model_pd.lagr.mean(): -19.846242904663086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4545], device='cuda:0')), ('power', tensor([-20.6256], device='cuda:0'))])
epoch£º785	 i:0 	 global-step:15700	 l-p:0.09709727764129639
epoch£º785	 i:1 	 global-step:15701	 l-p:0.15217186510562897
epoch£º785	 i:2 	 global-step:15702	 l-p:0.08606113493442535
epoch£º785	 i:3 	 global-step:15703	 l-p:0.11079198867082596
epoch£º785	 i:4 	 global-step:15704	 l-p:0.1333259642124176
epoch£º785	 i:5 	 global-step:15705	 l-p:0.21652916073799133
epoch£º785	 i:6 	 global-step:15706	 l-p:0.16218432784080505
epoch£º785	 i:7 	 global-step:15707	 l-p:0.189204141497612
epoch£º785	 i:8 	 global-step:15708	 l-p:0.12314412742853165
epoch£º785	 i:9 	 global-step:15709	 l-p:0.17388616502285004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:786
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0981, 5.0981, 5.0981],
        [5.0981, 5.4725, 5.3947],
        [5.0981, 5.0695, 4.7821],
        [5.0981, 5.0327, 5.0765]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:786, step:0 
model_pd.l_p.mean(): 0.13130228221416473 
model_pd.l_d.mean(): -20.733827590942383 
model_pd.lagr.mean(): -20.60252571105957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4303], device='cuda:0')), ('power', tensor([-21.4000], device='cuda:0'))])
epoch£º786	 i:0 	 global-step:15720	 l-p:0.13130228221416473
epoch£º786	 i:1 	 global-step:15721	 l-p:0.13991697132587433
epoch£º786	 i:2 	 global-step:15722	 l-p:0.15859055519104004
epoch£º786	 i:3 	 global-step:15723	 l-p:0.2110477089881897
epoch£º786	 i:4 	 global-step:15724	 l-p:0.12182720005512238
epoch£º786	 i:5 	 global-step:15725	 l-p:0.11517182737588882
epoch£º786	 i:6 	 global-step:15726	 l-p:0.1279682070016861
epoch£º786	 i:7 	 global-step:15727	 l-p:0.15819087624549866
epoch£º786	 i:8 	 global-step:15728	 l-p:0.11547791957855225
epoch£º786	 i:9 	 global-step:15729	 l-p:0.1530187726020813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:787
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1397, 5.0776, 4.7843],
        [5.1397, 5.0950, 5.1288],
        [5.1397, 5.1213, 5.1374],
        [5.1397, 5.0597, 5.1082]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:787, step:0 
model_pd.l_p.mean(): 0.11594755202531815 
model_pd.l_d.mean(): -19.420738220214844 
model_pd.lagr.mean(): -19.304790496826172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5621], device='cuda:0')), ('power', tensor([-20.2072], device='cuda:0'))])
epoch£º787	 i:0 	 global-step:15740	 l-p:0.11594755202531815
epoch£º787	 i:1 	 global-step:15741	 l-p:0.11264481395483017
epoch£º787	 i:2 	 global-step:15742	 l-p:0.1402708739042282
epoch£º787	 i:3 	 global-step:15743	 l-p:0.09987170249223709
epoch£º787	 i:4 	 global-step:15744	 l-p:0.09742782264947891
epoch£º787	 i:5 	 global-step:15745	 l-p:0.14124906063079834
epoch£º787	 i:6 	 global-step:15746	 l-p:0.11771616339683533
epoch£º787	 i:7 	 global-step:15747	 l-p:0.14665473997592926
epoch£º787	 i:8 	 global-step:15748	 l-p:0.15919430553913116
epoch£º787	 i:9 	 global-step:15749	 l-p:0.13428457081317902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:788
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1334, 4.8931, 4.8023],
        [5.1334, 5.1334, 5.1334],
        [5.1334, 4.8879, 4.7636],
        [5.1334, 5.2149, 4.9683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:788, step:0 
model_pd.l_p.mean(): 0.13661625981330872 
model_pd.l_d.mean(): -20.70550537109375 
model_pd.lagr.mean(): -20.568889617919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4173], device='cuda:0')), ('power', tensor([-21.3581], device='cuda:0'))])
epoch£º788	 i:0 	 global-step:15760	 l-p:0.13661625981330872
epoch£º788	 i:1 	 global-step:15761	 l-p:0.1460641324520111
epoch£º788	 i:2 	 global-step:15762	 l-p:0.11649766564369202
epoch£º788	 i:3 	 global-step:15763	 l-p:0.11406024545431137
epoch£º788	 i:4 	 global-step:15764	 l-p:0.20711961388587952
epoch£º788	 i:5 	 global-step:15765	 l-p:0.18832580745220184
epoch£º788	 i:6 	 global-step:15766	 l-p:0.1420540064573288
epoch£º788	 i:7 	 global-step:15767	 l-p:0.1261395812034607
epoch£º788	 i:8 	 global-step:15768	 l-p:0.15321774780750275
epoch£º788	 i:9 	 global-step:15769	 l-p:0.13957785069942474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:789
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0778, 5.0311, 5.0661],
        [5.0778, 4.8401, 4.7771],
        [5.0778, 5.0778, 5.0778],
        [5.0778, 5.0777, 5.0778]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:789, step:0 
model_pd.l_p.mean(): 0.12341083586215973 
model_pd.l_d.mean(): -20.432498931884766 
model_pd.lagr.mean(): -20.3090877532959 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4725], device='cuda:0')), ('power', tensor([-21.1385], device='cuda:0'))])
epoch£º789	 i:0 	 global-step:15780	 l-p:0.12341083586215973
epoch£º789	 i:1 	 global-step:15781	 l-p:0.1579974740743637
epoch£º789	 i:2 	 global-step:15782	 l-p:0.579541802406311
epoch£º789	 i:3 	 global-step:15783	 l-p:0.180793896317482
epoch£º789	 i:4 	 global-step:15784	 l-p:0.23227278888225555
epoch£º789	 i:5 	 global-step:15785	 l-p:0.15603698790073395
epoch£º789	 i:6 	 global-step:15786	 l-p:0.11762960255146027
epoch£º789	 i:7 	 global-step:15787	 l-p:0.12192615121603012
epoch£º789	 i:8 	 global-step:15788	 l-p:0.16214945912361145
epoch£º789	 i:9 	 global-step:15789	 l-p:0.14819540083408356
====================================================================================================
====================================================================================================
====================================================================================================

epoch:790
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0932, 5.0190, 5.0661],
        [5.0932, 5.0932, 5.0932],
        [5.0932, 5.0362, 4.7409],
        [5.0932, 4.8565, 4.7932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:790, step:0 
model_pd.l_p.mean(): 0.12042178958654404 
model_pd.l_d.mean(): -20.940799713134766 
model_pd.lagr.mean(): -20.820377349853516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4117], device='cuda:0')), ('power', tensor([-21.5902], device='cuda:0'))])
epoch£º790	 i:0 	 global-step:15800	 l-p:0.12042178958654404
epoch£º790	 i:1 	 global-step:15801	 l-p:0.20264652371406555
epoch£º790	 i:2 	 global-step:15802	 l-p:0.14785636961460114
epoch£º790	 i:3 	 global-step:15803	 l-p:0.13999266922473907
epoch£º790	 i:4 	 global-step:15804	 l-p:0.14033503830432892
epoch£º790	 i:5 	 global-step:15805	 l-p:0.12816254794597626
epoch£º790	 i:6 	 global-step:15806	 l-p:0.16316871345043182
epoch£º790	 i:7 	 global-step:15807	 l-p:0.12531600892543793
epoch£º790	 i:8 	 global-step:15808	 l-p:0.14026765525341034
epoch£º790	 i:9 	 global-step:15809	 l-p:0.08242043107748032
====================================================================================================
====================================================================================================
====================================================================================================

epoch:791
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1409, 4.9174, 4.8786],
        [5.1409, 5.5764, 5.5377],
        [5.1409, 5.0324, 5.0849],
        [5.1409, 4.9467, 4.9554]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:791, step:0 
model_pd.l_p.mean(): 0.1590735763311386 
model_pd.l_d.mean(): -20.650243759155273 
model_pd.lagr.mean(): -20.49117088317871 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4103], device='cuda:0')), ('power', tensor([-21.2951], device='cuda:0'))])
epoch£º791	 i:0 	 global-step:15820	 l-p:0.1590735763311386
epoch£º791	 i:1 	 global-step:15821	 l-p:0.1495177000761032
epoch£º791	 i:2 	 global-step:15822	 l-p:0.06450295448303223
epoch£º791	 i:3 	 global-step:15823	 l-p:0.12235122174024582
epoch£º791	 i:4 	 global-step:15824	 l-p:0.10152227431535721
epoch£º791	 i:5 	 global-step:15825	 l-p:0.13525399565696716
epoch£º791	 i:6 	 global-step:15826	 l-p:0.09870172291994095
epoch£º791	 i:7 	 global-step:15827	 l-p:0.1542769968509674
epoch£º791	 i:8 	 global-step:15828	 l-p:0.1280888319015503
epoch£º791	 i:9 	 global-step:15829	 l-p:0.15547588467597961
====================================================================================================
====================================================================================================
====================================================================================================

epoch:792
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1442, 4.9025, 4.7058],
        [5.1442, 5.0148, 4.7179],
        [5.1442, 5.0751, 5.1203],
        [5.1442, 5.0047, 5.0524]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:792, step:0 
model_pd.l_p.mean(): 0.1331094652414322 
model_pd.l_d.mean(): -20.794322967529297 
model_pd.lagr.mean(): -20.661212921142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4093], device='cuda:0')), ('power', tensor([-21.4397], device='cuda:0'))])
epoch£º792	 i:0 	 global-step:15840	 l-p:0.1331094652414322
epoch£º792	 i:1 	 global-step:15841	 l-p:0.07033127546310425
epoch£º792	 i:2 	 global-step:15842	 l-p:0.14236636459827423
epoch£º792	 i:3 	 global-step:15843	 l-p:0.14298519492149353
epoch£º792	 i:4 	 global-step:15844	 l-p:0.11900486797094345
epoch£º792	 i:5 	 global-step:15845	 l-p:0.1263476461172104
epoch£º792	 i:6 	 global-step:15846	 l-p:0.09354960918426514
epoch£º792	 i:7 	 global-step:15847	 l-p:0.21114909648895264
epoch£º792	 i:8 	 global-step:15848	 l-p:0.1588968187570572
epoch£º792	 i:9 	 global-step:15849	 l-p:0.5851907134056091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:793
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0676, 5.0676, 5.0676],
        [5.0676, 5.0648, 5.0675],
        [5.0676, 5.0676, 5.0676],
        [5.0676, 5.0200, 5.0556]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:793, step:0 
model_pd.l_p.mean(): 0.13223129510879517 
model_pd.l_d.mean(): -20.790634155273438 
model_pd.lagr.mean(): -20.658403396606445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4145], device='cuda:0')), ('power', tensor([-21.4413], device='cuda:0'))])
epoch£º793	 i:0 	 global-step:15860	 l-p:0.13223129510879517
epoch£º793	 i:1 	 global-step:15861	 l-p:0.15682214498519897
epoch£º793	 i:2 	 global-step:15862	 l-p:0.11395933479070663
epoch£º793	 i:3 	 global-step:15863	 l-p:0.11777068674564362
epoch£º793	 i:4 	 global-step:15864	 l-p:0.11921226233243942
epoch£º793	 i:5 	 global-step:15865	 l-p:-1.6312490701675415
epoch£º793	 i:6 	 global-step:15866	 l-p:0.13505111634731293
epoch£º793	 i:7 	 global-step:15867	 l-p:0.2228541523218155
epoch£º793	 i:8 	 global-step:15868	 l-p:0.16823932528495789
epoch£º793	 i:9 	 global-step:15869	 l-p:0.4583543539047241
====================================================================================================
====================================================================================================
====================================================================================================

epoch:794
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0474, 4.7892, 4.6398],
        [5.0474, 5.0254, 5.0443],
        [5.0474, 4.7911, 4.6659],
        [5.0474, 5.0474, 5.0474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:794, step:0 
model_pd.l_p.mean(): 0.17810066044330597 
model_pd.l_d.mean(): -20.268037796020508 
model_pd.lagr.mean(): -20.089937210083008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5129], device='cuda:0')), ('power', tensor([-21.0135], device='cuda:0'))])
epoch£º794	 i:0 	 global-step:15880	 l-p:0.17810066044330597
epoch£º794	 i:1 	 global-step:15881	 l-p:0.11435750871896744
epoch£º794	 i:2 	 global-step:15882	 l-p:0.21133862435817719
epoch£º794	 i:3 	 global-step:15883	 l-p:0.1308959722518921
epoch£º794	 i:4 	 global-step:15884	 l-p:0.18763530254364014
epoch£º794	 i:5 	 global-step:15885	 l-p:0.26050183176994324
epoch£º794	 i:6 	 global-step:15886	 l-p:0.1108027994632721
epoch£º794	 i:7 	 global-step:15887	 l-p:0.19284667074680328
epoch£º794	 i:8 	 global-step:15888	 l-p:0.11780589818954468
epoch£º794	 i:9 	 global-step:15889	 l-p:0.16978423297405243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:795
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1082, 5.1082, 5.1082],
        [5.1082, 4.9003, 4.6304],
        [5.1082, 5.0656, 5.0982],
        [5.1082, 5.0676, 5.0990]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:795, step:0 
model_pd.l_p.mean(): 0.10904286801815033 
model_pd.l_d.mean(): -20.387027740478516 
model_pd.lagr.mean(): -20.277984619140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4643], device='cuda:0')), ('power', tensor([-21.0842], device='cuda:0'))])
epoch£º795	 i:0 	 global-step:15900	 l-p:0.10904286801815033
epoch£º795	 i:1 	 global-step:15901	 l-p:0.1534448266029358
epoch£º795	 i:2 	 global-step:15902	 l-p:0.1612715870141983
epoch£º795	 i:3 	 global-step:15903	 l-p:0.1365595906972885
epoch£º795	 i:4 	 global-step:15904	 l-p:0.15180076658725739
epoch£º795	 i:5 	 global-step:15905	 l-p:0.09814891964197159
epoch£º795	 i:6 	 global-step:15906	 l-p:0.1852533370256424
epoch£º795	 i:7 	 global-step:15907	 l-p:0.12165864557027817
epoch£º795	 i:8 	 global-step:15908	 l-p:0.11427284777164459
epoch£º795	 i:9 	 global-step:15909	 l-p:0.12256128340959549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:796
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1360, 5.0851, 5.1223],
        [5.1360, 5.0857, 4.7921],
        [5.1360, 5.0044, 5.0549],
        [5.1360, 5.1360, 5.1360]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:796, step:0 
model_pd.l_p.mean(): 0.14138752222061157 
model_pd.l_d.mean(): -19.970430374145508 
model_pd.lagr.mean(): -19.829042434692383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4882], device='cuda:0')), ('power', tensor([-20.6874], device='cuda:0'))])
epoch£º796	 i:0 	 global-step:15920	 l-p:0.14138752222061157
epoch£º796	 i:1 	 global-step:15921	 l-p:0.12362521886825562
epoch£º796	 i:2 	 global-step:15922	 l-p:0.10004034638404846
epoch£º796	 i:3 	 global-step:15923	 l-p:0.15537050366401672
epoch£º796	 i:4 	 global-step:15924	 l-p:0.15801358222961426
epoch£º796	 i:5 	 global-step:15925	 l-p:0.1138303354382515
epoch£º796	 i:6 	 global-step:15926	 l-p:0.11427182704210281
epoch£º796	 i:7 	 global-step:15927	 l-p:0.1495213806629181
epoch£º796	 i:8 	 global-step:15928	 l-p:0.1365349292755127
epoch£º796	 i:9 	 global-step:15929	 l-p:0.16575641930103302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:797
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1241, 5.0344, 5.0856],
        [5.1241, 5.1077, 5.1222],
        [5.1241, 5.1235, 5.1241],
        [5.1241, 5.1241, 5.1241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:797, step:0 
model_pd.l_p.mean(): 0.11978735029697418 
model_pd.l_d.mean(): -20.52657127380371 
model_pd.lagr.mean(): -20.406784057617188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4572], device='cuda:0')), ('power', tensor([-21.2179], device='cuda:0'))])
epoch£º797	 i:0 	 global-step:15940	 l-p:0.11978735029697418
epoch£º797	 i:1 	 global-step:15941	 l-p:0.11870183050632477
epoch£º797	 i:2 	 global-step:15942	 l-p:0.14706702530384064
epoch£º797	 i:3 	 global-step:15943	 l-p:0.17688365280628204
epoch£º797	 i:4 	 global-step:15944	 l-p:0.23056136071681976
epoch£º797	 i:5 	 global-step:15945	 l-p:0.12099971622228622
epoch£º797	 i:6 	 global-step:15946	 l-p:0.17023417353630066
epoch£º797	 i:7 	 global-step:15947	 l-p:0.11572318524122238
epoch£º797	 i:8 	 global-step:15948	 l-p:0.17787761986255646
epoch£º797	 i:9 	 global-step:15949	 l-p:0.21950139105319977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:798
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0599, 5.0811, 4.8074],
        [5.0599, 5.0552, 5.0597],
        [5.0599, 4.9242, 4.9757],
        [5.0599, 5.3174, 5.1648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:798, step:0 
model_pd.l_p.mean(): 0.15402548015117645 
model_pd.l_d.mean(): -19.912242889404297 
model_pd.lagr.mean(): -19.758216857910156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4845], device='cuda:0')), ('power', tensor([-20.6248], device='cuda:0'))])
epoch£º798	 i:0 	 global-step:15960	 l-p:0.15402548015117645
epoch£º798	 i:1 	 global-step:15961	 l-p:0.11495887488126755
epoch£º798	 i:2 	 global-step:15962	 l-p:0.3396719694137573
epoch£º798	 i:3 	 global-step:15963	 l-p:0.1448686122894287
epoch£º798	 i:4 	 global-step:15964	 l-p:-0.0598863810300827
epoch£º798	 i:5 	 global-step:15965	 l-p:0.2135930359363556
epoch£º798	 i:6 	 global-step:15966	 l-p:0.11734660714864731
epoch£º798	 i:7 	 global-step:15967	 l-p:0.22652366757392883
epoch£º798	 i:8 	 global-step:15968	 l-p:0.26942938566207886
epoch£º798	 i:9 	 global-step:15969	 l-p:0.2033570110797882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:799
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0492, 5.0061, 5.0392],
        [5.0492, 5.0479, 5.0492],
        [5.0492, 5.0492, 5.0492],
        [5.0492, 5.0465, 5.0491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:799, step:0 
model_pd.l_p.mean(): 0.11547393351793289 
model_pd.l_d.mean(): -21.019989013671875 
model_pd.lagr.mean(): -20.90451431274414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4032], device='cuda:0')), ('power', tensor([-21.6616], device='cuda:0'))])
epoch£º799	 i:0 	 global-step:15980	 l-p:0.11547393351793289
epoch£º799	 i:1 	 global-step:15981	 l-p:-0.458607941865921
epoch£º799	 i:2 	 global-step:15982	 l-p:0.16880401968955994
epoch£º799	 i:3 	 global-step:15983	 l-p:0.12159671634435654
epoch£º799	 i:4 	 global-step:15984	 l-p:0.13034401834011078
epoch£º799	 i:5 	 global-step:15985	 l-p:0.10842211544513702
epoch£º799	 i:6 	 global-step:15986	 l-p:0.12069286406040192
epoch£º799	 i:7 	 global-step:15987	 l-p:0.12146260589361191
epoch£º799	 i:8 	 global-step:15988	 l-p:0.32561957836151123
epoch£º799	 i:9 	 global-step:15989	 l-p:0.13238383829593658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:800
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0613, 4.8375, 4.8148],
        [5.0613, 5.2169, 5.0059],
        [5.0613, 5.0534, 5.0607],
        [5.0613, 4.8024, 4.6490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:800, step:0 
model_pd.l_p.mean(): 0.12691615521907806 
model_pd.l_d.mean(): -18.543352127075195 
model_pd.lagr.mean(): -18.41643524169922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6012], device='cuda:0')), ('power', tensor([-19.3602], device='cuda:0'))])
epoch£º800	 i:0 	 global-step:16000	 l-p:0.12691615521907806
epoch£º800	 i:1 	 global-step:16001	 l-p:0.11220300197601318
epoch£º800	 i:2 	 global-step:16002	 l-p:0.1575838327407837
epoch£º800	 i:3 	 global-step:16003	 l-p:0.28640908002853394
epoch£º800	 i:4 	 global-step:16004	 l-p:0.1547372043132782
epoch£º800	 i:5 	 global-step:16005	 l-p:0.12311609089374542
epoch£º800	 i:6 	 global-step:16006	 l-p:0.46749091148376465
epoch£º800	 i:7 	 global-step:16007	 l-p:0.14184768497943878
epoch£º800	 i:8 	 global-step:16008	 l-p:0.19797863066196442
epoch£º800	 i:9 	 global-step:16009	 l-p:0.1347431093454361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:801
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7815,  0.7198,  1.0000,  0.6630,
          1.0000,  0.9211, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7501,  0.6816,  1.0000,  0.6193,
          1.0000,  0.9086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228]], device='cuda:0')
 pt:tensor([[5.0720, 5.2306, 5.0208],
        [5.0720, 5.1854, 4.9524],
        [5.0720, 4.8542, 4.5833],
        [5.0720, 4.8455, 4.8166]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:801, step:0 
model_pd.l_p.mean(): 0.13628855347633362 
model_pd.l_d.mean(): -19.737443923950195 
model_pd.lagr.mean(): -19.60115623474121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5053], device='cuda:0')), ('power', tensor([-20.4694], device='cuda:0'))])
epoch£º801	 i:0 	 global-step:16020	 l-p:0.13628855347633362
epoch£º801	 i:1 	 global-step:16021	 l-p:0.13286863267421722
epoch£º801	 i:2 	 global-step:16022	 l-p:0.10382091999053955
epoch£º801	 i:3 	 global-step:16023	 l-p:0.1671552062034607
epoch£º801	 i:4 	 global-step:16024	 l-p:0.1549278050661087
epoch£º801	 i:5 	 global-step:16025	 l-p:0.17528758943080902
epoch£º801	 i:6 	 global-step:16026	 l-p:0.12034793943166733
epoch£º801	 i:7 	 global-step:16027	 l-p:0.13907837867736816
epoch£º801	 i:8 	 global-step:16028	 l-p:0.23754099011421204
epoch£º801	 i:9 	 global-step:16029	 l-p:0.23110781610012054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:802
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0890, 5.0855, 5.0888],
        [5.0890, 4.9682, 5.0218],
        [5.0890, 5.0890, 5.0890],
        [5.0890, 4.8359, 4.7207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:802, step:0 
model_pd.l_p.mean(): 0.20712260901927948 
model_pd.l_d.mean(): -19.10508155822754 
model_pd.lagr.mean(): -18.897958755493164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5299], device='cuda:0')), ('power', tensor([-19.8552], device='cuda:0'))])
epoch£º802	 i:0 	 global-step:16040	 l-p:0.20712260901927948
epoch£º802	 i:1 	 global-step:16041	 l-p:0.15736831724643707
epoch£º802	 i:2 	 global-step:16042	 l-p:0.19169864058494568
epoch£º802	 i:3 	 global-step:16043	 l-p:0.12933705747127533
epoch£º802	 i:4 	 global-step:16044	 l-p:0.08772209286689758
epoch£º802	 i:5 	 global-step:16045	 l-p:0.13269144296646118
epoch£º802	 i:6 	 global-step:16046	 l-p:0.14875544607639313
epoch£º802	 i:7 	 global-step:16047	 l-p:0.13498146831989288
epoch£º802	 i:8 	 global-step:16048	 l-p:0.12906157970428467
epoch£º802	 i:9 	 global-step:16049	 l-p:0.13601963222026825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:803
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4241,  0.3187,  1.0000,  0.2394,
          1.0000,  0.7513, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3293,  0.2274,  1.0000,  0.1570,
          1.0000,  0.6906, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2741,  0.1781,  1.0000,  0.1157,
          1.0000,  0.6496, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[5.1199, 4.8931, 4.6429],
        [5.1199, 4.8670, 4.7145],
        [5.1199, 4.8770, 4.7966],
        [5.1199, 4.8671, 4.7223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:803, step:0 
model_pd.l_p.mean(): 0.14260300993919373 
model_pd.l_d.mean(): -20.727598190307617 
model_pd.lagr.mean(): -20.58499526977539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4378], device='cuda:0')), ('power', tensor([-21.4014], device='cuda:0'))])
epoch£º803	 i:0 	 global-step:16060	 l-p:0.14260300993919373
epoch£º803	 i:1 	 global-step:16061	 l-p:0.12226933985948563
epoch£º803	 i:2 	 global-step:16062	 l-p:0.10191516578197479
epoch£º803	 i:3 	 global-step:16063	 l-p:0.1444171816110611
epoch£º803	 i:4 	 global-step:16064	 l-p:0.17552867531776428
epoch£º803	 i:5 	 global-step:16065	 l-p:0.16446924209594727
epoch£º803	 i:6 	 global-step:16066	 l-p:0.10790760815143585
epoch£º803	 i:7 	 global-step:16067	 l-p:0.15541616082191467
epoch£º803	 i:8 	 global-step:16068	 l-p:0.1661224365234375
epoch£º803	 i:9 	 global-step:16069	 l-p:0.11301454156637192
====================================================================================================
====================================================================================================
====================================================================================================

epoch:804
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1296, 5.1269, 5.1295],
        [5.1296, 5.0945, 5.1225],
        [5.1296, 4.8836, 4.7879],
        [5.1296, 4.9987, 4.6973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:804, step:0 
model_pd.l_p.mean(): 0.13674381375312805 
model_pd.l_d.mean(): -19.8354549407959 
model_pd.lagr.mean(): -19.698711395263672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5401], device='cuda:0')), ('power', tensor([-20.6040], device='cuda:0'))])
epoch£º804	 i:0 	 global-step:16080	 l-p:0.13674381375312805
epoch£º804	 i:1 	 global-step:16081	 l-p:0.15996499359607697
epoch£º804	 i:2 	 global-step:16082	 l-p:0.10254105180501938
epoch£º804	 i:3 	 global-step:16083	 l-p:0.13617470860481262
epoch£º804	 i:4 	 global-step:16084	 l-p:0.12084928154945374
epoch£º804	 i:5 	 global-step:16085	 l-p:0.11444013565778732
epoch£º804	 i:6 	 global-step:16086	 l-p:0.15796956419944763
epoch£º804	 i:7 	 global-step:16087	 l-p:0.12883883714675903
epoch£º804	 i:8 	 global-step:16088	 l-p:0.09007176011800766
epoch£º804	 i:9 	 global-step:16089	 l-p:0.19411924481391907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:805
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1255, 5.1255, 5.1255],
        [5.1255, 5.1255, 5.1255],
        [5.1255, 5.4224, 5.2916],
        [5.1255, 5.0089, 5.0622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:805, step:0 
model_pd.l_p.mean(): 0.12525738775730133 
model_pd.l_d.mean(): -19.467208862304688 
model_pd.lagr.mean(): -19.341951370239258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5298], device='cuda:0')), ('power', tensor([-20.2212], device='cuda:0'))])
epoch£º805	 i:0 	 global-step:16100	 l-p:0.12525738775730133
epoch£º805	 i:1 	 global-step:16101	 l-p:0.12786759436130524
epoch£º805	 i:2 	 global-step:16102	 l-p:0.1714162826538086
epoch£º805	 i:3 	 global-step:16103	 l-p:0.10441325604915619
epoch£º805	 i:4 	 global-step:16104	 l-p:0.15120479464530945
epoch£º805	 i:5 	 global-step:16105	 l-p:0.13778051733970642
epoch£º805	 i:6 	 global-step:16106	 l-p:0.17024745047092438
epoch£º805	 i:7 	 global-step:16107	 l-p:0.1320648044347763
epoch£º805	 i:8 	 global-step:16108	 l-p:0.12366308271884918
epoch£º805	 i:9 	 global-step:16109	 l-p:0.13313044607639313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:806
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1268, 5.1267, 5.1268],
        [5.1268, 5.1261, 5.1268],
        [5.1268, 5.1268, 5.1268],
        [5.1268, 5.1242, 5.1267]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:806, step:0 
model_pd.l_p.mean(): 0.07495493441820145 
model_pd.l_d.mean(): -20.365943908691406 
model_pd.lagr.mean(): -20.29098892211914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4706], device='cuda:0')), ('power', tensor([-21.0693], device='cuda:0'))])
epoch£º806	 i:0 	 global-step:16120	 l-p:0.07495493441820145
epoch£º806	 i:1 	 global-step:16121	 l-p:0.17552590370178223
epoch£º806	 i:2 	 global-step:16122	 l-p:0.10332513600587845
epoch£º806	 i:3 	 global-step:16123	 l-p:0.16292129456996918
epoch£º806	 i:4 	 global-step:16124	 l-p:0.14183028042316437
epoch£º806	 i:5 	 global-step:16125	 l-p:0.13113096356391907
epoch£º806	 i:6 	 global-step:16126	 l-p:0.1262451708316803
epoch£º806	 i:7 	 global-step:16127	 l-p:0.12506933510303497
epoch£º806	 i:8 	 global-step:16128	 l-p:0.18045391142368317
epoch£º806	 i:9 	 global-step:16129	 l-p:0.1291998326778412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:807
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1391, 5.1385, 5.1391],
        [5.1391, 5.1387, 5.1391],
        [5.1391, 5.1378, 5.1391],
        [5.1391, 5.1392, 5.1391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:807, step:0 
model_pd.l_p.mean(): 0.09353563189506531 
model_pd.l_d.mean(): -20.914119720458984 
model_pd.lagr.mean(): -20.82058334350586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3957], device='cuda:0')), ('power', tensor([-21.5469], device='cuda:0'))])
epoch£º807	 i:0 	 global-step:16140	 l-p:0.09353563189506531
epoch£º807	 i:1 	 global-step:16141	 l-p:0.15286670625209808
epoch£º807	 i:2 	 global-step:16142	 l-p:0.13973629474639893
epoch£º807	 i:3 	 global-step:16143	 l-p:0.14886857569217682
epoch£º807	 i:4 	 global-step:16144	 l-p:0.1265394687652588
epoch£º807	 i:5 	 global-step:16145	 l-p:0.07406661659479141
epoch£º807	 i:6 	 global-step:16146	 l-p:0.11951463669538498
epoch£º807	 i:7 	 global-step:16147	 l-p:0.14778172969818115
epoch£º807	 i:8 	 global-step:16148	 l-p:0.12092062085866928
epoch£º807	 i:9 	 global-step:16149	 l-p:0.18315474689006805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:808
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1443, 5.1389, 5.1440],
        [5.1443, 5.1312, 5.1430],
        [5.1443, 5.1443, 5.1443],
        [5.1443, 5.1401, 5.1441]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:808, step:0 
model_pd.l_p.mean(): 0.1525542438030243 
model_pd.l_d.mean(): -20.04816436767578 
model_pd.lagr.mean(): -19.895610809326172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4510], device='cuda:0')), ('power', tensor([-20.7280], device='cuda:0'))])
epoch£º808	 i:0 	 global-step:16160	 l-p:0.1525542438030243
epoch£º808	 i:1 	 global-step:16161	 l-p:0.12426942586898804
epoch£º808	 i:2 	 global-step:16162	 l-p:0.15094956755638123
epoch£º808	 i:3 	 global-step:16163	 l-p:0.14355041086673737
epoch£º808	 i:4 	 global-step:16164	 l-p:0.05665326490998268
epoch£º808	 i:5 	 global-step:16165	 l-p:0.1635429710149765
epoch£º808	 i:6 	 global-step:16166	 l-p:0.1441032588481903
epoch£º808	 i:7 	 global-step:16167	 l-p:0.12315879762172699
epoch£º808	 i:8 	 global-step:16168	 l-p:0.13791215419769287
epoch£º808	 i:9 	 global-step:16169	 l-p:0.10863806307315826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:809
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1466, 5.1466, 5.1466],
        [5.1466, 5.1461, 5.1466],
        [5.1466, 5.0864, 5.1282],
        [5.1466, 5.1465, 5.1466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:809, step:0 
model_pd.l_p.mean(): 0.11688856035470963 
model_pd.l_d.mean(): -19.29277992248535 
model_pd.lagr.mean(): -19.175891876220703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5431], device='cuda:0')), ('power', tensor([-20.0584], device='cuda:0'))])
epoch£º809	 i:0 	 global-step:16180	 l-p:0.11688856035470963
epoch£º809	 i:1 	 global-step:16181	 l-p:0.10868873447179794
epoch£º809	 i:2 	 global-step:16182	 l-p:0.1273459494113922
epoch£º809	 i:3 	 global-step:16183	 l-p:0.10482173413038254
epoch£º809	 i:4 	 global-step:16184	 l-p:0.12327113002538681
epoch£º809	 i:5 	 global-step:16185	 l-p:0.12850455939769745
epoch£º809	 i:6 	 global-step:16186	 l-p:0.13956525921821594
epoch£º809	 i:7 	 global-step:16187	 l-p:0.160840705037117
epoch£º809	 i:8 	 global-step:16188	 l-p:0.12427617609500885
epoch£º809	 i:9 	 global-step:16189	 l-p:0.13314013183116913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:810
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1644, 4.9758, 4.9930],
        [5.1644, 4.9214, 4.7179],
        [5.1644, 5.6601, 5.6604],
        [5.1644, 5.1624, 5.1644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:810, step:0 
model_pd.l_p.mean(): 0.17554312944412231 
model_pd.l_d.mean(): -19.87322235107422 
model_pd.lagr.mean(): -19.69767951965332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4563], device='cuda:0')), ('power', tensor([-20.5566], device='cuda:0'))])
epoch£º810	 i:0 	 global-step:16200	 l-p:0.17554312944412231
epoch£º810	 i:1 	 global-step:16201	 l-p:0.15898139774799347
epoch£º810	 i:2 	 global-step:16202	 l-p:0.09000943601131439
epoch£º810	 i:3 	 global-step:16203	 l-p:0.1044473648071289
epoch£º810	 i:4 	 global-step:16204	 l-p:0.12444186955690384
epoch£º810	 i:5 	 global-step:16205	 l-p:0.0764816626906395
epoch£º810	 i:6 	 global-step:16206	 l-p:0.13607852160930634
epoch£º810	 i:7 	 global-step:16207	 l-p:0.11469709128141403
epoch£º810	 i:8 	 global-step:16208	 l-p:0.12316540628671646
epoch£º810	 i:9 	 global-step:16209	 l-p:0.13631008565425873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:811
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1606, 4.9105, 4.7652],
        [5.1606, 5.0129, 4.7154],
        [5.1606, 5.0714, 5.1225],
        [5.1606, 5.0214, 5.0702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:811, step:0 
model_pd.l_p.mean(): 0.05722471699118614 
model_pd.l_d.mean(): -19.555160522460938 
model_pd.lagr.mean(): -19.497936248779297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5389], device='cuda:0')), ('power', tensor([-20.3194], device='cuda:0'))])
epoch£º811	 i:0 	 global-step:16220	 l-p:0.05722471699118614
epoch£º811	 i:1 	 global-step:16221	 l-p:0.16067549586296082
epoch£º811	 i:2 	 global-step:16222	 l-p:0.10728641599416733
epoch£º811	 i:3 	 global-step:16223	 l-p:0.12351139634847641
epoch£º811	 i:4 	 global-step:16224	 l-p:0.16156291961669922
epoch£º811	 i:5 	 global-step:16225	 l-p:0.15871405601501465
epoch£º811	 i:6 	 global-step:16226	 l-p:0.12696877121925354
epoch£º811	 i:7 	 global-step:16227	 l-p:0.10919783264398575
epoch£º811	 i:8 	 global-step:16228	 l-p:0.09195870906114578
epoch£º811	 i:9 	 global-step:16229	 l-p:0.13391569256782532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:812
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1702, 5.1640, 5.1698],
        [5.1702, 5.1484, 5.1671],
        [5.1702, 5.3615, 5.1668],
        [5.1702, 5.1446, 5.1661]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:812, step:0 
model_pd.l_p.mean(): 0.031460002064704895 
model_pd.l_d.mean(): -20.702953338623047 
model_pd.lagr.mean(): -20.671493530273438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4317], device='cuda:0')), ('power', tensor([-21.3702], device='cuda:0'))])
epoch£º812	 i:0 	 global-step:16240	 l-p:0.031460002064704895
epoch£º812	 i:1 	 global-step:16241	 l-p:0.11762626469135284
epoch£º812	 i:2 	 global-step:16242	 l-p:0.1049908772110939
epoch£º812	 i:3 	 global-step:16243	 l-p:0.13845759630203247
epoch£º812	 i:4 	 global-step:16244	 l-p:0.1547560691833496
epoch£º812	 i:5 	 global-step:16245	 l-p:0.14725187420845032
epoch£º812	 i:6 	 global-step:16246	 l-p:0.11715461313724518
epoch£º812	 i:7 	 global-step:16247	 l-p:0.1289987415075302
epoch£º812	 i:8 	 global-step:16248	 l-p:0.13420961797237396
epoch£º812	 i:9 	 global-step:16249	 l-p:0.1238095834851265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:813
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1613, 5.0548, 5.1080],
        [5.1613, 4.9285, 4.8739],
        [5.1613, 4.9388, 4.9081],
        [5.1613, 5.1613, 5.1613]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:813, step:0 
model_pd.l_p.mean(): 0.14101408421993256 
model_pd.l_d.mean(): -19.81194496154785 
model_pd.lagr.mean(): -19.670930862426758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-20.5077], device='cuda:0'))])
epoch£º813	 i:0 	 global-step:16260	 l-p:0.14101408421993256
epoch£º813	 i:1 	 global-step:16261	 l-p:0.09995631873607635
epoch£º813	 i:2 	 global-step:16262	 l-p:0.12590830028057098
epoch£º813	 i:3 	 global-step:16263	 l-p:0.1238362118601799
epoch£º813	 i:4 	 global-step:16264	 l-p:0.1881512552499771
epoch£º813	 i:5 	 global-step:16265	 l-p:0.1011858657002449
epoch£º813	 i:6 	 global-step:16266	 l-p:0.10709062963724136
epoch£º813	 i:7 	 global-step:16267	 l-p:0.14075317978858948
epoch£º813	 i:8 	 global-step:16268	 l-p:0.1736343652009964
epoch£º813	 i:9 	 global-step:16269	 l-p:0.11481616646051407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:814
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1179, 5.1179, 5.1179],
        [5.1179, 5.1179, 5.1179],
        [5.1179, 5.0735, 5.1073],
        [5.1179, 5.1179, 5.1179]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:814, step:0 
model_pd.l_p.mean(): 0.13517427444458008 
model_pd.l_d.mean(): -20.719026565551758 
model_pd.lagr.mean(): -20.583852767944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4090], device='cuda:0')), ('power', tensor([-21.3633], device='cuda:0'))])
epoch£º814	 i:0 	 global-step:16280	 l-p:0.13517427444458008
epoch£º814	 i:1 	 global-step:16281	 l-p:0.20458054542541504
epoch£º814	 i:2 	 global-step:16282	 l-p:0.12286901473999023
epoch£º814	 i:3 	 global-step:16283	 l-p:0.10969742387533188
epoch£º814	 i:4 	 global-step:16284	 l-p:0.14958234131336212
epoch£º814	 i:5 	 global-step:16285	 l-p:0.18703141808509827
epoch£º814	 i:6 	 global-step:16286	 l-p:0.1383110135793686
epoch£º814	 i:7 	 global-step:16287	 l-p:0.0966663807630539
epoch£º814	 i:8 	 global-step:16288	 l-p:0.13148070871829987
epoch£º814	 i:9 	 global-step:16289	 l-p:0.17266032099723816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:815
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0947, 5.0947, 5.0947],
        [5.0947, 5.2625, 5.0561],
        [5.0947, 5.0098, 5.0605],
        [5.0947, 4.9905, 5.0443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:815, step:0 
model_pd.l_p.mean(): 0.11177515983581543 
model_pd.l_d.mean(): -19.049259185791016 
model_pd.lagr.mean(): -18.937484741210938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5747], device='cuda:0')), ('power', tensor([-19.8446], device='cuda:0'))])
epoch£º815	 i:0 	 global-step:16300	 l-p:0.11177515983581543
epoch£º815	 i:1 	 global-step:16301	 l-p:0.09525475651025772
epoch£º815	 i:2 	 global-step:16302	 l-p:0.14049869775772095
epoch£º815	 i:3 	 global-step:16303	 l-p:0.22105655074119568
epoch£º815	 i:4 	 global-step:16304	 l-p:0.12195655703544617
epoch£º815	 i:5 	 global-step:16305	 l-p:0.12931518256664276
epoch£º815	 i:6 	 global-step:16306	 l-p:0.19193637371063232
epoch£º815	 i:7 	 global-step:16307	 l-p:0.13757558166980743
epoch£º815	 i:8 	 global-step:16308	 l-p:0.17252086102962494
epoch£º815	 i:9 	 global-step:16309	 l-p:0.26374197006225586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:816
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0846, 5.0020, 5.0521],
        [5.0846, 4.9396, 4.9890],
        [5.0846, 5.0764, 5.0840],
        [5.0846, 5.0398, 5.0739]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:816, step:0 
model_pd.l_p.mean(): 0.15617337822914124 
model_pd.l_d.mean(): -20.876935958862305 
model_pd.lagr.mean(): -20.720762252807617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4020], device='cuda:0')), ('power', tensor([-21.5158], device='cuda:0'))])
epoch£º816	 i:0 	 global-step:16320	 l-p:0.15617337822914124
epoch£º816	 i:1 	 global-step:16321	 l-p:0.13322381675243378
epoch£º816	 i:2 	 global-step:16322	 l-p:0.13152721524238586
epoch£º816	 i:3 	 global-step:16323	 l-p:0.07684993743896484
epoch£º816	 i:4 	 global-step:16324	 l-p:0.3790312111377716
epoch£º816	 i:5 	 global-step:16325	 l-p:0.12415561079978943
epoch£º816	 i:6 	 global-step:16326	 l-p:0.2261493057012558
epoch£º816	 i:7 	 global-step:16327	 l-p:0.3070853352546692
epoch£º816	 i:8 	 global-step:16328	 l-p:0.1325913816690445
epoch£º816	 i:9 	 global-step:16329	 l-p:0.13207685947418213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:817
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0661, 5.2376, 5.0336],
        [5.0661, 5.0660, 5.0661],
        [5.0661, 5.0650, 5.0661],
        [5.0661, 4.8084, 4.6078]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:817, step:0 
model_pd.l_p.mean(): 0.0973469689488411 
model_pd.l_d.mean(): -18.634756088256836 
model_pd.lagr.mean(): -18.53740882873535 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6078], device='cuda:0')), ('power', tensor([-19.4594], device='cuda:0'))])
epoch£º817	 i:0 	 global-step:16340	 l-p:0.0973469689488411
epoch£º817	 i:1 	 global-step:16341	 l-p:0.17240627110004425
epoch£º817	 i:2 	 global-step:16342	 l-p:0.11911073327064514
epoch£º817	 i:3 	 global-step:16343	 l-p:0.1808302253484726
epoch£º817	 i:4 	 global-step:16344	 l-p:0.11888150870800018
epoch£º817	 i:5 	 global-step:16345	 l-p:0.27015724778175354
epoch£º817	 i:6 	 global-step:16346	 l-p:0.13324475288391113
epoch£º817	 i:7 	 global-step:16347	 l-p:0.3514593839645386
epoch£º817	 i:8 	 global-step:16348	 l-p:0.1803717166185379
epoch£º817	 i:9 	 global-step:16349	 l-p:0.12117134034633636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:818
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0859, 5.3459, 5.1923],
        [5.0859, 5.2378, 5.0228],
        [5.0859, 5.0859, 5.0859],
        [5.0859, 5.0132, 5.0603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:818, step:0 
model_pd.l_p.mean(): 0.1165890097618103 
model_pd.l_d.mean(): -19.264976501464844 
model_pd.lagr.mean(): -19.148387908935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5869], device='cuda:0')), ('power', tensor([-20.0751], device='cuda:0'))])
epoch£º818	 i:0 	 global-step:16360	 l-p:0.1165890097618103
epoch£º818	 i:1 	 global-step:16361	 l-p:0.15078115463256836
epoch£º818	 i:2 	 global-step:16362	 l-p:0.2658563256263733
epoch£º818	 i:3 	 global-step:16363	 l-p:0.13837780058383942
epoch£º818	 i:4 	 global-step:16364	 l-p:0.22575582563877106
epoch£º818	 i:5 	 global-step:16365	 l-p:0.1027320995926857
epoch£º818	 i:6 	 global-step:16366	 l-p:0.08952300995588303
epoch£º818	 i:7 	 global-step:16367	 l-p:0.12906363606452942
epoch£º818	 i:8 	 global-step:16368	 l-p:0.12635917961597443
epoch£º818	 i:9 	 global-step:16369	 l-p:0.1676950454711914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:819
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1148, 4.9971, 5.0510],
        [5.1148, 4.8705, 4.6439],
        [5.1148, 5.1100, 5.1145],
        [5.1148, 5.1665, 4.9030]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:819, step:0 
model_pd.l_p.mean(): 0.12660503387451172 
model_pd.l_d.mean(): -20.875686645507812 
model_pd.lagr.mean(): -20.749080657958984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4252], device='cuda:0')), ('power', tensor([-21.5382], device='cuda:0'))])
epoch£º819	 i:0 	 global-step:16380	 l-p:0.12660503387451172
epoch£º819	 i:1 	 global-step:16381	 l-p:0.21287383139133453
epoch£º819	 i:2 	 global-step:16382	 l-p:0.10948362946510315
epoch£º819	 i:3 	 global-step:16383	 l-p:0.1170608326792717
epoch£º819	 i:4 	 global-step:16384	 l-p:0.19921046495437622
epoch£º819	 i:5 	 global-step:16385	 l-p:0.12572447955608368
epoch£º819	 i:6 	 global-step:16386	 l-p:0.10945998877286911
epoch£º819	 i:7 	 global-step:16387	 l-p:0.17399688065052032
epoch£º819	 i:8 	 global-step:16388	 l-p:0.1668657511472702
epoch£º819	 i:9 	 global-step:16389	 l-p:0.08208206295967102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:820
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1106, 4.8810, 4.6254],
        [5.1106, 5.1093, 5.1106],
        [5.1106, 4.8550, 4.6720],
        [5.1106, 5.1106, 5.1106]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:820, step:0 
model_pd.l_p.mean(): 0.19525134563446045 
model_pd.l_d.mean(): -19.554088592529297 
model_pd.lagr.mean(): -19.358837127685547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4994], device='cuda:0')), ('power', tensor([-20.2780], device='cuda:0'))])
epoch£º820	 i:0 	 global-step:16400	 l-p:0.19525134563446045
epoch£º820	 i:1 	 global-step:16401	 l-p:0.11252954602241516
epoch£º820	 i:2 	 global-step:16402	 l-p:0.15224485099315643
epoch£º820	 i:3 	 global-step:16403	 l-p:0.1285502314567566
epoch£º820	 i:4 	 global-step:16404	 l-p:0.1333959549665451
epoch£º820	 i:5 	 global-step:16405	 l-p:0.09956932812929153
epoch£º820	 i:6 	 global-step:16406	 l-p:0.1765316128730774
epoch£º820	 i:7 	 global-step:16407	 l-p:0.13080741465091705
epoch£º820	 i:8 	 global-step:16408	 l-p:0.16824208199977875
epoch£º820	 i:9 	 global-step:16409	 l-p:0.10039453208446503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:821
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1345, 4.9815, 5.0266],
        [5.1345, 5.0501, 5.1004],
        [5.1345, 4.9699, 5.0089],
        [5.1345, 4.8811, 4.7523]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:821, step:0 
model_pd.l_p.mean(): 0.111989326775074 
model_pd.l_d.mean(): -20.966876983642578 
model_pd.lagr.mean(): -20.854887008666992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3908], device='cuda:0')), ('power', tensor([-21.5953], device='cuda:0'))])
epoch£º821	 i:0 	 global-step:16420	 l-p:0.111989326775074
epoch£º821	 i:1 	 global-step:16421	 l-p:0.1603800505399704
epoch£º821	 i:2 	 global-step:16422	 l-p:0.11098048090934753
epoch£º821	 i:3 	 global-step:16423	 l-p:0.1430225521326065
epoch£º821	 i:4 	 global-step:16424	 l-p:0.15928417444229126
epoch£º821	 i:5 	 global-step:16425	 l-p:0.09630363434553146
epoch£º821	 i:6 	 global-step:16426	 l-p:0.1759956181049347
epoch£º821	 i:7 	 global-step:16427	 l-p:0.12336259335279465
epoch£º821	 i:8 	 global-step:16428	 l-p:0.12077493220567703
epoch£º821	 i:9 	 global-step:16429	 l-p:0.09571002423763275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:822
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1635, 5.1560, 5.1630],
        [5.1635, 5.1034, 4.8052],
        [5.1635, 5.0517, 4.7489],
        [5.1635, 5.1632, 5.1635]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:822, step:0 
model_pd.l_p.mean(): 0.13249652087688446 
model_pd.l_d.mean(): -20.057886123657227 
model_pd.lagr.mean(): -19.925390243530273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4655], device='cuda:0')), ('power', tensor([-20.7526], device='cuda:0'))])
epoch£º822	 i:0 	 global-step:16440	 l-p:0.13249652087688446
epoch£º822	 i:1 	 global-step:16441	 l-p:0.07994377613067627
epoch£º822	 i:2 	 global-step:16442	 l-p:0.14754323661327362
epoch£º822	 i:3 	 global-step:16443	 l-p:0.08394020050764084
epoch£º822	 i:4 	 global-step:16444	 l-p:0.12071826308965683
epoch£º822	 i:5 	 global-step:16445	 l-p:0.12787774205207825
epoch£º822	 i:6 	 global-step:16446	 l-p:0.11415261775255203
epoch£º822	 i:7 	 global-step:16447	 l-p:0.09833738952875137
epoch£º822	 i:8 	 global-step:16448	 l-p:0.13477633893489838
epoch£º822	 i:9 	 global-step:16449	 l-p:0.18667812645435333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:823
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1670, 5.0816, 5.1320],
        [5.1670, 5.1670, 5.1670],
        [5.1670, 4.9438, 4.9133],
        [5.1670, 5.4831, 5.3618]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:823, step:0 
model_pd.l_p.mean(): 0.2050096094608307 
model_pd.l_d.mean(): -20.148298263549805 
model_pd.lagr.mean(): -19.943288803100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5188], device='cuda:0')), ('power', tensor([-20.8985], device='cuda:0'))])
epoch£º823	 i:0 	 global-step:16460	 l-p:0.2050096094608307
epoch£º823	 i:1 	 global-step:16461	 l-p:0.14471831917762756
epoch£º823	 i:2 	 global-step:16462	 l-p:0.1183275580406189
epoch£º823	 i:3 	 global-step:16463	 l-p:0.1625310629606247
epoch£º823	 i:4 	 global-step:16464	 l-p:0.13064980506896973
epoch£º823	 i:5 	 global-step:16465	 l-p:0.09622732549905777
epoch£º823	 i:6 	 global-step:16466	 l-p:0.04616376757621765
epoch£º823	 i:7 	 global-step:16467	 l-p:0.13864395022392273
epoch£º823	 i:8 	 global-step:16468	 l-p:0.08836673200130463
epoch£º823	 i:9 	 global-step:16469	 l-p:0.11704792082309723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:824
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1601, 4.9075, 4.7563],
        [5.1601, 5.1343, 5.1559],
        [5.1601, 5.0557, 5.1091],
        [5.1601, 5.1600, 5.1601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:824, step:0 
model_pd.l_p.mean(): 0.10931331664323807 
model_pd.l_d.mean(): -20.321666717529297 
model_pd.lagr.mean(): -20.212352752685547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4751], device='cuda:0')), ('power', tensor([-21.0291], device='cuda:0'))])
epoch£º824	 i:0 	 global-step:16480	 l-p:0.10931331664323807
epoch£º824	 i:1 	 global-step:16481	 l-p:0.10978679358959198
epoch£º824	 i:2 	 global-step:16482	 l-p:0.16056540608406067
epoch£º824	 i:3 	 global-step:16483	 l-p:0.12261796742677689
epoch£º824	 i:4 	 global-step:16484	 l-p:0.14036080241203308
epoch£º824	 i:5 	 global-step:16485	 l-p:0.04991128295660019
epoch£º824	 i:6 	 global-step:16486	 l-p:0.1338001936674118
epoch£º824	 i:7 	 global-step:16487	 l-p:0.1448352336883545
epoch£º824	 i:8 	 global-step:16488	 l-p:0.13885769248008728
epoch£º824	 i:9 	 global-step:16489	 l-p:0.15045203268527985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:825
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1480, 5.5302, 5.4518],
        [5.1480, 5.1476, 5.1480],
        [5.1480, 5.1480, 5.1480],
        [5.1480, 5.1072, 5.1389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:825, step:0 
model_pd.l_p.mean(): 0.08733416348695755 
model_pd.l_d.mean(): -19.81431007385254 
model_pd.lagr.mean(): -19.72697639465332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5072], device='cuda:0')), ('power', tensor([-20.5490], device='cuda:0'))])
epoch£º825	 i:0 	 global-step:16500	 l-p:0.08733416348695755
epoch£º825	 i:1 	 global-step:16501	 l-p:0.11821064352989197
epoch£º825	 i:2 	 global-step:16502	 l-p:0.133539080619812
epoch£º825	 i:3 	 global-step:16503	 l-p:0.12865611910820007
epoch£º825	 i:4 	 global-step:16504	 l-p:0.15922757983207703
epoch£º825	 i:5 	 global-step:16505	 l-p:0.12332215160131454
epoch£º825	 i:6 	 global-step:16506	 l-p:0.1266079545021057
epoch£º825	 i:7 	 global-step:16507	 l-p:0.15328188240528107
epoch£º825	 i:8 	 global-step:16508	 l-p:0.15621395409107208
epoch£º825	 i:9 	 global-step:16509	 l-p:0.14322887361049652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:826
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1349, 5.1312, 5.1347],
        [5.1349, 5.1249, 5.1341],
        [5.1349, 5.1322, 5.1348],
        [5.1349, 4.8868, 4.7952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:826, step:0 
model_pd.l_p.mean(): 0.12625247240066528 
model_pd.l_d.mean(): -20.68440818786621 
model_pd.lagr.mean(): -20.558155059814453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4222], device='cuda:0')), ('power', tensor([-21.3418], device='cuda:0'))])
epoch£º826	 i:0 	 global-step:16520	 l-p:0.12625247240066528
epoch£º826	 i:1 	 global-step:16521	 l-p:0.17270928621292114
epoch£º826	 i:2 	 global-step:16522	 l-p:0.13748201727867126
epoch£º826	 i:3 	 global-step:16523	 l-p:0.08168841898441315
epoch£º826	 i:4 	 global-step:16524	 l-p:0.10157999396324158
epoch£º826	 i:5 	 global-step:16525	 l-p:0.1250823587179184
epoch£º826	 i:6 	 global-step:16526	 l-p:0.13872751593589783
epoch£º826	 i:7 	 global-step:16527	 l-p:0.14636364579200745
epoch£º826	 i:8 	 global-step:16528	 l-p:0.14221885800361633
epoch£º826	 i:9 	 global-step:16529	 l-p:0.14217743277549744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:827
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1526, 5.1518, 5.1526],
        [5.1526, 5.1116, 5.1434],
        [5.1526, 5.1526, 5.1526],
        [5.1526, 5.0015, 5.0472]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:827, step:0 
model_pd.l_p.mean(): 0.12484878301620483 
model_pd.l_d.mean(): -20.371660232543945 
model_pd.lagr.mean(): -20.246810913085938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4477], device='cuda:0')), ('power', tensor([-21.0516], device='cuda:0'))])
epoch£º827	 i:0 	 global-step:16540	 l-p:0.12484878301620483
epoch£º827	 i:1 	 global-step:16541	 l-p:0.13708274066448212
epoch£º827	 i:2 	 global-step:16542	 l-p:0.11398875713348389
epoch£º827	 i:3 	 global-step:16543	 l-p:0.09228513389825821
epoch£º827	 i:4 	 global-step:16544	 l-p:0.164179727435112
epoch£º827	 i:5 	 global-step:16545	 l-p:0.18023286759853363
epoch£º827	 i:6 	 global-step:16546	 l-p:0.08108029514551163
epoch£º827	 i:7 	 global-step:16547	 l-p:0.1353766918182373
epoch£º827	 i:8 	 global-step:16548	 l-p:0.1281677633523941
epoch£º827	 i:9 	 global-step:16549	 l-p:0.11552952229976654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:828
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1662, 5.0532, 5.1068],
        [5.1662, 4.9728, 4.9870],
        [5.1662, 5.1210, 5.1552],
        [5.1662, 5.1271, 5.1577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:828, step:0 
model_pd.l_p.mean(): 0.11826298385858536 
model_pd.l_d.mean(): -20.460451126098633 
model_pd.lagr.mean(): -20.342187881469727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4352], device='cuda:0')), ('power', tensor([-21.1287], device='cuda:0'))])
epoch£º828	 i:0 	 global-step:16560	 l-p:0.11826298385858536
epoch£º828	 i:1 	 global-step:16561	 l-p:0.06398450583219528
epoch£º828	 i:2 	 global-step:16562	 l-p:0.06582857668399811
epoch£º828	 i:3 	 global-step:16563	 l-p:0.1271827220916748
epoch£º828	 i:4 	 global-step:16564	 l-p:0.15071342885494232
epoch£º828	 i:5 	 global-step:16565	 l-p:0.17004485428333282
epoch£º828	 i:6 	 global-step:16566	 l-p:0.14383716881275177
epoch£º828	 i:7 	 global-step:16567	 l-p:0.13767573237419128
epoch£º828	 i:8 	 global-step:16568	 l-p:0.12413328886032104
epoch£º828	 i:9 	 global-step:16569	 l-p:0.12054378539323807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:829
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1746, 5.0993, 5.1469],
        [5.1746, 5.0900, 4.7882],
        [5.1746, 5.1744, 5.1746],
        [5.1746, 5.1740, 5.1746]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:829, step:0 
model_pd.l_p.mean(): 0.07400938868522644 
model_pd.l_d.mean(): -19.602832794189453 
model_pd.lagr.mean(): -19.528823852539062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4884], device='cuda:0')), ('power', tensor([-20.3160], device='cuda:0'))])
epoch£º829	 i:0 	 global-step:16580	 l-p:0.07400938868522644
epoch£º829	 i:1 	 global-step:16581	 l-p:0.10771507024765015
epoch£º829	 i:2 	 global-step:16582	 l-p:0.13305388391017914
epoch£º829	 i:3 	 global-step:16583	 l-p:0.05641864612698555
epoch£º829	 i:4 	 global-step:16584	 l-p:0.14757966995239258
epoch£º829	 i:5 	 global-step:16585	 l-p:0.10894971340894699
epoch£º829	 i:6 	 global-step:16586	 l-p:0.19447119534015656
epoch£º829	 i:7 	 global-step:16587	 l-p:0.16069364547729492
epoch£º829	 i:8 	 global-step:16588	 l-p:0.13010545074939728
epoch£º829	 i:9 	 global-step:16589	 l-p:0.0900275781750679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:830
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1673, 5.1667, 5.1673],
        [5.1673, 4.9451, 4.9179],
        [5.1673, 4.9151, 4.7788],
        [5.1673, 5.1673, 5.1673]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:830, step:0 
model_pd.l_p.mean(): 0.14375339448451996 
model_pd.l_d.mean(): -20.621482849121094 
model_pd.lagr.mean(): -20.47772979736328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4289], device='cuda:0')), ('power', tensor([-21.2850], device='cuda:0'))])
epoch£º830	 i:0 	 global-step:16600	 l-p:0.14375339448451996
epoch£º830	 i:1 	 global-step:16601	 l-p:0.10635761171579361
epoch£º830	 i:2 	 global-step:16602	 l-p:0.13593627512454987
epoch£º830	 i:3 	 global-step:16603	 l-p:0.08389187604188919
epoch£º830	 i:4 	 global-step:16604	 l-p:0.1290505826473236
epoch£º830	 i:5 	 global-step:16605	 l-p:0.14762693643569946
epoch£º830	 i:6 	 global-step:16606	 l-p:0.16765747964382172
epoch£º830	 i:7 	 global-step:16607	 l-p:0.07932838797569275
epoch£º830	 i:8 	 global-step:16608	 l-p:0.2048596292734146
epoch£º830	 i:9 	 global-step:16609	 l-p:0.11698728054761887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:831
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1295, 5.1295, 5.1295],
        [5.1295, 5.1268, 5.1294],
        [5.1295, 5.1148, 5.1280],
        [5.1295, 5.1295, 5.1295]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:831, step:0 
model_pd.l_p.mean(): 0.13475126028060913 
model_pd.l_d.mean(): -20.044857025146484 
model_pd.lagr.mean(): -19.910106658935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4705], device='cuda:0')), ('power', tensor([-20.7446], device='cuda:0'))])
epoch£º831	 i:0 	 global-step:16620	 l-p:0.13475126028060913
epoch£º831	 i:1 	 global-step:16621	 l-p:0.09844166785478592
epoch£º831	 i:2 	 global-step:16622	 l-p:0.13759484887123108
epoch£º831	 i:3 	 global-step:16623	 l-p:0.1673482358455658
epoch£º831	 i:4 	 global-step:16624	 l-p:0.08839578181505203
epoch£º831	 i:5 	 global-step:16625	 l-p:0.15363498032093048
epoch£º831	 i:6 	 global-step:16626	 l-p:0.1624995917081833
epoch£º831	 i:7 	 global-step:16627	 l-p:0.1507432609796524
epoch£º831	 i:8 	 global-step:16628	 l-p:0.14386850595474243
epoch£º831	 i:9 	 global-step:16629	 l-p:0.1550336331129074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:832
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1172, 5.1129, 5.1170],
        [5.1172, 4.8897, 4.8604],
        [5.1172, 4.9586, 4.6552],
        [5.1172, 4.8896, 4.8601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:832, step:0 
model_pd.l_p.mean(): 0.10138759016990662 
model_pd.l_d.mean(): -20.260395050048828 
model_pd.lagr.mean(): -20.159008026123047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4724], device='cuda:0')), ('power', tensor([-20.9644], device='cuda:0'))])
epoch£º832	 i:0 	 global-step:16640	 l-p:0.10138759016990662
epoch£º832	 i:1 	 global-step:16641	 l-p:0.14736665785312653
epoch£º832	 i:2 	 global-step:16642	 l-p:0.0940643697977066
epoch£º832	 i:3 	 global-step:16643	 l-p:0.0853470116853714
epoch£º832	 i:4 	 global-step:16644	 l-p:0.13108320534229279
epoch£º832	 i:5 	 global-step:16645	 l-p:0.17208229005336761
epoch£º832	 i:6 	 global-step:16646	 l-p:0.27755415439605713
epoch£º832	 i:7 	 global-step:16647	 l-p:0.16000692546367645
epoch£º832	 i:8 	 global-step:16648	 l-p:0.14677821099758148
epoch£º832	 i:9 	 global-step:16649	 l-p:0.1825401335954666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:833
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1029, 5.1029, 5.1029],
        [5.1029, 5.0958, 5.1025],
        [5.1029, 4.9908, 5.0454],
        [5.1029, 5.0991, 5.1028]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:833, step:0 
model_pd.l_p.mean(): 0.13422517478466034 
model_pd.l_d.mean(): -19.798328399658203 
model_pd.lagr.mean(): -19.66410255432129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5331], device='cuda:0')), ('power', tensor([-20.5593], device='cuda:0'))])
epoch£º833	 i:0 	 global-step:16660	 l-p:0.13422517478466034
epoch£º833	 i:1 	 global-step:16661	 l-p:0.1485784351825714
epoch£º833	 i:2 	 global-step:16662	 l-p:0.13538028299808502
epoch£º833	 i:3 	 global-step:16663	 l-p:0.13850626349449158
epoch£º833	 i:4 	 global-step:16664	 l-p:0.10179956257343292
epoch£º833	 i:5 	 global-step:16665	 l-p:0.11890343576669693
epoch£º833	 i:6 	 global-step:16666	 l-p:0.1113370880484581
epoch£º833	 i:7 	 global-step:16667	 l-p:0.07887092232704163
epoch£º833	 i:8 	 global-step:16668	 l-p:0.22912879288196564
epoch£º833	 i:9 	 global-step:16669	 l-p:0.34852272272109985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:834
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0930, 5.0930, 5.0930],
        [5.0930, 5.4894, 5.4215],
        [5.0930, 5.0930, 5.0930],
        [5.0930, 4.9376, 4.9838]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:834, step:0 
model_pd.l_p.mean(): 0.15860363841056824 
model_pd.l_d.mean(): -19.74428367614746 
model_pd.lagr.mean(): -19.58568000793457 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5813], device='cuda:0')), ('power', tensor([-20.5539], device='cuda:0'))])
epoch£º834	 i:0 	 global-step:16680	 l-p:0.15860363841056824
epoch£º834	 i:1 	 global-step:16681	 l-p:0.1248730719089508
epoch£º834	 i:2 	 global-step:16682	 l-p:0.1190415620803833
epoch£º834	 i:3 	 global-step:16683	 l-p:0.12890170514583588
epoch£º834	 i:4 	 global-step:16684	 l-p:0.31171971559524536
epoch£º834	 i:5 	 global-step:16685	 l-p:0.1056668609380722
epoch£º834	 i:6 	 global-step:16686	 l-p:0.254648894071579
epoch£º834	 i:7 	 global-step:16687	 l-p:0.11100009828805923
epoch£º834	 i:8 	 global-step:16688	 l-p:0.09955675154924393
epoch£º834	 i:9 	 global-step:16689	 l-p:0.14532619714736938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:835
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0950, 5.0949, 5.0950],
        [5.0950, 4.9728, 5.0272],
        [5.0950, 4.8361, 4.6438],
        [5.0950, 4.8989, 4.6063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:835, step:0 
model_pd.l_p.mean(): 0.08088455349206924 
model_pd.l_d.mean(): -19.87643051147461 
model_pd.lagr.mean(): -19.79554557800293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5005], device='cuda:0')), ('power', tensor([-20.6050], device='cuda:0'))])
epoch£º835	 i:0 	 global-step:16700	 l-p:0.08088455349206924
epoch£º835	 i:1 	 global-step:16701	 l-p:0.15018050372600555
epoch£º835	 i:2 	 global-step:16702	 l-p:0.11968079209327698
epoch£º835	 i:3 	 global-step:16703	 l-p:0.22114460170269012
epoch£º835	 i:4 	 global-step:16704	 l-p:0.23689544200897217
epoch£º835	 i:5 	 global-step:16705	 l-p:0.1332511007785797
epoch£º835	 i:6 	 global-step:16706	 l-p:0.10081750899553299
epoch£º835	 i:7 	 global-step:16707	 l-p:0.18041415512561798
epoch£º835	 i:8 	 global-step:16708	 l-p:0.13926294445991516
epoch£º835	 i:9 	 global-step:16709	 l-p:0.13304778933525085
====================================================================================================
====================================================================================================
====================================================================================================

epoch:836
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1087, 5.0957, 5.1075],
        [5.1087, 5.1175, 4.8356],
        [5.1087, 5.1077, 5.1087],
        [5.1087, 5.0028, 5.0571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:836, step:0 
model_pd.l_p.mean(): 0.1499890387058258 
model_pd.l_d.mean(): -20.576370239257812 
model_pd.lagr.mean(): -20.426382064819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4659], device='cuda:0')), ('power', tensor([-21.2772], device='cuda:0'))])
epoch£º836	 i:0 	 global-step:16720	 l-p:0.1499890387058258
epoch£º836	 i:1 	 global-step:16721	 l-p:0.13747699558734894
epoch£º836	 i:2 	 global-step:16722	 l-p:0.11893024295568466
epoch£º836	 i:3 	 global-step:16723	 l-p:0.10367986559867859
epoch£º836	 i:4 	 global-step:16724	 l-p:0.17131106555461884
epoch£º836	 i:5 	 global-step:16725	 l-p:0.132735013961792
epoch£º836	 i:6 	 global-step:16726	 l-p:0.12673820555210114
epoch£º836	 i:7 	 global-step:16727	 l-p:0.14389415085315704
epoch£º836	 i:8 	 global-step:16728	 l-p:0.13568304479122162
epoch£º836	 i:9 	 global-step:16729	 l-p:0.19036732614040375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:837
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1225, 5.0514, 5.0979],
        [5.1225, 5.1222, 5.1225],
        [5.1225, 5.0230, 4.7157],
        [5.1225, 4.9036, 4.8895]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:837, step:0 
model_pd.l_p.mean(): 0.18871408700942993 
model_pd.l_d.mean(): -20.898317337036133 
model_pd.lagr.mean(): -20.70960235595703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4230], device='cuda:0')), ('power', tensor([-21.5588], device='cuda:0'))])
epoch£º837	 i:0 	 global-step:16740	 l-p:0.18871408700942993
epoch£º837	 i:1 	 global-step:16741	 l-p:0.13262280821800232
epoch£º837	 i:2 	 global-step:16742	 l-p:0.1273820996284485
epoch£º837	 i:3 	 global-step:16743	 l-p:0.12706300616264343
epoch£º837	 i:4 	 global-step:16744	 l-p:0.11951097100973129
epoch£º837	 i:5 	 global-step:16745	 l-p:0.13254386186599731
epoch£º837	 i:6 	 global-step:16746	 l-p:0.18859218060970306
epoch£º837	 i:7 	 global-step:16747	 l-p:0.0857633575797081
epoch£º837	 i:8 	 global-step:16748	 l-p:0.15309962630271912
epoch£º837	 i:9 	 global-step:16749	 l-p:0.1259048879146576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:838
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1295, 5.1295, 5.1295],
        [5.1295, 5.4978, 5.4100],
        [5.1295, 4.9773, 5.0239],
        [5.1295, 5.0077, 5.0617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:838, step:0 
model_pd.l_p.mean(): 0.10985858738422394 
model_pd.l_d.mean(): -19.7723388671875 
model_pd.lagr.mean(): -19.662479400634766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5867], device='cuda:0')), ('power', tensor([-20.5878], device='cuda:0'))])
epoch£º838	 i:0 	 global-step:16760	 l-p:0.10985858738422394
epoch£º838	 i:1 	 global-step:16761	 l-p:0.15439748764038086
epoch£º838	 i:2 	 global-step:16762	 l-p:0.12321936339139938
epoch£º838	 i:3 	 global-step:16763	 l-p:0.1270941197872162
epoch£º838	 i:4 	 global-step:16764	 l-p:0.11798243224620819
epoch£º838	 i:5 	 global-step:16765	 l-p:0.15342459082603455
epoch£º838	 i:6 	 global-step:16766	 l-p:0.12011853605508804
epoch£º838	 i:7 	 global-step:16767	 l-p:0.18561439216136932
epoch£º838	 i:8 	 global-step:16768	 l-p:0.13070455193519592
epoch£º838	 i:9 	 global-step:16769	 l-p:0.11829103529453278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:839
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1483, 5.1480, 5.1483],
        [5.1483, 5.1481, 5.1483],
        [5.1483, 5.1482, 5.1483],
        [5.1483, 5.1455, 5.1482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:839, step:0 
model_pd.l_p.mean(): 0.12283972650766373 
model_pd.l_d.mean(): -20.32892608642578 
model_pd.lagr.mean(): -20.206087112426758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4814], device='cuda:0')), ('power', tensor([-21.0428], device='cuda:0'))])
epoch£º839	 i:0 	 global-step:16780	 l-p:0.12283972650766373
epoch£º839	 i:1 	 global-step:16781	 l-p:0.17259761691093445
epoch£º839	 i:2 	 global-step:16782	 l-p:0.13366961479187012
epoch£º839	 i:3 	 global-step:16783	 l-p:0.18451131880283356
epoch£º839	 i:4 	 global-step:16784	 l-p:0.16915930807590485
epoch£º839	 i:5 	 global-step:16785	 l-p:0.10060781985521317
epoch£º839	 i:6 	 global-step:16786	 l-p:0.07804513722658157
epoch£º839	 i:7 	 global-step:16787	 l-p:0.11753226071596146
epoch£º839	 i:8 	 global-step:16788	 l-p:0.11957257986068726
epoch£º839	 i:9 	 global-step:16789	 l-p:0.11619902402162552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:840
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1480, 5.1481, 5.1481],
        [5.1480, 5.0216, 4.7153],
        [5.1480, 5.0236, 5.0770],
        [5.1480, 5.1481, 5.1481]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:840, step:0 
model_pd.l_p.mean(): 0.10806482285261154 
model_pd.l_d.mean(): -19.30133819580078 
model_pd.lagr.mean(): -19.193273544311523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5484], device='cuda:0')), ('power', tensor([-20.0725], device='cuda:0'))])
epoch£º840	 i:0 	 global-step:16800	 l-p:0.10806482285261154
epoch£º840	 i:1 	 global-step:16801	 l-p:0.10125703364610672
epoch£º840	 i:2 	 global-step:16802	 l-p:0.08892128616571426
epoch£º840	 i:3 	 global-step:16803	 l-p:0.1194399818778038
epoch£º840	 i:4 	 global-step:16804	 l-p:0.1765664964914322
epoch£º840	 i:5 	 global-step:16805	 l-p:0.16068035364151
epoch£º840	 i:6 	 global-step:16806	 l-p:0.12791192531585693
epoch£º840	 i:7 	 global-step:16807	 l-p:0.15694929659366608
epoch£º840	 i:8 	 global-step:16808	 l-p:0.09822770953178406
epoch£º840	 i:9 	 global-step:16809	 l-p:0.13912002742290497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:841
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1581, 4.9176, 4.6854],
        [5.1581, 5.1523, 5.1578],
        [5.1581, 5.1560, 5.1580],
        [5.1581, 5.0366, 4.7307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:841, step:0 
model_pd.l_p.mean(): 0.07779265940189362 
model_pd.l_d.mean(): -20.51120948791504 
model_pd.lagr.mean(): -20.43341636657715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4631], device='cuda:0')), ('power', tensor([-21.2084], device='cuda:0'))])
epoch£º841	 i:0 	 global-step:16820	 l-p:0.07779265940189362
epoch£º841	 i:1 	 global-step:16821	 l-p:0.09410770982503891
epoch£º841	 i:2 	 global-step:16822	 l-p:0.12285976111888885
epoch£º841	 i:3 	 global-step:16823	 l-p:0.1843796670436859
epoch£º841	 i:4 	 global-step:16824	 l-p:0.1261708289384842
epoch£º841	 i:5 	 global-step:16825	 l-p:0.1154470294713974
epoch£º841	 i:6 	 global-step:16826	 l-p:0.15536801517009735
epoch£º841	 i:7 	 global-step:16827	 l-p:0.10508207976818085
epoch£º841	 i:8 	 global-step:16828	 l-p:0.1299983710050583
epoch£º841	 i:9 	 global-step:16829	 l-p:0.16151881217956543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:842
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1500, 5.0662, 5.1166],
        [5.1500, 4.9480, 4.6656],
        [5.1500, 5.3282, 5.1245],
        [5.1500, 5.1500, 5.1500]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:842, step:0 
model_pd.l_p.mean(): 0.13017646968364716 
model_pd.l_d.mean(): -20.901262283325195 
model_pd.lagr.mean(): -20.771085739135742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3988], device='cuda:0')), ('power', tensor([-21.5370], device='cuda:0'))])
epoch£º842	 i:0 	 global-step:16840	 l-p:0.13017646968364716
epoch£º842	 i:1 	 global-step:16841	 l-p:0.06636381894350052
epoch£º842	 i:2 	 global-step:16842	 l-p:0.17091020941734314
epoch£º842	 i:3 	 global-step:16843	 l-p:0.12249570339918137
epoch£º842	 i:4 	 global-step:16844	 l-p:0.13595864176750183
epoch£º842	 i:5 	 global-step:16845	 l-p:0.11075909435749054
epoch£º842	 i:6 	 global-step:16846	 l-p:0.12726473808288574
epoch£º842	 i:7 	 global-step:16847	 l-p:0.10618752986192703
epoch£º842	 i:8 	 global-step:16848	 l-p:0.22161155939102173
epoch£º842	 i:9 	 global-step:16849	 l-p:0.1189381405711174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:843
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1447, 5.0077, 4.7016],
        [5.1447, 5.1447, 5.1447],
        [5.1447, 5.1294, 5.1430],
        [5.1447, 5.3148, 5.1066]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:843, step:0 
model_pd.l_p.mean(): 0.10594511777162552 
model_pd.l_d.mean(): -19.966232299804688 
model_pd.lagr.mean(): -19.860286712646484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4871], device='cuda:0')), ('power', tensor([-20.6820], device='cuda:0'))])
epoch£º843	 i:0 	 global-step:16860	 l-p:0.10594511777162552
epoch£º843	 i:1 	 global-step:16861	 l-p:0.1174294576048851
epoch£º843	 i:2 	 global-step:16862	 l-p:0.10084721446037292
epoch£º843	 i:3 	 global-step:16863	 l-p:0.14211928844451904
epoch£º843	 i:4 	 global-step:16864	 l-p:0.19254690408706665
epoch£º843	 i:5 	 global-step:16865	 l-p:0.08208349347114563
epoch£º843	 i:6 	 global-step:16866	 l-p:0.1479758322238922
epoch£º843	 i:7 	 global-step:16867	 l-p:0.10916134715080261
epoch£º843	 i:8 	 global-step:16868	 l-p:0.14256224036216736
epoch£º843	 i:9 	 global-step:16869	 l-p:0.17423145473003387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:844
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1483, 4.9927, 4.6896],
        [5.1483, 5.1483, 5.1483],
        [5.1483, 4.9126, 4.8636],
        [5.1483, 5.0121, 5.0636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:844, step:0 
model_pd.l_p.mean(): 0.12106964737176895 
model_pd.l_d.mean(): -20.67000389099121 
model_pd.lagr.mean(): -20.548934936523438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4304], device='cuda:0')), ('power', tensor([-21.3356], device='cuda:0'))])
epoch£º844	 i:0 	 global-step:16880	 l-p:0.12106964737176895
epoch£º844	 i:1 	 global-step:16881	 l-p:0.12909166514873505
epoch£º844	 i:2 	 global-step:16882	 l-p:0.20333075523376465
epoch£º844	 i:3 	 global-step:16883	 l-p:0.05222213268280029
epoch£º844	 i:4 	 global-step:16884	 l-p:0.14275434613227844
epoch£º844	 i:5 	 global-step:16885	 l-p:0.14103466272354126
epoch£º844	 i:6 	 global-step:16886	 l-p:0.1410558521747589
epoch£º844	 i:7 	 global-step:16887	 l-p:0.1520887017250061
epoch£º844	 i:8 	 global-step:16888	 l-p:0.1203007847070694
epoch£º844	 i:9 	 global-step:16889	 l-p:0.13067732751369476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:845
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1364, 5.1364, 5.1364],
        [5.1364, 5.1187, 5.1343],
        [5.1364, 4.9576, 4.9889],
        [5.1364, 4.8830, 4.7787]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:845, step:0 
model_pd.l_p.mean(): 0.13673634827136993 
model_pd.l_d.mean(): -20.444013595581055 
model_pd.lagr.mean(): -20.30727767944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4652], device='cuda:0')), ('power', tensor([-21.1427], device='cuda:0'))])
epoch£º845	 i:0 	 global-step:16900	 l-p:0.13673634827136993
epoch£º845	 i:1 	 global-step:16901	 l-p:0.12919579446315765
epoch£º845	 i:2 	 global-step:16902	 l-p:0.1285264790058136
epoch£º845	 i:3 	 global-step:16903	 l-p:0.17792247235774994
epoch£º845	 i:4 	 global-step:16904	 l-p:0.13560815155506134
epoch£º845	 i:5 	 global-step:16905	 l-p:0.12110116332769394
epoch£º845	 i:6 	 global-step:16906	 l-p:0.11138215661048889
epoch£º845	 i:7 	 global-step:16907	 l-p:0.13505591452121735
epoch£º845	 i:8 	 global-step:16908	 l-p:0.12474729865789413
epoch£º845	 i:9 	 global-step:16909	 l-p:0.1525694578886032
====================================================================================================
====================================================================================================
====================================================================================================

epoch:846
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1324, 5.0260, 5.0803],
        [5.1324, 5.0927, 5.1238],
        [5.1324, 5.0578, 5.1056],
        [5.1324, 5.1324, 5.1324]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:846, step:0 
model_pd.l_p.mean(): 0.13657616078853607 
model_pd.l_d.mean(): -19.534088134765625 
model_pd.lagr.mean(): -19.397512435913086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5191], device='cuda:0')), ('power', tensor([-20.2778], device='cuda:0'))])
epoch£º846	 i:0 	 global-step:16920	 l-p:0.13657616078853607
epoch£º846	 i:1 	 global-step:16921	 l-p:0.17283102869987488
epoch£º846	 i:2 	 global-step:16922	 l-p:0.20145829021930695
epoch£º846	 i:3 	 global-step:16923	 l-p:0.09293704479932785
epoch£º846	 i:4 	 global-step:16924	 l-p:0.12541471421718597
epoch£º846	 i:5 	 global-step:16925	 l-p:0.07220012694597244
epoch£º846	 i:6 	 global-step:16926	 l-p:0.11839424073696136
epoch£º846	 i:7 	 global-step:16927	 l-p:0.1408885419368744
epoch£º846	 i:8 	 global-step:16928	 l-p:0.14441156387329102
epoch£º846	 i:9 	 global-step:16929	 l-p:0.1444956213235855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:847
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1475, 5.0990, 5.1352],
        [5.1475, 5.1475, 5.1475],
        [5.1475, 5.0956, 5.1336],
        [5.1475, 5.0390, 5.0933]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:847, step:0 
model_pd.l_p.mean(): 0.12304307520389557 
model_pd.l_d.mean(): -20.743915557861328 
model_pd.lagr.mean(): -20.620872497558594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4119], device='cuda:0')), ('power', tensor([-21.3914], device='cuda:0'))])
epoch£º847	 i:0 	 global-step:16940	 l-p:0.12304307520389557
epoch£º847	 i:1 	 global-step:16941	 l-p:0.15312333405017853
epoch£º847	 i:2 	 global-step:16942	 l-p:0.1452844738960266
epoch£º847	 i:3 	 global-step:16943	 l-p:0.090825654566288
epoch£º847	 i:4 	 global-step:16944	 l-p:0.14948858320713043
epoch£º847	 i:5 	 global-step:16945	 l-p:0.1754876673221588
epoch£º847	 i:6 	 global-step:16946	 l-p:0.06488227844238281
epoch£º847	 i:7 	 global-step:16947	 l-p:0.15188227593898773
epoch£º847	 i:8 	 global-step:16948	 l-p:0.1704438030719757
epoch£º847	 i:9 	 global-step:16949	 l-p:0.12060727924108505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:848
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1399, 5.0669, 5.1141],
        [5.1399, 4.9354, 4.9421],
        [5.1399, 5.1394, 5.1399],
        [5.1399, 4.9227, 4.6489]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:848, step:0 
model_pd.l_p.mean(): 0.1763087809085846 
model_pd.l_d.mean(): -20.807641983032227 
model_pd.lagr.mean(): -20.631332397460938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4084], device='cuda:0')), ('power', tensor([-21.4522], device='cuda:0'))])
epoch£º848	 i:0 	 global-step:16960	 l-p:0.1763087809085846
epoch£º848	 i:1 	 global-step:16961	 l-p:0.06829705089330673
epoch£º848	 i:2 	 global-step:16962	 l-p:0.1379704475402832
epoch£º848	 i:3 	 global-step:16963	 l-p:0.15260301530361176
epoch£º848	 i:4 	 global-step:16964	 l-p:0.163045272231102
epoch£º848	 i:5 	 global-step:16965	 l-p:0.13934782147407532
epoch£º848	 i:6 	 global-step:16966	 l-p:0.13166716694831848
epoch£º848	 i:7 	 global-step:16967	 l-p:0.15713438391685486
epoch£º848	 i:8 	 global-step:16968	 l-p:0.08226215094327927
epoch£º848	 i:9 	 global-step:16969	 l-p:0.09634437412023544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:849
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1620, 4.9263, 4.8767],
        [5.1620, 5.1307, 4.8356],
        [5.1620, 4.9385, 4.9146],
        [5.1620, 5.1139, 5.1498]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:849, step:0 
model_pd.l_p.mean(): 0.12503665685653687 
model_pd.l_d.mean(): -18.957895278930664 
model_pd.lagr.mean(): -18.83285903930664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5522], device='cuda:0')), ('power', tensor([-19.7292], device='cuda:0'))])
epoch£º849	 i:0 	 global-step:16980	 l-p:0.12503665685653687
epoch£º849	 i:1 	 global-step:16981	 l-p:0.13706213235855103
epoch£º849	 i:2 	 global-step:16982	 l-p:0.12954293191432953
epoch£º849	 i:3 	 global-step:16983	 l-p:0.11774885654449463
epoch£º849	 i:4 	 global-step:16984	 l-p:0.08325996994972229
epoch£º849	 i:5 	 global-step:16985	 l-p:0.1652124524116516
epoch£º849	 i:6 	 global-step:16986	 l-p:0.13850991427898407
epoch£º849	 i:7 	 global-step:16987	 l-p:0.08025737851858139
epoch£º849	 i:8 	 global-step:16988	 l-p:0.1278461068868637
epoch£º849	 i:9 	 global-step:16989	 l-p:0.16060061752796173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:850
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8903,  0.8564,  1.0000,  0.8239,
          1.0000,  0.9620, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228]], device='cuda:0')
 pt:tensor([[5.1590, 5.4980, 5.3891],
        [5.1590, 4.9033, 4.7187],
        [5.1590, 5.0045, 5.0500],
        [5.1590, 5.5146, 5.4165]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:850, step:0 
model_pd.l_p.mean(): 0.11246564984321594 
model_pd.l_d.mean(): -20.325441360473633 
model_pd.lagr.mean(): -20.212976455688477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4755], device='cuda:0')), ('power', tensor([-21.0333], device='cuda:0'))])
epoch£º850	 i:0 	 global-step:17000	 l-p:0.11246564984321594
epoch£º850	 i:1 	 global-step:17001	 l-p:0.14236733317375183
epoch£º850	 i:2 	 global-step:17002	 l-p:0.11552014946937561
epoch£º850	 i:3 	 global-step:17003	 l-p:0.14016713201999664
epoch£º850	 i:4 	 global-step:17004	 l-p:0.10176193714141846
epoch£º850	 i:5 	 global-step:17005	 l-p:0.17859134078025818
epoch£º850	 i:6 	 global-step:17006	 l-p:0.11387164145708084
epoch£º850	 i:7 	 global-step:17007	 l-p:0.20674636960029602
epoch£º850	 i:8 	 global-step:17008	 l-p:0.10551321506500244
epoch£º850	 i:9 	 global-step:17009	 l-p:0.12544991075992584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:851
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1361, 5.1361, 5.1361],
        [5.1361, 5.1358, 5.1361],
        [5.1361, 5.0858, 5.1230],
        [5.1361, 4.9457, 4.6511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:851, step:0 
model_pd.l_p.mean(): 0.16377456486225128 
model_pd.l_d.mean(): -19.679414749145508 
model_pd.lagr.mean(): -19.515640258789062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5600], device='cuda:0')), ('power', tensor([-20.4666], device='cuda:0'))])
epoch£º851	 i:0 	 global-step:17020	 l-p:0.16377456486225128
epoch£º851	 i:1 	 global-step:17021	 l-p:0.1101970449090004
epoch£º851	 i:2 	 global-step:17022	 l-p:0.19248314201831818
epoch£º851	 i:3 	 global-step:17023	 l-p:0.11699538677930832
epoch£º851	 i:4 	 global-step:17024	 l-p:0.11756288260221481
epoch£º851	 i:5 	 global-step:17025	 l-p:0.10293343663215637
epoch£º851	 i:6 	 global-step:17026	 l-p:0.18728217482566833
epoch£º851	 i:7 	 global-step:17027	 l-p:0.08704618364572525
epoch£º851	 i:8 	 global-step:17028	 l-p:0.13376781344413757
epoch£º851	 i:9 	 global-step:17029	 l-p:0.11904287338256836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:852
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1428, 5.6128, 5.5922],
        [5.1428, 4.8839, 4.7463],
        [5.1428, 4.8908, 4.7974],
        [5.1428, 5.1427, 5.1428]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:852, step:0 
model_pd.l_p.mean(): 0.10573431849479675 
model_pd.l_d.mean(): -19.727046966552734 
model_pd.lagr.mean(): -19.621313095092773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5126], device='cuda:0')), ('power', tensor([-20.4663], device='cuda:0'))])
epoch£º852	 i:0 	 global-step:17040	 l-p:0.10573431849479675
epoch£º852	 i:1 	 global-step:17041	 l-p:0.09686845541000366
epoch£º852	 i:2 	 global-step:17042	 l-p:0.15308396518230438
epoch£º852	 i:3 	 global-step:17043	 l-p:0.14743845164775848
epoch£º852	 i:4 	 global-step:17044	 l-p:0.11440925300121307
epoch£º852	 i:5 	 global-step:17045	 l-p:0.12704156339168549
epoch£º852	 i:6 	 global-step:17046	 l-p:0.16436448693275452
epoch£º852	 i:7 	 global-step:17047	 l-p:0.13927887380123138
epoch£º852	 i:8 	 global-step:17048	 l-p:0.15471717715263367
epoch£º852	 i:9 	 global-step:17049	 l-p:0.1385815441608429
====================================================================================================
====================================================================================================
====================================================================================================

epoch:853
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1271, 5.1464, 4.8666],
        [5.1271, 5.1157, 5.1261],
        [5.1271, 4.9643, 5.0073],
        [5.1271, 4.9882, 5.0402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:853, step:0 
model_pd.l_p.mean(): 0.1791793406009674 
model_pd.l_d.mean(): -19.555376052856445 
model_pd.lagr.mean(): -19.376195907592773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5211], device='cuda:0')), ('power', tensor([-20.3015], device='cuda:0'))])
epoch£º853	 i:0 	 global-step:17060	 l-p:0.1791793406009674
epoch£º853	 i:1 	 global-step:17061	 l-p:0.1246434673666954
epoch£º853	 i:2 	 global-step:17062	 l-p:0.12569791078567505
epoch£º853	 i:3 	 global-step:17063	 l-p:0.1525023877620697
epoch£º853	 i:4 	 global-step:17064	 l-p:0.16917258501052856
epoch£º853	 i:5 	 global-step:17065	 l-p:0.1387668251991272
epoch£º853	 i:6 	 global-step:17066	 l-p:0.176969975233078
epoch£º853	 i:7 	 global-step:17067	 l-p:0.0845714583992958
epoch£º853	 i:8 	 global-step:17068	 l-p:0.15835636854171753
epoch£º853	 i:9 	 global-step:17069	 l-p:0.11276460438966751
====================================================================================================
====================================================================================================
====================================================================================================

epoch:854
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1205, 4.9118, 4.6250],
        [5.1205, 5.1204, 5.1205],
        [5.1205, 5.4421, 5.3228],
        [5.1205, 5.1205, 5.1205]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:854, step:0 
model_pd.l_p.mean(): 0.13774214684963226 
model_pd.l_d.mean(): -20.094539642333984 
model_pd.lagr.mean(): -19.956796646118164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5334], device='cuda:0')), ('power', tensor([-20.8591], device='cuda:0'))])
epoch£º854	 i:0 	 global-step:17080	 l-p:0.13774214684963226
epoch£º854	 i:1 	 global-step:17081	 l-p:0.24734097719192505
epoch£º854	 i:2 	 global-step:17082	 l-p:0.11098749190568924
epoch£º854	 i:3 	 global-step:17083	 l-p:0.17197135090827942
epoch£º854	 i:4 	 global-step:17084	 l-p:0.12644074857234955
epoch£º854	 i:5 	 global-step:17085	 l-p:0.09642011672258377
epoch£º854	 i:6 	 global-step:17086	 l-p:0.11962224543094635
epoch£º854	 i:7 	 global-step:17087	 l-p:0.14264430105686188
epoch£º854	 i:8 	 global-step:17088	 l-p:0.12807713449001312
epoch£º854	 i:9 	 global-step:17089	 l-p:0.13501404225826263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:855
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1184, 5.0128, 4.7016],
        [5.1184, 5.2353, 4.9986],
        [5.1184, 5.1127, 5.1181],
        [5.1184, 4.8785, 4.8299]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:855, step:0 
model_pd.l_p.mean(): 0.15948984026908875 
model_pd.l_d.mean(): -20.647497177124023 
model_pd.lagr.mean(): -20.488006591796875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4500], device='cuda:0')), ('power', tensor([-21.3329], device='cuda:0'))])
epoch£º855	 i:0 	 global-step:17100	 l-p:0.15948984026908875
epoch£º855	 i:1 	 global-step:17101	 l-p:0.13291293382644653
epoch£º855	 i:2 	 global-step:17102	 l-p:0.1203407272696495
epoch£º855	 i:3 	 global-step:17103	 l-p:0.1175217404961586
epoch£º855	 i:4 	 global-step:17104	 l-p:0.16739091277122498
epoch£º855	 i:5 	 global-step:17105	 l-p:0.16145487129688263
epoch£º855	 i:6 	 global-step:17106	 l-p:0.14251428842544556
epoch£º855	 i:7 	 global-step:17107	 l-p:0.2712922990322113
epoch£º855	 i:8 	 global-step:17108	 l-p:0.15731602907180786
epoch£º855	 i:9 	 global-step:17109	 l-p:0.12593939900398254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:856
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0939, 4.8898, 4.9020],
        [5.0939, 5.0875, 5.0935],
        [5.0939, 4.8857, 4.5938],
        [5.0939, 5.0939, 5.0939]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:856, step:0 
model_pd.l_p.mean(): 0.18742896616458893 
model_pd.l_d.mean(): -19.83010482788086 
model_pd.lagr.mean(): -19.642675399780273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5278], device='cuda:0')), ('power', tensor([-20.5860], device='cuda:0'))])
epoch£º856	 i:0 	 global-step:17120	 l-p:0.18742896616458893
epoch£º856	 i:1 	 global-step:17121	 l-p:0.13421456515789032
epoch£º856	 i:2 	 global-step:17122	 l-p:0.16805702447891235
epoch£º856	 i:3 	 global-step:17123	 l-p:0.11580497771501541
epoch£º856	 i:4 	 global-step:17124	 l-p:0.2806546986103058
epoch£º856	 i:5 	 global-step:17125	 l-p:0.2043834626674652
epoch£º856	 i:6 	 global-step:17126	 l-p:0.15201100707054138
epoch£º856	 i:7 	 global-step:17127	 l-p:0.11946450918912888
epoch£º856	 i:8 	 global-step:17128	 l-p:0.11353939771652222
epoch£º856	 i:9 	 global-step:17129	 l-p:0.10980834811925888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:857
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1085, 5.0003, 4.6880],
        [5.1085, 4.9004, 4.9069],
        [5.1085, 5.0893, 5.1061],
        [5.1085, 5.0375, 4.7301]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:857, step:0 
model_pd.l_p.mean(): 0.11207914352416992 
model_pd.l_d.mean(): -19.29145050048828 
model_pd.lagr.mean(): -19.179370880126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6026], device='cuda:0')), ('power', tensor([-20.1179], device='cuda:0'))])
epoch£º857	 i:0 	 global-step:17140	 l-p:0.11207914352416992
epoch£º857	 i:1 	 global-step:17141	 l-p:0.15845270454883575
epoch£º857	 i:2 	 global-step:17142	 l-p:0.17673145234584808
epoch£º857	 i:3 	 global-step:17143	 l-p:0.25483110547065735
epoch£º857	 i:4 	 global-step:17144	 l-p:0.09834591299295425
epoch£º857	 i:5 	 global-step:17145	 l-p:0.11960446834564209
epoch£º857	 i:6 	 global-step:17146	 l-p:0.12659668922424316
epoch£º857	 i:7 	 global-step:17147	 l-p:0.16438201069831848
epoch£º857	 i:8 	 global-step:17148	 l-p:0.13025861978530884
epoch£º857	 i:9 	 global-step:17149	 l-p:0.10596450418233871
====================================================================================================
====================================================================================================
====================================================================================================

epoch:858
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1386, 5.1382, 5.1386],
        [5.1386, 4.9708, 5.0109],
        [5.1386, 5.1386, 5.1386],
        [5.1386, 5.1310, 5.1381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:858, step:0 
model_pd.l_p.mean(): 0.1731037050485611 
model_pd.l_d.mean(): -20.66222381591797 
model_pd.lagr.mean(): -20.489120483398438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4234], device='cuda:0')), ('power', tensor([-21.3205], device='cuda:0'))])
epoch£º858	 i:0 	 global-step:17160	 l-p:0.1731037050485611
epoch£º858	 i:1 	 global-step:17161	 l-p:0.1492246389389038
epoch£º858	 i:2 	 global-step:17162	 l-p:0.129232257604599
epoch£º858	 i:3 	 global-step:17163	 l-p:0.13934940099716187
epoch£º858	 i:4 	 global-step:17164	 l-p:0.07051178067922592
epoch£º858	 i:5 	 global-step:17165	 l-p:0.15701870620250702
epoch£º858	 i:6 	 global-step:17166	 l-p:0.12101469933986664
epoch£º858	 i:7 	 global-step:17167	 l-p:0.10679826140403748
epoch£º858	 i:8 	 global-step:17168	 l-p:0.16851846873760223
epoch£º858	 i:9 	 global-step:17169	 l-p:0.10283059626817703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:859
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1535, 4.8956, 4.7082],
        [5.1535, 5.1534, 5.1535],
        [5.1535, 5.5598, 5.4946],
        [5.1535, 5.1026, 5.1401]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:859, step:0 
model_pd.l_p.mean(): 0.12029790878295898 
model_pd.l_d.mean(): -20.554492950439453 
model_pd.lagr.mean(): -20.434194564819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4335], device='cuda:0')), ('power', tensor([-21.2219], device='cuda:0'))])
epoch£º859	 i:0 	 global-step:17180	 l-p:0.12029790878295898
epoch£º859	 i:1 	 global-step:17181	 l-p:0.0989292711019516
epoch£º859	 i:2 	 global-step:17182	 l-p:0.14696185290813446
epoch£º859	 i:3 	 global-step:17183	 l-p:0.11397884786128998
epoch£º859	 i:4 	 global-step:17184	 l-p:0.10205215215682983
epoch£º859	 i:5 	 global-step:17185	 l-p:0.15540847182273865
epoch£º859	 i:6 	 global-step:17186	 l-p:0.1384742707014084
epoch£º859	 i:7 	 global-step:17187	 l-p:0.20695731043815613
epoch£º859	 i:8 	 global-step:17188	 l-p:0.14301101863384247
epoch£º859	 i:9 	 global-step:17189	 l-p:0.15954354405403137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:860
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1162, 5.0695, 5.1048],
        [5.1162, 5.1079, 5.1156],
        [5.1162, 4.9594, 5.0060],
        [5.1162, 5.1013, 5.1146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:860, step:0 
model_pd.l_p.mean(): 0.1106356605887413 
model_pd.l_d.mean(): -20.505847930908203 
model_pd.lagr.mean(): -20.395212173461914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4583], device='cuda:0')), ('power', tensor([-21.1981], device='cuda:0'))])
epoch£º860	 i:0 	 global-step:17200	 l-p:0.1106356605887413
epoch£º860	 i:1 	 global-step:17201	 l-p:0.15574999153614044
epoch£º860	 i:2 	 global-step:17202	 l-p:0.1498471200466156
epoch£º860	 i:3 	 global-step:17203	 l-p:0.21704155206680298
epoch£º860	 i:4 	 global-step:17204	 l-p:0.14687855541706085
epoch£º860	 i:5 	 global-step:17205	 l-p:0.09871268272399902
epoch£º860	 i:6 	 global-step:17206	 l-p:0.13056659698486328
epoch£º860	 i:7 	 global-step:17207	 l-p:0.13496075570583344
epoch£º860	 i:8 	 global-step:17208	 l-p:0.17720988392829895
epoch£º860	 i:9 	 global-step:17209	 l-p:0.16371123492717743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:861
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1089, 4.8499, 4.7456],
        [5.1089, 5.2173, 4.9761],
        [5.1089, 5.0192, 5.0717],
        [5.1089, 5.0965, 5.1077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:861, step:0 
model_pd.l_p.mean(): 0.17390458285808563 
model_pd.l_d.mean(): -20.639644622802734 
model_pd.lagr.mean(): -20.465740203857422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4502], device='cuda:0')), ('power', tensor([-21.3251], device='cuda:0'))])
epoch£º861	 i:0 	 global-step:17220	 l-p:0.17390458285808563
epoch£º861	 i:1 	 global-step:17221	 l-p:0.1329602748155594
epoch£º861	 i:2 	 global-step:17222	 l-p:0.12296518683433533
epoch£º861	 i:3 	 global-step:17223	 l-p:0.1754913330078125
epoch£º861	 i:4 	 global-step:17224	 l-p:0.13561978936195374
epoch£º861	 i:5 	 global-step:17225	 l-p:0.15516018867492676
epoch£º861	 i:6 	 global-step:17226	 l-p:0.12363561242818832
epoch£º861	 i:7 	 global-step:17227	 l-p:0.11335939913988113
epoch£º861	 i:8 	 global-step:17228	 l-p:0.127214252948761
epoch£º861	 i:9 	 global-step:17229	 l-p:0.1882844716310501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:862
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1206, 4.8780, 4.6269],
        [5.1206, 4.9218, 4.9385],
        [5.1206, 5.1126, 5.1200],
        [5.1206, 5.1206, 5.1206]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:862, step:0 
model_pd.l_p.mean(): 0.13166959583759308 
model_pd.l_d.mean(): -20.412172317504883 
model_pd.lagr.mean(): -20.280502319335938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4537], device='cuda:0')), ('power', tensor([-21.0987], device='cuda:0'))])
epoch£º862	 i:0 	 global-step:17240	 l-p:0.13166959583759308
epoch£º862	 i:1 	 global-step:17241	 l-p:0.13903377950191498
epoch£º862	 i:2 	 global-step:17242	 l-p:0.1296701431274414
epoch£º862	 i:3 	 global-step:17243	 l-p:0.11522648483514786
epoch£º862	 i:4 	 global-step:17244	 l-p:0.18683592975139618
epoch£º862	 i:5 	 global-step:17245	 l-p:0.19746312499046326
epoch£º862	 i:6 	 global-step:17246	 l-p:0.1402396410703659
epoch£º862	 i:7 	 global-step:17247	 l-p:0.14011341333389282
epoch£º862	 i:8 	 global-step:17248	 l-p:0.16735972464084625
epoch£º862	 i:9 	 global-step:17249	 l-p:0.1261352002620697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:863
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1173, 5.0772, 5.1086],
        [5.1173, 5.0964, 5.1145],
        [5.1173, 5.0347, 5.0853],
        [5.1173, 4.8793, 4.6196]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:863, step:0 
model_pd.l_p.mean(): 0.16294004023075104 
model_pd.l_d.mean(): -19.58490562438965 
model_pd.lagr.mean(): -19.421964645385742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5035], device='cuda:0')), ('power', tensor([-20.3133], device='cuda:0'))])
epoch£º863	 i:0 	 global-step:17260	 l-p:0.16294004023075104
epoch£º863	 i:1 	 global-step:17261	 l-p:0.1722145676612854
epoch£º863	 i:2 	 global-step:17262	 l-p:0.1310824155807495
epoch£º863	 i:3 	 global-step:17263	 l-p:0.12133060395717621
epoch£º863	 i:4 	 global-step:17264	 l-p:0.09280938655138016
epoch£º863	 i:5 	 global-step:17265	 l-p:0.12272509932518005
epoch£º863	 i:6 	 global-step:17266	 l-p:0.11546068638563156
epoch£º863	 i:7 	 global-step:17267	 l-p:0.2034306526184082
epoch£º863	 i:8 	 global-step:17268	 l-p:0.1556260883808136
epoch£º863	 i:9 	 global-step:17269	 l-p:0.1166999414563179
====================================================================================================
====================================================================================================
====================================================================================================

epoch:864
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1353, 5.0332, 4.7222],
        [5.1353, 5.1353, 5.1353],
        [5.1353, 5.1353, 5.1353],
        [5.1353, 5.5285, 5.4547]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:864, step:0 
model_pd.l_p.mean(): 0.1664159744977951 
model_pd.l_d.mean(): -19.50340461730957 
model_pd.lagr.mean(): -19.33698844909668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4862], device='cuda:0')), ('power', tensor([-20.2132], device='cuda:0'))])
epoch£º864	 i:0 	 global-step:17280	 l-p:0.1664159744977951
epoch£º864	 i:1 	 global-step:17281	 l-p:0.12387601286172867
epoch£º864	 i:2 	 global-step:17282	 l-p:0.10905344784259796
epoch£º864	 i:3 	 global-step:17283	 l-p:0.18383818864822388
epoch£º864	 i:4 	 global-step:17284	 l-p:0.1382758468389511
epoch£º864	 i:5 	 global-step:17285	 l-p:0.0668148472905159
epoch£º864	 i:6 	 global-step:17286	 l-p:0.12590323388576508
epoch£º864	 i:7 	 global-step:17287	 l-p:0.12390908598899841
epoch£º864	 i:8 	 global-step:17288	 l-p:0.14341825246810913
epoch£º864	 i:9 	 global-step:17289	 l-p:0.1448948085308075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:865
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1507, 5.1507, 5.1507],
        [5.1507, 5.1497, 5.1507],
        [5.1507, 5.1507, 5.1507],
        [5.1507, 5.1507, 5.1507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:865, step:0 
model_pd.l_p.mean(): 0.1279047280550003 
model_pd.l_d.mean(): -20.388723373413086 
model_pd.lagr.mean(): -20.260818481445312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4391], device='cuda:0')), ('power', tensor([-21.0601], device='cuda:0'))])
epoch£º865	 i:0 	 global-step:17300	 l-p:0.1279047280550003
epoch£º865	 i:1 	 global-step:17301	 l-p:0.18742884695529938
epoch£º865	 i:2 	 global-step:17302	 l-p:0.15379010140895844
epoch£º865	 i:3 	 global-step:17303	 l-p:0.07744012773036957
epoch£º865	 i:4 	 global-step:17304	 l-p:0.17431098222732544
epoch£º865	 i:5 	 global-step:17305	 l-p:0.09332412481307983
epoch£º865	 i:6 	 global-step:17306	 l-p:0.11288990080356598
epoch£º865	 i:7 	 global-step:17307	 l-p:0.09611236304044724
epoch£º865	 i:8 	 global-step:17308	 l-p:0.13090099394321442
epoch£º865	 i:9 	 global-step:17309	 l-p:0.14174321293830872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:866
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1630, 5.3256, 5.1112],
        [5.1630, 5.6373, 5.6180],
        [5.1630, 5.1013, 5.1441],
        [5.1630, 5.1602, 5.1629]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:866, step:0 
model_pd.l_p.mean(): 0.08017908781766891 
model_pd.l_d.mean(): -19.668787002563477 
model_pd.lagr.mean(): -19.588607788085938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5013], device='cuda:0')), ('power', tensor([-20.3958], device='cuda:0'))])
epoch£º866	 i:0 	 global-step:17320	 l-p:0.08017908781766891
epoch£º866	 i:1 	 global-step:17321	 l-p:0.16515806317329407
epoch£º866	 i:2 	 global-step:17322	 l-p:0.1234804093837738
epoch£º866	 i:3 	 global-step:17323	 l-p:0.14364652335643768
epoch£º866	 i:4 	 global-step:17324	 l-p:0.11097201704978943
epoch£º866	 i:5 	 global-step:17325	 l-p:0.16540370881557465
epoch£º866	 i:6 	 global-step:17326	 l-p:0.11059974133968353
epoch£º866	 i:7 	 global-step:17327	 l-p:0.1241687536239624
epoch£º866	 i:8 	 global-step:17328	 l-p:0.12169616669416428
epoch£º866	 i:9 	 global-step:17329	 l-p:0.11375224590301514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:867
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1622, 5.1622, 5.1622],
        [5.1622, 4.9880, 5.0237],
        [5.1622, 5.2698, 5.0272],
        [5.1622, 5.1587, 5.1620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:867, step:0 
model_pd.l_p.mean(): 0.13389424979686737 
model_pd.l_d.mean(): -19.490602493286133 
model_pd.lagr.mean(): -19.356708526611328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5311], device='cuda:0')), ('power', tensor([-20.2462], device='cuda:0'))])
epoch£º867	 i:0 	 global-step:17340	 l-p:0.13389424979686737
epoch£º867	 i:1 	 global-step:17341	 l-p:0.10996787995100021
epoch£º867	 i:2 	 global-step:17342	 l-p:0.21512489020824432
epoch£º867	 i:3 	 global-step:17343	 l-p:0.12747599184513092
epoch£º867	 i:4 	 global-step:17344	 l-p:0.1103474423289299
epoch£º867	 i:5 	 global-step:17345	 l-p:0.11048562079668045
epoch£º867	 i:6 	 global-step:17346	 l-p:0.12270733714103699
epoch£º867	 i:7 	 global-step:17347	 l-p:0.16290752589702606
epoch£º867	 i:8 	 global-step:17348	 l-p:0.10064321011304855
epoch£º867	 i:9 	 global-step:17349	 l-p:0.14548200368881226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:868
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7108,  0.6343,  1.0000,  0.5661,
          1.0000,  0.8924, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2047,  0.1207,  1.0000,  0.0711,
          1.0000,  0.5894, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228]], device='cuda:0')
 pt:tensor([[5.1147, 4.8493, 4.6917],
        [5.1147, 5.1676, 4.9001],
        [5.1147, 4.9058, 4.9125],
        [5.1147, 4.8530, 4.6495]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:868, step:0 
model_pd.l_p.mean(): 0.17390823364257812 
model_pd.l_d.mean(): -20.6077880859375 
model_pd.lagr.mean(): -20.433879852294922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4643], device='cuda:0')), ('power', tensor([-21.3074], device='cuda:0'))])
epoch£º868	 i:0 	 global-step:17360	 l-p:0.17390823364257812
epoch£º868	 i:1 	 global-step:17361	 l-p:0.11964526027441025
epoch£º868	 i:2 	 global-step:17362	 l-p:0.21777842938899994
epoch£º868	 i:3 	 global-step:17363	 l-p:0.16609689593315125
epoch£º868	 i:4 	 global-step:17364	 l-p:0.21025146543979645
epoch£º868	 i:5 	 global-step:17365	 l-p:0.11092796176671982
epoch£º868	 i:6 	 global-step:17366	 l-p:0.10829998552799225
epoch£º868	 i:7 	 global-step:17367	 l-p:0.1815056949853897
epoch£º868	 i:8 	 global-step:17368	 l-p:0.13094086945056915
epoch£º868	 i:9 	 global-step:17369	 l-p:0.09957689791917801
====================================================================================================
====================================================================================================
====================================================================================================

epoch:869
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0916, 5.4736, 5.3931],
        [5.0916, 5.0879, 5.0915],
        [5.0916, 4.8987, 4.5954],
        [5.0916, 4.9351, 4.9832]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:869, step:0 
model_pd.l_p.mean(): 0.13838158547878265 
model_pd.l_d.mean(): -19.542795181274414 
model_pd.lagr.mean(): -19.4044132232666 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5751], device='cuda:0')), ('power', tensor([-20.3439], device='cuda:0'))])
epoch£º869	 i:0 	 global-step:17380	 l-p:0.13838158547878265
epoch£º869	 i:1 	 global-step:17381	 l-p:0.09418042749166489
epoch£º869	 i:2 	 global-step:17382	 l-p:0.08896030485630035
epoch£º869	 i:3 	 global-step:17383	 l-p:0.1857849806547165
epoch£º869	 i:4 	 global-step:17384	 l-p:0.1270945966243744
epoch£º869	 i:5 	 global-step:17385	 l-p:0.32690200209617615
epoch£º869	 i:6 	 global-step:17386	 l-p:0.2087273746728897
epoch£º869	 i:7 	 global-step:17387	 l-p:-0.7649763822555542
epoch£º869	 i:8 	 global-step:17388	 l-p:0.1380232572555542
epoch£º869	 i:9 	 global-step:17389	 l-p:0.16813191771507263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:870
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2005,  0.1173,  1.0000,  0.0687,
          1.0000,  0.5853, 31.6228]], device='cuda:0')
 pt:tensor([[5.0703, 4.8583, 4.8653],
        [5.0703, 4.8006, 4.6596],
        [5.0703, 4.8126, 4.7320],
        [5.0703, 4.8626, 4.8746]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:870, step:0 
model_pd.l_p.mean(): 0.129263237118721 
model_pd.l_d.mean(): -20.99688148498535 
model_pd.lagr.mean(): -20.867618560791016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4075], device='cuda:0')), ('power', tensor([-21.6426], device='cuda:0'))])
epoch£º870	 i:0 	 global-step:17400	 l-p:0.129263237118721
epoch£º870	 i:1 	 global-step:17401	 l-p:0.265261173248291
epoch£º870	 i:2 	 global-step:17402	 l-p:0.10949598252773285
epoch£º870	 i:3 	 global-step:17403	 l-p:-1.79770827293396
epoch£º870	 i:4 	 global-step:17404	 l-p:0.12434045225381851
epoch£º870	 i:5 	 global-step:17405	 l-p:0.18770265579223633
epoch£º870	 i:6 	 global-step:17406	 l-p:0.15179511904716492
epoch£º870	 i:7 	 global-step:17407	 l-p:0.20089063048362732
epoch£º870	 i:8 	 global-step:17408	 l-p:0.12164078652858734
epoch£º870	 i:9 	 global-step:17409	 l-p:0.21619147062301636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:871
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0797, 4.8171, 4.7155],
        [5.0797, 5.0619, 5.0775],
        [5.0797, 4.9312, 4.6152],
        [5.0797, 4.8444, 4.8160]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:871, step:0 
model_pd.l_p.mean(): 0.18222366273403168 
model_pd.l_d.mean(): -20.26789093017578 
model_pd.lagr.mean(): -20.08566665649414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5189], device='cuda:0')), ('power', tensor([-21.0195], device='cuda:0'))])
epoch£º871	 i:0 	 global-step:17420	 l-p:0.18222366273403168
epoch£º871	 i:1 	 global-step:17421	 l-p:0.171981081366539
epoch£º871	 i:2 	 global-step:17422	 l-p:0.16726398468017578
epoch£º871	 i:3 	 global-step:17423	 l-p:0.13499175012111664
epoch£º871	 i:4 	 global-step:17424	 l-p:0.23572884500026703
epoch£º871	 i:5 	 global-step:17425	 l-p:0.12733738124370575
epoch£º871	 i:6 	 global-step:17426	 l-p:0.12576411664485931
epoch£º871	 i:7 	 global-step:17427	 l-p:0.17817182838916779
epoch£º871	 i:8 	 global-step:17428	 l-p:0.10654870420694351
epoch£º871	 i:9 	 global-step:17429	 l-p:0.14522051811218262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:872
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1192, 5.0771, 5.1097],
        [5.1192, 5.0473, 4.7384],
        [5.1192, 5.1192, 5.1192],
        [5.1192, 5.0868, 5.1132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:872, step:0 
model_pd.l_p.mean(): 0.2127954661846161 
model_pd.l_d.mean(): -20.4433536529541 
model_pd.lagr.mean(): -20.230558395385742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4819], device='cuda:0')), ('power', tensor([-21.1591], device='cuda:0'))])
epoch£º872	 i:0 	 global-step:17440	 l-p:0.2127954661846161
epoch£º872	 i:1 	 global-step:17441	 l-p:0.13171832263469696
epoch£º872	 i:2 	 global-step:17442	 l-p:0.11600717902183533
epoch£º872	 i:3 	 global-step:17443	 l-p:0.10460836440324783
epoch£º872	 i:4 	 global-step:17444	 l-p:0.14463134109973907
epoch£º872	 i:5 	 global-step:17445	 l-p:0.13401082158088684
epoch£º872	 i:6 	 global-step:17446	 l-p:0.1717054843902588
epoch£º872	 i:7 	 global-step:17447	 l-p:0.08904188126325607
epoch£º872	 i:8 	 global-step:17448	 l-p:0.17118598520755768
epoch£º872	 i:9 	 global-step:17449	 l-p:0.12314753979444504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:873
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1396, 5.0109, 5.0653],
        [5.1396, 5.1319, 5.1391],
        [5.1396, 4.8841, 4.7904],
        [5.1396, 4.9079, 4.8781]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:873, step:0 
model_pd.l_p.mean(): 0.10994438081979752 
model_pd.l_d.mean(): -19.426469802856445 
model_pd.lagr.mean(): -19.316524505615234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5357], device='cuda:0')), ('power', tensor([-20.1860], device='cuda:0'))])
epoch£º873	 i:0 	 global-step:17460	 l-p:0.10994438081979752
epoch£º873	 i:1 	 global-step:17461	 l-p:0.1422208845615387
epoch£º873	 i:2 	 global-step:17462	 l-p:0.1648307740688324
epoch£º873	 i:3 	 global-step:17463	 l-p:0.09707614779472351
epoch£º873	 i:4 	 global-step:17464	 l-p:0.14435382187366486
epoch£º873	 i:5 	 global-step:17465	 l-p:0.17002330720424652
epoch£º873	 i:6 	 global-step:17466	 l-p:0.15632757544517517
epoch£º873	 i:7 	 global-step:17467	 l-p:0.16743072867393494
epoch£º873	 i:8 	 global-step:17468	 l-p:0.15025608241558075
epoch£º873	 i:9 	 global-step:17469	 l-p:0.11983852088451385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:874
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1159, 5.1159, 5.1159],
        [5.1159, 5.1156, 5.1159],
        [5.1159, 5.1159, 5.1159],
        [5.1159, 5.2849, 5.0740]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:874, step:0 
model_pd.l_p.mean(): 0.11861880868673325 
model_pd.l_d.mean(): -20.686582565307617 
model_pd.lagr.mean(): -20.567964553833008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4237], device='cuda:0')), ('power', tensor([-21.3454], device='cuda:0'))])
epoch£º874	 i:0 	 global-step:17480	 l-p:0.11861880868673325
epoch£º874	 i:1 	 global-step:17481	 l-p:0.149002343416214
epoch£º874	 i:2 	 global-step:17482	 l-p:0.13746389746665955
epoch£º874	 i:3 	 global-step:17483	 l-p:0.16547560691833496
epoch£º874	 i:4 	 global-step:17484	 l-p:0.1739511787891388
epoch£º874	 i:5 	 global-step:17485	 l-p:0.12472321838140488
epoch£º874	 i:6 	 global-step:17486	 l-p:0.19517642259597778
epoch£º874	 i:7 	 global-step:17487	 l-p:0.1380283087491989
epoch£º874	 i:8 	 global-step:17488	 l-p:0.11297734081745148
epoch£º874	 i:9 	 global-step:17489	 l-p:0.13554547727108002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:875
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1261, 5.0820, 5.1159],
        [5.1261, 4.9635, 5.0080],
        [5.1261, 5.1261, 5.1261],
        [5.1261, 4.9350, 4.6340]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:875, step:0 
model_pd.l_p.mean(): 0.10090038180351257 
model_pd.l_d.mean(): -20.697429656982422 
model_pd.lagr.mean(): -20.596529006958008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4157], device='cuda:0')), ('power', tensor([-21.3483], device='cuda:0'))])
epoch£º875	 i:0 	 global-step:17500	 l-p:0.10090038180351257
epoch£º875	 i:1 	 global-step:17501	 l-p:0.15580317378044128
epoch£º875	 i:2 	 global-step:17502	 l-p:0.19700400531291962
epoch£º875	 i:3 	 global-step:17503	 l-p:0.19589100778102875
epoch£º875	 i:4 	 global-step:17504	 l-p:0.11338810622692108
epoch£º875	 i:5 	 global-step:17505	 l-p:0.11126060038805008
epoch£º875	 i:6 	 global-step:17506	 l-p:0.1375366598367691
epoch£º875	 i:7 	 global-step:17507	 l-p:0.11006952822208405
epoch£º875	 i:8 	 global-step:17508	 l-p:0.08932790905237198
epoch£º875	 i:9 	 global-step:17509	 l-p:0.1993437111377716
====================================================================================================
====================================================================================================
====================================================================================================

epoch:876
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1341, 4.9805, 4.6692],
        [5.1341, 5.5890, 5.5562],
        [5.1341, 5.5426, 5.4781],
        [5.1341, 5.1340, 5.1341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:876, step:0 
model_pd.l_p.mean(): 0.19836272299289703 
model_pd.l_d.mean(): -19.497695922851562 
model_pd.lagr.mean(): -19.299333572387695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5351], device='cuda:0')), ('power', tensor([-20.2575], device='cuda:0'))])
epoch£º876	 i:0 	 global-step:17520	 l-p:0.19836272299289703
epoch£º876	 i:1 	 global-step:17521	 l-p:0.10245838761329651
epoch£º876	 i:2 	 global-step:17522	 l-p:0.10754769295454025
epoch£º876	 i:3 	 global-step:17523	 l-p:0.13146567344665527
epoch£º876	 i:4 	 global-step:17524	 l-p:0.1232854425907135
epoch£º876	 i:5 	 global-step:17525	 l-p:0.10710195451974869
epoch£º876	 i:6 	 global-step:17526	 l-p:0.13140811026096344
epoch£º876	 i:7 	 global-step:17527	 l-p:0.15933384001255035
epoch£º876	 i:8 	 global-step:17528	 l-p:0.12394855916500092
epoch£º876	 i:9 	 global-step:17529	 l-p:0.17407546937465668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:877
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1332, 5.1317, 5.1331],
        [5.1332, 5.1332, 5.1332],
        [5.1332, 4.8876, 4.6384],
        [5.1332, 5.5874, 5.5539]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:877, step:0 
model_pd.l_p.mean(): 0.11046954244375229 
model_pd.l_d.mean(): -19.184993743896484 
model_pd.lagr.mean(): -19.07452392578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5113], device='cuda:0')), ('power', tensor([-19.9170], device='cuda:0'))])
epoch£º877	 i:0 	 global-step:17540	 l-p:0.11046954244375229
epoch£º877	 i:1 	 global-step:17541	 l-p:0.1488366723060608
epoch£º877	 i:2 	 global-step:17542	 l-p:0.15469218790531158
epoch£º877	 i:3 	 global-step:17543	 l-p:0.11304603517055511
epoch£º877	 i:4 	 global-step:17544	 l-p:0.11658567190170288
epoch£º877	 i:5 	 global-step:17545	 l-p:0.14106236398220062
epoch£º877	 i:6 	 global-step:17546	 l-p:0.164458230137825
epoch£º877	 i:7 	 global-step:17547	 l-p:0.13060221076011658
epoch£º877	 i:8 	 global-step:17548	 l-p:0.18003718554973602
epoch£º877	 i:9 	 global-step:17549	 l-p:0.15375889837741852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:878
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1300, 5.1300, 5.1300],
        [5.1300, 5.4498, 5.3271],
        [5.1300, 5.4573, 5.3395],
        [5.1300, 5.1300, 5.1300]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:878, step:0 
model_pd.l_p.mean(): 0.1725599467754364 
model_pd.l_d.mean(): -20.66476058959961 
model_pd.lagr.mean(): -20.49220085144043 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4292], device='cuda:0')), ('power', tensor([-21.3291], device='cuda:0'))])
epoch£º878	 i:0 	 global-step:17560	 l-p:0.1725599467754364
epoch£º878	 i:1 	 global-step:17561	 l-p:0.202617347240448
epoch£º878	 i:2 	 global-step:17562	 l-p:0.09791479259729385
epoch£º878	 i:3 	 global-step:17563	 l-p:0.14469242095947266
epoch£º878	 i:4 	 global-step:17564	 l-p:0.11924958229064941
epoch£º878	 i:5 	 global-step:17565	 l-p:0.13499704003334045
epoch£º878	 i:6 	 global-step:17566	 l-p:0.13200277090072632
epoch£º878	 i:7 	 global-step:17567	 l-p:0.10185961425304413
epoch£º878	 i:8 	 global-step:17568	 l-p:0.1400272399187088
epoch£º878	 i:9 	 global-step:17569	 l-p:0.12361400574445724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:879
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1386, 5.1384, 5.1386],
        [5.1386, 5.0530, 5.1046],
        [5.1386, 4.9274, 4.6384],
        [5.1386, 5.1057, 5.1325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:879, step:0 
model_pd.l_p.mean(): 0.14819368720054626 
model_pd.l_d.mean(): -20.59751319885254 
model_pd.lagr.mean(): -20.44931983947754 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4389], device='cuda:0')), ('power', tensor([-21.2710], device='cuda:0'))])
epoch£º879	 i:0 	 global-step:17580	 l-p:0.14819368720054626
epoch£º879	 i:1 	 global-step:17581	 l-p:0.10066987574100494
epoch£º879	 i:2 	 global-step:17582	 l-p:0.13071613013744354
epoch£º879	 i:3 	 global-step:17583	 l-p:0.1970570683479309
epoch£º879	 i:4 	 global-step:17584	 l-p:0.064193494617939
epoch£º879	 i:5 	 global-step:17585	 l-p:0.14007391035556793
epoch£º879	 i:6 	 global-step:17586	 l-p:0.11104047298431396
epoch£º879	 i:7 	 global-step:17587	 l-p:0.13296137750148773
epoch£º879	 i:8 	 global-step:17588	 l-p:0.21326838433742523
epoch£º879	 i:9 	 global-step:17589	 l-p:0.13941434025764465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:880
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1286, 4.8642, 4.6717],
        [5.1286, 5.0364, 5.0896],
        [5.1286, 4.8800, 4.8161],
        [5.1286, 5.1284, 5.1286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:880, step:0 
model_pd.l_p.mean(): 0.16644510626792908 
model_pd.l_d.mean(): -20.18071174621582 
model_pd.lagr.mean(): -20.014266967773438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5247], device='cuda:0')), ('power', tensor([-20.9373], device='cuda:0'))])
epoch£º880	 i:0 	 global-step:17600	 l-p:0.16644510626792908
epoch£º880	 i:1 	 global-step:17601	 l-p:0.12481144815683365
epoch£º880	 i:2 	 global-step:17602	 l-p:0.15031182765960693
epoch£º880	 i:3 	 global-step:17603	 l-p:0.11682014167308807
epoch£º880	 i:4 	 global-step:17604	 l-p:0.1741851419210434
epoch£º880	 i:5 	 global-step:17605	 l-p:0.21007266640663147
epoch£º880	 i:6 	 global-step:17606	 l-p:0.13274753093719482
epoch£º880	 i:7 	 global-step:17607	 l-p:0.13570532202720642
epoch£º880	 i:8 	 global-step:17608	 l-p:0.10070416331291199
epoch£º880	 i:9 	 global-step:17609	 l-p:0.14385677874088287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:881
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1171, 5.1171, 5.1171],
        [5.1171, 5.0090, 5.0646],
        [5.1171, 4.8675, 4.8038],
        [5.1171, 5.4131, 5.2756]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:881, step:0 
model_pd.l_p.mean(): 0.1691843420267105 
model_pd.l_d.mean(): -19.33864402770996 
model_pd.lagr.mean(): -19.16946029663086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5057], device='cuda:0')), ('power', tensor([-20.0666], device='cuda:0'))])
epoch£º881	 i:0 	 global-step:17620	 l-p:0.1691843420267105
epoch£º881	 i:1 	 global-step:17621	 l-p:0.2404135912656784
epoch£º881	 i:2 	 global-step:17622	 l-p:0.13801029324531555
epoch£º881	 i:3 	 global-step:17623	 l-p:0.12639130651950836
epoch£º881	 i:4 	 global-step:17624	 l-p:0.11096607148647308
epoch£º881	 i:5 	 global-step:17625	 l-p:0.14781087636947632
epoch£º881	 i:6 	 global-step:17626	 l-p:0.12590758502483368
epoch£º881	 i:7 	 global-step:17627	 l-p:0.1297488808631897
epoch£º881	 i:8 	 global-step:17628	 l-p:0.1312427520751953
epoch£º881	 i:9 	 global-step:17629	 l-p:0.10251379013061523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:882
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1257, 4.8826, 4.8341],
        [5.1257, 4.8683, 4.6412],
        [5.1257, 5.1257, 5.1257],
        [5.1257, 5.1217, 5.1255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:882, step:0 
model_pd.l_p.mean(): 0.09122470766305923 
model_pd.l_d.mean(): -20.42561912536621 
model_pd.lagr.mean(): -20.334394454956055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4805], device='cuda:0')), ('power', tensor([-21.1397], device='cuda:0'))])
epoch£º882	 i:0 	 global-step:17640	 l-p:0.09122470766305923
epoch£º882	 i:1 	 global-step:17641	 l-p:0.1504179835319519
epoch£º882	 i:2 	 global-step:17642	 l-p:0.18446406722068787
epoch£º882	 i:3 	 global-step:17643	 l-p:0.0900435820221901
epoch£º882	 i:4 	 global-step:17644	 l-p:0.26668334007263184
epoch£º882	 i:5 	 global-step:17645	 l-p:0.15009097754955292
epoch£º882	 i:6 	 global-step:17646	 l-p:0.1471850574016571
epoch£º882	 i:7 	 global-step:17647	 l-p:0.14233723282814026
epoch£º882	 i:8 	 global-step:17648	 l-p:0.1206117495894432
epoch£º882	 i:9 	 global-step:17649	 l-p:0.1259373128414154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:883
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1035, 5.1008, 5.1034],
        [5.1035, 5.1030, 5.1035],
        [5.1035, 5.0907, 5.1023],
        [5.1035, 4.9393, 4.9845]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:883, step:0 
model_pd.l_p.mean(): 0.13047321140766144 
model_pd.l_d.mean(): -20.674072265625 
model_pd.lagr.mean(): -20.543598175048828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4473], device='cuda:0')), ('power', tensor([-21.3570], device='cuda:0'))])
epoch£º883	 i:0 	 global-step:17660	 l-p:0.13047321140766144
epoch£º883	 i:1 	 global-step:17661	 l-p:0.14440380036830902
epoch£º883	 i:2 	 global-step:17662	 l-p:0.1378059983253479
epoch£º883	 i:3 	 global-step:17663	 l-p:0.2802692949771881
epoch£º883	 i:4 	 global-step:17664	 l-p:0.15977483987808228
epoch£º883	 i:5 	 global-step:17665	 l-p:0.16302315890789032
epoch£º883	 i:6 	 global-step:17666	 l-p:0.19555336236953735
epoch£º883	 i:7 	 global-step:17667	 l-p:0.10386699438095093
epoch£º883	 i:8 	 global-step:17668	 l-p:0.1119050681591034
epoch£º883	 i:9 	 global-step:17669	 l-p:0.13825547695159912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:884
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1178, 5.1178, 5.1178],
        [5.1178, 4.8530, 4.6496],
        [5.1178, 5.1178, 5.1178],
        [5.1178, 4.9139, 4.6157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:884, step:0 
model_pd.l_p.mean(): 0.11074691265821457 
model_pd.l_d.mean(): -19.961151123046875 
model_pd.lagr.mean(): -19.850404739379883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4842], device='cuda:0')), ('power', tensor([-20.6740], device='cuda:0'))])
epoch£º884	 i:0 	 global-step:17680	 l-p:0.11074691265821457
epoch£º884	 i:1 	 global-step:17681	 l-p:0.09594330936670303
epoch£º884	 i:2 	 global-step:17682	 l-p:0.1567411571741104
epoch£º884	 i:3 	 global-step:17683	 l-p:0.12863241136074066
epoch£º884	 i:4 	 global-step:17684	 l-p:0.14865924417972565
epoch£º884	 i:5 	 global-step:17685	 l-p:0.08789751678705215
epoch£º884	 i:6 	 global-step:17686	 l-p:0.15400804579257965
epoch£º884	 i:7 	 global-step:17687	 l-p:0.1573888212442398
epoch£º884	 i:8 	 global-step:17688	 l-p:0.2097444087266922
epoch£º884	 i:9 	 global-step:17689	 l-p:0.13504542410373688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:885
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1482, 4.9825, 4.6730],
        [5.1482, 5.1482, 5.1482],
        [5.1482, 4.8952, 4.8152],
        [5.1482, 5.1692, 4.8871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:885, step:0 
model_pd.l_p.mean(): 0.13887673616409302 
model_pd.l_d.mean(): -20.379507064819336 
model_pd.lagr.mean(): -20.240631103515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4774], device='cuda:0')), ('power', tensor([-21.0900], device='cuda:0'))])
epoch£º885	 i:0 	 global-step:17700	 l-p:0.13887673616409302
epoch£º885	 i:1 	 global-step:17701	 l-p:0.19937606155872345
epoch£º885	 i:2 	 global-step:17702	 l-p:0.07975476235151291
epoch£º885	 i:3 	 global-step:17703	 l-p:0.13752619922161102
epoch£º885	 i:4 	 global-step:17704	 l-p:0.13667523860931396
epoch£º885	 i:5 	 global-step:17705	 l-p:0.12141818553209305
epoch£º885	 i:6 	 global-step:17706	 l-p:0.12078065425157547
epoch£º885	 i:7 	 global-step:17707	 l-p:0.1271171271800995
epoch£º885	 i:8 	 global-step:17708	 l-p:0.12407827377319336
epoch£º885	 i:9 	 global-step:17709	 l-p:0.14039938151836395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:886
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1416, 5.1399, 5.1416],
        [5.1416, 4.9854, 5.0331],
        [5.1416, 5.1339, 5.1411],
        [5.1416, 5.0477, 5.1013]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:886, step:0 
model_pd.l_p.mean(): 0.14833413064479828 
model_pd.l_d.mean(): -19.38357162475586 
model_pd.lagr.mean(): -19.23523712158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5721], device='cuda:0')), ('power', tensor([-20.1799], device='cuda:0'))])
epoch£º886	 i:0 	 global-step:17720	 l-p:0.14833413064479828
epoch£º886	 i:1 	 global-step:17721	 l-p:0.10113111138343811
epoch£º886	 i:2 	 global-step:17722	 l-p:0.181290864944458
epoch£º886	 i:3 	 global-step:17723	 l-p:0.14870302379131317
epoch£º886	 i:4 	 global-step:17724	 l-p:0.11796711385250092
epoch£º886	 i:5 	 global-step:17725	 l-p:0.2384774386882782
epoch£º886	 i:6 	 global-step:17726	 l-p:0.16201938688755035
epoch£º886	 i:7 	 global-step:17727	 l-p:0.09827472269535065
epoch£º886	 i:8 	 global-step:17728	 l-p:0.14434312283992767
epoch£º886	 i:9 	 global-step:17729	 l-p:0.08594156056642532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:887
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1153, 5.0926, 5.1121],
        [5.1153, 4.8487, 4.7212],
        [5.1153, 4.9977, 5.0538],
        [5.1153, 5.0213, 5.0751]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:887, step:0 
model_pd.l_p.mean(): 0.12104791402816772 
model_pd.l_d.mean(): -20.98870277404785 
model_pd.lagr.mean(): -20.86765480041504 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3900], device='cuda:0')), ('power', tensor([-21.6165], device='cuda:0'))])
epoch£º887	 i:0 	 global-step:17740	 l-p:0.12104791402816772
epoch£º887	 i:1 	 global-step:17741	 l-p:0.11667950451374054
epoch£º887	 i:2 	 global-step:17742	 l-p:0.15145835280418396
epoch£º887	 i:3 	 global-step:17743	 l-p:0.12180783599615097
epoch£º887	 i:4 	 global-step:17744	 l-p:0.23206226527690887
epoch£º887	 i:5 	 global-step:17745	 l-p:0.12201138585805893
epoch£º887	 i:6 	 global-step:17746	 l-p:0.22996436059474945
epoch£º887	 i:7 	 global-step:17747	 l-p:0.12235403060913086
epoch£º887	 i:8 	 global-step:17748	 l-p:0.21859347820281982
epoch£º887	 i:9 	 global-step:17749	 l-p:0.0871298611164093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:888
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[5.1090, 5.4013, 5.2611],
        [5.1090, 4.8614, 4.8079],
        [5.1090, 5.3785, 5.2244],
        [5.1090, 4.8635, 4.8148]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:888, step:0 
model_pd.l_p.mean(): 0.07747864723205566 
model_pd.l_d.mean(): -19.44930076599121 
model_pd.lagr.mean(): -19.371822357177734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5881], device='cuda:0')), ('power', tensor([-20.2626], device='cuda:0'))])
epoch£º888	 i:0 	 global-step:17760	 l-p:0.07747864723205566
epoch£º888	 i:1 	 global-step:17761	 l-p:0.2817348539829254
epoch£º888	 i:2 	 global-step:17762	 l-p:0.13589096069335938
epoch£º888	 i:3 	 global-step:17763	 l-p:0.11780212074518204
epoch£º888	 i:4 	 global-step:17764	 l-p:0.18112897872924805
epoch£º888	 i:5 	 global-step:17765	 l-p:0.12481115758419037
epoch£º888	 i:6 	 global-step:17766	 l-p:0.10754135251045227
epoch£º888	 i:7 	 global-step:17767	 l-p:0.12337211519479752
epoch£º888	 i:8 	 global-step:17768	 l-p:0.20722226798534393
epoch£º888	 i:9 	 global-step:17769	 l-p:0.2010660469532013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:889
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0975, 5.0975, 5.0975],
        [5.0975, 5.0975, 5.0975],
        [5.0975, 4.9258, 4.9679],
        [5.0975, 5.0344, 5.0782]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:889, step:0 
model_pd.l_p.mean(): 0.16624152660369873 
model_pd.l_d.mean(): -20.907073974609375 
model_pd.lagr.mean(): -20.740833282470703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4280], device='cuda:0')), ('power', tensor([-21.5728], device='cuda:0'))])
epoch£º889	 i:0 	 global-step:17780	 l-p:0.16624152660369873
epoch£º889	 i:1 	 global-step:17781	 l-p:0.1955702155828476
epoch£º889	 i:2 	 global-step:17782	 l-p:0.13057993352413177
epoch£º889	 i:3 	 global-step:17783	 l-p:0.15612277388572693
epoch£º889	 i:4 	 global-step:17784	 l-p:0.12476279586553574
epoch£º889	 i:5 	 global-step:17785	 l-p:0.153916135430336
epoch£º889	 i:6 	 global-step:17786	 l-p:0.13147923350334167
epoch£º889	 i:7 	 global-step:17787	 l-p:0.17227214574813843
epoch£º889	 i:8 	 global-step:17788	 l-p:0.10969194769859314
epoch£º889	 i:9 	 global-step:17789	 l-p:0.18897101283073425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:890
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1220, 5.1220, 5.1220],
        [5.1220, 5.1220, 5.1220],
        [5.1220, 4.9104, 4.9168],
        [5.1220, 4.9021, 4.8977]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:890, step:0 
model_pd.l_p.mean(): 0.11576054245233536 
model_pd.l_d.mean(): -20.467586517333984 
model_pd.lagr.mean(): -20.351825714111328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4711], device='cuda:0')), ('power', tensor([-21.1726], device='cuda:0'))])
epoch£º890	 i:0 	 global-step:17800	 l-p:0.11576054245233536
epoch£º890	 i:1 	 global-step:17801	 l-p:0.13398543000221252
epoch£º890	 i:2 	 global-step:17802	 l-p:0.10162488371133804
epoch£º890	 i:3 	 global-step:17803	 l-p:0.13325850665569305
epoch£º890	 i:4 	 global-step:17804	 l-p:0.2163260281085968
epoch£º890	 i:5 	 global-step:17805	 l-p:0.1429731696844101
epoch£º890	 i:6 	 global-step:17806	 l-p:0.14012202620506287
epoch£º890	 i:7 	 global-step:17807	 l-p:0.14684006571769714
epoch£º890	 i:8 	 global-step:17808	 l-p:0.10809650272130966
epoch£º890	 i:9 	 global-step:17809	 l-p:0.13484320044517517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:891
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1542, 5.1332, 5.1514],
        [5.1542, 5.2528, 5.0037],
        [5.1542, 5.0951, 5.1369],
        [5.1542, 5.1518, 5.1542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:891, step:0 
model_pd.l_p.mean(): 0.13213717937469482 
model_pd.l_d.mean(): -19.123157501220703 
model_pd.lagr.mean(): -18.99102020263672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5539], device='cuda:0')), ('power', tensor([-19.8980], device='cuda:0'))])
epoch£º891	 i:0 	 global-step:17820	 l-p:0.13213717937469482
epoch£º891	 i:1 	 global-step:17821	 l-p:0.10747150331735611
epoch£º891	 i:2 	 global-step:17822	 l-p:0.13814781606197357
epoch£º891	 i:3 	 global-step:17823	 l-p:0.16045504808425903
epoch£º891	 i:4 	 global-step:17824	 l-p:0.11826243996620178
epoch£º891	 i:5 	 global-step:17825	 l-p:0.1248423159122467
epoch£º891	 i:6 	 global-step:17826	 l-p:0.10463004559278488
epoch£º891	 i:7 	 global-step:17827	 l-p:0.148655965924263
epoch£º891	 i:8 	 global-step:17828	 l-p:0.16481471061706543
epoch£º891	 i:9 	 global-step:17829	 l-p:0.1067856028676033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:892
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1619, 4.9242, 4.8847],
        [5.1619, 5.1619, 5.1619],
        [5.1619, 5.0311, 4.7167],
        [5.1619, 5.1574, 5.1617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:892, step:0 
model_pd.l_p.mean(): 0.1359741985797882 
model_pd.l_d.mean(): -20.005464553833008 
model_pd.lagr.mean(): -19.869489669799805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5284], device='cuda:0')), ('power', tensor([-20.7639], device='cuda:0'))])
epoch£º892	 i:0 	 global-step:17840	 l-p:0.1359741985797882
epoch£º892	 i:1 	 global-step:17841	 l-p:0.1401028037071228
epoch£º892	 i:2 	 global-step:17842	 l-p:0.18528853356838226
epoch£º892	 i:3 	 global-step:17843	 l-p:0.05316152796149254
epoch£º892	 i:4 	 global-step:17844	 l-p:0.16458779573440552
epoch£º892	 i:5 	 global-step:17845	 l-p:0.0952751412987709
epoch£º892	 i:6 	 global-step:17846	 l-p:0.10054515302181244
epoch£º892	 i:7 	 global-step:17847	 l-p:0.11714376509189606
epoch£º892	 i:8 	 global-step:17848	 l-p:0.14697016775608063
epoch£º892	 i:9 	 global-step:17849	 l-p:0.1418476700782776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:893
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1577, 5.2243, 4.9601],
        [5.1577, 5.5509, 5.4739],
        [5.1577, 5.1443, 5.1564],
        [5.1577, 4.9527, 4.9646]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:893, step:0 
model_pd.l_p.mean(): 0.07321221381425858 
model_pd.l_d.mean(): -19.396963119506836 
model_pd.lagr.mean(): -19.32375144958496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5881], device='cuda:0')), ('power', tensor([-20.2097], device='cuda:0'))])
epoch£º893	 i:0 	 global-step:17860	 l-p:0.07321221381425858
epoch£º893	 i:1 	 global-step:17861	 l-p:0.17149138450622559
epoch£º893	 i:2 	 global-step:17862	 l-p:0.13660041987895966
epoch£º893	 i:3 	 global-step:17863	 l-p:0.12432122975587845
epoch£º893	 i:4 	 global-step:17864	 l-p:0.20758630335330963
epoch£º893	 i:5 	 global-step:17865	 l-p:0.12003231793642044
epoch£º893	 i:6 	 global-step:17866	 l-p:0.11654824763536453
epoch£º893	 i:7 	 global-step:17867	 l-p:0.15761500597000122
epoch£º893	 i:8 	 global-step:17868	 l-p:0.10987279564142227
epoch£º893	 i:9 	 global-step:17869	 l-p:0.1325686275959015
====================================================================================================
====================================================================================================
====================================================================================================

epoch:894
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1298, 5.1293, 5.1298],
        [5.1298, 5.1238, 5.1294],
        [5.1298, 4.9134, 4.6220],
        [5.1298, 4.8616, 4.7194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:894, step:0 
model_pd.l_p.mean(): 0.12495668977499008 
model_pd.l_d.mean(): -20.05333709716797 
model_pd.lagr.mean(): -19.928380966186523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4805], device='cuda:0')), ('power', tensor([-20.7634], device='cuda:0'))])
epoch£º894	 i:0 	 global-step:17880	 l-p:0.12495668977499008
epoch£º894	 i:1 	 global-step:17881	 l-p:0.0742018073797226
epoch£º894	 i:2 	 global-step:17882	 l-p:0.13235163688659668
epoch£º894	 i:3 	 global-step:17883	 l-p:0.12412109225988388
epoch£º894	 i:4 	 global-step:17884	 l-p:0.09460596740245819
epoch£º894	 i:5 	 global-step:17885	 l-p:0.12298909574747086
epoch£º894	 i:6 	 global-step:17886	 l-p:0.10989737510681152
epoch£º894	 i:7 	 global-step:17887	 l-p:0.24034622311592102
epoch£º894	 i:8 	 global-step:17888	 l-p:0.34387245774269104
epoch£º894	 i:9 	 global-step:17889	 l-p:0.2388206422328949
====================================================================================================
====================================================================================================
====================================================================================================

epoch:895
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0944, 5.0944, 5.0944],
        [5.0944, 5.0814, 5.0931],
        [5.0944, 5.3798, 5.2351],
        [5.0944, 4.8224, 4.6803]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:895, step:0 
model_pd.l_p.mean(): 0.12866389751434326 
model_pd.l_d.mean(): -18.94009017944336 
model_pd.lagr.mean(): -18.811426162719727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6021], device='cuda:0')), ('power', tensor([-19.7621], device='cuda:0'))])
epoch£º895	 i:0 	 global-step:17900	 l-p:0.12866389751434326
epoch£º895	 i:1 	 global-step:17901	 l-p:0.17905020713806152
epoch£º895	 i:2 	 global-step:17902	 l-p:0.16133460402488708
epoch£º895	 i:3 	 global-step:17903	 l-p:0.16350898146629333
epoch£º895	 i:4 	 global-step:17904	 l-p:0.13974414765834808
epoch£º895	 i:5 	 global-step:17905	 l-p:0.119211845099926
epoch£º895	 i:6 	 global-step:17906	 l-p:0.13666516542434692
epoch£º895	 i:7 	 global-step:17907	 l-p:0.18474450707435608
epoch£º895	 i:8 	 global-step:17908	 l-p:0.243896022439003
epoch£º895	 i:9 	 global-step:17909	 l-p:0.09325913339853287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:896
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1162, 5.4996, 5.4170],
        [5.1162, 5.0663, 5.1036],
        [5.1162, 5.1162, 5.1162],
        [5.1162, 4.8479, 4.7189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:896, step:0 
model_pd.l_p.mean(): 0.1348285973072052 
model_pd.l_d.mean(): -20.26854705810547 
model_pd.lagr.mean(): -20.133718490600586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4878], device='cuda:0')), ('power', tensor([-20.9884], device='cuda:0'))])
epoch£º896	 i:0 	 global-step:17920	 l-p:0.1348285973072052
epoch£º896	 i:1 	 global-step:17921	 l-p:0.19040897488594055
epoch£º896	 i:2 	 global-step:17922	 l-p:0.11620130389928818
epoch£º896	 i:3 	 global-step:17923	 l-p:0.17694829404354095
epoch£º896	 i:4 	 global-step:17924	 l-p:0.17300952970981598
epoch£º896	 i:5 	 global-step:17925	 l-p:0.1276455819606781
epoch£º896	 i:6 	 global-step:17926	 l-p:0.11868272721767426
epoch£º896	 i:7 	 global-step:17927	 l-p:0.15234623849391937
epoch£º896	 i:8 	 global-step:17928	 l-p:0.11328572779893875
epoch£º896	 i:9 	 global-step:17929	 l-p:0.1313815414905548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:897
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1471, 5.1027, 5.1368],
        [5.1471, 5.1426, 5.1469],
        [5.1471, 5.1012, 5.1361],
        [5.1471, 4.9011, 4.8467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:897, step:0 
model_pd.l_p.mean(): 0.1405385583639145 
model_pd.l_d.mean(): -20.552013397216797 
model_pd.lagr.mean(): -20.411474227905273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4259], device='cuda:0')), ('power', tensor([-21.2117], device='cuda:0'))])
epoch£º897	 i:0 	 global-step:17940	 l-p:0.1405385583639145
epoch£º897	 i:1 	 global-step:17941	 l-p:0.07617510110139847
epoch£º897	 i:2 	 global-step:17942	 l-p:0.11860157549381256
epoch£º897	 i:3 	 global-step:17943	 l-p:0.137151300907135
epoch£º897	 i:4 	 global-step:17944	 l-p:0.15768025815486908
epoch£º897	 i:5 	 global-step:17945	 l-p:0.1400086134672165
epoch£º897	 i:6 	 global-step:17946	 l-p:0.14804229140281677
epoch£º897	 i:7 	 global-step:17947	 l-p:0.06956500560045242
epoch£º897	 i:8 	 global-step:17948	 l-p:0.2148088812828064
epoch£º897	 i:9 	 global-step:17949	 l-p:0.14213445782661438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:898
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1524, 4.9089, 4.8595],
        [5.1524, 5.1518, 5.1524],
        [5.1524, 5.1080, 5.1421],
        [5.1524, 5.6146, 5.5843]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:898, step:0 
model_pd.l_p.mean(): 0.15543821454048157 
model_pd.l_d.mean(): -19.659832000732422 
model_pd.lagr.mean(): -19.50439453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5131], device='cuda:0')), ('power', tensor([-20.3988], device='cuda:0'))])
epoch£º898	 i:0 	 global-step:17960	 l-p:0.15543821454048157
epoch£º898	 i:1 	 global-step:17961	 l-p:0.1602550446987152
epoch£º898	 i:2 	 global-step:17962	 l-p:0.15398821234703064
epoch£º898	 i:3 	 global-step:17963	 l-p:0.141977459192276
epoch£º898	 i:4 	 global-step:17964	 l-p:0.0631454586982727
epoch£º898	 i:5 	 global-step:17965	 l-p:0.08621136844158173
epoch£º898	 i:6 	 global-step:17966	 l-p:0.10648015886545181
epoch£º898	 i:7 	 global-step:17967	 l-p:0.14457355439662933
epoch£º898	 i:8 	 global-step:17968	 l-p:0.14493481814861298
epoch£º898	 i:9 	 global-step:17969	 l-p:0.10832006484270096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:899
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1895, 5.0904, 4.7776],
        [5.1895, 4.9387, 4.7068],
        [5.1895, 4.9486, 4.8989],
        [5.1895, 5.1884, 5.1894]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:899, step:0 
model_pd.l_p.mean(): 0.15450583398342133 
model_pd.l_d.mean(): -20.10215950012207 
model_pd.lagr.mean(): -19.94765281677246 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4475], device='cuda:0')), ('power', tensor([-20.7790], device='cuda:0'))])
epoch£º899	 i:0 	 global-step:17980	 l-p:0.15450583398342133
epoch£º899	 i:1 	 global-step:17981	 l-p:0.07469505816698074
epoch£º899	 i:2 	 global-step:17982	 l-p:0.0949106216430664
epoch£º899	 i:3 	 global-step:17983	 l-p:0.1198178082704544
epoch£º899	 i:4 	 global-step:17984	 l-p:0.13068503141403198
epoch£º899	 i:5 	 global-step:17985	 l-p:0.1853424459695816
epoch£º899	 i:6 	 global-step:17986	 l-p:0.15016765892505646
epoch£º899	 i:7 	 global-step:17987	 l-p:0.03174843266606331
epoch£º899	 i:8 	 global-step:17988	 l-p:0.12495982646942139
epoch£º899	 i:9 	 global-step:17989	 l-p:0.12331292778253555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:900
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1876, 4.9257, 4.7942],
        [5.1876, 5.0795, 5.1347],
        [5.1876, 5.1876, 5.1876],
        [5.1876, 5.0863, 5.1409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:900, step:0 
model_pd.l_p.mean(): 0.1468132883310318 
model_pd.l_d.mean(): -20.646038055419922 
model_pd.lagr.mean(): -20.499225616455078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4184], device='cuda:0')), ('power', tensor([-21.2991], device='cuda:0'))])
epoch£º900	 i:0 	 global-step:18000	 l-p:0.1468132883310318
epoch£º900	 i:1 	 global-step:18001	 l-p:0.05717797949910164
epoch£º900	 i:2 	 global-step:18002	 l-p:0.1520298570394516
epoch£º900	 i:3 	 global-step:18003	 l-p:0.15740741789340973
epoch£º900	 i:4 	 global-step:18004	 l-p:0.12954583764076233
epoch£º900	 i:5 	 global-step:18005	 l-p:0.1335964798927307
epoch£º900	 i:6 	 global-step:18006	 l-p:0.10060959309339523
epoch£º900	 i:7 	 global-step:18007	 l-p:0.12024062126874924
epoch£º900	 i:8 	 global-step:18008	 l-p:0.13040058314800262
epoch£º900	 i:9 	 global-step:18009	 l-p:0.11693563312292099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:901
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1488, 5.1064, 5.1393],
        [5.1488, 5.1445, 5.1486],
        [5.1488, 5.2631, 5.0209],
        [5.1488, 4.9177, 4.8946]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:901, step:0 
model_pd.l_p.mean(): 0.18384845554828644 
model_pd.l_d.mean(): -20.46051025390625 
model_pd.lagr.mean(): -20.276660919189453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4761], device='cuda:0')), ('power', tensor([-21.1705], device='cuda:0'))])
epoch£º901	 i:0 	 global-step:18020	 l-p:0.18384845554828644
epoch£º901	 i:1 	 global-step:18021	 l-p:0.14279763400554657
epoch£º901	 i:2 	 global-step:18022	 l-p:0.12435254454612732
epoch£º901	 i:3 	 global-step:18023	 l-p:0.0952175110578537
epoch£º901	 i:4 	 global-step:18024	 l-p:0.12640556693077087
epoch£º901	 i:5 	 global-step:18025	 l-p:0.15164078772068024
epoch£º901	 i:6 	 global-step:18026	 l-p:0.18436434864997864
epoch£º901	 i:7 	 global-step:18027	 l-p:0.10241561383008957
epoch£º901	 i:8 	 global-step:18028	 l-p:0.21855807304382324
epoch£º901	 i:9 	 global-step:18029	 l-p:0.11890838295221329
====================================================================================================
====================================================================================================
====================================================================================================

epoch:902
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1166, 5.1135, 5.1165],
        [5.1166, 4.8798, 4.8511],
        [5.1166, 4.9754, 4.6565],
        [5.1166, 5.0433, 5.0914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:902, step:0 
model_pd.l_p.mean(): 0.1330067664384842 
model_pd.l_d.mean(): -19.582441329956055 
model_pd.lagr.mean(): -19.449434280395508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4906], device='cuda:0')), ('power', tensor([-20.2976], device='cuda:0'))])
epoch£º902	 i:0 	 global-step:18040	 l-p:0.1330067664384842
epoch£º902	 i:1 	 global-step:18041	 l-p:0.16358241438865662
epoch£º902	 i:2 	 global-step:18042	 l-p:0.1981971710920334
epoch£º902	 i:3 	 global-step:18043	 l-p:0.19424240291118622
epoch£º902	 i:4 	 global-step:18044	 l-p:0.11179208755493164
epoch£º902	 i:5 	 global-step:18045	 l-p:0.17482082545757294
epoch£º902	 i:6 	 global-step:18046	 l-p:0.11626921594142914
epoch£º902	 i:7 	 global-step:18047	 l-p:0.10850599408149719
epoch£º902	 i:8 	 global-step:18048	 l-p:0.1379537284374237
epoch£º902	 i:9 	 global-step:18049	 l-p:0.11043556034564972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:903
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1373, 5.0898, 5.1257],
        [5.1373, 5.4359, 5.2979],
        [5.1373, 4.9528, 4.9858],
        [5.1373, 4.9907, 5.0428]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:903, step:0 
model_pd.l_p.mean(): 0.14441172778606415 
model_pd.l_d.mean(): -20.27094078063965 
model_pd.lagr.mean(): -20.126529693603516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5127], device='cuda:0')), ('power', tensor([-21.0162], device='cuda:0'))])
epoch£º903	 i:0 	 global-step:18060	 l-p:0.14441172778606415
epoch£º903	 i:1 	 global-step:18061	 l-p:0.13879148662090302
epoch£º903	 i:2 	 global-step:18062	 l-p:0.15012817084789276
epoch£º903	 i:3 	 global-step:18063	 l-p:0.13466979563236237
epoch£º903	 i:4 	 global-step:18064	 l-p:0.09959708899259567
epoch£º903	 i:5 	 global-step:18065	 l-p:0.1557495892047882
epoch£º903	 i:6 	 global-step:18066	 l-p:0.1325438916683197
epoch£º903	 i:7 	 global-step:18067	 l-p:0.12743739783763885
epoch£º903	 i:8 	 global-step:18068	 l-p:0.12659907341003418
epoch£º903	 i:9 	 global-step:18069	 l-p:0.14429618418216705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:904
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1474, 5.1221, 5.1435],
        [5.1474, 4.9664, 5.0014],
        [5.1474, 5.1474, 5.1474],
        [5.1474, 5.1432, 5.1472]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:904, step:0 
model_pd.l_p.mean(): 0.11691302806138992 
model_pd.l_d.mean(): -19.361412048339844 
model_pd.lagr.mean(): -19.24449920654297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5358], device='cuda:0')), ('power', tensor([-20.1204], device='cuda:0'))])
epoch£º904	 i:0 	 global-step:18080	 l-p:0.11691302806138992
epoch£º904	 i:1 	 global-step:18081	 l-p:0.15272074937820435
epoch£º904	 i:2 	 global-step:18082	 l-p:0.15227891504764557
epoch£º904	 i:3 	 global-step:18083	 l-p:0.1474522352218628
epoch£º904	 i:4 	 global-step:18084	 l-p:0.1270647943019867
epoch£º904	 i:5 	 global-step:18085	 l-p:0.19206245243549347
epoch£º904	 i:6 	 global-step:18086	 l-p:0.1227305680513382
epoch£º904	 i:7 	 global-step:18087	 l-p:0.19306838512420654
epoch£º904	 i:8 	 global-step:18088	 l-p:0.08038985729217529
epoch£º904	 i:9 	 global-step:18089	 l-p:0.12813346087932587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:905
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1187, 5.1179, 5.1187],
        [5.1187, 5.0089, 5.0652],
        [5.1187, 4.8502, 4.6456],
        [5.1187, 4.8658, 4.8017]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:905, step:0 
model_pd.l_p.mean(): 0.13852843642234802 
model_pd.l_d.mean(): -20.578229904174805 
model_pd.lagr.mean(): -20.439701080322266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4218], device='cuda:0')), ('power', tensor([-21.2340], device='cuda:0'))])
epoch£º905	 i:0 	 global-step:18100	 l-p:0.13852843642234802
epoch£º905	 i:1 	 global-step:18101	 l-p:0.1244869977235794
epoch£º905	 i:2 	 global-step:18102	 l-p:0.13225366175174713
epoch£º905	 i:3 	 global-step:18103	 l-p:0.12427759915590286
epoch£º905	 i:4 	 global-step:18104	 l-p:0.0919879823923111
epoch£º905	 i:5 	 global-step:18105	 l-p:0.20149356126785278
epoch£º905	 i:6 	 global-step:18106	 l-p:0.20019832253456116
epoch£º905	 i:7 	 global-step:18107	 l-p:0.2488803118467331
epoch£º905	 i:8 	 global-step:18108	 l-p:0.1368395835161209
epoch£º905	 i:9 	 global-step:18109	 l-p:0.17815297842025757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:906
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1113, 4.8767, 4.5957],
        [5.1113, 4.8880, 4.5964],
        [5.1113, 5.1950, 4.9381],
        [5.1113, 5.1110, 5.1113]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:906, step:0 
model_pd.l_p.mean(): 0.12637630105018616 
model_pd.l_d.mean(): -20.239238739013672 
model_pd.lagr.mean(): -20.11286163330078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4783], device='cuda:0')), ('power', tensor([-20.9490], device='cuda:0'))])
epoch£º906	 i:0 	 global-step:18120	 l-p:0.12637630105018616
epoch£º906	 i:1 	 global-step:18121	 l-p:0.10004483908414841
epoch£º906	 i:2 	 global-step:18122	 l-p:0.1326342225074768
epoch£º906	 i:3 	 global-step:18123	 l-p:0.24027343094348907
epoch£º906	 i:4 	 global-step:18124	 l-p:0.14798392355442047
epoch£º906	 i:5 	 global-step:18125	 l-p:0.18912607431411743
epoch£º906	 i:6 	 global-step:18126	 l-p:0.15066513419151306
epoch£º906	 i:7 	 global-step:18127	 l-p:0.12177130579948425
epoch£º906	 i:8 	 global-step:18128	 l-p:0.11678203195333481
epoch£º906	 i:9 	 global-step:18129	 l-p:0.15314006805419922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:907
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1370, 5.1096, 5.1326],
        [5.1370, 5.0115, 5.0675],
        [5.1370, 5.0583, 5.1082],
        [5.1370, 4.9009, 4.6267]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:907, step:0 
model_pd.l_p.mean(): 0.22050490975379944 
model_pd.l_d.mean(): -19.62529754638672 
model_pd.lagr.mean(): -19.40479278564453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4786], device='cuda:0')), ('power', tensor([-20.3287], device='cuda:0'))])
epoch£º907	 i:0 	 global-step:18140	 l-p:0.22050490975379944
epoch£º907	 i:1 	 global-step:18141	 l-p:0.12857143580913544
epoch£º907	 i:2 	 global-step:18142	 l-p:0.05919025465846062
epoch£º907	 i:3 	 global-step:18143	 l-p:0.15949313342571259
epoch£º907	 i:4 	 global-step:18144	 l-p:0.13437215983867645
epoch£º907	 i:5 	 global-step:18145	 l-p:0.11318584531545639
epoch£º907	 i:6 	 global-step:18146	 l-p:0.13405218720436096
epoch£º907	 i:7 	 global-step:18147	 l-p:0.11818893253803253
epoch£º907	 i:8 	 global-step:18148	 l-p:0.17843760550022125
epoch£º907	 i:9 	 global-step:18149	 l-p:0.12522049248218536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:908
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1810,  0.1024,  1.0000,  0.0579,
          1.0000,  0.5657, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4788,  0.3746,  1.0000,  0.2931,
          1.0000,  0.7823, 31.6228]], device='cuda:0')
 pt:tensor([[5.1478, 5.4261, 5.2751],
        [5.1478, 4.9594, 4.9890],
        [5.1478, 4.9469, 4.6459],
        [5.1478, 4.9379, 4.6422]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:908, step:0 
model_pd.l_p.mean(): 0.11717940121889114 
model_pd.l_d.mean(): -19.34300994873047 
model_pd.lagr.mean(): -19.225830078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5875], device='cuda:0')), ('power', tensor([-20.1546], device='cuda:0'))])
epoch£º908	 i:0 	 global-step:18160	 l-p:0.11717940121889114
epoch£º908	 i:1 	 global-step:18161	 l-p:0.13562655448913574
epoch£º908	 i:2 	 global-step:18162	 l-p:0.11830661445856094
epoch£º908	 i:3 	 global-step:18163	 l-p:0.12892332673072815
epoch£º908	 i:4 	 global-step:18164	 l-p:0.09413374960422516
epoch£º908	 i:5 	 global-step:18165	 l-p:0.13531965017318726
epoch£º908	 i:6 	 global-step:18166	 l-p:0.18477270007133484
epoch£º908	 i:7 	 global-step:18167	 l-p:0.13004983961582184
epoch£º908	 i:8 	 global-step:18168	 l-p:0.13567620515823364
epoch£º908	 i:9 	 global-step:18169	 l-p:0.15138383209705353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:909
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1578, 5.1438, 5.1564],
        [5.1578, 5.1578, 5.1578],
        [5.1578, 5.0485, 4.7320],
        [5.1578, 5.1578, 5.1578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:909, step:0 
model_pd.l_p.mean(): 0.13761349022388458 
model_pd.l_d.mean(): -20.40778350830078 
model_pd.lagr.mean(): -20.270170211791992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4466], device='cuda:0')), ('power', tensor([-21.0870], device='cuda:0'))])
epoch£º909	 i:0 	 global-step:18180	 l-p:0.13761349022388458
epoch£º909	 i:1 	 global-step:18181	 l-p:0.13326816260814667
epoch£º909	 i:2 	 global-step:18182	 l-p:0.11815786361694336
epoch£º909	 i:3 	 global-step:18183	 l-p:0.10781925916671753
epoch£º909	 i:4 	 global-step:18184	 l-p:0.1319415420293808
epoch£º909	 i:5 	 global-step:18185	 l-p:0.1226816326379776
epoch£º909	 i:6 	 global-step:18186	 l-p:0.21762515604496002
epoch£º909	 i:7 	 global-step:18187	 l-p:0.09396985918283463
epoch£º909	 i:8 	 global-step:18188	 l-p:0.10071620345115662
epoch£º909	 i:9 	 global-step:18189	 l-p:0.15020200610160828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:910
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1475, 5.1475, 5.1476],
        [5.1475, 5.1472, 5.1475],
        [5.1475, 5.1475, 5.1475],
        [5.1475, 4.9198, 4.6381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:910, step:0 
model_pd.l_p.mean(): 0.17950046062469482 
model_pd.l_d.mean(): -19.321773529052734 
model_pd.lagr.mean(): -19.14227294921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5372], device='cuda:0')), ('power', tensor([-20.0817], device='cuda:0'))])
epoch£º910	 i:0 	 global-step:18200	 l-p:0.17950046062469482
epoch£º910	 i:1 	 global-step:18201	 l-p:0.1536213457584381
epoch£º910	 i:2 	 global-step:18202	 l-p:0.06123919412493706
epoch£º910	 i:3 	 global-step:18203	 l-p:0.15052835643291473
epoch£º910	 i:4 	 global-step:18204	 l-p:0.16867698729038239
epoch£º910	 i:5 	 global-step:18205	 l-p:0.1247684583067894
epoch£º910	 i:6 	 global-step:18206	 l-p:0.11386823654174805
epoch£º910	 i:7 	 global-step:18207	 l-p:0.18941812217235565
epoch£º910	 i:8 	 global-step:18208	 l-p:0.10515301674604416
epoch£º910	 i:9 	 global-step:18209	 l-p:0.14381292462348938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:911
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1213, 4.9204, 4.9407],
        [5.1213, 5.1213, 5.1213],
        [5.1213, 4.9782, 4.6588],
        [5.1213, 4.8934, 4.8803]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:911, step:0 
model_pd.l_p.mean(): 0.16010162234306335 
model_pd.l_d.mean(): -20.740985870361328 
model_pd.lagr.mean(): -20.58088493347168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4366], device='cuda:0')), ('power', tensor([-21.4137], device='cuda:0'))])
epoch£º911	 i:0 	 global-step:18220	 l-p:0.16010162234306335
epoch£º911	 i:1 	 global-step:18221	 l-p:0.2720153331756592
epoch£º911	 i:2 	 global-step:18222	 l-p:0.13393226265907288
epoch£º911	 i:3 	 global-step:18223	 l-p:0.1853647232055664
epoch£º911	 i:4 	 global-step:18224	 l-p:0.13323552906513214
epoch£º911	 i:5 	 global-step:18225	 l-p:0.15191376209259033
epoch£º911	 i:6 	 global-step:18226	 l-p:0.09277520328760147
epoch£º911	 i:7 	 global-step:18227	 l-p:0.13148976862430573
epoch£º911	 i:8 	 global-step:18228	 l-p:0.11389511078596115
epoch£º911	 i:9 	 global-step:18229	 l-p:0.0951312929391861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:912
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1169, 5.2222, 4.9752],
        [5.1169, 5.0414, 5.0903],
        [5.1169, 4.8560, 4.6174],
        [5.1169, 5.1116, 5.1166]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:912, step:0 
model_pd.l_p.mean(): 0.15804187953472137 
model_pd.l_d.mean(): -19.421010971069336 
model_pd.lagr.mean(): -19.262969970703125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5874], device='cuda:0')), ('power', tensor([-20.2333], device='cuda:0'))])
epoch£º912	 i:0 	 global-step:18240	 l-p:0.15804187953472137
epoch£º912	 i:1 	 global-step:18241	 l-p:0.11387676000595093
epoch£º912	 i:2 	 global-step:18242	 l-p:0.17435066401958466
epoch£º912	 i:3 	 global-step:18243	 l-p:0.1744358092546463
epoch£º912	 i:4 	 global-step:18244	 l-p:0.10912265628576279
epoch£º912	 i:5 	 global-step:18245	 l-p:0.1730784773826599
epoch£º912	 i:6 	 global-step:18246	 l-p:0.13985003530979156
epoch£º912	 i:7 	 global-step:18247	 l-p:0.13975505530834198
epoch£º912	 i:8 	 global-step:18248	 l-p:0.16401824355125427
epoch£º912	 i:9 	 global-step:18249	 l-p:0.12788425385951996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:913
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1217, 5.1217, 5.1217],
        [5.1217, 4.8614, 4.6228],
        [5.1217, 5.1213, 5.1217],
        [5.1217, 4.9063, 4.6091]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:913, step:0 
model_pd.l_p.mean(): 0.11025514453649521 
model_pd.l_d.mean(): -20.150556564331055 
model_pd.lagr.mean(): -20.040302276611328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5135], device='cuda:0')), ('power', tensor([-20.8953], device='cuda:0'))])
epoch£º913	 i:0 	 global-step:18260	 l-p:0.11025514453649521
epoch£º913	 i:1 	 global-step:18261	 l-p:0.11603333055973053
epoch£º913	 i:2 	 global-step:18262	 l-p:0.1268656700849533
epoch£º913	 i:3 	 global-step:18263	 l-p:0.14143449068069458
epoch£º913	 i:4 	 global-step:18264	 l-p:0.14097675681114197
epoch£º913	 i:5 	 global-step:18265	 l-p:0.18132011592388153
epoch£º913	 i:6 	 global-step:18266	 l-p:0.17694473266601562
epoch£º913	 i:7 	 global-step:18267	 l-p:0.18328218162059784
epoch£º913	 i:8 	 global-step:18268	 l-p:0.22676989436149597
epoch£º913	 i:9 	 global-step:18269	 l-p:0.12226634472608566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:914
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1207, 4.8577, 4.7636],
        [5.1207, 5.1206, 5.1207],
        [5.1207, 5.0706, 5.1080],
        [5.1207, 4.8482, 4.6888]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:914, step:0 
model_pd.l_p.mean(): 0.12817786633968353 
model_pd.l_d.mean(): -19.955322265625 
model_pd.lagr.mean(): -19.827144622802734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4982], device='cuda:0')), ('power', tensor([-20.6824], device='cuda:0'))])
epoch£º914	 i:0 	 global-step:18280	 l-p:0.12817786633968353
epoch£º914	 i:1 	 global-step:18281	 l-p:0.10349869728088379
epoch£º914	 i:2 	 global-step:18282	 l-p:0.09234828501939774
epoch£º914	 i:3 	 global-step:18283	 l-p:0.16158942878246307
epoch£º914	 i:4 	 global-step:18284	 l-p:0.1394083946943283
epoch£º914	 i:5 	 global-step:18285	 l-p:0.1619793027639389
epoch£º914	 i:6 	 global-step:18286	 l-p:0.1890314817428589
epoch£º914	 i:7 	 global-step:18287	 l-p:0.12395535409450531
epoch£º914	 i:8 	 global-step:18288	 l-p:0.10286936163902283
epoch£º914	 i:9 	 global-step:18289	 l-p:0.16322143375873566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:915
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1673, 5.0930, 5.1413],
        [5.1673, 5.1651, 5.1672],
        [5.1673, 5.1673, 5.1673],
        [5.1673, 5.1628, 5.1671]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:915, step:0 
model_pd.l_p.mean(): 0.11111930757761002 
model_pd.l_d.mean(): -20.654756546020508 
model_pd.lagr.mean(): -20.543636322021484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4449], device='cuda:0')), ('power', tensor([-21.3349], device='cuda:0'))])
epoch£º915	 i:0 	 global-step:18300	 l-p:0.11111930757761002
epoch£º915	 i:1 	 global-step:18301	 l-p:0.12333764135837555
epoch£º915	 i:2 	 global-step:18302	 l-p:0.09065884351730347
epoch£º915	 i:3 	 global-step:18303	 l-p:0.13404910266399384
epoch£º915	 i:4 	 global-step:18304	 l-p:0.18742850422859192
epoch£º915	 i:5 	 global-step:18305	 l-p:0.10982071608304977
epoch£º915	 i:6 	 global-step:18306	 l-p:0.1860090047121048
epoch£º915	 i:7 	 global-step:18307	 l-p:0.10555265098810196
epoch£º915	 i:8 	 global-step:18308	 l-p:0.11599954217672348
epoch£º915	 i:9 	 global-step:18309	 l-p:0.12257260829210281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:916
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1741, 5.6480, 5.6239],
        [5.1741, 5.1741, 5.1741],
        [5.1741, 5.1488, 5.1702],
        [5.1741, 4.9450, 4.6680]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:916, step:0 
model_pd.l_p.mean(): 0.13451531529426575 
model_pd.l_d.mean(): -19.936481475830078 
model_pd.lagr.mean(): -19.801965713500977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4450], device='cuda:0')), ('power', tensor([-20.6090], device='cuda:0'))])
epoch£º916	 i:0 	 global-step:18320	 l-p:0.13451531529426575
epoch£º916	 i:1 	 global-step:18321	 l-p:0.10057393461465836
epoch£º916	 i:2 	 global-step:18322	 l-p:0.1946582794189453
epoch£º916	 i:3 	 global-step:18323	 l-p:0.10285469144582748
epoch£º916	 i:4 	 global-step:18324	 l-p:0.1228911355137825
epoch£º916	 i:5 	 global-step:18325	 l-p:0.10387714207172394
epoch£º916	 i:6 	 global-step:18326	 l-p:0.12802599370479584
epoch£º916	 i:7 	 global-step:18327	 l-p:0.15257546305656433
epoch£º916	 i:8 	 global-step:18328	 l-p:0.12864194810390472
epoch£º916	 i:9 	 global-step:18329	 l-p:0.1256014108657837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:917
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1509, 5.1083, 5.1414],
        [5.1509, 5.1509, 5.1510],
        [5.1509, 5.4244, 5.2697],
        [5.1509, 5.0817, 5.1282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:917, step:0 
model_pd.l_p.mean(): 0.1287328451871872 
model_pd.l_d.mean(): -20.917638778686523 
model_pd.lagr.mean(): -20.78890609741211 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3868], device='cuda:0')), ('power', tensor([-21.5413], device='cuda:0'))])
epoch£º917	 i:0 	 global-step:18340	 l-p:0.1287328451871872
epoch£º917	 i:1 	 global-step:18341	 l-p:0.1304072141647339
epoch£º917	 i:2 	 global-step:18342	 l-p:0.12353816628456116
epoch£º917	 i:3 	 global-step:18343	 l-p:0.1587163507938385
epoch£º917	 i:4 	 global-step:18344	 l-p:0.1598045825958252
epoch£º917	 i:5 	 global-step:18345	 l-p:0.12213030457496643
epoch£º917	 i:6 	 global-step:18346	 l-p:0.16952204704284668
epoch£º917	 i:7 	 global-step:18347	 l-p:0.19876979291439056
epoch£º917	 i:8 	 global-step:18348	 l-p:0.14733289182186127
epoch£º917	 i:9 	 global-step:18349	 l-p:0.09484561532735825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:918
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1198, 5.1198, 5.1198],
        [5.1198, 5.1060, 5.1184],
        [5.1198, 5.1198, 5.1198],
        [5.1198, 4.9688, 4.6486]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:918, step:0 
model_pd.l_p.mean(): 0.1682383120059967 
model_pd.l_d.mean(): -18.375885009765625 
model_pd.lagr.mean(): -18.2076473236084 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6165], device='cuda:0')), ('power', tensor([-19.2066], device='cuda:0'))])
epoch£º918	 i:0 	 global-step:18360	 l-p:0.1682383120059967
epoch£º918	 i:1 	 global-step:18361	 l-p:0.16254539787769318
epoch£º918	 i:2 	 global-step:18362	 l-p:0.07810292392969131
epoch£º918	 i:3 	 global-step:18363	 l-p:0.1636923998594284
epoch£º918	 i:4 	 global-step:18364	 l-p:0.16694924235343933
epoch£º918	 i:5 	 global-step:18365	 l-p:0.13313275575637817
epoch£º918	 i:6 	 global-step:18366	 l-p:0.13272467255592346
epoch£º918	 i:7 	 global-step:18367	 l-p:0.21115851402282715
epoch£º918	 i:8 	 global-step:18368	 l-p:0.11444348096847534
epoch£º918	 i:9 	 global-step:18369	 l-p:0.12185350060462952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:919
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228]], device='cuda:0')
 pt:tensor([[5.1261, 5.5030, 5.4143],
        [5.1261, 5.0691, 4.7583],
        [5.1261, 5.1789, 4.9068],
        [5.1261, 4.9219, 4.9398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:919, step:0 
model_pd.l_p.mean(): 0.1417936235666275 
model_pd.l_d.mean(): -19.46591567993164 
model_pd.lagr.mean(): -19.324121475219727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5172], device='cuda:0')), ('power', tensor([-20.2070], device='cuda:0'))])
epoch£º919	 i:0 	 global-step:18380	 l-p:0.1417936235666275
epoch£º919	 i:1 	 global-step:18381	 l-p:0.1624489724636078
epoch£º919	 i:2 	 global-step:18382	 l-p:0.16797734797000885
epoch£º919	 i:3 	 global-step:18383	 l-p:0.11398506164550781
epoch£º919	 i:4 	 global-step:18384	 l-p:0.18255561590194702
epoch£º919	 i:5 	 global-step:18385	 l-p:0.15125368535518646
epoch£º919	 i:6 	 global-step:18386	 l-p:0.13089154660701752
epoch£º919	 i:7 	 global-step:18387	 l-p:0.1386745572090149
epoch£º919	 i:8 	 global-step:18388	 l-p:0.11958824098110199
epoch£º919	 i:9 	 global-step:18389	 l-p:0.1319465935230255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:920
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1362, 4.9218, 4.6243],
        [5.1362, 5.1361, 5.1362],
        [5.1362, 4.9278, 4.9406],
        [5.1362, 4.8889, 4.8399]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:920, step:0 
model_pd.l_p.mean(): 0.15041209757328033 
model_pd.l_d.mean(): -19.878173828125 
model_pd.lagr.mean(): -19.72776222229004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5097], device='cuda:0')), ('power', tensor([-20.6161], device='cuda:0'))])
epoch£º920	 i:0 	 global-step:18400	 l-p:0.15041209757328033
epoch£º920	 i:1 	 global-step:18401	 l-p:0.11205204576253891
epoch£º920	 i:2 	 global-step:18402	 l-p:0.14563465118408203
epoch£º920	 i:3 	 global-step:18403	 l-p:0.11453458666801453
epoch£º920	 i:4 	 global-step:18404	 l-p:0.11815328150987625
epoch£º920	 i:5 	 global-step:18405	 l-p:0.1625051498413086
epoch£º920	 i:6 	 global-step:18406	 l-p:0.14439290761947632
epoch£º920	 i:7 	 global-step:18407	 l-p:0.10900463908910751
epoch£º920	 i:8 	 global-step:18408	 l-p:0.2031778246164322
epoch£º920	 i:9 	 global-step:18409	 l-p:0.15192657709121704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:921
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1251, 5.1251, 5.1251],
        [5.1251, 5.0024, 4.6814],
        [5.1251, 5.0492, 5.0983],
        [5.1251, 5.1251, 5.1251]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:921, step:0 
model_pd.l_p.mean(): 0.15935593843460083 
model_pd.l_d.mean(): -19.295961380004883 
model_pd.lagr.mean(): -19.136606216430664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5739], device='cuda:0')), ('power', tensor([-20.0931], device='cuda:0'))])
epoch£º921	 i:0 	 global-step:18420	 l-p:0.15935593843460083
epoch£º921	 i:1 	 global-step:18421	 l-p:0.13490886986255646
epoch£º921	 i:2 	 global-step:18422	 l-p:0.15347057580947876
epoch£º921	 i:3 	 global-step:18423	 l-p:0.157070592045784
epoch£º921	 i:4 	 global-step:18424	 l-p:0.10411473363637924
epoch£º921	 i:5 	 global-step:18425	 l-p:0.10756118595600128
epoch£º921	 i:6 	 global-step:18426	 l-p:0.12689627707004547
epoch£º921	 i:7 	 global-step:18427	 l-p:0.14193490147590637
epoch£º921	 i:8 	 global-step:18428	 l-p:0.15669624507427216
epoch£º921	 i:9 	 global-step:18429	 l-p:0.15180057287216187
====================================================================================================
====================================================================================================
====================================================================================================

epoch:922
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1571, 5.1571, 5.1571],
        [5.1571, 4.9679, 4.9977],
        [5.1571, 5.4584, 5.3203],
        [5.1571, 5.2649, 5.0179]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:922, step:0 
model_pd.l_p.mean(): 0.17014150321483612 
model_pd.l_d.mean(): -20.525798797607422 
model_pd.lagr.mean(): -20.35565757751465 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4575], device='cuda:0')), ('power', tensor([-21.2175], device='cuda:0'))])
epoch£º922	 i:0 	 global-step:18440	 l-p:0.17014150321483612
epoch£º922	 i:1 	 global-step:18441	 l-p:0.11440230906009674
epoch£º922	 i:2 	 global-step:18442	 l-p:0.13503634929656982
epoch£º922	 i:3 	 global-step:18443	 l-p:0.12038537114858627
epoch£º922	 i:4 	 global-step:18444	 l-p:0.11514711380004883
epoch£º922	 i:5 	 global-step:18445	 l-p:0.09793542325496674
epoch£º922	 i:6 	 global-step:18446	 l-p:0.09012220054864883
epoch£º922	 i:7 	 global-step:18447	 l-p:0.16845500469207764
epoch£º922	 i:8 	 global-step:18448	 l-p:0.19208461046218872
epoch£º922	 i:9 	 global-step:18449	 l-p:0.09889873117208481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:923
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1707, 4.9480, 4.6619],
        [5.1707, 4.9384, 4.9149],
        [5.1707, 5.0218, 5.0732],
        [5.1707, 5.1682, 5.1706]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:923, step:0 
model_pd.l_p.mean(): 0.12038355320692062 
model_pd.l_d.mean(): -20.480438232421875 
model_pd.lagr.mean(): -20.36005401611328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4397], device='cuda:0')), ('power', tensor([-21.1534], device='cuda:0'))])
epoch£º923	 i:0 	 global-step:18460	 l-p:0.12038355320692062
epoch£º923	 i:1 	 global-step:18461	 l-p:0.17864590883255005
epoch£º923	 i:2 	 global-step:18462	 l-p:0.11661126464605331
epoch£º923	 i:3 	 global-step:18463	 l-p:0.11057544499635696
epoch£º923	 i:4 	 global-step:18464	 l-p:0.12616194784641266
epoch£º923	 i:5 	 global-step:18465	 l-p:0.1257558912038803
epoch£º923	 i:6 	 global-step:18466	 l-p:0.10660331696271896
epoch£º923	 i:7 	 global-step:18467	 l-p:0.1566569060087204
epoch£º923	 i:8 	 global-step:18468	 l-p:0.13530376553535461
epoch£º923	 i:9 	 global-step:18469	 l-p:0.09655151516199112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:924
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1635, 4.9599, 4.9763],
        [5.1635, 4.9472, 4.6543],
        [5.1635, 5.1039, 5.1461],
        [5.1635, 5.0133, 5.0646]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:924, step:0 
model_pd.l_p.mean(): 0.08559082448482513 
model_pd.l_d.mean(): -20.249324798583984 
model_pd.lagr.mean(): -20.163734436035156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4905], device='cuda:0')), ('power', tensor([-20.9717], device='cuda:0'))])
epoch£º924	 i:0 	 global-step:18480	 l-p:0.08559082448482513
epoch£º924	 i:1 	 global-step:18481	 l-p:0.18143130838871002
epoch£º924	 i:2 	 global-step:18482	 l-p:0.14233052730560303
epoch£º924	 i:3 	 global-step:18483	 l-p:0.13328008353710175
epoch£º924	 i:4 	 global-step:18484	 l-p:0.12453226745128632
epoch£º924	 i:5 	 global-step:18485	 l-p:0.12470968067646027
epoch£º924	 i:6 	 global-step:18486	 l-p:0.20509478449821472
epoch£º924	 i:7 	 global-step:18487	 l-p:0.1073228120803833
epoch£º924	 i:8 	 global-step:18488	 l-p:0.08985481411218643
epoch£º924	 i:9 	 global-step:18489	 l-p:0.11309979856014252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:925
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1461, 5.1461, 5.1462],
        [5.1461, 5.1168, 4.8135],
        [5.1461, 4.9839, 4.6665],
        [5.1461, 4.9190, 4.6318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:925, step:0 
model_pd.l_p.mean(): 0.15714293718338013 
model_pd.l_d.mean(): -20.45099639892578 
model_pd.lagr.mean(): -20.293853759765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4523], device='cuda:0')), ('power', tensor([-21.1366], device='cuda:0'))])
epoch£º925	 i:0 	 global-step:18500	 l-p:0.15714293718338013
epoch£º925	 i:1 	 global-step:18501	 l-p:0.08499466627836227
epoch£º925	 i:2 	 global-step:18502	 l-p:0.12492626160383224
epoch£º925	 i:3 	 global-step:18503	 l-p:0.18069489300251007
epoch£º925	 i:4 	 global-step:18504	 l-p:0.11437232792377472
epoch£º925	 i:5 	 global-step:18505	 l-p:0.13434657454490662
epoch£º925	 i:6 	 global-step:18506	 l-p:0.1535119265317917
epoch£º925	 i:7 	 global-step:18507	 l-p:0.16209597885608673
epoch£º925	 i:8 	 global-step:18508	 l-p:0.1299423724412918
epoch£º925	 i:9 	 global-step:18509	 l-p:0.2051360011100769
====================================================================================================
====================================================================================================
====================================================================================================

epoch:926
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1235, 5.1234, 5.1235],
        [5.1235, 4.8517, 4.7220],
        [5.1235, 4.8500, 4.7060],
        [5.1235, 5.0268, 4.7072]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:926, step:0 
model_pd.l_p.mean(): 0.13151079416275024 
model_pd.l_d.mean(): -19.93901252746582 
model_pd.lagr.mean(): -19.8075008392334 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4576], device='cuda:0')), ('power', tensor([-20.6243], device='cuda:0'))])
epoch£º926	 i:0 	 global-step:18520	 l-p:0.13151079416275024
epoch£º926	 i:1 	 global-step:18521	 l-p:0.20642690360546112
epoch£º926	 i:2 	 global-step:18522	 l-p:0.11506843566894531
epoch£º926	 i:3 	 global-step:18523	 l-p:0.12379279732704163
epoch£º926	 i:4 	 global-step:18524	 l-p:0.10100238770246506
epoch£º926	 i:5 	 global-step:18525	 l-p:0.16219079494476318
epoch£º926	 i:6 	 global-step:18526	 l-p:0.17696574330329895
epoch£º926	 i:7 	 global-step:18527	 l-p:0.1125473603606224
epoch£º926	 i:8 	 global-step:18528	 l-p:0.23146182298660278
epoch£º926	 i:9 	 global-step:18529	 l-p:0.1265604943037033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:927
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1141, 5.1135, 5.1141],
        [5.1141, 5.0387, 5.0878],
        [5.1141, 5.1100, 5.1139],
        [5.1141, 5.0264, 5.0793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:927, step:0 
model_pd.l_p.mean(): 0.135043203830719 
model_pd.l_d.mean(): -18.623563766479492 
model_pd.lagr.mean(): -18.4885196685791 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5506], device='cuda:0')), ('power', tensor([-19.3896], device='cuda:0'))])
epoch£º927	 i:0 	 global-step:18540	 l-p:0.135043203830719
epoch£º927	 i:1 	 global-step:18541	 l-p:0.1611795425415039
epoch£º927	 i:2 	 global-step:18542	 l-p:0.12933073937892914
epoch£º927	 i:3 	 global-step:18543	 l-p:0.134820356965065
epoch£º927	 i:4 	 global-step:18544	 l-p:0.17003823816776276
epoch£º927	 i:5 	 global-step:18545	 l-p:0.11610078811645508
epoch£º927	 i:6 	 global-step:18546	 l-p:0.11895453929901123
epoch£º927	 i:7 	 global-step:18547	 l-p:0.2768927812576294
epoch£º927	 i:8 	 global-step:18548	 l-p:0.12208734452724457
epoch£º927	 i:9 	 global-step:18549	 l-p:0.16121722757816315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:928
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1125, 5.1125, 5.1125],
        [5.1125, 5.0031, 5.0599],
        [5.1125, 5.1124, 5.1125],
        [5.1125, 5.1107, 5.1124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:928, step:0 
model_pd.l_p.mean(): 0.11094744503498077 
model_pd.l_d.mean(): -20.422204971313477 
model_pd.lagr.mean(): -20.31125831604004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4491], device='cuda:0')), ('power', tensor([-21.1042], device='cuda:0'))])
epoch£º928	 i:0 	 global-step:18560	 l-p:0.11094744503498077
epoch£º928	 i:1 	 global-step:18561	 l-p:0.11811620742082596
epoch£º928	 i:2 	 global-step:18562	 l-p:0.1576148420572281
epoch£º928	 i:3 	 global-step:18563	 l-p:0.13852080702781677
epoch£º928	 i:4 	 global-step:18564	 l-p:0.18056432902812958
epoch£º928	 i:5 	 global-step:18565	 l-p:0.13176508247852325
epoch£º928	 i:6 	 global-step:18566	 l-p:0.18271715939044952
epoch£º928	 i:7 	 global-step:18567	 l-p:0.12066896259784698
epoch£º928	 i:8 	 global-step:18568	 l-p:0.1757030040025711
epoch£º928	 i:9 	 global-step:18569	 l-p:0.6026566028594971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:929
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0913, 5.0913, 5.0913],
        [5.0913, 5.0420, 5.0791],
        [5.0913, 4.8125, 4.6403],
        [5.0913, 5.0912, 5.0913]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:929, step:0 
model_pd.l_p.mean(): 0.0971868708729744 
model_pd.l_d.mean(): -20.64608383178711 
model_pd.lagr.mean(): -20.54889678955078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4479], device='cuda:0')), ('power', tensor([-21.3293], device='cuda:0'))])
epoch£º929	 i:0 	 global-step:18580	 l-p:0.0971868708729744
epoch£º929	 i:1 	 global-step:18581	 l-p:0.6563740968704224
epoch£º929	 i:2 	 global-step:18582	 l-p:0.22140994668006897
epoch£º929	 i:3 	 global-step:18583	 l-p:0.13467825949192047
epoch£º929	 i:4 	 global-step:18584	 l-p:0.09851765632629395
epoch£º929	 i:5 	 global-step:18585	 l-p:0.13903692364692688
epoch£º929	 i:6 	 global-step:18586	 l-p:0.15925373136997223
epoch£º929	 i:7 	 global-step:18587	 l-p:0.1348712295293808
epoch£º929	 i:8 	 global-step:18588	 l-p:0.1979171335697174
epoch£º929	 i:9 	 global-step:18589	 l-p:0.1364545077085495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:930
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0924, 5.0841, 5.0918],
        [5.0924, 4.9386, 4.6142],
        [5.0924, 5.0723, 5.0898],
        [5.0924, 5.0913, 5.0924]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:930, step:0 
model_pd.l_p.mean(): 0.10757426917552948 
model_pd.l_d.mean(): -20.564796447753906 
model_pd.lagr.mean(): -20.45722198486328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4658], device='cuda:0')), ('power', tensor([-21.2654], device='cuda:0'))])
epoch£º930	 i:0 	 global-step:18600	 l-p:0.10757426917552948
epoch£º930	 i:1 	 global-step:18601	 l-p:0.14044097065925598
epoch£º930	 i:2 	 global-step:18602	 l-p:0.18215908110141754
epoch£º930	 i:3 	 global-step:18603	 l-p:1.0709080696105957
epoch£º930	 i:4 	 global-step:18604	 l-p:0.21815814077854156
epoch£º930	 i:5 	 global-step:18605	 l-p:0.13780906796455383
epoch£º930	 i:6 	 global-step:18606	 l-p:0.18679872155189514
epoch£º930	 i:7 	 global-step:18607	 l-p:0.2567577064037323
epoch£º930	 i:8 	 global-step:18608	 l-p:0.09737709164619446
epoch£º930	 i:9 	 global-step:18609	 l-p:0.08636295050382614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:931
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1008, 5.0986, 5.1007],
        [5.1008, 4.8245, 4.6852],
        [5.1008, 5.1008, 5.1008],
        [5.1008, 4.9893, 5.0465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:931, step:0 
model_pd.l_p.mean(): 0.16742239892482758 
model_pd.l_d.mean(): -20.29622459411621 
model_pd.lagr.mean(): -20.128801345825195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4771], device='cuda:0')), ('power', tensor([-21.0054], device='cuda:0'))])
epoch£º931	 i:0 	 global-step:18620	 l-p:0.16742239892482758
epoch£º931	 i:1 	 global-step:18621	 l-p:0.1455955058336258
epoch£º931	 i:2 	 global-step:18622	 l-p:0.1644703447818756
epoch£º931	 i:3 	 global-step:18623	 l-p:0.20588897168636322
epoch£º931	 i:4 	 global-step:18624	 l-p:0.120920829474926
epoch£º931	 i:5 	 global-step:18625	 l-p:0.13275039196014404
epoch£º931	 i:6 	 global-step:18626	 l-p:0.10935761034488678
epoch£º931	 i:7 	 global-step:18627	 l-p:0.14510291814804077
epoch£º931	 i:8 	 global-step:18628	 l-p:0.2228877991437912
epoch£º931	 i:9 	 global-step:18629	 l-p:0.11863747239112854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:932
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1236, 4.9636, 4.6422],
        [5.1236, 5.0386, 5.0908],
        [5.1236, 4.8977, 4.8917],
        [5.1236, 5.1098, 5.1222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:932, step:0 
model_pd.l_p.mean(): 0.2072698473930359 
model_pd.l_d.mean(): -19.04498863220215 
model_pd.lagr.mean(): -18.837718963623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5720], device='cuda:0')), ('power', tensor([-19.8374], device='cuda:0'))])
epoch£º932	 i:0 	 global-step:18640	 l-p:0.2072698473930359
epoch£º932	 i:1 	 global-step:18641	 l-p:0.1671099066734314
epoch£º932	 i:2 	 global-step:18642	 l-p:0.12293573468923569
epoch£º932	 i:3 	 global-step:18643	 l-p:0.20422084629535675
epoch£º932	 i:4 	 global-step:18644	 l-p:0.09742719680070877
epoch£º932	 i:5 	 global-step:18645	 l-p:0.10597564280033112
epoch£º932	 i:6 	 global-step:18646	 l-p:0.12359490990638733
epoch£º932	 i:7 	 global-step:18647	 l-p:0.08331112563610077
epoch£º932	 i:8 	 global-step:18648	 l-p:0.13194678723812103
epoch£º932	 i:9 	 global-step:18649	 l-p:0.20495599508285522
====================================================================================================
====================================================================================================
====================================================================================================

epoch:933
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1308, 4.9849, 4.6625],
        [5.1308, 4.8810, 4.8313],
        [5.1308, 4.8563, 4.6706],
        [5.1308, 5.1308, 5.1308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:933, step:0 
model_pd.l_p.mean(): 0.13051611185073853 
model_pd.l_d.mean(): -20.61393928527832 
model_pd.lagr.mean(): -20.483423233032227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4569], device='cuda:0')), ('power', tensor([-21.3059], device='cuda:0'))])
epoch£º933	 i:0 	 global-step:18660	 l-p:0.13051611185073853
epoch£º933	 i:1 	 global-step:18661	 l-p:0.09990055859088898
epoch£º933	 i:2 	 global-step:18662	 l-p:0.15865366160869598
epoch£º933	 i:3 	 global-step:18663	 l-p:0.1259060651063919
epoch£º933	 i:4 	 global-step:18664	 l-p:0.15772578120231628
epoch£º933	 i:5 	 global-step:18665	 l-p:0.12266065180301666
epoch£º933	 i:6 	 global-step:18666	 l-p:0.2101183533668518
epoch£º933	 i:7 	 global-step:18667	 l-p:0.17836198210716248
epoch£º933	 i:8 	 global-step:18668	 l-p:0.10430111736059189
epoch£º933	 i:9 	 global-step:18669	 l-p:0.1554461121559143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:934
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1385, 5.1385, 5.1385],
        [5.1385, 5.1385, 5.1385],
        [5.1385, 4.9830, 5.0337],
        [5.1385, 5.0218, 5.0788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:934, step:0 
model_pd.l_p.mean(): 0.19095461070537567 
model_pd.l_d.mean(): -20.05409049987793 
model_pd.lagr.mean(): -19.863136291503906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4918], device='cuda:0')), ('power', tensor([-20.7757], device='cuda:0'))])
epoch£º934	 i:0 	 global-step:18680	 l-p:0.19095461070537567
epoch£º934	 i:1 	 global-step:18681	 l-p:0.1311843991279602
epoch£º934	 i:2 	 global-step:18682	 l-p:0.0947209894657135
epoch£º934	 i:3 	 global-step:18683	 l-p:0.1201867088675499
epoch£º934	 i:4 	 global-step:18684	 l-p:0.15399155020713806
epoch£º934	 i:5 	 global-step:18685	 l-p:0.1579383760690689
epoch£º934	 i:6 	 global-step:18686	 l-p:0.14365501701831818
epoch£º934	 i:7 	 global-step:18687	 l-p:0.11756578087806702
epoch£º934	 i:8 	 global-step:18688	 l-p:0.12805017828941345
epoch£º934	 i:9 	 global-step:18689	 l-p:0.13263855874538422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:935
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1465, 4.9247, 4.6298],
        [5.1465, 5.1320, 5.1450],
        [5.1465, 5.1387, 5.1460],
        [5.1465, 4.9950, 5.0469]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:935, step:0 
model_pd.l_p.mean(): 0.1592157781124115 
model_pd.l_d.mean(): -20.480087280273438 
model_pd.lagr.mean(): -20.320871353149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4702], device='cuda:0')), ('power', tensor([-21.1842], device='cuda:0'))])
epoch£º935	 i:0 	 global-step:18700	 l-p:0.1592157781124115
epoch£º935	 i:1 	 global-step:18701	 l-p:0.1257154494524002
epoch£º935	 i:2 	 global-step:18702	 l-p:0.14178994297981262
epoch£º935	 i:3 	 global-step:18703	 l-p:0.15058840811252594
epoch£º935	 i:4 	 global-step:18704	 l-p:0.11070574820041656
epoch£º935	 i:5 	 global-step:18705	 l-p:0.12992550432682037
epoch£º935	 i:6 	 global-step:18706	 l-p:0.15578964352607727
epoch£º935	 i:7 	 global-step:18707	 l-p:0.13604862987995148
epoch£º935	 i:8 	 global-step:18708	 l-p:0.17353317141532898
epoch£º935	 i:9 	 global-step:18709	 l-p:0.1313193440437317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:936
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1305, 5.0194, 5.0763],
        [5.1305, 5.0972, 5.1244],
        [5.1305, 4.8624, 4.7591],
        [5.1305, 5.2661, 5.0322]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:936, step:0 
model_pd.l_p.mean(): 0.16605308651924133 
model_pd.l_d.mean(): -19.761171340942383 
model_pd.lagr.mean(): -19.595117568969727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4878], device='cuda:0')), ('power', tensor([-20.4755], device='cuda:0'))])
epoch£º936	 i:0 	 global-step:18720	 l-p:0.16605308651924133
epoch£º936	 i:1 	 global-step:18721	 l-p:0.13515163958072662
epoch£º936	 i:2 	 global-step:18722	 l-p:0.1973268836736679
epoch£º936	 i:3 	 global-step:18723	 l-p:0.13730722665786743
epoch£º936	 i:4 	 global-step:18724	 l-p:0.1355249583721161
epoch£º936	 i:5 	 global-step:18725	 l-p:0.13887262344360352
epoch£º936	 i:6 	 global-step:18726	 l-p:0.11850357055664062
epoch£º936	 i:7 	 global-step:18727	 l-p:0.12242252379655838
epoch£º936	 i:8 	 global-step:18728	 l-p:0.12168622761964798
epoch£º936	 i:9 	 global-step:18729	 l-p:0.11485328525304794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:937
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1513, 4.8981, 4.8378],
        [5.1513, 5.1685, 4.8800],
        [5.1513, 4.9124, 4.8831],
        [5.1513, 5.1509, 5.1513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:937, step:0 
model_pd.l_p.mean(): 0.0867333710193634 
model_pd.l_d.mean(): -20.739734649658203 
model_pd.lagr.mean(): -20.65300178527832 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4265], device='cuda:0')), ('power', tensor([-21.4021], device='cuda:0'))])
epoch£º937	 i:0 	 global-step:18740	 l-p:0.0867333710193634
epoch£º937	 i:1 	 global-step:18741	 l-p:0.12127023190259933
epoch£º937	 i:2 	 global-step:18742	 l-p:0.13590511679649353
epoch£º937	 i:3 	 global-step:18743	 l-p:0.13457264006137848
epoch£º937	 i:4 	 global-step:18744	 l-p:0.12869040668010712
epoch£º937	 i:5 	 global-step:18745	 l-p:0.12513747811317444
epoch£º937	 i:6 	 global-step:18746	 l-p:0.15779638290405273
epoch£º937	 i:7 	 global-step:18747	 l-p:0.22211241722106934
epoch£º937	 i:8 	 global-step:18748	 l-p:0.12059108167886734
epoch£º937	 i:9 	 global-step:18749	 l-p:0.16038751602172852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:938
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1412, 5.1411, 5.1412],
        [5.1412, 5.1107, 5.1359],
        [5.1412, 4.8663, 4.7082],
        [5.1412, 5.0105, 5.0670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:938, step:0 
model_pd.l_p.mean(): 0.13194215297698975 
model_pd.l_d.mean(): -19.437252044677734 
model_pd.lagr.mean(): -19.305309295654297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4841], device='cuda:0')), ('power', tensor([-20.1443], device='cuda:0'))])
epoch£º938	 i:0 	 global-step:18760	 l-p:0.13194215297698975
epoch£º938	 i:1 	 global-step:18761	 l-p:0.16839492321014404
epoch£º938	 i:2 	 global-step:18762	 l-p:0.17099229991436005
epoch£º938	 i:3 	 global-step:18763	 l-p:0.10059898346662521
epoch£º938	 i:4 	 global-step:18764	 l-p:0.1569933146238327
epoch£º938	 i:5 	 global-step:18765	 l-p:0.16512100398540497
epoch£º938	 i:6 	 global-step:18766	 l-p:0.11692364513874054
epoch£º938	 i:7 	 global-step:18767	 l-p:0.05449024587869644
epoch£º938	 i:8 	 global-step:18768	 l-p:0.12100806832313538
epoch£º938	 i:9 	 global-step:18769	 l-p:0.1904786080121994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:939
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6033,  0.5098,  1.0000,  0.4308,
          1.0000,  0.8450, 31.6228]], device='cuda:0')
 pt:tensor([[5.1454, 5.3947, 5.2237],
        [5.1454, 4.9387, 4.6326],
        [5.1454, 4.8730, 4.7406],
        [5.1454, 5.0500, 4.7298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:939, step:0 
model_pd.l_p.mean(): 0.08962246775627136 
model_pd.l_d.mean(): -20.114788055419922 
model_pd.lagr.mean(): -20.025165557861328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4911], device='cuda:0')), ('power', tensor([-20.8363], device='cuda:0'))])
epoch£º939	 i:0 	 global-step:18780	 l-p:0.08962246775627136
epoch£º939	 i:1 	 global-step:18781	 l-p:0.11652839183807373
epoch£º939	 i:2 	 global-step:18782	 l-p:0.12363440543413162
epoch£º939	 i:3 	 global-step:18783	 l-p:0.16854701936244965
epoch£º939	 i:4 	 global-step:18784	 l-p:0.08782362937927246
epoch£º939	 i:5 	 global-step:18785	 l-p:0.1883973479270935
epoch£º939	 i:6 	 global-step:18786	 l-p:0.1146303117275238
epoch£º939	 i:7 	 global-step:18787	 l-p:0.16339446604251862
epoch£º939	 i:8 	 global-step:18788	 l-p:0.17960341274738312
epoch£º939	 i:9 	 global-step:18789	 l-p:0.1313980519771576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:940
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1522, 5.1511, 5.1522],
        [5.1522, 4.9465, 4.6407],
        [5.1522, 5.1377, 5.1507],
        [5.1522, 5.0763, 5.1255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:940, step:0 
model_pd.l_p.mean(): 0.13385269045829773 
model_pd.l_d.mean(): -19.64781379699707 
model_pd.lagr.mean(): -19.513961791992188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5396], device='cuda:0')), ('power', tensor([-20.4138], device='cuda:0'))])
epoch£º940	 i:0 	 global-step:18800	 l-p:0.13385269045829773
epoch£º940	 i:1 	 global-step:18801	 l-p:0.08688347786664963
epoch£º940	 i:2 	 global-step:18802	 l-p:0.14860793948173523
epoch£º940	 i:3 	 global-step:18803	 l-p:0.11223746836185455
epoch£º940	 i:4 	 global-step:18804	 l-p:0.17331355810165405
epoch£º940	 i:5 	 global-step:18805	 l-p:0.1089683324098587
epoch£º940	 i:6 	 global-step:18806	 l-p:0.16291846334934235
epoch£º940	 i:7 	 global-step:18807	 l-p:0.17129552364349365
epoch£º940	 i:8 	 global-step:18808	 l-p:0.08374536037445068
epoch£º940	 i:9 	 global-step:18809	 l-p:0.1659846007823944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:941
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1643, 5.0232, 4.7019],
        [5.1643, 5.1641, 5.1643],
        [5.1643, 4.9045, 4.8238],
        [5.1643, 4.9171, 4.6511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:941, step:0 
model_pd.l_p.mean(): 0.14388816058635712 
model_pd.l_d.mean(): -19.54527473449707 
model_pd.lagr.mean(): -19.401386260986328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4970], device='cuda:0')), ('power', tensor([-20.2666], device='cuda:0'))])
epoch£º941	 i:0 	 global-step:18820	 l-p:0.14388816058635712
epoch£º941	 i:1 	 global-step:18821	 l-p:0.12990154325962067
epoch£º941	 i:2 	 global-step:18822	 l-p:0.15019641816616058
epoch£º941	 i:3 	 global-step:18823	 l-p:0.09575282782316208
epoch£º941	 i:4 	 global-step:18824	 l-p:0.15145127475261688
epoch£º941	 i:5 	 global-step:18825	 l-p:0.16364246606826782
epoch£º941	 i:6 	 global-step:18826	 l-p:0.11008020490407944
epoch£º941	 i:7 	 global-step:18827	 l-p:0.12882623076438904
epoch£º941	 i:8 	 global-step:18828	 l-p:0.08189624547958374
epoch£º941	 i:9 	 global-step:18829	 l-p:0.12381118535995483
====================================================================================================
====================================================================================================
====================================================================================================

epoch:942
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1678, 4.9725, 4.9990],
        [5.1678, 5.5734, 5.5007],
        [5.1678, 4.9480, 4.6529],
        [5.1678, 5.1625, 5.1675]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:942, step:0 
model_pd.l_p.mean(): 0.12133384495973587 
model_pd.l_d.mean(): -20.32622718811035 
model_pd.lagr.mean(): -20.204893112182617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4407], device='cuda:0')), ('power', tensor([-20.9986], device='cuda:0'))])
epoch£º942	 i:0 	 global-step:18840	 l-p:0.12133384495973587
epoch£º942	 i:1 	 global-step:18841	 l-p:0.15506412088871002
epoch£º942	 i:2 	 global-step:18842	 l-p:0.12516918778419495
epoch£º942	 i:3 	 global-step:18843	 l-p:0.17316840589046478
epoch£º942	 i:4 	 global-step:18844	 l-p:0.1311488300561905
epoch£º942	 i:5 	 global-step:18845	 l-p:0.11675014346837997
epoch£º942	 i:6 	 global-step:18846	 l-p:0.12677505612373352
epoch£º942	 i:7 	 global-step:18847	 l-p:0.12403438240289688
epoch£º942	 i:8 	 global-step:18848	 l-p:0.17999663949012756
epoch£º942	 i:9 	 global-step:18849	 l-p:0.05848012864589691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:943
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1697, 5.1697, 5.1697],
        [5.1697, 4.9099, 4.8289],
        [5.1697, 5.1697, 5.1697],
        [5.1697, 5.1697, 5.1697]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:943, step:0 
model_pd.l_p.mean(): 0.12122587859630585 
model_pd.l_d.mean(): -20.382720947265625 
model_pd.lagr.mean(): -20.26149559020996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4181], device='cuda:0')), ('power', tensor([-21.0326], device='cuda:0'))])
epoch£º943	 i:0 	 global-step:18860	 l-p:0.12122587859630585
epoch£º943	 i:1 	 global-step:18861	 l-p:0.1371508687734604
epoch£º943	 i:2 	 global-step:18862	 l-p:0.13366751372814178
epoch£º943	 i:3 	 global-step:18863	 l-p:0.09552609175443649
epoch£º943	 i:4 	 global-step:18864	 l-p:0.14309100806713104
epoch£º943	 i:5 	 global-step:18865	 l-p:0.08880749344825745
epoch£º943	 i:6 	 global-step:18866	 l-p:0.15652447938919067
epoch£º943	 i:7 	 global-step:18867	 l-p:0.13002341985702515
epoch£º943	 i:8 	 global-step:18868	 l-p:0.11734195798635483
epoch£º943	 i:9 	 global-step:18869	 l-p:0.1619821935892105
====================================================================================================
====================================================================================================
====================================================================================================

epoch:944
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6128,  0.5205,  1.0000,  0.4421,
          1.0000,  0.8494, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1755,  0.0983,  1.0000,  0.0550,
          1.0000,  0.5599, 31.6228]], device='cuda:0')
 pt:tensor([[5.1810, 4.9285, 4.6745],
        [5.1810, 5.1038, 4.7873],
        [5.1810, 4.9381, 4.8985],
        [5.1810, 4.9964, 5.0309]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:944, step:0 
model_pd.l_p.mean(): 0.11381363868713379 
model_pd.l_d.mean(): -20.981212615966797 
model_pd.lagr.mean(): -20.867399215698242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3758], device='cuda:0')), ('power', tensor([-21.5944], device='cuda:0'))])
epoch£º944	 i:0 	 global-step:18880	 l-p:0.11381363868713379
epoch£º944	 i:1 	 global-step:18881	 l-p:0.10442918539047241
epoch£º944	 i:2 	 global-step:18882	 l-p:0.12017437815666199
epoch£º944	 i:3 	 global-step:18883	 l-p:0.14415743947029114
epoch£º944	 i:4 	 global-step:18884	 l-p:0.11532392352819443
epoch£º944	 i:5 	 global-step:18885	 l-p:0.11610624939203262
epoch£º944	 i:6 	 global-step:18886	 l-p:0.14351673424243927
epoch£º944	 i:7 	 global-step:18887	 l-p:0.15961308777332306
epoch£º944	 i:8 	 global-step:18888	 l-p:0.11771409958600998
epoch£º944	 i:9 	 global-step:18889	 l-p:0.1304677128791809
====================================================================================================
====================================================================================================
====================================================================================================

epoch:945
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1760, 5.0420, 4.7205],
        [5.1760, 5.1719, 5.1758],
        [5.1760, 5.0803, 5.1349],
        [5.1760, 5.1760, 5.1760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:945, step:0 
model_pd.l_p.mean(): 0.09894553571939468 
model_pd.l_d.mean(): -19.636241912841797 
model_pd.lagr.mean(): -19.537296295166016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4506], device='cuda:0')), ('power', tensor([-20.3112], device='cuda:0'))])
epoch£º945	 i:0 	 global-step:18900	 l-p:0.09894553571939468
epoch£º945	 i:1 	 global-step:18901	 l-p:0.12368360161781311
epoch£º945	 i:2 	 global-step:18902	 l-p:0.1262916773557663
epoch£º945	 i:3 	 global-step:18903	 l-p:0.06636473536491394
epoch£º945	 i:4 	 global-step:18904	 l-p:0.11341806501150131
epoch£º945	 i:5 	 global-step:18905	 l-p:0.1510503590106964
epoch£º945	 i:6 	 global-step:18906	 l-p:0.10464812815189362
epoch£º945	 i:7 	 global-step:18907	 l-p:0.14128854870796204
epoch£º945	 i:8 	 global-step:18908	 l-p:0.15640035271644592
epoch£º945	 i:9 	 global-step:18909	 l-p:0.2834621071815491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:946
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1419, 5.0002, 4.6760],
        [5.1419, 5.1256, 4.8238],
        [5.1419, 5.1418, 5.1419],
        [5.1419, 5.5825, 5.5337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:946, step:0 
model_pd.l_p.mean(): 0.21358056366443634 
model_pd.l_d.mean(): -19.6926212310791 
model_pd.lagr.mean(): -19.479040145874023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5318], device='cuda:0')), ('power', tensor([-20.4511], device='cuda:0'))])
epoch£º946	 i:0 	 global-step:18920	 l-p:0.21358056366443634
epoch£º946	 i:1 	 global-step:18921	 l-p:0.12290653586387634
epoch£º946	 i:2 	 global-step:18922	 l-p:0.13026700913906097
epoch£º946	 i:3 	 global-step:18923	 l-p:0.10167869180440903
epoch£º946	 i:4 	 global-step:18924	 l-p:0.12975537776947021
epoch£º946	 i:5 	 global-step:18925	 l-p:0.12997551262378693
epoch£º946	 i:6 	 global-step:18926	 l-p:0.20235365629196167
epoch£º946	 i:7 	 global-step:18927	 l-p:0.10003679245710373
epoch£º946	 i:8 	 global-step:18928	 l-p:0.14173918962478638
epoch£º946	 i:9 	 global-step:18929	 l-p:0.12102830410003662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:947
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1377, 5.1241, 5.1364],
        [5.1377, 5.1356, 5.1377],
        [5.1377, 5.1377, 5.1377],
        [5.1377, 5.1377, 5.1377]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:947, step:0 
model_pd.l_p.mean(): 0.21255117654800415 
model_pd.l_d.mean(): -20.84613800048828 
model_pd.lagr.mean(): -20.633586883544922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4212], device='cuda:0')), ('power', tensor([-21.5042], device='cuda:0'))])
epoch£º947	 i:0 	 global-step:18940	 l-p:0.21255117654800415
epoch£º947	 i:1 	 global-step:18941	 l-p:0.15970733761787415
epoch£º947	 i:2 	 global-step:18942	 l-p:0.1280539333820343
epoch£º947	 i:3 	 global-step:18943	 l-p:0.12143762409687042
epoch£º947	 i:4 	 global-step:18944	 l-p:0.10375729948282242
epoch£º947	 i:5 	 global-step:18945	 l-p:0.15004870295524597
epoch£º947	 i:6 	 global-step:18946	 l-p:0.14553818106651306
epoch£º947	 i:7 	 global-step:18947	 l-p:0.10192970186471939
epoch£º947	 i:8 	 global-step:18948	 l-p:0.1504438817501068
epoch£º947	 i:9 	 global-step:18949	 l-p:0.17691917717456818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:948
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1281, 5.1507, 4.8630],
        [5.1281, 5.0534, 5.1024],
        [5.1281, 5.1280, 5.1281],
        [5.1281, 4.9927, 5.0494]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:948, step:0 
model_pd.l_p.mean(): 0.11776331812143326 
model_pd.l_d.mean(): -19.349945068359375 
model_pd.lagr.mean(): -19.232181549072266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5836], device='cuda:0')), ('power', tensor([-20.1576], device='cuda:0'))])
epoch£º948	 i:0 	 global-step:18960	 l-p:0.11776331812143326
epoch£º948	 i:1 	 global-step:18961	 l-p:0.12182357162237167
epoch£º948	 i:2 	 global-step:18962	 l-p:0.1182774230837822
epoch£º948	 i:3 	 global-step:18963	 l-p:0.1526036560535431
epoch£º948	 i:4 	 global-step:18964	 l-p:0.14727342128753662
epoch£º948	 i:5 	 global-step:18965	 l-p:0.17589148879051208
epoch£º948	 i:6 	 global-step:18966	 l-p:0.16254660487174988
epoch£º948	 i:7 	 global-step:18967	 l-p:0.07160482555627823
epoch£º948	 i:8 	 global-step:18968	 l-p:0.1430550515651703
epoch£º948	 i:9 	 global-step:18969	 l-p:0.21880348026752472
====================================================================================================
====================================================================================================
====================================================================================================

epoch:949
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1344, 5.1333, 5.1344],
        [5.1344, 5.0157, 5.0733],
        [5.1344, 5.0575, 5.1073],
        [5.1344, 5.0799, 5.1199]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:949, step:0 
model_pd.l_p.mean(): 0.12749679386615753 
model_pd.l_d.mean(): -20.787534713745117 
model_pd.lagr.mean(): -20.660037994384766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4083], device='cuda:0')), ('power', tensor([-21.4318], device='cuda:0'))])
epoch£º949	 i:0 	 global-step:18980	 l-p:0.12749679386615753
epoch£º949	 i:1 	 global-step:18981	 l-p:0.10274764895439148
epoch£º949	 i:2 	 global-step:18982	 l-p:0.1603332906961441
epoch£º949	 i:3 	 global-step:18983	 l-p:0.1508721262216568
epoch£º949	 i:4 	 global-step:18984	 l-p:0.21483546495437622
epoch£º949	 i:5 	 global-step:18985	 l-p:0.18167941272258759
epoch£º949	 i:6 	 global-step:18986	 l-p:0.1292276233434677
epoch£º949	 i:7 	 global-step:18987	 l-p:0.08614911884069443
epoch£º949	 i:8 	 global-step:18988	 l-p:0.15115907788276672
epoch£º949	 i:9 	 global-step:18989	 l-p:0.12386504560709
====================================================================================================
====================================================================================================
====================================================================================================

epoch:950
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1503, 5.1424, 5.1497],
        [5.1503, 4.8739, 4.7152],
        [5.1503, 4.8839, 4.6474],
        [5.1503, 5.1179, 5.1445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:950, step:0 
model_pd.l_p.mean(): 0.1165679320693016 
model_pd.l_d.mean(): -19.73138999938965 
model_pd.lagr.mean(): -19.614822387695312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5009], device='cuda:0')), ('power', tensor([-20.4587], device='cuda:0'))])
epoch£º950	 i:0 	 global-step:19000	 l-p:0.1165679320693016
epoch£º950	 i:1 	 global-step:19001	 l-p:0.19745966792106628
epoch£º950	 i:2 	 global-step:19002	 l-p:0.1555013656616211
epoch£º950	 i:3 	 global-step:19003	 l-p:0.13818389177322388
epoch£º950	 i:4 	 global-step:19004	 l-p:0.1129065677523613
epoch£º950	 i:5 	 global-step:19005	 l-p:0.1496249884366989
epoch£º950	 i:6 	 global-step:19006	 l-p:0.16983088850975037
epoch£º950	 i:7 	 global-step:19007	 l-p:0.11340700089931488
epoch£º950	 i:8 	 global-step:19008	 l-p:0.1361631602048874
epoch£º950	 i:9 	 global-step:19009	 l-p:0.09226769953966141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:951
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1401, 4.9645, 5.0072],
        [5.1401, 5.1401, 5.1401],
        [5.1401, 5.1401, 5.1401],
        [5.1401, 4.8624, 4.7037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:951, step:0 
model_pd.l_p.mean(): 0.2178977131843567 
model_pd.l_d.mean(): -19.063377380371094 
model_pd.lagr.mean(): -18.84547996520996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5331], device='cuda:0')), ('power', tensor([-19.8163], device='cuda:0'))])
epoch£º951	 i:0 	 global-step:19020	 l-p:0.2178977131843567
epoch£º951	 i:1 	 global-step:19021	 l-p:0.13878189027309418
epoch£º951	 i:2 	 global-step:19022	 l-p:0.13207502663135529
epoch£º951	 i:3 	 global-step:19023	 l-p:0.08902972936630249
epoch£º951	 i:4 	 global-step:19024	 l-p:0.12836359441280365
epoch£º951	 i:5 	 global-step:19025	 l-p:0.15954065322875977
epoch£º951	 i:6 	 global-step:19026	 l-p:0.16311317682266235
epoch£º951	 i:7 	 global-step:19027	 l-p:0.13879890739917755
epoch£º951	 i:8 	 global-step:19028	 l-p:0.11240848898887634
epoch£º951	 i:9 	 global-step:19029	 l-p:0.12607212364673615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:952
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1380, 5.1319, 5.1377],
        [5.1380, 4.9876, 4.6624],
        [5.1380, 5.0964, 5.1290],
        [5.1380, 5.1147, 5.1347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:952, step:0 
model_pd.l_p.mean(): 0.1187979131937027 
model_pd.l_d.mean(): -20.348024368286133 
model_pd.lagr.mean(): -20.22922706604004 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4623], device='cuda:0')), ('power', tensor([-21.0427], device='cuda:0'))])
epoch£º952	 i:0 	 global-step:19040	 l-p:0.1187979131937027
epoch£º952	 i:1 	 global-step:19041	 l-p:0.15058813989162445
epoch£º952	 i:2 	 global-step:19042	 l-p:0.11438533663749695
epoch£º952	 i:3 	 global-step:19043	 l-p:0.1612040251493454
epoch£º952	 i:4 	 global-step:19044	 l-p:0.1871681809425354
epoch£º952	 i:5 	 global-step:19045	 l-p:0.1393052041530609
epoch£º952	 i:6 	 global-step:19046	 l-p:0.11780603975057602
epoch£º952	 i:7 	 global-step:19047	 l-p:0.1271972954273224
epoch£º952	 i:8 	 global-step:19048	 l-p:0.1327681541442871
epoch£º952	 i:9 	 global-step:19049	 l-p:0.1915416270494461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:953
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1227, 4.8431, 4.6526],
        [5.1227, 5.1226, 5.1227],
        [5.1227, 5.1226, 5.1227],
        [5.1227, 5.1189, 5.1225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:953, step:0 
model_pd.l_p.mean(): 0.10162397474050522 
model_pd.l_d.mean(): -20.4505558013916 
model_pd.lagr.mean(): -20.34893226623535 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4791], device='cuda:0')), ('power', tensor([-21.1635], device='cuda:0'))])
epoch£º953	 i:0 	 global-step:19060	 l-p:0.10162397474050522
epoch£º953	 i:1 	 global-step:19061	 l-p:0.22584496438503265
epoch£º953	 i:2 	 global-step:19062	 l-p:0.17164982855319977
epoch£º953	 i:3 	 global-step:19063	 l-p:0.17965325713157654
epoch£º953	 i:4 	 global-step:19064	 l-p:0.15858933329582214
epoch£º953	 i:5 	 global-step:19065	 l-p:0.13385812938213348
epoch£º953	 i:6 	 global-step:19066	 l-p:0.09878520667552948
epoch£º953	 i:7 	 global-step:19067	 l-p:0.10910814255475998
epoch£º953	 i:8 	 global-step:19068	 l-p:0.20907847583293915
epoch£º953	 i:9 	 global-step:19069	 l-p:0.16717681288719177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:954
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1068, 5.0390, 5.0854],
        [5.1068, 5.4779, 5.3826],
        [5.1068, 5.1039, 5.1067],
        [5.1068, 4.9180, 4.9542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:954, step:0 
model_pd.l_p.mean(): 0.14574378728866577 
model_pd.l_d.mean(): -19.84201431274414 
model_pd.lagr.mean(): -19.696269989013672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5624], device='cuda:0')), ('power', tensor([-20.6334], device='cuda:0'))])
epoch£º954	 i:0 	 global-step:19080	 l-p:0.14574378728866577
epoch£º954	 i:1 	 global-step:19081	 l-p:0.1105252057313919
epoch£º954	 i:2 	 global-step:19082	 l-p:0.30123430490493774
epoch£º954	 i:3 	 global-step:19083	 l-p:0.2133903056383133
epoch£º954	 i:4 	 global-step:19084	 l-p:0.09770022332668304
epoch£º954	 i:5 	 global-step:19085	 l-p:0.1233142837882042
epoch£º954	 i:6 	 global-step:19086	 l-p:0.24810874462127686
epoch£º954	 i:7 	 global-step:19087	 l-p:0.10532642155885696
epoch£º954	 i:8 	 global-step:19088	 l-p:0.16243042051792145
epoch£º954	 i:9 	 global-step:19089	 l-p:0.12967105209827423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:955
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1057, 5.1057, 5.1057],
        [5.1057, 5.4762, 5.3805],
        [5.1057, 5.0269, 5.0776],
        [5.1057, 5.0641, 5.0967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:955, step:0 
model_pd.l_p.mean(): 0.0962308943271637 
model_pd.l_d.mean(): -20.19759178161621 
model_pd.lagr.mean(): -20.101360321044922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5163], device='cuda:0')), ('power', tensor([-20.9458], device='cuda:0'))])
epoch£º955	 i:0 	 global-step:19100	 l-p:0.0962308943271637
epoch£º955	 i:1 	 global-step:19101	 l-p:0.116981141269207
epoch£º955	 i:2 	 global-step:19102	 l-p:0.09438972920179367
epoch£º955	 i:3 	 global-step:19103	 l-p:-148.85240173339844
epoch£º955	 i:4 	 global-step:19104	 l-p:0.20291437208652496
epoch£º955	 i:5 	 global-step:19105	 l-p:0.13263477385044098
epoch£º955	 i:6 	 global-step:19106	 l-p:0.18594934046268463
epoch£º955	 i:7 	 global-step:19107	 l-p:0.19839942455291748
epoch£º955	 i:8 	 global-step:19108	 l-p:0.15156646072864532
epoch£º955	 i:9 	 global-step:19109	 l-p:0.49916285276412964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:956
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0745, 5.0385, 4.7281],
        [5.0745, 5.0738, 5.0745],
        [5.0745, 5.0733, 5.0744],
        [5.0745, 4.7889, 4.6303]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:956, step:0 
model_pd.l_p.mean(): 0.1486387401819229 
model_pd.l_d.mean(): -20.99659538269043 
model_pd.lagr.mean(): -20.84795570373535 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4182], device='cuda:0')), ('power', tensor([-21.6533], device='cuda:0'))])
epoch£º956	 i:0 	 global-step:19120	 l-p:0.1486387401819229
epoch£º956	 i:1 	 global-step:19121	 l-p:0.16047419607639313
epoch£º956	 i:2 	 global-step:19122	 l-p:0.12310567498207092
epoch£º956	 i:3 	 global-step:19123	 l-p:0.18299323320388794
epoch£º956	 i:4 	 global-step:19124	 l-p:0.12591953575611115
epoch£º956	 i:5 	 global-step:19125	 l-p:0.16200225055217743
epoch£º956	 i:6 	 global-step:19126	 l-p:0.5134000778198242
epoch£º956	 i:7 	 global-step:19127	 l-p:0.1267133355140686
epoch£º956	 i:8 	 global-step:19128	 l-p:-0.09949330240488052
epoch£º956	 i:9 	 global-step:19129	 l-p:0.2233377993106842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:957
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0877, 5.0873, 5.0877],
        [5.0877, 4.9095, 4.9534],
        [5.0877, 5.0659, 5.0847],
        [5.0877, 5.0642, 5.0843]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:957, step:0 
model_pd.l_p.mean(): 0.16312310099601746 
model_pd.l_d.mean(): -20.727907180786133 
model_pd.lagr.mean(): -20.56478500366211 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4548], device='cuda:0')), ('power', tensor([-21.4191], device='cuda:0'))])
epoch£º957	 i:0 	 global-step:19140	 l-p:0.16312310099601746
epoch£º957	 i:1 	 global-step:19141	 l-p:0.13503451645374298
epoch£º957	 i:2 	 global-step:19142	 l-p:0.20632793009281158
epoch£º957	 i:3 	 global-step:19143	 l-p:0.11171382665634155
epoch£º957	 i:4 	 global-step:19144	 l-p:0.09642469137907028
epoch£º957	 i:5 	 global-step:19145	 l-p:0.11949226260185242
epoch£º957	 i:6 	 global-step:19146	 l-p:0.15368671715259552
epoch£º957	 i:7 	 global-step:19147	 l-p:0.21385276317596436
epoch£º957	 i:8 	 global-step:19148	 l-p:0.3276558518409729
epoch£º957	 i:9 	 global-step:19149	 l-p:0.1982988566160202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:958
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1105, 5.0871, 5.1072],
        [5.1105, 5.0395, 5.0873],
        [5.1105, 5.0317, 5.0824],
        [5.1105, 5.0046, 4.6782]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:958, step:0 
model_pd.l_p.mean(): 0.14017212390899658 
model_pd.l_d.mean(): -20.846128463745117 
model_pd.lagr.mean(): -20.705955505371094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4178], device='cuda:0')), ('power', tensor([-21.5008], device='cuda:0'))])
epoch£º958	 i:0 	 global-step:19160	 l-p:0.14017212390899658
epoch£º958	 i:1 	 global-step:19161	 l-p:0.2518761456012726
epoch£º958	 i:2 	 global-step:19162	 l-p:0.12306442111730576
epoch£º958	 i:3 	 global-step:19163	 l-p:0.17492841184139252
epoch£º958	 i:4 	 global-step:19164	 l-p:0.15939480066299438
epoch£º958	 i:5 	 global-step:19165	 l-p:0.10257129371166229
epoch£º958	 i:6 	 global-step:19166	 l-p:0.10997245460748672
epoch£º958	 i:7 	 global-step:19167	 l-p:0.16536742448806763
epoch£º958	 i:8 	 global-step:19168	 l-p:0.1273118406534195
epoch£º958	 i:9 	 global-step:19169	 l-p:0.13843253254890442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:959
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1711, 4.9925, 4.6739],
        [5.1711, 5.1711, 5.1711],
        [5.1711, 4.9357, 4.6491],
        [5.1711, 5.1682, 5.1710]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:959, step:0 
model_pd.l_p.mean(): 0.05556810647249222 
model_pd.l_d.mean(): -20.572607040405273 
model_pd.lagr.mean(): -20.517038345336914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4373], device='cuda:0')), ('power', tensor([-21.2442], device='cuda:0'))])
epoch£º959	 i:0 	 global-step:19180	 l-p:0.05556810647249222
epoch£º959	 i:1 	 global-step:19181	 l-p:0.1503414511680603
epoch£º959	 i:2 	 global-step:19182	 l-p:0.16560542583465576
epoch£º959	 i:3 	 global-step:19183	 l-p:0.13678130507469177
epoch£º959	 i:4 	 global-step:19184	 l-p:0.12186610698699951
epoch£º959	 i:5 	 global-step:19185	 l-p:0.12580889463424683
epoch£º959	 i:6 	 global-step:19186	 l-p:0.13836251199245453
epoch£º959	 i:7 	 global-step:19187	 l-p:0.13874179124832153
epoch£º959	 i:8 	 global-step:19188	 l-p:0.11566540598869324
epoch£º959	 i:9 	 global-step:19189	 l-p:0.09969345480203629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:960
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1998, 5.1997, 5.1998],
        [5.1998, 5.1962, 5.1997],
        [5.1998, 5.5038, 5.3629],
        [5.1998, 5.4924, 5.3445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:960, step:0 
model_pd.l_p.mean(): 0.10651368647813797 
model_pd.l_d.mean(): -19.579511642456055 
model_pd.lagr.mean(): -19.472997665405273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5015], device='cuda:0')), ('power', tensor([-20.3058], device='cuda:0'))])
epoch£º960	 i:0 	 global-step:19200	 l-p:0.10651368647813797
epoch£º960	 i:1 	 global-step:19201	 l-p:0.16429179906845093
epoch£º960	 i:2 	 global-step:19202	 l-p:0.1297416239976883
epoch£º960	 i:3 	 global-step:19203	 l-p:0.12039880454540253
epoch£º960	 i:4 	 global-step:19204	 l-p:0.136735737323761
epoch£º960	 i:5 	 global-step:19205	 l-p:0.10925092548131943
epoch£º960	 i:6 	 global-step:19206	 l-p:0.11767023056745529
epoch£º960	 i:7 	 global-step:19207	 l-p:0.10526105761528015
epoch£º960	 i:8 	 global-step:19208	 l-p:0.1250717043876648
epoch£º960	 i:9 	 global-step:19209	 l-p:0.11676975339651108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:961
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1799, 5.0361, 4.7123],
        [5.1799, 5.1064, 5.1548],
        [5.1799, 5.0149, 4.6937],
        [5.1799, 5.1749, 5.1796]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:961, step:0 
model_pd.l_p.mean(): 0.12772119045257568 
model_pd.l_d.mean(): -19.579267501831055 
model_pd.lagr.mean(): -19.45154571533203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5754], device='cuda:0')), ('power', tensor([-20.3811], device='cuda:0'))])
epoch£º961	 i:0 	 global-step:19220	 l-p:0.12772119045257568
epoch£º961	 i:1 	 global-step:19221	 l-p:0.12163551151752472
epoch£º961	 i:2 	 global-step:19222	 l-p:0.169041708111763
epoch£º961	 i:3 	 global-step:19223	 l-p:0.13297677040100098
epoch£º961	 i:4 	 global-step:19224	 l-p:0.16711637377738953
epoch£º961	 i:5 	 global-step:19225	 l-p:0.12400127202272415
epoch£º961	 i:6 	 global-step:19226	 l-p:0.10734906792640686
epoch£º961	 i:7 	 global-step:19227	 l-p:0.1354978084564209
epoch£º961	 i:8 	 global-step:19228	 l-p:0.12002786993980408
epoch£º961	 i:9 	 global-step:19229	 l-p:0.12126729637384415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:962
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1555, 5.1069, 5.1436],
        [5.1555, 5.1555, 5.1555],
        [5.1555, 5.1541, 5.1554],
        [5.1555, 5.0548, 5.1109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:962, step:0 
model_pd.l_p.mean(): 0.1140131726861 
model_pd.l_d.mean(): -20.83733367919922 
model_pd.lagr.mean(): -20.72332000732422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4018], device='cuda:0')), ('power', tensor([-21.4755], device='cuda:0'))])
epoch£º962	 i:0 	 global-step:19240	 l-p:0.1140131726861
epoch£º962	 i:1 	 global-step:19241	 l-p:0.09620145708322525
epoch£º962	 i:2 	 global-step:19242	 l-p:0.12822209298610687
epoch£º962	 i:3 	 global-step:19243	 l-p:0.14525103569030762
epoch£º962	 i:4 	 global-step:19244	 l-p:0.10701067000627518
epoch£º962	 i:5 	 global-step:19245	 l-p:0.23448446393013
epoch£º962	 i:6 	 global-step:19246	 l-p:0.1099928617477417
epoch£º962	 i:7 	 global-step:19247	 l-p:0.3065716624259949
epoch£º962	 i:8 	 global-step:19248	 l-p:0.1579389125108719
epoch£º962	 i:9 	 global-step:19249	 l-p:0.1780613213777542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:963
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1057, 5.1032, 5.1056],
        [5.1057, 5.1041, 5.1056],
        [5.1057, 5.0503, 5.0908],
        [5.1057, 5.0618, 5.0958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:963, step:0 
model_pd.l_p.mean(): 0.12297407537698746 
model_pd.l_d.mean(): -20.072538375854492 
model_pd.lagr.mean(): -19.94956398010254 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4972], device='cuda:0')), ('power', tensor([-20.7999], device='cuda:0'))])
epoch£º963	 i:0 	 global-step:19260	 l-p:0.12297407537698746
epoch£º963	 i:1 	 global-step:19261	 l-p:0.1396215856075287
epoch£º963	 i:2 	 global-step:19262	 l-p:0.13058429956436157
epoch£º963	 i:3 	 global-step:19263	 l-p:0.08844397962093353
epoch£º963	 i:4 	 global-step:19264	 l-p:0.19184228777885437
epoch£º963	 i:5 	 global-step:19265	 l-p:0.29360097646713257
epoch£º963	 i:6 	 global-step:19266	 l-p:-0.49616479873657227
epoch£º963	 i:7 	 global-step:19267	 l-p:0.1576470285654068
epoch£º963	 i:8 	 global-step:19268	 l-p:0.2630844712257385
epoch£º963	 i:9 	 global-step:19269	 l-p:0.11809565871953964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:964
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0842, 5.0825, 5.0841],
        [5.0842, 5.0838, 5.0842],
        [5.0842, 5.5049, 5.4423],
        [5.0842, 4.8675, 4.5510]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:964, step:0 
model_pd.l_p.mean(): 0.2151343673467636 
model_pd.l_d.mean(): -18.611183166503906 
model_pd.lagr.mean(): -18.39604949951172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6019], device='cuda:0')), ('power', tensor([-19.4295], device='cuda:0'))])
epoch£º964	 i:0 	 global-step:19280	 l-p:0.2151343673467636
epoch£º964	 i:1 	 global-step:19281	 l-p:0.2153387814760208
epoch£º964	 i:2 	 global-step:19282	 l-p:0.15758292376995087
epoch£º964	 i:3 	 global-step:19283	 l-p:0.2862611413002014
epoch£º964	 i:4 	 global-step:19284	 l-p:0.15120860934257507
epoch£º964	 i:5 	 global-step:19285	 l-p:18.503164291381836
epoch£º964	 i:6 	 global-step:19286	 l-p:0.13465401530265808
epoch£º964	 i:7 	 global-step:19287	 l-p:0.16304011642932892
epoch£º964	 i:8 	 global-step:19288	 l-p:0.10710550844669342
epoch£º964	 i:9 	 global-step:19289	 l-p:0.07430212199687958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:965
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[5.0986, 4.8595, 4.5603],
        [5.0986, 4.8250, 4.7303],
        [5.0986, 4.8614, 4.5606],
        [5.0986, 4.8418, 4.7934]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:965, step:0 
model_pd.l_p.mean(): 0.1900932341814041 
model_pd.l_d.mean(): -20.532472610473633 
model_pd.lagr.mean(): -20.342378616333008 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4567], device='cuda:0')), ('power', tensor([-21.2234], device='cuda:0'))])
epoch£º965	 i:0 	 global-step:19300	 l-p:0.1900932341814041
epoch£º965	 i:1 	 global-step:19301	 l-p:0.14647291600704193
epoch£º965	 i:2 	 global-step:19302	 l-p:0.1445789486169815
epoch£º965	 i:3 	 global-step:19303	 l-p:0.09684757143259048
epoch£º965	 i:4 	 global-step:19304	 l-p:0.1314963698387146
epoch£º965	 i:5 	 global-step:19305	 l-p:0.2337651252746582
epoch£º965	 i:6 	 global-step:19306	 l-p:0.2394697517156601
epoch£º965	 i:7 	 global-step:19307	 l-p:0.1695154905319214
epoch£º965	 i:8 	 global-step:19308	 l-p:0.1302880495786667
epoch£º965	 i:9 	 global-step:19309	 l-p:0.07824306190013885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:966
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1387, 5.2437, 4.9915],
        [5.1387, 5.1382, 5.1387],
        [5.1387, 5.2138, 4.9472],
        [5.1387, 4.8897, 4.8509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:966, step:0 
model_pd.l_p.mean(): 0.22258567810058594 
model_pd.l_d.mean(): -19.515663146972656 
model_pd.lagr.mean(): -19.29307746887207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5472], device='cuda:0')), ('power', tensor([-20.2879], device='cuda:0'))])
epoch£º966	 i:0 	 global-step:19320	 l-p:0.22258567810058594
epoch£º966	 i:1 	 global-step:19321	 l-p:0.11322485655546188
epoch£º966	 i:2 	 global-step:19322	 l-p:0.07509870827198029
epoch£º966	 i:3 	 global-step:19323	 l-p:0.1250758171081543
epoch£º966	 i:4 	 global-step:19324	 l-p:0.1330118030309677
epoch£º966	 i:5 	 global-step:19325	 l-p:0.14634552597999573
epoch£º966	 i:6 	 global-step:19326	 l-p:0.14067581295967102
epoch£º966	 i:7 	 global-step:19327	 l-p:0.12015439569950104
epoch£º966	 i:8 	 global-step:19328	 l-p:0.14088289439678192
epoch£º966	 i:9 	 global-step:19329	 l-p:0.18049699068069458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:967
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1466, 5.1465, 5.1466],
        [5.1466, 5.1466, 5.1466],
        [5.1466, 5.1466, 5.1466],
        [5.1466, 4.8763, 4.6385]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:967, step:0 
model_pd.l_p.mean(): 0.16333335638046265 
model_pd.l_d.mean(): -20.839330673217773 
model_pd.lagr.mean(): -20.675996780395508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4358], device='cuda:0')), ('power', tensor([-21.5123], device='cuda:0'))])
epoch£º967	 i:0 	 global-step:19340	 l-p:0.16333335638046265
epoch£º967	 i:1 	 global-step:19341	 l-p:0.14187170565128326
epoch£º967	 i:2 	 global-step:19342	 l-p:0.15925781428813934
epoch£º967	 i:3 	 global-step:19343	 l-p:0.1229848638176918
epoch£º967	 i:4 	 global-step:19344	 l-p:0.11283215880393982
epoch£º967	 i:5 	 global-step:19345	 l-p:0.1307927668094635
epoch£º967	 i:6 	 global-step:19346	 l-p:0.16680513322353363
epoch£º967	 i:7 	 global-step:19347	 l-p:0.16524110734462738
epoch£º967	 i:8 	 global-step:19348	 l-p:0.16547951102256775
epoch£º967	 i:9 	 global-step:19349	 l-p:0.12749341130256653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:968
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1344, 5.0665, 4.7470],
        [5.1344, 4.9794, 4.6515],
        [5.1344, 5.0480, 5.1011],
        [5.1344, 4.8638, 4.6207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:968, step:0 
model_pd.l_p.mean(): 0.0905892550945282 
model_pd.l_d.mean(): -20.287960052490234 
model_pd.lagr.mean(): -20.197370529174805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5094], device='cuda:0')), ('power', tensor([-21.0301], device='cuda:0'))])
epoch£º968	 i:0 	 global-step:19360	 l-p:0.0905892550945282
epoch£º968	 i:1 	 global-step:19361	 l-p:0.19013579189777374
epoch£º968	 i:2 	 global-step:19362	 l-p:0.11258595436811447
epoch£º968	 i:3 	 global-step:19363	 l-p:0.15493017435073853
epoch£º968	 i:4 	 global-step:19364	 l-p:0.09541045874357224
epoch£º968	 i:5 	 global-step:19365	 l-p:0.16546055674552917
epoch£º968	 i:6 	 global-step:19366	 l-p:0.10617177933454514
epoch£º968	 i:7 	 global-step:19367	 l-p:0.15008656680583954
epoch£º968	 i:8 	 global-step:19368	 l-p:0.2043219357728958
epoch£º968	 i:9 	 global-step:19369	 l-p:0.14209672808647156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:969
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1447, 4.8879, 4.6185],
        [5.1447, 5.2098, 4.9382],
        [5.1447, 4.9888, 4.6616],
        [5.1447, 5.0212, 5.0792]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:969, step:0 
model_pd.l_p.mean(): 0.15547499060630798 
model_pd.l_d.mean(): -20.949983596801758 
model_pd.lagr.mean(): -20.79450798034668 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3918], device='cuda:0')), ('power', tensor([-21.5792], device='cuda:0'))])
epoch£º969	 i:0 	 global-step:19380	 l-p:0.15547499060630798
epoch£º969	 i:1 	 global-step:19381	 l-p:0.1326770782470703
epoch£º969	 i:2 	 global-step:19382	 l-p:0.11917775124311447
epoch£º969	 i:3 	 global-step:19383	 l-p:0.12038899958133698
epoch£º969	 i:4 	 global-step:19384	 l-p:0.1442895531654358
epoch£º969	 i:5 	 global-step:19385	 l-p:0.20392005145549774
epoch£º969	 i:6 	 global-step:19386	 l-p:0.12422551214694977
epoch£º969	 i:7 	 global-step:19387	 l-p:0.1365908533334732
epoch£º969	 i:8 	 global-step:19388	 l-p:0.20137858390808105
epoch£º969	 i:9 	 global-step:19389	 l-p:0.09847627580165863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:970
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1433, 5.1432, 5.1433],
        [5.1433, 4.9130, 4.9064],
        [5.1433, 4.9898, 5.0433],
        [5.1433, 4.8697, 4.7629]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:970, step:0 
model_pd.l_p.mean(): 0.12282752990722656 
model_pd.l_d.mean(): -20.658349990844727 
model_pd.lagr.mean(): -20.5355224609375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4408], device='cuda:0')), ('power', tensor([-21.3344], device='cuda:0'))])
epoch£º970	 i:0 	 global-step:19400	 l-p:0.12282752990722656
epoch£º970	 i:1 	 global-step:19401	 l-p:0.11650554090738297
epoch£º970	 i:2 	 global-step:19402	 l-p:0.18667417764663696
epoch£º970	 i:3 	 global-step:19403	 l-p:0.19084960222244263
epoch£º970	 i:4 	 global-step:19404	 l-p:0.13624103367328644
epoch£º970	 i:5 	 global-step:19405	 l-p:0.10940562933683395
epoch£º970	 i:6 	 global-step:19406	 l-p:0.12021190673112869
epoch£º970	 i:7 	 global-step:19407	 l-p:0.12679527699947357
epoch£º970	 i:8 	 global-step:19408	 l-p:0.137618288397789
epoch£º970	 i:9 	 global-step:19409	 l-p:0.1112872064113617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:971
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1661, 5.0727, 5.1274],
        [5.1661, 5.0773, 5.1308],
        [5.1661, 5.1652, 5.1660],
        [5.1661, 4.8903, 4.6804]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:971, step:0 
model_pd.l_p.mean(): 0.10284137725830078 
model_pd.l_d.mean(): -20.756301879882812 
model_pd.lagr.mean(): -20.653461456298828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4145], device='cuda:0')), ('power', tensor([-21.4066], device='cuda:0'))])
epoch£º971	 i:0 	 global-step:19420	 l-p:0.10284137725830078
epoch£º971	 i:1 	 global-step:19421	 l-p:0.13744871318340302
epoch£º971	 i:2 	 global-step:19422	 l-p:0.09148188680410385
epoch£º971	 i:3 	 global-step:19423	 l-p:0.14079825580120087
epoch£º971	 i:4 	 global-step:19424	 l-p:0.1466190665960312
epoch£º971	 i:5 	 global-step:19425	 l-p:0.1606166660785675
epoch£º971	 i:6 	 global-step:19426	 l-p:0.18325822055339813
epoch£º971	 i:7 	 global-step:19427	 l-p:0.10864609479904175
epoch£º971	 i:8 	 global-step:19428	 l-p:0.15085706114768982
epoch£º971	 i:9 	 global-step:19429	 l-p:0.14024361968040466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:972
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1535, 5.0856, 5.1320],
        [5.1535, 5.1519, 5.1535],
        [5.1535, 4.8996, 4.8503],
        [5.1535, 5.1506, 5.1534]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:972, step:0 
model_pd.l_p.mean(): 0.15700823068618774 
model_pd.l_d.mean(): -19.96533203125 
model_pd.lagr.mean(): -19.80832290649414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5255], device='cuda:0')), ('power', tensor([-20.7203], device='cuda:0'))])
epoch£º972	 i:0 	 global-step:19440	 l-p:0.15700823068618774
epoch£º972	 i:1 	 global-step:19441	 l-p:0.11928464472293854
epoch£º972	 i:2 	 global-step:19442	 l-p:0.1402774304151535
epoch£º972	 i:3 	 global-step:19443	 l-p:0.14508822560310364
epoch£º972	 i:4 	 global-step:19444	 l-p:0.16129980981349945
epoch£º972	 i:5 	 global-step:19445	 l-p:0.14650127291679382
epoch£º972	 i:6 	 global-step:19446	 l-p:0.19327789545059204
epoch£º972	 i:7 	 global-step:19447	 l-p:0.0832836925983429
epoch£º972	 i:8 	 global-step:19448	 l-p:0.10882455855607986
epoch£º972	 i:9 	 global-step:19449	 l-p:0.10775930434465408
====================================================================================================
====================================================================================================
====================================================================================================

epoch:973
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1589, 5.1451, 5.1575],
        [5.1589, 4.9444, 4.9569],
        [5.1589, 5.0468, 5.1045],
        [5.1589, 5.1193, 4.8073]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:973, step:0 
model_pd.l_p.mean(): 0.18182842433452606 
model_pd.l_d.mean(): -19.990211486816406 
model_pd.lagr.mean(): -19.80838394165039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4458], device='cuda:0')), ('power', tensor([-20.6641], device='cuda:0'))])
epoch£º973	 i:0 	 global-step:19460	 l-p:0.18182842433452606
epoch£º973	 i:1 	 global-step:19461	 l-p:0.13105984032154083
epoch£º973	 i:2 	 global-step:19462	 l-p:0.07070005685091019
epoch£º973	 i:3 	 global-step:19463	 l-p:0.13438667356967926
epoch£º973	 i:4 	 global-step:19464	 l-p:0.16247783601284027
epoch£º973	 i:5 	 global-step:19465	 l-p:0.122939832508564
epoch£º973	 i:6 	 global-step:19466	 l-p:0.15267634391784668
epoch£º973	 i:7 	 global-step:19467	 l-p:0.16919846832752228
epoch£º973	 i:8 	 global-step:19468	 l-p:0.11390634626150131
epoch£º973	 i:9 	 global-step:19469	 l-p:0.12116844952106476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:974
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1430, 4.8752, 4.7926],
        [5.1430, 5.0892, 5.1289],
        [5.1430, 4.9231, 4.9308],
        [5.1430, 5.1779, 4.8926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:974, step:0 
model_pd.l_p.mean(): 0.14173050224781036 
model_pd.l_d.mean(): -19.241893768310547 
model_pd.lagr.mean(): -19.100162506103516 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5085], device='cuda:0')), ('power', tensor([-19.9716], device='cuda:0'))])
epoch£º974	 i:0 	 global-step:19480	 l-p:0.14173050224781036
epoch£º974	 i:1 	 global-step:19481	 l-p:0.12147108465433121
epoch£º974	 i:2 	 global-step:19482	 l-p:0.1347278207540512
epoch£º974	 i:3 	 global-step:19483	 l-p:0.14254438877105713
epoch£º974	 i:4 	 global-step:19484	 l-p:0.23446246981620789
epoch£º974	 i:5 	 global-step:19485	 l-p:0.13789115846157074
epoch£º974	 i:6 	 global-step:19486	 l-p:0.13200129568576813
epoch£º974	 i:7 	 global-step:19487	 l-p:0.11337359994649887
epoch£º974	 i:8 	 global-step:19488	 l-p:0.13743332028388977
epoch£º974	 i:9 	 global-step:19489	 l-p:0.17831306159496307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:975
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1294, 5.1294, 5.1294],
        [5.1294, 5.1294, 5.1294],
        [5.1294, 4.8463, 4.6521],
        [5.1294, 4.9959, 5.0539]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:975, step:0 
model_pd.l_p.mean(): 0.10934068262577057 
model_pd.l_d.mean(): -19.542537689208984 
model_pd.lagr.mean(): -19.433197021484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4842], device='cuda:0')), ('power', tensor([-20.2508], device='cuda:0'))])
epoch£º975	 i:0 	 global-step:19500	 l-p:0.10934068262577057
epoch£º975	 i:1 	 global-step:19501	 l-p:0.1989986002445221
epoch£º975	 i:2 	 global-step:19502	 l-p:0.16428539156913757
epoch£º975	 i:3 	 global-step:19503	 l-p:0.13348865509033203
epoch£º975	 i:4 	 global-step:19504	 l-p:0.10295387357473373
epoch£º975	 i:5 	 global-step:19505	 l-p:0.15016835927963257
epoch£º975	 i:6 	 global-step:19506	 l-p:0.10732371360063553
epoch£º975	 i:7 	 global-step:19507	 l-p:0.3315337598323822
epoch£º975	 i:8 	 global-step:19508	 l-p:0.11222100257873535
epoch£º975	 i:9 	 global-step:19509	 l-p:0.1478019654750824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:976
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1080, 4.8414, 4.7720],
        [5.1080, 5.0816, 5.1040],
        [5.1080, 5.1402, 4.8534],
        [5.1080, 5.1080, 5.1080]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:976, step:0 
model_pd.l_p.mean(): 0.2512776553630829 
model_pd.l_d.mean(): -20.544076919555664 
model_pd.lagr.mean(): -20.29279899597168 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4753], device='cuda:0')), ('power', tensor([-21.2542], device='cuda:0'))])
epoch£º976	 i:0 	 global-step:19520	 l-p:0.2512776553630829
epoch£º976	 i:1 	 global-step:19521	 l-p:0.39701786637306213
epoch£º976	 i:2 	 global-step:19522	 l-p:0.154605433344841
epoch£º976	 i:3 	 global-step:19523	 l-p:0.15889804065227509
epoch£º976	 i:4 	 global-step:19524	 l-p:0.16386856138706207
epoch£º976	 i:5 	 global-step:19525	 l-p:0.12757931649684906
epoch£º976	 i:6 	 global-step:19526	 l-p:0.11142707616090775
epoch£º976	 i:7 	 global-step:19527	 l-p:0.12841053307056427
epoch£º976	 i:8 	 global-step:19528	 l-p:0.12257310748100281
epoch£º976	 i:9 	 global-step:19529	 l-p:0.14491799473762512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:977
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1163, 5.1163, 5.1163],
        [5.1163, 5.1163, 5.1163],
        [5.1163, 5.0196, 5.0755],
        [5.1163, 4.8590, 4.8106]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:977, step:0 
model_pd.l_p.mean(): 0.17066945135593414 
model_pd.l_d.mean(): -20.670316696166992 
model_pd.lagr.mean(): -20.49964714050293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4613], device='cuda:0')), ('power', tensor([-21.3675], device='cuda:0'))])
epoch£º977	 i:0 	 global-step:19540	 l-p:0.17066945135593414
epoch£º977	 i:1 	 global-step:19541	 l-p:0.18079780042171478
epoch£º977	 i:2 	 global-step:19542	 l-p:0.11535324156284332
epoch£º977	 i:3 	 global-step:19543	 l-p:0.12401560693979263
epoch£º977	 i:4 	 global-step:19544	 l-p:0.16822852194309235
epoch£º977	 i:5 	 global-step:19545	 l-p:0.08964810520410538
epoch£º977	 i:6 	 global-step:19546	 l-p:0.11380837112665176
epoch£º977	 i:7 	 global-step:19547	 l-p:0.21903125941753387
epoch£º977	 i:8 	 global-step:19548	 l-p:0.5145299434661865
epoch£º977	 i:9 	 global-step:19549	 l-p:0.2074463665485382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:978
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1077, 5.1070, 5.1077],
        [5.1077, 5.0256, 5.0777],
        [5.1077, 5.1077, 5.1077],
        [5.1077, 4.8207, 4.6471]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:978, step:0 
model_pd.l_p.mean(): 0.1337909698486328 
model_pd.l_d.mean(): -20.316991806030273 
model_pd.lagr.mean(): -20.18320083618164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4906], device='cuda:0')), ('power', tensor([-21.0402], device='cuda:0'))])
epoch£º978	 i:0 	 global-step:19560	 l-p:0.1337909698486328
epoch£º978	 i:1 	 global-step:19561	 l-p:0.16765648126602173
epoch£º978	 i:2 	 global-step:19562	 l-p:0.25930967926979065
epoch£º978	 i:3 	 global-step:19563	 l-p:0.11995963007211685
epoch£º978	 i:4 	 global-step:19564	 l-p:0.1503484547138214
epoch£º978	 i:5 	 global-step:19565	 l-p:0.1327466517686844
epoch£º978	 i:6 	 global-step:19566	 l-p:0.19666191935539246
epoch£º978	 i:7 	 global-step:19567	 l-p:0.28812503814697266
epoch£º978	 i:8 	 global-step:19568	 l-p:0.08743643760681152
epoch£º978	 i:9 	 global-step:19569	 l-p:0.12212757021188736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:979
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228]], device='cuda:0')
 pt:tensor([[5.1185, 5.5199, 5.4421],
        [5.1185, 4.8374, 4.6123],
        [5.1185, 4.8627, 4.5799],
        [5.1185, 4.8416, 4.7375]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:979, step:0 
model_pd.l_p.mean(): 0.17127494513988495 
model_pd.l_d.mean(): -19.83484649658203 
model_pd.lagr.mean(): -19.663572311401367 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5463], device='cuda:0')), ('power', tensor([-20.6097], device='cuda:0'))])
epoch£º979	 i:0 	 global-step:19580	 l-p:0.17127494513988495
epoch£º979	 i:1 	 global-step:19581	 l-p:0.12166192382574081
epoch£º979	 i:2 	 global-step:19582	 l-p:0.18725785613059998
epoch£º979	 i:3 	 global-step:19583	 l-p:0.14952202141284943
epoch£º979	 i:4 	 global-step:19584	 l-p:0.1160484105348587
epoch£º979	 i:5 	 global-step:19585	 l-p:0.12457136064767838
epoch£º979	 i:6 	 global-step:19586	 l-p:0.12430480867624283
epoch£º979	 i:7 	 global-step:19587	 l-p:0.1748572736978531
epoch£º979	 i:8 	 global-step:19588	 l-p:0.1733381599187851
epoch£º979	 i:9 	 global-step:19589	 l-p:0.12443841993808746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:980
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1458, 5.1458, 5.1458],
        [5.1458, 5.1428, 5.1457],
        [5.1458, 5.0997, 5.1351],
        [5.1458, 5.1454, 5.1458]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:980, step:0 
model_pd.l_p.mean(): 0.1933499425649643 
model_pd.l_d.mean(): -20.67875099182129 
model_pd.lagr.mean(): -20.485401153564453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4340], device='cuda:0')), ('power', tensor([-21.3481], device='cuda:0'))])
epoch£º980	 i:0 	 global-step:19600	 l-p:0.1933499425649643
epoch£º980	 i:1 	 global-step:19601	 l-p:0.14959821105003357
epoch£º980	 i:2 	 global-step:19602	 l-p:0.14057999849319458
epoch£º980	 i:3 	 global-step:19603	 l-p:0.085992231965065
epoch£º980	 i:4 	 global-step:19604	 l-p:0.10806737840175629
epoch£º980	 i:5 	 global-step:19605	 l-p:0.20056270062923431
epoch£º980	 i:6 	 global-step:19606	 l-p:0.11663582175970078
epoch£º980	 i:7 	 global-step:19607	 l-p:0.15303461253643036
epoch£º980	 i:8 	 global-step:19608	 l-p:0.12658162415027618
epoch£º980	 i:9 	 global-step:19609	 l-p:0.11213862895965576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:981
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1606, 5.0367, 5.0950],
        [5.1606, 5.1606, 5.1606],
        [5.1606, 5.1584, 5.1605],
        [5.1606, 4.9098, 4.6300]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:981, step:0 
model_pd.l_p.mean(): 0.14088046550750732 
model_pd.l_d.mean(): -19.756404876708984 
model_pd.lagr.mean(): -19.615524291992188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4861], device='cuda:0')), ('power', tensor([-20.4689], device='cuda:0'))])
epoch£º981	 i:0 	 global-step:19620	 l-p:0.14088046550750732
epoch£º981	 i:1 	 global-step:19621	 l-p:0.1318483203649521
epoch£º981	 i:2 	 global-step:19622	 l-p:0.13269473612308502
epoch£º981	 i:3 	 global-step:19623	 l-p:0.14150485396385193
epoch£º981	 i:4 	 global-step:19624	 l-p:0.13202744722366333
epoch£º981	 i:5 	 global-step:19625	 l-p:0.1660405695438385
epoch£º981	 i:6 	 global-step:19626	 l-p:0.15817293524742126
epoch£º981	 i:7 	 global-step:19627	 l-p:0.16681788861751556
epoch£º981	 i:8 	 global-step:19628	 l-p:0.09052684903144836
epoch£º981	 i:9 	 global-step:19629	 l-p:0.18432113528251648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:982
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1350, 4.8833, 4.8447],
        [5.1350, 5.2849, 5.0542],
        [5.1350, 5.1350, 5.1350],
        [5.1350, 5.0477, 5.1013]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:982, step:0 
model_pd.l_p.mean(): 0.14812113344669342 
model_pd.l_d.mean(): -20.513887405395508 
model_pd.lagr.mean(): -20.365766525268555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4695], device='cuda:0')), ('power', tensor([-21.2177], device='cuda:0'))])
epoch£º982	 i:0 	 global-step:19640	 l-p:0.14812113344669342
epoch£º982	 i:1 	 global-step:19641	 l-p:0.16074500977993011
epoch£º982	 i:2 	 global-step:19642	 l-p:0.17012469470500946
epoch£º982	 i:3 	 global-step:19643	 l-p:0.1495475172996521
epoch£º982	 i:4 	 global-step:19644	 l-p:0.1490781307220459
epoch£º982	 i:5 	 global-step:19645	 l-p:0.06706596165895462
epoch£º982	 i:6 	 global-step:19646	 l-p:0.14360783994197845
epoch£º982	 i:7 	 global-step:19647	 l-p:0.12306251376867294
epoch£º982	 i:8 	 global-step:19648	 l-p:0.16231989860534668
epoch£º982	 i:9 	 global-step:19649	 l-p:0.16474004089832306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:983
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1437, 4.9725, 5.0200],
        [5.1437, 5.0100, 5.0680],
        [5.1437, 5.0564, 5.1099],
        [5.1437, 5.1406, 5.1435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:983, step:0 
model_pd.l_p.mean(): 0.06857999414205551 
model_pd.l_d.mean(): -20.683368682861328 
model_pd.lagr.mean(): -20.614788055419922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4508], device='cuda:0')), ('power', tensor([-21.3699], device='cuda:0'))])
epoch£º983	 i:0 	 global-step:19660	 l-p:0.06857999414205551
epoch£º983	 i:1 	 global-step:19661	 l-p:0.11851108074188232
epoch£º983	 i:2 	 global-step:19662	 l-p:0.14259174466133118
epoch£º983	 i:3 	 global-step:19663	 l-p:0.13130146265029907
epoch£º983	 i:4 	 global-step:19664	 l-p:0.13005958497524261
epoch£º983	 i:5 	 global-step:19665	 l-p:0.11757313460111618
epoch£º983	 i:6 	 global-step:19666	 l-p:0.14756621420383453
epoch£º983	 i:7 	 global-step:19667	 l-p:0.14465361833572388
epoch£º983	 i:8 	 global-step:19668	 l-p:0.2400229275226593
epoch£º983	 i:9 	 global-step:19669	 l-p:0.1478087306022644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:984
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1527, 4.9638, 4.6400],
        [5.1527, 5.1465, 5.1523],
        [5.1527, 5.1526, 5.1527],
        [5.1527, 5.1475, 5.1524]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:984, step:0 
model_pd.l_p.mean(): 0.07879792153835297 
model_pd.l_d.mean(): -20.200185775756836 
model_pd.lagr.mean(): -20.121387481689453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4980], device='cuda:0')), ('power', tensor([-20.9297], device='cuda:0'))])
epoch£º984	 i:0 	 global-step:19680	 l-p:0.07879792153835297
epoch£º984	 i:1 	 global-step:19681	 l-p:0.18439216911792755
epoch£º984	 i:2 	 global-step:19682	 l-p:0.11874574422836304
epoch£º984	 i:3 	 global-step:19683	 l-p:0.13845981657505035
epoch£º984	 i:4 	 global-step:19684	 l-p:0.12116511911153793
epoch£º984	 i:5 	 global-step:19685	 l-p:0.11577647179365158
epoch£º984	 i:6 	 global-step:19686	 l-p:0.11531217396259308
epoch£º984	 i:7 	 global-step:19687	 l-p:0.18339358270168304
epoch£º984	 i:8 	 global-step:19688	 l-p:0.21183820068836212
epoch£º984	 i:9 	 global-step:19689	 l-p:0.14377902448177338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:985
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1364, 5.5188, 5.4273],
        [5.1364, 4.9648, 5.0126],
        [5.1364, 5.1708, 4.8840],
        [5.1364, 5.1364, 5.1364]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:985, step:0 
model_pd.l_p.mean(): 0.19575610756874084 
model_pd.l_d.mean(): -20.449750900268555 
model_pd.lagr.mean(): -20.25399398803711 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4674], device='cuda:0')), ('power', tensor([-21.1508], device='cuda:0'))])
epoch£º985	 i:0 	 global-step:19700	 l-p:0.19575610756874084
epoch£º985	 i:1 	 global-step:19701	 l-p:0.12905381619930267
epoch£º985	 i:2 	 global-step:19702	 l-p:0.14829924702644348
epoch£º985	 i:3 	 global-step:19703	 l-p:0.16448234021663666
epoch£º985	 i:4 	 global-step:19704	 l-p:0.16381217539310455
epoch£º985	 i:5 	 global-step:19705	 l-p:0.11454419791698456
epoch£º985	 i:6 	 global-step:19706	 l-p:0.13426640629768372
epoch£º985	 i:7 	 global-step:19707	 l-p:0.13008417189121246
epoch£º985	 i:8 	 global-step:19708	 l-p:0.19049440324306488
epoch£º985	 i:9 	 global-step:19709	 l-p:0.09028956294059753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:986
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1421, 5.1421, 5.1421],
        [5.1421, 5.0494, 5.1044],
        [5.1421, 5.1377, 5.1419],
        [5.1421, 5.0114, 5.0699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:986, step:0 
model_pd.l_p.mean(): 0.09825906902551651 
model_pd.l_d.mean(): -20.51933479309082 
model_pd.lagr.mean(): -20.42107582092285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4617], device='cuda:0')), ('power', tensor([-21.2153], device='cuda:0'))])
epoch£º986	 i:0 	 global-step:19720	 l-p:0.09825906902551651
epoch£º986	 i:1 	 global-step:19721	 l-p:0.16573511064052582
epoch£º986	 i:2 	 global-step:19722	 l-p:0.1229981780052185
epoch£º986	 i:3 	 global-step:19723	 l-p:0.09351752698421478
epoch£º986	 i:4 	 global-step:19724	 l-p:0.1591743379831314
epoch£º986	 i:5 	 global-step:19725	 l-p:0.22293250262737274
epoch£º986	 i:6 	 global-step:19726	 l-p:0.18249744176864624
epoch£º986	 i:7 	 global-step:19727	 l-p:0.13709315657615662
epoch£º986	 i:8 	 global-step:19728	 l-p:0.10434290021657944
epoch£º986	 i:9 	 global-step:19729	 l-p:0.10836294293403625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:987
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1654, 5.1654, 5.1654],
        [5.1654, 5.1654, 5.1654],
        [5.1654, 5.1162, 5.1534],
        [5.1654, 5.0782, 5.1316]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:987, step:0 
model_pd.l_p.mean(): 0.11577431112527847 
model_pd.l_d.mean(): -19.380075454711914 
model_pd.lagr.mean(): -19.264301300048828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4869], device='cuda:0')), ('power', tensor([-20.0893], device='cuda:0'))])
epoch£º987	 i:0 	 global-step:19740	 l-p:0.11577431112527847
epoch£º987	 i:1 	 global-step:19741	 l-p:0.16279742121696472
epoch£º987	 i:2 	 global-step:19742	 l-p:0.04601817950606346
epoch£º987	 i:3 	 global-step:19743	 l-p:0.09652050584554672
epoch£º987	 i:4 	 global-step:19744	 l-p:0.1545354723930359
epoch£º987	 i:5 	 global-step:19745	 l-p:0.1478850394487381
epoch£º987	 i:6 	 global-step:19746	 l-p:0.13316573202610016
epoch£º987	 i:7 	 global-step:19747	 l-p:0.179534912109375
epoch£º987	 i:8 	 global-step:19748	 l-p:0.16503986716270447
epoch£º987	 i:9 	 global-step:19749	 l-p:0.14622704684734344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:988
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1583, 4.8783, 4.6625],
        [5.1583, 4.8777, 4.7437],
        [5.1583, 5.1579, 5.1583],
        [5.1583, 5.1583, 5.1583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:988, step:0 
model_pd.l_p.mean(): 0.16968455910682678 
model_pd.l_d.mean(): -20.75419807434082 
model_pd.lagr.mean(): -20.58451271057129 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4142], device='cuda:0')), ('power', tensor([-21.4042], device='cuda:0'))])
epoch£º988	 i:0 	 global-step:19760	 l-p:0.16968455910682678
epoch£º988	 i:1 	 global-step:19761	 l-p:0.11294034123420715
epoch£º988	 i:2 	 global-step:19762	 l-p:0.13128337264060974
epoch£º988	 i:3 	 global-step:19763	 l-p:0.15321215987205505
epoch£º988	 i:4 	 global-step:19764	 l-p:0.1742779165506363
epoch£º988	 i:5 	 global-step:19765	 l-p:0.11697065830230713
epoch£º988	 i:6 	 global-step:19766	 l-p:0.09628015011548996
epoch£º988	 i:7 	 global-step:19767	 l-p:0.18542130291461945
epoch£º988	 i:8 	 global-step:19768	 l-p:0.15842872858047485
epoch£º988	 i:9 	 global-step:19769	 l-p:0.11040520668029785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:989
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1479, 4.9358, 4.9542],
        [5.1479, 5.1447, 5.1478],
        [5.1479, 5.0171, 5.0756],
        [5.1479, 5.1479, 5.1479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:989, step:0 
model_pd.l_p.mean(): 0.13163691759109497 
model_pd.l_d.mean(): -20.772653579711914 
model_pd.lagr.mean(): -20.641016006469727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4111], device='cuda:0')), ('power', tensor([-21.4197], device='cuda:0'))])
epoch£º989	 i:0 	 global-step:19780	 l-p:0.13163691759109497
epoch£º989	 i:1 	 global-step:19781	 l-p:0.1953323930501938
epoch£º989	 i:2 	 global-step:19782	 l-p:0.1441376507282257
epoch£º989	 i:3 	 global-step:19783	 l-p:0.07324405014514923
epoch£º989	 i:4 	 global-step:19784	 l-p:0.16969558596611023
epoch£º989	 i:5 	 global-step:19785	 l-p:0.1690664440393448
epoch£º989	 i:6 	 global-step:19786	 l-p:0.12999854981899261
epoch£º989	 i:7 	 global-step:19787	 l-p:0.1590796262025833
epoch£º989	 i:8 	 global-step:19788	 l-p:0.0745943933725357
epoch£º989	 i:9 	 global-step:19789	 l-p:0.13371960818767548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:990
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228]], device='cuda:0')
 pt:tensor([[5.1767, 5.4655, 5.3130],
        [5.1767, 4.9052, 4.8092],
        [5.1767, 5.0118, 4.6839],
        [5.1767, 5.4911, 5.3545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:990, step:0 
model_pd.l_p.mean(): 0.09904623031616211 
model_pd.l_d.mean(): -19.17342758178711 
model_pd.lagr.mean(): -19.07438087463379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5878], device='cuda:0')), ('power', tensor([-19.9834], device='cuda:0'))])
epoch£º990	 i:0 	 global-step:19800	 l-p:0.09904623031616211
epoch£º990	 i:1 	 global-step:19801	 l-p:0.13480617105960846
epoch£º990	 i:2 	 global-step:19802	 l-p:0.12056364864110947
epoch£º990	 i:3 	 global-step:19803	 l-p:0.1275157481431961
epoch£º990	 i:4 	 global-step:19804	 l-p:0.11090152710676193
epoch£º990	 i:5 	 global-step:19805	 l-p:0.1506190001964569
epoch£º990	 i:6 	 global-step:19806	 l-p:0.1438901126384735
epoch£º990	 i:7 	 global-step:19807	 l-p:0.14008596539497375
epoch£º990	 i:8 	 global-step:19808	 l-p:0.14086773991584778
epoch£º990	 i:9 	 global-step:19809	 l-p:0.06564359366893768
====================================================================================================
====================================================================================================
====================================================================================================

epoch:991
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2036, 5.1533, 5.1910],
        [5.2036, 4.9257, 4.7796],
        [5.2036, 5.1232, 5.1744],
        [5.2036, 5.1994, 5.2034]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:991, step:0 
model_pd.l_p.mean(): 0.13096188008785248 
model_pd.l_d.mean(): -20.9014949798584 
model_pd.lagr.mean(): -20.770532608032227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3805], device='cuda:0')), ('power', tensor([-21.5186], device='cuda:0'))])
epoch£º991	 i:0 	 global-step:19820	 l-p:0.13096188008785248
epoch£º991	 i:1 	 global-step:19821	 l-p:0.045623358339071274
epoch£º991	 i:2 	 global-step:19822	 l-p:0.13268575072288513
epoch£º991	 i:3 	 global-step:19823	 l-p:0.1552969366312027
epoch£º991	 i:4 	 global-step:19824	 l-p:0.12707602977752686
epoch£º991	 i:5 	 global-step:19825	 l-p:0.11736680567264557
epoch£º991	 i:6 	 global-step:19826	 l-p:0.1404000073671341
epoch£º991	 i:7 	 global-step:19827	 l-p:0.16478858888149261
epoch£º991	 i:8 	 global-step:19828	 l-p:0.1049482524394989
epoch£º991	 i:9 	 global-step:19829	 l-p:0.1087794080376625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:992
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1828, 5.1827, 5.1828],
        [5.1828, 5.1679, 5.1813],
        [5.1828, 5.2307, 4.9489],
        [5.1828, 5.0309, 4.7018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:992, step:0 
model_pd.l_p.mean(): 0.11562873423099518 
model_pd.l_d.mean(): -20.66679573059082 
model_pd.lagr.mean(): -20.551166534423828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4179], device='cuda:0')), ('power', tensor([-21.3195], device='cuda:0'))])
epoch£º992	 i:0 	 global-step:19840	 l-p:0.11562873423099518
epoch£º992	 i:1 	 global-step:19841	 l-p:0.15219873189926147
epoch£º992	 i:2 	 global-step:19842	 l-p:0.1321391612291336
epoch£º992	 i:3 	 global-step:19843	 l-p:0.10739283263683319
epoch£º992	 i:4 	 global-step:19844	 l-p:0.16384656727313995
epoch£º992	 i:5 	 global-step:19845	 l-p:0.10373610258102417
epoch£º992	 i:6 	 global-step:19846	 l-p:0.16157592833042145
epoch£º992	 i:7 	 global-step:19847	 l-p:0.12957705557346344
epoch£º992	 i:8 	 global-step:19848	 l-p:0.10995347797870636
epoch£º992	 i:9 	 global-step:19849	 l-p:0.14082489907741547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:993
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1603, 5.2634, 5.0073],
        [5.1603, 4.8767, 4.6839],
        [5.1603, 5.1603, 5.1603],
        [5.1603, 5.1493, 5.1594]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:993, step:0 
model_pd.l_p.mean(): 0.25397658348083496 
model_pd.l_d.mean(): -20.501848220825195 
model_pd.lagr.mean(): -20.24787139892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-21.2052], device='cuda:0'))])
epoch£º993	 i:0 	 global-step:19860	 l-p:0.25397658348083496
epoch£º993	 i:1 	 global-step:19861	 l-p:0.15096630156040192
epoch£º993	 i:2 	 global-step:19862	 l-p:0.12214015424251556
epoch£º993	 i:3 	 global-step:19863	 l-p:0.15202222764492035
epoch£º993	 i:4 	 global-step:19864	 l-p:0.0979117676615715
epoch£º993	 i:5 	 global-step:19865	 l-p:0.09899690747261047
epoch£º993	 i:6 	 global-step:19866	 l-p:0.12213245034217834
epoch£º993	 i:7 	 global-step:19867	 l-p:0.13134971261024475
epoch£º993	 i:8 	 global-step:19868	 l-p:0.13798250257968903
epoch£º993	 i:9 	 global-step:19869	 l-p:0.13174590468406677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:994
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1282, 5.1232, 5.1280],
        [5.1282, 5.1516, 4.8593],
        [5.1282, 4.8865, 4.5845],
        [5.1282, 5.1280, 5.1282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:994, step:0 
model_pd.l_p.mean(): 0.13313981890678406 
model_pd.l_d.mean(): -20.370807647705078 
model_pd.lagr.mean(): -20.237667083740234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4758], device='cuda:0')), ('power', tensor([-21.0795], device='cuda:0'))])
epoch£º994	 i:0 	 global-step:19880	 l-p:0.13313981890678406
epoch£º994	 i:1 	 global-step:19881	 l-p:0.11254815757274628
epoch£º994	 i:2 	 global-step:19882	 l-p:0.11702059209346771
epoch£º994	 i:3 	 global-step:19883	 l-p:0.14820542931556702
epoch£º994	 i:4 	 global-step:19884	 l-p:0.12588481605052948
epoch£º994	 i:5 	 global-step:19885	 l-p:0.13192015886306763
epoch£º994	 i:6 	 global-step:19886	 l-p:0.8872485160827637
epoch£º994	 i:7 	 global-step:19887	 l-p:0.20507022738456726
epoch£º994	 i:8 	 global-step:19888	 l-p:0.1896461844444275
epoch£º994	 i:9 	 global-step:19889	 l-p:0.17757275700569153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:995
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1054, 4.9843, 5.0438],
        [5.1054, 4.9023, 4.5748],
        [5.1054, 4.9061, 4.9390],
        [5.1054, 5.1033, 5.1054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:995, step:0 
model_pd.l_p.mean(): 0.1783345341682434 
model_pd.l_d.mean(): -20.72828483581543 
model_pd.lagr.mean(): -20.549949645996094 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4509], device='cuda:0')), ('power', tensor([-21.4154], device='cuda:0'))])
epoch£º995	 i:0 	 global-step:19900	 l-p:0.1783345341682434
epoch£º995	 i:1 	 global-step:19901	 l-p:0.11323598027229309
epoch£º995	 i:2 	 global-step:19902	 l-p:0.12805594503879547
epoch£º995	 i:3 	 global-step:19903	 l-p:0.2766474187374115
epoch£º995	 i:4 	 global-step:19904	 l-p:0.15709446370601654
epoch£º995	 i:5 	 global-step:19905	 l-p:0.3279973566532135
epoch£º995	 i:6 	 global-step:19906	 l-p:0.11968184262514114
epoch£º995	 i:7 	 global-step:19907	 l-p:0.08723300695419312
epoch£º995	 i:8 	 global-step:19908	 l-p:0.15697482228279114
epoch£º995	 i:9 	 global-step:19909	 l-p:0.123763307929039
====================================================================================================
====================================================================================================
====================================================================================================

epoch:996
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1236, 4.9004, 4.9095],
        [5.1236, 5.4335, 5.2947],
        [5.1236, 4.8349, 4.6455],
        [5.1236, 5.1229, 5.1236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:996, step:0 
model_pd.l_p.mean(): 0.11545553058385849 
model_pd.l_d.mean(): -18.669401168823242 
model_pd.lagr.mean(): -18.553945541381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5987], device='cuda:0')), ('power', tensor([-19.4851], device='cuda:0'))])
epoch£º996	 i:0 	 global-step:19920	 l-p:0.11545553058385849
epoch£º996	 i:1 	 global-step:19921	 l-p:0.26378893852233887
epoch£º996	 i:2 	 global-step:19922	 l-p:0.14425000548362732
epoch£º996	 i:3 	 global-step:19923	 l-p:0.15057145059108734
epoch£º996	 i:4 	 global-step:19924	 l-p:0.17630206048488617
epoch£º996	 i:5 	 global-step:19925	 l-p:0.14401279389858246
epoch£º996	 i:6 	 global-step:19926	 l-p:0.12903748452663422
epoch£º996	 i:7 	 global-step:19927	 l-p:0.1400299221277237
epoch£º996	 i:8 	 global-step:19928	 l-p:0.12809468805789948
epoch£º996	 i:9 	 global-step:19929	 l-p:0.11936461925506592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:997
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1469, 5.0684, 5.1193],
        [5.1469, 5.1470, 5.1469],
        [5.1469, 4.8636, 4.7295],
        [5.1469, 5.2152, 4.9422]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:997, step:0 
model_pd.l_p.mean(): 0.1528414487838745 
model_pd.l_d.mean(): -19.939966201782227 
model_pd.lagr.mean(): -19.787124633789062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5410], device='cuda:0')), ('power', tensor([-20.7105], device='cuda:0'))])
epoch£º997	 i:0 	 global-step:19940	 l-p:0.1528414487838745
epoch£º997	 i:1 	 global-step:19941	 l-p:0.1404407024383545
epoch£º997	 i:2 	 global-step:19942	 l-p:0.11646153032779694
epoch£º997	 i:3 	 global-step:19943	 l-p:0.11016049236059189
epoch£º997	 i:4 	 global-step:19944	 l-p:0.17041267454624176
epoch£º997	 i:5 	 global-step:19945	 l-p:0.19256484508514404
epoch£º997	 i:6 	 global-step:19946	 l-p:0.14257948100566864
epoch£º997	 i:7 	 global-step:19947	 l-p:0.130960613489151
epoch£º997	 i:8 	 global-step:19948	 l-p:0.10939054191112518
epoch£º997	 i:9 	 global-step:19949	 l-p:0.19577540457248688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:998
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1394, 4.9578, 4.6275],
        [5.1394, 5.1378, 5.1394],
        [5.1394, 5.1394, 5.1394],
        [5.1394, 5.1012, 5.1318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:998, step:0 
model_pd.l_p.mean(): 0.1356106847524643 
model_pd.l_d.mean(): -21.029680252075195 
model_pd.lagr.mean(): -20.89406967163086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3788], device='cuda:0')), ('power', tensor([-21.6464], device='cuda:0'))])
epoch£º998	 i:0 	 global-step:19960	 l-p:0.1356106847524643
epoch£º998	 i:1 	 global-step:19961	 l-p:0.08183591067790985
epoch£º998	 i:2 	 global-step:19962	 l-p:0.1565084606409073
epoch£º998	 i:3 	 global-step:19963	 l-p:0.2095540165901184
epoch£º998	 i:4 	 global-step:19964	 l-p:0.12744098901748657
epoch£º998	 i:5 	 global-step:19965	 l-p:0.1466256082057953
epoch£º998	 i:6 	 global-step:19966	 l-p:0.16125339269638062
epoch£º998	 i:7 	 global-step:19967	 l-p:0.10383085161447525
epoch£º998	 i:8 	 global-step:19968	 l-p:0.16532458364963531
epoch£º998	 i:9 	 global-step:19969	 l-p:0.1481863409280777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:999
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1528, 5.1526, 5.1529],
        [5.1528, 4.9521, 4.9819],
        [5.1528, 5.1499, 5.1527],
        [5.1528, 5.1483, 5.1526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:999, step:0 
model_pd.l_p.mean(): 0.15111644566059113 
model_pd.l_d.mean(): -18.054903030395508 
model_pd.lagr.mean(): -17.903785705566406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6044], device='cuda:0')), ('power', tensor([-18.8697], device='cuda:0'))])
epoch£º999	 i:0 	 global-step:19980	 l-p:0.15111644566059113
epoch£º999	 i:1 	 global-step:19981	 l-p:0.0995865911245346
epoch£º999	 i:2 	 global-step:19982	 l-p:0.17851509153842926
epoch£º999	 i:3 	 global-step:19983	 l-p:0.1258729100227356
epoch£º999	 i:4 	 global-step:19984	 l-p:0.12189672142267227
epoch£º999	 i:5 	 global-step:19985	 l-p:0.1467227190732956
epoch£º999	 i:6 	 global-step:19986	 l-p:0.1300099492073059
epoch£º999	 i:7 	 global-step:19987	 l-p:0.1269722282886505
epoch£º999	 i:8 	 global-step:19988	 l-p:0.17898669838905334
epoch£º999	 i:9 	 global-step:19989	 l-p:0.11461956799030304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1000
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1602, 5.0724, 5.1262],
        [5.1602, 4.9297, 4.6223],
        [5.1602, 5.0671, 5.1223],
        [5.1602, 5.1923, 4.9030]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1000, step:0 
model_pd.l_p.mean(): 0.12314102798700333 
model_pd.l_d.mean(): -20.521446228027344 
model_pd.lagr.mean(): -20.398305892944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4409], device='cuda:0')), ('power', tensor([-21.1961], device='cuda:0'))])
epoch£º1000	 i:0 	 global-step:20000	 l-p:0.12314102798700333
epoch£º1000	 i:1 	 global-step:20001	 l-p:0.12423700094223022
epoch£º1000	 i:2 	 global-step:20002	 l-p:0.13601669669151306
epoch£º1000	 i:3 	 global-step:20003	 l-p:0.1647462099790573
epoch£º1000	 i:4 	 global-step:20004	 l-p:0.13585832715034485
epoch£º1000	 i:5 	 global-step:20005	 l-p:0.13140498101711273
epoch£º1000	 i:6 	 global-step:20006	 l-p:0.13892875611782074
epoch£º1000	 i:7 	 global-step:20007	 l-p:0.09437032788991928
epoch£º1000	 i:8 	 global-step:20008	 l-p:0.2170383334159851
epoch£º1000	 i:9 	 global-step:20009	 l-p:0.1455557942390442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1001
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1455, 5.1444, 5.1455],
        [5.1455, 4.8921, 4.8544],
        [5.1455, 4.9808, 5.0323],
        [5.1455, 5.1455, 5.1455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1001, step:0 
model_pd.l_p.mean(): 0.1054750308394432 
model_pd.l_d.mean(): -19.46760368347168 
model_pd.lagr.mean(): -19.36212921142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4820], device='cuda:0')), ('power', tensor([-20.1727], device='cuda:0'))])
epoch£º1001	 i:0 	 global-step:20020	 l-p:0.1054750308394432
epoch£º1001	 i:1 	 global-step:20021	 l-p:0.19694876670837402
epoch£º1001	 i:2 	 global-step:20022	 l-p:0.1425226628780365
epoch£º1001	 i:3 	 global-step:20023	 l-p:0.21651022136211395
epoch£º1001	 i:4 	 global-step:20024	 l-p:0.08871421962976456
epoch£º1001	 i:5 	 global-step:20025	 l-p:0.08529507368803024
epoch£º1001	 i:6 	 global-step:20026	 l-p:0.13152353465557098
epoch£º1001	 i:7 	 global-step:20027	 l-p:0.1733892410993576
epoch£º1001	 i:8 	 global-step:20028	 l-p:0.19140440225601196
epoch£º1001	 i:9 	 global-step:20029	 l-p:0.10881701111793518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1002
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1402, 5.1184, 5.1373],
        [5.1402, 4.9168, 4.9252],
        [5.1402, 5.5056, 5.4013],
        [5.1402, 5.0052, 5.0639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1002, step:0 
model_pd.l_p.mean(): 0.10625803470611572 
model_pd.l_d.mean(): -20.51041603088379 
model_pd.lagr.mean(): -20.404157638549805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4559], device='cuda:0')), ('power', tensor([-21.2002], device='cuda:0'))])
epoch£º1002	 i:0 	 global-step:20040	 l-p:0.10625803470611572
epoch£º1002	 i:1 	 global-step:20041	 l-p:0.22294706106185913
epoch£º1002	 i:2 	 global-step:20042	 l-p:0.1317492127418518
epoch£º1002	 i:3 	 global-step:20043	 l-p:0.11200650036334991
epoch£º1002	 i:4 	 global-step:20044	 l-p:0.10785109549760818
epoch£º1002	 i:5 	 global-step:20045	 l-p:0.21364787220954895
epoch£º1002	 i:6 	 global-step:20046	 l-p:0.15023173391819
epoch£º1002	 i:7 	 global-step:20047	 l-p:0.1343635469675064
epoch£º1002	 i:8 	 global-step:20048	 l-p:0.14100870490074158
epoch£º1002	 i:9 	 global-step:20049	 l-p:0.13002252578735352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1003
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1451, 4.9715, 4.6397],
        [5.1451, 5.1451, 5.1451],
        [5.1451, 5.1451, 5.1451],
        [5.1451, 5.1026, 5.1359]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1003, step:0 
model_pd.l_p.mean(): 0.1290445476770401 
model_pd.l_d.mean(): -19.749889373779297 
model_pd.lagr.mean(): -19.6208438873291 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5525], device='cuda:0')), ('power', tensor([-20.5302], device='cuda:0'))])
epoch£º1003	 i:0 	 global-step:20060	 l-p:0.1290445476770401
epoch£º1003	 i:1 	 global-step:20061	 l-p:0.1456499695777893
epoch£º1003	 i:2 	 global-step:20062	 l-p:0.16002283990383148
epoch£º1003	 i:3 	 global-step:20063	 l-p:0.21204455196857452
epoch£º1003	 i:4 	 global-step:20064	 l-p:0.1284499168395996
epoch£º1003	 i:5 	 global-step:20065	 l-p:0.14450791478157043
epoch£º1003	 i:6 	 global-step:20066	 l-p:0.11586686968803406
epoch£º1003	 i:7 	 global-step:20067	 l-p:0.10963699221611023
epoch£º1003	 i:8 	 global-step:20068	 l-p:0.1402847021818161
epoch£º1003	 i:9 	 global-step:20069	 l-p:0.1562347561120987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1004
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1348, 4.8472, 4.6447],
        [5.1348, 5.1219, 4.8147],
        [5.1348, 4.8874, 4.8625],
        [5.1348, 5.1346, 5.1348]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1004, step:0 
model_pd.l_p.mean(): 0.17863141000270844 
model_pd.l_d.mean(): -20.30947494506836 
model_pd.lagr.mean(): -20.130844116210938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5052], device='cuda:0')), ('power', tensor([-21.0475], device='cuda:0'))])
epoch£º1004	 i:0 	 global-step:20080	 l-p:0.17863141000270844
epoch£º1004	 i:1 	 global-step:20081	 l-p:0.1754799783229828
epoch£º1004	 i:2 	 global-step:20082	 l-p:0.1360023021697998
epoch£º1004	 i:3 	 global-step:20083	 l-p:0.1369314193725586
epoch£º1004	 i:4 	 global-step:20084	 l-p:0.13086897134780884
epoch£º1004	 i:5 	 global-step:20085	 l-p:0.12145551294088364
epoch£º1004	 i:6 	 global-step:20086	 l-p:0.10664064437150955
epoch£º1004	 i:7 	 global-step:20087	 l-p:0.1450621336698532
epoch£º1004	 i:8 	 global-step:20088	 l-p:0.16526153683662415
epoch£º1004	 i:9 	 global-step:20089	 l-p:0.24481359124183655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1005
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1263, 5.0120, 5.0710],
        [5.1263, 4.8764, 4.8484],
        [5.1263, 5.1249, 5.1263],
        [5.1263, 4.9528, 5.0013]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1005, step:0 
model_pd.l_p.mean(): 0.128987655043602 
model_pd.l_d.mean(): -20.522592544555664 
model_pd.lagr.mean(): -20.393604278564453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4372], device='cuda:0')), ('power', tensor([-21.1935], device='cuda:0'))])
epoch£º1005	 i:0 	 global-step:20100	 l-p:0.128987655043602
epoch£º1005	 i:1 	 global-step:20101	 l-p:0.1153109073638916
epoch£º1005	 i:2 	 global-step:20102	 l-p:0.14587068557739258
epoch£º1005	 i:3 	 global-step:20103	 l-p:0.18792174756526947
epoch£º1005	 i:4 	 global-step:20104	 l-p:0.09937109053134918
epoch£º1005	 i:5 	 global-step:20105	 l-p:0.20857837796211243
epoch£º1005	 i:6 	 global-step:20106	 l-p:0.18791919946670532
epoch£º1005	 i:7 	 global-step:20107	 l-p:0.15325239300727844
epoch£º1005	 i:8 	 global-step:20108	 l-p:0.10973893851041794
epoch£º1005	 i:9 	 global-step:20109	 l-p:0.23624829947948456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1006
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]], device='cuda:0')
 pt:tensor([[5.1252, 5.4112, 5.2569],
        [5.1252, 4.9473, 4.6141],
        [5.1252, 5.2631, 5.0241],
        [5.1252, 5.2058, 4.9381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1006, step:0 
model_pd.l_p.mean(): 0.1643798053264618 
model_pd.l_d.mean(): -19.135807037353516 
model_pd.lagr.mean(): -18.97142791748047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6091], device='cuda:0')), ('power', tensor([-19.9672], device='cuda:0'))])
epoch£º1006	 i:0 	 global-step:20120	 l-p:0.1643798053264618
epoch£º1006	 i:1 	 global-step:20121	 l-p:0.10754340887069702
epoch£º1006	 i:2 	 global-step:20122	 l-p:0.1693444401025772
epoch£º1006	 i:3 	 global-step:20123	 l-p:0.1065254658460617
epoch£º1006	 i:4 	 global-step:20124	 l-p:0.1711113303899765
epoch£º1006	 i:5 	 global-step:20125	 l-p:0.17351990938186646
epoch£º1006	 i:6 	 global-step:20126	 l-p:0.12270202487707138
epoch£º1006	 i:7 	 global-step:20127	 l-p:0.14781039953231812
epoch£º1006	 i:8 	 global-step:20128	 l-p:0.1494140625
epoch£º1006	 i:9 	 global-step:20129	 l-p:0.1994127631187439
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1007
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1366, 5.0807, 5.1217],
        [5.1366, 5.1359, 5.1365],
        [5.1366, 5.1365, 5.1366],
        [5.1366, 5.1144, 5.1336]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1007, step:0 
model_pd.l_p.mean(): 0.1504230946302414 
model_pd.l_d.mean(): -20.031986236572266 
model_pd.lagr.mean(): -19.881563186645508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5353], device='cuda:0')), ('power', tensor([-20.7977], device='cuda:0'))])
epoch£º1007	 i:0 	 global-step:20140	 l-p:0.1504230946302414
epoch£º1007	 i:1 	 global-step:20141	 l-p:0.23570816218852997
epoch£º1007	 i:2 	 global-step:20142	 l-p:0.13004513084888458
epoch£º1007	 i:3 	 global-step:20143	 l-p:0.11788821965456009
epoch£º1007	 i:4 	 global-step:20144	 l-p:0.14370723068714142
epoch£º1007	 i:5 	 global-step:20145	 l-p:0.1161099225282669
epoch£º1007	 i:6 	 global-step:20146	 l-p:0.1662626564502716
epoch£º1007	 i:7 	 global-step:20147	 l-p:0.11943503469228745
epoch£º1007	 i:8 	 global-step:20148	 l-p:0.13307832181453705
epoch£º1007	 i:9 	 global-step:20149	 l-p:0.15235474705696106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1008
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1417, 4.9432, 4.9756],
        [5.1417, 5.1075, 5.1354],
        [5.1417, 4.9458, 4.6184],
        [5.1417, 4.8891, 4.5978]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1008, step:0 
model_pd.l_p.mean(): 0.09332079440355301 
model_pd.l_d.mean(): -20.674802780151367 
model_pd.lagr.mean(): -20.58148193359375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4591], device='cuda:0')), ('power', tensor([-21.3698], device='cuda:0'))])
epoch£º1008	 i:0 	 global-step:20160	 l-p:0.09332079440355301
epoch£º1008	 i:1 	 global-step:20161	 l-p:0.17879557609558105
epoch£º1008	 i:2 	 global-step:20162	 l-p:0.10606471449136734
epoch£º1008	 i:3 	 global-step:20163	 l-p:0.18600770831108093
epoch£º1008	 i:4 	 global-step:20164	 l-p:0.15490514039993286
epoch£º1008	 i:5 	 global-step:20165	 l-p:0.17780974507331848
epoch£º1008	 i:6 	 global-step:20166	 l-p:0.10616443306207657
epoch£º1008	 i:7 	 global-step:20167	 l-p:0.14075703918933868
epoch£º1008	 i:8 	 global-step:20168	 l-p:0.13975130021572113
epoch£º1008	 i:9 	 global-step:20169	 l-p:0.14328685402870178
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1009
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1569, 5.1540, 5.1568],
        [5.1569, 5.1568, 5.1569],
        [5.1569, 4.9666, 5.0044],
        [5.1569, 5.0355, 5.0945]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1009, step:0 
model_pd.l_p.mean(): 0.1785639375448227 
model_pd.l_d.mean(): -20.763572692871094 
model_pd.lagr.mean(): -20.58500862121582 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4315], device='cuda:0')), ('power', tensor([-21.4313], device='cuda:0'))])
epoch£º1009	 i:0 	 global-step:20180	 l-p:0.1785639375448227
epoch£º1009	 i:1 	 global-step:20181	 l-p:0.11829858273267746
epoch£º1009	 i:2 	 global-step:20182	 l-p:0.15251460671424866
epoch£º1009	 i:3 	 global-step:20183	 l-p:0.10653159022331238
epoch£º1009	 i:4 	 global-step:20184	 l-p:0.18210077285766602
epoch£º1009	 i:5 	 global-step:20185	 l-p:0.11513230949640274
epoch£º1009	 i:6 	 global-step:20186	 l-p:0.13791383802890778
epoch£º1009	 i:7 	 global-step:20187	 l-p:0.12298592925071716
epoch£º1009	 i:8 	 global-step:20188	 l-p:0.08680791407823563
epoch£º1009	 i:9 	 global-step:20189	 l-p:0.16177257895469666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1010
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1664, 5.0089, 4.6767],
        [5.1664, 5.1663, 5.1664],
        [5.1664, 5.1664, 5.1664],
        [5.1664, 5.1444, 5.1634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1010, step:0 
model_pd.l_p.mean(): 0.12480825930833817 
model_pd.l_d.mean(): -20.134654998779297 
model_pd.lagr.mean(): -20.00984764099121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4867], device='cuda:0')), ('power', tensor([-20.8519], device='cuda:0'))])
epoch£º1010	 i:0 	 global-step:20200	 l-p:0.12480825930833817
epoch£º1010	 i:1 	 global-step:20201	 l-p:0.15216585993766785
epoch£º1010	 i:2 	 global-step:20202	 l-p:0.069109246134758
epoch£º1010	 i:3 	 global-step:20203	 l-p:0.18484240770339966
epoch£º1010	 i:4 	 global-step:20204	 l-p:0.18977439403533936
epoch£º1010	 i:5 	 global-step:20205	 l-p:0.12471301108598709
epoch£º1010	 i:6 	 global-step:20206	 l-p:0.11381294578313828
epoch£º1010	 i:7 	 global-step:20207	 l-p:0.11378882825374603
epoch£º1010	 i:8 	 global-step:20208	 l-p:0.09742294996976852
epoch£º1010	 i:9 	 global-step:20209	 l-p:0.18562446534633636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1011
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1609, 5.2313, 4.9585],
        [5.1609, 5.2614, 5.0029],
        [5.1609, 5.2513, 4.9880],
        [5.1609, 5.1604, 5.1609]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1011, step:0 
model_pd.l_p.mean(): 0.21863001585006714 
model_pd.l_d.mean(): -18.42253303527832 
model_pd.lagr.mean(): -18.203903198242188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6148], device='cuda:0')), ('power', tensor([-19.2520], device='cuda:0'))])
epoch£º1011	 i:0 	 global-step:20220	 l-p:0.21863001585006714
epoch£º1011	 i:1 	 global-step:20221	 l-p:0.09221076965332031
epoch£º1011	 i:2 	 global-step:20222	 l-p:0.1456470787525177
epoch£º1011	 i:3 	 global-step:20223	 l-p:0.11709403991699219
epoch£º1011	 i:4 	 global-step:20224	 l-p:0.10401347279548645
epoch£º1011	 i:5 	 global-step:20225	 l-p:0.14722564816474915
epoch£º1011	 i:6 	 global-step:20226	 l-p:0.13353051245212555
epoch£º1011	 i:7 	 global-step:20227	 l-p:0.1409367322921753
epoch£º1011	 i:8 	 global-step:20228	 l-p:0.12434729933738708
epoch£º1011	 i:9 	 global-step:20229	 l-p:0.13977503776550293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1012
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1618, 5.0061, 5.0604],
        [5.1618, 5.1571, 5.1615],
        [5.1618, 4.8789, 4.6621],
        [5.1618, 5.0450, 5.1039]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1012, step:0 
model_pd.l_p.mean(): 0.13124747574329376 
model_pd.l_d.mean(): -20.757272720336914 
model_pd.lagr.mean(): -20.626026153564453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4215], device='cuda:0')), ('power', tensor([-21.4147], device='cuda:0'))])
epoch£º1012	 i:0 	 global-step:20240	 l-p:0.13124747574329376
epoch£º1012	 i:1 	 global-step:20241	 l-p:0.14326228201389313
epoch£º1012	 i:2 	 global-step:20242	 l-p:0.18447448313236237
epoch£º1012	 i:3 	 global-step:20243	 l-p:0.1868242621421814
epoch£º1012	 i:4 	 global-step:20244	 l-p:0.1244642436504364
epoch£º1012	 i:5 	 global-step:20245	 l-p:0.11297730356454849
epoch£º1012	 i:6 	 global-step:20246	 l-p:0.13677793741226196
epoch£º1012	 i:7 	 global-step:20247	 l-p:0.14722394943237305
epoch£º1012	 i:8 	 global-step:20248	 l-p:0.13153617084026337
epoch£º1012	 i:9 	 global-step:20249	 l-p:0.08491931110620499
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1013
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1529, 5.0380, 4.7055],
        [5.1529, 5.1529, 5.1529],
        [5.1529, 4.8662, 4.7050],
        [5.1529, 4.8667, 4.6668]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1013, step:0 
model_pd.l_p.mean(): 0.14811095595359802 
model_pd.l_d.mean(): -20.617218017578125 
model_pd.lagr.mean(): -20.469106674194336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4566], device='cuda:0')), ('power', tensor([-21.3090], device='cuda:0'))])
epoch£º1013	 i:0 	 global-step:20260	 l-p:0.14811095595359802
epoch£º1013	 i:1 	 global-step:20261	 l-p:0.1326240599155426
epoch£º1013	 i:2 	 global-step:20262	 l-p:0.12540942430496216
epoch£º1013	 i:3 	 global-step:20263	 l-p:0.2272939234972
epoch£º1013	 i:4 	 global-step:20264	 l-p:0.10828781127929688
epoch£º1013	 i:5 	 global-step:20265	 l-p:0.11803947389125824
epoch£º1013	 i:6 	 global-step:20266	 l-p:0.10357777029275894
epoch£º1013	 i:7 	 global-step:20267	 l-p:0.15328526496887207
epoch£º1013	 i:8 	 global-step:20268	 l-p:0.168124258518219
epoch£º1013	 i:9 	 global-step:20269	 l-p:0.11684373766183853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1014
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1465, 5.5793, 5.5194],
        [5.1465, 5.1461, 5.1465],
        [5.1465, 5.1465, 5.1465],
        [5.1465, 4.8590, 4.6655]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1014, step:0 
model_pd.l_p.mean(): 0.14279697835445404 
model_pd.l_d.mean(): -19.90481185913086 
model_pd.lagr.mean(): -19.762014389038086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4851], device='cuda:0')), ('power', tensor([-20.6180], device='cuda:0'))])
epoch£º1014	 i:0 	 global-step:20280	 l-p:0.14279697835445404
epoch£º1014	 i:1 	 global-step:20281	 l-p:0.20295076072216034
epoch£º1014	 i:2 	 global-step:20282	 l-p:0.1173166036605835
epoch£º1014	 i:3 	 global-step:20283	 l-p:0.09870224446058273
epoch£º1014	 i:4 	 global-step:20284	 l-p:0.1271986961364746
epoch£º1014	 i:5 	 global-step:20285	 l-p:0.20124101638793945
epoch£º1014	 i:6 	 global-step:20286	 l-p:0.12363449484109879
epoch£º1014	 i:7 	 global-step:20287	 l-p:0.20344699919223785
epoch£º1014	 i:8 	 global-step:20288	 l-p:0.0984092727303505
epoch£º1014	 i:9 	 global-step:20289	 l-p:0.15886704623699188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1015
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1291, 4.9690, 4.6336],
        [5.1291, 5.1230, 5.1287],
        [5.1291, 5.1271, 5.1290],
        [5.1291, 5.4944, 5.3900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1015, step:0 
model_pd.l_p.mean(): 0.15481004118919373 
model_pd.l_d.mean(): -20.198862075805664 
model_pd.lagr.mean(): -20.044052124023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5137], device='cuda:0')), ('power', tensor([-20.9444], device='cuda:0'))])
epoch£º1015	 i:0 	 global-step:20300	 l-p:0.15481004118919373
epoch£º1015	 i:1 	 global-step:20301	 l-p:0.13961882889270782
epoch£º1015	 i:2 	 global-step:20302	 l-p:0.1217905804514885
epoch£º1015	 i:3 	 global-step:20303	 l-p:0.15762761235237122
epoch£º1015	 i:4 	 global-step:20304	 l-p:0.1036502942442894
epoch£º1015	 i:5 	 global-step:20305	 l-p:0.23735858500003815
epoch£º1015	 i:6 	 global-step:20306	 l-p:0.16273273527622223
epoch£º1015	 i:7 	 global-step:20307	 l-p:0.15293778479099274
epoch£º1015	 i:8 	 global-step:20308	 l-p:0.3152689039707184
epoch£º1015	 i:9 	 global-step:20309	 l-p:0.12321940809488297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1016
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1158, 5.1155, 5.1158],
        [5.1158, 5.1158, 5.1158],
        [5.1158, 5.0594, 5.1007],
        [5.1158, 5.1142, 5.1158]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1016, step:0 
model_pd.l_p.mean(): 0.14526699483394623 
model_pd.l_d.mean(): -19.089012145996094 
model_pd.lagr.mean(): -18.943744659423828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4957], device='cuda:0')), ('power', tensor([-19.8041], device='cuda:0'))])
epoch£º1016	 i:0 	 global-step:20320	 l-p:0.14526699483394623
epoch£º1016	 i:1 	 global-step:20321	 l-p:0.25461241602897644
epoch£º1016	 i:2 	 global-step:20322	 l-p:0.10900892317295074
epoch£º1016	 i:3 	 global-step:20323	 l-p:0.14531229436397552
epoch£º1016	 i:4 	 global-step:20324	 l-p:0.10487870872020721
epoch£º1016	 i:5 	 global-step:20325	 l-p:0.10665091872215271
epoch£º1016	 i:6 	 global-step:20326	 l-p:0.31025147438049316
epoch£º1016	 i:7 	 global-step:20327	 l-p:0.1575128585100174
epoch£º1016	 i:8 	 global-step:20328	 l-p:0.21397919952869415
epoch£º1016	 i:9 	 global-step:20329	 l-p:0.1049773320555687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1017
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1193, 5.1192, 5.1193],
        [5.1193, 5.0269, 5.0823],
        [5.1193, 4.8834, 4.8787],
        [5.1193, 5.1193, 5.1193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1017, step:0 
model_pd.l_p.mean(): 0.11636270582675934 
model_pd.l_d.mean(): -20.148557662963867 
model_pd.lagr.mean(): -20.032194137573242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5048], device='cuda:0')), ('power', tensor([-20.8845], device='cuda:0'))])
epoch£º1017	 i:0 	 global-step:20340	 l-p:0.11636270582675934
epoch£º1017	 i:1 	 global-step:20341	 l-p:0.13033366203308105
epoch£º1017	 i:2 	 global-step:20342	 l-p:0.13101747632026672
epoch£º1017	 i:3 	 global-step:20343	 l-p:0.3613928258419037
epoch£º1017	 i:4 	 global-step:20344	 l-p:0.14744722843170166
epoch£º1017	 i:5 	 global-step:20345	 l-p:0.11778347939252853
epoch£º1017	 i:6 	 global-step:20346	 l-p:0.1308291256427765
epoch£º1017	 i:7 	 global-step:20347	 l-p:0.1773826628923416
epoch£º1017	 i:8 	 global-step:20348	 l-p:0.21233201026916504
epoch£º1017	 i:9 	 global-step:20349	 l-p:0.16419808566570282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1018
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1133, 5.1086, 5.1131],
        [5.1133, 5.0305, 5.0832],
        [5.1133, 5.0785, 4.7631],
        [5.1133, 4.8341, 4.7400]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1018, step:0 
model_pd.l_p.mean(): 0.12154573947191238 
model_pd.l_d.mean(): -19.656373977661133 
model_pd.lagr.mean(): -19.534828186035156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5108], device='cuda:0')), ('power', tensor([-20.3930], device='cuda:0'))])
epoch£º1018	 i:0 	 global-step:20360	 l-p:0.12154573947191238
epoch£º1018	 i:1 	 global-step:20361	 l-p:0.4703291952610016
epoch£º1018	 i:2 	 global-step:20362	 l-p:0.14550508558750153
epoch£º1018	 i:3 	 global-step:20363	 l-p:0.17101380228996277
epoch£º1018	 i:4 	 global-step:20364	 l-p:0.13422904908657074
epoch£º1018	 i:5 	 global-step:20365	 l-p:0.09778034687042236
epoch£º1018	 i:6 	 global-step:20366	 l-p:0.19694910943508148
epoch£º1018	 i:7 	 global-step:20367	 l-p:0.13892433047294617
epoch£º1018	 i:8 	 global-step:20368	 l-p:0.12440168112516403
epoch£º1018	 i:9 	 global-step:20369	 l-p:0.12923164665699005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1019
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1187, 5.1078, 5.1178],
        [5.1187, 4.8502, 4.7859],
        [5.1187, 5.1363, 4.8403],
        [5.1187, 5.5129, 5.4277]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1019, step:0 
model_pd.l_p.mean(): 0.13997361063957214 
model_pd.l_d.mean(): -20.54766273498535 
model_pd.lagr.mean(): -20.407690048217773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4368], device='cuda:0')), ('power', tensor([-21.2184], device='cuda:0'))])
epoch£º1019	 i:0 	 global-step:20380	 l-p:0.13997361063957214
epoch£º1019	 i:1 	 global-step:20381	 l-p:0.20216548442840576
epoch£º1019	 i:2 	 global-step:20382	 l-p:0.20225150883197784
epoch£º1019	 i:3 	 global-step:20383	 l-p:0.14261554181575775
epoch£º1019	 i:4 	 global-step:20384	 l-p:0.13311435282230377
epoch£º1019	 i:5 	 global-step:20385	 l-p:0.2760297656059265
epoch£º1019	 i:6 	 global-step:20386	 l-p:0.12261173874139786
epoch£º1019	 i:7 	 global-step:20387	 l-p:0.18234698474407196
epoch£º1019	 i:8 	 global-step:20388	 l-p:0.09090335667133331
epoch£º1019	 i:9 	 global-step:20389	 l-p:0.11925990134477615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1020
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1252, 4.8563, 4.5822],
        [5.1252, 5.0341, 5.0892],
        [5.1252, 4.8347, 4.6410],
        [5.1252, 4.9829, 4.6465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1020, step:0 
model_pd.l_p.mean(): 0.13831214606761932 
model_pd.l_d.mean(): -20.48356819152832 
model_pd.lagr.mean(): -20.345256805419922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4680], device='cuda:0')), ('power', tensor([-21.1855], device='cuda:0'))])
epoch£º1020	 i:0 	 global-step:20400	 l-p:0.13831214606761932
epoch£º1020	 i:1 	 global-step:20401	 l-p:0.2454630583524704
epoch£º1020	 i:2 	 global-step:20402	 l-p:0.16865716874599457
epoch£º1020	 i:3 	 global-step:20403	 l-p:0.26340430974960327
epoch£º1020	 i:4 	 global-step:20404	 l-p:0.10128603875637054
epoch£º1020	 i:5 	 global-step:20405	 l-p:0.07470010221004486
epoch£º1020	 i:6 	 global-step:20406	 l-p:0.12930086255073547
epoch£º1020	 i:7 	 global-step:20407	 l-p:0.17028190195560455
epoch£º1020	 i:8 	 global-step:20408	 l-p:0.12787510454654694
epoch£º1020	 i:9 	 global-step:20409	 l-p:0.13164454698562622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1021
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1290, 4.8404, 4.6931],
        [5.1290, 5.1290, 5.1290],
        [5.1290, 4.8505, 4.5979],
        [5.1290, 4.8678, 4.8190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1021, step:0 
model_pd.l_p.mean(): 0.12966667115688324 
model_pd.l_d.mean(): -20.747835159301758 
model_pd.lagr.mean(): -20.618167877197266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4403], device='cuda:0')), ('power', tensor([-21.4244], device='cuda:0'))])
epoch£º1021	 i:0 	 global-step:20420	 l-p:0.12966667115688324
epoch£º1021	 i:1 	 global-step:20421	 l-p:0.1240512803196907
epoch£º1021	 i:2 	 global-step:20422	 l-p:0.10990466177463531
epoch£º1021	 i:3 	 global-step:20423	 l-p:0.2573160231113434
epoch£º1021	 i:4 	 global-step:20424	 l-p:0.1292945146560669
epoch£º1021	 i:5 	 global-step:20425	 l-p:0.1514575332403183
epoch£º1021	 i:6 	 global-step:20426	 l-p:0.2206767201423645
epoch£º1021	 i:7 	 global-step:20427	 l-p:0.24811771512031555
epoch£º1021	 i:8 	 global-step:20428	 l-p:0.10552665591239929
epoch£º1021	 i:9 	 global-step:20429	 l-p:0.14621511101722717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1022
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1155, 5.1153, 5.1155],
        [5.1155, 4.8449, 4.5705],
        [5.1155, 4.9113, 4.9413],
        [5.1155, 5.1155, 5.1155]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1022, step:0 
model_pd.l_p.mean(): 0.1431412249803543 
model_pd.l_d.mean(): -20.697006225585938 
model_pd.lagr.mean(): -20.553865432739258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4387], device='cuda:0')), ('power', tensor([-21.3713], device='cuda:0'))])
epoch£º1022	 i:0 	 global-step:20440	 l-p:0.1431412249803543
epoch£º1022	 i:1 	 global-step:20441	 l-p:0.17026515305042267
epoch£º1022	 i:2 	 global-step:20442	 l-p:0.1795264035463333
epoch£º1022	 i:3 	 global-step:20443	 l-p:0.135905459523201
epoch£º1022	 i:4 	 global-step:20444	 l-p:0.11402938514947891
epoch£º1022	 i:5 	 global-step:20445	 l-p:0.1115712970495224
epoch£º1022	 i:6 	 global-step:20446	 l-p:0.21673449873924255
epoch£º1022	 i:7 	 global-step:20447	 l-p:0.15465813875198364
epoch£º1022	 i:8 	 global-step:20448	 l-p:0.40534284710884094
epoch£º1022	 i:9 	 global-step:20449	 l-p:0.12017935514450073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1023
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1149, 5.1142, 5.1149],
        [5.1149, 4.8307, 4.5886],
        [5.1149, 5.1149, 5.1149],
        [5.1149, 4.9460, 4.9973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1023, step:0 
model_pd.l_p.mean(): 0.14662683010101318 
model_pd.l_d.mean(): -21.068565368652344 
model_pd.lagr.mean(): -20.921937942504883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3761], device='cuda:0')), ('power', tensor([-21.6830], device='cuda:0'))])
epoch£º1023	 i:0 	 global-step:20460	 l-p:0.14662683010101318
epoch£º1023	 i:1 	 global-step:20461	 l-p:0.2814285457134247
epoch£º1023	 i:2 	 global-step:20462	 l-p:0.11949145048856735
epoch£º1023	 i:3 	 global-step:20463	 l-p:0.3702585995197296
epoch£º1023	 i:4 	 global-step:20464	 l-p:0.18219228088855743
epoch£º1023	 i:5 	 global-step:20465	 l-p:0.13690650463104248
epoch£º1023	 i:6 	 global-step:20466	 l-p:0.12249879539012909
epoch£º1023	 i:7 	 global-step:20467	 l-p:0.09024956822395325
epoch£º1023	 i:8 	 global-step:20468	 l-p:0.12366330623626709
epoch£º1023	 i:9 	 global-step:20469	 l-p:0.13913120329380035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1024
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1189, 5.1189, 5.1189],
        [5.1189, 5.0848, 4.7694],
        [5.1189, 5.0597, 4.7365],
        [5.1189, 5.1189, 5.1189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1024, step:0 
model_pd.l_p.mean(): 0.22122202813625336 
model_pd.l_d.mean(): -19.722858428955078 
model_pd.lagr.mean(): -19.501636505126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5452], device='cuda:0')), ('power', tensor([-20.4954], device='cuda:0'))])
epoch£º1024	 i:0 	 global-step:20480	 l-p:0.22122202813625336
epoch£º1024	 i:1 	 global-step:20481	 l-p:0.1408095508813858
epoch£º1024	 i:2 	 global-step:20482	 l-p:0.11717551946640015
epoch£º1024	 i:3 	 global-step:20483	 l-p:0.2911040484905243
epoch£º1024	 i:4 	 global-step:20484	 l-p:0.12958529591560364
epoch£º1024	 i:5 	 global-step:20485	 l-p:0.09609314799308777
epoch£º1024	 i:6 	 global-step:20486	 l-p:0.182968407869339
epoch£º1024	 i:7 	 global-step:20487	 l-p:0.12843148410320282
epoch£º1024	 i:8 	 global-step:20488	 l-p:0.12212300300598145
epoch£º1024	 i:9 	 global-step:20489	 l-p:0.179749995470047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1025
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1262, 5.1248, 5.1262],
        [5.1262, 5.1209, 5.1260],
        [5.1262, 5.1262, 5.1262],
        [5.1262, 5.1254, 5.1262]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1025, step:0 
model_pd.l_p.mean(): 0.22994351387023926 
model_pd.l_d.mean(): -20.74462127685547 
model_pd.lagr.mean(): -20.514677047729492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4183], device='cuda:0')), ('power', tensor([-21.3986], device='cuda:0'))])
epoch£º1025	 i:0 	 global-step:20500	 l-p:0.22994351387023926
epoch£º1025	 i:1 	 global-step:20501	 l-p:0.15205322206020355
epoch£º1025	 i:2 	 global-step:20502	 l-p:0.13477134704589844
epoch£º1025	 i:3 	 global-step:20503	 l-p:0.1301054209470749
epoch£º1025	 i:4 	 global-step:20504	 l-p:0.17850235104560852
epoch£º1025	 i:5 	 global-step:20505	 l-p:0.13115698099136353
epoch£º1025	 i:6 	 global-step:20506	 l-p:0.127597838640213
epoch£º1025	 i:7 	 global-step:20507	 l-p:0.12035930156707764
epoch£º1025	 i:8 	 global-step:20508	 l-p:0.1768106371164322
epoch£º1025	 i:9 	 global-step:20509	 l-p:0.16535057127475739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1026
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7151,  0.6395,  1.0000,  0.5719,
          1.0000,  0.8943, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4406,  0.3353,  1.0000,  0.2551,
          1.0000,  0.7609, 31.6228]], device='cuda:0')
 pt:tensor([[5.1327, 5.5312, 5.4482],
        [5.1327, 4.8443, 4.6315],
        [5.1327, 5.1593, 4.8668],
        [5.1327, 4.8705, 4.5866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1026, step:0 
model_pd.l_p.mean(): 0.080235555768013 
model_pd.l_d.mean(): -19.945703506469727 
model_pd.lagr.mean(): -19.865467071533203 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5395], device='cuda:0')), ('power', tensor([-20.7148], device='cuda:0'))])
epoch£º1026	 i:0 	 global-step:20520	 l-p:0.080235555768013
epoch£º1026	 i:1 	 global-step:20521	 l-p:0.13060306012630463
epoch£º1026	 i:2 	 global-step:20522	 l-p:0.1667933613061905
epoch£º1026	 i:3 	 global-step:20523	 l-p:0.22205707430839539
epoch£º1026	 i:4 	 global-step:20524	 l-p:0.15909934043884277
epoch£º1026	 i:5 	 global-step:20525	 l-p:0.14416193962097168
epoch£º1026	 i:6 	 global-step:20526	 l-p:0.14572080969810486
epoch£º1026	 i:7 	 global-step:20527	 l-p:0.15973220765590668
epoch£º1026	 i:8 	 global-step:20528	 l-p:0.14077085256576538
epoch£º1026	 i:9 	 global-step:20529	 l-p:0.1579628586769104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1027
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1368, 4.8467, 4.6710],
        [5.1368, 4.9715, 4.6363],
        [5.1368, 5.0467, 5.1015],
        [5.1368, 4.8755, 4.8259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1027, step:0 
model_pd.l_p.mean(): 0.13869570195674896 
model_pd.l_d.mean(): -19.98228645324707 
model_pd.lagr.mean(): -19.843591690063477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5464], device='cuda:0')), ('power', tensor([-20.7589], device='cuda:0'))])
epoch£º1027	 i:0 	 global-step:20540	 l-p:0.13869570195674896
epoch£º1027	 i:1 	 global-step:20541	 l-p:0.13576269149780273
epoch£º1027	 i:2 	 global-step:20542	 l-p:0.15433652698993683
epoch£º1027	 i:3 	 global-step:20543	 l-p:0.1394682228565216
epoch£º1027	 i:4 	 global-step:20544	 l-p:0.09841618686914444
epoch£º1027	 i:5 	 global-step:20545	 l-p:0.24297723174095154
epoch£º1027	 i:6 	 global-step:20546	 l-p:0.16181036829948425
epoch£º1027	 i:7 	 global-step:20547	 l-p:0.12641538679599762
epoch£º1027	 i:8 	 global-step:20548	 l-p:0.1321556568145752
epoch£º1027	 i:9 	 global-step:20549	 l-p:0.1436501145362854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1028
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1411, 5.1404, 5.1411],
        [5.1411, 5.1411, 5.1411],
        [5.1411, 5.1366, 5.1409],
        [5.1411, 5.1192, 5.1382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1028, step:0 
model_pd.l_p.mean(): 0.1808374673128128 
model_pd.l_d.mean(): -20.979307174682617 
model_pd.lagr.mean(): -20.79846954345703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3782], device='cuda:0')), ('power', tensor([-21.5949], device='cuda:0'))])
epoch£º1028	 i:0 	 global-step:20560	 l-p:0.1808374673128128
epoch£º1028	 i:1 	 global-step:20561	 l-p:0.1884308010339737
epoch£º1028	 i:2 	 global-step:20562	 l-p:0.11248388886451721
epoch£º1028	 i:3 	 global-step:20563	 l-p:0.1485716551542282
epoch£º1028	 i:4 	 global-step:20564	 l-p:0.20016159117221832
epoch£º1028	 i:5 	 global-step:20565	 l-p:0.15664328634738922
epoch£º1028	 i:6 	 global-step:20566	 l-p:0.1048930361866951
epoch£º1028	 i:7 	 global-step:20567	 l-p:0.1288626492023468
epoch£º1028	 i:8 	 global-step:20568	 l-p:0.1068594753742218
epoch£º1028	 i:9 	 global-step:20569	 l-p:0.11530087888240814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1029
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1523, 4.8890, 4.6115],
        [5.1523, 5.1523, 5.1523],
        [5.1523, 5.1523, 5.1523],
        [5.1523, 4.9417, 4.9640]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1029, step:0 
model_pd.l_p.mean(): 0.1298840343952179 
model_pd.l_d.mean(): -20.835355758666992 
model_pd.lagr.mean(): -20.70547103881836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4014], device='cuda:0')), ('power', tensor([-21.4731], device='cuda:0'))])
epoch£º1029	 i:0 	 global-step:20580	 l-p:0.1298840343952179
epoch£º1029	 i:1 	 global-step:20581	 l-p:0.12820254266262054
epoch£º1029	 i:2 	 global-step:20582	 l-p:0.08698436617851257
epoch£º1029	 i:3 	 global-step:20583	 l-p:0.16460509598255157
epoch£º1029	 i:4 	 global-step:20584	 l-p:0.24871020019054413
epoch£º1029	 i:5 	 global-step:20585	 l-p:0.10574682056903839
epoch£º1029	 i:6 	 global-step:20586	 l-p:0.12351831048727036
epoch£º1029	 i:7 	 global-step:20587	 l-p:0.13191723823547363
epoch£º1029	 i:8 	 global-step:20588	 l-p:0.10577324032783508
epoch£º1029	 i:9 	 global-step:20589	 l-p:0.20402085781097412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1030
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1454, 5.1325, 5.1442],
        [5.1454, 5.1078, 5.1380],
        [5.1454, 5.2265, 4.9582],
        [5.1454, 4.9161, 4.9184]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1030, step:0 
model_pd.l_p.mean(): 0.1554870307445526 
model_pd.l_d.mean(): -20.25390625 
model_pd.lagr.mean(): -20.098419189453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4887], device='cuda:0')), ('power', tensor([-20.9745], device='cuda:0'))])
epoch£º1030	 i:0 	 global-step:20600	 l-p:0.1554870307445526
epoch£º1030	 i:1 	 global-step:20601	 l-p:0.12938269972801208
epoch£º1030	 i:2 	 global-step:20602	 l-p:0.1665850728750229
epoch£º1030	 i:3 	 global-step:20603	 l-p:0.09938918799161911
epoch£º1030	 i:4 	 global-step:20604	 l-p:0.11035978049039841
epoch£º1030	 i:5 	 global-step:20605	 l-p:0.16581736505031586
epoch£º1030	 i:6 	 global-step:20606	 l-p:0.12724222242832184
epoch£º1030	 i:7 	 global-step:20607	 l-p:0.24691051244735718
epoch£º1030	 i:8 	 global-step:20608	 l-p:0.08267104625701904
epoch£º1030	 i:9 	 global-step:20609	 l-p:0.16704709827899933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1031
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1434, 5.4030, 5.2317],
        [5.1434, 5.0001, 4.6643],
        [5.1434, 4.8701, 4.7888],
        [5.1434, 5.1212, 5.1404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1031, step:0 
model_pd.l_p.mean(): 0.13041123747825623 
model_pd.l_d.mean(): -19.927234649658203 
model_pd.lagr.mean(): -19.796823501586914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4974], device='cuda:0')), ('power', tensor([-20.6531], device='cuda:0'))])
epoch£º1031	 i:0 	 global-step:20620	 l-p:0.13041123747825623
epoch£º1031	 i:1 	 global-step:20621	 l-p:0.16392149031162262
epoch£º1031	 i:2 	 global-step:20622	 l-p:0.1444711685180664
epoch£º1031	 i:3 	 global-step:20623	 l-p:0.17783069610595703
epoch£º1031	 i:4 	 global-step:20624	 l-p:0.12498804181814194
epoch£º1031	 i:5 	 global-step:20625	 l-p:0.1299886703491211
epoch£º1031	 i:6 	 global-step:20626	 l-p:0.13123469054698944
epoch£º1031	 i:7 	 global-step:20627	 l-p:0.08776940405368805
epoch£º1031	 i:8 	 global-step:20628	 l-p:0.10353347659111023
epoch£º1031	 i:9 	 global-step:20629	 l-p:0.2409897893667221
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1032
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1483, 4.9087, 4.8966],
        [5.1483, 5.0716, 5.1220],
        [5.1483, 5.1479, 5.1483],
        [5.1483, 5.0055, 5.0634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1032, step:0 
model_pd.l_p.mean(): 0.18315541744232178 
model_pd.l_d.mean(): -20.24297332763672 
model_pd.lagr.mean(): -20.059818267822266 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4741], device='cuda:0')), ('power', tensor([-20.9485], device='cuda:0'))])
epoch£º1032	 i:0 	 global-step:20640	 l-p:0.18315541744232178
epoch£º1032	 i:1 	 global-step:20641	 l-p:0.1307927966117859
epoch£º1032	 i:2 	 global-step:20642	 l-p:0.17223501205444336
epoch£º1032	 i:3 	 global-step:20643	 l-p:0.14780980348587036
epoch£º1032	 i:4 	 global-step:20644	 l-p:0.12891890108585358
epoch£º1032	 i:5 	 global-step:20645	 l-p:0.14626243710517883
epoch£º1032	 i:6 	 global-step:20646	 l-p:0.10486256331205368
epoch£º1032	 i:7 	 global-step:20647	 l-p:0.1699647456407547
epoch£º1032	 i:8 	 global-step:20648	 l-p:0.07945825159549713
epoch£º1032	 i:9 	 global-step:20649	 l-p:0.1350594311952591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1033
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1615, 5.1614, 5.1615],
        [5.1615, 4.8742, 4.7128],
        [5.1615, 4.9435, 4.9580],
        [5.1615, 5.1603, 5.1614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1033, step:0 
model_pd.l_p.mean(): 0.07686477154493332 
model_pd.l_d.mean(): -20.554073333740234 
model_pd.lagr.mean(): -20.477209091186523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4599], device='cuda:0')), ('power', tensor([-21.2485], device='cuda:0'))])
epoch£º1033	 i:0 	 global-step:20660	 l-p:0.07686477154493332
epoch£º1033	 i:1 	 global-step:20661	 l-p:0.13988631963729858
epoch£º1033	 i:2 	 global-step:20662	 l-p:0.2017863392829895
epoch£º1033	 i:3 	 global-step:20663	 l-p:0.16080406308174133
epoch£º1033	 i:4 	 global-step:20664	 l-p:0.1314130276441574
epoch£º1033	 i:5 	 global-step:20665	 l-p:0.15249738097190857
epoch£º1033	 i:6 	 global-step:20666	 l-p:0.12943100929260254
epoch£º1033	 i:7 	 global-step:20667	 l-p:0.12368326634168625
epoch£º1033	 i:8 	 global-step:20668	 l-p:0.1531221717596054
epoch£º1033	 i:9 	 global-step:20669	 l-p:0.10059810429811478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1034
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1613, 4.9152, 4.6176],
        [5.1613, 5.1502, 5.1604],
        [5.1613, 5.1055, 5.1465],
        [5.1613, 5.1584, 5.1612]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1034, step:0 
model_pd.l_p.mean(): 0.22556808590888977 
model_pd.l_d.mean(): -20.911006927490234 
model_pd.lagr.mean(): -20.68543815612793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3864], device='cuda:0')), ('power', tensor([-21.5343], device='cuda:0'))])
epoch£º1034	 i:0 	 global-step:20680	 l-p:0.22556808590888977
epoch£º1034	 i:1 	 global-step:20681	 l-p:0.13739250600337982
epoch£º1034	 i:2 	 global-step:20682	 l-p:0.1145501509308815
epoch£º1034	 i:3 	 global-step:20683	 l-p:0.13954316079616547
epoch£º1034	 i:4 	 global-step:20684	 l-p:0.20038191974163055
epoch£º1034	 i:5 	 global-step:20685	 l-p:0.17890895903110504
epoch£º1034	 i:6 	 global-step:20686	 l-p:0.061273954808712006
epoch£º1034	 i:7 	 global-step:20687	 l-p:0.10465677827596664
epoch£º1034	 i:8 	 global-step:20688	 l-p:0.08442052453756332
epoch£º1034	 i:9 	 global-step:20689	 l-p:0.11470969766378403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1035
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1691, 5.1560, 5.1679],
        [5.1691, 5.1603, 5.1685],
        [5.1691, 4.9056, 4.6320],
        [5.1691, 5.1505, 5.1669]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1035, step:0 
model_pd.l_p.mean(): 0.08555297553539276 
model_pd.l_d.mean(): -19.69300651550293 
model_pd.lagr.mean(): -19.607454299926758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4842], device='cuda:0')), ('power', tensor([-20.4028], device='cuda:0'))])
epoch£º1035	 i:0 	 global-step:20700	 l-p:0.08555297553539276
epoch£º1035	 i:1 	 global-step:20701	 l-p:0.14804042875766754
epoch£º1035	 i:2 	 global-step:20702	 l-p:0.17288434505462646
epoch£º1035	 i:3 	 global-step:20703	 l-p:0.1164097785949707
epoch£º1035	 i:4 	 global-step:20704	 l-p:0.09991317242383957
epoch£º1035	 i:5 	 global-step:20705	 l-p:0.13887833058834076
epoch£º1035	 i:6 	 global-step:20706	 l-p:0.13969393074512482
epoch£º1035	 i:7 	 global-step:20707	 l-p:0.125407412648201
epoch£º1035	 i:8 	 global-step:20708	 l-p:0.19550780951976776
epoch£º1035	 i:9 	 global-step:20709	 l-p:0.11647123098373413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1036
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1667, 5.1429, 5.1633],
        [5.1667, 5.6049, 5.5474],
        [5.1667, 4.9745, 5.0113],
        [5.1667, 4.9918, 5.0390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1036, step:0 
model_pd.l_p.mean(): 0.1430596113204956 
model_pd.l_d.mean(): -19.42757225036621 
model_pd.lagr.mean(): -19.284513473510742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5482], device='cuda:0')), ('power', tensor([-20.1999], device='cuda:0'))])
epoch£º1036	 i:0 	 global-step:20720	 l-p:0.1430596113204956
epoch£º1036	 i:1 	 global-step:20721	 l-p:0.15321585536003113
epoch£º1036	 i:2 	 global-step:20722	 l-p:0.16859300434589386
epoch£º1036	 i:3 	 global-step:20723	 l-p:0.10092204809188843
epoch£º1036	 i:4 	 global-step:20724	 l-p:0.11474323272705078
epoch£º1036	 i:5 	 global-step:20725	 l-p:0.08760374039411545
epoch£º1036	 i:6 	 global-step:20726	 l-p:0.16151568293571472
epoch£º1036	 i:7 	 global-step:20727	 l-p:0.1518133580684662
epoch£º1036	 i:8 	 global-step:20728	 l-p:0.12983836233615875
epoch£º1036	 i:9 	 global-step:20729	 l-p:0.15521304309368134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1037
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1624, 5.1417, 5.1597],
        [5.1624, 4.9823, 5.0269],
        [5.1624, 4.8750, 4.7160],
        [5.1624, 5.1582, 5.1622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1037, step:0 
model_pd.l_p.mean(): 0.12345988303422928 
model_pd.l_d.mean(): -18.97972297668457 
model_pd.lagr.mean(): -18.85626220703125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5491], device='cuda:0')), ('power', tensor([-19.7481], device='cuda:0'))])
epoch£º1037	 i:0 	 global-step:20740	 l-p:0.12345988303422928
epoch£º1037	 i:1 	 global-step:20741	 l-p:0.1646672636270523
epoch£º1037	 i:2 	 global-step:20742	 l-p:0.1559167206287384
epoch£º1037	 i:3 	 global-step:20743	 l-p:0.07071693241596222
epoch£º1037	 i:4 	 global-step:20744	 l-p:0.14829804003238678
epoch£º1037	 i:5 	 global-step:20745	 l-p:0.11902708560228348
epoch£º1037	 i:6 	 global-step:20746	 l-p:0.13888803124427795
epoch£º1037	 i:7 	 global-step:20747	 l-p:0.1737786829471588
epoch£º1037	 i:8 	 global-step:20748	 l-p:0.12772059440612793
epoch£º1037	 i:9 	 global-step:20749	 l-p:0.13194145262241364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1038
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1691, 4.9174, 4.8833],
        [5.1691, 5.0612, 4.7290],
        [5.1691, 4.9223, 4.8969],
        [5.1691, 5.1691, 5.1691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1038, step:0 
model_pd.l_p.mean(): 0.13930030167102814 
model_pd.l_d.mean(): -19.811107635498047 
model_pd.lagr.mean(): -19.67180824279785 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4396], device='cuda:0')), ('power', tensor([-20.4767], device='cuda:0'))])
epoch£º1038	 i:0 	 global-step:20760	 l-p:0.13930030167102814
epoch£º1038	 i:1 	 global-step:20761	 l-p:0.09136458486318588
epoch£º1038	 i:2 	 global-step:20762	 l-p:0.12359119206666946
epoch£º1038	 i:3 	 global-step:20763	 l-p:0.19817042350769043
epoch£º1038	 i:4 	 global-step:20764	 l-p:0.08150454610586166
epoch£º1038	 i:5 	 global-step:20765	 l-p:0.17532813549041748
epoch£º1038	 i:6 	 global-step:20766	 l-p:0.15679989755153656
epoch£º1038	 i:7 	 global-step:20767	 l-p:0.10947353392839432
epoch£º1038	 i:8 	 global-step:20768	 l-p:0.16786527633666992
epoch£º1038	 i:9 	 global-step:20769	 l-p:0.11996845155954361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1039
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1631, 5.0724, 5.1272],
        [5.1631, 4.9968, 5.0480],
        [5.1631, 5.1085, 5.1488],
        [5.1631, 4.9702, 4.6419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1039, step:0 
model_pd.l_p.mean(): 0.14671745896339417 
model_pd.l_d.mean(): -20.596269607543945 
model_pd.lagr.mean(): -20.449552536010742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4634], device='cuda:0')), ('power', tensor([-21.2948], device='cuda:0'))])
epoch£º1039	 i:0 	 global-step:20780	 l-p:0.14671745896339417
epoch£º1039	 i:1 	 global-step:20781	 l-p:0.12054971605539322
epoch£º1039	 i:2 	 global-step:20782	 l-p:0.1204138994216919
epoch£º1039	 i:3 	 global-step:20783	 l-p:0.11561912298202515
epoch£º1039	 i:4 	 global-step:20784	 l-p:0.1303061693906784
epoch£º1039	 i:5 	 global-step:20785	 l-p:0.12680204212665558
epoch£º1039	 i:6 	 global-step:20786	 l-p:0.1553502231836319
epoch£º1039	 i:7 	 global-step:20787	 l-p:0.12302383780479431
epoch£º1039	 i:8 	 global-step:20788	 l-p:0.28696945309638977
epoch£º1039	 i:9 	 global-step:20789	 l-p:0.09892957657575607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1040
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1439, 5.1439, 5.1439],
        [5.1439, 5.1090, 5.1374],
        [5.1439, 4.8660, 4.6130],
        [5.1439, 5.1409, 5.1438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1040, step:0 
model_pd.l_p.mean(): 0.09292779862880707 
model_pd.l_d.mean(): -20.113765716552734 
model_pd.lagr.mean(): -20.020837783813477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5286], device='cuda:0')), ('power', tensor([-20.8736], device='cuda:0'))])
epoch£º1040	 i:0 	 global-step:20800	 l-p:0.09292779862880707
epoch£º1040	 i:1 	 global-step:20801	 l-p:0.13291817903518677
epoch£º1040	 i:2 	 global-step:20802	 l-p:0.15300282835960388
epoch£º1040	 i:3 	 global-step:20803	 l-p:0.14341770112514496
epoch£º1040	 i:4 	 global-step:20804	 l-p:0.11306875944137573
epoch£º1040	 i:5 	 global-step:20805	 l-p:0.18566030263900757
epoch£º1040	 i:6 	 global-step:20806	 l-p:0.17460250854492188
epoch£º1040	 i:7 	 global-step:20807	 l-p:0.18417640030384064
epoch£º1040	 i:8 	 global-step:20808	 l-p:0.13887643814086914
epoch£º1040	 i:9 	 global-step:20809	 l-p:0.14950953423976898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1041
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1435, 5.1435, 5.1435],
        [5.1435, 4.8875, 4.8489],
        [5.1435, 5.1392, 5.1433],
        [5.1435, 5.1196, 5.1401]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1041, step:0 
model_pd.l_p.mean(): 0.11868512630462646 
model_pd.l_d.mean(): -18.896991729736328 
model_pd.lagr.mean(): -18.77830696105957 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5565], device='cuda:0')), ('power', tensor([-19.6720], device='cuda:0'))])
epoch£º1041	 i:0 	 global-step:20820	 l-p:0.11868512630462646
epoch£º1041	 i:1 	 global-step:20821	 l-p:0.13506783545017242
epoch£º1041	 i:2 	 global-step:20822	 l-p:0.1126537099480629
epoch£º1041	 i:3 	 global-step:20823	 l-p:0.1199733167886734
epoch£º1041	 i:4 	 global-step:20824	 l-p:0.1566922962665558
epoch£º1041	 i:5 	 global-step:20825	 l-p:0.20945020020008087
epoch£º1041	 i:6 	 global-step:20826	 l-p:0.1313493549823761
epoch£º1041	 i:7 	 global-step:20827	 l-p:0.20799857378005981
epoch£º1041	 i:8 	 global-step:20828	 l-p:0.163107767701149
epoch£º1041	 i:9 	 global-step:20829	 l-p:0.06930456310510635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1042
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1550, 5.1550, 5.1550],
        [5.1550, 4.9815, 5.0298],
        [5.1550, 5.1548, 5.1550],
        [5.1550, 4.9565, 4.6287]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1042, step:0 
model_pd.l_p.mean(): 0.1509019285440445 
model_pd.l_d.mean(): -20.426204681396484 
model_pd.lagr.mean(): -20.27530288696289 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4577], device='cuda:0')), ('power', tensor([-21.1170], device='cuda:0'))])
epoch£º1042	 i:0 	 global-step:20840	 l-p:0.1509019285440445
epoch£º1042	 i:1 	 global-step:20841	 l-p:0.12213606387376785
epoch£º1042	 i:2 	 global-step:20842	 l-p:0.13385675847530365
epoch£º1042	 i:3 	 global-step:20843	 l-p:0.11775048822164536
epoch£º1042	 i:4 	 global-step:20844	 l-p:0.15210409462451935
epoch£º1042	 i:5 	 global-step:20845	 l-p:0.09863721579313278
epoch£º1042	 i:6 	 global-step:20846	 l-p:0.11466047912836075
epoch£º1042	 i:7 	 global-step:20847	 l-p:0.1582792103290558
epoch£º1042	 i:8 	 global-step:20848	 l-p:0.17999035120010376
epoch£º1042	 i:9 	 global-step:20849	 l-p:0.1347866952419281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1043
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1724, 5.1724, 5.1724],
        [5.1724, 5.0853, 5.1391],
        [5.1724, 5.5097, 5.3853],
        [5.1724, 4.9402, 4.9370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1043, step:0 
model_pd.l_p.mean(): 0.16989871859550476 
model_pd.l_d.mean(): -20.59667205810547 
model_pd.lagr.mean(): -20.426773071289062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4480], device='cuda:0')), ('power', tensor([-21.2794], device='cuda:0'))])
epoch£º1043	 i:0 	 global-step:20860	 l-p:0.16989871859550476
epoch£º1043	 i:1 	 global-step:20861	 l-p:0.11314372718334198
epoch£º1043	 i:2 	 global-step:20862	 l-p:0.16445428133010864
epoch£º1043	 i:3 	 global-step:20863	 l-p:0.1354522556066513
epoch£º1043	 i:4 	 global-step:20864	 l-p:0.134904682636261
epoch£º1043	 i:5 	 global-step:20865	 l-p:0.11237029731273651
epoch£º1043	 i:6 	 global-step:20866	 l-p:0.0682174563407898
epoch£º1043	 i:7 	 global-step:20867	 l-p:0.16025328636169434
epoch£º1043	 i:8 	 global-step:20868	 l-p:0.11425478756427765
epoch£º1043	 i:9 	 global-step:20869	 l-p:0.14812703430652618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1044
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1761, 5.1750, 5.1761],
        [5.1761, 5.1760, 5.1761],
        [5.1761, 5.1200, 5.1611],
        [5.1761, 5.0865, 4.7572]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1044, step:0 
model_pd.l_p.mean(): 0.19799336791038513 
model_pd.l_d.mean(): -20.31093978881836 
model_pd.lagr.mean(): -20.112945556640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4588], device='cuda:0')), ('power', tensor([-21.0016], device='cuda:0'))])
epoch£º1044	 i:0 	 global-step:20880	 l-p:0.19799336791038513
epoch£º1044	 i:1 	 global-step:20881	 l-p:0.14600777626037598
epoch£º1044	 i:2 	 global-step:20882	 l-p:0.040957897901535034
epoch£º1044	 i:3 	 global-step:20883	 l-p:0.13430450856685638
epoch£º1044	 i:4 	 global-step:20884	 l-p:0.1239539310336113
epoch£º1044	 i:5 	 global-step:20885	 l-p:0.13324286043643951
epoch£º1044	 i:6 	 global-step:20886	 l-p:0.17428676784038544
epoch£º1044	 i:7 	 global-step:20887	 l-p:0.11307413876056671
epoch£º1044	 i:8 	 global-step:20888	 l-p:0.10668639838695526
epoch£º1044	 i:9 	 global-step:20889	 l-p:0.14892692863941193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1045
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1725, 5.4731, 5.3256],
        [5.1725, 5.0810, 5.1360],
        [5.1725, 5.1665, 5.1722],
        [5.1725, 4.8905, 4.6630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1045, step:0 
model_pd.l_p.mean(): 0.15133541822433472 
model_pd.l_d.mean(): -19.028568267822266 
model_pd.lagr.mean(): -18.877233505249023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5326], device='cuda:0')), ('power', tensor([-19.7806], device='cuda:0'))])
epoch£º1045	 i:0 	 global-step:20900	 l-p:0.15133541822433472
epoch£º1045	 i:1 	 global-step:20901	 l-p:0.17133913934230804
epoch£º1045	 i:2 	 global-step:20902	 l-p:0.11971313506364822
epoch£º1045	 i:3 	 global-step:20903	 l-p:0.20035724341869354
epoch£º1045	 i:4 	 global-step:20904	 l-p:0.11345081031322479
epoch£º1045	 i:5 	 global-step:20905	 l-p:0.14548400044441223
epoch£º1045	 i:6 	 global-step:20906	 l-p:0.08473522216081619
epoch£º1045	 i:7 	 global-step:20907	 l-p:0.13826178014278412
epoch£º1045	 i:8 	 global-step:20908	 l-p:0.09149137139320374
epoch£º1045	 i:9 	 global-step:20909	 l-p:0.11267072707414627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1046
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1745, 4.8954, 4.7872],
        [5.1745, 5.1738, 5.1744],
        [5.1745, 5.0023, 4.6701],
        [5.1745, 5.1745, 5.1745]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1046, step:0 
model_pd.l_p.mean(): 0.10096225142478943 
model_pd.l_d.mean(): -20.342275619506836 
model_pd.lagr.mean(): -20.241313934326172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4652], device='cuda:0')), ('power', tensor([-21.0398], device='cuda:0'))])
epoch£º1046	 i:0 	 global-step:20920	 l-p:0.10096225142478943
epoch£º1046	 i:1 	 global-step:20921	 l-p:0.11622140556573868
epoch£º1046	 i:2 	 global-step:20922	 l-p:0.09130652248859406
epoch£º1046	 i:3 	 global-step:20923	 l-p:0.1598317176103592
epoch£º1046	 i:4 	 global-step:20924	 l-p:0.12429732829332352
epoch£º1046	 i:5 	 global-step:20925	 l-p:0.1153656542301178
epoch£º1046	 i:6 	 global-step:20926	 l-p:0.1405123472213745
epoch£º1046	 i:7 	 global-step:20927	 l-p:0.1543843299150467
epoch£º1046	 i:8 	 global-step:20928	 l-p:0.2112436294555664
epoch£º1046	 i:9 	 global-step:20929	 l-p:0.1293000429868698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1047
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1649, 5.0168, 5.0736],
        [5.1649, 5.1383, 5.1608],
        [5.1649, 4.8772, 4.7179],
        [5.1649, 5.0957, 5.1430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1047, step:0 
model_pd.l_p.mean(): 0.10839705914258957 
model_pd.l_d.mean(): -19.975791931152344 
model_pd.lagr.mean(): -19.867395401000977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4650], device='cuda:0')), ('power', tensor([-20.6691], device='cuda:0'))])
epoch£º1047	 i:0 	 global-step:20940	 l-p:0.10839705914258957
epoch£º1047	 i:1 	 global-step:20941	 l-p:0.20097202062606812
epoch£º1047	 i:2 	 global-step:20942	 l-p:0.12995558977127075
epoch£º1047	 i:3 	 global-step:20943	 l-p:0.14209996163845062
epoch£º1047	 i:4 	 global-step:20944	 l-p:0.11310366541147232
epoch£º1047	 i:5 	 global-step:20945	 l-p:0.20336513221263885
epoch£º1047	 i:6 	 global-step:20946	 l-p:0.11669006198644638
epoch£º1047	 i:7 	 global-step:20947	 l-p:0.14483650028705597
epoch£º1047	 i:8 	 global-step:20948	 l-p:0.0988009050488472
epoch£º1047	 i:9 	 global-step:20949	 l-p:0.11468074470758438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1048
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1607, 5.3084, 5.0728],
        [5.1607, 4.9796, 5.0241],
        [5.1607, 5.1607, 5.1607],
        [5.1607, 4.8897, 4.6247]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1048, step:0 
model_pd.l_p.mean(): 0.08636434376239777 
model_pd.l_d.mean(): -19.884010314941406 
model_pd.lagr.mean(): -19.797645568847656 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5211], device='cuda:0')), ('power', tensor([-20.6336], device='cuda:0'))])
epoch£º1048	 i:0 	 global-step:20960	 l-p:0.08636434376239777
epoch£º1048	 i:1 	 global-step:20961	 l-p:0.08265835791826248
epoch£º1048	 i:2 	 global-step:20962	 l-p:0.14688412845134735
epoch£º1048	 i:3 	 global-step:20963	 l-p:0.13144755363464355
epoch£º1048	 i:4 	 global-step:20964	 l-p:0.12024030089378357
epoch£º1048	 i:5 	 global-step:20965	 l-p:0.23202471435070038
epoch£º1048	 i:6 	 global-step:20966	 l-p:0.11348874866962433
epoch£º1048	 i:7 	 global-step:20967	 l-p:0.19998247921466827
epoch£º1048	 i:8 	 global-step:20968	 l-p:0.12627559900283813
epoch£º1048	 i:9 	 global-step:20969	 l-p:0.15820357203483582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1049
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1530, 5.0771, 5.1272],
        [5.1530, 4.9736, 4.6402],
        [5.1530, 5.5317, 5.4344],
        [5.1530, 4.9597, 4.9969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1049, step:0 
model_pd.l_p.mean(): 0.1285492330789566 
model_pd.l_d.mean(): -19.477235794067383 
model_pd.lagr.mean(): -19.34868621826172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5208], device='cuda:0')), ('power', tensor([-20.2221], device='cuda:0'))])
epoch£º1049	 i:0 	 global-step:20980	 l-p:0.1285492330789566
epoch£º1049	 i:1 	 global-step:20981	 l-p:0.1193821132183075
epoch£º1049	 i:2 	 global-step:20982	 l-p:0.12862025201320648
epoch£º1049	 i:3 	 global-step:20983	 l-p:0.157724529504776
epoch£º1049	 i:4 	 global-step:20984	 l-p:0.1842103898525238
epoch£º1049	 i:5 	 global-step:20985	 l-p:0.10810811817646027
epoch£º1049	 i:6 	 global-step:20986	 l-p:0.1506473869085312
epoch£º1049	 i:7 	 global-step:20987	 l-p:0.15588000416755676
epoch£º1049	 i:8 	 global-step:20988	 l-p:0.13305328786373138
epoch£º1049	 i:9 	 global-step:20989	 l-p:0.14163146913051605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1050
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1547, 4.8811, 4.7994],
        [5.1547, 5.1090, 5.1443],
        [5.1547, 5.1532, 5.1546],
        [5.1547, 5.1170, 5.1473]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1050, step:0 
model_pd.l_p.mean(): 0.13257765769958496 
model_pd.l_d.mean(): -20.877450942993164 
model_pd.lagr.mean(): -20.744873046875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4146], device='cuda:0')), ('power', tensor([-21.5291], device='cuda:0'))])
epoch£º1050	 i:0 	 global-step:21000	 l-p:0.13257765769958496
epoch£º1050	 i:1 	 global-step:21001	 l-p:0.11078038811683655
epoch£º1050	 i:2 	 global-step:21002	 l-p:0.11326678842306137
epoch£º1050	 i:3 	 global-step:21003	 l-p:0.1473303735256195
epoch£º1050	 i:4 	 global-step:21004	 l-p:0.14912782609462738
epoch£º1050	 i:5 	 global-step:21005	 l-p:0.11574793606996536
epoch£º1050	 i:6 	 global-step:21006	 l-p:0.22856231033802032
epoch£º1050	 i:7 	 global-step:21007	 l-p:0.15808720886707306
epoch£º1050	 i:8 	 global-step:21008	 l-p:0.12078949064016342
epoch£º1050	 i:9 	 global-step:21009	 l-p:0.17778295278549194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1051
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1412, 4.8499, 4.6742],
        [5.1412, 5.1401, 5.1412],
        [5.1412, 4.8501, 4.6593],
        [5.1412, 4.8538, 4.7209]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1051, step:0 
model_pd.l_p.mean(): 0.1574203073978424 
model_pd.l_d.mean(): -19.061880111694336 
model_pd.lagr.mean(): -18.90445899963379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5570], device='cuda:0')), ('power', tensor([-19.8392], device='cuda:0'))])
epoch£º1051	 i:0 	 global-step:21020	 l-p:0.1574203073978424
epoch£º1051	 i:1 	 global-step:21021	 l-p:0.15802250802516937
epoch£º1051	 i:2 	 global-step:21022	 l-p:0.21102330088615417
epoch£º1051	 i:3 	 global-step:21023	 l-p:0.10748890787363052
epoch£º1051	 i:4 	 global-step:21024	 l-p:0.14835278689861298
epoch£º1051	 i:5 	 global-step:21025	 l-p:0.09143754094839096
epoch£º1051	 i:6 	 global-step:21026	 l-p:0.1266075223684311
epoch£º1051	 i:7 	 global-step:21027	 l-p:0.12565107643604279
epoch£º1051	 i:8 	 global-step:21028	 l-p:0.14542943239212036
epoch£º1051	 i:9 	 global-step:21029	 l-p:0.1812402307987213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1052
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1492, 5.1492, 5.1492],
        [5.1492, 5.1492, 5.1492],
        [5.1492, 5.0094, 5.0681],
        [5.1492, 4.9003, 4.8753]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1052, step:0 
model_pd.l_p.mean(): 0.10037382692098618 
model_pd.l_d.mean(): -20.677486419677734 
model_pd.lagr.mean(): -20.577112197875977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4309], device='cuda:0')), ('power', tensor([-21.3436], device='cuda:0'))])
epoch£º1052	 i:0 	 global-step:21040	 l-p:0.10037382692098618
epoch£º1052	 i:1 	 global-step:21041	 l-p:0.1413731724023819
epoch£º1052	 i:2 	 global-step:21042	 l-p:0.1399228572845459
epoch£º1052	 i:3 	 global-step:21043	 l-p:0.13452422618865967
epoch£º1052	 i:4 	 global-step:21044	 l-p:0.1943199336528778
epoch£º1052	 i:5 	 global-step:21045	 l-p:0.09984748810529709
epoch£º1052	 i:6 	 global-step:21046	 l-p:0.08493120223283768
epoch£º1052	 i:7 	 global-step:21047	 l-p:0.2374265193939209
epoch£º1052	 i:8 	 global-step:21048	 l-p:0.13071630895137787
epoch£º1052	 i:9 	 global-step:21049	 l-p:0.13949649035930634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1053
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228]], device='cuda:0')
 pt:tensor([[5.1593, 4.9328, 4.6171],
        [5.1593, 5.4377, 5.2766],
        [5.1593, 4.8982, 4.8482],
        [5.1593, 4.9560, 4.9855]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1053, step:0 
model_pd.l_p.mean(): 0.15978755056858063 
model_pd.l_d.mean(): -20.53955078125 
model_pd.lagr.mean(): -20.379762649536133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4613], device='cuda:0')), ('power', tensor([-21.2353], device='cuda:0'))])
epoch£º1053	 i:0 	 global-step:21060	 l-p:0.15978755056858063
epoch£º1053	 i:1 	 global-step:21061	 l-p:0.11822754889726639
epoch£º1053	 i:2 	 global-step:21062	 l-p:0.17825429141521454
epoch£º1053	 i:3 	 global-step:21063	 l-p:0.1438298523426056
epoch£º1053	 i:4 	 global-step:21064	 l-p:0.10649880766868591
epoch£º1053	 i:5 	 global-step:21065	 l-p:0.11194802075624466
epoch£º1053	 i:6 	 global-step:21066	 l-p:0.11585409194231033
epoch£º1053	 i:7 	 global-step:21067	 l-p:0.15290093421936035
epoch£º1053	 i:8 	 global-step:21068	 l-p:0.1701616495847702
epoch£º1053	 i:9 	 global-step:21069	 l-p:0.11987853795289993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1054
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1641, 5.1611, 5.1640],
        [5.1641, 5.1641, 5.1641],
        [5.1641, 5.1638, 5.1641],
        [5.1641, 5.1598, 5.1639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1054, step:0 
model_pd.l_p.mean(): 0.12020538747310638 
model_pd.l_d.mean(): -17.69849395751953 
model_pd.lagr.mean(): -17.578289031982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6186], device='cuda:0')), ('power', tensor([-18.5239], device='cuda:0'))])
epoch£º1054	 i:0 	 global-step:21080	 l-p:0.12020538747310638
epoch£º1054	 i:1 	 global-step:21081	 l-p:0.12044931948184967
epoch£º1054	 i:2 	 global-step:21082	 l-p:0.14605756103992462
epoch£º1054	 i:3 	 global-step:21083	 l-p:0.12166126817464828
epoch£º1054	 i:4 	 global-step:21084	 l-p:0.1933668851852417
epoch£º1054	 i:5 	 global-step:21085	 l-p:0.09500907361507416
epoch£º1054	 i:6 	 global-step:21086	 l-p:0.14130952954292297
epoch£º1054	 i:7 	 global-step:21087	 l-p:0.14104849100112915
epoch£º1054	 i:8 	 global-step:21088	 l-p:0.17426669597625732
epoch£º1054	 i:9 	 global-step:21089	 l-p:0.09876452386379242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1055
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1709, 5.1650, 5.1705],
        [5.1709, 5.1236, 5.1598],
        [5.1709, 4.8876, 4.6599],
        [5.1709, 5.0029, 5.0536]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1055, step:0 
model_pd.l_p.mean(): 0.17811472713947296 
model_pd.l_d.mean(): -19.483320236206055 
model_pd.lagr.mean(): -19.305206298828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5831], device='cuda:0')), ('power', tensor([-20.2920], device='cuda:0'))])
epoch£º1055	 i:0 	 global-step:21100	 l-p:0.17811472713947296
epoch£º1055	 i:1 	 global-step:21101	 l-p:0.064728282392025
epoch£º1055	 i:2 	 global-step:21102	 l-p:0.13935697078704834
epoch£º1055	 i:3 	 global-step:21103	 l-p:0.16335228085517883
epoch£º1055	 i:4 	 global-step:21104	 l-p:0.14973114430904388
epoch£º1055	 i:5 	 global-step:21105	 l-p:0.13765420019626617
epoch£º1055	 i:6 	 global-step:21106	 l-p:0.11322927474975586
epoch£º1055	 i:7 	 global-step:21107	 l-p:0.08241923898458481
epoch£º1055	 i:8 	 global-step:21108	 l-p:0.1610639989376068
epoch£º1055	 i:9 	 global-step:21109	 l-p:0.13217870891094208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1056
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1841, 5.1841, 5.1841],
        [5.1841, 5.1836, 5.1841],
        [5.1841, 5.1765, 5.1836],
        [5.1841, 5.1841, 5.1841]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1056, step:0 
model_pd.l_p.mean(): 0.1833580732345581 
model_pd.l_d.mean(): -21.001075744628906 
model_pd.lagr.mean(): -20.817718505859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3678], device='cuda:0')), ('power', tensor([-21.6063], device='cuda:0'))])
epoch£º1056	 i:0 	 global-step:21120	 l-p:0.1833580732345581
epoch£º1056	 i:1 	 global-step:21121	 l-p:0.12307727336883545
epoch£º1056	 i:2 	 global-step:21122	 l-p:0.16321368515491486
epoch£º1056	 i:3 	 global-step:21123	 l-p:0.09681975841522217
epoch£º1056	 i:4 	 global-step:21124	 l-p:0.12210347503423691
epoch£º1056	 i:5 	 global-step:21125	 l-p:0.08731956034898758
epoch£º1056	 i:6 	 global-step:21126	 l-p:0.10846013575792313
epoch£º1056	 i:7 	 global-step:21127	 l-p:0.1640693098306656
epoch£º1056	 i:8 	 global-step:21128	 l-p:0.0967775359749794
epoch£º1056	 i:9 	 global-step:21129	 l-p:0.15781663358211517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1057
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1786, 5.1163, 5.1605],
        [5.1786, 5.2499, 4.9758],
        [5.1786, 5.1786, 5.1786],
        [5.1786, 5.0878, 5.1426]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1057, step:0 
model_pd.l_p.mean(): 0.17009004950523376 
model_pd.l_d.mean(): -20.511560440063477 
model_pd.lagr.mean(): -20.34147071838379 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4459], device='cuda:0')), ('power', tensor([-21.1912], device='cuda:0'))])
epoch£º1057	 i:0 	 global-step:21140	 l-p:0.17009004950523376
epoch£º1057	 i:1 	 global-step:21141	 l-p:0.10288701951503754
epoch£º1057	 i:2 	 global-step:21142	 l-p:0.11507800966501236
epoch£º1057	 i:3 	 global-step:21143	 l-p:0.10022775083780289
epoch£º1057	 i:4 	 global-step:21144	 l-p:0.1348457932472229
epoch£º1057	 i:5 	 global-step:21145	 l-p:0.12990784645080566
epoch£º1057	 i:6 	 global-step:21146	 l-p:0.11446376889944077
epoch£º1057	 i:7 	 global-step:21147	 l-p:0.1628367155790329
epoch£º1057	 i:8 	 global-step:21148	 l-p:0.15321022272109985
epoch£º1057	 i:9 	 global-step:21149	 l-p:0.1295774281024933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1058
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1766, 5.1579, 5.1743],
        [5.1766, 4.9437, 4.9406],
        [5.1766, 4.9130, 4.8551],
        [5.1766, 5.1203, 5.1615]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1058, step:0 
model_pd.l_p.mean(): 0.1345926821231842 
model_pd.l_d.mean(): -20.770103454589844 
model_pd.lagr.mean(): -20.63551139831543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4085], device='cuda:0')), ('power', tensor([-21.4144], device='cuda:0'))])
epoch£º1058	 i:0 	 global-step:21160	 l-p:0.1345926821231842
epoch£º1058	 i:1 	 global-step:21161	 l-p:0.10899707674980164
epoch£º1058	 i:2 	 global-step:21162	 l-p:0.11331372708082199
epoch£º1058	 i:3 	 global-step:21163	 l-p:0.1277705579996109
epoch£º1058	 i:4 	 global-step:21164	 l-p:0.1675356775522232
epoch£º1058	 i:5 	 global-step:21165	 l-p:0.17925624549388885
epoch£º1058	 i:6 	 global-step:21166	 l-p:0.159811869263649
epoch£º1058	 i:7 	 global-step:21167	 l-p:0.07746023684740067
epoch£º1058	 i:8 	 global-step:21168	 l-p:0.1261071115732193
epoch£º1058	 i:9 	 global-step:21169	 l-p:0.19390419125556946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1059
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1506, 5.1469, 5.1505],
        [5.1506, 4.9582, 4.9966],
        [5.1506, 5.1318, 5.1484],
        [5.1506, 5.1284, 5.1476]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1059, step:0 
model_pd.l_p.mean(): 0.12434282153844833 
model_pd.l_d.mean(): -20.5903263092041 
model_pd.lagr.mean(): -20.465984344482422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4446], device='cuda:0')), ('power', tensor([-21.2695], device='cuda:0'))])
epoch£º1059	 i:0 	 global-step:21180	 l-p:0.12434282153844833
epoch£º1059	 i:1 	 global-step:21181	 l-p:0.11230504512786865
epoch£º1059	 i:2 	 global-step:21182	 l-p:0.09785319864749908
epoch£º1059	 i:3 	 global-step:21183	 l-p:0.18406112492084503
epoch£º1059	 i:4 	 global-step:21184	 l-p:0.28535863757133484
epoch£º1059	 i:5 	 global-step:21185	 l-p:0.14325709640979767
epoch£º1059	 i:6 	 global-step:21186	 l-p:0.14562200009822845
epoch£º1059	 i:7 	 global-step:21187	 l-p:0.11727181077003479
epoch£º1059	 i:8 	 global-step:21188	 l-p:0.13830125331878662
epoch£º1059	 i:9 	 global-step:21189	 l-p:0.13002777099609375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1060
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1330, 5.1300, 5.1329],
        [5.1330, 4.8517, 4.5977],
        [5.1330, 4.9300, 4.5992],
        [5.1330, 5.1325, 5.1330]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1060, step:0 
model_pd.l_p.mean(): 0.13391555845737457 
model_pd.l_d.mean(): -19.929746627807617 
model_pd.lagr.mean(): -19.79583168029785 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4543], device='cuda:0')), ('power', tensor([-20.6117], device='cuda:0'))])
epoch£º1060	 i:0 	 global-step:21200	 l-p:0.13391555845737457
epoch£º1060	 i:1 	 global-step:21201	 l-p:0.17986662685871124
epoch£º1060	 i:2 	 global-step:21202	 l-p:0.12901687622070312
epoch£º1060	 i:3 	 global-step:21203	 l-p:0.1668582558631897
epoch£º1060	 i:4 	 global-step:21204	 l-p:0.11494620889425278
epoch£º1060	 i:5 	 global-step:21205	 l-p:0.14037910103797913
epoch£º1060	 i:6 	 global-step:21206	 l-p:0.14567875862121582
epoch£º1060	 i:7 	 global-step:21207	 l-p:0.19624347984790802
epoch£º1060	 i:8 	 global-step:21208	 l-p:0.10890308767557144
epoch£º1060	 i:9 	 global-step:21209	 l-p:0.26198047399520874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1061
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1279, 5.1279, 5.1279],
        [5.1279, 4.9330, 4.9709],
        [5.1279, 5.4880, 5.3787],
        [5.1279, 4.8513, 4.7702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1061, step:0 
model_pd.l_p.mean(): 0.12789911031723022 
model_pd.l_d.mean(): -19.713159561157227 
model_pd.lagr.mean(): -19.58526039123535 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5292], device='cuda:0')), ('power', tensor([-20.4692], device='cuda:0'))])
epoch£º1061	 i:0 	 global-step:21220	 l-p:0.12789911031723022
epoch£º1061	 i:1 	 global-step:21221	 l-p:0.15915808081626892
epoch£º1061	 i:2 	 global-step:21222	 l-p:0.21150973439216614
epoch£º1061	 i:3 	 global-step:21223	 l-p:0.21201421320438385
epoch£º1061	 i:4 	 global-step:21224	 l-p:0.09491335600614548
epoch£º1061	 i:5 	 global-step:21225	 l-p:0.13841365277767181
epoch£º1061	 i:6 	 global-step:21226	 l-p:0.13414247334003448
epoch£º1061	 i:7 	 global-step:21227	 l-p:0.12527623772621155
epoch£º1061	 i:8 	 global-step:21228	 l-p:0.18143267929553986
epoch£º1061	 i:9 	 global-step:21229	 l-p:0.16933205723762512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1062
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1367, 5.1363, 5.1367],
        [5.1367, 5.0908, 5.1263],
        [5.1367, 4.9598, 5.0078],
        [5.1367, 4.9927, 5.0512]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1062, step:0 
model_pd.l_p.mean(): 0.09896007925271988 
model_pd.l_d.mean(): -20.227161407470703 
model_pd.lagr.mean(): -20.12820053100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4925], device='cuda:0')), ('power', tensor([-20.9513], device='cuda:0'))])
epoch£º1062	 i:0 	 global-step:21240	 l-p:0.09896007925271988
epoch£º1062	 i:1 	 global-step:21241	 l-p:0.13606856763362885
epoch£º1062	 i:2 	 global-step:21242	 l-p:0.1325904279947281
epoch£º1062	 i:3 	 global-step:21243	 l-p:0.21152523159980774
epoch£º1062	 i:4 	 global-step:21244	 l-p:0.11383620649576187
epoch£º1062	 i:5 	 global-step:21245	 l-p:0.1167788878083229
epoch£º1062	 i:6 	 global-step:21246	 l-p:0.13955387473106384
epoch£º1062	 i:7 	 global-step:21247	 l-p:0.12317675352096558
epoch£º1062	 i:8 	 global-step:21248	 l-p:0.12171489745378494
epoch£º1062	 i:9 	 global-step:21249	 l-p:0.3108433783054352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1063
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1360, 5.1357, 5.1360],
        [5.1360, 4.8781, 4.8397],
        [5.1360, 5.1360, 5.1360],
        [5.1360, 4.8558, 4.7612]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1063, step:0 
model_pd.l_p.mean(): 0.14088593423366547 
model_pd.l_d.mean(): -20.652437210083008 
model_pd.lagr.mean(): -20.511550903320312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4301], device='cuda:0')), ('power', tensor([-21.3176], device='cuda:0'))])
epoch£º1063	 i:0 	 global-step:21260	 l-p:0.14088593423366547
epoch£º1063	 i:1 	 global-step:21261	 l-p:0.06189778074622154
epoch£º1063	 i:2 	 global-step:21262	 l-p:0.16754718124866486
epoch£º1063	 i:3 	 global-step:21263	 l-p:0.14585207402706146
epoch£º1063	 i:4 	 global-step:21264	 l-p:0.130377858877182
epoch£º1063	 i:5 	 global-step:21265	 l-p:0.20675981044769287
epoch£º1063	 i:6 	 global-step:21266	 l-p:0.16789603233337402
epoch£º1063	 i:7 	 global-step:21267	 l-p:0.24560175836086273
epoch£º1063	 i:8 	 global-step:21268	 l-p:0.11819619685411453
epoch£º1063	 i:9 	 global-step:21269	 l-p:0.1662713587284088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1064
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1311, 5.1308, 5.1311],
        [5.1311, 4.8380, 4.6735],
        [5.1311, 4.9091, 4.9233],
        [5.1311, 5.0537, 5.1045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1064, step:0 
model_pd.l_p.mean(): 0.3108038902282715 
model_pd.l_d.mean(): -20.609100341796875 
model_pd.lagr.mean(): -20.298295974731445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4332], device='cuda:0')), ('power', tensor([-21.2768], device='cuda:0'))])
epoch£º1064	 i:0 	 global-step:21280	 l-p:0.3108038902282715
epoch£º1064	 i:1 	 global-step:21281	 l-p:0.16155759990215302
epoch£º1064	 i:2 	 global-step:21282	 l-p:0.16747303307056427
epoch£º1064	 i:3 	 global-step:21283	 l-p:0.13037046790122986
epoch£º1064	 i:4 	 global-step:21284	 l-p:0.13418839871883392
epoch£º1064	 i:5 	 global-step:21285	 l-p:0.12316105514764786
epoch£º1064	 i:6 	 global-step:21286	 l-p:0.0943339616060257
epoch£º1064	 i:7 	 global-step:21287	 l-p:0.11921815574169159
epoch£º1064	 i:8 	 global-step:21288	 l-p:0.1636817306280136
epoch£º1064	 i:9 	 global-step:21289	 l-p:0.1307363659143448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1065
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1373, 5.1373, 5.1373],
        [5.1373, 5.1373, 5.1373],
        [5.1373, 5.0129, 4.6754],
        [5.1373, 5.1366, 5.1373]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1065, step:0 
model_pd.l_p.mean(): 0.13577423989772797 
model_pd.l_d.mean(): -19.869964599609375 
model_pd.lagr.mean(): -19.734189987182617 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4982], device='cuda:0')), ('power', tensor([-20.5960], device='cuda:0'))])
epoch£º1065	 i:0 	 global-step:21300	 l-p:0.13577423989772797
epoch£º1065	 i:1 	 global-step:21301	 l-p:0.25604158639907837
epoch£º1065	 i:2 	 global-step:21302	 l-p:0.14871564507484436
epoch£º1065	 i:3 	 global-step:21303	 l-p:0.1419404298067093
epoch£º1065	 i:4 	 global-step:21304	 l-p:0.23492492735385895
epoch£º1065	 i:5 	 global-step:21305	 l-p:0.11677226424217224
epoch£º1065	 i:6 	 global-step:21306	 l-p:0.1258859932422638
epoch£º1065	 i:7 	 global-step:21307	 l-p:0.06289278715848923
epoch£º1065	 i:8 	 global-step:21308	 l-p:0.12297308444976807
epoch£º1065	 i:9 	 global-step:21309	 l-p:0.1597091257572174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1066
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1361, 5.4988, 5.3907],
        [5.1361, 4.9017, 4.5858],
        [5.1361, 5.0219, 5.0814],
        [5.1361, 4.8884, 4.5829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1066, step:0 
model_pd.l_p.mean(): 0.22248686850070953 
model_pd.l_d.mean(): -20.528297424316406 
model_pd.lagr.mean(): -20.305810928344727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4477], device='cuda:0')), ('power', tensor([-21.2100], device='cuda:0'))])
epoch£º1066	 i:0 	 global-step:21320	 l-p:0.22248686850070953
epoch£º1066	 i:1 	 global-step:21321	 l-p:0.11600524932146072
epoch£º1066	 i:2 	 global-step:21322	 l-p:0.12708967924118042
epoch£º1066	 i:3 	 global-step:21323	 l-p:0.20583148300647736
epoch£º1066	 i:4 	 global-step:21324	 l-p:0.1725478172302246
epoch£º1066	 i:5 	 global-step:21325	 l-p:0.1348327398300171
epoch£º1066	 i:6 	 global-step:21326	 l-p:0.09691143035888672
epoch£º1066	 i:7 	 global-step:21327	 l-p:0.12857882678508759
epoch£º1066	 i:8 	 global-step:21328	 l-p:0.16171275079250336
epoch£º1066	 i:9 	 global-step:21329	 l-p:0.13787956535816193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1067
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1345, 5.1345, 5.1345],
        [5.1345, 5.1333, 5.1345],
        [5.1345, 5.1293, 5.1342],
        [5.1345, 5.4560, 5.3216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1067, step:0 
model_pd.l_p.mean(): 0.1819428950548172 
model_pd.l_d.mean(): -20.48694610595703 
model_pd.lagr.mean(): -20.305004119873047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4688], device='cuda:0')), ('power', tensor([-21.1897], device='cuda:0'))])
epoch£º1067	 i:0 	 global-step:21340	 l-p:0.1819428950548172
epoch£º1067	 i:1 	 global-step:21341	 l-p:0.13519760966300964
epoch£º1067	 i:2 	 global-step:21342	 l-p:0.1727963536977768
epoch£º1067	 i:3 	 global-step:21343	 l-p:0.08586856722831726
epoch£º1067	 i:4 	 global-step:21344	 l-p:0.15023741126060486
epoch£º1067	 i:5 	 global-step:21345	 l-p:0.16913720965385437
epoch£º1067	 i:6 	 global-step:21346	 l-p:0.16236135363578796
epoch£º1067	 i:7 	 global-step:21347	 l-p:0.1715150773525238
epoch£º1067	 i:8 	 global-step:21348	 l-p:0.11179126799106598
epoch£º1067	 i:9 	 global-step:21349	 l-p:0.1681908518075943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1068
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1411, 5.1411, 5.1411],
        [5.1411, 5.1406, 5.1411],
        [5.1411, 5.1410, 5.1411],
        [5.1411, 5.1411, 5.1411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1068, step:0 
model_pd.l_p.mean(): 0.21485859155654907 
model_pd.l_d.mean(): -19.528799057006836 
model_pd.lagr.mean(): -19.313940048217773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4942], device='cuda:0')), ('power', tensor([-20.2470], device='cuda:0'))])
epoch£º1068	 i:0 	 global-step:21360	 l-p:0.21485859155654907
epoch£º1068	 i:1 	 global-step:21361	 l-p:0.17373012006282806
epoch£º1068	 i:2 	 global-step:21362	 l-p:0.12718309462070465
epoch£º1068	 i:3 	 global-step:21363	 l-p:0.13975758850574493
epoch£º1068	 i:4 	 global-step:21364	 l-p:0.07306238263845444
epoch£º1068	 i:5 	 global-step:21365	 l-p:0.1733797937631607
epoch£º1068	 i:6 	 global-step:21366	 l-p:0.11656276136636734
epoch£º1068	 i:7 	 global-step:21367	 l-p:0.14477525651454926
epoch£º1068	 i:8 	 global-step:21368	 l-p:0.17742957174777985
epoch£º1068	 i:9 	 global-step:21369	 l-p:0.1055966168642044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1069
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1574, 4.9086, 4.8842],
        [5.1574, 5.1438, 5.1561],
        [5.1574, 5.1365, 5.1547],
        [5.1574, 5.0191, 5.0782]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1069, step:0 
model_pd.l_p.mean(): 0.11204958707094193 
model_pd.l_d.mean(): -20.739179611206055 
model_pd.lagr.mean(): -20.62713050842285 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4087], device='cuda:0')), ('power', tensor([-21.3834], device='cuda:0'))])
epoch£º1069	 i:0 	 global-step:21380	 l-p:0.11204958707094193
epoch£º1069	 i:1 	 global-step:21381	 l-p:0.09443814307451248
epoch£º1069	 i:2 	 global-step:21382	 l-p:0.1651172786951065
epoch£º1069	 i:3 	 global-step:21383	 l-p:0.1490844041109085
epoch£º1069	 i:4 	 global-step:21384	 l-p:0.1294781118631363
epoch£º1069	 i:5 	 global-step:21385	 l-p:0.15146762132644653
epoch£º1069	 i:6 	 global-step:21386	 l-p:0.1190471425652504
epoch£º1069	 i:7 	 global-step:21387	 l-p:0.13101805746555328
epoch£º1069	 i:8 	 global-step:21388	 l-p:0.17298579216003418
epoch£º1069	 i:9 	 global-step:21389	 l-p:0.14400450885295868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1070
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1778, 5.3154, 5.0735],
        [5.1778, 5.1763, 5.1777],
        [5.1778, 5.1401, 5.1703],
        [5.1778, 5.2467, 4.9711]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1070, step:0 
model_pd.l_p.mean(): 0.09998106956481934 
model_pd.l_d.mean(): -19.49178123474121 
model_pd.lagr.mean(): -19.391799926757812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5520], device='cuda:0')), ('power', tensor([-20.2687], device='cuda:0'))])
epoch£º1070	 i:0 	 global-step:21400	 l-p:0.09998106956481934
epoch£º1070	 i:1 	 global-step:21401	 l-p:0.14349350333213806
epoch£º1070	 i:2 	 global-step:21402	 l-p:0.14133082330226898
epoch£º1070	 i:3 	 global-step:21403	 l-p:0.13652566075325012
epoch£º1070	 i:4 	 global-step:21404	 l-p:0.13763031363487244
epoch£º1070	 i:5 	 global-step:21405	 l-p:0.1292906552553177
epoch£º1070	 i:6 	 global-step:21406	 l-p:0.0794200673699379
epoch£º1070	 i:7 	 global-step:21407	 l-p:0.16039405763149261
epoch£º1070	 i:8 	 global-step:21408	 l-p:0.1572536677122116
epoch£º1070	 i:9 	 global-step:21409	 l-p:0.11188845336437225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1071
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1923, 5.1923, 5.1923],
        [5.1923, 5.1742, 5.1902],
        [5.1923, 5.0381, 5.0933],
        [5.1923, 4.9471, 4.9246]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1071, step:0 
model_pd.l_p.mean(): 0.1087353378534317 
model_pd.l_d.mean(): -20.029460906982422 
model_pd.lagr.mean(): -19.920724868774414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4774], device='cuda:0')), ('power', tensor([-20.7360], device='cuda:0'))])
epoch£º1071	 i:0 	 global-step:21420	 l-p:0.1087353378534317
epoch£º1071	 i:1 	 global-step:21421	 l-p:0.11209668964147568
epoch£º1071	 i:2 	 global-step:21422	 l-p:0.14022281765937805
epoch£º1071	 i:3 	 global-step:21423	 l-p:0.1468425691127777
epoch£º1071	 i:4 	 global-step:21424	 l-p:0.15084055066108704
epoch£º1071	 i:5 	 global-step:21425	 l-p:0.0877705067396164
epoch£º1071	 i:6 	 global-step:21426	 l-p:0.09587636590003967
epoch£º1071	 i:7 	 global-step:21427	 l-p:0.1237676814198494
epoch£º1071	 i:8 	 global-step:21428	 l-p:0.17046009004116058
epoch£º1071	 i:9 	 global-step:21429	 l-p:0.13922151923179626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1072
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1913, 5.1913, 5.1913],
        [5.1913, 5.5813, 5.4897],
        [5.1913, 5.1904, 5.1912],
        [5.1913, 4.9053, 4.7015]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1072, step:0 
model_pd.l_p.mean(): 0.14999981224536896 
model_pd.l_d.mean(): -19.53510093688965 
model_pd.lagr.mean(): -19.385101318359375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5471], device='cuda:0')), ('power', tensor([-20.3075], device='cuda:0'))])
epoch£º1072	 i:0 	 global-step:21440	 l-p:0.14999981224536896
epoch£º1072	 i:1 	 global-step:21441	 l-p:0.10012943297624588
epoch£º1072	 i:2 	 global-step:21442	 l-p:0.10085668414831161
epoch£º1072	 i:3 	 global-step:21443	 l-p:0.10463017225265503
epoch£º1072	 i:4 	 global-step:21444	 l-p:0.11956017464399338
epoch£º1072	 i:5 	 global-step:21445	 l-p:0.15888036787509918
epoch£º1072	 i:6 	 global-step:21446	 l-p:0.1499299556016922
epoch£º1072	 i:7 	 global-step:21447	 l-p:0.12169592827558517
epoch£º1072	 i:8 	 global-step:21448	 l-p:0.12105504423379898
epoch£º1072	 i:9 	 global-step:21449	 l-p:0.1446818858385086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1073
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1901, 5.1682, 5.1872],
        [5.1901, 5.1833, 5.1897],
        [5.1901, 5.1179, 4.7912],
        [5.1901, 5.1742, 5.1884]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1073, step:0 
model_pd.l_p.mean(): 0.09862475842237473 
model_pd.l_d.mean(): -19.679227828979492 
model_pd.lagr.mean(): -19.580602645874023 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5335], device='cuda:0')), ('power', tensor([-20.4393], device='cuda:0'))])
epoch£º1073	 i:0 	 global-step:21460	 l-p:0.09862475842237473
epoch£º1073	 i:1 	 global-step:21461	 l-p:0.10655049979686737
epoch£º1073	 i:2 	 global-step:21462	 l-p:0.12316402047872543
epoch£º1073	 i:3 	 global-step:21463	 l-p:0.13163447380065918
epoch£º1073	 i:4 	 global-step:21464	 l-p:0.11102671176195145
epoch£º1073	 i:5 	 global-step:21465	 l-p:0.12261029332876205
epoch£º1073	 i:6 	 global-step:21466	 l-p:0.17548656463623047
epoch£º1073	 i:7 	 global-step:21467	 l-p:0.14601662755012512
epoch£º1073	 i:8 	 global-step:21468	 l-p:0.1377929300069809
epoch£º1073	 i:9 	 global-step:21469	 l-p:0.15247611701488495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1074
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1735, 5.1735, 5.1735],
        [5.1735, 4.8877, 4.7556],
        [5.1735, 5.0810, 5.1365],
        [5.1735, 4.8872, 4.7517]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1074, step:0 
model_pd.l_p.mean(): 0.097490593791008 
model_pd.l_d.mean(): -19.83233070373535 
model_pd.lagr.mean(): -19.734840393066406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4957], device='cuda:0')), ('power', tensor([-20.5555], device='cuda:0'))])
epoch£º1074	 i:0 	 global-step:21480	 l-p:0.097490593791008
epoch£º1074	 i:1 	 global-step:21481	 l-p:0.12350549548864365
epoch£º1074	 i:2 	 global-step:21482	 l-p:0.13428223133087158
epoch£º1074	 i:3 	 global-step:21483	 l-p:0.15282361209392548
epoch£º1074	 i:4 	 global-step:21484	 l-p:0.16660140454769135
epoch£º1074	 i:5 	 global-step:21485	 l-p:0.16870971024036407
epoch£º1074	 i:6 	 global-step:21486	 l-p:0.12217427790164948
epoch£º1074	 i:7 	 global-step:21487	 l-p:0.1508241444826126
epoch£º1074	 i:8 	 global-step:21488	 l-p:0.1687096357345581
epoch£º1074	 i:9 	 global-step:21489	 l-p:0.09568973630666733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1075
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1566, 5.0582, 5.1152],
        [5.1566, 5.1527, 5.1564],
        [5.1566, 5.1562, 5.1566],
        [5.1566, 4.9218, 4.9191]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1075, step:0 
model_pd.l_p.mean(): 0.09401877969503403 
model_pd.l_d.mean(): -20.808658599853516 
model_pd.lagr.mean(): -20.71463966369629 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4045], device='cuda:0')), ('power', tensor([-21.4493], device='cuda:0'))])
epoch£º1075	 i:0 	 global-step:21500	 l-p:0.09401877969503403
epoch£º1075	 i:1 	 global-step:21501	 l-p:0.162611722946167
epoch£º1075	 i:2 	 global-step:21502	 l-p:0.1341610699892044
epoch£º1075	 i:3 	 global-step:21503	 l-p:0.1341261863708496
epoch£º1075	 i:4 	 global-step:21504	 l-p:0.16966615617275238
epoch£º1075	 i:5 	 global-step:21505	 l-p:0.12878620624542236
epoch£º1075	 i:6 	 global-step:21506	 l-p:0.13245567679405212
epoch£º1075	 i:7 	 global-step:21507	 l-p:0.1392144113779068
epoch£º1075	 i:8 	 global-step:21508	 l-p:0.19316810369491577
epoch£º1075	 i:9 	 global-step:21509	 l-p:0.12652000784873962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1076
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1492, 5.1300, 5.1468],
        [5.1492, 5.3917, 5.2084],
        [5.1492, 5.4217, 5.2564],
        [5.1492, 5.1141, 5.1426]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1076, step:0 
model_pd.l_p.mean(): 0.11392395943403244 
model_pd.l_d.mean(): -20.371856689453125 
model_pd.lagr.mean(): -20.257932662963867 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4904], device='cuda:0')), ('power', tensor([-21.0955], device='cuda:0'))])
epoch£º1076	 i:0 	 global-step:21520	 l-p:0.11392395943403244
epoch£º1076	 i:1 	 global-step:21521	 l-p:0.15892893075942993
epoch£º1076	 i:2 	 global-step:21522	 l-p:0.17833758890628815
epoch£º1076	 i:3 	 global-step:21523	 l-p:0.13382922112941742
epoch£º1076	 i:4 	 global-step:21524	 l-p:0.13206297159194946
epoch£º1076	 i:5 	 global-step:21525	 l-p:0.10654648393392563
epoch£º1076	 i:6 	 global-step:21526	 l-p:0.19639450311660767
epoch£º1076	 i:7 	 global-step:21527	 l-p:0.08309869468212128
epoch£º1076	 i:8 	 global-step:21528	 l-p:0.20636272430419922
epoch£º1076	 i:9 	 global-step:21529	 l-p:0.13172683119773865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1077
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1476, 4.8660, 4.7662],
        [5.1476, 4.8821, 4.8279],
        [5.1476, 5.1425, 5.1473],
        [5.1476, 5.1469, 5.1476]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1077, step:0 
model_pd.l_p.mean(): 0.16894057393074036 
model_pd.l_d.mean(): -20.060306549072266 
model_pd.lagr.mean(): -19.89136505126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4315], device='cuda:0')), ('power', tensor([-20.7203], device='cuda:0'))])
epoch£º1077	 i:0 	 global-step:21540	 l-p:0.16894057393074036
epoch£º1077	 i:1 	 global-step:21541	 l-p:0.10825201123952866
epoch£º1077	 i:2 	 global-step:21542	 l-p:0.12676337361335754
epoch£º1077	 i:3 	 global-step:21543	 l-p:0.1994819939136505
epoch£º1077	 i:4 	 global-step:21544	 l-p:0.20417369902133942
epoch£º1077	 i:5 	 global-step:21545	 l-p:0.1193203553557396
epoch£º1077	 i:6 	 global-step:21546	 l-p:0.12699033319950104
epoch£º1077	 i:7 	 global-step:21547	 l-p:0.1586119383573532
epoch£º1077	 i:8 	 global-step:21548	 l-p:0.1029539629817009
epoch£º1077	 i:9 	 global-step:21549	 l-p:0.12248410284519196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1078
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1507, 4.8621, 4.7289],
        [5.1507, 4.9607, 5.0014],
        [5.1507, 4.8984, 4.8700],
        [5.1507, 5.1507, 5.1507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1078, step:0 
model_pd.l_p.mean(): 0.16131560504436493 
model_pd.l_d.mean(): -19.363510131835938 
model_pd.lagr.mean(): -19.202194213867188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5407], device='cuda:0')), ('power', tensor([-20.1275], device='cuda:0'))])
epoch£º1078	 i:0 	 global-step:21560	 l-p:0.16131560504436493
epoch£º1078	 i:1 	 global-step:21561	 l-p:0.13724927604198456
epoch£º1078	 i:2 	 global-step:21562	 l-p:0.12632256746292114
epoch£º1078	 i:3 	 global-step:21563	 l-p:0.1561117321252823
epoch£º1078	 i:4 	 global-step:21564	 l-p:0.12506017088890076
epoch£º1078	 i:5 	 global-step:21565	 l-p:0.10138574987649918
epoch£º1078	 i:6 	 global-step:21566	 l-p:0.12096654623746872
epoch£º1078	 i:7 	 global-step:21567	 l-p:0.15218032896518707
epoch£º1078	 i:8 	 global-step:21568	 l-p:0.22959788143634796
epoch£º1078	 i:9 	 global-step:21569	 l-p:0.1447000801563263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1079
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1448, 5.1054, 5.1369],
        [5.1448, 5.0187, 4.6805],
        [5.1448, 5.1401, 5.1446],
        [5.1448, 4.8520, 4.6872]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1079, step:0 
model_pd.l_p.mean(): 0.28691527247428894 
model_pd.l_d.mean(): -20.174055099487305 
model_pd.lagr.mean(): -19.88714027404785 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5068], device='cuda:0')), ('power', tensor([-20.9123], device='cuda:0'))])
epoch£º1079	 i:0 	 global-step:21580	 l-p:0.28691527247428894
epoch£º1079	 i:1 	 global-step:21581	 l-p:0.11991769820451736
epoch£º1079	 i:2 	 global-step:21582	 l-p:0.06565862894058228
epoch£º1079	 i:3 	 global-step:21583	 l-p:0.1352970153093338
epoch£º1079	 i:4 	 global-step:21584	 l-p:0.12319112569093704
epoch£º1079	 i:5 	 global-step:21585	 l-p:0.14093095064163208
epoch£º1079	 i:6 	 global-step:21586	 l-p:0.13302063941955566
epoch£º1079	 i:7 	 global-step:21587	 l-p:0.14247773587703705
epoch£º1079	 i:8 	 global-step:21588	 l-p:0.17159408330917358
epoch£º1079	 i:9 	 global-step:21589	 l-p:0.13765791058540344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1080
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1471, 5.1188, 5.1426],
        [5.1471, 5.0467, 5.1042],
        [5.1471, 5.4026, 5.2270],
        [5.1471, 4.9391, 4.6098]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1080, step:0 
model_pd.l_p.mean(): 0.1143517717719078 
model_pd.l_d.mean(): -20.253419876098633 
model_pd.lagr.mean(): -20.139068603515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4916], device='cuda:0')), ('power', tensor([-20.9770], device='cuda:0'))])
epoch£º1080	 i:0 	 global-step:21600	 l-p:0.1143517717719078
epoch£º1080	 i:1 	 global-step:21601	 l-p:0.11503272503614426
epoch£º1080	 i:2 	 global-step:21602	 l-p:0.1183624118566513
epoch£º1080	 i:3 	 global-step:21603	 l-p:0.13852764666080475
epoch£º1080	 i:4 	 global-step:21604	 l-p:0.23177771270275116
epoch£º1080	 i:5 	 global-step:21605	 l-p:0.11048924922943115
epoch£º1080	 i:6 	 global-step:21606	 l-p:0.13476698100566864
epoch£º1080	 i:7 	 global-step:21607	 l-p:0.1984163373708725
epoch£º1080	 i:8 	 global-step:21608	 l-p:0.16102637350559235
epoch£º1080	 i:9 	 global-step:21609	 l-p:0.15963220596313477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1081
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1413, 5.1411, 5.1413],
        [5.1413, 5.1413, 5.1413],
        [5.1413, 5.1398, 5.1412],
        [5.1413, 5.1327, 5.1407]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1081, step:0 
model_pd.l_p.mean(): 0.11353104561567307 
model_pd.l_d.mean(): -19.3463191986084 
model_pd.lagr.mean(): -19.2327880859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5317], device='cuda:0')), ('power', tensor([-20.1009], device='cuda:0'))])
epoch£º1081	 i:0 	 global-step:21620	 l-p:0.11353104561567307
epoch£º1081	 i:1 	 global-step:21621	 l-p:0.22263489663600922
epoch£º1081	 i:2 	 global-step:21622	 l-p:0.19329474866390228
epoch£º1081	 i:3 	 global-step:21623	 l-p:0.13071556389331818
epoch£º1081	 i:4 	 global-step:21624	 l-p:0.23602169752120972
epoch£º1081	 i:5 	 global-step:21625	 l-p:0.12686482071876526
epoch£º1081	 i:6 	 global-step:21626	 l-p:0.10393568128347397
epoch£º1081	 i:7 	 global-step:21627	 l-p:0.08998830616474152
epoch£º1081	 i:8 	 global-step:21628	 l-p:0.10968545824289322
epoch£º1081	 i:9 	 global-step:21629	 l-p:0.14781174063682556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1082
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1460, 5.1460, 5.1460],
        [5.1460, 5.0227, 5.0828],
        [5.1460, 4.8873, 4.5928],
        [5.1460, 4.9778, 4.6394]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1082, step:0 
model_pd.l_p.mean(): 0.11238345503807068 
model_pd.l_d.mean(): -19.372844696044922 
model_pd.lagr.mean(): -19.260461807250977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5472], device='cuda:0')), ('power', tensor([-20.1436], device='cuda:0'))])
epoch£º1082	 i:0 	 global-step:21640	 l-p:0.11238345503807068
epoch£º1082	 i:1 	 global-step:21641	 l-p:0.11212020367383957
epoch£º1082	 i:2 	 global-step:21642	 l-p:0.13486029207706451
epoch£º1082	 i:3 	 global-step:21643	 l-p:0.13864776492118835
epoch£º1082	 i:4 	 global-step:21644	 l-p:0.1189330443739891
epoch£º1082	 i:5 	 global-step:21645	 l-p:0.1915232092142105
epoch£º1082	 i:6 	 global-step:21646	 l-p:0.28927889466285706
epoch£º1082	 i:7 	 global-step:21647	 l-p:0.14124760031700134
epoch£º1082	 i:8 	 global-step:21648	 l-p:0.147025927901268
epoch£º1082	 i:9 	 global-step:21649	 l-p:0.10903986543416977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1083
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1337, 5.1307, 5.1336],
        [5.1337, 4.8799, 4.8518],
        [5.1337, 5.4146, 5.2545],
        [5.1337, 4.9866, 4.6463]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1083, step:0 
model_pd.l_p.mean(): 0.1205219179391861 
model_pd.l_d.mean(): -20.183761596679688 
model_pd.lagr.mean(): -20.06324005126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5202], device='cuda:0')), ('power', tensor([-20.9357], device='cuda:0'))])
epoch£º1083	 i:0 	 global-step:21660	 l-p:0.1205219179391861
epoch£º1083	 i:1 	 global-step:21661	 l-p:0.24678850173950195
epoch£º1083	 i:2 	 global-step:21662	 l-p:0.15411880612373352
epoch£º1083	 i:3 	 global-step:21663	 l-p:0.12856881320476532
epoch£º1083	 i:4 	 global-step:21664	 l-p:0.10027924925088882
epoch£º1083	 i:5 	 global-step:21665	 l-p:0.11509636789560318
epoch£º1083	 i:6 	 global-step:21666	 l-p:0.2445409744977951
epoch£º1083	 i:7 	 global-step:21667	 l-p:0.16178661584854126
epoch£º1083	 i:8 	 global-step:21668	 l-p:0.10218431055545807
epoch£º1083	 i:9 	 global-step:21669	 l-p:0.21043263375759125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1084
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1245, 5.4210, 5.2705],
        [5.1245, 5.1102, 5.1231],
        [5.1245, 4.8303, 4.6158],
        [5.1245, 4.9753, 4.6342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1084, step:0 
model_pd.l_p.mean(): 0.12887698411941528 
model_pd.l_d.mean(): -20.2289981842041 
model_pd.lagr.mean(): -20.100120544433594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5196], device='cuda:0')), ('power', tensor([-20.9809], device='cuda:0'))])
epoch£º1084	 i:0 	 global-step:21680	 l-p:0.12887698411941528
epoch£º1084	 i:1 	 global-step:21681	 l-p:0.17800292372703552
epoch£º1084	 i:2 	 global-step:21682	 l-p:0.12619911134243011
epoch£º1084	 i:3 	 global-step:21683	 l-p:0.15557928383350372
epoch£º1084	 i:4 	 global-step:21684	 l-p:0.10324502736330032
epoch£º1084	 i:5 	 global-step:21685	 l-p:0.12196771055459976
epoch£º1084	 i:6 	 global-step:21686	 l-p:0.1401427537202835
epoch£º1084	 i:7 	 global-step:21687	 l-p:0.11159862577915192
epoch£º1084	 i:8 	 global-step:21688	 l-p:0.24260284006595612
epoch£º1084	 i:9 	 global-step:21689	 l-p:0.4353863298892975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1085
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1184, 4.9991, 5.0594],
        [5.1184, 4.8402, 4.5673],
        [5.1184, 4.8789, 4.8747],
        [5.1184, 5.1156, 5.1183]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1085, step:0 
model_pd.l_p.mean(): 0.38414084911346436 
model_pd.l_d.mean(): -19.67106819152832 
model_pd.lagr.mean(): -19.286928176879883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5027], device='cuda:0')), ('power', tensor([-20.3996], device='cuda:0'))])
epoch£º1085	 i:0 	 global-step:21700	 l-p:0.38414084911346436
epoch£º1085	 i:1 	 global-step:21701	 l-p:0.13779079914093018
epoch£º1085	 i:2 	 global-step:21702	 l-p:0.14799369871616364
epoch£º1085	 i:3 	 global-step:21703	 l-p:0.13239352405071259
epoch£º1085	 i:4 	 global-step:21704	 l-p:0.08681419491767883
epoch£º1085	 i:5 	 global-step:21705	 l-p:0.19761531054973602
epoch£º1085	 i:6 	 global-step:21706	 l-p:0.13585786521434784
epoch£º1085	 i:7 	 global-step:21707	 l-p:0.11183110624551773
epoch£º1085	 i:8 	 global-step:21708	 l-p:0.10880246013402939
epoch£º1085	 i:9 	 global-step:21709	 l-p:0.33010050654411316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1086
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1150, 5.0236, 5.0792],
        [5.1150, 5.1898, 4.9165],
        [5.1150, 5.1150, 5.1150],
        [5.1150, 5.1149, 5.1150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1086, step:0 
model_pd.l_p.mean(): 0.2200605571269989 
model_pd.l_d.mean(): -19.875463485717773 
model_pd.lagr.mean(): -19.65540313720703 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4335], device='cuda:0')), ('power', tensor([-20.5356], device='cuda:0'))])
epoch£º1086	 i:0 	 global-step:21720	 l-p:0.2200605571269989
epoch£º1086	 i:1 	 global-step:21721	 l-p:0.13421566784381866
epoch£º1086	 i:2 	 global-step:21722	 l-p:0.07159879803657532
epoch£º1086	 i:3 	 global-step:21723	 l-p:0.13625942170619965
epoch£º1086	 i:4 	 global-step:21724	 l-p:0.11302922666072845
epoch£º1086	 i:5 	 global-step:21725	 l-p:0.17578302323818207
epoch£º1086	 i:6 	 global-step:21726	 l-p:0.2623501420021057
epoch£º1086	 i:7 	 global-step:21727	 l-p:0.11270663887262344
epoch£º1086	 i:8 	 global-step:21728	 l-p:0.14525745809078217
epoch£º1086	 i:9 	 global-step:21729	 l-p:0.3752669394016266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1087
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1211, 5.1210, 5.1211],
        [5.1211, 5.0875, 5.1151],
        [5.1211, 5.1018, 5.1188],
        [5.1211, 4.9430, 4.6030]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1087, step:0 
model_pd.l_p.mean(): 0.1855034977197647 
model_pd.l_d.mean(): -19.564624786376953 
model_pd.lagr.mean(): -19.379121780395508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5351], device='cuda:0')), ('power', tensor([-20.3251], device='cuda:0'))])
epoch£º1087	 i:0 	 global-step:21740	 l-p:0.1855034977197647
epoch£º1087	 i:1 	 global-step:21741	 l-p:0.13259418308734894
epoch£º1087	 i:2 	 global-step:21742	 l-p:0.1767200380563736
epoch£º1087	 i:3 	 global-step:21743	 l-p:0.14926953613758087
epoch£º1087	 i:4 	 global-step:21744	 l-p:0.12049523741006851
epoch£º1087	 i:5 	 global-step:21745	 l-p:0.3156796097755432
epoch£º1087	 i:6 	 global-step:21746	 l-p:0.12167567759752274
epoch£º1087	 i:7 	 global-step:21747	 l-p:0.13404250144958496
epoch£º1087	 i:8 	 global-step:21748	 l-p:0.10417415201663971
epoch£º1087	 i:9 	 global-step:21749	 l-p:0.17818698287010193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1088
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1301, 5.1192, 5.1292],
        [5.1301, 5.1072, 4.7928],
        [5.1301, 5.3903, 5.2173],
        [5.1301, 5.0631, 4.7348]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1088, step:0 
model_pd.l_p.mean(): 0.1428680568933487 
model_pd.l_d.mean(): -20.137962341308594 
model_pd.lagr.mean(): -19.995094299316406 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5095], device='cuda:0')), ('power', tensor([-20.8785], device='cuda:0'))])
epoch£º1088	 i:0 	 global-step:21760	 l-p:0.1428680568933487
epoch£º1088	 i:1 	 global-step:21761	 l-p:0.13345366716384888
epoch£º1088	 i:2 	 global-step:21762	 l-p:0.1814001053571701
epoch£º1088	 i:3 	 global-step:21763	 l-p:0.1465264856815338
epoch£º1088	 i:4 	 global-step:21764	 l-p:0.07391320168972015
epoch£º1088	 i:5 	 global-step:21765	 l-p:0.19860370457172394
epoch£º1088	 i:6 	 global-step:21766	 l-p:0.10865890234708786
epoch£º1088	 i:7 	 global-step:21767	 l-p:0.2881767153739929
epoch£º1088	 i:8 	 global-step:21768	 l-p:0.13310599327087402
epoch£º1088	 i:9 	 global-step:21769	 l-p:0.14885495603084564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1089
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1393, 5.0473, 5.1030],
        [5.1393, 5.0426, 5.0995],
        [5.1393, 4.8639, 4.5913],
        [5.1393, 5.1393, 5.1393]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1089, step:0 
model_pd.l_p.mean(): 0.1523190140724182 
model_pd.l_d.mean(): -20.70821762084961 
model_pd.lagr.mean(): -20.555898666381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4428], device='cuda:0')), ('power', tensor([-21.3869], device='cuda:0'))])
epoch£º1089	 i:0 	 global-step:21780	 l-p:0.1523190140724182
epoch£º1089	 i:1 	 global-step:21781	 l-p:0.20361602306365967
epoch£º1089	 i:2 	 global-step:21782	 l-p:0.16253021359443665
epoch£º1089	 i:3 	 global-step:21783	 l-p:0.14279554784297943
epoch£º1089	 i:4 	 global-step:21784	 l-p:0.11808326095342636
epoch£º1089	 i:5 	 global-step:21785	 l-p:0.11837340891361237
epoch£º1089	 i:6 	 global-step:21786	 l-p:0.12488171458244324
epoch£º1089	 i:7 	 global-step:21787	 l-p:0.10273958742618561
epoch£º1089	 i:8 	 global-step:21788	 l-p:0.15917500853538513
epoch£º1089	 i:9 	 global-step:21789	 l-p:0.18023166060447693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1090
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1537, 4.8629, 4.7188],
        [5.1537, 4.8720, 4.6186],
        [5.1537, 5.2136, 4.9331],
        [5.1537, 5.1523, 5.1537]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1090, step:0 
model_pd.l_p.mean(): 0.20137129724025726 
model_pd.l_d.mean(): -20.682039260864258 
model_pd.lagr.mean(): -20.480667114257812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4310], device='cuda:0')), ('power', tensor([-21.3484], device='cuda:0'))])
epoch£º1090	 i:0 	 global-step:21800	 l-p:0.20137129724025726
epoch£º1090	 i:1 	 global-step:21801	 l-p:0.15172067284584045
epoch£º1090	 i:2 	 global-step:21802	 l-p:0.107703298330307
epoch£º1090	 i:3 	 global-step:21803	 l-p:0.11953132599592209
epoch£º1090	 i:4 	 global-step:21804	 l-p:0.14241212606430054
epoch£º1090	 i:5 	 global-step:21805	 l-p:0.15563254058361053
epoch£º1090	 i:6 	 global-step:21806	 l-p:0.16745887696743011
epoch£º1090	 i:7 	 global-step:21807	 l-p:0.12828226387500763
epoch£º1090	 i:8 	 global-step:21808	 l-p:0.11989164352416992
epoch£º1090	 i:9 	 global-step:21809	 l-p:0.11910507827997208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1091
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1588, 5.0452, 4.7084],
        [5.1588, 4.8761, 4.6272],
        [5.1588, 4.9792, 4.6427],
        [5.1588, 5.1586, 5.1588]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1091, step:0 
model_pd.l_p.mean(): 0.15347820520401 
model_pd.l_d.mean(): -20.311445236206055 
model_pd.lagr.mean(): -20.15796661376953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4352], device='cuda:0')), ('power', tensor([-20.9780], device='cuda:0'))])
epoch£º1091	 i:0 	 global-step:21820	 l-p:0.15347820520401
epoch£º1091	 i:1 	 global-step:21821	 l-p:0.1404518187046051
epoch£º1091	 i:2 	 global-step:21822	 l-p:0.1845293790102005
epoch£º1091	 i:3 	 global-step:21823	 l-p:0.1427190601825714
epoch£º1091	 i:4 	 global-step:21824	 l-p:0.1594177484512329
epoch£º1091	 i:5 	 global-step:21825	 l-p:0.15863458812236786
epoch£º1091	 i:6 	 global-step:21826	 l-p:0.12478864192962646
epoch£º1091	 i:7 	 global-step:21827	 l-p:0.09103689342737198
epoch£º1091	 i:8 	 global-step:21828	 l-p:0.1395077258348465
epoch£º1091	 i:9 	 global-step:21829	 l-p:0.12397684156894684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1092
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1531, 5.0050, 5.0630],
        [5.1531, 5.1342, 5.1508],
        [5.1531, 5.0912, 4.7648],
        [5.1531, 4.9002, 4.8720]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1092, step:0 
model_pd.l_p.mean(): 0.13140247762203217 
model_pd.l_d.mean(): -20.513744354248047 
model_pd.lagr.mean(): -20.382341384887695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4734], device='cuda:0')), ('power', tensor([-21.2215], device='cuda:0'))])
epoch£º1092	 i:0 	 global-step:21840	 l-p:0.13140247762203217
epoch£º1092	 i:1 	 global-step:21841	 l-p:0.12294957786798477
epoch£º1092	 i:2 	 global-step:21842	 l-p:0.1395316869020462
epoch£º1092	 i:3 	 global-step:21843	 l-p:0.13204628229141235
epoch£º1092	 i:4 	 global-step:21844	 l-p:0.2657225728034973
epoch£º1092	 i:5 	 global-step:21845	 l-p:0.14200979471206665
epoch£º1092	 i:6 	 global-step:21846	 l-p:0.08088115602731705
epoch£º1092	 i:7 	 global-step:21847	 l-p:0.17519839107990265
epoch£º1092	 i:8 	 global-step:21848	 l-p:0.12967006862163544
epoch£º1092	 i:9 	 global-step:21849	 l-p:0.12624795734882355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1093
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1436, 5.0792, 4.7518],
        [5.1436, 5.1434, 5.1436],
        [5.1436, 5.1436, 5.1436],
        [5.1436, 5.1435, 5.1436]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1093, step:0 
model_pd.l_p.mean(): 0.1797705441713333 
model_pd.l_d.mean(): -20.49274253845215 
model_pd.lagr.mean(): -20.312971115112305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4625], device='cuda:0')), ('power', tensor([-21.1892], device='cuda:0'))])
epoch£º1093	 i:0 	 global-step:21860	 l-p:0.1797705441713333
epoch£º1093	 i:1 	 global-step:21861	 l-p:0.12255608290433884
epoch£º1093	 i:2 	 global-step:21862	 l-p:0.11103463172912598
epoch£º1093	 i:3 	 global-step:21863	 l-p:0.113603375852108
epoch£º1093	 i:4 	 global-step:21864	 l-p:0.15037801861763
epoch£º1093	 i:5 	 global-step:21865	 l-p:0.10610000044107437
epoch£º1093	 i:6 	 global-step:21866	 l-p:0.1656663864850998
epoch£º1093	 i:7 	 global-step:21867	 l-p:0.10730266571044922
epoch£º1093	 i:8 	 global-step:21868	 l-p:0.21326525509357452
epoch£º1093	 i:9 	 global-step:21869	 l-p:0.2312699854373932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1094
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1392, 5.0577, 5.1102],
        [5.1392, 5.0345, 5.0930],
        [5.1392, 5.1392, 5.1392],
        [5.1392, 4.8890, 4.8673]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1094, step:0 
model_pd.l_p.mean(): 0.1612512767314911 
model_pd.l_d.mean(): -20.848960876464844 
model_pd.lagr.mean(): -20.68770980834961 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4105], device='cuda:0')), ('power', tensor([-21.4962], device='cuda:0'))])
epoch£º1094	 i:0 	 global-step:21880	 l-p:0.1612512767314911
epoch£º1094	 i:1 	 global-step:21881	 l-p:0.12785691022872925
epoch£º1094	 i:2 	 global-step:21882	 l-p:0.11410973966121674
epoch£º1094	 i:3 	 global-step:21883	 l-p:0.13118034601211548
epoch£º1094	 i:4 	 global-step:21884	 l-p:0.173121377825737
epoch£º1094	 i:5 	 global-step:21885	 l-p:0.12705489993095398
epoch£º1094	 i:6 	 global-step:21886	 l-p:0.11091624945402145
epoch£º1094	 i:7 	 global-step:21887	 l-p:0.1805967092514038
epoch£º1094	 i:8 	 global-step:21888	 l-p:0.23780973255634308
epoch£º1094	 i:9 	 global-step:21889	 l-p:0.1751323640346527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1095
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1359, 5.1336, 5.1358],
        [5.1359, 4.9771, 5.0331],
        [5.1359, 4.8815, 4.8534],
        [5.1359, 5.1359, 5.1359]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1095, step:0 
model_pd.l_p.mean(): 0.1402839571237564 
model_pd.l_d.mean(): -20.379438400268555 
model_pd.lagr.mean(): -20.239154815673828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4726], device='cuda:0')), ('power', tensor([-21.0849], device='cuda:0'))])
epoch£º1095	 i:0 	 global-step:21900	 l-p:0.1402839571237564
epoch£º1095	 i:1 	 global-step:21901	 l-p:0.10218657553195953
epoch£º1095	 i:2 	 global-step:21902	 l-p:0.21701642870903015
epoch£º1095	 i:3 	 global-step:21903	 l-p:0.12851765751838684
epoch£º1095	 i:4 	 global-step:21904	 l-p:0.1558566689491272
epoch£º1095	 i:5 	 global-step:21905	 l-p:0.10182538628578186
epoch£º1095	 i:6 	 global-step:21906	 l-p:0.10934241861104965
epoch£º1095	 i:7 	 global-step:21907	 l-p:0.26003265380859375
epoch£º1095	 i:8 	 global-step:21908	 l-p:0.19131894409656525
epoch£º1095	 i:9 	 global-step:21909	 l-p:0.13490909337997437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1096
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1329, 4.8821, 4.8606],
        [5.1329, 4.8781, 4.8498],
        [5.1329, 5.1327, 5.1329],
        [5.1329, 4.9154, 4.9362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1096, step:0 
model_pd.l_p.mean(): 0.12370605021715164 
model_pd.l_d.mean(): -17.805986404418945 
model_pd.lagr.mean(): -17.682279586791992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6458], device='cuda:0')), ('power', tensor([-18.6603], device='cuda:0'))])
epoch£º1096	 i:0 	 global-step:21920	 l-p:0.12370605021715164
epoch£º1096	 i:1 	 global-step:21921	 l-p:0.13905207812786102
epoch£º1096	 i:2 	 global-step:21922	 l-p:0.2544941008090973
epoch£º1096	 i:3 	 global-step:21923	 l-p:0.09856540709733963
epoch£º1096	 i:4 	 global-step:21924	 l-p:0.18968427181243896
epoch£º1096	 i:5 	 global-step:21925	 l-p:0.12534740567207336
epoch£º1096	 i:6 	 global-step:21926	 l-p:0.1292768120765686
epoch£º1096	 i:7 	 global-step:21927	 l-p:0.14908543229103088
epoch£º1096	 i:8 	 global-step:21928	 l-p:0.17019698023796082
epoch£º1096	 i:9 	 global-step:21929	 l-p:0.15229517221450806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1097
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1466, 5.1466, 5.1466],
        [5.1466, 5.1450, 5.1465],
        [5.1466, 5.1466, 5.1466],
        [5.1466, 5.2653, 5.0128]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1097, step:0 
model_pd.l_p.mean(): 0.1043802872300148 
model_pd.l_d.mean(): -19.592437744140625 
model_pd.lagr.mean(): -19.48805809020996 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4841], device='cuda:0')), ('power', tensor([-20.3011], device='cuda:0'))])
epoch£º1097	 i:0 	 global-step:21940	 l-p:0.1043802872300148
epoch£º1097	 i:1 	 global-step:21941	 l-p:0.1301274597644806
epoch£º1097	 i:2 	 global-step:21942	 l-p:0.1380513608455658
epoch£º1097	 i:3 	 global-step:21943	 l-p:0.12701448798179626
epoch£º1097	 i:4 	 global-step:21944	 l-p:0.13299624621868134
epoch£º1097	 i:5 	 global-step:21945	 l-p:0.13203151524066925
epoch£º1097	 i:6 	 global-step:21946	 l-p:0.20765478909015656
epoch£º1097	 i:7 	 global-step:21947	 l-p:0.1340557187795639
epoch£º1097	 i:8 	 global-step:21948	 l-p:0.16087940335273743
epoch£º1097	 i:9 	 global-step:21949	 l-p:0.18640044331550598
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1098
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1488, 5.1440, 5.1485],
        [5.1488, 5.1358, 5.1476],
        [5.1488, 4.8548, 4.6896],
        [5.1488, 4.9286, 4.9455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1098, step:0 
model_pd.l_p.mean(): 0.1865348517894745 
model_pd.l_d.mean(): -20.845619201660156 
model_pd.lagr.mean(): -20.65908432006836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4064], device='cuda:0')), ('power', tensor([-21.4886], device='cuda:0'))])
epoch£º1098	 i:0 	 global-step:21960	 l-p:0.1865348517894745
epoch£º1098	 i:1 	 global-step:21961	 l-p:0.2047928124666214
epoch£º1098	 i:2 	 global-step:21962	 l-p:0.17593879997730255
epoch£º1098	 i:3 	 global-step:21963	 l-p:0.1668602079153061
epoch£º1098	 i:4 	 global-step:21964	 l-p:0.16598980128765106
epoch£º1098	 i:5 	 global-step:21965	 l-p:0.11557527631521225
epoch£º1098	 i:6 	 global-step:21966	 l-p:0.07365711033344269
epoch£º1098	 i:7 	 global-step:21967	 l-p:0.11072847247123718
epoch£º1098	 i:8 	 global-step:21968	 l-p:0.11692498624324799
epoch£º1098	 i:9 	 global-step:21969	 l-p:0.1020560935139656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1099
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1684, 5.1542, 4.8429],
        [5.1684, 5.1683, 5.1684],
        [5.1684, 5.0910, 5.1419],
        [5.1684, 5.1684, 5.1684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1099, step:0 
model_pd.l_p.mean(): 0.13146021962165833 
model_pd.l_d.mean(): -20.574934005737305 
model_pd.lagr.mean(): -20.44347381591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-21.2607], device='cuda:0'))])
epoch£º1099	 i:0 	 global-step:21980	 l-p:0.13146021962165833
epoch£º1099	 i:1 	 global-step:21981	 l-p:0.20891061425209045
epoch£º1099	 i:2 	 global-step:21982	 l-p:0.11640921980142593
epoch£º1099	 i:3 	 global-step:21983	 l-p:0.1089499443769455
epoch£º1099	 i:4 	 global-step:21984	 l-p:0.09384673088788986
epoch£º1099	 i:5 	 global-step:21985	 l-p:0.19895771145820618
epoch£º1099	 i:6 	 global-step:21986	 l-p:0.08446932584047318
epoch£º1099	 i:7 	 global-step:21987	 l-p:0.1796247512102127
epoch£º1099	 i:8 	 global-step:21988	 l-p:0.11721742153167725
epoch£º1099	 i:9 	 global-step:21989	 l-p:0.11452900618314743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1772, 5.1772, 5.1772],
        [5.1772, 5.5143, 5.3876],
        [5.1772, 5.2628, 4.9936],
        [5.1772, 4.9224, 4.8883]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1100, step:0 
model_pd.l_p.mean(): 0.10984402149915695 
model_pd.l_d.mean(): -20.40746307373047 
model_pd.lagr.mean(): -20.297618865966797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4714], device='cuda:0')), ('power', tensor([-21.1120], device='cuda:0'))])
epoch£º1100	 i:0 	 global-step:22000	 l-p:0.10984402149915695
epoch£º1100	 i:1 	 global-step:22001	 l-p:0.17206892371177673
epoch£º1100	 i:2 	 global-step:22002	 l-p:0.10543724149465561
epoch£º1100	 i:3 	 global-step:22003	 l-p:0.1518024057149887
epoch£º1100	 i:4 	 global-step:22004	 l-p:0.1287938803434372
epoch£º1100	 i:5 	 global-step:22005	 l-p:0.1015237420797348
epoch£º1100	 i:6 	 global-step:22006	 l-p:0.10053841024637222
epoch£º1100	 i:7 	 global-step:22007	 l-p:0.17443057894706726
epoch£º1100	 i:8 	 global-step:22008	 l-p:0.14060497283935547
epoch£º1100	 i:9 	 global-step:22009	 l-p:0.1343463659286499
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1818, 4.9324, 4.6322],
        [5.1818, 5.1550, 5.1777],
        [5.1818, 5.1767, 5.1816],
        [5.1818, 4.9544, 4.6368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1101, step:0 
model_pd.l_p.mean(): 0.11678621172904968 
model_pd.l_d.mean(): -20.347482681274414 
model_pd.lagr.mean(): -20.230695724487305 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4777], device='cuda:0')), ('power', tensor([-21.0579], device='cuda:0'))])
epoch£º1101	 i:0 	 global-step:22020	 l-p:0.11678621172904968
epoch£º1101	 i:1 	 global-step:22021	 l-p:0.12287092208862305
epoch£º1101	 i:2 	 global-step:22022	 l-p:0.1598970741033554
epoch£º1101	 i:3 	 global-step:22023	 l-p:0.07930877059698105
epoch£º1101	 i:4 	 global-step:22024	 l-p:0.1387285441160202
epoch£º1101	 i:5 	 global-step:22025	 l-p:0.122526153922081
epoch£º1101	 i:6 	 global-step:22026	 l-p:0.11592648178339005
epoch£º1101	 i:7 	 global-step:22027	 l-p:0.11256171762943268
epoch£º1101	 i:8 	 global-step:22028	 l-p:0.11587518453598022
epoch£º1101	 i:9 	 global-step:22029	 l-p:0.22062839567661285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1857, 4.9237, 4.8740],
        [5.1857, 5.0071, 4.6722],
        [5.1857, 5.1771, 5.1851],
        [5.1857, 4.8955, 4.7314]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1102, step:0 
model_pd.l_p.mean(): 0.1292903572320938 
model_pd.l_d.mean(): -19.66971206665039 
model_pd.lagr.mean(): -19.540422439575195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4678], device='cuda:0')), ('power', tensor([-20.3626], device='cuda:0'))])
epoch£º1102	 i:0 	 global-step:22040	 l-p:0.1292903572320938
epoch£º1102	 i:1 	 global-step:22041	 l-p:0.15754467248916626
epoch£º1102	 i:2 	 global-step:22042	 l-p:0.1501985639333725
epoch£º1102	 i:3 	 global-step:22043	 l-p:0.15449683368206024
epoch£º1102	 i:4 	 global-step:22044	 l-p:0.1425364911556244
epoch£º1102	 i:5 	 global-step:22045	 l-p:0.10014684498310089
epoch£º1102	 i:6 	 global-step:22046	 l-p:0.12992030382156372
epoch£º1102	 i:7 	 global-step:22047	 l-p:0.11389052867889404
epoch£º1102	 i:8 	 global-step:22048	 l-p:0.16841398179531097
epoch£º1102	 i:9 	 global-step:22049	 l-p:0.07210572808980942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1740, 5.1519, 5.1711],
        [5.1740, 5.0109, 5.0646],
        [5.1740, 5.1686, 4.8604],
        [5.1740, 5.1740, 5.1740]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1103, step:0 
model_pd.l_p.mean(): 0.14002570509910583 
model_pd.l_d.mean(): -20.47996711730957 
model_pd.lagr.mean(): -20.339941024780273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4610], device='cuda:0')), ('power', tensor([-21.1748], device='cuda:0'))])
epoch£º1103	 i:0 	 global-step:22060	 l-p:0.14002570509910583
epoch£º1103	 i:1 	 global-step:22061	 l-p:0.14729957282543182
epoch£º1103	 i:2 	 global-step:22062	 l-p:0.19791541993618011
epoch£º1103	 i:3 	 global-step:22063	 l-p:0.13668370246887207
epoch£º1103	 i:4 	 global-step:22064	 l-p:0.14635291695594788
epoch£º1103	 i:5 	 global-step:22065	 l-p:0.057681839913129807
epoch£º1103	 i:6 	 global-step:22066	 l-p:0.11533863097429276
epoch£º1103	 i:7 	 global-step:22067	 l-p:0.13133195042610168
epoch£º1103	 i:8 	 global-step:22068	 l-p:0.10900979489088058
epoch£º1103	 i:9 	 global-step:22069	 l-p:0.1672123372554779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1731, 5.4651, 5.3101],
        [5.1731, 5.0828, 5.1378],
        [5.1731, 5.1731, 5.1731],
        [5.1731, 5.1687, 5.1729]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1104, step:0 
model_pd.l_p.mean(): 0.11722978204488754 
model_pd.l_d.mean(): -20.530237197875977 
model_pd.lagr.mean(): -20.413007736206055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4587], device='cuda:0')), ('power', tensor([-21.2232], device='cuda:0'))])
epoch£º1104	 i:0 	 global-step:22080	 l-p:0.11722978204488754
epoch£º1104	 i:1 	 global-step:22081	 l-p:0.14891324937343597
epoch£º1104	 i:2 	 global-step:22082	 l-p:0.07119077444076538
epoch£º1104	 i:3 	 global-step:22083	 l-p:0.1326792687177658
epoch£º1104	 i:4 	 global-step:22084	 l-p:0.12361707538366318
epoch£º1104	 i:5 	 global-step:22085	 l-p:0.14586177468299866
epoch£º1104	 i:6 	 global-step:22086	 l-p:0.16439414024353027
epoch£º1104	 i:7 	 global-step:22087	 l-p:0.11505646258592606
epoch£º1104	 i:8 	 global-step:22088	 l-p:0.20796748995780945
epoch£º1104	 i:9 	 global-step:22089	 l-p:0.126913920044899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1693, 4.9706, 4.6384],
        [5.1693, 5.1616, 5.1688],
        [5.1693, 5.0713, 4.7365],
        [5.1693, 4.9940, 5.0429]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1105, step:0 
model_pd.l_p.mean(): 0.16313254833221436 
model_pd.l_d.mean(): -20.41912269592285 
model_pd.lagr.mean(): -20.255990982055664 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4682], device='cuda:0')), ('power', tensor([-21.1206], device='cuda:0'))])
epoch£º1105	 i:0 	 global-step:22100	 l-p:0.16313254833221436
epoch£º1105	 i:1 	 global-step:22101	 l-p:0.13027575612068176
epoch£º1105	 i:2 	 global-step:22102	 l-p:0.13326072692871094
epoch£º1105	 i:3 	 global-step:22103	 l-p:0.17428426444530487
epoch£º1105	 i:4 	 global-step:22104	 l-p:0.15336394309997559
epoch£º1105	 i:5 	 global-step:22105	 l-p:0.1625388264656067
epoch£º1105	 i:6 	 global-step:22106	 l-p:0.09360054135322571
epoch£º1105	 i:7 	 global-step:22107	 l-p:0.11738157272338867
epoch£º1105	 i:8 	 global-step:22108	 l-p:0.09416721016168594
epoch£º1105	 i:9 	 global-step:22109	 l-p:0.1506602019071579
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1600, 5.2422, 4.9713],
        [5.1600, 5.1569, 5.1598],
        [5.1600, 5.1358, 5.1565],
        [5.1600, 5.1263, 5.1539]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1106, step:0 
model_pd.l_p.mean(): 0.13667304813861847 
model_pd.l_d.mean(): -20.301082611083984 
model_pd.lagr.mean(): -20.164409637451172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4616], device='cuda:0')), ('power', tensor([-20.9945], device='cuda:0'))])
epoch£º1106	 i:0 	 global-step:22120	 l-p:0.13667304813861847
epoch£º1106	 i:1 	 global-step:22121	 l-p:0.10588507354259491
epoch£º1106	 i:2 	 global-step:22122	 l-p:0.12543468177318573
epoch£º1106	 i:3 	 global-step:22123	 l-p:0.14531585574150085
epoch£º1106	 i:4 	 global-step:22124	 l-p:0.11292356252670288
epoch£º1106	 i:5 	 global-step:22125	 l-p:0.22051270306110382
epoch£º1106	 i:6 	 global-step:22126	 l-p:0.13435247540473938
epoch£º1106	 i:7 	 global-step:22127	 l-p:0.16845615208148956
epoch£º1106	 i:8 	 global-step:22128	 l-p:0.12395873665809631
epoch£º1106	 i:9 	 global-step:22129	 l-p:0.14823295176029205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1520, 5.2873, 5.0430],
        [5.1520, 5.1520, 5.1520],
        [5.1520, 5.0523, 5.1098],
        [5.1520, 5.1489, 5.1519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1107, step:0 
model_pd.l_p.mean(): 0.1090535894036293 
model_pd.l_d.mean(): -20.50389862060547 
model_pd.lagr.mean(): -20.394845962524414 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4717], device='cuda:0')), ('power', tensor([-21.2099], device='cuda:0'))])
epoch£º1107	 i:0 	 global-step:22140	 l-p:0.1090535894036293
epoch£º1107	 i:1 	 global-step:22141	 l-p:0.1360386610031128
epoch£º1107	 i:2 	 global-step:22142	 l-p:0.16981570422649384
epoch£º1107	 i:3 	 global-step:22143	 l-p:0.12018810212612152
epoch£º1107	 i:4 	 global-step:22144	 l-p:0.19387614727020264
epoch£º1107	 i:5 	 global-step:22145	 l-p:0.16191840171813965
epoch£º1107	 i:6 	 global-step:22146	 l-p:0.12665902078151703
epoch£º1107	 i:7 	 global-step:22147	 l-p:0.1323113888502121
epoch£º1107	 i:8 	 global-step:22148	 l-p:0.11460397392511368
epoch£º1107	 i:9 	 global-step:22149	 l-p:0.17250429093837738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1530, 4.8645, 4.7426],
        [5.1530, 5.1530, 5.1530],
        [5.1530, 4.9144, 4.9097],
        [5.1530, 4.8651, 4.6255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1108, step:0 
model_pd.l_p.mean(): 0.13903173804283142 
model_pd.l_d.mean(): -20.238746643066406 
model_pd.lagr.mean(): -20.099714279174805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4833], device='cuda:0')), ('power', tensor([-20.9536], device='cuda:0'))])
epoch£º1108	 i:0 	 global-step:22160	 l-p:0.13903173804283142
epoch£º1108	 i:1 	 global-step:22161	 l-p:0.1956021785736084
epoch£º1108	 i:2 	 global-step:22162	 l-p:0.17674438655376434
epoch£º1108	 i:3 	 global-step:22163	 l-p:0.11993545293807983
epoch£º1108	 i:4 	 global-step:22164	 l-p:0.1762814223766327
epoch£º1108	 i:5 	 global-step:22165	 l-p:0.12352649122476578
epoch£º1108	 i:6 	 global-step:22166	 l-p:0.09563051164150238
epoch£º1108	 i:7 	 global-step:22167	 l-p:0.09979695081710815
epoch£º1108	 i:8 	 global-step:22168	 l-p:0.16625529527664185
epoch£º1108	 i:9 	 global-step:22169	 l-p:0.13338594138622284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1585, 5.2433, 4.9736],
        [5.1585, 5.1584, 5.1585],
        [5.1585, 4.9629, 5.0008],
        [5.1585, 4.9355, 4.9493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1109, step:0 
model_pd.l_p.mean(): 0.16051159799098969 
model_pd.l_d.mean(): -20.332643508911133 
model_pd.lagr.mean(): -20.17213249206543 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4720], device='cuda:0')), ('power', tensor([-21.0371], device='cuda:0'))])
epoch£º1109	 i:0 	 global-step:22180	 l-p:0.16051159799098969
epoch£º1109	 i:1 	 global-step:22181	 l-p:0.19171738624572754
epoch£º1109	 i:2 	 global-step:22182	 l-p:0.14082159101963043
epoch£º1109	 i:3 	 global-step:22183	 l-p:0.14615963399410248
epoch£º1109	 i:4 	 global-step:22184	 l-p:0.15766465663909912
epoch£º1109	 i:5 	 global-step:22185	 l-p:0.12658339738845825
epoch£º1109	 i:6 	 global-step:22186	 l-p:0.10443076491355896
epoch£º1109	 i:7 	 global-step:22187	 l-p:0.08708047866821289
epoch£º1109	 i:8 	 global-step:22188	 l-p:0.14048507809638977
epoch£º1109	 i:9 	 global-step:22189	 l-p:0.12876318395137787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1110
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228]], device='cuda:0')
 pt:tensor([[5.1742, 4.9919, 5.0372],
        [5.1742, 5.0081, 4.6698],
        [5.1742, 5.2060, 4.9123],
        [5.1742, 5.2548, 4.9829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1110, step:0 
model_pd.l_p.mean(): 0.1729341447353363 
model_pd.l_d.mean(): -20.215356826782227 
model_pd.lagr.mean(): -20.042423248291016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5053], device='cuda:0')), ('power', tensor([-20.9525], device='cuda:0'))])
epoch£º1110	 i:0 	 global-step:22200	 l-p:0.1729341447353363
epoch£º1110	 i:1 	 global-step:22201	 l-p:0.12111730128526688
epoch£º1110	 i:2 	 global-step:22202	 l-p:0.10720441490411758
epoch£º1110	 i:3 	 global-step:22203	 l-p:0.17474150657653809
epoch£º1110	 i:4 	 global-step:22204	 l-p:0.15862521529197693
epoch£º1110	 i:5 	 global-step:22205	 l-p:0.1353137493133545
epoch£º1110	 i:6 	 global-step:22206	 l-p:0.12685036659240723
epoch£º1110	 i:7 	 global-step:22207	 l-p:0.08980639278888702
epoch£º1110	 i:8 	 global-step:22208	 l-p:0.11744926124811172
epoch£º1110	 i:9 	 global-step:22209	 l-p:0.13256259262561798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1813, 5.1538, 4.8375],
        [5.1813, 5.1425, 5.1735],
        [5.1813, 5.1813, 5.1813],
        [5.1813, 4.8899, 4.7244]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1111, step:0 
model_pd.l_p.mean(): 0.13256917893886566 
model_pd.l_d.mean(): -18.865507125854492 
model_pd.lagr.mean(): -18.732938766479492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5370], device='cuda:0')), ('power', tensor([-19.6203], device='cuda:0'))])
epoch£º1111	 i:0 	 global-step:22220	 l-p:0.13256917893886566
epoch£º1111	 i:1 	 global-step:22221	 l-p:0.09433083981275558
epoch£º1111	 i:2 	 global-step:22222	 l-p:0.14843639731407166
epoch£º1111	 i:3 	 global-step:22223	 l-p:0.15843191742897034
epoch£º1111	 i:4 	 global-step:22224	 l-p:0.13308483362197876
epoch£º1111	 i:5 	 global-step:22225	 l-p:0.1879463940858841
epoch£º1111	 i:6 	 global-step:22226	 l-p:0.12953495979309082
epoch£º1111	 i:7 	 global-step:22227	 l-p:0.10851237177848816
epoch£º1111	 i:8 	 global-step:22228	 l-p:0.06300313025712967
epoch£º1111	 i:9 	 global-step:22229	 l-p:0.2059929221868515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1112
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[5.1633, 4.9173, 4.6088],
        [5.1633, 4.9408, 4.6165],
        [5.1633, 4.8988, 4.8488],
        [5.1633, 4.9174, 4.6088]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1112, step:0 
model_pd.l_p.mean(): 0.13452798128128052 
model_pd.l_d.mean(): -19.846654891967773 
model_pd.lagr.mean(): -19.712127685546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4931], device='cuda:0')), ('power', tensor([-20.5673], device='cuda:0'))])
epoch£º1112	 i:0 	 global-step:22240	 l-p:0.13452798128128052
epoch£º1112	 i:1 	 global-step:22241	 l-p:0.2000235766172409
epoch£º1112	 i:2 	 global-step:22242	 l-p:0.07746823132038116
epoch£º1112	 i:3 	 global-step:22243	 l-p:0.18905647099018097
epoch£º1112	 i:4 	 global-step:22244	 l-p:0.16848872601985931
epoch£º1112	 i:5 	 global-step:22245	 l-p:0.11751823127269745
epoch£º1112	 i:6 	 global-step:22246	 l-p:0.09552443027496338
epoch£º1112	 i:7 	 global-step:22247	 l-p:0.13479699194431305
epoch£º1112	 i:8 	 global-step:22248	 l-p:0.13231374323368073
epoch£º1112	 i:9 	 global-step:22249	 l-p:0.13157223165035248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1680, 5.1668, 5.1680],
        [5.1680, 5.1674, 5.1680],
        [5.1680, 5.1680, 5.1680],
        [5.1680, 4.9175, 4.8934]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1113, step:0 
model_pd.l_p.mean(): 0.16637982428073883 
model_pd.l_d.mean(): -19.421377182006836 
model_pd.lagr.mean(): -19.25499725341797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5304], device='cuda:0')), ('power', tensor([-20.1755], device='cuda:0'))])
epoch£º1113	 i:0 	 global-step:22260	 l-p:0.16637982428073883
epoch£º1113	 i:1 	 global-step:22261	 l-p:0.1690102517604828
epoch£º1113	 i:2 	 global-step:22262	 l-p:0.12626662850379944
epoch£º1113	 i:3 	 global-step:22263	 l-p:0.13244327902793884
epoch£º1113	 i:4 	 global-step:22264	 l-p:0.10822142660617828
epoch£º1113	 i:5 	 global-step:22265	 l-p:0.15797244012355804
epoch£º1113	 i:6 	 global-step:22266	 l-p:0.1282513588666916
epoch£º1113	 i:7 	 global-step:22267	 l-p:0.11184356361627579
epoch£º1113	 i:8 	 global-step:22268	 l-p:0.16621793806552887
epoch£º1113	 i:9 	 global-step:22269	 l-p:0.11030248552560806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1635, 5.0806, 5.1335],
        [5.1635, 4.8697, 4.6987],
        [5.1635, 4.8867, 4.6190],
        [5.1635, 5.1606, 5.1634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1114, step:0 
model_pd.l_p.mean(): 0.13110420107841492 
model_pd.l_d.mean(): -18.50684356689453 
model_pd.lagr.mean(): -18.37574005126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5411], device='cuda:0')), ('power', tensor([-19.2619], device='cuda:0'))])
epoch£º1114	 i:0 	 global-step:22280	 l-p:0.13110420107841492
epoch£º1114	 i:1 	 global-step:22281	 l-p:0.15390343964099884
epoch£º1114	 i:2 	 global-step:22282	 l-p:0.15560156106948853
epoch£º1114	 i:3 	 global-step:22283	 l-p:0.15258494019508362
epoch£º1114	 i:4 	 global-step:22284	 l-p:0.12445682287216187
epoch£º1114	 i:5 	 global-step:22285	 l-p:0.15817400813102722
epoch£º1114	 i:6 	 global-step:22286	 l-p:0.1714571714401245
epoch£º1114	 i:7 	 global-step:22287	 l-p:0.13880078494548798
epoch£º1114	 i:8 	 global-step:22288	 l-p:0.10398425906896591
epoch£º1114	 i:9 	 global-step:22289	 l-p:0.09442262351512909
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1642, 4.9799, 4.6427],
        [5.1642, 5.1573, 5.1637],
        [5.1642, 5.2412, 4.9674],
        [5.1642, 5.1496, 5.1627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1115, step:0 
model_pd.l_p.mean(): 0.12190788984298706 
model_pd.l_d.mean(): -20.144775390625 
model_pd.lagr.mean(): -20.02286720275879 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4935], device='cuda:0')), ('power', tensor([-20.8691], device='cuda:0'))])
epoch£º1115	 i:0 	 global-step:22300	 l-p:0.12190788984298706
epoch£º1115	 i:1 	 global-step:22301	 l-p:0.13920237123966217
epoch£º1115	 i:2 	 global-step:22302	 l-p:0.12980732321739197
epoch£º1115	 i:3 	 global-step:22303	 l-p:0.1256132870912552
epoch£º1115	 i:4 	 global-step:22304	 l-p:0.2203260362148285
epoch£º1115	 i:5 	 global-step:22305	 l-p:0.15297748148441315
epoch£º1115	 i:6 	 global-step:22306	 l-p:0.14198531210422516
epoch£º1115	 i:7 	 global-step:22307	 l-p:0.11978600919246674
epoch£º1115	 i:8 	 global-step:22308	 l-p:0.12480548769235611
epoch£º1115	 i:9 	 global-step:22309	 l-p:0.10821156948804855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1630, 4.9354, 4.9443],
        [5.1630, 4.9130, 4.8911],
        [5.1630, 4.9089, 4.6073],
        [5.1630, 5.1630, 5.1630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1116, step:0 
model_pd.l_p.mean(): 0.1392781287431717 
model_pd.l_d.mean(): -20.511661529541016 
model_pd.lagr.mean(): -20.37238311767578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4435], device='cuda:0')), ('power', tensor([-21.1889], device='cuda:0'))])
epoch£º1116	 i:0 	 global-step:22320	 l-p:0.1392781287431717
epoch£º1116	 i:1 	 global-step:22321	 l-p:0.18015697598457336
epoch£º1116	 i:2 	 global-step:22322	 l-p:0.12325825542211533
epoch£º1116	 i:3 	 global-step:22323	 l-p:0.18033798038959503
epoch£º1116	 i:4 	 global-step:22324	 l-p:0.1516740918159485
epoch£º1116	 i:5 	 global-step:22325	 l-p:0.1280226707458496
epoch£º1116	 i:6 	 global-step:22326	 l-p:0.1107296422123909
epoch£º1116	 i:7 	 global-step:22327	 l-p:0.10652536898851395
epoch£º1116	 i:8 	 global-step:22328	 l-p:0.1396017074584961
epoch£º1116	 i:9 	 global-step:22329	 l-p:0.14086265861988068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1601, 5.0969, 5.1418],
        [5.1601, 5.1518, 5.1595],
        [5.1601, 4.9011, 4.8634],
        [5.1601, 5.1600, 5.1601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1117, step:0 
model_pd.l_p.mean(): 0.13478706777095795 
model_pd.l_d.mean(): -20.714458465576172 
model_pd.lagr.mean(): -20.57967185974121 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4186], device='cuda:0')), ('power', tensor([-21.3685], device='cuda:0'))])
epoch£º1117	 i:0 	 global-step:22340	 l-p:0.13478706777095795
epoch£º1117	 i:1 	 global-step:22341	 l-p:0.12551432847976685
epoch£º1117	 i:2 	 global-step:22342	 l-p:0.21790902316570282
epoch£º1117	 i:3 	 global-step:22343	 l-p:0.053511038422584534
epoch£º1117	 i:4 	 global-step:22344	 l-p:0.17717412114143372
epoch£º1117	 i:5 	 global-step:22345	 l-p:0.17165203392505646
epoch£º1117	 i:6 	 global-step:22346	 l-p:0.1085052564740181
epoch£º1117	 i:7 	 global-step:22347	 l-p:0.1047852486371994
epoch£º1117	 i:8 	 global-step:22348	 l-p:0.18277330696582794
epoch£º1117	 i:9 	 global-step:22349	 l-p:0.12984946370124817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1603, 5.1548, 5.1600],
        [5.1603, 4.9842, 5.0335],
        [5.1603, 5.0696, 5.1249],
        [5.1603, 5.1575, 5.1602]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1118, step:0 
model_pd.l_p.mean(): 0.12018855661153793 
model_pd.l_d.mean(): -19.263381958007812 
model_pd.lagr.mean(): -19.1431941986084 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5061], device='cuda:0')), ('power', tensor([-19.9909], device='cuda:0'))])
epoch£º1118	 i:0 	 global-step:22360	 l-p:0.12018855661153793
epoch£º1118	 i:1 	 global-step:22361	 l-p:0.12833641469478607
epoch£º1118	 i:2 	 global-step:22362	 l-p:0.10173753648996353
epoch£º1118	 i:3 	 global-step:22363	 l-p:0.12580923736095428
epoch£º1118	 i:4 	 global-step:22364	 l-p:0.14667908847332
epoch£º1118	 i:5 	 global-step:22365	 l-p:0.17407125234603882
epoch£º1118	 i:6 	 global-step:22366	 l-p:0.204909086227417
epoch£º1118	 i:7 	 global-step:22367	 l-p:0.18671295046806335
epoch£º1118	 i:8 	 global-step:22368	 l-p:0.12380561232566833
epoch£º1118	 i:9 	 global-step:22369	 l-p:0.11331064999103546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1550, 4.9110, 4.8993],
        [5.1550, 5.2801, 5.0301],
        [5.1550, 5.1497, 5.1547],
        [5.1550, 5.5135, 5.4002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1119, step:0 
model_pd.l_p.mean(): 0.10887784510850906 
model_pd.l_d.mean(): -19.276376724243164 
model_pd.lagr.mean(): -19.167499542236328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5675], device='cuda:0')), ('power', tensor([-20.0668], device='cuda:0'))])
epoch£º1119	 i:0 	 global-step:22380	 l-p:0.10887784510850906
epoch£º1119	 i:1 	 global-step:22381	 l-p:0.12530198693275452
epoch£º1119	 i:2 	 global-step:22382	 l-p:0.16125606000423431
epoch£º1119	 i:3 	 global-step:22383	 l-p:0.16697148978710175
epoch£º1119	 i:4 	 global-step:22384	 l-p:0.15496553480625153
epoch£º1119	 i:5 	 global-step:22385	 l-p:0.11214400082826614
epoch£º1119	 i:6 	 global-step:22386	 l-p:0.1190698891878128
epoch£º1119	 i:7 	 global-step:22387	 l-p:0.14663752913475037
epoch£º1119	 i:8 	 global-step:22388	 l-p:0.1502276062965393
epoch£º1119	 i:9 	 global-step:22389	 l-p:0.15927988290786743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1679, 4.8844, 4.6330],
        [5.1679, 5.0422, 5.1025],
        [5.1679, 5.1179, 5.1558],
        [5.1679, 5.2586, 4.9912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1120, step:0 
model_pd.l_p.mean(): 0.14754511415958405 
model_pd.l_d.mean(): -19.439748764038086 
model_pd.lagr.mean(): -19.292203903198242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5475], device='cuda:0')), ('power', tensor([-20.2115], device='cuda:0'))])
epoch£º1120	 i:0 	 global-step:22400	 l-p:0.14754511415958405
epoch£º1120	 i:1 	 global-step:22401	 l-p:0.1824418604373932
epoch£º1120	 i:2 	 global-step:22402	 l-p:0.07677172124385834
epoch£º1120	 i:3 	 global-step:22403	 l-p:0.12136521190404892
epoch£º1120	 i:4 	 global-step:22404	 l-p:0.14985866844654083
epoch£º1120	 i:5 	 global-step:22405	 l-p:0.12655340135097504
epoch£º1120	 i:6 	 global-step:22406	 l-p:0.14187707006931305
epoch£º1120	 i:7 	 global-step:22407	 l-p:0.14052638411521912
epoch£º1120	 i:8 	 global-step:22408	 l-p:0.13234037160873413
epoch£º1120	 i:9 	 global-step:22409	 l-p:0.1328887939453125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1837, 5.0635, 4.7255],
        [5.1837, 5.2094, 4.9127],
        [5.1837, 5.1728, 5.1828],
        [5.1837, 5.1833, 5.1837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1121, step:0 
model_pd.l_p.mean(): 0.1218130812048912 
model_pd.l_d.mean(): -20.908588409423828 
model_pd.lagr.mean(): -20.786775588989258 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3955], device='cuda:0')), ('power', tensor([-21.5411], device='cuda:0'))])
epoch£º1121	 i:0 	 global-step:22420	 l-p:0.1218130812048912
epoch£º1121	 i:1 	 global-step:22421	 l-p:0.15032154321670532
epoch£º1121	 i:2 	 global-step:22422	 l-p:0.11326372623443604
epoch£º1121	 i:3 	 global-step:22423	 l-p:0.1375274807214737
epoch£º1121	 i:4 	 global-step:22424	 l-p:0.1302589774131775
epoch£º1121	 i:5 	 global-step:22425	 l-p:0.09938891977071762
epoch£º1121	 i:6 	 global-step:22426	 l-p:0.17037276923656464
epoch£º1121	 i:7 	 global-step:22427	 l-p:0.10513455420732498
epoch£º1121	 i:8 	 global-step:22428	 l-p:0.12573963403701782
epoch£º1121	 i:9 	 global-step:22429	 l-p:0.18042689561843872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1736, 5.1303, 5.1642],
        [5.1736, 5.1400, 5.1675],
        [5.1736, 5.0844, 5.1392],
        [5.1736, 5.4332, 5.2580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1122, step:0 
model_pd.l_p.mean(): 0.16053050756454468 
model_pd.l_d.mean(): -20.017976760864258 
model_pd.lagr.mean(): -19.857446670532227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5273], device='cuda:0')), ('power', tensor([-20.7754], device='cuda:0'))])
epoch£º1122	 i:0 	 global-step:22440	 l-p:0.16053050756454468
epoch£º1122	 i:1 	 global-step:22441	 l-p:0.16367611289024353
epoch£º1122	 i:2 	 global-step:22442	 l-p:0.10707283765077591
epoch£º1122	 i:3 	 global-step:22443	 l-p:0.10787777602672577
epoch£º1122	 i:4 	 global-step:22444	 l-p:0.1305166333913803
epoch£º1122	 i:5 	 global-step:22445	 l-p:0.1404763013124466
epoch£º1122	 i:6 	 global-step:22446	 l-p:0.18764682114124298
epoch£º1122	 i:7 	 global-step:22447	 l-p:0.13326743245124817
epoch£º1122	 i:8 	 global-step:22448	 l-p:0.11795803904533386
epoch£º1122	 i:9 	 global-step:22449	 l-p:0.09594621509313583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1755, 4.8923, 4.6425],
        [5.1755, 4.9726, 4.6407],
        [5.1755, 5.0864, 5.1412],
        [5.1755, 5.1700, 5.1752]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1123, step:0 
model_pd.l_p.mean(): 0.12092380225658417 
model_pd.l_d.mean(): -20.375446319580078 
model_pd.lagr.mean(): -20.2545223236084 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4814], device='cuda:0')), ('power', tensor([-21.0899], device='cuda:0'))])
epoch£º1123	 i:0 	 global-step:22460	 l-p:0.12092380225658417
epoch£º1123	 i:1 	 global-step:22461	 l-p:0.11072974652051926
epoch£º1123	 i:2 	 global-step:22462	 l-p:0.16685111820697784
epoch£º1123	 i:3 	 global-step:22463	 l-p:0.11128906160593033
epoch£º1123	 i:4 	 global-step:22464	 l-p:0.11939632147550583
epoch£º1123	 i:5 	 global-step:22465	 l-p:0.15887655317783356
epoch£º1123	 i:6 	 global-step:22466	 l-p:0.102256640791893
epoch£º1123	 i:7 	 global-step:22467	 l-p:0.14935751259326935
epoch£º1123	 i:8 	 global-step:22468	 l-p:0.17850320041179657
epoch£º1123	 i:9 	 global-step:22469	 l-p:0.1282299906015396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1727, 4.9984, 4.6597],
        [5.1727, 5.0537, 5.1138],
        [5.1727, 4.9035, 4.8426],
        [5.1727, 5.0680, 5.1265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1124, step:0 
model_pd.l_p.mean(): 0.16208747029304504 
model_pd.l_d.mean(): -20.68067169189453 
model_pd.lagr.mean(): -20.518583297729492 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4230], device='cuda:0')), ('power', tensor([-21.3388], device='cuda:0'))])
epoch£º1124	 i:0 	 global-step:22480	 l-p:0.16208747029304504
epoch£º1124	 i:1 	 global-step:22481	 l-p:0.10944347083568573
epoch£º1124	 i:2 	 global-step:22482	 l-p:0.13048900663852692
epoch£º1124	 i:3 	 global-step:22483	 l-p:0.11503899097442627
epoch£º1124	 i:4 	 global-step:22484	 l-p:0.1373644769191742
epoch£º1124	 i:5 	 global-step:22485	 l-p:0.08997389674186707
epoch£º1124	 i:6 	 global-step:22486	 l-p:0.15276454389095306
epoch£º1124	 i:7 	 global-step:22487	 l-p:0.12789177894592285
epoch£º1124	 i:8 	 global-step:22488	 l-p:0.1731577217578888
epoch£º1124	 i:9 	 global-step:22489	 l-p:0.186617910861969
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1624, 5.0688, 5.1249],
        [5.1624, 4.8676, 4.6715],
        [5.1624, 4.8678, 4.7034],
        [5.1624, 5.1534, 5.1617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1125, step:0 
model_pd.l_p.mean(): 0.15859517455101013 
model_pd.l_d.mean(): -20.893028259277344 
model_pd.lagr.mean(): -20.734432220458984 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4164], device='cuda:0')), ('power', tensor([-21.5467], device='cuda:0'))])
epoch£º1125	 i:0 	 global-step:22500	 l-p:0.15859517455101013
epoch£º1125	 i:1 	 global-step:22501	 l-p:0.1560298502445221
epoch£º1125	 i:2 	 global-step:22502	 l-p:0.1295236498117447
epoch£º1125	 i:3 	 global-step:22503	 l-p:0.11153563857078552
epoch£º1125	 i:4 	 global-step:22504	 l-p:0.1402583122253418
epoch£º1125	 i:5 	 global-step:22505	 l-p:0.14628486335277557
epoch£º1125	 i:6 	 global-step:22506	 l-p:0.1556805819272995
epoch£º1125	 i:7 	 global-step:22507	 l-p:0.08834285289049149
epoch£º1125	 i:8 	 global-step:22508	 l-p:0.14222250878810883
epoch£º1125	 i:9 	 global-step:22509	 l-p:0.16096056997776031
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1668, 5.1668, 5.1668],
        [5.1668, 5.0771, 5.1322],
        [5.1668, 4.9987, 5.0515],
        [5.1668, 4.8783, 4.7561]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1126, step:0 
model_pd.l_p.mean(): 0.10765689611434937 
model_pd.l_d.mean(): -20.743755340576172 
model_pd.lagr.mean(): -20.636098861694336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4342], device='cuda:0')), ('power', tensor([-21.4141], device='cuda:0'))])
epoch£º1126	 i:0 	 global-step:22520	 l-p:0.10765689611434937
epoch£º1126	 i:1 	 global-step:22521	 l-p:0.21340228617191315
epoch£º1126	 i:2 	 global-step:22522	 l-p:0.17621326446533203
epoch£º1126	 i:3 	 global-step:22523	 l-p:0.10986264795064926
epoch£º1126	 i:4 	 global-step:22524	 l-p:0.11843894422054291
epoch£º1126	 i:5 	 global-step:22525	 l-p:0.12972915172576904
epoch£º1126	 i:6 	 global-step:22526	 l-p:0.16188664734363556
epoch£º1126	 i:7 	 global-step:22527	 l-p:0.1192757710814476
epoch£º1126	 i:8 	 global-step:22528	 l-p:0.1297765076160431
epoch£º1126	 i:9 	 global-step:22529	 l-p:0.09446954727172852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1754, 4.8835, 4.7344],
        [5.1754, 5.1749, 5.1754],
        [5.1754, 5.1752, 5.1754],
        [5.1754, 4.9922, 4.6550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1127, step:0 
model_pd.l_p.mean(): 0.17872600257396698 
model_pd.l_d.mean(): -20.70252227783203 
model_pd.lagr.mean(): -20.52379608154297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4533], device='cuda:0')), ('power', tensor([-21.3918], device='cuda:0'))])
epoch£º1127	 i:0 	 global-step:22540	 l-p:0.17872600257396698
epoch£º1127	 i:1 	 global-step:22541	 l-p:0.08143817633390427
epoch£º1127	 i:2 	 global-step:22542	 l-p:0.13425691425800323
epoch£º1127	 i:3 	 global-step:22543	 l-p:0.11208923161029816
epoch£º1127	 i:4 	 global-step:22544	 l-p:0.11549463123083115
epoch£º1127	 i:5 	 global-step:22545	 l-p:0.1541910171508789
epoch£º1127	 i:6 	 global-step:22546	 l-p:0.12989747524261475
epoch£º1127	 i:7 	 global-step:22547	 l-p:0.14883431792259216
epoch£º1127	 i:8 	 global-step:22548	 l-p:0.11094354093074799
epoch£º1127	 i:9 	 global-step:22549	 l-p:0.18227243423461914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1726, 5.1726, 5.1726],
        [5.1726, 5.1726, 5.1726],
        [5.1726, 5.1172, 5.1581],
        [5.1726, 4.8784, 4.7006]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1128, step:0 
model_pd.l_p.mean(): 0.18871848285198212 
model_pd.l_d.mean(): -18.80526351928711 
model_pd.lagr.mean(): -18.616544723510742 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5547], device='cuda:0')), ('power', tensor([-19.5774], device='cuda:0'))])
epoch£º1128	 i:0 	 global-step:22560	 l-p:0.18871848285198212
epoch£º1128	 i:1 	 global-step:22561	 l-p:0.1489758938550949
epoch£º1128	 i:2 	 global-step:22562	 l-p:0.11865217238664627
epoch£º1128	 i:3 	 global-step:22563	 l-p:0.11065042018890381
epoch£º1128	 i:4 	 global-step:22564	 l-p:0.12305577099323273
epoch£º1128	 i:5 	 global-step:22565	 l-p:0.16043037176132202
epoch£º1128	 i:6 	 global-step:22566	 l-p:0.10042798519134521
epoch£º1128	 i:7 	 global-step:22567	 l-p:0.1213635578751564
epoch£º1128	 i:8 	 global-step:22568	 l-p:0.16117991507053375
epoch£º1128	 i:9 	 global-step:22569	 l-p:0.12947572767734528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1662, 5.1662, 5.1662],
        [5.1662, 5.0739, 5.1297],
        [5.1662, 5.1538, 5.1651],
        [5.1662, 5.1501, 5.1644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1129, step:0 
model_pd.l_p.mean(): 0.11578704416751862 
model_pd.l_d.mean(): -19.71537208557129 
model_pd.lagr.mean(): -19.599584579467773 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4607], device='cuda:0')), ('power', tensor([-20.4015], device='cuda:0'))])
epoch£º1129	 i:0 	 global-step:22580	 l-p:0.11578704416751862
epoch£º1129	 i:1 	 global-step:22581	 l-p:0.18677562475204468
epoch£º1129	 i:2 	 global-step:22582	 l-p:0.11348405480384827
epoch£º1129	 i:3 	 global-step:22583	 l-p:0.12418629974126816
epoch£º1129	 i:4 	 global-step:22584	 l-p:0.13788366317749023
epoch£º1129	 i:5 	 global-step:22585	 l-p:0.15148697793483734
epoch£º1129	 i:6 	 global-step:22586	 l-p:0.18599237501621246
epoch£º1129	 i:7 	 global-step:22587	 l-p:0.1980368196964264
epoch£º1129	 i:8 	 global-step:22588	 l-p:0.08722684532403946
epoch£º1129	 i:9 	 global-step:22589	 l-p:0.09044820815324783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1624, 5.4032, 5.2167],
        [5.1624, 5.1624, 5.1624],
        [5.1624, 5.4333, 5.2648],
        [5.1624, 4.9119, 4.8902]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1130, step:0 
model_pd.l_p.mean(): 0.17400683462619781 
model_pd.l_d.mean(): -19.22056007385254 
model_pd.lagr.mean(): -19.046552658081055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5610], device='cuda:0')), ('power', tensor([-20.0038], device='cuda:0'))])
epoch£º1130	 i:0 	 global-step:22600	 l-p:0.17400683462619781
epoch£º1130	 i:1 	 global-step:22601	 l-p:0.1424546241760254
epoch£º1130	 i:2 	 global-step:22602	 l-p:0.11584798246622086
epoch£º1130	 i:3 	 global-step:22603	 l-p:0.11598700284957886
epoch£º1130	 i:4 	 global-step:22604	 l-p:0.16776275634765625
epoch£º1130	 i:5 	 global-step:22605	 l-p:0.18270757794380188
epoch£º1130	 i:6 	 global-step:22606	 l-p:0.11813986301422119
epoch£º1130	 i:7 	 global-step:22607	 l-p:0.09605389088392258
epoch£º1130	 i:8 	 global-step:22608	 l-p:0.12429305911064148
epoch£º1130	 i:9 	 global-step:22609	 l-p:0.14729228615760803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1673, 5.1673, 5.1673],
        [5.1673, 5.1240, 5.1580],
        [5.1673, 5.2274, 4.9452],
        [5.1673, 5.0722, 5.1287]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1131, step:0 
model_pd.l_p.mean(): 0.1626005470752716 
model_pd.l_d.mean(): -19.978103637695312 
model_pd.lagr.mean(): -19.815502166748047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4939], device='cuda:0')), ('power', tensor([-20.7010], device='cuda:0'))])
epoch£º1131	 i:0 	 global-step:22620	 l-p:0.1626005470752716
epoch£º1131	 i:1 	 global-step:22621	 l-p:0.14320524036884308
epoch£º1131	 i:2 	 global-step:22622	 l-p:0.13286389410495758
epoch£º1131	 i:3 	 global-step:22623	 l-p:0.10828662663698196
epoch£º1131	 i:4 	 global-step:22624	 l-p:0.12933307886123657
epoch£º1131	 i:5 	 global-step:22625	 l-p:0.19161225855350494
epoch£º1131	 i:6 	 global-step:22626	 l-p:0.1006220206618309
epoch£º1131	 i:7 	 global-step:22627	 l-p:0.0975237712264061
epoch£º1131	 i:8 	 global-step:22628	 l-p:0.19076640903949738
epoch£º1131	 i:9 	 global-step:22629	 l-p:0.12326456606388092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1132
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228]], device='cuda:0')
 pt:tensor([[5.1681, 5.4248, 5.2475],
        [5.1681, 4.9138, 4.8853],
        [5.1681, 4.9080, 4.6117],
        [5.1681, 4.8894, 4.8056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1132, step:0 
model_pd.l_p.mean(): 0.12956997752189636 
model_pd.l_d.mean(): -20.587961196899414 
model_pd.lagr.mean(): -20.458391189575195 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4434], device='cuda:0')), ('power', tensor([-21.2659], device='cuda:0'))])
epoch£º1132	 i:0 	 global-step:22640	 l-p:0.12956997752189636
epoch£º1132	 i:1 	 global-step:22641	 l-p:0.17556054890155792
epoch£º1132	 i:2 	 global-step:22642	 l-p:0.16031640768051147
epoch£º1132	 i:3 	 global-step:22643	 l-p:0.16948838531970978
epoch£º1132	 i:4 	 global-step:22644	 l-p:0.07086136192083359
epoch£º1132	 i:5 	 global-step:22645	 l-p:0.11546186357736588
epoch£º1132	 i:6 	 global-step:22646	 l-p:0.13084186613559723
epoch£º1132	 i:7 	 global-step:22647	 l-p:0.13834738731384277
epoch£º1132	 i:8 	 global-step:22648	 l-p:0.1146758645772934
epoch£º1132	 i:9 	 global-step:22649	 l-p:0.15800833702087402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1763, 5.1763, 5.1763],
        [5.1763, 4.9539, 4.6291],
        [5.1763, 4.9594, 4.9796],
        [5.1763, 5.1273, 5.1647]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1133, step:0 
model_pd.l_p.mean(): 0.0838862955570221 
model_pd.l_d.mean(): -20.02433204650879 
model_pd.lagr.mean(): -19.940444946289062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4944], device='cuda:0')), ('power', tensor([-20.7483], device='cuda:0'))])
epoch£º1133	 i:0 	 global-step:22660	 l-p:0.0838862955570221
epoch£º1133	 i:1 	 global-step:22661	 l-p:0.13278265297412872
epoch£º1133	 i:2 	 global-step:22662	 l-p:0.14477892220020294
epoch£º1133	 i:3 	 global-step:22663	 l-p:0.1245914027094841
epoch£º1133	 i:4 	 global-step:22664	 l-p:0.15234915912151337
epoch£º1133	 i:5 	 global-step:22665	 l-p:0.12662498652935028
epoch£º1133	 i:6 	 global-step:22666	 l-p:0.11231967806816101
epoch£º1133	 i:7 	 global-step:22667	 l-p:0.16971032321453094
epoch£º1133	 i:8 	 global-step:22668	 l-p:0.11604566127061844
epoch£º1133	 i:9 	 global-step:22669	 l-p:0.16570787131786346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1850, 4.9355, 4.9133],
        [5.1850, 4.9583, 4.9675],
        [5.1850, 5.0944, 5.1497],
        [5.1850, 5.0073, 5.0554]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1134, step:0 
model_pd.l_p.mean(): 0.16066955029964447 
model_pd.l_d.mean(): -20.307607650756836 
model_pd.lagr.mean(): -20.14693832397461 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4864], device='cuda:0')), ('power', tensor([-21.0264], device='cuda:0'))])
epoch£º1134	 i:0 	 global-step:22680	 l-p:0.16066955029964447
epoch£º1134	 i:1 	 global-step:22681	 l-p:0.1529310792684555
epoch£º1134	 i:2 	 global-step:22682	 l-p:0.13690203428268433
epoch£º1134	 i:3 	 global-step:22683	 l-p:0.10647653043270111
epoch£º1134	 i:4 	 global-step:22684	 l-p:0.17104090750217438
epoch£º1134	 i:5 	 global-step:22685	 l-p:0.14389841258525848
epoch£º1134	 i:6 	 global-step:22686	 l-p:0.1074155792593956
epoch£º1134	 i:7 	 global-step:22687	 l-p:0.1210196390748024
epoch£º1134	 i:8 	 global-step:22688	 l-p:0.12344197928905487
epoch£º1134	 i:9 	 global-step:22689	 l-p:0.09136644005775452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1849, 5.4627, 5.2976],
        [5.1849, 4.9076, 4.6428],
        [5.1849, 5.1335, 5.1722],
        [5.1849, 4.8918, 4.7259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1135, step:0 
model_pd.l_p.mean(): 0.1401844024658203 
model_pd.l_d.mean(): -20.239973068237305 
model_pd.lagr.mean(): -20.099788665771484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5021], device='cuda:0')), ('power', tensor([-20.9741], device='cuda:0'))])
epoch£º1135	 i:0 	 global-step:22700	 l-p:0.1401844024658203
epoch£º1135	 i:1 	 global-step:22701	 l-p:0.13126249611377716
epoch£º1135	 i:2 	 global-step:22702	 l-p:0.12674227356910706
epoch£º1135	 i:3 	 global-step:22703	 l-p:0.1211986318230629
epoch£º1135	 i:4 	 global-step:22704	 l-p:0.14344987273216248
epoch£º1135	 i:5 	 global-step:22705	 l-p:0.06585540622472763
epoch£º1135	 i:6 	 global-step:22706	 l-p:0.13149510324001312
epoch£º1135	 i:7 	 global-step:22707	 l-p:0.11359769850969315
epoch£º1135	 i:8 	 global-step:22708	 l-p:0.1790497601032257
epoch£º1135	 i:9 	 global-step:22709	 l-p:0.1470174342393875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1936, 5.1652, 5.1891],
        [5.1936, 5.1916, 5.1935],
        [5.1936, 5.0236, 5.0750],
        [5.1936, 4.9015, 4.7380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1136, step:0 
model_pd.l_p.mean(): 0.10734136402606964 
model_pd.l_d.mean(): -20.818927764892578 
model_pd.lagr.mean(): -20.711585998535156 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3842], device='cuda:0')), ('power', tensor([-21.4389], device='cuda:0'))])
epoch£º1136	 i:0 	 global-step:22720	 l-p:0.10734136402606964
epoch£º1136	 i:1 	 global-step:22721	 l-p:0.09734054654836655
epoch£º1136	 i:2 	 global-step:22722	 l-p:0.1502586007118225
epoch£º1136	 i:3 	 global-step:22723	 l-p:0.13533733785152435
epoch£º1136	 i:4 	 global-step:22724	 l-p:0.1286436915397644
epoch£º1136	 i:5 	 global-step:22725	 l-p:0.10097511857748032
epoch£º1136	 i:6 	 global-step:22726	 l-p:0.14205089211463928
epoch£º1136	 i:7 	 global-step:22727	 l-p:0.13173429667949677
epoch£º1136	 i:8 	 global-step:22728	 l-p:0.10960618406534195
epoch£º1136	 i:9 	 global-step:22729	 l-p:0.16925090551376343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1998, 5.1994, 5.1998],
        [5.1998, 5.1998, 5.1998],
        [5.1998, 5.1947, 5.1996],
        [5.1998, 4.9091, 4.7036]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1137, step:0 
model_pd.l_p.mean(): 0.09343697130680084 
model_pd.l_d.mean(): -20.307327270507812 
model_pd.lagr.mean(): -20.213890075683594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4669], device='cuda:0')), ('power', tensor([-21.0063], device='cuda:0'))])
epoch£º1137	 i:0 	 global-step:22740	 l-p:0.09343697130680084
epoch£º1137	 i:1 	 global-step:22741	 l-p:0.10439470410346985
epoch£º1137	 i:2 	 global-step:22742	 l-p:0.08753563463687897
epoch£º1137	 i:3 	 global-step:22743	 l-p:0.13681992888450623
epoch£º1137	 i:4 	 global-step:22744	 l-p:0.15468931198120117
epoch£º1137	 i:5 	 global-step:22745	 l-p:0.17271386086940765
epoch£º1137	 i:6 	 global-step:22746	 l-p:0.15076345205307007
epoch£º1137	 i:7 	 global-step:22747	 l-p:0.11629503965377808
epoch£º1137	 i:8 	 global-step:22748	 l-p:0.16818761825561523
epoch£º1137	 i:9 	 global-step:22749	 l-p:0.08796073496341705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1893, 5.1850, 5.1891],
        [5.1893, 5.1893, 5.1893],
        [5.1893, 5.1346, 5.1752],
        [5.1893, 5.1350, 4.8093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1138, step:0 
model_pd.l_p.mean(): 0.11312190443277359 
model_pd.l_d.mean(): -19.466819763183594 
model_pd.lagr.mean(): -19.35369873046875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4878], device='cuda:0')), ('power', tensor([-20.1779], device='cuda:0'))])
epoch£º1138	 i:0 	 global-step:22760	 l-p:0.11312190443277359
epoch£º1138	 i:1 	 global-step:22761	 l-p:0.1496977061033249
epoch£º1138	 i:2 	 global-step:22762	 l-p:0.14944235980510712
epoch£º1138	 i:3 	 global-step:22763	 l-p:0.12916377186775208
epoch£º1138	 i:4 	 global-step:22764	 l-p:0.18202155828475952
epoch£º1138	 i:5 	 global-step:22765	 l-p:0.08547293394804001
epoch£º1138	 i:6 	 global-step:22766	 l-p:0.11414913088083267
epoch£º1138	 i:7 	 global-step:22767	 l-p:0.17378778755664825
epoch£º1138	 i:8 	 global-step:22768	 l-p:0.13673782348632812
epoch£º1138	 i:9 	 global-step:22769	 l-p:0.10245294123888016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1740, 5.1740, 5.1741],
        [5.1740, 5.1603, 5.1727],
        [5.1740, 4.9156, 4.6174],
        [5.1740, 5.1579, 5.1723]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1139, step:0 
model_pd.l_p.mean(): 0.13389943540096283 
model_pd.l_d.mean(): -20.178442001342773 
model_pd.lagr.mean(): -20.04454231262207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4881], device='cuda:0')), ('power', tensor([-20.8976], device='cuda:0'))])
epoch£º1139	 i:0 	 global-step:22780	 l-p:0.13389943540096283
epoch£º1139	 i:1 	 global-step:22781	 l-p:0.1204567477107048
epoch£º1139	 i:2 	 global-step:22782	 l-p:0.14542748034000397
epoch£º1139	 i:3 	 global-step:22783	 l-p:0.13450005650520325
epoch£º1139	 i:4 	 global-step:22784	 l-p:0.1520126312971115
epoch£º1139	 i:5 	 global-step:22785	 l-p:0.16448600590229034
epoch£º1139	 i:6 	 global-step:22786	 l-p:0.26721352338790894
epoch£º1139	 i:7 	 global-step:22787	 l-p:0.09047491103410721
epoch£º1139	 i:8 	 global-step:22788	 l-p:0.1134987622499466
epoch£º1139	 i:9 	 global-step:22789	 l-p:0.08179923892021179
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1540, 5.1540, 5.1540],
        [5.1540, 5.1032, 5.1417],
        [5.1540, 5.1529, 5.1540],
        [5.1540, 5.1540, 5.1540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1140, step:0 
model_pd.l_p.mean(): 0.17060111463069916 
model_pd.l_d.mean(): -20.9510498046875 
model_pd.lagr.mean(): -20.78044891357422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3879], device='cuda:0')), ('power', tensor([-21.5762], device='cuda:0'))])
epoch£º1140	 i:0 	 global-step:22800	 l-p:0.17060111463069916
epoch£º1140	 i:1 	 global-step:22801	 l-p:0.1429140865802765
epoch£º1140	 i:2 	 global-step:22802	 l-p:0.06587176024913788
epoch£º1140	 i:3 	 global-step:22803	 l-p:0.22948771715164185
epoch£º1140	 i:4 	 global-step:22804	 l-p:0.1309204399585724
epoch£º1140	 i:5 	 global-step:22805	 l-p:0.14276838302612305
epoch£º1140	 i:6 	 global-step:22806	 l-p:0.14272305369377136
epoch£º1140	 i:7 	 global-step:22807	 l-p:0.13159580528736115
epoch£º1140	 i:8 	 global-step:22808	 l-p:0.19005416333675385
epoch£º1140	 i:9 	 global-step:22809	 l-p:0.12862809002399445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1453, 5.0138, 5.0747],
        [5.1453, 5.1446, 5.1453],
        [5.1453, 5.0295, 4.6886],
        [5.1453, 4.9156, 4.9250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1141, step:0 
model_pd.l_p.mean(): 0.12366176396608353 
model_pd.l_d.mean(): -19.08802604675293 
model_pd.lagr.mean(): -18.964365005493164 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6095], device='cuda:0')), ('power', tensor([-19.9193], device='cuda:0'))])
epoch£º1141	 i:0 	 global-step:22820	 l-p:0.12366176396608353
epoch£º1141	 i:1 	 global-step:22821	 l-p:0.21673686802387238
epoch£º1141	 i:2 	 global-step:22822	 l-p:0.14768007397651672
epoch£º1141	 i:3 	 global-step:22823	 l-p:0.11139731854200363
epoch£º1141	 i:4 	 global-step:22824	 l-p:0.1264515072107315
epoch£º1141	 i:5 	 global-step:22825	 l-p:0.18872496485710144
epoch£º1141	 i:6 	 global-step:22826	 l-p:0.2071673721075058
epoch£º1141	 i:7 	 global-step:22827	 l-p:0.09377144277095795
epoch£º1141	 i:8 	 global-step:22828	 l-p:0.143169566988945
epoch£º1141	 i:9 	 global-step:22829	 l-p:0.11562814563512802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1544, 5.1540, 5.1544],
        [5.1544, 5.5244, 5.4177],
        [5.1544, 5.1544, 5.1545],
        [5.1544, 5.0875, 4.7571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1142, step:0 
model_pd.l_p.mean(): 0.12657999992370605 
model_pd.l_d.mean(): -19.79383087158203 
model_pd.lagr.mean(): -19.667251586914062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4536], device='cuda:0')), ('power', tensor([-20.4735], device='cuda:0'))])
epoch£º1142	 i:0 	 global-step:22840	 l-p:0.12657999992370605
epoch£º1142	 i:1 	 global-step:22841	 l-p:0.2558722198009491
epoch£º1142	 i:2 	 global-step:22842	 l-p:0.13785099983215332
epoch£º1142	 i:3 	 global-step:22843	 l-p:0.1338246911764145
epoch£º1142	 i:4 	 global-step:22844	 l-p:0.15041613578796387
epoch£º1142	 i:5 	 global-step:22845	 l-p:0.13093343377113342
epoch£º1142	 i:6 	 global-step:22846	 l-p:0.13296572864055634
epoch£º1142	 i:7 	 global-step:22847	 l-p:0.13334596157073975
epoch£º1142	 i:8 	 global-step:22848	 l-p:0.11381813138723373
epoch£º1142	 i:9 	 global-step:22849	 l-p:0.1126985177397728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1605, 5.4261, 5.2539],
        [5.1605, 5.1266, 5.1544],
        [5.1605, 4.9636, 5.0019],
        [5.1605, 5.1593, 5.1605]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1143, step:0 
model_pd.l_p.mean(): 0.10595893859863281 
model_pd.l_d.mean(): -20.134458541870117 
model_pd.lagr.mean(): -20.028499603271484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4322], device='cuda:0')), ('power', tensor([-20.7960], device='cuda:0'))])
epoch£º1143	 i:0 	 global-step:22860	 l-p:0.10595893859863281
epoch£º1143	 i:1 	 global-step:22861	 l-p:0.1318477839231491
epoch£º1143	 i:2 	 global-step:22862	 l-p:0.18012966215610504
epoch£º1143	 i:3 	 global-step:22863	 l-p:0.14334701001644135
epoch£º1143	 i:4 	 global-step:22864	 l-p:0.12517587840557098
epoch£º1143	 i:5 	 global-step:22865	 l-p:0.13179141283035278
epoch£º1143	 i:6 	 global-step:22866	 l-p:0.10559264570474625
epoch£º1143	 i:7 	 global-step:22867	 l-p:0.13283558189868927
epoch£º1143	 i:8 	 global-step:22868	 l-p:0.2233285903930664
epoch£º1143	 i:9 	 global-step:22869	 l-p:0.14819853007793427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1570, 5.1569, 5.1570],
        [5.1570, 5.1570, 5.1570],
        [5.1570, 5.1568, 5.1570],
        [5.1570, 4.8599, 4.6884]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1144, step:0 
model_pd.l_p.mean(): 0.09837526082992554 
model_pd.l_d.mean(): -20.43570899963379 
model_pd.lagr.mean(): -20.33733367919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4597], device='cuda:0')), ('power', tensor([-21.1287], device='cuda:0'))])
epoch£º1144	 i:0 	 global-step:22880	 l-p:0.09837526082992554
epoch£º1144	 i:1 	 global-step:22881	 l-p:0.15449947118759155
epoch£º1144	 i:2 	 global-step:22882	 l-p:0.11511896550655365
epoch£º1144	 i:3 	 global-step:22883	 l-p:0.15457355976104736
epoch£º1144	 i:4 	 global-step:22884	 l-p:0.10976359993219376
epoch£º1144	 i:5 	 global-step:22885	 l-p:0.09518188238143921
epoch£º1144	 i:6 	 global-step:22886	 l-p:0.18343526124954224
epoch£º1144	 i:7 	 global-step:22887	 l-p:0.12330657988786697
epoch£º1144	 i:8 	 global-step:22888	 l-p:0.2566196024417877
epoch£º1144	 i:9 	 global-step:22889	 l-p:0.13684014976024628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1580, 5.0678, 5.1232],
        [5.1580, 5.0158, 5.0757],
        [5.1580, 5.1516, 5.1576],
        [5.1580, 5.1470, 5.1571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1145, step:0 
model_pd.l_p.mean(): 0.11905818432569504 
model_pd.l_d.mean(): -19.94693374633789 
model_pd.lagr.mean(): -19.8278751373291 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5150], device='cuda:0')), ('power', tensor([-20.6911], device='cuda:0'))])
epoch£º1145	 i:0 	 global-step:22900	 l-p:0.11905818432569504
epoch£º1145	 i:1 	 global-step:22901	 l-p:0.13076038658618927
epoch£º1145	 i:2 	 global-step:22902	 l-p:0.1215779110789299
epoch£º1145	 i:3 	 global-step:22903	 l-p:0.17351394891738892
epoch£º1145	 i:4 	 global-step:22904	 l-p:0.13427528738975525
epoch£º1145	 i:5 	 global-step:22905	 l-p:0.14593911170959473
epoch£º1145	 i:6 	 global-step:22906	 l-p:0.06909798085689545
epoch£º1145	 i:7 	 global-step:22907	 l-p:0.1798512041568756
epoch£º1145	 i:8 	 global-step:22908	 l-p:0.15784655511379242
epoch£º1145	 i:9 	 global-step:22909	 l-p:0.19128665328025818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1600, 5.1600, 5.1600],
        [5.1600, 5.5557, 5.4658],
        [5.1600, 4.8638, 4.7028],
        [5.1600, 4.8655, 4.7206]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1146, step:0 
model_pd.l_p.mean(): 0.1809803992509842 
model_pd.l_d.mean(): -19.880779266357422 
model_pd.lagr.mean(): -19.699798583984375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5509], device='cuda:0')), ('power', tensor([-20.6608], device='cuda:0'))])
epoch£º1146	 i:0 	 global-step:22920	 l-p:0.1809803992509842
epoch£º1146	 i:1 	 global-step:22921	 l-p:0.15853507816791534
epoch£º1146	 i:2 	 global-step:22922	 l-p:0.10849818587303162
epoch£º1146	 i:3 	 global-step:22923	 l-p:0.11020497232675552
epoch£º1146	 i:4 	 global-step:22924	 l-p:0.14965440332889557
epoch£º1146	 i:5 	 global-step:22925	 l-p:0.11067470908164978
epoch£º1146	 i:6 	 global-step:22926	 l-p:0.17211787402629852
epoch£º1146	 i:7 	 global-step:22927	 l-p:0.1477091908454895
epoch£º1146	 i:8 	 global-step:22928	 l-p:0.12808750569820404
epoch£º1146	 i:9 	 global-step:22929	 l-p:0.1327897012233734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1647, 5.2897, 5.0384],
        [5.1647, 5.1642, 5.1647],
        [5.1647, 5.1636, 5.1647],
        [5.1647, 4.9773, 4.6381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1147, step:0 
model_pd.l_p.mean(): 0.08852437138557434 
model_pd.l_d.mean(): -20.252580642700195 
model_pd.lagr.mean(): -20.1640567779541 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4993], device='cuda:0')), ('power', tensor([-20.9840], device='cuda:0'))])
epoch£º1147	 i:0 	 global-step:22940	 l-p:0.08852437138557434
epoch£º1147	 i:1 	 global-step:22941	 l-p:0.15107028186321259
epoch£º1147	 i:2 	 global-step:22942	 l-p:0.13354839384555817
epoch£º1147	 i:3 	 global-step:22943	 l-p:0.14989805221557617
epoch£º1147	 i:4 	 global-step:22944	 l-p:0.13046406209468842
epoch£º1147	 i:5 	 global-step:22945	 l-p:0.15239354968070984
epoch£º1147	 i:6 	 global-step:22946	 l-p:0.14373768866062164
epoch£º1147	 i:7 	 global-step:22947	 l-p:0.13651655614376068
epoch£º1147	 i:8 	 global-step:22948	 l-p:0.12686476111412048
epoch£º1147	 i:9 	 global-step:22949	 l-p:0.18563492596149445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1611, 5.4138, 5.2337],
        [5.1611, 5.0791, 5.1319],
        [5.1611, 5.0444, 5.1047],
        [5.1611, 5.1567, 5.1609]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1148, step:0 
model_pd.l_p.mean(): 0.11973313987255096 
model_pd.l_d.mean(): -19.761594772338867 
model_pd.lagr.mean(): -19.641860961914062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5190], device='cuda:0')), ('power', tensor([-20.5077], device='cuda:0'))])
epoch£º1148	 i:0 	 global-step:22960	 l-p:0.11973313987255096
epoch£º1148	 i:1 	 global-step:22961	 l-p:0.1630549430847168
epoch£º1148	 i:2 	 global-step:22962	 l-p:0.1540381908416748
epoch£º1148	 i:3 	 global-step:22963	 l-p:0.15186433494091034
epoch£º1148	 i:4 	 global-step:22964	 l-p:0.14004209637641907
epoch£º1148	 i:5 	 global-step:22965	 l-p:0.14232951402664185
epoch£º1148	 i:6 	 global-step:22966	 l-p:0.13090133666992188
epoch£º1148	 i:7 	 global-step:22967	 l-p:0.11670216917991638
epoch£º1148	 i:8 	 global-step:22968	 l-p:0.13166223466396332
epoch£º1148	 i:9 	 global-step:22969	 l-p:0.18026156723499298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1546, 5.0011, 4.6576],
        [5.1546, 4.8963, 4.5924],
        [5.1546, 5.0198, 5.0805],
        [5.1546, 5.0371, 5.0976]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1149, step:0 
model_pd.l_p.mean(): 0.18205487728118896 
model_pd.l_d.mean(): -19.878211975097656 
model_pd.lagr.mean(): -19.696157455444336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5278], device='cuda:0')), ('power', tensor([-20.6347], device='cuda:0'))])
epoch£º1149	 i:0 	 global-step:22980	 l-p:0.18205487728118896
epoch£º1149	 i:1 	 global-step:22981	 l-p:0.07380553334951401
epoch£º1149	 i:2 	 global-step:22982	 l-p:0.1892520636320114
epoch£º1149	 i:3 	 global-step:22983	 l-p:0.1549805998802185
epoch£º1149	 i:4 	 global-step:22984	 l-p:0.10008237510919571
epoch£º1149	 i:5 	 global-step:22985	 l-p:0.12316522002220154
epoch£º1149	 i:6 	 global-step:22986	 l-p:0.1385609358549118
epoch£º1149	 i:7 	 global-step:22987	 l-p:0.17533066868782043
epoch£º1149	 i:8 	 global-step:22988	 l-p:0.19330796599388123
epoch£º1149	 i:9 	 global-step:22989	 l-p:0.08230800181627274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1656, 5.1656, 5.1656],
        [5.1656, 5.1652, 5.1656],
        [5.1656, 5.1653, 5.1656],
        [5.1656, 5.1651, 5.1656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1150, step:0 
model_pd.l_p.mean(): 0.11513013392686844 
model_pd.l_d.mean(): -20.519329071044922 
model_pd.lagr.mean(): -20.404199600219727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4183], device='cuda:0')), ('power', tensor([-21.1708], device='cuda:0'))])
epoch£º1150	 i:0 	 global-step:23000	 l-p:0.11513013392686844
epoch£º1150	 i:1 	 global-step:23001	 l-p:0.11737331002950668
epoch£º1150	 i:2 	 global-step:23002	 l-p:0.09162195026874542
epoch£º1150	 i:3 	 global-step:23003	 l-p:0.14113804697990417
epoch£º1150	 i:4 	 global-step:23004	 l-p:0.22105824947357178
epoch£º1150	 i:5 	 global-step:23005	 l-p:0.1458355039358139
epoch£º1150	 i:6 	 global-step:23006	 l-p:0.1067059338092804
epoch£º1150	 i:7 	 global-step:23007	 l-p:0.15425896644592285
epoch£º1150	 i:8 	 global-step:23008	 l-p:0.15615561604499817
epoch£º1150	 i:9 	 global-step:23009	 l-p:0.1460675448179245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1679, 5.1679, 5.1679],
        [5.1679, 5.1678, 5.1679],
        [5.1679, 5.0781, 5.1334],
        [5.1679, 4.8992, 4.8444]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1151, step:0 
model_pd.l_p.mean(): 0.18727615475654602 
model_pd.l_d.mean(): -20.68337631225586 
model_pd.lagr.mean(): -20.4960994720459 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4268], device='cuda:0')), ('power', tensor([-21.3454], device='cuda:0'))])
epoch£º1151	 i:0 	 global-step:23020	 l-p:0.18727615475654602
epoch£º1151	 i:1 	 global-step:23021	 l-p:0.10343904048204422
epoch£º1151	 i:2 	 global-step:23022	 l-p:0.12589393556118011
epoch£º1151	 i:3 	 global-step:23023	 l-p:0.14151698350906372
epoch£º1151	 i:4 	 global-step:23024	 l-p:0.12125799804925919
epoch£º1151	 i:5 	 global-step:23025	 l-p:0.15537890791893005
epoch£º1151	 i:6 	 global-step:23026	 l-p:0.10611667484045029
epoch£º1151	 i:7 	 global-step:23027	 l-p:0.1297263652086258
epoch£º1151	 i:8 	 global-step:23028	 l-p:0.12919078767299652
epoch£º1151	 i:9 	 global-step:23029	 l-p:0.1565726399421692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1152
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1820, 5.1813, 5.1820],
        [5.1820, 4.8869, 4.7079],
        [5.1820, 5.1820, 5.1820],
        [5.1820, 4.9903, 5.0314]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1152, step:0 
model_pd.l_p.mean(): 0.14370355010032654 
model_pd.l_d.mean(): -20.900861740112305 
model_pd.lagr.mean(): -20.757158279418945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3764], device='cuda:0')), ('power', tensor([-21.5137], device='cuda:0'))])
epoch£º1152	 i:0 	 global-step:23040	 l-p:0.14370355010032654
epoch£º1152	 i:1 	 global-step:23041	 l-p:0.09887462109327316
epoch£º1152	 i:2 	 global-step:23042	 l-p:0.13131703436374664
epoch£º1152	 i:3 	 global-step:23043	 l-p:0.054588187485933304
epoch£º1152	 i:4 	 global-step:23044	 l-p:0.16444458067417145
epoch£º1152	 i:5 	 global-step:23045	 l-p:0.13510309159755707
epoch£º1152	 i:6 	 global-step:23046	 l-p:0.11397243291139603
epoch£º1152	 i:7 	 global-step:23047	 l-p:0.14319244027137756
epoch£º1152	 i:8 	 global-step:23048	 l-p:0.17135053873062134
epoch£º1152	 i:9 	 global-step:23049	 l-p:0.17010517418384552
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1848, 5.1837, 5.1848],
        [5.1848, 5.1320, 5.1715],
        [5.1848, 5.0616, 5.1220],
        [5.1848, 5.1706, 5.1834]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1153, step:0 
model_pd.l_p.mean(): 0.16625311970710754 
model_pd.l_d.mean(): -20.469579696655273 
model_pd.lagr.mean(): -20.303325653076172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4649], device='cuda:0')), ('power', tensor([-21.1682], device='cuda:0'))])
epoch£º1153	 i:0 	 global-step:23060	 l-p:0.16625311970710754
epoch£º1153	 i:1 	 global-step:23061	 l-p:0.15153689682483673
epoch£º1153	 i:2 	 global-step:23062	 l-p:0.12034539878368378
epoch£º1153	 i:3 	 global-step:23063	 l-p:0.11931362748146057
epoch£º1153	 i:4 	 global-step:23064	 l-p:0.11997253447771072
epoch£º1153	 i:5 	 global-step:23065	 l-p:0.08208904415369034
epoch£º1153	 i:6 	 global-step:23066	 l-p:0.12837056815624237
epoch£º1153	 i:7 	 global-step:23067	 l-p:0.1084444522857666
epoch£º1153	 i:8 	 global-step:23068	 l-p:0.16068942844867706
epoch£º1153	 i:9 	 global-step:23069	 l-p:0.17547637224197388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1739, 5.1341, 5.1658],
        [5.1739, 5.0454, 5.1060],
        [5.1739, 5.1739, 5.1739],
        [5.1739, 5.1400, 5.1678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1154, step:0 
model_pd.l_p.mean(): 0.13798080384731293 
model_pd.l_d.mean(): -20.679729461669922 
model_pd.lagr.mean(): -20.541748046875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4379], device='cuda:0')), ('power', tensor([-21.3531], device='cuda:0'))])
epoch£º1154	 i:0 	 global-step:23080	 l-p:0.13798080384731293
epoch£º1154	 i:1 	 global-step:23081	 l-p:0.1597781479358673
epoch£º1154	 i:2 	 global-step:23082	 l-p:0.1218922808766365
epoch£º1154	 i:3 	 global-step:23083	 l-p:0.1413823962211609
epoch£º1154	 i:4 	 global-step:23084	 l-p:0.17387191951274872
epoch£º1154	 i:5 	 global-step:23085	 l-p:0.13744619488716125
epoch£º1154	 i:6 	 global-step:23086	 l-p:0.06562208384275436
epoch£º1154	 i:7 	 global-step:23087	 l-p:0.1603551059961319
epoch£º1154	 i:8 	 global-step:23088	 l-p:0.13650363683700562
epoch£º1154	 i:9 	 global-step:23089	 l-p:0.13127142190933228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1731, 5.1683, 5.1729],
        [5.1731, 5.1731, 5.1731],
        [5.1731, 5.1731, 5.1731],
        [5.1731, 5.4827, 5.3367]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1155, step:0 
model_pd.l_p.mean(): 0.0950385108590126 
model_pd.l_d.mean(): -19.34296989440918 
model_pd.lagr.mean(): -19.2479305267334 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5409], device='cuda:0')), ('power', tensor([-20.1069], device='cuda:0'))])
epoch£º1155	 i:0 	 global-step:23100	 l-p:0.0950385108590126
epoch£º1155	 i:1 	 global-step:23101	 l-p:0.12468257546424866
epoch£º1155	 i:2 	 global-step:23102	 l-p:0.10234848409891129
epoch£º1155	 i:3 	 global-step:23103	 l-p:0.2175755798816681
epoch£º1155	 i:4 	 global-step:23104	 l-p:0.17714296281337738
epoch£º1155	 i:5 	 global-step:23105	 l-p:0.14945051074028015
epoch£º1155	 i:6 	 global-step:23106	 l-p:0.15157338976860046
epoch£º1155	 i:7 	 global-step:23107	 l-p:0.11308763176202774
epoch£º1155	 i:8 	 global-step:23108	 l-p:0.11367104947566986
epoch£º1155	 i:9 	 global-step:23109	 l-p:0.12783141434192657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1692, 5.1239, 5.1591],
        [5.1692, 5.1692, 5.1692],
        [5.1692, 5.1294, 5.1612],
        [5.1692, 4.8943, 4.6156]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1156, step:0 
model_pd.l_p.mean(): 0.16079112887382507 
model_pd.l_d.mean(): -21.00736427307129 
model_pd.lagr.mean(): -20.846572875976562 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3803], device='cuda:0')), ('power', tensor([-21.6254], device='cuda:0'))])
epoch£º1156	 i:0 	 global-step:23120	 l-p:0.16079112887382507
epoch£º1156	 i:1 	 global-step:23121	 l-p:0.13307969272136688
epoch£º1156	 i:2 	 global-step:23122	 l-p:0.10970776528120041
epoch£º1156	 i:3 	 global-step:23123	 l-p:0.1961868852376938
epoch£º1156	 i:4 	 global-step:23124	 l-p:0.17743945121765137
epoch£º1156	 i:5 	 global-step:23125	 l-p:0.14156180620193481
epoch£º1156	 i:6 	 global-step:23126	 l-p:0.15714658796787262
epoch£º1156	 i:7 	 global-step:23127	 l-p:0.11668605357408524
epoch£º1156	 i:8 	 global-step:23128	 l-p:0.09726651012897491
epoch£º1156	 i:9 	 global-step:23129	 l-p:0.12480505555868149
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1559, 5.1559, 5.1559],
        [5.1559, 4.8964, 4.8628],
        [5.1559, 5.1529, 5.1558],
        [5.1559, 5.0546, 5.1129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1157, step:0 
model_pd.l_p.mean(): 0.130065456032753 
model_pd.l_d.mean(): -20.230266571044922 
model_pd.lagr.mean(): -20.100200653076172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4977], device='cuda:0')), ('power', tensor([-20.9598], device='cuda:0'))])
epoch£º1157	 i:0 	 global-step:23140	 l-p:0.130065456032753
epoch£º1157	 i:1 	 global-step:23141	 l-p:0.18122448027133942
epoch£º1157	 i:2 	 global-step:23142	 l-p:0.14351722598075867
epoch£º1157	 i:3 	 global-step:23143	 l-p:0.17440518736839294
epoch£º1157	 i:4 	 global-step:23144	 l-p:0.12994283437728882
epoch£º1157	 i:5 	 global-step:23145	 l-p:0.1287965178489685
epoch£º1157	 i:6 	 global-step:23146	 l-p:0.19483157992362976
epoch£º1157	 i:7 	 global-step:23147	 l-p:0.08824096620082855
epoch£º1157	 i:8 	 global-step:23148	 l-p:0.12394914031028748
epoch£º1157	 i:9 	 global-step:23149	 l-p:0.11443435400724411
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1695, 5.0527, 5.1131],
        [5.1695, 5.1694, 5.1695],
        [5.1695, 5.1695, 5.1695],
        [5.1695, 4.8979, 4.8371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1158, step:0 
model_pd.l_p.mean(): 0.1534264087677002 
model_pd.l_d.mean(): -20.058624267578125 
model_pd.lagr.mean(): -19.905197143554688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4977], device='cuda:0')), ('power', tensor([-20.7863], device='cuda:0'))])
epoch£º1158	 i:0 	 global-step:23160	 l-p:0.1534264087677002
epoch£º1158	 i:1 	 global-step:23161	 l-p:0.12315845489501953
epoch£º1158	 i:2 	 global-step:23162	 l-p:0.14104434847831726
epoch£º1158	 i:3 	 global-step:23163	 l-p:0.1549992561340332
epoch£º1158	 i:4 	 global-step:23164	 l-p:0.11871646344661713
epoch£º1158	 i:5 	 global-step:23165	 l-p:0.13104745745658875
epoch£º1158	 i:6 	 global-step:23166	 l-p:0.13171686232089996
epoch£º1158	 i:7 	 global-step:23167	 l-p:0.1918707937002182
epoch£º1158	 i:8 	 global-step:23168	 l-p:0.11342281848192215
epoch£º1158	 i:9 	 global-step:23169	 l-p:0.13202546536922455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1581, 4.8617, 4.6454],
        [5.1581, 5.1580, 5.1581],
        [5.1581, 5.1577, 5.1581],
        [5.1581, 5.1009, 5.1429]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1159, step:0 
model_pd.l_p.mean(): 0.19523616135120392 
model_pd.l_d.mean(): -20.553359985351562 
model_pd.lagr.mean(): -20.358123779296875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4469], device='cuda:0')), ('power', tensor([-21.2345], device='cuda:0'))])
epoch£º1159	 i:0 	 global-step:23180	 l-p:0.19523616135120392
epoch£º1159	 i:1 	 global-step:23181	 l-p:0.1513291448354721
epoch£º1159	 i:2 	 global-step:23182	 l-p:0.1208086833357811
epoch£º1159	 i:3 	 global-step:23183	 l-p:0.1255866438150406
epoch£º1159	 i:4 	 global-step:23184	 l-p:0.13273507356643677
epoch£º1159	 i:5 	 global-step:23185	 l-p:0.10987557470798492
epoch£º1159	 i:6 	 global-step:23186	 l-p:0.14772339165210724
epoch£º1159	 i:7 	 global-step:23187	 l-p:0.14757652580738068
epoch£º1159	 i:8 	 global-step:23188	 l-p:0.14276310801506042
epoch£º1159	 i:9 	 global-step:23189	 l-p:0.16360700130462646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1544, 4.8790, 4.5959],
        [5.1544, 5.1488, 5.1541],
        [5.1544, 4.9136, 4.5932],
        [5.1544, 5.1543, 5.1544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1160, step:0 
model_pd.l_p.mean(): 0.12987744808197021 
model_pd.l_d.mean(): -20.11284065246582 
model_pd.lagr.mean(): -19.98296356201172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4551], device='cuda:0')), ('power', tensor([-20.7975], device='cuda:0'))])
epoch£º1160	 i:0 	 global-step:23200	 l-p:0.12987744808197021
epoch£º1160	 i:1 	 global-step:23201	 l-p:0.1763738989830017
epoch£º1160	 i:2 	 global-step:23202	 l-p:0.15387262403964996
epoch£º1160	 i:3 	 global-step:23203	 l-p:0.12488125264644623
epoch£º1160	 i:4 	 global-step:23204	 l-p:0.16224221885204315
epoch£º1160	 i:5 	 global-step:23205	 l-p:0.09592171758413315
epoch£º1160	 i:6 	 global-step:23206	 l-p:0.1305638998746872
epoch£º1160	 i:7 	 global-step:23207	 l-p:0.10733745247125626
epoch£º1160	 i:8 	 global-step:23208	 l-p:0.24951624870300293
epoch£º1160	 i:9 	 global-step:23209	 l-p:0.13952897489070892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1489, 4.9508, 4.6110],
        [5.1489, 5.1489, 5.1489],
        [5.1489, 5.0470, 5.1054],
        [5.1489, 4.9257, 4.9433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1161, step:0 
model_pd.l_p.mean(): 0.08759074658155441 
model_pd.l_d.mean(): -18.63136100769043 
model_pd.lagr.mean(): -18.54376983642578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6383], device='cuda:0')), ('power', tensor([-19.4871], device='cuda:0'))])
epoch£º1161	 i:0 	 global-step:23220	 l-p:0.08759074658155441
epoch£º1161	 i:1 	 global-step:23221	 l-p:0.13691110908985138
epoch£º1161	 i:2 	 global-step:23222	 l-p:0.11309392005205154
epoch£º1161	 i:3 	 global-step:23223	 l-p:0.13597436249256134
epoch£º1161	 i:4 	 global-step:23224	 l-p:0.10105876624584198
epoch£º1161	 i:5 	 global-step:23225	 l-p:0.2208140641450882
epoch£º1161	 i:6 	 global-step:23226	 l-p:0.19001789391040802
epoch£º1161	 i:7 	 global-step:23227	 l-p:0.19422854483127594
epoch£º1161	 i:8 	 global-step:23228	 l-p:0.11905375868082047
epoch£º1161	 i:9 	 global-step:23229	 l-p:0.14899148046970367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1162
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228]], device='cuda:0')
 pt:tensor([[5.1612, 5.0974, 4.7671],
        [5.1612, 5.0205, 5.0807],
        [5.1612, 4.9046, 4.8760],
        [5.1612, 4.8945, 4.5998]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1162, step:0 
model_pd.l_p.mean(): 0.22075636684894562 
model_pd.l_d.mean(): -20.218473434448242 
model_pd.lagr.mean(): -19.997716903686523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5200], device='cuda:0')), ('power', tensor([-20.9707], device='cuda:0'))])
epoch£º1162	 i:0 	 global-step:23240	 l-p:0.22075636684894562
epoch£º1162	 i:1 	 global-step:23241	 l-p:0.09517550468444824
epoch£º1162	 i:2 	 global-step:23242	 l-p:0.12466568499803543
epoch£º1162	 i:3 	 global-step:23243	 l-p:0.15141561627388
epoch£º1162	 i:4 	 global-step:23244	 l-p:0.1219434142112732
epoch£º1162	 i:5 	 global-step:23245	 l-p:0.08279882371425629
epoch£º1162	 i:6 	 global-step:23246	 l-p:0.1933850795030594
epoch£º1162	 i:7 	 global-step:23247	 l-p:0.11589166522026062
epoch£º1162	 i:8 	 global-step:23248	 l-p:0.1387120485305786
epoch£º1162	 i:9 	 global-step:23249	 l-p:0.14322172105312347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1773, 5.0327, 4.6899],
        [5.1773, 5.1773, 5.1773],
        [5.1773, 5.1773, 5.1773],
        [5.1773, 5.5038, 5.3680]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1163, step:0 
model_pd.l_p.mean(): 0.11722058802843094 
model_pd.l_d.mean(): -19.8554630279541 
model_pd.lagr.mean(): -19.738243103027344 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4877], device='cuda:0')), ('power', tensor([-20.5707], device='cuda:0'))])
epoch£º1163	 i:0 	 global-step:23260	 l-p:0.11722058802843094
epoch£º1163	 i:1 	 global-step:23261	 l-p:0.10214449465274811
epoch£º1163	 i:2 	 global-step:23262	 l-p:0.1935635209083557
epoch£º1163	 i:3 	 global-step:23263	 l-p:0.11217683553695679
epoch£º1163	 i:4 	 global-step:23264	 l-p:0.16115841269493103
epoch£º1163	 i:5 	 global-step:23265	 l-p:0.14524589478969574
epoch£º1163	 i:6 	 global-step:23266	 l-p:0.09765390306711197
epoch£º1163	 i:7 	 global-step:23267	 l-p:0.08442670851945877
epoch£º1163	 i:8 	 global-step:23268	 l-p:0.1607745736837387
epoch£º1163	 i:9 	 global-step:23269	 l-p:0.16069960594177246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1164
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1270,  0.0638,  1.0000,  0.0321,
          1.0000,  0.5026, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1497,  0.0795,  1.0000,  0.0422,
          1.0000,  0.5310, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6301, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]], device='cuda:0')
 pt:tensor([[5.1811, 5.0468, 5.1073],
        [5.1811, 5.0161, 5.0705],
        [5.1811, 4.9151, 4.8658],
        [5.1811, 5.5572, 5.4532]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1164, step:0 
model_pd.l_p.mean(): 0.15725116431713104 
model_pd.l_d.mean(): -20.091838836669922 
model_pd.lagr.mean(): -19.934587478637695 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4992], device='cuda:0')), ('power', tensor([-20.8214], device='cuda:0'))])
epoch£º1164	 i:0 	 global-step:23280	 l-p:0.15725116431713104
epoch£º1164	 i:1 	 global-step:23281	 l-p:0.10591978579759598
epoch£º1164	 i:2 	 global-step:23282	 l-p:0.19286806881427765
epoch£º1164	 i:3 	 global-step:23283	 l-p:0.0473438985645771
epoch£º1164	 i:4 	 global-step:23284	 l-p:0.13195565342903137
epoch£º1164	 i:5 	 global-step:23285	 l-p:0.15745504200458527
epoch£º1164	 i:6 	 global-step:23286	 l-p:0.1311829686164856
epoch£º1164	 i:7 	 global-step:23287	 l-p:0.14329054951667786
epoch£º1164	 i:8 	 global-step:23288	 l-p:0.1500067412853241
epoch£º1164	 i:9 	 global-step:23289	 l-p:0.11545317620038986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1803, 4.9317, 4.6211],
        [5.1803, 5.0526, 4.7107],
        [5.1803, 4.9585, 4.9755],
        [5.1803, 5.1803, 5.1803]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1165, step:0 
model_pd.l_p.mean(): 0.11747514456510544 
model_pd.l_d.mean(): -19.959440231323242 
model_pd.lagr.mean(): -19.841964721679688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5162], device='cuda:0')), ('power', tensor([-20.7049], device='cuda:0'))])
epoch£º1165	 i:0 	 global-step:23300	 l-p:0.11747514456510544
epoch£º1165	 i:1 	 global-step:23301	 l-p:0.07991804927587509
epoch£º1165	 i:2 	 global-step:23302	 l-p:0.15245859324932098
epoch£º1165	 i:3 	 global-step:23303	 l-p:0.17584629356861115
epoch£º1165	 i:4 	 global-step:23304	 l-p:0.10584361106157303
epoch£º1165	 i:5 	 global-step:23305	 l-p:0.10649888962507248
epoch£º1165	 i:6 	 global-step:23306	 l-p:0.09149906039237976
epoch£º1165	 i:7 	 global-step:23307	 l-p:0.22320930659770966
epoch£º1165	 i:8 	 global-step:23308	 l-p:0.19903205335140228
epoch£º1165	 i:9 	 global-step:23309	 l-p:0.11469337344169617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1687, 5.1687, 5.1687],
        [5.1687, 5.0262, 5.0862],
        [5.1687, 5.1542, 5.1673],
        [5.1687, 5.0340, 5.0947]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1166, step:0 
model_pd.l_p.mean(): 0.18336521089076996 
model_pd.l_d.mean(): -20.525665283203125 
model_pd.lagr.mean(): -20.342300415039062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4520], device='cuda:0')), ('power', tensor([-21.2118], device='cuda:0'))])
epoch£º1166	 i:0 	 global-step:23320	 l-p:0.18336521089076996
epoch£º1166	 i:1 	 global-step:23321	 l-p:0.1572914719581604
epoch£º1166	 i:2 	 global-step:23322	 l-p:0.15226316452026367
epoch£º1166	 i:3 	 global-step:23323	 l-p:0.10873827338218689
epoch£º1166	 i:4 	 global-step:23324	 l-p:0.1427178680896759
epoch£º1166	 i:5 	 global-step:23325	 l-p:0.10806253552436829
epoch£º1166	 i:6 	 global-step:23326	 l-p:0.12302611768245697
epoch£º1166	 i:7 	 global-step:23327	 l-p:0.1191558986902237
epoch£º1166	 i:8 	 global-step:23328	 l-p:0.1466718167066574
epoch£º1166	 i:9 	 global-step:23329	 l-p:0.14180988073349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1628, 5.0571, 5.1162],
        [5.1628, 5.1405, 5.1598],
        [5.1628, 4.8953, 4.8463],
        [5.1628, 4.9385, 4.9542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1167, step:0 
model_pd.l_p.mean(): 0.1446966528892517 
model_pd.l_d.mean(): -20.45947265625 
model_pd.lagr.mean(): -20.314775466918945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4973], device='cuda:0')), ('power', tensor([-21.1911], device='cuda:0'))])
epoch£º1167	 i:0 	 global-step:23340	 l-p:0.1446966528892517
epoch£º1167	 i:1 	 global-step:23341	 l-p:0.08319833874702454
epoch£º1167	 i:2 	 global-step:23342	 l-p:0.18379805982112885
epoch£º1167	 i:3 	 global-step:23343	 l-p:0.20843113958835602
epoch£º1167	 i:4 	 global-step:23344	 l-p:0.08745665103197098
epoch£º1167	 i:5 	 global-step:23345	 l-p:0.1272566318511963
epoch£º1167	 i:6 	 global-step:23346	 l-p:0.15638847649097443
epoch£º1167	 i:7 	 global-step:23347	 l-p:0.13150189816951752
epoch£º1167	 i:8 	 global-step:23348	 l-p:0.12694568932056427
epoch£º1167	 i:9 	 global-step:23349	 l-p:0.18719886243343353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1515, 5.1469, 5.1512],
        [5.1515, 5.1631, 4.8584],
        [5.1515, 5.1454, 5.1511],
        [5.1515, 5.5820, 5.5146]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1168, step:0 
model_pd.l_p.mean(): 0.17724411189556122 
model_pd.l_d.mean(): -17.40449333190918 
model_pd.lagr.mean(): -17.227249145507812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6637], device='cuda:0')), ('power', tensor([-18.2728], device='cuda:0'))])
epoch£º1168	 i:0 	 global-step:23360	 l-p:0.17724411189556122
epoch£º1168	 i:1 	 global-step:23361	 l-p:0.11927686631679535
epoch£º1168	 i:2 	 global-step:23362	 l-p:0.13169364631175995
epoch£º1168	 i:3 	 global-step:23363	 l-p:0.2739368677139282
epoch£º1168	 i:4 	 global-step:23364	 l-p:0.12665441632270813
epoch£º1168	 i:5 	 global-step:23365	 l-p:0.15890443325042725
epoch£º1168	 i:6 	 global-step:23366	 l-p:0.11662782728672028
epoch£º1168	 i:7 	 global-step:23367	 l-p:0.128142312169075
epoch£º1168	 i:8 	 global-step:23368	 l-p:0.130604550242424
epoch£º1168	 i:9 	 global-step:23369	 l-p:0.11242037266492844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1532, 5.0010, 4.6557],
        [5.1532, 5.1509, 5.1532],
        [5.1532, 5.0828, 4.7501],
        [5.1532, 5.0473, 5.1065]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1169, step:0 
model_pd.l_p.mean(): 0.1513359248638153 
model_pd.l_d.mean(): -20.66635513305664 
model_pd.lagr.mean(): -20.515018463134766 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4270], device='cuda:0')), ('power', tensor([-21.3285], device='cuda:0'))])
epoch£º1169	 i:0 	 global-step:23380	 l-p:0.1513359248638153
epoch£º1169	 i:1 	 global-step:23381	 l-p:0.14936573803424835
epoch£º1169	 i:2 	 global-step:23382	 l-p:0.14134593307971954
epoch£º1169	 i:3 	 global-step:23383	 l-p:0.2031019926071167
epoch£º1169	 i:4 	 global-step:23384	 l-p:0.13502304255962372
epoch£º1169	 i:5 	 global-step:23385	 l-p:0.11658084392547607
epoch£º1169	 i:6 	 global-step:23386	 l-p:0.10172718018293381
epoch£º1169	 i:7 	 global-step:23387	 l-p:0.16668708622455597
epoch£º1169	 i:8 	 global-step:23388	 l-p:0.17200905084609985
epoch£º1169	 i:9 	 global-step:23389	 l-p:0.11941992491483688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1582, 5.1582, 5.1582],
        [5.1582, 4.8829, 4.8175],
        [5.1582, 4.9166, 4.9122],
        [5.1582, 4.9054, 4.5936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1170, step:0 
model_pd.l_p.mean(): 0.1432739645242691 
model_pd.l_d.mean(): -20.062448501586914 
model_pd.lagr.mean(): -19.919174194335938 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5120], device='cuda:0')), ('power', tensor([-20.8047], device='cuda:0'))])
epoch£º1170	 i:0 	 global-step:23400	 l-p:0.1432739645242691
epoch£º1170	 i:1 	 global-step:23401	 l-p:0.1285814642906189
epoch£º1170	 i:2 	 global-step:23402	 l-p:0.1261167675256729
epoch£º1170	 i:3 	 global-step:23403	 l-p:0.0927431732416153
epoch£º1170	 i:4 	 global-step:23404	 l-p:0.12403024733066559
epoch£º1170	 i:5 	 global-step:23405	 l-p:0.1708439588546753
epoch£º1170	 i:6 	 global-step:23406	 l-p:0.18085792660713196
epoch£º1170	 i:7 	 global-step:23407	 l-p:0.20298509299755096
epoch£º1170	 i:8 	 global-step:23408	 l-p:0.1551830768585205
epoch£º1170	 i:9 	 global-step:23409	 l-p:0.0964512899518013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1632, 5.1625, 5.1632],
        [5.1632, 5.0419, 4.6992],
        [5.1632, 5.0310, 4.6873],
        [5.1632, 5.0362, 5.0972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1171, step:0 
model_pd.l_p.mean(): 0.1335727423429489 
model_pd.l_d.mean(): -20.419137954711914 
model_pd.lagr.mean(): -20.285564422607422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4501], device='cuda:0')), ('power', tensor([-21.1021], device='cuda:0'))])
epoch£º1171	 i:0 	 global-step:23420	 l-p:0.1335727423429489
epoch£º1171	 i:1 	 global-step:23421	 l-p:0.19427579641342163
epoch£º1171	 i:2 	 global-step:23422	 l-p:0.11169567704200745
epoch£º1171	 i:3 	 global-step:23423	 l-p:0.13674521446228027
epoch£º1171	 i:4 	 global-step:23424	 l-p:0.16789819300174713
epoch£º1171	 i:5 	 global-step:23425	 l-p:0.0865548849105835
epoch£º1171	 i:6 	 global-step:23426	 l-p:0.1994439661502838
epoch£º1171	 i:7 	 global-step:23427	 l-p:0.12067832797765732
epoch£º1171	 i:8 	 global-step:23428	 l-p:0.11812371760606766
epoch£º1171	 i:9 	 global-step:23429	 l-p:0.14581070840358734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1593, 5.1563, 5.1592],
        [5.1593, 5.1562, 5.1592],
        [5.1593, 5.1553, 5.1592],
        [5.1593, 5.0165, 5.0766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1172, step:0 
model_pd.l_p.mean(): 0.13136906921863556 
model_pd.l_d.mean(): -20.805503845214844 
model_pd.lagr.mean(): -20.674135208129883 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4144], device='cuda:0')), ('power', tensor([-21.4562], device='cuda:0'))])
epoch£º1172	 i:0 	 global-step:23440	 l-p:0.13136906921863556
epoch£º1172	 i:1 	 global-step:23441	 l-p:0.17960377037525177
epoch£º1172	 i:2 	 global-step:23442	 l-p:0.14679798483848572
epoch£º1172	 i:3 	 global-step:23443	 l-p:0.09081073850393295
epoch£º1172	 i:4 	 global-step:23444	 l-p:0.14635182917118073
epoch£º1172	 i:5 	 global-step:23445	 l-p:0.14627836644649506
epoch£º1172	 i:6 	 global-step:23446	 l-p:0.13263601064682007
epoch£º1172	 i:7 	 global-step:23447	 l-p:0.21352210640907288
epoch£º1172	 i:8 	 global-step:23448	 l-p:0.13660724461078644
epoch£º1172	 i:9 	 global-step:23449	 l-p:0.11674804240465164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1557, 4.8560, 4.6623],
        [5.1557, 5.1558, 5.1557],
        [5.1557, 4.8733, 4.7896],
        [5.1557, 5.1557, 5.1558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1173, step:0 
model_pd.l_p.mean(): 0.1330976039171219 
model_pd.l_d.mean(): -20.106657028198242 
model_pd.lagr.mean(): -19.973560333251953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4649], device='cuda:0')), ('power', tensor([-20.8013], device='cuda:0'))])
epoch£º1173	 i:0 	 global-step:23460	 l-p:0.1330976039171219
epoch£º1173	 i:1 	 global-step:23461	 l-p:0.10364589095115662
epoch£º1173	 i:2 	 global-step:23462	 l-p:0.16133996844291687
epoch£º1173	 i:3 	 global-step:23463	 l-p:0.12848025560379028
epoch£º1173	 i:4 	 global-step:23464	 l-p:0.11614661663770676
epoch£º1173	 i:5 	 global-step:23465	 l-p:0.11484647542238235
epoch£º1173	 i:6 	 global-step:23466	 l-p:0.15873083472251892
epoch£º1173	 i:7 	 global-step:23467	 l-p:0.15042543411254883
epoch£º1173	 i:8 	 global-step:23468	 l-p:0.20219096541404724
epoch£º1173	 i:9 	 global-step:23469	 l-p:0.16322524845600128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1626, 4.9916, 5.0446],
        [5.1626, 5.0333, 5.0944],
        [5.1626, 4.8729, 4.6210],
        [5.1626, 5.2154, 4.9282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1174, step:0 
model_pd.l_p.mean(): 0.16562099754810333 
model_pd.l_d.mean(): -19.68338966369629 
model_pd.lagr.mean(): -19.51776885986328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5095], device='cuda:0')), ('power', tensor([-20.4190], device='cuda:0'))])
epoch£º1174	 i:0 	 global-step:23480	 l-p:0.16562099754810333
epoch£º1174	 i:1 	 global-step:23481	 l-p:0.1838161051273346
epoch£º1174	 i:2 	 global-step:23482	 l-p:0.12982980906963348
epoch£º1174	 i:3 	 global-step:23483	 l-p:0.13292008638381958
epoch£º1174	 i:4 	 global-step:23484	 l-p:0.15813857316970825
epoch£º1174	 i:5 	 global-step:23485	 l-p:0.11903001368045807
epoch£º1174	 i:6 	 global-step:23486	 l-p:0.11326650530099869
epoch£º1174	 i:7 	 global-step:23487	 l-p:0.10810726881027222
epoch£º1174	 i:8 	 global-step:23488	 l-p:0.1331886500120163
epoch£º1174	 i:9 	 global-step:23489	 l-p:0.15828928351402283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1651, 5.5353, 5.4273],
        [5.1651, 5.0013, 5.0568],
        [5.1651, 4.8684, 4.7196],
        [5.1651, 5.1650, 5.1651]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1175, step:0 
model_pd.l_p.mean(): 0.1179586797952652 
model_pd.l_d.mean(): -19.02401351928711 
model_pd.lagr.mean(): -18.906055450439453 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5912], device='cuda:0')), ('power', tensor([-19.8359], device='cuda:0'))])
epoch£º1175	 i:0 	 global-step:23500	 l-p:0.1179586797952652
epoch£º1175	 i:1 	 global-step:23501	 l-p:0.2153257131576538
epoch£º1175	 i:2 	 global-step:23502	 l-p:0.13284151256084442
epoch£º1175	 i:3 	 global-step:23503	 l-p:0.1282481998205185
epoch£º1175	 i:4 	 global-step:23504	 l-p:0.08322807401418686
epoch£º1175	 i:5 	 global-step:23505	 l-p:0.11057642102241516
epoch£º1175	 i:6 	 global-step:23506	 l-p:0.12963642179965973
epoch£º1175	 i:7 	 global-step:23507	 l-p:0.15358750522136688
epoch£º1175	 i:8 	 global-step:23508	 l-p:0.1974366158246994
epoch£º1175	 i:9 	 global-step:23509	 l-p:0.13677510619163513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1633, 4.9059, 4.8774],
        [5.1633, 4.8641, 4.6845],
        [5.1633, 5.1633, 5.1633],
        [5.1633, 4.9082, 4.8835]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1176, step:0 
model_pd.l_p.mean(): 0.12348166853189468 
model_pd.l_d.mean(): -19.95426368713379 
model_pd.lagr.mean(): -19.830781936645508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4365], device='cuda:0')), ('power', tensor([-20.6183], device='cuda:0'))])
epoch£º1176	 i:0 	 global-step:23520	 l-p:0.12348166853189468
epoch£º1176	 i:1 	 global-step:23521	 l-p:0.12437289953231812
epoch£º1176	 i:2 	 global-step:23522	 l-p:0.18394644558429718
epoch£º1176	 i:3 	 global-step:23523	 l-p:0.11115776002407074
epoch£º1176	 i:4 	 global-step:23524	 l-p:0.09181249141693115
epoch£º1176	 i:5 	 global-step:23525	 l-p:0.12243860960006714
epoch£º1176	 i:6 	 global-step:23526	 l-p:0.1819511353969574
epoch£º1176	 i:7 	 global-step:23527	 l-p:0.14480943977832794
epoch£º1176	 i:8 	 global-step:23528	 l-p:0.16538019478321075
epoch£º1176	 i:9 	 global-step:23529	 l-p:0.14784203469753265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1685, 5.1391, 5.1637],
        [5.1685, 5.1112, 5.1532],
        [5.1685, 5.1659, 5.1684],
        [5.1685, 5.2429, 4.9654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1177, step:0 
model_pd.l_p.mean(): 0.18866895139217377 
model_pd.l_d.mean(): -20.39105224609375 
model_pd.lagr.mean(): -20.202383041381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4686], device='cuda:0')), ('power', tensor([-21.0926], device='cuda:0'))])
epoch£º1177	 i:0 	 global-step:23540	 l-p:0.18866895139217377
epoch£º1177	 i:1 	 global-step:23541	 l-p:0.08793795108795166
epoch£º1177	 i:2 	 global-step:23542	 l-p:0.15076452493667603
epoch£º1177	 i:3 	 global-step:23543	 l-p:0.13643118739128113
epoch£º1177	 i:4 	 global-step:23544	 l-p:0.10757022351026535
epoch£º1177	 i:5 	 global-step:23545	 l-p:0.12418460845947266
epoch£º1177	 i:6 	 global-step:23546	 l-p:0.14689527451992035
epoch£º1177	 i:7 	 global-step:23547	 l-p:0.17252425849437714
epoch£º1177	 i:8 	 global-step:23548	 l-p:0.138432115316391
epoch£º1177	 i:9 	 global-step:23549	 l-p:0.13704705238342285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1678, 5.1677, 5.1678],
        [5.1678, 5.1405, 5.1636],
        [5.1678, 4.8729, 4.7360],
        [5.1678, 5.1678, 5.1678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1178, step:0 
model_pd.l_p.mean(): 0.19468091428279877 
model_pd.l_d.mean(): -20.94093132019043 
model_pd.lagr.mean(): -20.74625015258789 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3913], device='cuda:0')), ('power', tensor([-21.5696], device='cuda:0'))])
epoch£º1178	 i:0 	 global-step:23560	 l-p:0.19468091428279877
epoch£º1178	 i:1 	 global-step:23561	 l-p:0.10133296996355057
epoch£º1178	 i:2 	 global-step:23562	 l-p:0.12198223173618317
epoch£º1178	 i:3 	 global-step:23563	 l-p:0.14907453954219818
epoch£º1178	 i:4 	 global-step:23564	 l-p:0.11109087616205215
epoch£º1178	 i:5 	 global-step:23565	 l-p:0.11161695420742035
epoch£º1178	 i:6 	 global-step:23566	 l-p:0.1693304181098938
epoch£º1178	 i:7 	 global-step:23567	 l-p:0.12156786024570465
epoch£º1178	 i:8 	 global-step:23568	 l-p:0.16368623077869415
epoch£º1178	 i:9 	 global-step:23569	 l-p:0.13866184651851654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1748, 4.9969, 5.0467],
        [5.1748, 5.1553, 5.1724],
        [5.1748, 5.1087, 4.7772],
        [5.1748, 5.1407, 5.1686]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1179, step:0 
model_pd.l_p.mean(): 0.13572145998477936 
model_pd.l_d.mean(): -20.696104049682617 
model_pd.lagr.mean(): -20.560382843017578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4248], device='cuda:0')), ('power', tensor([-21.3562], device='cuda:0'))])
epoch£º1179	 i:0 	 global-step:23580	 l-p:0.13572145998477936
epoch£º1179	 i:1 	 global-step:23581	 l-p:0.11209128051996231
epoch£º1179	 i:2 	 global-step:23582	 l-p:0.1573520451784134
epoch£º1179	 i:3 	 global-step:23583	 l-p:0.12752747535705566
epoch£º1179	 i:4 	 global-step:23584	 l-p:0.12817882001399994
epoch£º1179	 i:5 	 global-step:23585	 l-p:0.1301049292087555
epoch£º1179	 i:6 	 global-step:23586	 l-p:0.14560094475746155
epoch£º1179	 i:7 	 global-step:23587	 l-p:0.18268750607967377
epoch£º1179	 i:8 	 global-step:23588	 l-p:0.1280476301908493
epoch£º1179	 i:9 	 global-step:23589	 l-p:0.11322668939828873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1776, 5.1686, 5.1770],
        [5.1776, 5.1204, 5.1624],
        [5.1776, 5.1035, 5.1534],
        [5.1776, 5.1580, 5.1752]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1180, step:0 
model_pd.l_p.mean(): 0.14107352495193481 
model_pd.l_d.mean(): -19.731233596801758 
model_pd.lagr.mean(): -19.590160369873047 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4687], device='cuda:0')), ('power', tensor([-20.4256], device='cuda:0'))])
epoch£º1180	 i:0 	 global-step:23600	 l-p:0.14107352495193481
epoch£º1180	 i:1 	 global-step:23601	 l-p:0.1241002231836319
epoch£º1180	 i:2 	 global-step:23602	 l-p:0.1322607845067978
epoch£º1180	 i:3 	 global-step:23603	 l-p:0.14908359944820404
epoch£º1180	 i:4 	 global-step:23604	 l-p:0.13507845997810364
epoch£º1180	 i:5 	 global-step:23605	 l-p:0.15939299762248993
epoch£º1180	 i:6 	 global-step:23606	 l-p:0.0905667394399643
epoch£º1180	 i:7 	 global-step:23607	 l-p:0.11642582714557648
epoch£º1180	 i:8 	 global-step:23608	 l-p:0.15717901289463043
epoch£º1180	 i:9 	 global-step:23609	 l-p:0.20180313289165497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1586, 5.4561, 5.3018],
        [5.1586, 5.1500, 5.1580],
        [5.1586, 5.1362, 5.1556],
        [5.1586, 5.4067, 5.2225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1181, step:0 
model_pd.l_p.mean(): 0.1253141164779663 
model_pd.l_d.mean(): -20.023530960083008 
model_pd.lagr.mean(): -19.898216247558594 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5338], device='cuda:0')), ('power', tensor([-20.7877], device='cuda:0'))])
epoch£º1181	 i:0 	 global-step:23620	 l-p:0.1253141164779663
epoch£º1181	 i:1 	 global-step:23621	 l-p:0.18931323289871216
epoch£º1181	 i:2 	 global-step:23622	 l-p:0.12641888856887817
epoch£º1181	 i:3 	 global-step:23623	 l-p:0.2049322873353958
epoch£º1181	 i:4 	 global-step:23624	 l-p:0.16863659024238586
epoch£º1181	 i:5 	 global-step:23625	 l-p:0.09493011236190796
epoch£º1181	 i:6 	 global-step:23626	 l-p:0.1164768859744072
epoch£º1181	 i:7 	 global-step:23627	 l-p:0.08672502636909485
epoch£º1181	 i:8 	 global-step:23628	 l-p:0.1818651705980301
epoch£º1181	 i:9 	 global-step:23629	 l-p:0.1531241536140442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1522, 5.0222, 4.6773],
        [5.1522, 4.9734, 5.0236],
        [5.1522, 4.9468, 4.9811],
        [5.1522, 5.1521, 5.1522]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1182, step:0 
model_pd.l_p.mean(): 0.1274947077035904 
model_pd.l_d.mean(): -20.63651466369629 
model_pd.lagr.mean(): -20.50901985168457 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4303], device='cuda:0')), ('power', tensor([-21.3016], device='cuda:0'))])
epoch£º1182	 i:0 	 global-step:23640	 l-p:0.1274947077035904
epoch£º1182	 i:1 	 global-step:23641	 l-p:0.1510363221168518
epoch£º1182	 i:2 	 global-step:23642	 l-p:0.20402903854846954
epoch£º1182	 i:3 	 global-step:23643	 l-p:0.12829595804214478
epoch£º1182	 i:4 	 global-step:23644	 l-p:0.10847845673561096
epoch£º1182	 i:5 	 global-step:23645	 l-p:0.22761215269565582
epoch£º1182	 i:6 	 global-step:23646	 l-p:0.08218807727098465
epoch£º1182	 i:7 	 global-step:23647	 l-p:0.16979281604290009
epoch£º1182	 i:8 	 global-step:23648	 l-p:0.13028626143932343
epoch£º1182	 i:9 	 global-step:23649	 l-p:0.17031924426555634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1496, 5.4058, 5.2266],
        [5.1496, 5.3029, 5.0652],
        [5.1496, 5.1497, 5.1497],
        [5.1496, 4.8573, 4.6049]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1183, step:0 
model_pd.l_p.mean(): 0.1019812524318695 
model_pd.l_d.mean(): -20.256603240966797 
model_pd.lagr.mean(): -20.154621124267578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5094], device='cuda:0')), ('power', tensor([-20.9984], device='cuda:0'))])
epoch£º1183	 i:0 	 global-step:23660	 l-p:0.1019812524318695
epoch£º1183	 i:1 	 global-step:23661	 l-p:0.1425607055425644
epoch£º1183	 i:2 	 global-step:23662	 l-p:0.0932885929942131
epoch£º1183	 i:3 	 global-step:23663	 l-p:0.14671598374843597
epoch£º1183	 i:4 	 global-step:23664	 l-p:0.20186182856559753
epoch£º1183	 i:5 	 global-step:23665	 l-p:0.2232697308063507
epoch£º1183	 i:6 	 global-step:23666	 l-p:0.14110319316387177
epoch£º1183	 i:7 	 global-step:23667	 l-p:0.1361616551876068
epoch£º1183	 i:8 	 global-step:23668	 l-p:0.11892557144165039
epoch£º1183	 i:9 	 global-step:23669	 l-p:0.16194796562194824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1542, 5.0861, 4.7532],
        [5.1542, 5.1458, 5.1536],
        [5.1542, 5.1929, 4.8989],
        [5.1542, 4.9116, 4.9074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1184, step:0 
model_pd.l_p.mean(): 0.17302744090557098 
model_pd.l_d.mean(): -20.637866973876953 
model_pd.lagr.mean(): -20.464839935302734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4412], device='cuda:0')), ('power', tensor([-21.3141], device='cuda:0'))])
epoch£º1184	 i:0 	 global-step:23680	 l-p:0.17302744090557098
epoch£º1184	 i:1 	 global-step:23681	 l-p:0.1716679483652115
epoch£º1184	 i:2 	 global-step:23682	 l-p:0.17806006968021393
epoch£º1184	 i:3 	 global-step:23683	 l-p:0.11701110750436783
epoch£º1184	 i:4 	 global-step:23684	 l-p:0.10234206914901733
epoch£º1184	 i:5 	 global-step:23685	 l-p:0.14977045357227325
epoch£º1184	 i:6 	 global-step:23686	 l-p:0.1422499567270279
epoch£º1184	 i:7 	 global-step:23687	 l-p:0.07623421400785446
epoch£º1184	 i:8 	 global-step:23688	 l-p:0.20194107294082642
epoch£º1184	 i:9 	 global-step:23689	 l-p:0.1156095489859581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1687, 4.8737, 4.7389],
        [5.1687, 4.9428, 4.9575],
        [5.1687, 4.9393, 4.9503],
        [5.1687, 5.0443, 5.1054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1185, step:0 
model_pd.l_p.mean(): 0.15372057259082794 
model_pd.l_d.mean(): -20.50882339477539 
model_pd.lagr.mean(): -20.3551025390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4500], device='cuda:0')), ('power', tensor([-21.1926], device='cuda:0'))])
epoch£º1185	 i:0 	 global-step:23700	 l-p:0.15372057259082794
epoch£º1185	 i:1 	 global-step:23701	 l-p:0.1269986629486084
epoch£º1185	 i:2 	 global-step:23702	 l-p:0.1591271609067917
epoch£º1185	 i:3 	 global-step:23703	 l-p:0.12167119979858398
epoch£º1185	 i:4 	 global-step:23704	 l-p:0.12564529478549957
epoch£º1185	 i:5 	 global-step:23705	 l-p:0.19023318588733673
epoch£º1185	 i:6 	 global-step:23706	 l-p:0.1352599561214447
epoch£º1185	 i:7 	 global-step:23707	 l-p:0.14076420664787292
epoch£º1185	 i:8 	 global-step:23708	 l-p:0.11216933280229568
epoch£º1185	 i:9 	 global-step:23709	 l-p:0.10889829695224762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1763, 5.1326, 5.1669],
        [5.1763, 5.1238, 5.1633],
        [5.1763, 4.9061, 4.8514],
        [5.1763, 5.1336, 4.8087]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1186, step:0 
model_pd.l_p.mean(): 0.08921296894550323 
model_pd.l_d.mean(): -20.493200302124023 
model_pd.lagr.mean(): -20.403987884521484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4552], device='cuda:0')), ('power', tensor([-21.1822], device='cuda:0'))])
epoch£º1186	 i:0 	 global-step:23720	 l-p:0.08921296894550323
epoch£º1186	 i:1 	 global-step:23721	 l-p:0.1198740005493164
epoch£º1186	 i:2 	 global-step:23722	 l-p:0.13846005499362946
epoch£º1186	 i:3 	 global-step:23723	 l-p:0.14787127077579498
epoch£º1186	 i:4 	 global-step:23724	 l-p:0.13783733546733856
epoch£º1186	 i:5 	 global-step:23725	 l-p:0.190897136926651
epoch£º1186	 i:6 	 global-step:23726	 l-p:0.11073054373264313
epoch£º1186	 i:7 	 global-step:23727	 l-p:0.13799789547920227
epoch£º1186	 i:8 	 global-step:23728	 l-p:0.11790712177753448
epoch£º1186	 i:9 	 global-step:23729	 l-p:0.157816082239151
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1803, 5.0271, 5.0853],
        [5.1803, 5.3054, 5.0524],
        [5.1803, 5.0979, 5.1509],
        [5.1803, 5.1788, 5.1802]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1187, step:0 
model_pd.l_p.mean(): 0.18181167542934418 
model_pd.l_d.mean(): -20.48325538635254 
model_pd.lagr.mean(): -20.301443099975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4534], device='cuda:0')), ('power', tensor([-21.1703], device='cuda:0'))])
epoch£º1187	 i:0 	 global-step:23740	 l-p:0.18181167542934418
epoch£º1187	 i:1 	 global-step:23741	 l-p:0.14728359878063202
epoch£º1187	 i:2 	 global-step:23742	 l-p:0.11542485654354095
epoch£º1187	 i:3 	 global-step:23743	 l-p:0.1530785858631134
epoch£º1187	 i:4 	 global-step:23744	 l-p:0.12184440344572067
epoch£º1187	 i:5 	 global-step:23745	 l-p:0.14468346536159515
epoch£º1187	 i:6 	 global-step:23746	 l-p:0.09640055149793625
epoch£º1187	 i:7 	 global-step:23747	 l-p:0.1566661149263382
epoch£º1187	 i:8 	 global-step:23748	 l-p:0.07108765095472336
epoch£º1187	 i:9 	 global-step:23749	 l-p:0.15668228268623352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1832, 5.0902, 5.1464],
        [5.1832, 5.1832, 5.1832],
        [5.1832, 5.6048, 5.5298],
        [5.1832, 5.1377, 5.1731]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1188, step:0 
model_pd.l_p.mean(): 0.12487411499023438 
model_pd.l_d.mean(): -20.435211181640625 
model_pd.lagr.mean(): -20.31033706665039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4002], device='cuda:0')), ('power', tensor([-21.0673], device='cuda:0'))])
epoch£º1188	 i:0 	 global-step:23760	 l-p:0.12487411499023438
epoch£º1188	 i:1 	 global-step:23761	 l-p:0.16368767619132996
epoch£º1188	 i:2 	 global-step:23762	 l-p:0.11226276308298111
epoch£º1188	 i:3 	 global-step:23763	 l-p:0.1234666109085083
epoch£º1188	 i:4 	 global-step:23764	 l-p:0.11039149761199951
epoch£º1188	 i:5 	 global-step:23765	 l-p:0.19060547649860382
epoch£º1188	 i:6 	 global-step:23766	 l-p:0.06459561735391617
epoch£º1188	 i:7 	 global-step:23767	 l-p:0.18342924118041992
epoch£º1188	 i:8 	 global-step:23768	 l-p:0.13655634224414825
epoch£º1188	 i:9 	 global-step:23769	 l-p:0.12056200951337814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1881, 5.0472, 5.1074],
        [5.1881, 4.9655, 4.9826],
        [5.1881, 5.1881, 5.1881],
        [5.1881, 5.1862, 5.1880]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1189, step:0 
model_pd.l_p.mean(): 0.11270921677350998 
model_pd.l_d.mean(): -20.998559951782227 
model_pd.lagr.mean(): -20.88585090637207 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3635], device='cuda:0')), ('power', tensor([-21.5994], device='cuda:0'))])
epoch£º1189	 i:0 	 global-step:23780	 l-p:0.11270921677350998
epoch£º1189	 i:1 	 global-step:23781	 l-p:0.20018291473388672
epoch£º1189	 i:2 	 global-step:23782	 l-p:0.16956070065498352
epoch£º1189	 i:3 	 global-step:23783	 l-p:0.05632515624165535
epoch£º1189	 i:4 	 global-step:23784	 l-p:0.13954691588878632
epoch£º1189	 i:5 	 global-step:23785	 l-p:0.12787120044231415
epoch£º1189	 i:6 	 global-step:23786	 l-p:0.19415326416492462
epoch£º1189	 i:7 	 global-step:23787	 l-p:0.09244789928197861
epoch£º1189	 i:8 	 global-step:23788	 l-p:0.12164358794689178
epoch£º1189	 i:9 	 global-step:23789	 l-p:0.10141868144273758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1883, 5.1883, 5.1883],
        [5.1883, 5.1691, 5.1860],
        [5.1883, 5.1883, 5.1883],
        [5.1883, 5.1720, 5.1865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1190, step:0 
model_pd.l_p.mean(): 0.08750993758440018 
model_pd.l_d.mean(): -19.69672203063965 
model_pd.lagr.mean(): -19.60921287536621 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4931], device='cuda:0')), ('power', tensor([-20.4157], device='cuda:0'))])
epoch£º1190	 i:0 	 global-step:23800	 l-p:0.08750993758440018
epoch£º1190	 i:1 	 global-step:23801	 l-p:0.16695484519004822
epoch£º1190	 i:2 	 global-step:23802	 l-p:0.07278028130531311
epoch£º1190	 i:3 	 global-step:23803	 l-p:0.16241778433322906
epoch£º1190	 i:4 	 global-step:23804	 l-p:0.1585099697113037
epoch£º1190	 i:5 	 global-step:23805	 l-p:0.19425395131111145
epoch£º1190	 i:6 	 global-step:23806	 l-p:0.13384555280208588
epoch£º1190	 i:7 	 global-step:23807	 l-p:0.12907257676124573
epoch£º1190	 i:8 	 global-step:23808	 l-p:0.08944503217935562
epoch£º1190	 i:9 	 global-step:23809	 l-p:0.12060800194740295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1904, 5.5536, 5.4398],
        [5.1904, 5.1904, 5.1904],
        [5.1904, 5.1467, 5.1810],
        [5.1904, 4.9663, 4.9818]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1191, step:0 
model_pd.l_p.mean(): 0.07405693829059601 
model_pd.l_d.mean(): -19.26784324645996 
model_pd.lagr.mean(): -19.19378662109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5707], device='cuda:0')), ('power', tensor([-20.0614], device='cuda:0'))])
epoch£º1191	 i:0 	 global-step:23820	 l-p:0.07405693829059601
epoch£º1191	 i:1 	 global-step:23821	 l-p:0.12397794425487518
epoch£º1191	 i:2 	 global-step:23822	 l-p:0.10660301148891449
epoch£º1191	 i:3 	 global-step:23823	 l-p:0.15052315592765808
epoch£º1191	 i:4 	 global-step:23824	 l-p:0.0912964791059494
epoch£º1191	 i:5 	 global-step:23825	 l-p:0.156595841050148
epoch£º1191	 i:6 	 global-step:23826	 l-p:0.1514272838830948
epoch£º1191	 i:7 	 global-step:23827	 l-p:0.12825055420398712
epoch£º1191	 i:8 	 global-step:23828	 l-p:0.14117147028446198
epoch£º1191	 i:9 	 global-step:23829	 l-p:0.19261832535266876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1880, 5.1094, 5.1610],
        [5.1880, 5.0964, 5.1523],
        [5.1880, 5.0265, 5.0825],
        [5.1880, 5.1829, 5.1878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1192, step:0 
model_pd.l_p.mean(): 0.14281347393989563 
model_pd.l_d.mean(): -19.760385513305664 
model_pd.lagr.mean(): -19.617572784423828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5173], device='cuda:0')), ('power', tensor([-20.5048], device='cuda:0'))])
epoch£º1192	 i:0 	 global-step:23840	 l-p:0.14281347393989563
epoch£º1192	 i:1 	 global-step:23841	 l-p:0.18732352554798126
epoch£º1192	 i:2 	 global-step:23842	 l-p:0.13515430688858032
epoch£º1192	 i:3 	 global-step:23843	 l-p:0.08695787191390991
epoch£º1192	 i:4 	 global-step:23844	 l-p:0.1337495595216751
epoch£º1192	 i:5 	 global-step:23845	 l-p:0.15438325703144073
epoch£º1192	 i:6 	 global-step:23846	 l-p:0.13299761712551117
epoch£º1192	 i:7 	 global-step:23847	 l-p:0.11005769670009613
epoch£º1192	 i:8 	 global-step:23848	 l-p:0.14809907972812653
epoch£º1192	 i:9 	 global-step:23849	 l-p:0.09392049908638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1839, 4.8937, 4.6468],
        [5.1839, 5.1279, 5.1693],
        [5.1839, 5.1791, 5.1837],
        [5.1839, 5.1612, 5.1809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1193, step:0 
model_pd.l_p.mean(): 0.14136606454849243 
model_pd.l_d.mean(): -20.16807746887207 
model_pd.lagr.mean(): -20.026710510253906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5159], device='cuda:0')), ('power', tensor([-20.9155], device='cuda:0'))])
epoch£º1193	 i:0 	 global-step:23860	 l-p:0.14136606454849243
epoch£º1193	 i:1 	 global-step:23861	 l-p:0.10095986723899841
epoch£º1193	 i:2 	 global-step:23862	 l-p:0.11098838597536087
epoch£º1193	 i:3 	 global-step:23863	 l-p:0.14780370891094208
epoch£º1193	 i:4 	 global-step:23864	 l-p:0.1428510993719101
epoch£º1193	 i:5 	 global-step:23865	 l-p:0.14859743416309357
epoch£º1193	 i:6 	 global-step:23866	 l-p:0.13254722952842712
epoch£º1193	 i:7 	 global-step:23867	 l-p:0.1526220738887787
epoch£º1193	 i:8 	 global-step:23868	 l-p:0.1273844689130783
epoch£º1193	 i:9 	 global-step:23869	 l-p:0.1333843320608139
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1801, 4.9287, 4.6164],
        [5.1801, 5.1790, 5.1801],
        [5.1801, 5.0310, 4.6860],
        [5.1801, 4.8830, 4.6671]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1194, step:0 
model_pd.l_p.mean(): 0.11989717930555344 
model_pd.l_d.mean(): -20.501296997070312 
model_pd.lagr.mean(): -20.381399154663086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4479], device='cuda:0')), ('power', tensor([-21.1829], device='cuda:0'))])
epoch£º1194	 i:0 	 global-step:23880	 l-p:0.11989717930555344
epoch£º1194	 i:1 	 global-step:23881	 l-p:0.13674601912498474
epoch£º1194	 i:2 	 global-step:23882	 l-p:0.12999700009822845
epoch£º1194	 i:3 	 global-step:23883	 l-p:0.16110916435718536
epoch£º1194	 i:4 	 global-step:23884	 l-p:0.14170363545417786
epoch£º1194	 i:5 	 global-step:23885	 l-p:0.16347455978393555
epoch£º1194	 i:6 	 global-step:23886	 l-p:0.10958687216043472
epoch£º1194	 i:7 	 global-step:23887	 l-p:0.12482909113168716
epoch£º1194	 i:8 	 global-step:23888	 l-p:0.14999723434448242
epoch£º1194	 i:9 	 global-step:23889	 l-p:0.17276133596897125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1606, 5.1554, 5.1603],
        [5.1606, 5.1602, 5.1606],
        [5.1606, 5.0250, 5.0862],
        [5.1606, 4.8858, 4.8251]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1195, step:0 
model_pd.l_p.mean(): 0.1419495940208435 
model_pd.l_d.mean(): -20.509746551513672 
model_pd.lagr.mean(): -20.3677978515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4528], device='cuda:0')), ('power', tensor([-21.1964], device='cuda:0'))])
epoch£º1195	 i:0 	 global-step:23900	 l-p:0.1419495940208435
epoch£º1195	 i:1 	 global-step:23901	 l-p:0.11874701082706451
epoch£º1195	 i:2 	 global-step:23902	 l-p:0.17221656441688538
epoch£º1195	 i:3 	 global-step:23903	 l-p:0.1688331812620163
epoch£º1195	 i:4 	 global-step:23904	 l-p:0.12426503002643585
epoch£º1195	 i:5 	 global-step:23905	 l-p:0.08791638165712357
epoch£º1195	 i:6 	 global-step:23906	 l-p:0.13496661186218262
epoch£º1195	 i:7 	 global-step:23907	 l-p:0.13967986404895782
epoch£º1195	 i:8 	 global-step:23908	 l-p:0.1563095599412918
epoch£º1195	 i:9 	 global-step:23909	 l-p:0.179080992937088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1643, 5.0815, 5.1348],
        [5.1643, 5.1328, 5.1589],
        [5.1643, 5.3813, 5.1784],
        [5.1643, 5.2375, 4.9587]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1196, step:0 
model_pd.l_p.mean(): 0.1351013481616974 
model_pd.l_d.mean(): -20.300336837768555 
model_pd.lagr.mean(): -20.16523551940918 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4856], device='cuda:0')), ('power', tensor([-21.0183], device='cuda:0'))])
epoch£º1196	 i:0 	 global-step:23920	 l-p:0.1351013481616974
epoch£º1196	 i:1 	 global-step:23921	 l-p:0.14670202136039734
epoch£º1196	 i:2 	 global-step:23922	 l-p:0.1885502189397812
epoch£º1196	 i:3 	 global-step:23923	 l-p:0.14969868957996368
epoch£º1196	 i:4 	 global-step:23924	 l-p:0.10147910565137863
epoch£º1196	 i:5 	 global-step:23925	 l-p:0.14439506828784943
epoch£º1196	 i:6 	 global-step:23926	 l-p:0.13999420404434204
epoch£º1196	 i:7 	 global-step:23927	 l-p:0.12294340878725052
epoch£º1196	 i:8 	 global-step:23928	 l-p:0.1351172775030136
epoch£º1196	 i:9 	 global-step:23929	 l-p:0.13693596422672272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1668, 4.8836, 4.7994],
        [5.1668, 5.0826, 5.1364],
        [5.1668, 5.4858, 5.3442],
        [5.1668, 4.9562, 4.6185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1197, step:0 
model_pd.l_p.mean(): 0.14782224595546722 
model_pd.l_d.mean(): -20.151063919067383 
model_pd.lagr.mean(): -20.00324249267578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5088], device='cuda:0')), ('power', tensor([-20.8910], device='cuda:0'))])
epoch£º1197	 i:0 	 global-step:23940	 l-p:0.14782224595546722
epoch£º1197	 i:1 	 global-step:23941	 l-p:0.14017489552497864
epoch£º1197	 i:2 	 global-step:23942	 l-p:0.11628004163503647
epoch£º1197	 i:3 	 global-step:23943	 l-p:0.14359453320503235
epoch£º1197	 i:4 	 global-step:23944	 l-p:0.14978301525115967
epoch£º1197	 i:5 	 global-step:23945	 l-p:0.20829293131828308
epoch£º1197	 i:6 	 global-step:23946	 l-p:0.13853001594543457
epoch£º1197	 i:7 	 global-step:23947	 l-p:0.16263151168823242
epoch£º1197	 i:8 	 global-step:23948	 l-p:0.12820422649383545
epoch£º1197	 i:9 	 global-step:23949	 l-p:0.07327718287706375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1680, 5.1426, 4.8227],
        [5.1680, 5.1680, 5.1680],
        [5.1680, 5.1636, 5.1678],
        [5.1680, 5.1511, 5.1662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1198, step:0 
model_pd.l_p.mean(): 0.10633613169193268 
model_pd.l_d.mean(): -20.049394607543945 
model_pd.lagr.mean(): -19.943058013916016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4964], device='cuda:0')), ('power', tensor([-20.7756], device='cuda:0'))])
epoch£º1198	 i:0 	 global-step:23960	 l-p:0.10633613169193268
epoch£º1198	 i:1 	 global-step:23961	 l-p:0.12862972915172577
epoch£º1198	 i:2 	 global-step:23962	 l-p:0.09576199948787689
epoch£º1198	 i:3 	 global-step:23963	 l-p:0.1668105274438858
epoch£º1198	 i:4 	 global-step:23964	 l-p:0.142281636595726
epoch£º1198	 i:5 	 global-step:23965	 l-p:0.12035425007343292
epoch£º1198	 i:6 	 global-step:23966	 l-p:0.13642200827598572
epoch£º1198	 i:7 	 global-step:23967	 l-p:0.14143052697181702
epoch£º1198	 i:8 	 global-step:23968	 l-p:0.21477895975112915
epoch£º1198	 i:9 	 global-step:23969	 l-p:0.1459961235523224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1685, 5.1671, 5.1685],
        [5.1685, 5.1685, 5.1685],
        [5.1685, 5.1677, 5.1685],
        [5.1685, 5.1685, 5.1685]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1199, step:0 
model_pd.l_p.mean(): 0.10643929988145828 
model_pd.l_d.mean(): -19.273792266845703 
model_pd.lagr.mean(): -19.1673526763916 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5533], device='cuda:0')), ('power', tensor([-20.0497], device='cuda:0'))])
epoch£º1199	 i:0 	 global-step:23980	 l-p:0.10643929988145828
epoch£º1199	 i:1 	 global-step:23981	 l-p:0.1888938993215561
epoch£º1199	 i:2 	 global-step:23982	 l-p:0.09328977018594742
epoch£º1199	 i:3 	 global-step:23983	 l-p:0.14306116104125977
epoch£º1199	 i:4 	 global-step:23984	 l-p:0.1317272186279297
epoch£º1199	 i:5 	 global-step:23985	 l-p:0.15044745802879333
epoch£º1199	 i:6 	 global-step:23986	 l-p:0.09271203726530075
epoch£º1199	 i:7 	 global-step:23987	 l-p:0.18792928755283356
epoch£º1199	 i:8 	 global-step:23988	 l-p:0.13391201198101044
epoch£º1199	 i:9 	 global-step:23989	 l-p:0.16302700340747833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1679, 5.0875, 5.1399],
        [5.1679, 5.1679, 5.1679],
        [5.1679, 4.9364, 4.9460],
        [5.1679, 4.9312, 4.9345]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1200, step:0 
model_pd.l_p.mean(): 0.05045133829116821 
model_pd.l_d.mean(): -20.296100616455078 
model_pd.lagr.mean(): -20.245649337768555 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4669], device='cuda:0')), ('power', tensor([-20.9949], device='cuda:0'))])
epoch£º1200	 i:0 	 global-step:24000	 l-p:0.05045133829116821
epoch£º1200	 i:1 	 global-step:24001	 l-p:0.12011890113353729
epoch£º1200	 i:2 	 global-step:24002	 l-p:0.11237864941358566
epoch£º1200	 i:3 	 global-step:24003	 l-p:0.13578373193740845
epoch£º1200	 i:4 	 global-step:24004	 l-p:0.1689596325159073
epoch£º1200	 i:5 	 global-step:24005	 l-p:0.13003985583782196
epoch£º1200	 i:6 	 global-step:24006	 l-p:0.16689014434814453
epoch£º1200	 i:7 	 global-step:24007	 l-p:0.21905170381069183
epoch£º1200	 i:8 	 global-step:24008	 l-p:0.11326530575752258
epoch£º1200	 i:9 	 global-step:24009	 l-p:0.18388785421848297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1669, 5.1669, 5.1669],
        [5.1669, 5.1423, 5.1634],
        [5.1669, 4.8737, 4.6263],
        [5.1669, 4.8684, 4.6461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1201, step:0 
model_pd.l_p.mean(): 0.13737957179546356 
model_pd.l_d.mean(): -19.02622413635254 
model_pd.lagr.mean(): -18.888845443725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5389], device='cuda:0')), ('power', tensor([-19.7847], device='cuda:0'))])
epoch£º1201	 i:0 	 global-step:24020	 l-p:0.13737957179546356
epoch£º1201	 i:1 	 global-step:24021	 l-p:0.13995227217674255
epoch£º1201	 i:2 	 global-step:24022	 l-p:0.18793925642967224
epoch£º1201	 i:3 	 global-step:24023	 l-p:0.10703086853027344
epoch£º1201	 i:4 	 global-step:24024	 l-p:0.1325419545173645
epoch£º1201	 i:5 	 global-step:24025	 l-p:0.12231738865375519
epoch£º1201	 i:6 	 global-step:24026	 l-p:0.1342206597328186
epoch£º1201	 i:7 	 global-step:24027	 l-p:0.14482595026493073
epoch£º1201	 i:8 	 global-step:24028	 l-p:0.18843889236450195
epoch£º1201	 i:9 	 global-step:24029	 l-p:0.11252941191196442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1647, 5.0804, 5.1342],
        [5.1647, 4.8773, 4.7814],
        [5.1647, 5.1647, 5.1647],
        [5.1647, 5.4431, 5.2763]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1202, step:0 
model_pd.l_p.mean(): 0.08771577477455139 
model_pd.l_d.mean(): -19.824583053588867 
model_pd.lagr.mean(): -19.736867904663086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5127], device='cuda:0')), ('power', tensor([-20.5650], device='cuda:0'))])
epoch£º1202	 i:0 	 global-step:24040	 l-p:0.08771577477455139
epoch£º1202	 i:1 	 global-step:24041	 l-p:0.14890089631080627
epoch£º1202	 i:2 	 global-step:24042	 l-p:0.1423964649438858
epoch£º1202	 i:3 	 global-step:24043	 l-p:0.1432304084300995
epoch£º1202	 i:4 	 global-step:24044	 l-p:0.09396302700042725
epoch£º1202	 i:5 	 global-step:24045	 l-p:0.10289136320352554
epoch£º1202	 i:6 	 global-step:24046	 l-p:0.15050558745861053
epoch£º1202	 i:7 	 global-step:24047	 l-p:0.2743476331233978
epoch£º1202	 i:8 	 global-step:24048	 l-p:0.15229827165603638
epoch£º1202	 i:9 	 global-step:24049	 l-p:0.14007896184921265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1574, 5.1334, 5.1540],
        [5.1574, 5.1570, 5.1574],
        [5.1574, 4.9772, 4.6315],
        [5.1574, 4.9013, 4.5878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1203, step:0 
model_pd.l_p.mean(): 0.2291094958782196 
model_pd.l_d.mean(): -20.43722915649414 
model_pd.lagr.mean(): -20.208120346069336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4939], device='cuda:0')), ('power', tensor([-21.1652], device='cuda:0'))])
epoch£º1203	 i:0 	 global-step:24060	 l-p:0.2291094958782196
epoch£º1203	 i:1 	 global-step:24061	 l-p:0.15798714756965637
epoch£º1203	 i:2 	 global-step:24062	 l-p:0.11424430459737778
epoch£º1203	 i:3 	 global-step:24063	 l-p:0.17232666909694672
epoch£º1203	 i:4 	 global-step:24064	 l-p:0.12203320115804672
epoch£º1203	 i:5 	 global-step:24065	 l-p:0.15386758744716644
epoch£º1203	 i:6 	 global-step:24066	 l-p:0.1061670258641243
epoch£º1203	 i:7 	 global-step:24067	 l-p:0.13582062721252441
epoch£º1203	 i:8 	 global-step:24068	 l-p:0.13233737647533417
epoch£º1203	 i:9 	 global-step:24069	 l-p:0.12694194912910461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1557, 5.1466, 5.1551],
        [5.1557, 5.1262, 5.1510],
        [5.1557, 4.8645, 4.7580],
        [5.1557, 4.9942, 5.0513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1204, step:0 
model_pd.l_p.mean(): 0.1673640012741089 
model_pd.l_d.mean(): -20.729738235473633 
model_pd.lagr.mean(): -20.562374114990234 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4288], device='cuda:0')), ('power', tensor([-21.3943], device='cuda:0'))])
epoch£º1204	 i:0 	 global-step:24080	 l-p:0.1673640012741089
epoch£º1204	 i:1 	 global-step:24081	 l-p:0.14200949668884277
epoch£º1204	 i:2 	 global-step:24082	 l-p:0.1289091259241104
epoch£º1204	 i:3 	 global-step:24083	 l-p:0.1240319237112999
epoch£º1204	 i:4 	 global-step:24084	 l-p:0.12975426018238068
epoch£º1204	 i:5 	 global-step:24085	 l-p:0.26770326495170593
epoch£º1204	 i:6 	 global-step:24086	 l-p:0.08250770717859268
epoch£º1204	 i:7 	 global-step:24087	 l-p:0.14445710182189941
epoch£º1204	 i:8 	 global-step:24088	 l-p:0.112851582467556
epoch£º1204	 i:9 	 global-step:24089	 l-p:0.16077518463134766
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1554, 4.8972, 4.5852],
        [5.1554, 5.1018, 5.1420],
        [5.1554, 5.2713, 5.0133],
        [5.1554, 4.9220, 4.5921]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1205, step:0 
model_pd.l_p.mean(): 0.1165478453040123 
model_pd.l_d.mean(): -20.545408248901367 
model_pd.lagr.mean(): -20.42885971069336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4679], device='cuda:0')), ('power', tensor([-21.2479], device='cuda:0'))])
epoch£º1205	 i:0 	 global-step:24100	 l-p:0.1165478453040123
epoch£º1205	 i:1 	 global-step:24101	 l-p:0.15044018626213074
epoch£º1205	 i:2 	 global-step:24102	 l-p:0.12719754874706268
epoch£º1205	 i:3 	 global-step:24103	 l-p:0.22367411851882935
epoch£º1205	 i:4 	 global-step:24104	 l-p:0.10346204787492752
epoch£º1205	 i:5 	 global-step:24105	 l-p:0.14759187400341034
epoch£º1205	 i:6 	 global-step:24106	 l-p:0.16526271402835846
epoch£º1205	 i:7 	 global-step:24107	 l-p:0.12977777421474457
epoch£º1205	 i:8 	 global-step:24108	 l-p:0.19426947832107544
epoch£º1205	 i:9 	 global-step:24109	 l-p:0.1172594353556633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1517, 5.1517, 5.1517],
        [5.1517, 4.8491, 4.6510],
        [5.1517, 5.1003, 5.1392],
        [5.1517, 4.8740, 4.8089]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1206, step:0 
model_pd.l_p.mean(): 0.16959550976753235 
model_pd.l_d.mean(): -20.764328002929688 
model_pd.lagr.mean(): -20.5947322845459 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4186], device='cuda:0')), ('power', tensor([-21.4189], device='cuda:0'))])
epoch£º1206	 i:0 	 global-step:24120	 l-p:0.16959550976753235
epoch£º1206	 i:1 	 global-step:24121	 l-p:0.1269717514514923
epoch£º1206	 i:2 	 global-step:24122	 l-p:0.07409404218196869
epoch£º1206	 i:3 	 global-step:24123	 l-p:0.14216269552707672
epoch£º1206	 i:4 	 global-step:24124	 l-p:0.16339969635009766
epoch£º1206	 i:5 	 global-step:24125	 l-p:0.18497468531131744
epoch£º1206	 i:6 	 global-step:24126	 l-p:0.1750171184539795
epoch£º1206	 i:7 	 global-step:24127	 l-p:0.09269078820943832
epoch£º1206	 i:8 	 global-step:24128	 l-p:0.12751884758472443
epoch£º1206	 i:9 	 global-step:24129	 l-p:0.21542994678020477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1560, 4.9746, 5.0241],
        [5.1560, 4.8765, 4.5915],
        [5.1560, 5.1547, 5.1559],
        [5.1560, 4.8546, 4.6901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1207, step:0 
model_pd.l_p.mean(): 0.17253705859184265 
model_pd.l_d.mean(): -20.205385208129883 
model_pd.lagr.mean(): -20.032848358154297 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5077], device='cuda:0')), ('power', tensor([-20.9449], device='cuda:0'))])
epoch£º1207	 i:0 	 global-step:24140	 l-p:0.17253705859184265
epoch£º1207	 i:1 	 global-step:24141	 l-p:0.13325512409210205
epoch£º1207	 i:2 	 global-step:24142	 l-p:0.11836894601583481
epoch£º1207	 i:3 	 global-step:24143	 l-p:0.17072363197803497
epoch£º1207	 i:4 	 global-step:24144	 l-p:0.17338664829730988
epoch£º1207	 i:5 	 global-step:24145	 l-p:0.12879568338394165
epoch£º1207	 i:6 	 global-step:24146	 l-p:0.14723321795463562
epoch£º1207	 i:7 	 global-step:24147	 l-p:0.1540098488330841
epoch£º1207	 i:8 	 global-step:24148	 l-p:0.12448686361312866
epoch£º1207	 i:9 	 global-step:24149	 l-p:0.11877935379743576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1620, 4.9832, 4.6375],
        [5.1620, 5.0293, 5.0907],
        [5.1620, 5.1620, 5.1620],
        [5.1620, 5.0108, 5.0702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1208, step:0 
model_pd.l_p.mean(): 0.12956848740577698 
model_pd.l_d.mean(): -20.089754104614258 
model_pd.lagr.mean(): -19.960186004638672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5130], device='cuda:0')), ('power', tensor([-20.8334], device='cuda:0'))])
epoch£º1208	 i:0 	 global-step:24160	 l-p:0.12956848740577698
epoch£º1208	 i:1 	 global-step:24161	 l-p:0.20847995579242706
epoch£º1208	 i:2 	 global-step:24162	 l-p:0.17738981544971466
epoch£º1208	 i:3 	 global-step:24163	 l-p:0.12399424612522125
epoch£º1208	 i:4 	 global-step:24164	 l-p:0.1362425982952118
epoch£º1208	 i:5 	 global-step:24165	 l-p:0.18555369973182678
epoch£º1208	 i:6 	 global-step:24166	 l-p:0.13863162696361542
epoch£º1208	 i:7 	 global-step:24167	 l-p:0.08500063419342041
epoch£º1208	 i:8 	 global-step:24168	 l-p:0.13210317492485046
epoch£º1208	 i:9 	 global-step:24169	 l-p:0.1088637039065361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1610, 5.0660, 5.1231],
        [5.1610, 5.1485, 5.1599],
        [5.1610, 5.1610, 5.1610],
        [5.1610, 5.4371, 5.2688]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1209, step:0 
model_pd.l_p.mean(): 0.1664462685585022 
model_pd.l_d.mean(): -19.66077423095703 
model_pd.lagr.mean(): -19.494327545166016 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4742], device='cuda:0')), ('power', tensor([-20.3601], device='cuda:0'))])
epoch£º1209	 i:0 	 global-step:24180	 l-p:0.1664462685585022
epoch£º1209	 i:1 	 global-step:24181	 l-p:0.17307469248771667
epoch£º1209	 i:2 	 global-step:24182	 l-p:0.16179053485393524
epoch£º1209	 i:3 	 global-step:24183	 l-p:0.10485577583312988
epoch£º1209	 i:4 	 global-step:24184	 l-p:0.14041084051132202
epoch£º1209	 i:5 	 global-step:24185	 l-p:0.1148008480668068
epoch£º1209	 i:6 	 global-step:24186	 l-p:0.11662635207176208
epoch£º1209	 i:7 	 global-step:24187	 l-p:0.214762344956398
epoch£º1209	 i:8 	 global-step:24188	 l-p:0.122037373483181
epoch£º1209	 i:9 	 global-step:24189	 l-p:0.12426451593637466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1572, 5.1554, 5.1571],
        [5.1572, 5.3712, 5.1663],
        [5.1572, 5.1518, 5.1569],
        [5.1572, 5.1049, 5.1443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1210, step:0 
model_pd.l_p.mean(): 0.1543268859386444 
model_pd.l_d.mean(): -20.060598373413086 
model_pd.lagr.mean(): -19.90627098083496 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5284], device='cuda:0')), ('power', tensor([-20.8196], device='cuda:0'))])
epoch£º1210	 i:0 	 global-step:24200	 l-p:0.1543268859386444
epoch£º1210	 i:1 	 global-step:24201	 l-p:0.12633438408374786
epoch£º1210	 i:2 	 global-step:24202	 l-p:0.12297240644693375
epoch£º1210	 i:3 	 global-step:24203	 l-p:0.17195269465446472
epoch£º1210	 i:4 	 global-step:24204	 l-p:0.10259032994508743
epoch£º1210	 i:5 	 global-step:24205	 l-p:0.1026599258184433
epoch£º1210	 i:6 	 global-step:24206	 l-p:0.1421653926372528
epoch£º1210	 i:7 	 global-step:24207	 l-p:0.16298004984855652
epoch£º1210	 i:8 	 global-step:24208	 l-p:0.2568971514701843
epoch£º1210	 i:9 	 global-step:24209	 l-p:0.12101373076438904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1541, 5.1541, 5.1541],
        [5.1541, 5.1397, 5.1527],
        [5.1541, 5.1541, 5.1541],
        [5.1541, 5.1541, 5.1541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1211, step:0 
model_pd.l_p.mean(): 0.19432520866394043 
model_pd.l_d.mean(): -20.592695236206055 
model_pd.lagr.mean(): -20.39837074279785 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4602], device='cuda:0')), ('power', tensor([-21.2878], device='cuda:0'))])
epoch£º1211	 i:0 	 global-step:24220	 l-p:0.19432520866394043
epoch£º1211	 i:1 	 global-step:24221	 l-p:0.11876121163368225
epoch£º1211	 i:2 	 global-step:24222	 l-p:0.15176227688789368
epoch£º1211	 i:3 	 global-step:24223	 l-p:0.11896365880966187
epoch£º1211	 i:4 	 global-step:24224	 l-p:0.2439037710428238
epoch£º1211	 i:5 	 global-step:24225	 l-p:0.13710518181324005
epoch£º1211	 i:6 	 global-step:24226	 l-p:0.1277671903371811
epoch£º1211	 i:7 	 global-step:24227	 l-p:0.0770273506641388
epoch£º1211	 i:8 	 global-step:24228	 l-p:0.15800291299819946
epoch£º1211	 i:9 	 global-step:24229	 l-p:0.14036835730075836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1546, 5.1484, 5.1543],
        [5.1546, 5.1527, 5.1546],
        [5.1546, 4.9829, 5.0367],
        [5.1546, 5.1545, 5.1546]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1212, step:0 
model_pd.l_p.mean(): 0.19034497439861298 
model_pd.l_d.mean(): -19.30355453491211 
model_pd.lagr.mean(): -19.113208770751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5560], device='cuda:0')), ('power', tensor([-20.0826], device='cuda:0'))])
epoch£º1212	 i:0 	 global-step:24240	 l-p:0.19034497439861298
epoch£º1212	 i:1 	 global-step:24241	 l-p:0.09395093470811844
epoch£º1212	 i:2 	 global-step:24242	 l-p:0.13933005928993225
epoch£º1212	 i:3 	 global-step:24243	 l-p:0.08454020321369171
epoch£º1212	 i:4 	 global-step:24244	 l-p:0.16088351607322693
epoch£º1212	 i:5 	 global-step:24245	 l-p:0.21699918806552887
epoch£º1212	 i:6 	 global-step:24246	 l-p:0.1971006691455841
epoch£º1212	 i:7 	 global-step:24247	 l-p:0.12423969805240631
epoch£º1212	 i:8 	 global-step:24248	 l-p:0.10986736416816711
epoch£º1212	 i:9 	 global-step:24249	 l-p:0.1404714435338974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1592, 5.1591, 5.1592],
        [5.1592, 5.1591, 5.1592],
        [5.1592, 5.0741, 5.1282],
        [5.1592, 5.0291, 5.0906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1213, step:0 
model_pd.l_p.mean(): 0.155745729804039 
model_pd.l_d.mean(): -20.399389266967773 
model_pd.lagr.mean(): -20.243642807006836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4725], device='cuda:0')), ('power', tensor([-21.1050], device='cuda:0'))])
epoch£º1213	 i:0 	 global-step:24260	 l-p:0.155745729804039
epoch£º1213	 i:1 	 global-step:24261	 l-p:0.13772091269493103
epoch£º1213	 i:2 	 global-step:24262	 l-p:0.11736086755990982
epoch£º1213	 i:3 	 global-step:24263	 l-p:0.10053720325231552
epoch£º1213	 i:4 	 global-step:24264	 l-p:0.10212251543998718
epoch£º1213	 i:5 	 global-step:24265	 l-p:0.13081717491149902
epoch£º1213	 i:6 	 global-step:24266	 l-p:0.15234385430812836
epoch£º1213	 i:7 	 global-step:24267	 l-p:0.13250036537647247
epoch£º1213	 i:8 	 global-step:24268	 l-p:0.19708377122879028
epoch£º1213	 i:9 	 global-step:24269	 l-p:0.2096075564622879
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1606, 5.0076, 5.0667],
        [5.1606, 4.9481, 4.6095],
        [5.1606, 5.0961, 5.1419],
        [5.1606, 5.0825, 5.1342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1214, step:0 
model_pd.l_p.mean(): 0.13828366994857788 
model_pd.l_d.mean(): -20.47381019592285 
model_pd.lagr.mean(): -20.335527420043945 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4578], device='cuda:0')), ('power', tensor([-21.1652], device='cuda:0'))])
epoch£º1214	 i:0 	 global-step:24280	 l-p:0.13828366994857788
epoch£º1214	 i:1 	 global-step:24281	 l-p:0.12106481194496155
epoch£º1214	 i:2 	 global-step:24282	 l-p:0.1857292205095291
epoch£º1214	 i:3 	 global-step:24283	 l-p:0.1431596875190735
epoch£º1214	 i:4 	 global-step:24284	 l-p:0.15036816895008087
epoch£º1214	 i:5 	 global-step:24285	 l-p:0.12916715443134308
epoch£º1214	 i:6 	 global-step:24286	 l-p:0.11656156927347183
epoch£º1214	 i:7 	 global-step:24287	 l-p:0.1621011644601822
epoch£º1214	 i:8 	 global-step:24288	 l-p:0.11582383513450623
epoch£º1214	 i:9 	 global-step:24289	 l-p:0.17338740825653076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1607, 4.9616, 4.6191],
        [5.1607, 4.9807, 4.6349],
        [5.1607, 4.9192, 4.9174],
        [5.1607, 5.1607, 5.1607]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1215, step:0 
model_pd.l_p.mean(): 0.1313423216342926 
model_pd.l_d.mean(): -20.672748565673828 
model_pd.lagr.mean(): -20.541406631469727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4408], device='cuda:0')), ('power', tensor([-21.3489], device='cuda:0'))])
epoch£º1215	 i:0 	 global-step:24300	 l-p:0.1313423216342926
epoch£º1215	 i:1 	 global-step:24301	 l-p:0.15158973634243011
epoch£º1215	 i:2 	 global-step:24302	 l-p:0.14299175143241882
epoch£º1215	 i:3 	 global-step:24303	 l-p:0.15929999947547913
epoch£º1215	 i:4 	 global-step:24304	 l-p:0.17226260900497437
epoch£º1215	 i:5 	 global-step:24305	 l-p:0.11413747817277908
epoch£º1215	 i:6 	 global-step:24306	 l-p:0.14973577857017517
epoch£º1215	 i:7 	 global-step:24307	 l-p:0.16671256721019745
epoch£º1215	 i:8 	 global-step:24308	 l-p:0.1044592633843422
epoch£º1215	 i:9 	 global-step:24309	 l-p:0.137750044465065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1626, 4.9832, 4.6375],
        [5.1626, 5.1144, 4.7864],
        [5.1626, 5.0676, 5.1247],
        [5.1626, 5.1623, 5.1626]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1216, step:0 
model_pd.l_p.mean(): 0.12206777185201645 
model_pd.l_d.mean(): -19.462451934814453 
model_pd.lagr.mean(): -19.340383529663086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4771], device='cuda:0')), ('power', tensor([-20.1626], device='cuda:0'))])
epoch£º1216	 i:0 	 global-step:24320	 l-p:0.12206777185201645
epoch£º1216	 i:1 	 global-step:24321	 l-p:0.14498315751552582
epoch£º1216	 i:2 	 global-step:24322	 l-p:0.1475190371274948
epoch£º1216	 i:3 	 global-step:24323	 l-p:0.13337133824825287
epoch£º1216	 i:4 	 global-step:24324	 l-p:0.11134418845176697
epoch£º1216	 i:5 	 global-step:24325	 l-p:0.0880923792719841
epoch£º1216	 i:6 	 global-step:24326	 l-p:0.12395325303077698
epoch£º1216	 i:7 	 global-step:24327	 l-p:0.15177024900913239
epoch£º1216	 i:8 	 global-step:24328	 l-p:0.24673183262348175
epoch£º1216	 i:9 	 global-step:24329	 l-p:0.16821803152561188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1600, 5.1568, 5.1598],
        [5.1600, 4.8625, 4.7275],
        [5.1600, 5.0058, 4.6585],
        [5.1600, 4.9728, 5.0194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1217, step:0 
model_pd.l_p.mean(): 0.15545308589935303 
model_pd.l_d.mean(): -19.491363525390625 
model_pd.lagr.mean(): -19.33591079711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5407], device='cuda:0')), ('power', tensor([-20.2567], device='cuda:0'))])
epoch£º1217	 i:0 	 global-step:24340	 l-p:0.15545308589935303
epoch£º1217	 i:1 	 global-step:24341	 l-p:0.13063453137874603
epoch£º1217	 i:2 	 global-step:24342	 l-p:0.1420026421546936
epoch£º1217	 i:3 	 global-step:24343	 l-p:0.17954784631729126
epoch£º1217	 i:4 	 global-step:24344	 l-p:0.1134699136018753
epoch£º1217	 i:5 	 global-step:24345	 l-p:0.2149464190006256
epoch£º1217	 i:6 	 global-step:24346	 l-p:0.1497391313314438
epoch£º1217	 i:7 	 global-step:24347	 l-p:0.12910424172878265
epoch£º1217	 i:8 	 global-step:24348	 l-p:0.056845538318157196
epoch£º1217	 i:9 	 global-step:24349	 l-p:0.15647226572036743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1638, 4.9653, 4.6230],
        [5.1638, 4.8622, 4.6828],
        [5.1638, 5.1638, 5.1638],
        [5.1638, 5.0470, 5.1079]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1218, step:0 
model_pd.l_p.mean(): 0.1708643138408661 
model_pd.l_d.mean(): -19.899747848510742 
model_pd.lagr.mean(): -19.728883743286133 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5005], device='cuda:0')), ('power', tensor([-20.6285], device='cuda:0'))])
epoch£º1218	 i:0 	 global-step:24360	 l-p:0.1708643138408661
epoch£º1218	 i:1 	 global-step:24361	 l-p:0.16111697256565094
epoch£º1218	 i:2 	 global-step:24362	 l-p:0.1726752072572708
epoch£º1218	 i:3 	 global-step:24363	 l-p:0.1423744410276413
epoch£º1218	 i:4 	 global-step:24364	 l-p:0.17530891299247742
epoch£º1218	 i:5 	 global-step:24365	 l-p:0.0736483782529831
epoch£º1218	 i:6 	 global-step:24366	 l-p:0.15018729865550995
epoch£º1218	 i:7 	 global-step:24367	 l-p:0.10276981443166733
epoch£º1218	 i:8 	 global-step:24368	 l-p:0.13069604337215424
epoch£º1218	 i:9 	 global-step:24369	 l-p:0.13224180042743683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1664, 4.8657, 4.6582],
        [5.1664, 5.0820, 5.1359],
        [5.1664, 5.1269, 5.1585],
        [5.1664, 5.1664, 5.1664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1219, step:0 
model_pd.l_p.mean(): 0.13197900354862213 
model_pd.l_d.mean(): -19.33940315246582 
model_pd.lagr.mean(): -19.20742416381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5069], device='cuda:0')), ('power', tensor([-20.0686], device='cuda:0'))])
epoch£º1219	 i:0 	 global-step:24380	 l-p:0.13197900354862213
epoch£º1219	 i:1 	 global-step:24381	 l-p:0.13253408670425415
epoch£º1219	 i:2 	 global-step:24382	 l-p:0.0714716911315918
epoch£º1219	 i:3 	 global-step:24383	 l-p:0.1748550683259964
epoch£º1219	 i:4 	 global-step:24384	 l-p:0.11264990270137787
epoch£º1219	 i:5 	 global-step:24385	 l-p:0.1774127185344696
epoch£º1219	 i:6 	 global-step:24386	 l-p:0.16756807267665863
epoch£º1219	 i:7 	 global-step:24387	 l-p:0.12120243906974792
epoch£º1219	 i:8 	 global-step:24388	 l-p:0.208809956908226
epoch£º1219	 i:9 	 global-step:24389	 l-p:0.11039967089891434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1668, 5.1667, 5.1668],
        [5.1668, 5.3836, 5.1800],
        [5.1668, 5.0751, 4.7357],
        [5.1668, 4.9894, 4.6437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1220, step:0 
model_pd.l_p.mean(): 0.17849792540073395 
model_pd.l_d.mean(): -20.754459381103516 
model_pd.lagr.mean(): -20.57596206665039 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4272], device='cuda:0')), ('power', tensor([-21.4177], device='cuda:0'))])
epoch£º1220	 i:0 	 global-step:24400	 l-p:0.17849792540073395
epoch£º1220	 i:1 	 global-step:24401	 l-p:0.1371774524450302
epoch£º1220	 i:2 	 global-step:24402	 l-p:0.12903645634651184
epoch£º1220	 i:3 	 global-step:24403	 l-p:0.12298251688480377
epoch£º1220	 i:4 	 global-step:24404	 l-p:0.1885826140642166
epoch£º1220	 i:5 	 global-step:24405	 l-p:0.12107565253973007
epoch£º1220	 i:6 	 global-step:24406	 l-p:0.07594460248947144
epoch£º1220	 i:7 	 global-step:24407	 l-p:0.14281755685806274
epoch£º1220	 i:8 	 global-step:24408	 l-p:0.13298794627189636
epoch£º1220	 i:9 	 global-step:24409	 l-p:0.18064239621162415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1646, 5.1635, 5.1645],
        [5.1646, 5.1645, 5.1646],
        [5.1646, 5.1373, 4.8162],
        [5.1646, 4.8632, 4.6625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1221, step:0 
model_pd.l_p.mean(): 0.11056873202323914 
model_pd.l_d.mean(): -20.423419952392578 
model_pd.lagr.mean(): -20.312850952148438 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4852], device='cuda:0')), ('power', tensor([-21.1423], device='cuda:0'))])
epoch£º1221	 i:0 	 global-step:24420	 l-p:0.11056873202323914
epoch£º1221	 i:1 	 global-step:24421	 l-p:0.10976787656545639
epoch£º1221	 i:2 	 global-step:24422	 l-p:0.1545330435037613
epoch£º1221	 i:3 	 global-step:24423	 l-p:0.1764908879995346
epoch£º1221	 i:4 	 global-step:24424	 l-p:0.16585411131381989
epoch£º1221	 i:5 	 global-step:24425	 l-p:0.1341152936220169
epoch£º1221	 i:6 	 global-step:24426	 l-p:0.15348434448242188
epoch£º1221	 i:7 	 global-step:24427	 l-p:0.18437692523002625
epoch£º1221	 i:8 	 global-step:24428	 l-p:0.11026113480329514
epoch£º1221	 i:9 	 global-step:24429	 l-p:0.12398677319288254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1222
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1619, 5.1619, 5.1619],
        [5.1619, 5.1055, 5.1472],
        [5.1619, 5.1449, 5.1600],
        [5.1619, 5.1613, 5.1619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1222, step:0 
model_pd.l_p.mean(): 0.13512876629829407 
model_pd.l_d.mean(): -20.79899787902832 
model_pd.lagr.mean(): -20.663869857788086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4126], device='cuda:0')), ('power', tensor([-21.4478], device='cuda:0'))])
epoch£º1222	 i:0 	 global-step:24440	 l-p:0.13512876629829407
epoch£º1222	 i:1 	 global-step:24441	 l-p:0.13249409198760986
epoch£º1222	 i:2 	 global-step:24442	 l-p:0.1390060931444168
epoch£º1222	 i:3 	 global-step:24443	 l-p:0.10845688730478287
epoch£º1222	 i:4 	 global-step:24444	 l-p:0.11537859588861465
epoch£º1222	 i:5 	 global-step:24445	 l-p:0.15246263146400452
epoch£º1222	 i:6 	 global-step:24446	 l-p:0.1404912769794464
epoch£º1222	 i:7 	 global-step:24447	 l-p:0.16456511616706848
epoch£º1222	 i:8 	 global-step:24448	 l-p:0.2435123175382614
epoch£º1222	 i:9 	 global-step:24449	 l-p:0.13000279664993286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1544, 5.1034, 5.1421],
        [5.1544, 5.1404, 5.1530],
        [5.1544, 5.0737, 5.1263],
        [5.1544, 4.9319, 4.9525]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1223, step:0 
model_pd.l_p.mean(): 0.11750542372465134 
model_pd.l_d.mean(): -19.446531295776367 
model_pd.lagr.mean(): -19.329025268554688 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5045], device='cuda:0')), ('power', tensor([-20.1744], device='cuda:0'))])
epoch£º1223	 i:0 	 global-step:24460	 l-p:0.11750542372465134
epoch£º1223	 i:1 	 global-step:24461	 l-p:0.19887593388557434
epoch£º1223	 i:2 	 global-step:24462	 l-p:0.13955314457416534
epoch£º1223	 i:3 	 global-step:24463	 l-p:0.14870482683181763
epoch£º1223	 i:4 	 global-step:24464	 l-p:0.2033664584159851
epoch£º1223	 i:5 	 global-step:24465	 l-p:0.16613814234733582
epoch£º1223	 i:6 	 global-step:24466	 l-p:0.1206156313419342
epoch£º1223	 i:7 	 global-step:24467	 l-p:0.10804884880781174
epoch£º1223	 i:8 	 global-step:24468	 l-p:0.14024025201797485
epoch£º1223	 i:9 	 global-step:24469	 l-p:0.1311689019203186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1531, 5.0715, 5.1244],
        [5.1531, 5.1531, 5.1531],
        [5.1531, 5.5143, 5.3995],
        [5.1531, 4.8516, 4.6338]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1224, step:0 
model_pd.l_p.mean(): 0.21644456684589386 
model_pd.l_d.mean(): -19.18001937866211 
model_pd.lagr.mean(): -18.96357536315918 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5243], device='cuda:0')), ('power', tensor([-19.9252], device='cuda:0'))])
epoch£º1224	 i:0 	 global-step:24480	 l-p:0.21644456684589386
epoch£º1224	 i:1 	 global-step:24481	 l-p:0.16951212286949158
epoch£º1224	 i:2 	 global-step:24482	 l-p:0.13256710767745972
epoch£º1224	 i:3 	 global-step:24483	 l-p:0.1080753430724144
epoch£º1224	 i:4 	 global-step:24484	 l-p:0.20399276912212372
epoch£º1224	 i:5 	 global-step:24485	 l-p:0.1303396224975586
epoch£º1224	 i:6 	 global-step:24486	 l-p:0.13525480031967163
epoch£º1224	 i:7 	 global-step:24487	 l-p:0.15075556933879852
epoch£º1224	 i:8 	 global-step:24488	 l-p:0.09423379600048065
epoch£º1224	 i:9 	 global-step:24489	 l-p:0.13396261632442474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1530, 5.1301, 5.1499],
        [5.1530, 5.1528, 5.1530],
        [5.1530, 5.1344, 5.1508],
        [5.1530, 5.1530, 5.1530]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1225, step:0 
model_pd.l_p.mean(): 0.11017102748155594 
model_pd.l_d.mean(): -20.467119216918945 
model_pd.lagr.mean(): -20.356948852539062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4417], device='cuda:0')), ('power', tensor([-21.1420], device='cuda:0'))])
epoch£º1225	 i:0 	 global-step:24500	 l-p:0.11017102748155594
epoch£º1225	 i:1 	 global-step:24501	 l-p:0.22776858508586884
epoch£º1225	 i:2 	 global-step:24502	 l-p:0.13607585430145264
epoch£º1225	 i:3 	 global-step:24503	 l-p:0.12547948956489563
epoch£º1225	 i:4 	 global-step:24504	 l-p:0.12364514172077179
epoch£º1225	 i:5 	 global-step:24505	 l-p:0.12957663834095
epoch£º1225	 i:6 	 global-step:24506	 l-p:0.16957905888557434
epoch£º1225	 i:7 	 global-step:24507	 l-p:0.2067531943321228
epoch£º1225	 i:8 	 global-step:24508	 l-p:0.1145167127251625
epoch£º1225	 i:9 	 global-step:24509	 l-p:0.15104572474956512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1493, 5.0162, 5.0779],
        [5.1493, 4.8761, 4.5788],
        [5.1493, 5.1493, 5.1493],
        [5.1493, 5.1493, 5.1493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1226, step:0 
model_pd.l_p.mean(): 0.12767811119556427 
model_pd.l_d.mean(): -19.13213348388672 
model_pd.lagr.mean(): -19.00445556640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5732], device='cuda:0')), ('power', tensor([-19.9268], device='cuda:0'))])
epoch£º1226	 i:0 	 global-step:24520	 l-p:0.12767811119556427
epoch£º1226	 i:1 	 global-step:24521	 l-p:0.1673157513141632
epoch£º1226	 i:2 	 global-step:24522	 l-p:0.11693967878818512
epoch£º1226	 i:3 	 global-step:24523	 l-p:0.17882806062698364
epoch£º1226	 i:4 	 global-step:24524	 l-p:0.20930621027946472
epoch£º1226	 i:5 	 global-step:24525	 l-p:0.16654027998447418
epoch£º1226	 i:6 	 global-step:24526	 l-p:0.11790143698453903
epoch£º1226	 i:7 	 global-step:24527	 l-p:0.09796661138534546
epoch£º1226	 i:8 	 global-step:24528	 l-p:0.16022200882434845
epoch£º1226	 i:9 	 global-step:24529	 l-p:0.1579703539609909
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1498, 5.1442, 5.1495],
        [5.1498, 5.1486, 5.1498],
        [5.1498, 5.0584, 5.1146],
        [5.1498, 5.1498, 5.1498]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1227, step:0 
model_pd.l_p.mean(): 0.10071053355932236 
model_pd.l_d.mean(): -20.191938400268555 
model_pd.lagr.mean(): -20.091228485107422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5033], device='cuda:0')), ('power', tensor([-20.9268], device='cuda:0'))])
epoch£º1227	 i:0 	 global-step:24540	 l-p:0.10071053355932236
epoch£º1227	 i:1 	 global-step:24541	 l-p:0.1849110722541809
epoch£º1227	 i:2 	 global-step:24542	 l-p:0.12252140045166016
epoch£º1227	 i:3 	 global-step:24543	 l-p:0.14558643102645874
epoch£º1227	 i:4 	 global-step:24544	 l-p:0.09484731405973434
epoch£º1227	 i:5 	 global-step:24545	 l-p:0.14398373663425446
epoch£º1227	 i:6 	 global-step:24546	 l-p:0.17049306631088257
epoch£º1227	 i:7 	 global-step:24547	 l-p:0.21375194191932678
epoch£º1227	 i:8 	 global-step:24548	 l-p:0.14126025140285492
epoch£º1227	 i:9 	 global-step:24549	 l-p:0.17345155775547028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1524, 4.9888, 4.6407],
        [5.1524, 5.1413, 5.1515],
        [5.1524, 5.1492, 5.1523],
        [5.1524, 4.9702, 4.6236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1228, step:0 
model_pd.l_p.mean(): 0.13247641921043396 
model_pd.l_d.mean(): -20.512361526489258 
model_pd.lagr.mean(): -20.379884719848633 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4510], device='cuda:0')), ('power', tensor([-21.1972], device='cuda:0'))])
epoch£º1228	 i:0 	 global-step:24560	 l-p:0.13247641921043396
epoch£º1228	 i:1 	 global-step:24561	 l-p:0.14810802042484283
epoch£º1228	 i:2 	 global-step:24562	 l-p:0.1504976451396942
epoch£º1228	 i:3 	 global-step:24563	 l-p:0.12407448142766953
epoch£º1228	 i:4 	 global-step:24564	 l-p:0.13115836679935455
epoch£º1228	 i:5 	 global-step:24565	 l-p:0.17016726732254028
epoch£º1228	 i:6 	 global-step:24566	 l-p:0.09610622376203537
epoch£º1228	 i:7 	 global-step:24567	 l-p:0.2267444133758545
epoch£º1228	 i:8 	 global-step:24568	 l-p:0.1333695948123932
epoch£º1228	 i:9 	 global-step:24569	 l-p:0.15074783563613892
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1589, 5.1066, 5.1460],
        [5.1589, 5.1511, 5.1584],
        [5.1589, 4.8565, 4.6774],
        [5.1589, 4.8611, 4.7260]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1229, step:0 
model_pd.l_p.mean(): 0.09426447004079819 
model_pd.l_d.mean(): -19.38581085205078 
model_pd.lagr.mean(): -19.291545867919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4930], device='cuda:0')), ('power', tensor([-20.1013], device='cuda:0'))])
epoch£º1229	 i:0 	 global-step:24580	 l-p:0.09426447004079819
epoch£º1229	 i:1 	 global-step:24581	 l-p:0.20113198459148407
epoch£º1229	 i:2 	 global-step:24582	 l-p:0.12556886672973633
epoch£º1229	 i:3 	 global-step:24583	 l-p:0.13858714699745178
epoch£º1229	 i:4 	 global-step:24584	 l-p:0.19380897283554077
epoch£º1229	 i:5 	 global-step:24585	 l-p:0.11637496948242188
epoch£º1229	 i:6 	 global-step:24586	 l-p:0.130433589220047
epoch£º1229	 i:7 	 global-step:24587	 l-p:0.16276638209819794
epoch£º1229	 i:8 	 global-step:24588	 l-p:0.13477851450443268
epoch£º1229	 i:9 	 global-step:24589	 l-p:0.1290544718503952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1646, 5.1090, 5.1503],
        [5.1646, 5.1558, 5.1640],
        [5.1646, 5.1641, 5.1646],
        [5.1646, 5.0344, 4.6883]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1230, step:0 
model_pd.l_p.mean(): 0.12754122912883759 
model_pd.l_d.mean(): -20.926074981689453 
model_pd.lagr.mean(): -20.798534393310547 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3965], device='cuda:0')), ('power', tensor([-21.5598], device='cuda:0'))])
epoch£º1230	 i:0 	 global-step:24600	 l-p:0.12754122912883759
epoch£º1230	 i:1 	 global-step:24601	 l-p:0.14345288276672363
epoch£º1230	 i:2 	 global-step:24602	 l-p:0.20383593440055847
epoch£º1230	 i:3 	 global-step:24603	 l-p:0.12352141737937927
epoch£º1230	 i:4 	 global-step:24604	 l-p:0.12387017905712128
epoch£º1230	 i:5 	 global-step:24605	 l-p:0.10228999704122543
epoch£º1230	 i:6 	 global-step:24606	 l-p:0.13333459198474884
epoch£º1230	 i:7 	 global-step:24607	 l-p:0.19215862452983856
epoch£º1230	 i:8 	 global-step:24608	 l-p:0.11818310618400574
epoch£º1230	 i:9 	 global-step:24609	 l-p:0.1453324407339096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1642, 4.9137, 4.5954],
        [5.1642, 5.1642, 5.1642],
        [5.1642, 5.1642, 5.1642],
        [5.1642, 5.0298, 4.6833]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1231, step:0 
model_pd.l_p.mean(): 0.09260282665491104 
model_pd.l_d.mean(): -20.65206527709961 
model_pd.lagr.mean(): -20.55946159362793 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4347], device='cuda:0')), ('power', tensor([-21.3218], device='cuda:0'))])
epoch£º1231	 i:0 	 global-step:24620	 l-p:0.09260282665491104
epoch£º1231	 i:1 	 global-step:24621	 l-p:0.13638213276863098
epoch£º1231	 i:2 	 global-step:24622	 l-p:0.20715120434761047
epoch£º1231	 i:3 	 global-step:24623	 l-p:0.15120474994182587
epoch£º1231	 i:4 	 global-step:24624	 l-p:0.26076221466064453
epoch£º1231	 i:5 	 global-step:24625	 l-p:0.14813491702079773
epoch£º1231	 i:6 	 global-step:24626	 l-p:0.12218139320611954
epoch£º1231	 i:7 	 global-step:24627	 l-p:0.11820026487112045
epoch£º1231	 i:8 	 global-step:24628	 l-p:0.12716089189052582
epoch£º1231	 i:9 	 global-step:24629	 l-p:0.06898277997970581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1552, 4.8908, 4.5837],
        [5.1552, 5.1389, 5.1535],
        [5.1552, 4.8705, 4.5932],
        [5.1552, 4.8817, 4.8271]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1232, step:0 
model_pd.l_p.mean(): 0.17465294897556305 
model_pd.l_d.mean(): -20.794485092163086 
model_pd.lagr.mean(): -20.61983299255371 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4262], device='cuda:0')), ('power', tensor([-21.4571], device='cuda:0'))])
epoch£º1232	 i:0 	 global-step:24640	 l-p:0.17465294897556305
epoch£º1232	 i:1 	 global-step:24641	 l-p:0.07173886895179749
epoch£º1232	 i:2 	 global-step:24642	 l-p:0.1317133754491806
epoch£º1232	 i:3 	 global-step:24643	 l-p:0.15924988687038422
epoch£º1232	 i:4 	 global-step:24644	 l-p:0.15171019732952118
epoch£º1232	 i:5 	 global-step:24645	 l-p:0.13718940317630768
epoch£º1232	 i:6 	 global-step:24646	 l-p:0.25725269317626953
epoch£º1232	 i:7 	 global-step:24647	 l-p:0.1402927041053772
epoch£º1232	 i:8 	 global-step:24648	 l-p:0.14215119183063507
epoch£º1232	 i:9 	 global-step:24649	 l-p:0.12486112117767334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1474, 4.8616, 4.7779],
        [5.1474, 5.0238, 4.6775],
        [5.1474, 5.1474, 5.1474],
        [5.1474, 5.1382, 5.1467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1233, step:0 
model_pd.l_p.mean(): 0.10920400172472 
model_pd.l_d.mean(): -20.611860275268555 
model_pd.lagr.mean(): -20.502656936645508 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4333], device='cuda:0')), ('power', tensor([-21.2798], device='cuda:0'))])
epoch£º1233	 i:0 	 global-step:24660	 l-p:0.10920400172472
epoch£º1233	 i:1 	 global-step:24661	 l-p:0.11758239567279816
epoch£º1233	 i:2 	 global-step:24662	 l-p:0.09801702201366425
epoch£º1233	 i:3 	 global-step:24663	 l-p:0.12959353625774384
epoch£º1233	 i:4 	 global-step:24664	 l-p:0.19733013212680817
epoch£º1233	 i:5 	 global-step:24665	 l-p:0.27299925684928894
epoch£º1233	 i:6 	 global-step:24666	 l-p:0.2573959529399872
epoch£º1233	 i:7 	 global-step:24667	 l-p:0.11699412763118744
epoch£º1233	 i:8 	 global-step:24668	 l-p:0.15767605602741241
epoch£º1233	 i:9 	 global-step:24669	 l-p:0.10401508957147598
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1411, 5.1394, 5.1411],
        [5.1411, 5.1411, 5.1411],
        [5.1411, 5.1411, 5.1411],
        [5.1411, 4.8693, 4.8206]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1234, step:0 
model_pd.l_p.mean(): 0.1271803379058838 
model_pd.l_d.mean(): -20.77033233642578 
model_pd.lagr.mean(): -20.643152236938477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4050], device='cuda:0')), ('power', tensor([-21.4110], device='cuda:0'))])
epoch£º1234	 i:0 	 global-step:24680	 l-p:0.1271803379058838
epoch£º1234	 i:1 	 global-step:24681	 l-p:0.1189856305718422
epoch£º1234	 i:2 	 global-step:24682	 l-p:0.11979333311319351
epoch£º1234	 i:3 	 global-step:24683	 l-p:0.15740682184696198
epoch£º1234	 i:4 	 global-step:24684	 l-p:0.10915359109640121
epoch£º1234	 i:5 	 global-step:24685	 l-p:0.2625133693218231
epoch£º1234	 i:6 	 global-step:24686	 l-p:0.36728763580322266
epoch£º1234	 i:7 	 global-step:24687	 l-p:0.11011788249015808
epoch£º1234	 i:8 	 global-step:24688	 l-p:0.10613063722848892
epoch£º1234	 i:9 	 global-step:24689	 l-p:0.12308771163225174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1361, 5.0921, 5.1267],
        [5.1361, 5.1330, 5.1360],
        [5.1361, 5.1006, 5.1296],
        [5.1361, 5.1277, 5.1355]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1235, step:0 
model_pd.l_p.mean(): 0.14007242023944855 
model_pd.l_d.mean(): -20.71497917175293 
model_pd.lagr.mean(): -20.574907302856445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4187], device='cuda:0')), ('power', tensor([-21.3691], device='cuda:0'))])
epoch£º1235	 i:0 	 global-step:24700	 l-p:0.14007242023944855
epoch£º1235	 i:1 	 global-step:24701	 l-p:0.19685880839824677
epoch£º1235	 i:2 	 global-step:24702	 l-p:0.21453288197517395
epoch£º1235	 i:3 	 global-step:24703	 l-p:0.30799734592437744
epoch£º1235	 i:4 	 global-step:24704	 l-p:0.10929912328720093
epoch£º1235	 i:5 	 global-step:24705	 l-p:0.11934791505336761
epoch£º1235	 i:6 	 global-step:24706	 l-p:0.093778096139431
epoch£º1235	 i:7 	 global-step:24707	 l-p:0.0974094346165657
epoch£º1235	 i:8 	 global-step:24708	 l-p:0.13281705975532532
epoch£º1235	 i:9 	 global-step:24709	 l-p:0.23519164323806763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1333, 4.8581, 4.8038],
        [5.1333, 4.9611, 5.0156],
        [5.1333, 4.9673, 5.0238],
        [5.1333, 5.1333, 5.1333]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1236, step:0 
model_pd.l_p.mean(): 0.09480361640453339 
model_pd.l_d.mean(): -20.320465087890625 
model_pd.lagr.mean(): -20.225662231445312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4991], device='cuda:0')), ('power', tensor([-21.0524], device='cuda:0'))])
epoch£º1236	 i:0 	 global-step:24720	 l-p:0.09480361640453339
epoch£º1236	 i:1 	 global-step:24721	 l-p:0.13528643548488617
epoch£º1236	 i:2 	 global-step:24722	 l-p:0.21207767724990845
epoch£º1236	 i:3 	 global-step:24723	 l-p:0.12318359315395355
epoch£º1236	 i:4 	 global-step:24724	 l-p:0.1755337119102478
epoch£º1236	 i:5 	 global-step:24725	 l-p:0.3391263782978058
epoch£º1236	 i:6 	 global-step:24726	 l-p:0.14058610796928406
epoch£º1236	 i:7 	 global-step:24727	 l-p:0.13984470069408417
epoch£º1236	 i:8 	 global-step:24728	 l-p:0.19867530465126038
epoch£º1236	 i:9 	 global-step:24729	 l-p:0.10680738091468811
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1369, 5.1370, 5.1369],
        [5.1369, 5.1369, 5.1370],
        [5.1369, 5.1365, 5.1369],
        [5.1369, 5.0910, 5.1267]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1237, step:0 
model_pd.l_p.mean(): 0.12279502302408218 
model_pd.l_d.mean(): -20.04400634765625 
model_pd.lagr.mean(): -19.92121124267578 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4630], device='cuda:0')), ('power', tensor([-20.7360], device='cuda:0'))])
epoch£º1237	 i:0 	 global-step:24740	 l-p:0.12279502302408218
epoch£º1237	 i:1 	 global-step:24741	 l-p:0.24822120368480682
epoch£º1237	 i:2 	 global-step:24742	 l-p:0.13494132459163666
epoch£º1237	 i:3 	 global-step:24743	 l-p:0.10234873741865158
epoch£º1237	 i:4 	 global-step:24744	 l-p:0.14445044100284576
epoch£º1237	 i:5 	 global-step:24745	 l-p:0.1708509922027588
epoch£º1237	 i:6 	 global-step:24746	 l-p:0.1509217917919159
epoch£º1237	 i:7 	 global-step:24747	 l-p:0.09703526645898819
epoch£º1237	 i:8 	 global-step:24748	 l-p:0.10548174381256104
epoch£º1237	 i:9 	 global-step:24749	 l-p:0.32291290163993835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1397, 4.8347, 4.6544],
        [5.1397, 4.8677, 4.8192],
        [5.1397, 5.0749, 5.1209],
        [5.1397, 5.0935, 5.1294]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1238, step:0 
model_pd.l_p.mean(): 0.12949499487876892 
model_pd.l_d.mean(): -21.109655380249023 
model_pd.lagr.mean(): -20.980159759521484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3618], device='cuda:0')), ('power', tensor([-21.7099], device='cuda:0'))])
epoch£º1238	 i:0 	 global-step:24760	 l-p:0.12949499487876892
epoch£º1238	 i:1 	 global-step:24761	 l-p:0.11015024036169052
epoch£º1238	 i:2 	 global-step:24762	 l-p:0.1512737274169922
epoch£º1238	 i:3 	 global-step:24763	 l-p:0.15852393209934235
epoch£º1238	 i:4 	 global-step:24764	 l-p:0.1596423238515854
epoch£º1238	 i:5 	 global-step:24765	 l-p:0.13036522269248962
epoch£º1238	 i:6 	 global-step:24766	 l-p:0.32081007957458496
epoch£º1238	 i:7 	 global-step:24767	 l-p:0.1411474347114563
epoch£º1238	 i:8 	 global-step:24768	 l-p:0.14137664437294006
epoch£º1238	 i:9 	 global-step:24769	 l-p:0.11684173345565796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1458, 5.1453, 5.1458],
        [5.1458, 5.1379, 5.1453],
        [5.1458, 5.2511, 4.9873],
        [5.1458, 5.1324, 5.1445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1239, step:0 
model_pd.l_p.mean(): 0.12059089541435242 
model_pd.l_d.mean(): -20.972036361694336 
model_pd.lagr.mean(): -20.8514461517334 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3918], device='cuda:0')), ('power', tensor([-21.6015], device='cuda:0'))])
epoch£º1239	 i:0 	 global-step:24780	 l-p:0.12059089541435242
epoch£º1239	 i:1 	 global-step:24781	 l-p:0.13799132406711578
epoch£º1239	 i:2 	 global-step:24782	 l-p:0.12542931735515594
epoch£º1239	 i:3 	 global-step:24783	 l-p:0.1307043880224228
epoch£º1239	 i:4 	 global-step:24784	 l-p:0.1079757884144783
epoch£º1239	 i:5 	 global-step:24785	 l-p:0.35192447900772095
epoch£º1239	 i:6 	 global-step:24786	 l-p:0.11885110288858414
epoch£º1239	 i:7 	 global-step:24787	 l-p:0.1898558884859085
epoch£º1239	 i:8 	 global-step:24788	 l-p:0.1284400224685669
epoch£º1239	 i:9 	 global-step:24789	 l-p:0.12888377904891968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1438, 5.5261, 5.4250],
        [5.1438, 5.4213, 5.2538],
        [5.1438, 5.0220, 5.0837],
        [5.1438, 4.9210, 4.5831]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1240, step:0 
model_pd.l_p.mean(): 0.12168526649475098 
model_pd.l_d.mean(): -20.13517951965332 
model_pd.lagr.mean(): -20.01349449157715 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4995], device='cuda:0')), ('power', tensor([-20.8655], device='cuda:0'))])
epoch£º1240	 i:0 	 global-step:24800	 l-p:0.12168526649475098
epoch£º1240	 i:1 	 global-step:24801	 l-p:0.24362891912460327
epoch£º1240	 i:2 	 global-step:24802	 l-p:0.18574409186840057
epoch£º1240	 i:3 	 global-step:24803	 l-p:0.13266980648040771
epoch£º1240	 i:4 	 global-step:24804	 l-p:0.13859564065933228
epoch£º1240	 i:5 	 global-step:24805	 l-p:0.11656706780195236
epoch£º1240	 i:6 	 global-step:24806	 l-p:0.12980657815933228
epoch£º1240	 i:7 	 global-step:24807	 l-p:0.1723029613494873
epoch£º1240	 i:8 	 global-step:24808	 l-p:0.16184455156326294
epoch£º1240	 i:9 	 global-step:24809	 l-p:0.13995444774627686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1458, 5.0172, 5.0791],
        [5.1458, 4.8558, 4.7601],
        [5.1458, 4.9367, 4.9699],
        [5.1458, 5.0774, 5.1251]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1241, step:0 
model_pd.l_p.mean(): 0.1424795538187027 
model_pd.l_d.mean(): -18.601411819458008 
model_pd.lagr.mean(): -18.458932876586914 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6014], device='cuda:0')), ('power', tensor([-19.4191], device='cuda:0'))])
epoch£º1241	 i:0 	 global-step:24820	 l-p:0.1424795538187027
epoch£º1241	 i:1 	 global-step:24821	 l-p:0.16245023906230927
epoch£º1241	 i:2 	 global-step:24822	 l-p:0.2752423882484436
epoch£º1241	 i:3 	 global-step:24823	 l-p:0.12886594235897064
epoch£º1241	 i:4 	 global-step:24824	 l-p:0.12044593691825867
epoch£º1241	 i:5 	 global-step:24825	 l-p:0.18973681330680847
epoch£º1241	 i:6 	 global-step:24826	 l-p:0.12375516444444656
epoch£º1241	 i:7 	 global-step:24827	 l-p:0.09547615051269531
epoch£º1241	 i:8 	 global-step:24828	 l-p:0.1041938066482544
epoch£º1241	 i:9 	 global-step:24829	 l-p:0.18402321636676788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1470, 5.0234, 4.6769],
        [5.1470, 5.1368, 4.8215],
        [5.1470, 5.1470, 5.1470],
        [5.1470, 5.1427, 5.1468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1242, step:0 
model_pd.l_p.mean(): 0.18528027832508087 
model_pd.l_d.mean(): -20.153226852416992 
model_pd.lagr.mean(): -19.967947006225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5116], device='cuda:0')), ('power', tensor([-20.8961], device='cuda:0'))])
epoch£º1242	 i:0 	 global-step:24840	 l-p:0.18528027832508087
epoch£º1242	 i:1 	 global-step:24841	 l-p:0.11220327019691467
epoch£º1242	 i:2 	 global-step:24842	 l-p:0.14961160719394684
epoch£º1242	 i:3 	 global-step:24843	 l-p:0.09671815484762192
epoch£º1242	 i:4 	 global-step:24844	 l-p:0.1432041972875595
epoch£º1242	 i:5 	 global-step:24845	 l-p:0.1831953376531601
epoch£º1242	 i:6 	 global-step:24846	 l-p:0.222223162651062
epoch£º1242	 i:7 	 global-step:24847	 l-p:0.15483300387859344
epoch£º1242	 i:8 	 global-step:24848	 l-p:0.11580205708742142
epoch£º1242	 i:9 	 global-step:24849	 l-p:0.14663158357143402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1516, 5.1512, 5.1516],
        [5.1516, 5.0698, 5.1229],
        [5.1516, 5.0209, 5.0827],
        [5.1516, 4.8564, 4.7395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1243, step:0 
model_pd.l_p.mean(): 0.19026531279087067 
model_pd.l_d.mean(): -20.350830078125 
model_pd.lagr.mean(): -20.160564422607422 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4777], device='cuda:0')), ('power', tensor([-21.0613], device='cuda:0'))])
epoch£º1243	 i:0 	 global-step:24860	 l-p:0.19026531279087067
epoch£º1243	 i:1 	 global-step:24861	 l-p:0.1411154568195343
epoch£º1243	 i:2 	 global-step:24862	 l-p:0.12847478687763214
epoch£º1243	 i:3 	 global-step:24863	 l-p:0.12322463095188141
epoch£º1243	 i:4 	 global-step:24864	 l-p:0.17443187534809113
epoch£º1243	 i:5 	 global-step:24865	 l-p:0.12791749835014343
epoch£º1243	 i:6 	 global-step:24866	 l-p:0.13476358354091644
epoch£º1243	 i:7 	 global-step:24867	 l-p:0.1605328768491745
epoch£º1243	 i:8 	 global-step:24868	 l-p:0.13392281532287598
epoch£º1243	 i:9 	 global-step:24869	 l-p:0.16599977016448975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1562, 5.0846, 5.1336],
        [5.1562, 5.0045, 5.0641],
        [5.1562, 4.9238, 4.9342],
        [5.1562, 5.1562, 5.1562]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1244, step:0 
model_pd.l_p.mean(): 0.18000930547714233 
model_pd.l_d.mean(): -20.313152313232422 
model_pd.lagr.mean(): -20.133142471313477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4309], device='cuda:0')), ('power', tensor([-20.9754], device='cuda:0'))])
epoch£º1244	 i:0 	 global-step:24880	 l-p:0.18000930547714233
epoch£º1244	 i:1 	 global-step:24881	 l-p:0.14069509506225586
epoch£º1244	 i:2 	 global-step:24882	 l-p:0.2541189193725586
epoch£º1244	 i:3 	 global-step:24883	 l-p:0.134261816740036
epoch£º1244	 i:4 	 global-step:24884	 l-p:0.12117141485214233
epoch£º1244	 i:5 	 global-step:24885	 l-p:0.11651506274938583
epoch£º1244	 i:6 	 global-step:24886	 l-p:0.13681238889694214
epoch£º1244	 i:7 	 global-step:24887	 l-p:0.14511941373348236
epoch£º1244	 i:8 	 global-step:24888	 l-p:0.07987184822559357
epoch£º1244	 i:9 	 global-step:24889	 l-p:0.13983391225337982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1245
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6075,  0.5145,  1.0000,  0.4357,
          1.0000,  0.8469, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[5.1624, 5.0359, 4.6898],
        [5.1624, 5.0087, 4.6608],
        [5.1624, 5.2106, 4.9195],
        [5.1624, 4.9058, 4.5917]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1245, step:0 
model_pd.l_p.mean(): 0.1657911241054535 
model_pd.l_d.mean(): -20.58956527709961 
model_pd.lagr.mean(): -20.42377471923828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4622], device='cuda:0')), ('power', tensor([-21.2868], device='cuda:0'))])
epoch£º1245	 i:0 	 global-step:24900	 l-p:0.1657911241054535
epoch£º1245	 i:1 	 global-step:24901	 l-p:0.23464366793632507
epoch£º1245	 i:2 	 global-step:24902	 l-p:0.11435364186763763
epoch£º1245	 i:3 	 global-step:24903	 l-p:0.06407982856035233
epoch£º1245	 i:4 	 global-step:24904	 l-p:0.16483764350414276
epoch£º1245	 i:5 	 global-step:24905	 l-p:0.16567513346672058
epoch£º1245	 i:6 	 global-step:24906	 l-p:0.12895818054676056
epoch£º1245	 i:7 	 global-step:24907	 l-p:0.10358859598636627
epoch£º1245	 i:8 	 global-step:24908	 l-p:0.15885409712791443
epoch£º1245	 i:9 	 global-step:24909	 l-p:0.12388338148593903
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1632, 4.9066, 4.5925],
        [5.1632, 5.1632, 5.1632],
        [5.1632, 5.1191, 5.1537],
        [5.1632, 5.1632, 5.1632]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1246, step:0 
model_pd.l_p.mean(): 0.21949335932731628 
model_pd.l_d.mean(): -20.1395206451416 
model_pd.lagr.mean(): -19.920026779174805 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5297], device='cuda:0')), ('power', tensor([-20.9008], device='cuda:0'))])
epoch£º1246	 i:0 	 global-step:24920	 l-p:0.21949335932731628
epoch£º1246	 i:1 	 global-step:24921	 l-p:0.11929576843976974
epoch£º1246	 i:2 	 global-step:24922	 l-p:0.14571033418178558
epoch£º1246	 i:3 	 global-step:24923	 l-p:0.1343892514705658
epoch£º1246	 i:4 	 global-step:24924	 l-p:0.13251779973506927
epoch£º1246	 i:5 	 global-step:24925	 l-p:0.11566159129142761
epoch£º1246	 i:6 	 global-step:24926	 l-p:0.11753253638744354
epoch£º1246	 i:7 	 global-step:24927	 l-p:0.11944025009870529
epoch£º1246	 i:8 	 global-step:24928	 l-p:0.1548953354358673
epoch£º1246	 i:9 	 global-step:24929	 l-p:0.17603009939193726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1584, 4.8578, 4.7083],
        [5.1584, 5.1584, 5.1584],
        [5.1584, 4.9843, 5.0374],
        [5.1584, 5.2672, 5.0049]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1247, step:0 
model_pd.l_p.mean(): 0.1579647660255432 
model_pd.l_d.mean(): -20.06709861755371 
model_pd.lagr.mean(): -19.909133911132812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4865], device='cuda:0')), ('power', tensor([-20.7834], device='cuda:0'))])
epoch£º1247	 i:0 	 global-step:24940	 l-p:0.1579647660255432
epoch£º1247	 i:1 	 global-step:24941	 l-p:0.11898116767406464
epoch£º1247	 i:2 	 global-step:24942	 l-p:0.1078290343284607
epoch£º1247	 i:3 	 global-step:24943	 l-p:0.153671532869339
epoch£º1247	 i:4 	 global-step:24944	 l-p:0.15902134776115417
epoch£º1247	 i:5 	 global-step:24945	 l-p:0.12154801934957504
epoch£º1247	 i:6 	 global-step:24946	 l-p:0.1701449602842331
epoch£º1247	 i:7 	 global-step:24947	 l-p:0.1383761614561081
epoch£º1247	 i:8 	 global-step:24948	 l-p:0.1440323293209076
epoch£º1247	 i:9 	 global-step:24949	 l-p:0.17801795899868011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1588, 5.0674, 5.1236],
        [5.1588, 5.1588, 5.1588],
        [5.1588, 4.8742, 4.7920],
        [5.1588, 4.8564, 4.6889]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1248, step:0 
model_pd.l_p.mean(): 0.13421577215194702 
model_pd.l_d.mean(): -21.03441619873047 
model_pd.lagr.mean(): -20.90019989013672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3805], device='cuda:0')), ('power', tensor([-21.6530], device='cuda:0'))])
epoch£º1248	 i:0 	 global-step:24960	 l-p:0.13421577215194702
epoch£º1248	 i:1 	 global-step:24961	 l-p:0.17118318378925323
epoch£º1248	 i:2 	 global-step:24962	 l-p:0.12793666124343872
epoch£º1248	 i:3 	 global-step:24963	 l-p:0.13443630933761597
epoch£º1248	 i:4 	 global-step:24964	 l-p:0.2171000838279724
epoch£º1248	 i:5 	 global-step:24965	 l-p:0.17529092729091644
epoch£º1248	 i:6 	 global-step:24966	 l-p:0.13269385695457458
epoch£º1248	 i:7 	 global-step:24967	 l-p:0.15571127831935883
epoch£º1248	 i:8 	 global-step:24968	 l-p:0.07916846126317978
epoch£º1248	 i:9 	 global-step:24969	 l-p:0.12115547806024551
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1597, 4.8580, 4.6413],
        [5.1597, 5.1597, 5.1597],
        [5.1597, 4.8794, 4.5938],
        [5.1597, 5.1597, 5.1597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1249, step:0 
model_pd.l_p.mean(): 0.1777859479188919 
model_pd.l_d.mean(): -20.322011947631836 
model_pd.lagr.mean(): -20.14422607421875 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4855], device='cuda:0')), ('power', tensor([-21.0400], device='cuda:0'))])
epoch£º1249	 i:0 	 global-step:24980	 l-p:0.1777859479188919
epoch£º1249	 i:1 	 global-step:24981	 l-p:0.08948326855897903
epoch£º1249	 i:2 	 global-step:24982	 l-p:0.1878884881734848
epoch£º1249	 i:3 	 global-step:24983	 l-p:0.16512641310691833
epoch£º1249	 i:4 	 global-step:24984	 l-p:0.10123206675052643
epoch£º1249	 i:5 	 global-step:24985	 l-p:0.17574091255664825
epoch£º1249	 i:6 	 global-step:24986	 l-p:0.12444468587636948
epoch£º1249	 i:7 	 global-step:24987	 l-p:0.16457362473011017
epoch£º1249	 i:8 	 global-step:24988	 l-p:0.10973096638917923
epoch£º1249	 i:9 	 global-step:24989	 l-p:0.1454586535692215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1250
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1828,  0.1038,  1.0000,  0.0589,
          1.0000,  0.5675, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1353,  0.0695,  1.0000,  0.0357,
          1.0000,  0.5134, 31.6228]], device='cuda:0')
 pt:tensor([[5.1616, 4.9531, 4.9860],
        [5.1616, 4.9289, 4.9387],
        [5.1616, 5.0069, 5.0659],
        [5.1616, 5.0137, 5.0740]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1250, step:0 
model_pd.l_p.mean(): 0.1333114355802536 
model_pd.l_d.mean(): -20.393768310546875 
model_pd.lagr.mean(): -20.260456085205078 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4518], device='cuda:0')), ('power', tensor([-21.0782], device='cuda:0'))])
epoch£º1250	 i:0 	 global-step:25000	 l-p:0.1333114355802536
epoch£º1250	 i:1 	 global-step:25001	 l-p:0.17402522265911102
epoch£º1250	 i:2 	 global-step:25002	 l-p:0.1360589563846588
epoch£º1250	 i:3 	 global-step:25003	 l-p:0.12868407368659973
epoch£º1250	 i:4 	 global-step:25004	 l-p:0.12920647859573364
epoch£º1250	 i:5 	 global-step:25005	 l-p:0.2265641689300537
epoch£º1250	 i:6 	 global-step:25006	 l-p:0.07665670663118362
epoch£º1250	 i:7 	 global-step:25007	 l-p:0.12253466993570328
epoch£º1250	 i:8 	 global-step:25008	 l-p:0.11102865636348724
epoch£º1250	 i:9 	 global-step:25009	 l-p:0.20560960471630096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1594, 4.8747, 4.7923],
        [5.1594, 5.1585, 5.1594],
        [5.1594, 5.1594, 5.1594],
        [5.1594, 4.9508, 4.9837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1251, step:0 
model_pd.l_p.mean(): 0.09621766209602356 
model_pd.l_d.mean(): -20.092483520507812 
model_pd.lagr.mean(): -19.996265411376953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4974], device='cuda:0')), ('power', tensor([-20.8202], device='cuda:0'))])
epoch£º1251	 i:0 	 global-step:25020	 l-p:0.09621766209602356
epoch£º1251	 i:1 	 global-step:25021	 l-p:0.1329316794872284
epoch£º1251	 i:2 	 global-step:25022	 l-p:0.19077923893928528
epoch£º1251	 i:3 	 global-step:25023	 l-p:0.13267876207828522
epoch£º1251	 i:4 	 global-step:25024	 l-p:0.1786230355501175
epoch£º1251	 i:5 	 global-step:25025	 l-p:0.11390548199415207
epoch£º1251	 i:6 	 global-step:25026	 l-p:0.11369633674621582
epoch£º1251	 i:7 	 global-step:25027	 l-p:0.2362203598022461
epoch£º1251	 i:8 	 global-step:25028	 l-p:0.1223425567150116
epoch£º1251	 i:9 	 global-step:25029	 l-p:0.12678766250610352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1603, 5.0124, 5.0727],
        [5.1603, 4.9796, 4.6327],
        [5.1603, 5.1603, 5.1603],
        [5.1603, 4.8840, 4.5921]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1252, step:0 
model_pd.l_p.mean(): 0.16066768765449524 
model_pd.l_d.mean(): -20.099830627441406 
model_pd.lagr.mean(): -19.939163208007812 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5084], device='cuda:0')), ('power', tensor([-20.8389], device='cuda:0'))])
epoch£º1252	 i:0 	 global-step:25040	 l-p:0.16066768765449524
epoch£º1252	 i:1 	 global-step:25041	 l-p:0.21367621421813965
epoch£º1252	 i:2 	 global-step:25042	 l-p:0.13875330984592438
epoch£º1252	 i:3 	 global-step:25043	 l-p:0.12421073019504547
epoch£º1252	 i:4 	 global-step:25044	 l-p:0.1108936071395874
epoch£º1252	 i:5 	 global-step:25045	 l-p:0.15207096934318542
epoch£º1252	 i:6 	 global-step:25046	 l-p:0.10045280307531357
epoch£º1252	 i:7 	 global-step:25047	 l-p:0.16644969582557678
epoch£º1252	 i:8 	 global-step:25048	 l-p:0.14482426643371582
epoch£º1252	 i:9 	 global-step:25049	 l-p:0.122169129550457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1620, 5.1620, 5.1620],
        [5.1620, 5.1614, 5.1620],
        [5.1620, 5.1620, 5.1620],
        [5.1620, 5.1620, 5.1620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1253, step:0 
model_pd.l_p.mean(): 0.12512387335300446 
model_pd.l_d.mean(): -20.216079711914062 
model_pd.lagr.mean(): -20.09095573425293 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4315], device='cuda:0')), ('power', tensor([-20.8778], device='cuda:0'))])
epoch£º1253	 i:0 	 global-step:25060	 l-p:0.12512387335300446
epoch£º1253	 i:1 	 global-step:25061	 l-p:0.13595961034297943
epoch£º1253	 i:2 	 global-step:25062	 l-p:0.1526690423488617
epoch£º1253	 i:3 	 global-step:25063	 l-p:0.1651298850774765
epoch£º1253	 i:4 	 global-step:25064	 l-p:0.16340550780296326
epoch£º1253	 i:5 	 global-step:25065	 l-p:0.1339510828256607
epoch£º1253	 i:6 	 global-step:25066	 l-p:0.09829123318195343
epoch£º1253	 i:7 	 global-step:25067	 l-p:0.09305126965045929
epoch£º1253	 i:8 	 global-step:25068	 l-p:0.11355534940958023
epoch£º1253	 i:9 	 global-step:25069	 l-p:0.2659291625022888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1582, 5.4133, 5.2318],
        [5.1582, 4.9841, 5.0372],
        [5.1582, 5.1551, 5.1581],
        [5.1582, 4.8558, 4.6910]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1254, step:0 
model_pd.l_p.mean(): 0.12367784231901169 
model_pd.l_d.mean(): -19.303817749023438 
model_pd.lagr.mean(): -19.180139541625977 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5421], device='cuda:0')), ('power', tensor([-20.0686], device='cuda:0'))])
epoch£º1254	 i:0 	 global-step:25080	 l-p:0.12367784231901169
epoch£º1254	 i:1 	 global-step:25081	 l-p:0.10985743999481201
epoch£º1254	 i:2 	 global-step:25082	 l-p:0.11173199117183685
epoch£º1254	 i:3 	 global-step:25083	 l-p:0.11642738431692123
epoch£º1254	 i:4 	 global-step:25084	 l-p:0.142578586935997
epoch£º1254	 i:5 	 global-step:25085	 l-p:0.12265798449516296
epoch£º1254	 i:6 	 global-step:25086	 l-p:0.2666778862476349
epoch£º1254	 i:7 	 global-step:25087	 l-p:0.12349113821983337
epoch£º1254	 i:8 	 global-step:25088	 l-p:0.21669284999370575
epoch£º1254	 i:9 	 global-step:25089	 l-p:0.13134132325649261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1549, 4.9729, 5.0227],
        [5.1549, 4.8560, 4.7208],
        [5.1549, 5.0216, 5.0834],
        [5.1549, 5.1549, 5.1549]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1255, step:0 
model_pd.l_p.mean(): 0.1162550300359726 
model_pd.l_d.mean(): -19.218055725097656 
model_pd.lagr.mean(): -19.1018009185791 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5669], device='cuda:0')), ('power', tensor([-20.0073], device='cuda:0'))])
epoch£º1255	 i:0 	 global-step:25100	 l-p:0.1162550300359726
epoch£º1255	 i:1 	 global-step:25101	 l-p:0.18546922504901886
epoch£º1255	 i:2 	 global-step:25102	 l-p:0.12012657523155212
epoch£º1255	 i:3 	 global-step:25103	 l-p:0.18710418045520782
epoch£º1255	 i:4 	 global-step:25104	 l-p:0.11747582256793976
epoch£º1255	 i:5 	 global-step:25105	 l-p:0.20886877179145813
epoch£º1255	 i:6 	 global-step:25106	 l-p:0.16189372539520264
epoch£º1255	 i:7 	 global-step:25107	 l-p:0.12095243483781815
epoch£º1255	 i:8 	 global-step:25108	 l-p:0.14401188492774963
epoch£º1255	 i:9 	 global-step:25109	 l-p:0.10659212619066238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1562, 5.0536, 5.1126],
        [5.1562, 4.8777, 4.8125],
        [5.1562, 5.4287, 5.2578],
        [5.1562, 5.1475, 5.1556]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1256, step:0 
model_pd.l_p.mean(): 0.08797074854373932 
model_pd.l_d.mean(): -20.558441162109375 
model_pd.lagr.mean(): -20.470470428466797 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4434], device='cuda:0')), ('power', tensor([-21.2361], device='cuda:0'))])
epoch£º1256	 i:0 	 global-step:25120	 l-p:0.08797074854373932
epoch£º1256	 i:1 	 global-step:25121	 l-p:0.15026694536209106
epoch£º1256	 i:2 	 global-step:25122	 l-p:0.16815021634101868
epoch£º1256	 i:3 	 global-step:25123	 l-p:0.18892180919647217
epoch£º1256	 i:4 	 global-step:25124	 l-p:0.1746273785829544
epoch£º1256	 i:5 	 global-step:25125	 l-p:0.15261906385421753
epoch£º1256	 i:6 	 global-step:25126	 l-p:0.10239427536725998
epoch£º1256	 i:7 	 global-step:25127	 l-p:0.13228943943977356
epoch£º1256	 i:8 	 global-step:25128	 l-p:0.16392341256141663
epoch£º1256	 i:9 	 global-step:25129	 l-p:0.14558839797973633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1554, 4.8602, 4.7432],
        [5.1554, 5.0353, 4.6893],
        [5.1554, 5.1550, 5.1554],
        [5.1554, 5.1554, 5.1554]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1257, step:0 
model_pd.l_p.mean(): 0.11624524742364883 
model_pd.l_d.mean(): -20.429140090942383 
model_pd.lagr.mean(): -20.312894821166992 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4735], device='cuda:0')), ('power', tensor([-21.1361], device='cuda:0'))])
epoch£º1257	 i:0 	 global-step:25140	 l-p:0.11624524742364883
epoch£º1257	 i:1 	 global-step:25141	 l-p:0.2198743373155594
epoch£º1257	 i:2 	 global-step:25142	 l-p:0.11939474195241928
epoch£º1257	 i:3 	 global-step:25143	 l-p:0.2350878119468689
epoch£º1257	 i:4 	 global-step:25144	 l-p:0.10355019569396973
epoch£º1257	 i:5 	 global-step:25145	 l-p:0.11181283742189407
epoch£º1257	 i:6 	 global-step:25146	 l-p:0.15002113580703735
epoch£º1257	 i:7 	 global-step:25147	 l-p:0.10658223927021027
epoch£º1257	 i:8 	 global-step:25148	 l-p:0.1511734426021576
epoch£º1257	 i:9 	 global-step:25149	 l-p:0.1568109691143036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1546, 5.1542, 5.1546],
        [5.1546, 5.1546, 5.1546],
        [5.1546, 4.8942, 4.8663],
        [5.1546, 4.8663, 4.5941]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1258, step:0 
model_pd.l_p.mean(): 0.13549400866031647 
model_pd.l_d.mean(): -20.837474822998047 
model_pd.lagr.mean(): -20.701980590820312 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4177], device='cuda:0')), ('power', tensor([-21.4919], device='cuda:0'))])
epoch£º1258	 i:0 	 global-step:25160	 l-p:0.13549400866031647
epoch£º1258	 i:1 	 global-step:25161	 l-p:0.19425125420093536
epoch£º1258	 i:2 	 global-step:25162	 l-p:0.20140090584754944
epoch£º1258	 i:3 	 global-step:25163	 l-p:0.1293947696685791
epoch£º1258	 i:4 	 global-step:25164	 l-p:0.1213991641998291
epoch£º1258	 i:5 	 global-step:25165	 l-p:0.23069576919078827
epoch£º1258	 i:6 	 global-step:25166	 l-p:0.06684666126966476
epoch£º1258	 i:7 	 global-step:25167	 l-p:0.1222672164440155
epoch£º1258	 i:8 	 global-step:25168	 l-p:0.14520007371902466
epoch£º1258	 i:9 	 global-step:25169	 l-p:0.12266843765974045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1589, 5.1545, 5.1587],
        [5.1589, 4.8805, 4.8149],
        [5.1589, 5.1547, 5.1587],
        [5.1589, 4.9784, 4.6312]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1259, step:0 
model_pd.l_p.mean(): 0.14563192427158356 
model_pd.l_d.mean(): -20.00652503967285 
model_pd.lagr.mean(): -19.86089324951172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5156], device='cuda:0')), ('power', tensor([-20.7519], device='cuda:0'))])
epoch£º1259	 i:0 	 global-step:25180	 l-p:0.14563192427158356
epoch£º1259	 i:1 	 global-step:25181	 l-p:0.16434861719608307
epoch£º1259	 i:2 	 global-step:25182	 l-p:0.13473773002624512
epoch£º1259	 i:3 	 global-step:25183	 l-p:0.12102169543504715
epoch£º1259	 i:4 	 global-step:25184	 l-p:0.07922950387001038
epoch£º1259	 i:5 	 global-step:25185	 l-p:0.17776435613632202
epoch£º1259	 i:6 	 global-step:25186	 l-p:0.13098204135894775
epoch£º1259	 i:7 	 global-step:25187	 l-p:0.13797493278980255
epoch£º1259	 i:8 	 global-step:25188	 l-p:0.2016940414905548
epoch£º1259	 i:9 	 global-step:25189	 l-p:0.14535653591156006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1624, 5.1580, 5.1622],
        [5.1624, 4.8974, 4.8599],
        [5.1624, 5.1625, 5.1625],
        [5.1624, 5.0759, 5.1306]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1260, step:0 
model_pd.l_p.mean(): 0.16493602097034454 
model_pd.l_d.mean(): -20.498947143554688 
model_pd.lagr.mean(): -20.33401107788086 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4600], device='cuda:0')), ('power', tensor([-21.1929], device='cuda:0'))])
epoch£º1260	 i:0 	 global-step:25200	 l-p:0.16493602097034454
epoch£º1260	 i:1 	 global-step:25201	 l-p:0.19382469356060028
epoch£º1260	 i:2 	 global-step:25202	 l-p:0.15985624492168427
epoch£º1260	 i:3 	 global-step:25203	 l-p:0.117564357817173
epoch£º1260	 i:4 	 global-step:25204	 l-p:0.16692455112934113
epoch£º1260	 i:5 	 global-step:25205	 l-p:0.12899284064769745
epoch£º1260	 i:6 	 global-step:25206	 l-p:0.10244815796613693
epoch£º1260	 i:7 	 global-step:25207	 l-p:0.18854250013828278
epoch£º1260	 i:8 	 global-step:25208	 l-p:0.06368179619312286
epoch£º1260	 i:9 	 global-step:25209	 l-p:0.1354236900806427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1664, 5.1664, 5.1664],
        [5.1664, 5.1662, 5.1664],
        [5.1664, 5.1494, 5.1645],
        [5.1664, 5.1615, 5.1661]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1261, step:0 
model_pd.l_p.mean(): 0.17270687222480774 
model_pd.l_d.mean(): -20.820755004882812 
model_pd.lagr.mean(): -20.648048400878906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4149], device='cuda:0')), ('power', tensor([-21.4722], device='cuda:0'))])
epoch£º1261	 i:0 	 global-step:25220	 l-p:0.17270687222480774
epoch£º1261	 i:1 	 global-step:25221	 l-p:0.16077739000320435
epoch£º1261	 i:2 	 global-step:25222	 l-p:0.21673263609409332
epoch£º1261	 i:3 	 global-step:25223	 l-p:0.11573291569948196
epoch£º1261	 i:4 	 global-step:25224	 l-p:0.10415536910295486
epoch£º1261	 i:5 	 global-step:25225	 l-p:0.09701347351074219
epoch£º1261	 i:6 	 global-step:25226	 l-p:0.11491653323173523
epoch£º1261	 i:7 	 global-step:25227	 l-p:0.1259644627571106
epoch£º1261	 i:8 	 global-step:25228	 l-p:0.18494278192520142
epoch£º1261	 i:9 	 global-step:25229	 l-p:0.11449190229177475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1677, 5.1778, 4.8703],
        [5.1677, 5.1431, 5.1642],
        [5.1677, 5.1760, 4.8678],
        [5.1677, 5.0477, 4.7024]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1262, step:0 
model_pd.l_p.mean(): 0.18313772976398468 
model_pd.l_d.mean(): -19.627992630004883 
model_pd.lagr.mean(): -19.444854736328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4919], device='cuda:0')), ('power', tensor([-20.3450], device='cuda:0'))])
epoch£º1262	 i:0 	 global-step:25240	 l-p:0.18313772976398468
epoch£º1262	 i:1 	 global-step:25241	 l-p:0.1639811098575592
epoch£º1262	 i:2 	 global-step:25242	 l-p:0.1635025143623352
epoch£º1262	 i:3 	 global-step:25243	 l-p:0.18442422151565552
epoch£º1262	 i:4 	 global-step:25244	 l-p:0.11622769385576248
epoch£º1262	 i:5 	 global-step:25245	 l-p:0.11552930623292923
epoch£º1262	 i:6 	 global-step:25246	 l-p:0.14992482960224152
epoch£º1262	 i:7 	 global-step:25247	 l-p:0.08302194625139236
epoch£º1262	 i:8 	 global-step:25248	 l-p:0.10176655650138855
epoch£º1262	 i:9 	 global-step:25249	 l-p:0.14495091140270233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1681, 5.2973, 5.0453],
        [5.1681, 5.1193, 5.1567],
        [5.1681, 5.0712, 5.1288],
        [5.1681, 5.0050, 5.0617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1263, step:0 
model_pd.l_p.mean(): 0.08187524974346161 
model_pd.l_d.mean(): -18.970643997192383 
model_pd.lagr.mean(): -18.888769149780273 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5340], device='cuda:0')), ('power', tensor([-19.7235], device='cuda:0'))])
epoch£º1263	 i:0 	 global-step:25260	 l-p:0.08187524974346161
epoch£º1263	 i:1 	 global-step:25261	 l-p:0.14137989282608032
epoch£º1263	 i:2 	 global-step:25262	 l-p:0.21516159176826477
epoch£º1263	 i:3 	 global-step:25263	 l-p:0.11724502593278885
epoch£º1263	 i:4 	 global-step:25264	 l-p:0.11668907105922699
epoch£º1263	 i:5 	 global-step:25265	 l-p:0.19853240251541138
epoch£º1263	 i:6 	 global-step:25266	 l-p:0.13859708607196808
epoch£º1263	 i:7 	 global-step:25267	 l-p:0.10507907718420029
epoch£º1263	 i:8 	 global-step:25268	 l-p:0.13091488182544708
epoch£º1263	 i:9 	 global-step:25269	 l-p:0.15774723887443542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1690, 4.8690, 4.6461],
        [5.1690, 4.9504, 4.9745],
        [5.1690, 5.1691, 5.1690],
        [5.1690, 5.1690, 5.1691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1264, step:0 
model_pd.l_p.mean(): 0.10536880046129227 
model_pd.l_d.mean(): -19.195472717285156 
model_pd.lagr.mean(): -19.090103149414062 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5224], device='cuda:0')), ('power', tensor([-19.9389], device='cuda:0'))])
epoch£º1264	 i:0 	 global-step:25280	 l-p:0.10536880046129227
epoch£º1264	 i:1 	 global-step:25281	 l-p:0.22056637704372406
epoch£º1264	 i:2 	 global-step:25282	 l-p:0.144142284989357
epoch£º1264	 i:3 	 global-step:25283	 l-p:0.1639828085899353
epoch£º1264	 i:4 	 global-step:25284	 l-p:0.12456101924180984
epoch£º1264	 i:5 	 global-step:25285	 l-p:0.10421153157949448
epoch£º1264	 i:6 	 global-step:25286	 l-p:0.19428063929080963
epoch£º1264	 i:7 	 global-step:25287	 l-p:0.1027289554476738
epoch£º1264	 i:8 	 global-step:25288	 l-p:0.13337171077728271
epoch£º1264	 i:9 	 global-step:25289	 l-p:0.10890904068946838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1672, 5.1620, 5.1670],
        [5.1672, 4.9560, 4.9866],
        [5.1672, 5.1132, 5.1536],
        [5.1672, 5.1672, 5.1672]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1265, step:0 
model_pd.l_p.mean(): 0.10751824826002121 
model_pd.l_d.mean(): -19.404052734375 
model_pd.lagr.mean(): -19.296533584594727 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5297], device='cuda:0')), ('power', tensor([-20.1572], device='cuda:0'))])
epoch£º1265	 i:0 	 global-step:25300	 l-p:0.10751824826002121
epoch£º1265	 i:1 	 global-step:25301	 l-p:0.1389743834733963
epoch£º1265	 i:2 	 global-step:25302	 l-p:0.14409184455871582
epoch£º1265	 i:3 	 global-step:25303	 l-p:0.15777385234832764
epoch£º1265	 i:4 	 global-step:25304	 l-p:0.12644818425178528
epoch£º1265	 i:5 	 global-step:25305	 l-p:0.11696424335241318
epoch£º1265	 i:6 	 global-step:25306	 l-p:0.2174689620733261
epoch£º1265	 i:7 	 global-step:25307	 l-p:0.19551719725131989
epoch£º1265	 i:8 	 global-step:25308	 l-p:0.12048184871673584
epoch£º1265	 i:9 	 global-step:25309	 l-p:0.10189687460660934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1266
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1995,  0.1166,  1.0000,  0.0681,
          1.0000,  0.5843, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2321,  0.1426,  1.0000,  0.0876,
          1.0000,  0.6145, 31.6228]], device='cuda:0')
 pt:tensor([[5.1627, 4.9356, 4.9519],
        [5.1627, 5.0079, 5.0669],
        [5.1627, 4.9520, 4.9833],
        [5.1627, 4.9053, 4.8815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1266, step:0 
model_pd.l_p.mean(): 0.20780925452709198 
model_pd.l_d.mean(): -19.86368751525879 
model_pd.lagr.mean(): -19.6558780670166 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4755], device='cuda:0')), ('power', tensor([-20.5665], device='cuda:0'))])
epoch£º1266	 i:0 	 global-step:25320	 l-p:0.20780925452709198
epoch£º1266	 i:1 	 global-step:25321	 l-p:0.15909743309020996
epoch£º1266	 i:2 	 global-step:25322	 l-p:0.13127650320529938
epoch£º1266	 i:3 	 global-step:25323	 l-p:0.13930784165859222
epoch£º1266	 i:4 	 global-step:25324	 l-p:0.10711988061666489
epoch£º1266	 i:5 	 global-step:25325	 l-p:0.19577614963054657
epoch£º1266	 i:6 	 global-step:25326	 l-p:0.10456552356481552
epoch£º1266	 i:7 	 global-step:25327	 l-p:0.09007734060287476
epoch£º1266	 i:8 	 global-step:25328	 l-p:0.13039980828762054
epoch£º1266	 i:9 	 global-step:25329	 l-p:0.16513296961784363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1624, 4.8888, 4.8343],
        [5.1624, 5.1618, 5.1624],
        [5.1624, 5.1559, 5.1620],
        [5.1624, 5.1624, 5.1624]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1267, step:0 
model_pd.l_p.mean(): 0.13635802268981934 
model_pd.l_d.mean(): -20.806276321411133 
model_pd.lagr.mean(): -20.669918060302734 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4241], device='cuda:0')), ('power', tensor([-21.4669], device='cuda:0'))])
epoch£º1267	 i:0 	 global-step:25340	 l-p:0.13635802268981934
epoch£º1267	 i:1 	 global-step:25341	 l-p:0.12317917495965958
epoch£º1267	 i:2 	 global-step:25342	 l-p:0.12549768388271332
epoch£º1267	 i:3 	 global-step:25343	 l-p:0.10115665197372437
epoch£º1267	 i:4 	 global-step:25344	 l-p:0.1690930873155594
epoch£º1267	 i:5 	 global-step:25345	 l-p:0.1802368462085724
epoch£º1267	 i:6 	 global-step:25346	 l-p:0.1563645452260971
epoch£º1267	 i:7 	 global-step:25347	 l-p:0.1323041170835495
epoch£º1267	 i:8 	 global-step:25348	 l-p:0.18490096926689148
epoch£º1267	 i:9 	 global-step:25349	 l-p:0.1341920793056488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1601, 5.1584, 5.1601],
        [5.1601, 4.9614, 5.0017],
        [5.1601, 5.1601, 5.1601],
        [5.1601, 4.9041, 4.8829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1268, step:0 
model_pd.l_p.mean(): 0.13399915397167206 
model_pd.l_d.mean(): -19.55402374267578 
model_pd.lagr.mean(): -19.420024871826172 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4708], device='cuda:0')), ('power', tensor([-20.2487], device='cuda:0'))])
epoch£º1268	 i:0 	 global-step:25360	 l-p:0.13399915397167206
epoch£º1268	 i:1 	 global-step:25361	 l-p:0.12674306333065033
epoch£º1268	 i:2 	 global-step:25362	 l-p:0.15566915273666382
epoch£º1268	 i:3 	 global-step:25363	 l-p:0.1696547269821167
epoch£º1268	 i:4 	 global-step:25364	 l-p:0.13072238862514496
epoch£º1268	 i:5 	 global-step:25365	 l-p:0.12834133207798004
epoch£º1268	 i:6 	 global-step:25366	 l-p:0.20198236405849457
epoch£º1268	 i:7 	 global-step:25367	 l-p:0.09105265140533447
epoch£º1268	 i:8 	 global-step:25368	 l-p:0.13299113512039185
epoch£º1268	 i:9 	 global-step:25369	 l-p:0.16915147006511688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1625, 4.9826, 5.0332],
        [5.1625, 5.1546, 5.1620],
        [5.1625, 5.1625, 5.1625],
        [5.1625, 5.1625, 5.1625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1269, step:0 
model_pd.l_p.mean(): 0.07070528715848923 
model_pd.l_d.mean(): -20.287033081054688 
model_pd.lagr.mean(): -20.216327667236328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4692], device='cuda:0')), ('power', tensor([-20.9881], device='cuda:0'))])
epoch£º1269	 i:0 	 global-step:25380	 l-p:0.07070528715848923
epoch£º1269	 i:1 	 global-step:25381	 l-p:0.17193889617919922
epoch£º1269	 i:2 	 global-step:25382	 l-p:0.12876495718955994
epoch£º1269	 i:3 	 global-step:25383	 l-p:0.1788807213306427
epoch£º1269	 i:4 	 global-step:25384	 l-p:0.1168178915977478
epoch£º1269	 i:5 	 global-step:25385	 l-p:0.16568489372730255
epoch£º1269	 i:6 	 global-step:25386	 l-p:0.15799899399280548
epoch£º1269	 i:7 	 global-step:25387	 l-p:0.14373162388801575
epoch£º1269	 i:8 	 global-step:25388	 l-p:0.12851834297180176
epoch£º1269	 i:9 	 global-step:25389	 l-p:0.15366242825984955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1717, 5.2837, 5.0227],
        [5.1717, 5.1706, 5.1717],
        [5.1717, 4.8702, 4.6623],
        [5.1717, 4.8771, 4.6289]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1270, step:0 
model_pd.l_p.mean(): 0.13075779378414154 
model_pd.l_d.mean(): -20.49592399597168 
model_pd.lagr.mean(): -20.36516571044922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4529], device='cuda:0')), ('power', tensor([-21.1826], device='cuda:0'))])
epoch£º1270	 i:0 	 global-step:25400	 l-p:0.13075779378414154
epoch£º1270	 i:1 	 global-step:25401	 l-p:0.13829158246517181
epoch£º1270	 i:2 	 global-step:25402	 l-p:0.04887734353542328
epoch£º1270	 i:3 	 global-step:25403	 l-p:0.18994863331317902
epoch£º1270	 i:4 	 global-step:25404	 l-p:0.16214591264724731
epoch£º1270	 i:5 	 global-step:25405	 l-p:0.14726106822490692
epoch£º1270	 i:6 	 global-step:25406	 l-p:0.14967837929725647
epoch£º1270	 i:7 	 global-step:25407	 l-p:0.14174126088619232
epoch£º1270	 i:8 	 global-step:25408	 l-p:0.14550653100013733
epoch£º1270	 i:9 	 global-step:25409	 l-p:0.12407132983207703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1783, 5.1755, 4.8627],
        [5.1783, 4.9885, 4.6443],
        [5.1783, 5.1255, 5.1652],
        [5.1783, 4.8813, 4.7460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1271, step:0 
model_pd.l_p.mean(): 0.13174475729465485 
model_pd.l_d.mean(): -19.430126190185547 
model_pd.lagr.mean(): -19.298381805419922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5113], device='cuda:0')), ('power', tensor([-20.1648], device='cuda:0'))])
epoch£º1271	 i:0 	 global-step:25420	 l-p:0.13174475729465485
epoch£º1271	 i:1 	 global-step:25421	 l-p:0.15702353417873383
epoch£º1271	 i:2 	 global-step:25422	 l-p:0.14991885423660278
epoch£º1271	 i:3 	 global-step:25423	 l-p:0.12035220116376877
epoch£º1271	 i:4 	 global-step:25424	 l-p:0.1385260969400406
epoch£º1271	 i:5 	 global-step:25425	 l-p:0.14826880395412445
epoch£º1271	 i:6 	 global-step:25426	 l-p:0.18835221230983734
epoch£º1271	 i:7 	 global-step:25427	 l-p:0.12419261038303375
epoch£º1271	 i:8 	 global-step:25428	 l-p:0.1245931014418602
epoch£º1271	 i:9 	 global-step:25429	 l-p:0.08901306241750717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1272
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228]], device='cuda:0')
 pt:tensor([[5.1758, 4.9777, 4.6349],
        [5.1758, 4.8804, 4.6371],
        [5.1758, 4.9026, 4.6089],
        [5.1758, 5.3935, 5.1897]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1272, step:0 
model_pd.l_p.mean(): 0.1362762451171875 
model_pd.l_d.mean(): -20.06844711303711 
model_pd.lagr.mean(): -19.932170867919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5271], device='cuda:0')), ('power', tensor([-20.8263], device='cuda:0'))])
epoch£º1272	 i:0 	 global-step:25440	 l-p:0.1362762451171875
epoch£º1272	 i:1 	 global-step:25441	 l-p:0.13189324736595154
epoch£º1272	 i:2 	 global-step:25442	 l-p:0.12191806733608246
epoch£º1272	 i:3 	 global-step:25443	 l-p:0.13429990410804749
epoch£º1272	 i:4 	 global-step:25444	 l-p:0.10051033645868301
epoch£º1272	 i:5 	 global-step:25445	 l-p:0.14341580867767334
epoch£º1272	 i:6 	 global-step:25446	 l-p:0.14495202898979187
epoch£º1272	 i:7 	 global-step:25447	 l-p:0.14212672412395477
epoch£º1272	 i:8 	 global-step:25448	 l-p:0.1326807737350464
epoch£º1272	 i:9 	 global-step:25449	 l-p:0.17749087512493134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1803, 4.8795, 4.6743],
        [5.1803, 5.1247, 5.1659],
        [5.1803, 5.1799, 5.1803],
        [5.1803, 5.1756, 5.1801]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1273, step:0 
model_pd.l_p.mean(): 0.15757034718990326 
model_pd.l_d.mean(): -19.88332748413086 
model_pd.lagr.mean(): -19.725757598876953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5377], device='cuda:0')), ('power', tensor([-20.6499], device='cuda:0'))])
epoch£º1273	 i:0 	 global-step:25460	 l-p:0.15757034718990326
epoch£º1273	 i:1 	 global-step:25461	 l-p:0.19221021234989166
epoch£º1273	 i:2 	 global-step:25462	 l-p:0.08524410426616669
epoch£º1273	 i:3 	 global-step:25463	 l-p:0.14712615311145782
epoch£º1273	 i:4 	 global-step:25464	 l-p:0.11357128620147705
epoch£º1273	 i:5 	 global-step:25465	 l-p:0.09979617595672607
epoch£º1273	 i:6 	 global-step:25466	 l-p:0.17851102352142334
epoch£º1273	 i:7 	 global-step:25467	 l-p:0.12562642991542816
epoch£º1273	 i:8 	 global-step:25468	 l-p:0.10857851058244705
epoch£º1273	 i:9 	 global-step:25469	 l-p:0.14347170293331146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1814, 5.1814, 5.1814],
        [5.1814, 5.0021, 5.0524],
        [5.1814, 4.9707, 5.0011],
        [5.1814, 5.1791, 5.1813]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1274, step:0 
model_pd.l_p.mean(): 0.1492636501789093 
model_pd.l_d.mean(): -20.054218292236328 
model_pd.lagr.mean(): -19.90495491027832 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4620], device='cuda:0')), ('power', tensor([-20.7454], device='cuda:0'))])
epoch£º1274	 i:0 	 global-step:25480	 l-p:0.1492636501789093
epoch£º1274	 i:1 	 global-step:25481	 l-p:0.12743087112903595
epoch£º1274	 i:2 	 global-step:25482	 l-p:0.14403687417507172
epoch£º1274	 i:3 	 global-step:25483	 l-p:0.10874421149492264
epoch£º1274	 i:4 	 global-step:25484	 l-p:0.17603200674057007
epoch£º1274	 i:5 	 global-step:25485	 l-p:0.12557944655418396
epoch£º1274	 i:6 	 global-step:25486	 l-p:0.12154277414083481
epoch£º1274	 i:7 	 global-step:25487	 l-p:0.08049467206001282
epoch£º1274	 i:8 	 global-step:25488	 l-p:0.1996345818042755
epoch£º1274	 i:9 	 global-step:25489	 l-p:0.13948211073875427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1726, 5.0443, 5.1060],
        [5.1726, 5.1705, 5.1725],
        [5.1726, 5.1719, 5.1726],
        [5.1726, 5.5256, 5.4044]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1275, step:0 
model_pd.l_p.mean(): 0.09640109539031982 
model_pd.l_d.mean(): -20.242998123168945 
model_pd.lagr.mean(): -20.146596908569336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4899], device='cuda:0')), ('power', tensor([-20.9647], device='cuda:0'))])
epoch£º1275	 i:0 	 global-step:25500	 l-p:0.09640109539031982
epoch£º1275	 i:1 	 global-step:25501	 l-p:0.14719153940677643
epoch£º1275	 i:2 	 global-step:25502	 l-p:0.10182885825634003
epoch£º1275	 i:3 	 global-step:25503	 l-p:0.1368226557970047
epoch£º1275	 i:4 	 global-step:25504	 l-p:0.12077289074659348
epoch£º1275	 i:5 	 global-step:25505	 l-p:0.1368764489889145
epoch£º1275	 i:6 	 global-step:25506	 l-p:0.1717696189880371
epoch£º1275	 i:7 	 global-step:25507	 l-p:0.15598246455192566
epoch£º1275	 i:8 	 global-step:25508	 l-p:0.13142111897468567
epoch£º1275	 i:9 	 global-step:25509	 l-p:0.20081941783428192
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1684, 5.1679, 5.1684],
        [5.1684, 4.9731, 4.6287],
        [5.1684, 5.0149, 4.6667],
        [5.1684, 4.8699, 4.7325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1276, step:0 
model_pd.l_p.mean(): 0.12085782736539841 
model_pd.l_d.mean(): -20.519393920898438 
model_pd.lagr.mean(): -20.398536682128906 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4696], device='cuda:0')), ('power', tensor([-21.2234], device='cuda:0'))])
epoch£º1276	 i:0 	 global-step:25520	 l-p:0.12085782736539841
epoch£º1276	 i:1 	 global-step:25521	 l-p:0.1423470377922058
epoch£º1276	 i:2 	 global-step:25522	 l-p:0.14803168177604675
epoch£º1276	 i:3 	 global-step:25523	 l-p:0.17757683992385864
epoch£º1276	 i:4 	 global-step:25524	 l-p:0.13209359347820282
epoch£º1276	 i:5 	 global-step:25525	 l-p:0.12398452311754227
epoch£º1276	 i:6 	 global-step:25526	 l-p:0.1216522753238678
epoch£º1276	 i:7 	 global-step:25527	 l-p:0.11883335560560226
epoch£º1276	 i:8 	 global-step:25528	 l-p:0.12589815258979797
epoch£º1276	 i:9 	 global-step:25529	 l-p:0.19246813654899597
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1693, 4.9267, 4.6015],
        [5.1693, 5.1678, 5.1693],
        [5.1693, 5.4317, 5.2542],
        [5.1693, 5.1688, 5.1693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1277, step:0 
model_pd.l_p.mean(): 0.18495917320251465 
model_pd.l_d.mean(): -20.399675369262695 
model_pd.lagr.mean(): -20.2147159576416 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5082], device='cuda:0')), ('power', tensor([-21.1418], device='cuda:0'))])
epoch£º1277	 i:0 	 global-step:25540	 l-p:0.18495917320251465
epoch£º1277	 i:1 	 global-step:25541	 l-p:0.11898589879274368
epoch£º1277	 i:2 	 global-step:25542	 l-p:0.14440007507801056
epoch£º1277	 i:3 	 global-step:25543	 l-p:0.12119679898023605
epoch£º1277	 i:4 	 global-step:25544	 l-p:0.15357796847820282
epoch£º1277	 i:5 	 global-step:25545	 l-p:0.11128704994916916
epoch£º1277	 i:6 	 global-step:25546	 l-p:0.12435348331928253
epoch£º1277	 i:7 	 global-step:25547	 l-p:0.11183253675699234
epoch£º1277	 i:8 	 global-step:25548	 l-p:0.12652045488357544
epoch£º1277	 i:9 	 global-step:25549	 l-p:0.19621314108371735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1729, 5.1729, 5.1729],
        [5.1729, 5.1820, 4.8738],
        [5.1729, 5.0793, 4.7385],
        [5.1729, 5.1581, 5.1714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1278, step:0 
model_pd.l_p.mean(): 0.1447392702102661 
model_pd.l_d.mean(): -20.56146812438965 
model_pd.lagr.mean(): -20.416728973388672 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4379], device='cuda:0')), ('power', tensor([-21.2335], device='cuda:0'))])
epoch£º1278	 i:0 	 global-step:25560	 l-p:0.1447392702102661
epoch£º1278	 i:1 	 global-step:25561	 l-p:0.1611262410879135
epoch£º1278	 i:2 	 global-step:25562	 l-p:0.1136908307671547
epoch£º1278	 i:3 	 global-step:25563	 l-p:0.16852310299873352
epoch£º1278	 i:4 	 global-step:25564	 l-p:0.13317905366420746
epoch£º1278	 i:5 	 global-step:25565	 l-p:0.15951289236545563
epoch£º1278	 i:6 	 global-step:25566	 l-p:0.12018513679504395
epoch£º1278	 i:7 	 global-step:25567	 l-p:0.09484543651342392
epoch£º1278	 i:8 	 global-step:25568	 l-p:0.15136504173278809
epoch£º1278	 i:9 	 global-step:25569	 l-p:0.13346834480762482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1749, 5.0814, 5.1381],
        [5.1749, 5.1747, 5.1749],
        [5.1749, 4.9834, 4.6389],
        [5.1749, 5.1748, 5.1749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1279, step:0 
model_pd.l_p.mean(): 0.13521289825439453 
model_pd.l_d.mean(): -18.0810489654541 
model_pd.lagr.mean(): -17.94583511352539 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6620], device='cuda:0')), ('power', tensor([-18.9550], device='cuda:0'))])
epoch£º1279	 i:0 	 global-step:25580	 l-p:0.13521289825439453
epoch£º1279	 i:1 	 global-step:25581	 l-p:0.1308095008134842
epoch£º1279	 i:2 	 global-step:25582	 l-p:0.15414847433567047
epoch£º1279	 i:3 	 global-step:25583	 l-p:0.12042959779500961
epoch£º1279	 i:4 	 global-step:25584	 l-p:0.17595992982387543
epoch£º1279	 i:5 	 global-step:25585	 l-p:0.10708075016736984
epoch£º1279	 i:6 	 global-step:25586	 l-p:0.15767429769039154
epoch£º1279	 i:7 	 global-step:25587	 l-p:0.13032734394073486
epoch£º1279	 i:8 	 global-step:25588	 l-p:0.1420545130968094
epoch£º1279	 i:9 	 global-step:25589	 l-p:0.12808792293071747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1722, 4.8712, 4.6543],
        [5.1722, 5.0175, 5.0764],
        [5.1722, 5.1693, 5.1721],
        [5.1722, 5.1705, 5.1721]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1280, step:0 
model_pd.l_p.mean(): 0.14746566116809845 
model_pd.l_d.mean(): -19.765905380249023 
model_pd.lagr.mean(): -19.618440628051758 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5006], device='cuda:0')), ('power', tensor([-20.4934], device='cuda:0'))])
epoch£º1280	 i:0 	 global-step:25600	 l-p:0.14746566116809845
epoch£º1280	 i:1 	 global-step:25601	 l-p:0.14529889822006226
epoch£º1280	 i:2 	 global-step:25602	 l-p:0.13381327688694
epoch£º1280	 i:3 	 global-step:25603	 l-p:0.14222364127635956
epoch£º1280	 i:4 	 global-step:25604	 l-p:0.13010287284851074
epoch£º1280	 i:5 	 global-step:25605	 l-p:0.12152604013681412
epoch£º1280	 i:6 	 global-step:25606	 l-p:0.15604059398174286
epoch£º1280	 i:7 	 global-step:25607	 l-p:0.1293260008096695
epoch£º1280	 i:8 	 global-step:25608	 l-p:0.15019814670085907
epoch£º1280	 i:9 	 global-step:25609	 l-p:0.13995206356048584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1691, 5.1477, 5.1664],
        [5.1691, 4.8957, 4.8412],
        [5.1691, 5.0323, 5.0938],
        [5.1691, 4.8846, 4.8020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1281, step:0 
model_pd.l_p.mean(): 0.09014420211315155 
model_pd.l_d.mean(): -20.580642700195312 
model_pd.lagr.mean(): -20.490497589111328 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4352], device='cuda:0')), ('power', tensor([-21.2502], device='cuda:0'))])
epoch£º1281	 i:0 	 global-step:25620	 l-p:0.09014420211315155
epoch£º1281	 i:1 	 global-step:25621	 l-p:0.1770787537097931
epoch£º1281	 i:2 	 global-step:25622	 l-p:0.14310958981513977
epoch£º1281	 i:3 	 global-step:25623	 l-p:0.16145014762878418
epoch£º1281	 i:4 	 global-step:25624	 l-p:0.16880744695663452
epoch£º1281	 i:5 	 global-step:25625	 l-p:0.1275874227285385
epoch£º1281	 i:6 	 global-step:25626	 l-p:0.17408567667007446
epoch£º1281	 i:7 	 global-step:25627	 l-p:0.12367831915616989
epoch£º1281	 i:8 	 global-step:25628	 l-p:0.09295225888490677
epoch£º1281	 i:9 	 global-step:25629	 l-p:0.1326315701007843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1751, 5.1750, 5.1751],
        [5.1751, 5.1728, 5.1750],
        [5.1751, 4.9307, 4.9256],
        [5.1751, 5.0419, 4.6950]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1282, step:0 
model_pd.l_p.mean(): 0.10565075278282166 
model_pd.l_d.mean(): -20.390243530273438 
model_pd.lagr.mean(): -20.28459358215332 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4704], device='cuda:0')), ('power', tensor([-21.0936], device='cuda:0'))])
epoch£º1282	 i:0 	 global-step:25640	 l-p:0.10565075278282166
epoch£º1282	 i:1 	 global-step:25641	 l-p:0.08064858615398407
epoch£º1282	 i:2 	 global-step:25642	 l-p:0.15086255967617035
epoch£º1282	 i:3 	 global-step:25643	 l-p:0.16114909946918488
epoch£º1282	 i:4 	 global-step:25644	 l-p:0.11058710515499115
epoch£º1282	 i:5 	 global-step:25645	 l-p:0.1361158937215805
epoch£º1282	 i:6 	 global-step:25646	 l-p:0.09968415647745132
epoch£º1282	 i:7 	 global-step:25647	 l-p:0.1933654546737671
epoch£º1282	 i:8 	 global-step:25648	 l-p:0.20942193269729614
epoch£º1282	 i:9 	 global-step:25649	 l-p:0.14054372906684875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1719, 4.8736, 4.7360],
        [5.1719, 5.1712, 5.1719],
        [5.1719, 5.1695, 5.1718],
        [5.1719, 5.1719, 5.1719]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1283, step:0 
model_pd.l_p.mean(): 0.0814584419131279 
model_pd.l_d.mean(): -20.2327823638916 
model_pd.lagr.mean(): -20.151323318481445 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4758], device='cuda:0')), ('power', tensor([-20.9400], device='cuda:0'))])
epoch£º1283	 i:0 	 global-step:25660	 l-p:0.0814584419131279
epoch£º1283	 i:1 	 global-step:25661	 l-p:0.13872142136096954
epoch£º1283	 i:2 	 global-step:25662	 l-p:0.16458232700824738
epoch£º1283	 i:3 	 global-step:25663	 l-p:0.1472700536251068
epoch£º1283	 i:4 	 global-step:25664	 l-p:0.18655017018318176
epoch£º1283	 i:5 	 global-step:25665	 l-p:0.08498603850603104
epoch£º1283	 i:6 	 global-step:25666	 l-p:0.12299568206071854
epoch£º1283	 i:7 	 global-step:25667	 l-p:0.1551656424999237
epoch£º1283	 i:8 	 global-step:25668	 l-p:0.1513841450214386
epoch£º1283	 i:9 	 global-step:25669	 l-p:0.15048372745513916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1762, 5.2439, 4.9611],
        [5.1762, 5.1732, 5.1761],
        [5.1762, 5.0916, 5.1456],
        [5.1762, 5.1761, 5.1762]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1284, step:0 
model_pd.l_p.mean(): 0.12916989624500275 
model_pd.l_d.mean(): -20.82097816467285 
model_pd.lagr.mean(): -20.691808700561523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4172], device='cuda:0')), ('power', tensor([-21.4747], device='cuda:0'))])
epoch£º1284	 i:0 	 global-step:25680	 l-p:0.12916989624500275
epoch£º1284	 i:1 	 global-step:25681	 l-p:0.15224072337150574
epoch£º1284	 i:2 	 global-step:25682	 l-p:0.12541300058364868
epoch£º1284	 i:3 	 global-step:25683	 l-p:0.12554025650024414
epoch£º1284	 i:4 	 global-step:25684	 l-p:0.14984098076820374
epoch£º1284	 i:5 	 global-step:25685	 l-p:0.1947597861289978
epoch£º1284	 i:6 	 global-step:25686	 l-p:0.13917623460292816
epoch£º1284	 i:7 	 global-step:25687	 l-p:0.14133517444133759
epoch£º1284	 i:8 	 global-step:25688	 l-p:0.0954267680644989
epoch£º1284	 i:9 	 global-step:25689	 l-p:0.13027851283550262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1719, 4.9587, 4.6193],
        [5.1719, 5.1664, 5.1717],
        [5.1719, 4.9013, 4.8522],
        [5.1719, 5.0521, 4.7065]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1285, step:0 
model_pd.l_p.mean(): 0.21326811611652374 
model_pd.l_d.mean(): -20.68511390686035 
model_pd.lagr.mean(): -20.471845626831055 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4421], device='cuda:0')), ('power', tensor([-21.3628], device='cuda:0'))])
epoch£º1285	 i:0 	 global-step:25700	 l-p:0.21326811611652374
epoch£º1285	 i:1 	 global-step:25701	 l-p:0.16308370232582092
epoch£º1285	 i:2 	 global-step:25702	 l-p:0.11281808465719223
epoch£º1285	 i:3 	 global-step:25703	 l-p:0.0985715240240097
epoch£º1285	 i:4 	 global-step:25704	 l-p:0.1256038248538971
epoch£º1285	 i:5 	 global-step:25705	 l-p:0.13477586209774017
epoch£º1285	 i:6 	 global-step:25706	 l-p:0.10482393950223923
epoch£º1285	 i:7 	 global-step:25707	 l-p:0.16691002249717712
epoch£º1285	 i:8 	 global-step:25708	 l-p:0.14136819541454315
epoch£º1285	 i:9 	 global-step:25709	 l-p:0.1360243409872055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1677, 4.8829, 4.8003],
        [5.1677, 5.1235, 5.1581],
        [5.1677, 5.4871, 5.3446],
        [5.1677, 5.2959, 5.0430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1286, step:0 
model_pd.l_p.mean(): 0.12232806533575058 
model_pd.l_d.mean(): -19.570411682128906 
model_pd.lagr.mean(): -19.448083877563477 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5156], device='cuda:0')), ('power', tensor([-20.3110], device='cuda:0'))])
epoch£º1286	 i:0 	 global-step:25720	 l-p:0.12232806533575058
epoch£º1286	 i:1 	 global-step:25721	 l-p:0.1300811916589737
epoch£º1286	 i:2 	 global-step:25722	 l-p:0.12104364484548569
epoch£º1286	 i:3 	 global-step:25723	 l-p:0.1961115151643753
epoch£º1286	 i:4 	 global-step:25724	 l-p:0.16245929896831512
epoch£º1286	 i:5 	 global-step:25725	 l-p:0.19026298820972443
epoch£º1286	 i:6 	 global-step:25726	 l-p:0.14439153671264648
epoch£º1286	 i:7 	 global-step:25727	 l-p:0.11857058107852936
epoch£º1286	 i:8 	 global-step:25728	 l-p:0.12343406677246094
epoch£º1286	 i:9 	 global-step:25729	 l-p:0.09916288405656815
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1694, 5.1000, 4.7648],
        [5.1694, 4.8673, 4.7021],
        [5.1694, 5.1693, 5.1694],
        [5.1694, 5.1694, 5.1694]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1287, step:0 
model_pd.l_p.mean(): 0.12659001350402832 
model_pd.l_d.mean(): -20.435962677001953 
model_pd.lagr.mean(): -20.309371948242188 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4937], device='cuda:0')), ('power', tensor([-21.1637], device='cuda:0'))])
epoch£º1287	 i:0 	 global-step:25740	 l-p:0.12659001350402832
epoch£º1287	 i:1 	 global-step:25741	 l-p:0.11686485260725021
epoch£º1287	 i:2 	 global-step:25742	 l-p:0.1522359997034073
epoch£º1287	 i:3 	 global-step:25743	 l-p:0.14787033200263977
epoch£º1287	 i:4 	 global-step:25744	 l-p:0.13572025299072266
epoch£º1287	 i:5 	 global-step:25745	 l-p:0.12475790828466415
epoch£º1287	 i:6 	 global-step:25746	 l-p:0.1122724786400795
epoch£º1287	 i:7 	 global-step:25747	 l-p:0.12283240258693695
epoch£º1287	 i:8 	 global-step:25748	 l-p:0.18504415452480316
epoch£º1287	 i:9 	 global-step:25749	 l-p:0.1858273148536682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1657, 5.1657, 5.1657],
        [5.1657, 5.1656, 5.1657],
        [5.1657, 4.8802, 4.7961],
        [5.1657, 5.0384, 4.6915]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1288, step:0 
model_pd.l_p.mean(): 0.17518864572048187 
model_pd.l_d.mean(): -19.950115203857422 
model_pd.lagr.mean(): -19.774927139282227 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4812], device='cuda:0')), ('power', tensor([-20.6597], device='cuda:0'))])
epoch£º1288	 i:0 	 global-step:25760	 l-p:0.17518864572048187
epoch£º1288	 i:1 	 global-step:25761	 l-p:0.16120807826519012
epoch£º1288	 i:2 	 global-step:25762	 l-p:0.13268239796161652
epoch£º1288	 i:3 	 global-step:25763	 l-p:0.19599120318889618
epoch£º1288	 i:4 	 global-step:25764	 l-p:0.1208600178360939
epoch£º1288	 i:5 	 global-step:25765	 l-p:0.08243570476770401
epoch£º1288	 i:6 	 global-step:25766	 l-p:0.13236743211746216
epoch£º1288	 i:7 	 global-step:25767	 l-p:0.13052287697792053
epoch£º1288	 i:8 	 global-step:25768	 l-p:0.10534819215536118
epoch£º1288	 i:9 	 global-step:25769	 l-p:0.18072110414505005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1671, 5.2932, 5.0391],
        [5.1671, 5.0735, 5.1303],
        [5.1671, 5.1672, 5.1671],
        [5.1671, 5.1232, 5.1577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1289, step:0 
model_pd.l_p.mean(): 0.10724687576293945 
model_pd.l_d.mean(): -20.32125473022461 
model_pd.lagr.mean(): -20.214008331298828 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4353], device='cuda:0')), ('power', tensor([-20.9880], device='cuda:0'))])
epoch£º1289	 i:0 	 global-step:25780	 l-p:0.10724687576293945
epoch£º1289	 i:1 	 global-step:25781	 l-p:0.10706179589033127
epoch£º1289	 i:2 	 global-step:25782	 l-p:0.11722224205732346
epoch£º1289	 i:3 	 global-step:25783	 l-p:0.22708949446678162
epoch£º1289	 i:4 	 global-step:25784	 l-p:0.13067713379859924
epoch£º1289	 i:5 	 global-step:25785	 l-p:0.13324353098869324
epoch£º1289	 i:6 	 global-step:25786	 l-p:0.12288132309913635
epoch£º1289	 i:7 	 global-step:25787	 l-p:0.213143989443779
epoch£º1289	 i:8 	 global-step:25788	 l-p:0.1376350224018097
epoch£º1289	 i:9 	 global-step:25789	 l-p:0.11805073171854019
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1662, 5.1662, 5.1662],
        [5.1662, 4.9405, 4.9585],
        [5.1662, 4.9640, 5.0020],
        [5.1662, 5.0757, 5.1317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1290, step:0 
model_pd.l_p.mean(): 0.10400088876485825 
model_pd.l_d.mean(): -19.644081115722656 
model_pd.lagr.mean(): -19.540081024169922 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5240], device='cuda:0')), ('power', tensor([-20.3941], device='cuda:0'))])
epoch£º1290	 i:0 	 global-step:25800	 l-p:0.10400088876485825
epoch£º1290	 i:1 	 global-step:25801	 l-p:0.23306533694267273
epoch£º1290	 i:2 	 global-step:25802	 l-p:0.14948174357414246
epoch£º1290	 i:3 	 global-step:25803	 l-p:0.12459397315979004
epoch£º1290	 i:4 	 global-step:25804	 l-p:0.23042809963226318
epoch£º1290	 i:5 	 global-step:25805	 l-p:0.1354600340127945
epoch£º1290	 i:6 	 global-step:25806	 l-p:0.08206368237733841
epoch£º1290	 i:7 	 global-step:25807	 l-p:0.09668437391519547
epoch£º1290	 i:8 	 global-step:25808	 l-p:0.09591628611087799
epoch£º1290	 i:9 	 global-step:25809	 l-p:0.16755814850330353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1644, 4.9357, 4.9506],
        [5.1644, 4.9258, 4.9294],
        [5.1644, 4.8633, 4.7136],
        [5.1644, 5.1643, 5.1644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1291, step:0 
model_pd.l_p.mean(): 0.12983369827270508 
model_pd.l_d.mean(): -20.897663116455078 
model_pd.lagr.mean(): -20.76782989501953 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4018], device='cuda:0')), ('power', tensor([-21.5365], device='cuda:0'))])
epoch£º1291	 i:0 	 global-step:25820	 l-p:0.12983369827270508
epoch£º1291	 i:1 	 global-step:25821	 l-p:0.1371445506811142
epoch£º1291	 i:2 	 global-step:25822	 l-p:0.12681469321250916
epoch£º1291	 i:3 	 global-step:25823	 l-p:0.2402566820383072
epoch£º1291	 i:4 	 global-step:25824	 l-p:0.13681535422801971
epoch£º1291	 i:5 	 global-step:25825	 l-p:0.177239790558815
epoch£º1291	 i:6 	 global-step:25826	 l-p:0.1412981152534485
epoch£º1291	 i:7 	 global-step:25827	 l-p:0.07036338001489639
epoch£º1291	 i:8 	 global-step:25828	 l-p:0.12864147126674652
epoch£º1291	 i:9 	 global-step:25829	 l-p:0.14598369598388672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1638, 5.1634, 5.1638],
        [5.1638, 5.5516, 5.4529],
        [5.1638, 5.1626, 5.1638],
        [5.1638, 5.0820, 5.1351]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1292, step:0 
model_pd.l_p.mean(): 0.12196911871433258 
model_pd.l_d.mean(): -18.961753845214844 
model_pd.lagr.mean(): -18.839784622192383 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5317], device='cuda:0')), ('power', tensor([-19.7122], device='cuda:0'))])
epoch£º1292	 i:0 	 global-step:25840	 l-p:0.12196911871433258
epoch£º1292	 i:1 	 global-step:25841	 l-p:0.19962678849697113
epoch£º1292	 i:2 	 global-step:25842	 l-p:0.1402738392353058
epoch£º1292	 i:3 	 global-step:25843	 l-p:0.12641781568527222
epoch£º1292	 i:4 	 global-step:25844	 l-p:0.11431057006120682
epoch£º1292	 i:5 	 global-step:25845	 l-p:0.14246471226215363
epoch£º1292	 i:6 	 global-step:25846	 l-p:0.17501121759414673
epoch£º1292	 i:7 	 global-step:25847	 l-p:0.13597232103347778
epoch£º1292	 i:8 	 global-step:25848	 l-p:0.13978075981140137
epoch£º1292	 i:9 	 global-step:25849	 l-p:0.12812389433383942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1649, 5.1607, 5.1647],
        [5.1649, 4.8643, 4.7180],
        [5.1649, 5.0024, 5.0596],
        [5.1649, 5.0463, 5.1077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1293, step:0 
model_pd.l_p.mean(): 0.09973817318677902 
model_pd.l_d.mean(): -20.339170455932617 
model_pd.lagr.mean(): -20.239431381225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4788], device='cuda:0')), ('power', tensor([-21.0506], device='cuda:0'))])
epoch£º1293	 i:0 	 global-step:25860	 l-p:0.09973817318677902
epoch£º1293	 i:1 	 global-step:25861	 l-p:0.12064802646636963
epoch£º1293	 i:2 	 global-step:25862	 l-p:0.25287407636642456
epoch£º1293	 i:3 	 global-step:25863	 l-p:0.1515701711177826
epoch£º1293	 i:4 	 global-step:25864	 l-p:0.15940210223197937
epoch£º1293	 i:5 	 global-step:25865	 l-p:0.1074962317943573
epoch£º1293	 i:6 	 global-step:25866	 l-p:0.12663617730140686
epoch£º1293	 i:7 	 global-step:25867	 l-p:0.08298461884260178
epoch£º1293	 i:8 	 global-step:25868	 l-p:0.2017633616924286
epoch£º1293	 i:9 	 global-step:25869	 l-p:0.13255567848682404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1609, 5.1599, 5.1609],
        [5.1609, 5.1108, 5.1490],
        [5.1609, 4.8579, 4.6953],
        [5.1609, 5.1604, 5.1609]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1294, step:0 
model_pd.l_p.mean(): 0.09536495059728622 
model_pd.l_d.mean(): -19.339908599853516 
model_pd.lagr.mean(): -19.244543075561523 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5355], device='cuda:0')), ('power', tensor([-20.0984], device='cuda:0'))])
epoch£º1294	 i:0 	 global-step:25880	 l-p:0.09536495059728622
epoch£º1294	 i:1 	 global-step:25881	 l-p:0.15803615748882294
epoch£º1294	 i:2 	 global-step:25882	 l-p:0.11373751610517502
epoch£º1294	 i:3 	 global-step:25883	 l-p:0.24649165570735931
epoch£º1294	 i:4 	 global-step:25884	 l-p:0.12608812749385834
epoch£º1294	 i:5 	 global-step:25885	 l-p:0.14450982213020325
epoch£º1294	 i:6 	 global-step:25886	 l-p:0.1560848355293274
epoch£º1294	 i:7 	 global-step:25887	 l-p:0.12384622544050217
epoch£º1294	 i:8 	 global-step:25888	 l-p:0.14958986639976501
epoch£º1294	 i:9 	 global-step:25889	 l-p:0.13228310644626617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1601, 5.1577, 5.1600],
        [5.1601, 5.1461, 5.1587],
        [5.1601, 5.1090, 5.1477],
        [5.1601, 5.2315, 4.9503]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1295, step:0 
model_pd.l_p.mean(): 0.16989322006702423 
model_pd.l_d.mean(): -20.448631286621094 
model_pd.lagr.mean(): -20.278738021850586 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4619], device='cuda:0')), ('power', tensor([-21.1440], device='cuda:0'))])
epoch£º1295	 i:0 	 global-step:25900	 l-p:0.16989322006702423
epoch£º1295	 i:1 	 global-step:25901	 l-p:0.07527957111597061
epoch£º1295	 i:2 	 global-step:25902	 l-p:0.21142138540744781
epoch£º1295	 i:3 	 global-step:25903	 l-p:0.1266656517982483
epoch£º1295	 i:4 	 global-step:25904	 l-p:0.1259569525718689
epoch£º1295	 i:5 	 global-step:25905	 l-p:0.10794692486524582
epoch£º1295	 i:6 	 global-step:25906	 l-p:0.16589027643203735
epoch£º1295	 i:7 	 global-step:25907	 l-p:0.16941794753074646
epoch£º1295	 i:8 	 global-step:25908	 l-p:0.15252439677715302
epoch£º1295	 i:9 	 global-step:25909	 l-p:0.13901397585868835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1613, 5.0745, 5.1294],
        [5.1613, 5.0928, 5.1405],
        [5.1613, 5.1613, 5.1613],
        [5.1613, 5.1415, 5.1589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1296, step:0 
model_pd.l_p.mean(): 0.14651884138584137 
model_pd.l_d.mean(): -19.412111282348633 
model_pd.lagr.mean(): -19.265592575073242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5219], device='cuda:0')), ('power', tensor([-20.1575], device='cuda:0'))])
epoch£º1296	 i:0 	 global-step:25920	 l-p:0.14651884138584137
epoch£º1296	 i:1 	 global-step:25921	 l-p:0.10953189432621002
epoch£º1296	 i:2 	 global-step:25922	 l-p:0.15870141983032227
epoch£º1296	 i:3 	 global-step:25923	 l-p:0.1215679869055748
epoch£º1296	 i:4 	 global-step:25924	 l-p:0.1744861602783203
epoch£º1296	 i:5 	 global-step:25925	 l-p:0.15821747481822968
epoch£º1296	 i:6 	 global-step:25926	 l-p:0.1863577961921692
epoch£º1296	 i:7 	 global-step:25927	 l-p:0.14113174378871918
epoch£º1296	 i:8 	 global-step:25928	 l-p:0.12249400466680527
epoch£º1296	 i:9 	 global-step:25929	 l-p:0.11209886521100998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1297
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4712,  0.3667,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2304,  0.1412,  1.0000,  0.0866,
          1.0000,  0.6130, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2326,  0.1431,  1.0000,  0.0880,
          1.0000,  0.6150, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]], device='cuda:0')
 pt:tensor([[5.1670, 4.9093, 4.5946],
        [5.1670, 4.9107, 4.8892],
        [5.1670, 4.9088, 4.8843],
        [5.1670, 4.8635, 4.6840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1297, step:0 
model_pd.l_p.mean(): 0.12204184383153915 
model_pd.l_d.mean(): -18.86663818359375 
model_pd.lagr.mean(): -18.744596481323242 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5383], device='cuda:0')), ('power', tensor([-19.6228], device='cuda:0'))])
epoch£º1297	 i:0 	 global-step:25940	 l-p:0.12204184383153915
epoch£º1297	 i:1 	 global-step:25941	 l-p:0.12125588953495026
epoch£º1297	 i:2 	 global-step:25942	 l-p:0.20370382070541382
epoch£º1297	 i:3 	 global-step:25943	 l-p:0.14765217900276184
epoch£º1297	 i:4 	 global-step:25944	 l-p:0.1993388682603836
epoch£º1297	 i:5 	 global-step:25945	 l-p:0.18860873579978943
epoch£º1297	 i:6 	 global-step:25946	 l-p:0.09862244129180908
epoch£º1297	 i:7 	 global-step:25947	 l-p:0.08562171459197998
epoch£º1297	 i:8 	 global-step:25948	 l-p:0.14759720861911774
epoch£º1297	 i:9 	 global-step:25949	 l-p:0.1034768745303154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1663, 5.1661, 5.1663],
        [5.1663, 4.8855, 4.5993],
        [5.1663, 4.9698, 4.6248],
        [5.1663, 4.8721, 4.6164]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1298, step:0 
model_pd.l_p.mean(): 0.10746350884437561 
model_pd.l_d.mean(): -20.473365783691406 
model_pd.lagr.mean(): -20.365901947021484 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4763], device='cuda:0')), ('power', tensor([-21.1837], device='cuda:0'))])
epoch£º1298	 i:0 	 global-step:25960	 l-p:0.10746350884437561
epoch£º1298	 i:1 	 global-step:25961	 l-p:0.17787593603134155
epoch£º1298	 i:2 	 global-step:25962	 l-p:0.20694153010845184
epoch£º1298	 i:3 	 global-step:25963	 l-p:0.08176816999912262
epoch£º1298	 i:4 	 global-step:25964	 l-p:0.10726679861545563
epoch£º1298	 i:5 	 global-step:25965	 l-p:0.12335274368524551
epoch£º1298	 i:6 	 global-step:25966	 l-p:0.19238121807575226
epoch£º1298	 i:7 	 global-step:25967	 l-p:0.14003883302211761
epoch£º1298	 i:8 	 global-step:25968	 l-p:0.20748938620090485
epoch£º1298	 i:9 	 global-step:25969	 l-p:0.08286983519792557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1625, 5.1619, 5.1625],
        [5.1625, 4.8585, 4.6597],
        [5.1625, 5.1625, 5.1625],
        [5.1625, 5.1300, 5.1569]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1299, step:0 
model_pd.l_p.mean(): 0.12369249761104584 
model_pd.l_d.mean(): -20.93096923828125 
model_pd.lagr.mean(): -20.80727767944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0106], device='cuda:0')), ('power', tensor([0.9892], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4024], device='cuda:0')), ('power', tensor([-21.5708], device='cuda:0'))])
epoch£º1299	 i:0 	 global-step:25980	 l-p:0.12369249761104584
epoch£º1299	 i:1 	 global-step:25981	 l-p:0.12362365424633026
epoch£º1299	 i:2 	 global-step:25982	 l-p:0.21743614971637726
epoch£º1299	 i:3 	 global-step:25983	 l-p:0.14958858489990234
epoch£º1299	 i:4 	 global-step:25984	 l-p:0.12824013829231262
epoch£º1299	 i:5 	 global-step:25985	 l-p:0.12152156233787537
epoch£º1299	 i:6 	 global-step:25986	 l-p:0.12092454731464386
epoch£º1299	 i:7 	 global-step:25987	 l-p:0.1588628739118576
epoch£º1299	 i:8 	 global-step:25988	 l-p:0.1425296515226364
epoch£º1299	 i:9 	 global-step:25989	 l-p:0.15742775797843933
