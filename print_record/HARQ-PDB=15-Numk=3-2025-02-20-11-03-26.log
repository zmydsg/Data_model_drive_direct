
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): -0.1705528199672699 
model_pd.l_d.mean(): -19.24897003173828 
model_pd.lagr.mean(): -19.419523239135742 
model_pd.lambdas: dict_items([('pout', tensor([1.0014], device='cuda:0')), ('power', tensor([0.9990], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3984], device='cuda:0')), ('power', tensor([-20.6474], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:-0.1705528199672699
epoch£º0	 i:1 	 global-step:1	 l-p:-0.20870685577392578
epoch£º0	 i:2 	 global-step:2	 l-p:-0.2769811153411865
epoch£º0	 i:3 	 global-step:3	 l-p:-0.4038802683353424
epoch£º0	 i:4 	 global-step:4	 l-p:-0.710580050945282
epoch£º0	 i:5 	 global-step:5	 l-p:-2.5309367179870605
epoch£º0	 i:6 	 global-step:6	 l-p:1.7505425214767456
epoch£º0	 i:7 	 global-step:7	 l-p:0.49168169498443604
epoch£º0	 i:8 	 global-step:8	 l-p:0.6606585383415222
epoch£º0	 i:9 	 global-step:9	 l-p:0.05680982396006584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5667, 3.5671, 3.5667],
        [3.5667, 3.7813, 3.7378],
        [3.5667, 3.6280, 3.5888],
        [3.5667, 4.5730, 5.2699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.37725773453712463 
model_pd.l_d.mean(): -18.738479614257812 
model_pd.lagr.mean(): -18.361221313476562 
model_pd.lambdas: dict_items([('pout', tensor([1.0124], device='cuda:0')), ('power', tensor([0.9889], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9113], device='cuda:0')), ('power', tensor([-19.8624], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.37725773453712463
epoch£º1	 i:1 	 global-step:21	 l-p:0.2680797576904297
epoch£º1	 i:2 	 global-step:22	 l-p:0.1878846138715744
epoch£º1	 i:3 	 global-step:23	 l-p:0.24290229380130768
epoch£º1	 i:4 	 global-step:24	 l-p:0.13980993628501892
epoch£º1	 i:5 	 global-step:25	 l-p:0.18688258528709412
epoch£º1	 i:6 	 global-step:26	 l-p:0.17738373577594757
epoch£º1	 i:7 	 global-step:27	 l-p:0.21870821714401245
epoch£º1	 i:8 	 global-step:28	 l-p:0.22566209733486176
epoch£º1	 i:9 	 global-step:29	 l-p:0.11669803410768509
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9155, 5.9890, 6.5151],
        [4.9155, 5.0133, 4.9532],
        [4.9155, 5.7273, 5.9884],
        [4.9155, 4.9975, 4.9437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.13359056413173676 
model_pd.l_d.mean(): -20.127248764038086 
model_pd.lagr.mean(): -19.9936580657959 
model_pd.lambdas: dict_items([('pout', tensor([1.0162], device='cuda:0')), ('power', tensor([0.9832], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4446], device='cuda:0')), ('power', tensor([-20.9244], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.13359056413173676
epoch£º2	 i:1 	 global-step:41	 l-p:0.10821321606636047
epoch£º2	 i:2 	 global-step:42	 l-p:0.10349393635988235
epoch£º2	 i:3 	 global-step:43	 l-p:0.10669863224029541
epoch£º2	 i:4 	 global-step:44	 l-p:0.11863960325717926
epoch£º2	 i:5 	 global-step:45	 l-p:0.1057601273059845
epoch£º2	 i:6 	 global-step:46	 l-p:0.11227868497371674
epoch£º2	 i:7 	 global-step:47	 l-p:1.555920124053955
epoch£º2	 i:8 	 global-step:48	 l-p:0.10760611295700073
epoch£º2	 i:9 	 global-step:49	 l-p:0.11721696704626083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4074, 6.4997, 6.9733],
        [5.4074, 5.4586, 5.4196],
        [5.4074, 5.5137, 5.4478],
        [5.4074, 6.9835, 8.0210]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.08074731379747391 
model_pd.l_d.mean(): -18.569534301757812 
model_pd.lagr.mean(): -18.488786697387695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9818], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4170], device='cuda:0')), ('power', tensor([-19.3446], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.08074731379747391
epoch£º3	 i:1 	 global-step:61	 l-p:0.08568032830953598
epoch£º3	 i:2 	 global-step:62	 l-p:-0.0470542311668396
epoch£º3	 i:3 	 global-step:63	 l-p:0.12177307158708572
epoch£º3	 i:4 	 global-step:64	 l-p:0.11827289313077927
epoch£º3	 i:5 	 global-step:65	 l-p:0.11643493175506592
epoch£º3	 i:6 	 global-step:66	 l-p:0.11279250681400299
epoch£º3	 i:7 	 global-step:67	 l-p:0.11705859005451202
epoch£º3	 i:8 	 global-step:68	 l-p:0.1252945512533188
epoch£º3	 i:9 	 global-step:69	 l-p:0.12358064204454422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9978, 5.7240, 5.9021],
        [4.9978, 5.8848, 6.2076],
        [4.9978, 6.5444, 7.6349],
        [4.9978, 5.0864, 5.0295]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.11452259123325348 
model_pd.l_d.mean(): -18.904130935668945 
model_pd.lagr.mean(): -18.789608001708984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9817], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4799], device='cuda:0')), ('power', tensor([-19.7545], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.11452259123325348
epoch£º4	 i:1 	 global-step:81	 l-p:0.1306811273097992
epoch£º4	 i:2 	 global-step:82	 l-p:0.1647750437259674
epoch£º4	 i:3 	 global-step:83	 l-p:0.13171032071113586
epoch£º4	 i:4 	 global-step:84	 l-p:0.1525900661945343
epoch£º4	 i:5 	 global-step:85	 l-p:0.1260961890220642
epoch£º4	 i:6 	 global-step:86	 l-p:0.12957434356212616
epoch£º4	 i:7 	 global-step:87	 l-p:0.13372308015823364
epoch£º4	 i:8 	 global-step:88	 l-p:0.139829620718956
epoch£º4	 i:9 	 global-step:89	 l-p:0.15763939917087555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6637, 4.6639, 4.6637],
        [4.6637, 4.6699, 4.6642],
        [4.6637, 4.7574, 4.7003],
        [4.6637, 4.7980, 4.7299]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.18277551233768463 
model_pd.l_d.mean(): -19.944305419921875 
model_pd.lagr.mean(): -19.76152992248535 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5379], device='cuda:0')), ('power', tensor([-20.8743], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.18277551233768463
epoch£º5	 i:1 	 global-step:101	 l-p:0.14999274909496307
epoch£º5	 i:2 	 global-step:102	 l-p:0.1647845208644867
epoch£º5	 i:3 	 global-step:103	 l-p:0.07728911936283112
epoch£º5	 i:4 	 global-step:104	 l-p:0.1327490508556366
epoch£º5	 i:5 	 global-step:105	 l-p:0.15239235758781433
epoch£º5	 i:6 	 global-step:106	 l-p:0.12010861933231354
epoch£º5	 i:7 	 global-step:107	 l-p:0.13868780434131622
epoch£º5	 i:8 	 global-step:108	 l-p:0.1365223377943039
epoch£º5	 i:9 	 global-step:109	 l-p:0.13613934814929962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8715, 4.8993, 4.8764],
        [4.8715, 4.8715, 4.8715],
        [4.8715, 6.2817, 7.2225],
        [4.8715, 4.8763, 4.8718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.13218234479427338 
model_pd.l_d.mean(): -20.418516159057617 
model_pd.lagr.mean(): -20.286333084106445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4251], device='cuda:0')), ('power', tensor([-21.2406], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.13218234479427338
epoch£º6	 i:1 	 global-step:121	 l-p:0.1315603405237198
epoch£º6	 i:2 	 global-step:122	 l-p:0.13860024511814117
epoch£º6	 i:3 	 global-step:123	 l-p:0.14717994630336761
epoch£º6	 i:4 	 global-step:124	 l-p:0.1291673183441162
epoch£º6	 i:5 	 global-step:125	 l-p:0.15118078887462616
epoch£º6	 i:6 	 global-step:126	 l-p:0.13693654537200928
epoch£º6	 i:7 	 global-step:127	 l-p:0.12906837463378906
epoch£º6	 i:8 	 global-step:128	 l-p:0.13058076798915863
epoch£º6	 i:9 	 global-step:129	 l-p:0.12362636625766754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8997, 5.5526, 5.6828],
        [4.8997, 4.9624, 4.9180],
        [4.8997, 4.8997, 4.8997],
        [4.8997, 4.9209, 4.9028]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.1269131600856781 
model_pd.l_d.mean(): -20.290233612060547 
model_pd.lagr.mean(): -20.163320541381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4325], device='cuda:0')), ('power', tensor([-21.1176], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.1269131600856781
epoch£º7	 i:1 	 global-step:141	 l-p:0.11748652905225754
epoch£º7	 i:2 	 global-step:142	 l-p:0.13488292694091797
epoch£º7	 i:3 	 global-step:143	 l-p:0.13659486174583435
epoch£º7	 i:4 	 global-step:144	 l-p:0.14762330055236816
epoch£º7	 i:5 	 global-step:145	 l-p:3.5950937271118164
epoch£º7	 i:6 	 global-step:146	 l-p:0.12868492305278778
epoch£º7	 i:7 	 global-step:147	 l-p:0.13810274004936218
epoch£º7	 i:8 	 global-step:148	 l-p:0.16356855630874634
epoch£º7	 i:9 	 global-step:149	 l-p:0.13958829641342163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7086, 4.7086, 4.7086],
        [4.7086, 4.7087, 4.7086],
        [4.7086, 5.5089, 5.7876],
        [4.7086, 4.8519, 4.7821]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.13321110606193542 
model_pd.l_d.mean(): -18.60624885559082 
model_pd.lagr.mean(): -18.473037719726562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5675], device='cuda:0')), ('power', tensor([-19.5419], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.13321110606193542
epoch£º8	 i:1 	 global-step:161	 l-p:0.09418896585702896
epoch£º8	 i:2 	 global-step:162	 l-p:0.13871242105960846
epoch£º8	 i:3 	 global-step:163	 l-p:0.1417110562324524
epoch£º8	 i:4 	 global-step:164	 l-p:0.1269940286874771
epoch£º8	 i:5 	 global-step:165	 l-p:0.14698314666748047
epoch£º8	 i:6 	 global-step:166	 l-p:0.12891383469104767
epoch£º8	 i:7 	 global-step:167	 l-p:0.12988880276679993
epoch£º8	 i:8 	 global-step:168	 l-p:0.13997721672058105
epoch£º8	 i:9 	 global-step:169	 l-p:0.13983555138111115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8737, 4.8755, 4.8738],
        [4.8737, 4.8757, 4.8738],
        [4.8737, 5.0219, 4.9496],
        [4.8737, 4.8945, 4.8768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.13453274965286255 
model_pd.l_d.mean(): -17.52029037475586 
model_pd.lagr.mean(): -17.385757446289062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5526], device='cuda:0')), ('power', tensor([-18.4202], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.13453274965286255
epoch£º9	 i:1 	 global-step:181	 l-p:0.12755954265594482
epoch£º9	 i:2 	 global-step:182	 l-p:0.13793879747390747
epoch£º9	 i:3 	 global-step:183	 l-p:0.12984104454517365
epoch£º9	 i:4 	 global-step:184	 l-p:0.1204809918999672
epoch£º9	 i:5 	 global-step:185	 l-p:0.14611764252185822
epoch£º9	 i:6 	 global-step:186	 l-p:0.16321858763694763
epoch£º9	 i:7 	 global-step:187	 l-p:0.12874849140644073
epoch£º9	 i:8 	 global-step:188	 l-p:0.12757402658462524
epoch£º9	 i:9 	 global-step:189	 l-p:0.10954374074935913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8109, 6.0359, 6.7620],
        [4.8109, 5.6466, 5.9486],
        [4.8109, 4.8160, 4.8113],
        [4.8109, 4.8109, 4.8109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.15186405181884766 
model_pd.l_d.mean(): -20.063066482543945 
model_pd.lagr.mean(): -19.91120147705078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4818], device='cuda:0')), ('power', tensor([-20.9373], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.15186405181884766
epoch£º10	 i:1 	 global-step:201	 l-p:0.13907676935195923
epoch£º10	 i:2 	 global-step:202	 l-p:0.11484933644533157
epoch£º10	 i:3 	 global-step:203	 l-p:0.1323915421962738
epoch£º10	 i:4 	 global-step:204	 l-p:0.1326514333486557
epoch£º10	 i:5 	 global-step:205	 l-p:0.13942541182041168
epoch£º10	 i:6 	 global-step:206	 l-p:0.14921298623085022
epoch£º10	 i:7 	 global-step:207	 l-p:0.16363024711608887
epoch£º10	 i:8 	 global-step:208	 l-p:-0.12438510358333588
epoch£º10	 i:9 	 global-step:209	 l-p:0.14140933752059937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8006, 5.3208, 5.3671],
        [4.8006, 6.1096, 6.9424],
        [4.8006, 5.5127, 5.7048],
        [4.8006, 4.8237, 4.8043]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.11777813732624054 
model_pd.l_d.mean(): -19.93014907836914 
model_pd.lagr.mean(): -19.81237030029297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4915], device='cuda:0')), ('power', tensor([-20.8119], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.11777813732624054
epoch£º11	 i:1 	 global-step:221	 l-p:0.1492534577846527
epoch£º11	 i:2 	 global-step:222	 l-p:0.12402510643005371
epoch£º11	 i:3 	 global-step:223	 l-p:0.12629680335521698
epoch£º11	 i:4 	 global-step:224	 l-p:0.14166323840618134
epoch£º11	 i:5 	 global-step:225	 l-p:0.13777083158493042
epoch£º11	 i:6 	 global-step:226	 l-p:0.12515591084957123
epoch£º11	 i:7 	 global-step:227	 l-p:0.1959259808063507
epoch£º11	 i:8 	 global-step:228	 l-p:0.12264566868543625
epoch£º11	 i:9 	 global-step:229	 l-p:0.15399609506130219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9297, 5.3333, 5.3075],
        [4.9297, 4.9664, 4.9374],
        [4.9297, 4.9609, 4.9356],
        [4.9297, 5.1005, 5.0252]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.12222376465797424 
model_pd.l_d.mean(): -19.18491554260254 
model_pd.lagr.mean(): -19.062692642211914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5152], device='cuda:0')), ('power', tensor([-20.0772], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.12222376465797424
epoch£º12	 i:1 	 global-step:241	 l-p:0.13547232747077942
epoch£º12	 i:2 	 global-step:242	 l-p:0.11961772292852402
epoch£º12	 i:3 	 global-step:243	 l-p:0.13583286106586456
epoch£º12	 i:4 	 global-step:244	 l-p:0.1375637799501419
epoch£º12	 i:5 	 global-step:245	 l-p:0.13812550902366638
epoch£º12	 i:6 	 global-step:246	 l-p:0.13514664769172668
epoch£º12	 i:7 	 global-step:247	 l-p:0.1331748366355896
epoch£º12	 i:8 	 global-step:248	 l-p:0.13848130404949188
epoch£º12	 i:9 	 global-step:249	 l-p:0.09729297459125519
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8578, 6.0011, 6.6264],
        [4.8578, 4.8578, 4.8578],
        [4.8578, 4.8611, 4.8579],
        [4.8578, 5.2529, 5.2275]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.12300754338502884 
model_pd.l_d.mean(): -18.557348251342773 
model_pd.lagr.mean(): -18.434341430664062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5242], device='cuda:0')), ('power', tensor([-19.4473], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.12300754338502884
epoch£º13	 i:1 	 global-step:261	 l-p:0.18685704469680786
epoch£º13	 i:2 	 global-step:262	 l-p:0.1279771625995636
epoch£º13	 i:3 	 global-step:263	 l-p:0.13032415509223938
epoch£º13	 i:4 	 global-step:264	 l-p:0.13789190351963043
epoch£º13	 i:5 	 global-step:265	 l-p:0.1422492414712906
epoch£º13	 i:6 	 global-step:266	 l-p:0.11201074719429016
epoch£º13	 i:7 	 global-step:267	 l-p:0.13406595587730408
epoch£º13	 i:8 	 global-step:268	 l-p:0.1378362476825714
epoch£º13	 i:9 	 global-step:269	 l-p:0.1230882778763771
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8394, 4.8394, 4.8394],
        [4.8394, 5.7370, 6.1009],
        [4.8394, 4.8735, 4.8464],
        [4.8394, 5.6711, 5.9705]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.149934783577919 
model_pd.l_d.mean(): -18.42837905883789 
model_pd.lagr.mean(): -18.278444290161133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5182], device='cuda:0')), ('power', tensor([-19.3097], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.149934783577919
epoch£º14	 i:1 	 global-step:281	 l-p:0.12439268827438354
epoch£º14	 i:2 	 global-step:282	 l-p:0.16560618579387665
epoch£º14	 i:3 	 global-step:283	 l-p:0.1400095373392105
epoch£º14	 i:4 	 global-step:284	 l-p:0.13504065573215485
epoch£º14	 i:5 	 global-step:285	 l-p:0.12264902144670486
epoch£º14	 i:6 	 global-step:286	 l-p:0.11743034422397614
epoch£º14	 i:7 	 global-step:287	 l-p:0.13086523115634918
epoch£º14	 i:8 	 global-step:288	 l-p:0.13037759065628052
epoch£º14	 i:9 	 global-step:289	 l-p:-0.026051977649331093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7564, 4.7874, 4.7625],
        [4.7564, 4.7601, 4.7566],
        [4.7564, 4.9276, 4.8556],
        [4.7564, 4.7670, 4.7575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.15474551916122437 
model_pd.l_d.mean(): -20.384502410888672 
model_pd.lagr.mean(): -20.22975730895996 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4647], device='cuda:0')), ('power', tensor([-21.2470], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.15474551916122437
epoch£º15	 i:1 	 global-step:301	 l-p:0.1601480096578598
epoch£º15	 i:2 	 global-step:302	 l-p:0.12358197569847107
epoch£º15	 i:3 	 global-step:303	 l-p:0.1438024789094925
epoch£º15	 i:4 	 global-step:304	 l-p:0.15779419243335724
epoch£º15	 i:5 	 global-step:305	 l-p:0.14565031230449677
epoch£º15	 i:6 	 global-step:306	 l-p:0.11899891495704651
epoch£º15	 i:7 	 global-step:307	 l-p:0.1423284262418747
epoch£º15	 i:8 	 global-step:308	 l-p:0.13303373754024506
epoch£º15	 i:9 	 global-step:309	 l-p:0.20450687408447266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8643, 4.8662, 4.8644],
        [4.8643, 4.8904, 4.8689],
        [4.8643, 4.8647, 4.8643],
        [4.8643, 5.2214, 5.1815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.12048551440238953 
model_pd.l_d.mean(): -18.84018325805664 
model_pd.lagr.mean(): -18.719697952270508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4854], device='cuda:0')), ('power', tensor([-19.6953], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.12048551440238953
epoch£º16	 i:1 	 global-step:321	 l-p:0.11616602540016174
epoch£º16	 i:2 	 global-step:322	 l-p:0.15070821344852448
epoch£º16	 i:3 	 global-step:323	 l-p:0.13504715263843536
epoch£º16	 i:4 	 global-step:324	 l-p:0.12404441833496094
epoch£º16	 i:5 	 global-step:325	 l-p:0.11437847465276718
epoch£º16	 i:6 	 global-step:326	 l-p:0.13316607475280762
epoch£º16	 i:7 	 global-step:327	 l-p:0.15229082107543945
epoch£º16	 i:8 	 global-step:328	 l-p:0.12954570353031158
epoch£º16	 i:9 	 global-step:329	 l-p:0.128326877951622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9148, 5.8438, 6.2328],
        [4.9148, 5.2293, 5.1732],
        [4.9148, 4.9149, 4.9148],
        [4.9148, 5.8316, 6.2083]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.15870104730129242 
model_pd.l_d.mean(): -20.015905380249023 
model_pd.lagr.mean(): -19.85720443725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4497], device='cuda:0')), ('power', tensor([-20.8559], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.15870104730129242
epoch£º17	 i:1 	 global-step:341	 l-p:0.12112600356340408
epoch£º17	 i:2 	 global-step:342	 l-p:0.13697052001953125
epoch£º17	 i:3 	 global-step:343	 l-p:0.139126256108284
epoch£º17	 i:4 	 global-step:344	 l-p:0.13112737238407135
epoch£º17	 i:5 	 global-step:345	 l-p:0.12582308053970337
epoch£º17	 i:6 	 global-step:346	 l-p:0.13766808807849884
epoch£º17	 i:7 	 global-step:347	 l-p:0.14249178767204285
epoch£º17	 i:8 	 global-step:348	 l-p:0.13055185973644257
epoch£º17	 i:9 	 global-step:349	 l-p:0.13331280648708344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7533, 4.8013, 4.7659],
        [4.7533, 4.7536, 4.7533],
        [4.7533, 4.8647, 4.8029],
        [4.7533, 4.7657, 4.7547]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.14220768213272095 
model_pd.l_d.mean(): -19.897676467895508 
model_pd.lagr.mean(): -19.755468368530273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5123], device='cuda:0')), ('power', tensor([-20.8004], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.14220768213272095
epoch£º18	 i:1 	 global-step:361	 l-p:0.1941501945257187
epoch£º18	 i:2 	 global-step:362	 l-p:0.13479341566562653
epoch£º18	 i:3 	 global-step:363	 l-p:0.1730835735797882
epoch£º18	 i:4 	 global-step:364	 l-p:0.13130293786525726
epoch£º18	 i:5 	 global-step:365	 l-p:0.13695693016052246
epoch£º18	 i:6 	 global-step:366	 l-p:0.1332370638847351
epoch£º18	 i:7 	 global-step:367	 l-p:0.04292752221226692
epoch£º18	 i:8 	 global-step:368	 l-p:0.13950838148593903
epoch£º18	 i:9 	 global-step:369	 l-p:0.14108100533485413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7806, 4.9532, 4.8817],
        [4.7806, 4.8101, 4.7863],
        [4.7806, 4.7806, 4.7806],
        [4.7806, 4.9857, 4.9143]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.1465989500284195 
model_pd.l_d.mean(): -20.357336044311523 
model_pd.lagr.mean(): -20.210737228393555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4510], device='cuda:0')), ('power', tensor([-21.2052], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.1465989500284195
epoch£º19	 i:1 	 global-step:381	 l-p:0.1452765017747879
epoch£º19	 i:2 	 global-step:382	 l-p:0.13275618851184845
epoch£º19	 i:3 	 global-step:383	 l-p:0.13093310594558716
epoch£º19	 i:4 	 global-step:384	 l-p:0.12443283945322037
epoch£º19	 i:5 	 global-step:385	 l-p:0.1444663256406784
epoch£º19	 i:6 	 global-step:386	 l-p:0.12928685545921326
epoch£º19	 i:7 	 global-step:387	 l-p:0.12424381077289581
epoch£º19	 i:8 	 global-step:388	 l-p:0.10942992568016052
epoch£º19	 i:9 	 global-step:389	 l-p:0.1688329130411148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9303, 4.9330, 4.9305],
        [4.9303, 5.3530, 5.3416],
        [4.9303, 4.9726, 4.9403],
        [4.9303, 4.9913, 4.9485]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.12406428158283234 
model_pd.l_d.mean(): -20.310688018798828 
model_pd.lagr.mean(): -20.18662452697754 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4214], device='cuda:0')), ('power', tensor([-21.1269], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.12406428158283234
epoch£º20	 i:1 	 global-step:401	 l-p:0.12725572288036346
epoch£º20	 i:2 	 global-step:402	 l-p:0.1217140182852745
epoch£º20	 i:3 	 global-step:403	 l-p:0.18929217755794525
epoch£º20	 i:4 	 global-step:404	 l-p:0.1399947553873062
epoch£º20	 i:5 	 global-step:405	 l-p:0.13850507140159607
epoch£º20	 i:6 	 global-step:406	 l-p:0.12436216324567795
epoch£º20	 i:7 	 global-step:407	 l-p:0.13034433126449585
epoch£º20	 i:8 	 global-step:408	 l-p:0.12495027482509613
epoch£º20	 i:9 	 global-step:409	 l-p:0.1272892951965332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9305, 6.3456, 7.2993],
        [4.9305, 4.9921, 4.9490],
        [4.9305, 4.9305, 4.9305],
        [4.9305, 5.0307, 4.9714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.1321074664592743 
model_pd.l_d.mean(): -20.15224838256836 
model_pd.lagr.mean(): -20.0201416015625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4383], device='cuda:0')), ('power', tensor([-20.9830], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.1321074664592743
epoch£º21	 i:1 	 global-step:421	 l-p:0.12820331752300262
epoch£º21	 i:2 	 global-step:422	 l-p:0.13581880927085876
epoch£º21	 i:3 	 global-step:423	 l-p:0.12991531193256378
epoch£º21	 i:4 	 global-step:424	 l-p:0.16044455766677856
epoch£º21	 i:5 	 global-step:425	 l-p:0.13445843756198883
epoch£º21	 i:6 	 global-step:426	 l-p:0.15661875903606415
epoch£º21	 i:7 	 global-step:427	 l-p:0.16106420755386353
epoch£º21	 i:8 	 global-step:428	 l-p:0.18458300828933716
epoch£º21	 i:9 	 global-step:429	 l-p:0.13455376029014587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7049, 4.8011, 4.7447],
        [4.7049, 4.9854, 4.9294],
        [4.7049, 4.7070, 4.7049],
        [4.7049, 4.7350, 4.7108]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.18089930713176727 
model_pd.l_d.mean(): -20.365995407104492 
model_pd.lagr.mean(): -20.185096740722656 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4818], device='cuda:0')), ('power', tensor([-21.2458], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.18089930713176727
epoch£º22	 i:1 	 global-step:441	 l-p:0.13948875665664673
epoch£º22	 i:2 	 global-step:442	 l-p:0.1341520994901657
epoch£º22	 i:3 	 global-step:443	 l-p:0.13339613378047943
epoch£º22	 i:4 	 global-step:444	 l-p:0.11401380598545074
epoch£º22	 i:5 	 global-step:445	 l-p:0.1458567976951599
epoch£º22	 i:6 	 global-step:446	 l-p:0.11964736878871918
epoch£º22	 i:7 	 global-step:447	 l-p:0.13929767906665802
epoch£º22	 i:8 	 global-step:448	 l-p:0.1352204531431198
epoch£º22	 i:9 	 global-step:449	 l-p:0.12017141282558441
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0081, 6.4428, 7.4071],
        [5.0081, 5.5637, 5.6280],
        [5.0081, 5.0087, 5.0081],
        [5.0081, 5.9409, 6.3278]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.10455396771430969 
model_pd.l_d.mean(): -19.83930015563965 
model_pd.lagr.mean(): -19.7347469329834 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4411], device='cuda:0')), ('power', tensor([-20.6671], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.10455396771430969
epoch£º23	 i:1 	 global-step:461	 l-p:0.1312193125486374
epoch£º23	 i:2 	 global-step:462	 l-p:0.13759589195251465
epoch£º23	 i:3 	 global-step:463	 l-p:0.1152392327785492
epoch£º23	 i:4 	 global-step:464	 l-p:0.13728076219558716
epoch£º23	 i:5 	 global-step:465	 l-p:0.13085095584392548
epoch£º23	 i:6 	 global-step:466	 l-p:0.1501273214817047
epoch£º23	 i:7 	 global-step:467	 l-p:0.1275864988565445
epoch£º23	 i:8 	 global-step:468	 l-p:0.14217019081115723
epoch£º23	 i:9 	 global-step:469	 l-p:0.14453592896461487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8415, 5.2905, 5.3007],
        [4.8415, 5.1257, 5.0669],
        [4.8415, 4.9219, 4.8707],
        [4.8415, 4.8415, 4.8415]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.13746453821659088 
model_pd.l_d.mean(): -20.788225173950195 
model_pd.lagr.mean(): -20.650760650634766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3978], device='cuda:0')), ('power', tensor([-21.5889], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.13746453821659088
epoch£º24	 i:1 	 global-step:481	 l-p:0.14466258883476257
epoch£º24	 i:2 	 global-step:482	 l-p:0.12076641619205475
epoch£º24	 i:3 	 global-step:483	 l-p:0.11947612464427948
epoch£º24	 i:4 	 global-step:484	 l-p:0.1403537541627884
epoch£º24	 i:5 	 global-step:485	 l-p:0.11619935929775238
epoch£º24	 i:6 	 global-step:486	 l-p:0.1423410028219223
epoch£º24	 i:7 	 global-step:487	 l-p:0.14003777503967285
epoch£º24	 i:8 	 global-step:488	 l-p:0.18100424110889435
epoch£º24	 i:9 	 global-step:489	 l-p:0.12379538267850876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8980, 4.8980, 4.8980],
        [4.8980, 4.8980, 4.8980],
        [4.8980, 4.8980, 4.8980],
        [4.8980, 6.0289, 6.6475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.13305102288722992 
model_pd.l_d.mean(): -18.20371437072754 
model_pd.lagr.mean(): -18.070663452148438 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5730], device='cuda:0')), ('power', tensor([-19.1376], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.13305102288722992
epoch£º25	 i:1 	 global-step:501	 l-p:0.1339624673128128
epoch£º25	 i:2 	 global-step:502	 l-p:0.1412295252084732
epoch£º25	 i:3 	 global-step:503	 l-p:0.12349506467580795
epoch£º25	 i:4 	 global-step:504	 l-p:0.2816760838031769
epoch£º25	 i:5 	 global-step:505	 l-p:0.14605237543582916
epoch£º25	 i:6 	 global-step:506	 l-p:0.12947280704975128
epoch£º25	 i:7 	 global-step:507	 l-p:0.12791021168231964
epoch£º25	 i:8 	 global-step:508	 l-p:0.1480344533920288
epoch£º25	 i:9 	 global-step:509	 l-p:0.12818939983844757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8380, 5.0279, 4.9567],
        [4.8380, 5.4360, 5.5466],
        [4.8380, 4.8970, 4.8557],
        [4.8380, 5.3537, 5.4062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.15482544898986816 
model_pd.l_d.mean(): -20.40583038330078 
model_pd.lagr.mean(): -20.251005172729492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4437], device='cuda:0')), ('power', tensor([-21.2469], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.15482544898986816
epoch£º26	 i:1 	 global-step:521	 l-p:0.13785654306411743
epoch£º26	 i:2 	 global-step:522	 l-p:0.1387440413236618
epoch£º26	 i:3 	 global-step:523	 l-p:0.12667307257652283
epoch£º26	 i:4 	 global-step:524	 l-p:-0.0066443681716918945
epoch£º26	 i:5 	 global-step:525	 l-p:0.1559193879365921
epoch£º26	 i:6 	 global-step:526	 l-p:0.14105840027332306
epoch£º26	 i:7 	 global-step:527	 l-p:0.14322657883167267
epoch£º26	 i:8 	 global-step:528	 l-p:0.14025826752185822
epoch£º26	 i:9 	 global-step:529	 l-p:0.1292801797389984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8569, 4.9725, 4.9099],
        [4.8569, 5.1866, 5.1424],
        [4.8569, 4.8569, 4.8569],
        [4.8569, 4.8594, 4.8570]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.12456660717725754 
model_pd.l_d.mean(): -18.112218856811523 
model_pd.lagr.mean(): -17.987651824951172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5407], device='cuda:0')), ('power', tensor([-19.0109], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.12456660717725754
epoch£º27	 i:1 	 global-step:541	 l-p:0.13089346885681152
epoch£º27	 i:2 	 global-step:542	 l-p:0.11914840340614319
epoch£º27	 i:3 	 global-step:543	 l-p:0.13946668803691864
epoch£º27	 i:4 	 global-step:544	 l-p:0.12975473701953888
epoch£º27	 i:5 	 global-step:545	 l-p:0.13794122636318207
epoch£º27	 i:6 	 global-step:546	 l-p:0.13376444578170776
epoch£º27	 i:7 	 global-step:547	 l-p:0.1461104303598404
epoch£º27	 i:8 	 global-step:548	 l-p:0.2802717387676239
epoch£º27	 i:9 	 global-step:549	 l-p:0.13422837853431702
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8425, 5.0409, 4.9704],
        [4.8425, 5.4369, 5.5459],
        [4.8425, 4.8905, 4.8552],
        [4.8425, 6.1483, 6.9892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.13190031051635742 
model_pd.l_d.mean(): -19.747665405273438 
model_pd.lagr.mean(): -19.615764617919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4967], device='cuda:0')), ('power', tensor([-20.6314], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.13190031051635742
epoch£º28	 i:1 	 global-step:561	 l-p:0.15929755568504333
epoch£º28	 i:2 	 global-step:562	 l-p:0.12898124754428864
epoch£º28	 i:3 	 global-step:563	 l-p:0.12719666957855225
epoch£º28	 i:4 	 global-step:564	 l-p:0.1358698308467865
epoch£º28	 i:5 	 global-step:565	 l-p:0.15626247227191925
epoch£º28	 i:6 	 global-step:566	 l-p:0.12274213880300522
epoch£º28	 i:7 	 global-step:567	 l-p:0.12279241532087326
epoch£º28	 i:8 	 global-step:568	 l-p:0.12731575965881348
epoch£º28	 i:9 	 global-step:569	 l-p:0.1377333104610443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0190, 5.0190, 5.0190],
        [5.0190, 5.2241, 5.1505],
        [5.0190, 5.6689, 5.8055],
        [5.0190, 5.0233, 5.0192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.1358269900083542 
model_pd.l_d.mean(): -19.511545181274414 
model_pd.lagr.mean(): -19.37571907043457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4460], device='cuda:0')), ('power', tensor([-20.3384], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.1358269900083542
epoch£º29	 i:1 	 global-step:581	 l-p:0.10703989118337631
epoch£º29	 i:2 	 global-step:582	 l-p:0.11940094083547592
epoch£º29	 i:3 	 global-step:583	 l-p:0.13809190690517426
epoch£º29	 i:4 	 global-step:584	 l-p:0.13165901601314545
epoch£º29	 i:5 	 global-step:585	 l-p:0.1396390050649643
epoch£º29	 i:6 	 global-step:586	 l-p:0.20349252223968506
epoch£º29	 i:7 	 global-step:587	 l-p:0.12456325441598892
epoch£º29	 i:8 	 global-step:588	 l-p:0.1283804029226303
epoch£º29	 i:9 	 global-step:589	 l-p:0.13200712203979492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8416, 4.8432, 4.8417],
        [4.8416, 6.1302, 6.9518],
        [4.8416, 5.0536, 4.9844],
        [4.8416, 6.0115, 6.6901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.14958329498767853 
model_pd.l_d.mean(): -18.799638748168945 
model_pd.lagr.mean(): -18.650054931640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5591], device='cuda:0')), ('power', tensor([-19.7302], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.14958329498767853
epoch£º30	 i:1 	 global-step:601	 l-p:0.12171147018671036
epoch£º30	 i:2 	 global-step:602	 l-p:0.12703466415405273
epoch£º30	 i:3 	 global-step:603	 l-p:-0.10031507909297943
epoch£º30	 i:4 	 global-step:604	 l-p:0.13050204515457153
epoch£º30	 i:5 	 global-step:605	 l-p:0.1659736931324005
epoch£º30	 i:6 	 global-step:606	 l-p:0.1371471881866455
epoch£º30	 i:7 	 global-step:607	 l-p:0.13003598153591156
epoch£º30	 i:8 	 global-step:608	 l-p:0.12919752299785614
epoch£º30	 i:9 	 global-step:609	 l-p:0.15413495898246765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7297, 4.7298, 4.7297],
        [4.7297, 4.7377, 4.7305],
        [4.7297, 6.0483, 6.9312],
        [4.7297, 4.7836, 4.7455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.13342322409152985 
model_pd.l_d.mean(): -17.924278259277344 
model_pd.lagr.mean(): -17.790855407714844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6312], device='cuda:0')), ('power', tensor([-18.9132], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.13342322409152985
epoch£º31	 i:1 	 global-step:621	 l-p:0.16901881992816925
epoch£º31	 i:2 	 global-step:622	 l-p:0.1203412339091301
epoch£º31	 i:3 	 global-step:623	 l-p:0.13260026276111603
epoch£º31	 i:4 	 global-step:624	 l-p:0.13140983879566193
epoch£º31	 i:5 	 global-step:625	 l-p:0.1262805014848709
epoch£º31	 i:6 	 global-step:626	 l-p:0.1482732594013214
epoch£º31	 i:7 	 global-step:627	 l-p:0.1943294256925583
epoch£º31	 i:8 	 global-step:628	 l-p:0.14663641154766083
epoch£º31	 i:9 	 global-step:629	 l-p:0.1626594364643097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8204, 6.2530, 7.2642],
        [4.8204, 5.7503, 6.1683],
        [4.8204, 4.8769, 4.8372],
        [4.8204, 4.8219, 4.8205]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.13449986279010773 
model_pd.l_d.mean(): -19.75826072692871 
model_pd.lagr.mean(): -19.623760223388672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4763], device='cuda:0')), ('power', tensor([-20.6210], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.13449986279010773
epoch£º32	 i:1 	 global-step:641	 l-p:0.13999111950397491
epoch£º32	 i:2 	 global-step:642	 l-p:0.1512090414762497
epoch£º32	 i:3 	 global-step:643	 l-p:0.12232215702533722
epoch£º32	 i:4 	 global-step:644	 l-p:0.17643560469150543
epoch£º32	 i:5 	 global-step:645	 l-p:0.13882814347743988
epoch£º32	 i:6 	 global-step:646	 l-p:0.15360412001609802
epoch£º32	 i:7 	 global-step:647	 l-p:0.2623617351055145
epoch£º32	 i:8 	 global-step:648	 l-p:0.11902746558189392
epoch£º32	 i:9 	 global-step:649	 l-p:0.1314990222454071
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9114, 4.9117, 4.9114],
        [4.9114, 5.1562, 5.0899],
        [4.9114, 4.9121, 4.9114],
        [4.9114, 5.1564, 5.0902]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.11754680424928665 
model_pd.l_d.mean(): -20.188016891479492 
model_pd.lagr.mean(): -20.070470809936523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4443], device='cuda:0')), ('power', tensor([-21.0256], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.11754680424928665
epoch£º33	 i:1 	 global-step:661	 l-p:0.19598165154457092
epoch£º33	 i:2 	 global-step:662	 l-p:0.13850435614585876
epoch£º33	 i:3 	 global-step:663	 l-p:0.11721779406070709
epoch£º33	 i:4 	 global-step:664	 l-p:0.13960042595863342
epoch£º33	 i:5 	 global-step:665	 l-p:0.1327797770500183
epoch£º33	 i:6 	 global-step:666	 l-p:0.14769713580608368
epoch£º33	 i:7 	 global-step:667	 l-p:0.15611161291599274
epoch£º33	 i:8 	 global-step:668	 l-p:0.1344776153564453
epoch£º33	 i:9 	 global-step:669	 l-p:0.19178561866283417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7650, 5.3446, 5.4528],
        [4.7650, 4.7652, 4.7650],
        [4.7650, 5.0357, 4.9794],
        [4.7650, 4.8187, 4.7807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.14855214953422546 
model_pd.l_d.mean(): -20.245502471923828 
model_pd.lagr.mean(): -20.09695053100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4809], device='cuda:0')), ('power', tensor([-21.1221], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.14855214953422546
epoch£º34	 i:1 	 global-step:681	 l-p:0.14329156279563904
epoch£º34	 i:2 	 global-step:682	 l-p:0.1319740116596222
epoch£º34	 i:3 	 global-step:683	 l-p:0.12682463228702545
epoch£º34	 i:4 	 global-step:684	 l-p:0.1559409499168396
epoch£º34	 i:5 	 global-step:685	 l-p:0.14071790874004364
epoch£º34	 i:6 	 global-step:686	 l-p:0.1390901356935501
epoch£º34	 i:7 	 global-step:687	 l-p:0.1361132711172104
epoch£º34	 i:8 	 global-step:688	 l-p:0.12391102313995361
epoch£º34	 i:9 	 global-step:689	 l-p:0.1699853092432022
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9343, 5.3890, 5.4026],
        [4.9343, 4.9343, 4.9342],
        [4.9343, 5.0663, 5.0002],
        [4.9343, 6.2903, 7.1812]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.11928947269916534 
model_pd.l_d.mean(): -19.74420928955078 
model_pd.lagr.mean(): -19.624919891357422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4773], device='cuda:0')), ('power', tensor([-20.6077], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.11928947269916534
epoch£º35	 i:1 	 global-step:701	 l-p:0.13338235020637512
epoch£º35	 i:2 	 global-step:702	 l-p:0.15139418840408325
epoch£º35	 i:3 	 global-step:703	 l-p:0.12809225916862488
epoch£º35	 i:4 	 global-step:704	 l-p:0.11593816429376602
epoch£º35	 i:5 	 global-step:705	 l-p:0.1318429410457611
epoch£º35	 i:6 	 global-step:706	 l-p:0.14819779992103577
epoch£º35	 i:7 	 global-step:707	 l-p:0.12719331681728363
epoch£º35	 i:8 	 global-step:708	 l-p:0.10716976225376129
epoch£º35	 i:9 	 global-step:709	 l-p:0.12216411530971527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8912, 4.8912, 4.8912],
        [4.8912, 4.8912, 4.8912],
        [4.8912, 5.5273, 5.6691],
        [4.8912, 4.9370, 4.9031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.13497722148895264 
model_pd.l_d.mean(): -18.880868911743164 
model_pd.lagr.mean(): -18.745891571044922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5108], device='cuda:0')), ('power', tensor([-19.7630], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.13497722148895264
epoch£º36	 i:1 	 global-step:721	 l-p:0.11781992763280869
epoch£º36	 i:2 	 global-step:722	 l-p:0.14313441514968872
epoch£º36	 i:3 	 global-step:723	 l-p:0.11881996691226959
epoch£º36	 i:4 	 global-step:724	 l-p:0.1341509073972702
epoch£º36	 i:5 	 global-step:725	 l-p:0.150386780500412
epoch£º36	 i:6 	 global-step:726	 l-p:0.1522763967514038
epoch£º36	 i:7 	 global-step:727	 l-p:0.13849741220474243
epoch£º36	 i:8 	 global-step:728	 l-p:-1.9889860153198242
epoch£º36	 i:9 	 global-step:729	 l-p:0.13113698363304138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8249, 4.8309, 4.8254],
        [4.8249, 6.1056, 6.9268],
        [4.8249, 5.2139, 5.2003],
        [4.8249, 5.3129, 5.3547]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.1283782720565796 
model_pd.l_d.mean(): -18.386497497558594 
model_pd.lagr.mean(): -18.258119583129883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5705], device='cuda:0')), ('power', tensor([-19.3212], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.1283782720565796
epoch£º37	 i:1 	 global-step:741	 l-p:0.12662328779697418
epoch£º37	 i:2 	 global-step:742	 l-p:0.22843153774738312
epoch£º37	 i:3 	 global-step:743	 l-p:0.12609504163265228
epoch£º37	 i:4 	 global-step:744	 l-p:0.140171080827713
epoch£º37	 i:5 	 global-step:745	 l-p:0.1320103257894516
epoch£º37	 i:6 	 global-step:746	 l-p:0.12672768533229828
epoch£º37	 i:7 	 global-step:747	 l-p:0.11788617819547653
epoch£º37	 i:8 	 global-step:748	 l-p:0.12281251698732376
epoch£º37	 i:9 	 global-step:749	 l-p:0.11865806579589844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9127, 5.0225, 4.9621],
        [4.9127, 4.9779, 4.9339],
        [4.9127, 4.9379, 4.9173],
        [4.9127, 4.9749, 4.9323]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.15942497551441193 
model_pd.l_d.mean(): -19.73154640197754 
model_pd.lagr.mean(): -19.572120666503906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4822], device='cuda:0')), ('power', tensor([-20.6000], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.15942497551441193
epoch£º38	 i:1 	 global-step:761	 l-p:0.1410004198551178
epoch£º38	 i:2 	 global-step:762	 l-p:0.12883219122886658
epoch£º38	 i:3 	 global-step:763	 l-p:0.12412212044000626
epoch£º38	 i:4 	 global-step:764	 l-p:0.13281235098838806
epoch£º38	 i:5 	 global-step:765	 l-p:0.12320206314325333
epoch£º38	 i:6 	 global-step:766	 l-p:0.17454276978969574
epoch£º38	 i:7 	 global-step:767	 l-p:0.13884511590003967
epoch£º38	 i:8 	 global-step:768	 l-p:0.14511655271053314
epoch£º38	 i:9 	 global-step:769	 l-p:0.12236213684082031
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7759, 4.7759, 4.7759],
        [4.7759, 5.8211, 6.3756],
        [4.7759, 4.7759, 4.7759],
        [4.7759, 4.7778, 4.7760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.13733181357383728 
model_pd.l_d.mean(): -20.61395835876465 
model_pd.lagr.mean(): -20.476627349853516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4396], device='cuda:0')), ('power', tensor([-21.4548], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.13733181357383728
epoch£º39	 i:1 	 global-step:781	 l-p:0.13291825354099274
epoch£º39	 i:2 	 global-step:782	 l-p:0.10598301887512207
epoch£º39	 i:3 	 global-step:783	 l-p:0.13837002217769623
epoch£º39	 i:4 	 global-step:784	 l-p:0.16713492572307587
epoch£º39	 i:5 	 global-step:785	 l-p:0.19006167352199554
epoch£º39	 i:6 	 global-step:786	 l-p:0.1447121649980545
epoch£º39	 i:7 	 global-step:787	 l-p:0.14358565211296082
epoch£º39	 i:8 	 global-step:788	 l-p:0.13650628924369812
epoch£º39	 i:9 	 global-step:789	 l-p:0.12330317497253418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9677, 4.9679, 4.9677],
        [4.9677, 4.9863, 4.9705],
        [4.9677, 5.0802, 5.0188],
        [4.9677, 4.9752, 4.9684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.1116957813501358 
model_pd.l_d.mean(): -20.385908126831055 
model_pd.lagr.mean(): -20.274211883544922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4098], device='cuda:0')), ('power', tensor([-21.1916], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.1116957813501358
epoch£º40	 i:1 	 global-step:801	 l-p:0.1297234296798706
epoch£º40	 i:2 	 global-step:802	 l-p:0.11579782515764236
epoch£º40	 i:3 	 global-step:803	 l-p:0.15086960792541504
epoch£º40	 i:4 	 global-step:804	 l-p:0.12315420061349869
epoch£º40	 i:5 	 global-step:805	 l-p:0.11637749522924423
epoch£º40	 i:6 	 global-step:806	 l-p:0.125308096408844
epoch£º40	 i:7 	 global-step:807	 l-p:0.1904204934835434
epoch£º40	 i:8 	 global-step:808	 l-p:0.1286517232656479
epoch£º40	 i:9 	 global-step:809	 l-p:0.14410533010959625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9452, 4.9478, 4.9453],
        [4.9452, 4.9455, 4.9452],
        [4.9452, 5.1288, 5.0586],
        [4.9452, 5.0598, 4.9981]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.14146535098552704 
model_pd.l_d.mean(): -20.310747146606445 
model_pd.lagr.mean(): -20.169281005859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4177], device='cuda:0')), ('power', tensor([-21.1231], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.14146535098552704
epoch£º41	 i:1 	 global-step:821	 l-p:0.12122326344251633
epoch£º41	 i:2 	 global-step:822	 l-p:0.11717048287391663
epoch£º41	 i:3 	 global-step:823	 l-p:0.1319550722837448
epoch£º41	 i:4 	 global-step:824	 l-p:0.20661765336990356
epoch£º41	 i:5 	 global-step:825	 l-p:0.12286627292633057
epoch£º41	 i:6 	 global-step:826	 l-p:0.13028188049793243
epoch£º41	 i:7 	 global-step:827	 l-p:0.12176599353551865
epoch£º41	 i:8 	 global-step:828	 l-p:0.13030791282653809
epoch£º41	 i:9 	 global-step:829	 l-p:0.16163703799247742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8443, 5.1233, 5.0689],
        [4.8443, 4.8443, 4.8443],
        [4.8443, 6.2088, 7.1351],
        [4.8443, 4.8599, 4.8464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.12175033241510391 
model_pd.l_d.mean(): -18.8397159576416 
model_pd.lagr.mean(): -18.717966079711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5206], device='cuda:0')), ('power', tensor([-19.7312], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.12175033241510391
epoch£º42	 i:1 	 global-step:841	 l-p:0.1343744695186615
epoch£º42	 i:2 	 global-step:842	 l-p:0.1388714462518692
epoch£º42	 i:3 	 global-step:843	 l-p:0.20727430284023285
epoch£º42	 i:4 	 global-step:844	 l-p:0.15114736557006836
epoch£º42	 i:5 	 global-step:845	 l-p:0.146175816655159
epoch£º42	 i:6 	 global-step:846	 l-p:0.13443420827388763
epoch£º42	 i:7 	 global-step:847	 l-p:0.0913648009300232
epoch£º42	 i:8 	 global-step:848	 l-p:0.13778936862945557
epoch£º42	 i:9 	 global-step:849	 l-p:0.1585918813943863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4331,  0.3277,  1.0000,  0.2480,
          1.0000,  0.7566, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[4.7761, 5.9809, 6.7238],
        [4.7761, 5.3184, 5.4036],
        [4.7761, 5.0750, 5.0292],
        [4.7761, 5.9210, 6.5927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.05469449236989021 
model_pd.l_d.mean(): -19.768604278564453 
model_pd.lagr.mean(): -19.713909149169922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5163], device='cuda:0')), ('power', tensor([-20.6730], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.05469449236989021
epoch£º43	 i:1 	 global-step:861	 l-p:0.1478758454322815
epoch£º43	 i:2 	 global-step:862	 l-p:0.13405953347682953
epoch£º43	 i:3 	 global-step:863	 l-p:0.1317068636417389
epoch£º43	 i:4 	 global-step:864	 l-p:0.13965946435928345
epoch£º43	 i:5 	 global-step:865	 l-p:0.12777991592884064
epoch£º43	 i:6 	 global-step:866	 l-p:0.1297401636838913
epoch£º43	 i:7 	 global-step:867	 l-p:0.14066365361213684
epoch£º43	 i:8 	 global-step:868	 l-p:0.11958066374063492
epoch£º43	 i:9 	 global-step:869	 l-p:0.13988259434700012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8964, 5.0224, 4.9589],
        [4.8964, 4.9034, 4.8970],
        [4.8964, 4.8987, 4.8965],
        [4.8964, 4.8965, 4.8964]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.13603362441062927 
model_pd.l_d.mean(): -20.27545928955078 
model_pd.lagr.mean(): -20.13942527770996 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4302], device='cuda:0')), ('power', tensor([-21.1002], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.13603362441062927
epoch£º44	 i:1 	 global-step:881	 l-p:0.1375376284122467
epoch£º44	 i:2 	 global-step:882	 l-p:-0.13869856297969818
epoch£º44	 i:3 	 global-step:883	 l-p:0.1393871158361435
epoch£º44	 i:4 	 global-step:884	 l-p:0.13581441342830658
epoch£º44	 i:5 	 global-step:885	 l-p:0.12904642522335052
epoch£º44	 i:6 	 global-step:886	 l-p:0.11224675923585892
epoch£º44	 i:7 	 global-step:887	 l-p:0.1511635035276413
epoch£º44	 i:8 	 global-step:888	 l-p:0.16516666114330292
epoch£º44	 i:9 	 global-step:889	 l-p:0.1267377883195877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8645, 4.8646, 4.8645],
        [4.8645, 4.8656, 4.8646],
        [4.8645, 5.0966, 5.0325],
        [4.8645, 4.8645, 4.8645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.13244186341762543 
model_pd.l_d.mean(): -20.66280746459961 
model_pd.lagr.mean(): -20.530364990234375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4106], device='cuda:0')), ('power', tensor([-21.4744], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.13244186341762543
epoch£º45	 i:1 	 global-step:901	 l-p:0.13065017759799957
epoch£º45	 i:2 	 global-step:902	 l-p:0.11144464462995529
epoch£º45	 i:3 	 global-step:903	 l-p:0.12557147443294525
epoch£º45	 i:4 	 global-step:904	 l-p:0.1491808444261551
epoch£º45	 i:5 	 global-step:905	 l-p:-0.040254782885313034
epoch£º45	 i:6 	 global-step:906	 l-p:0.14079926908016205
epoch£º45	 i:7 	 global-step:907	 l-p:0.14134658873081207
epoch£º45	 i:8 	 global-step:908	 l-p:0.15947148203849792
epoch£º45	 i:9 	 global-step:909	 l-p:0.12821629643440247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8932, 4.8974, 4.8934],
        [4.8932, 4.9196, 4.8981],
        [4.8932, 4.8932, 4.8932],
        [4.8932, 4.9190, 4.8980]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.11778853833675385 
model_pd.l_d.mean(): -19.52469253540039 
model_pd.lagr.mean(): -19.406904220581055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4899], device='cuda:0')), ('power', tensor([-20.3972], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.11778853833675385
epoch£º46	 i:1 	 global-step:921	 l-p:0.21427735686302185
epoch£º46	 i:2 	 global-step:922	 l-p:0.1270037293434143
epoch£º46	 i:3 	 global-step:923	 l-p:0.1249251738190651
epoch£º46	 i:4 	 global-step:924	 l-p:0.13933169841766357
epoch£º46	 i:5 	 global-step:925	 l-p:0.115501269698143
epoch£º46	 i:6 	 global-step:926	 l-p:0.1442699134349823
epoch£º46	 i:7 	 global-step:927	 l-p:0.13146446645259857
epoch£º46	 i:8 	 global-step:928	 l-p:0.12773989140987396
epoch£º46	 i:9 	 global-step:929	 l-p:0.15421903133392334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8377, 4.8935, 4.8546],
        [4.8377, 4.9442, 4.8860],
        [4.8377, 4.8395, 4.8378],
        [4.8377, 5.1796, 5.1481]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.11969245970249176 
model_pd.l_d.mean(): -18.279401779174805 
model_pd.lagr.mean(): -18.159709930419922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5298], device='cuda:0')), ('power', tensor([-19.1700], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.11969245970249176
epoch£º47	 i:1 	 global-step:941	 l-p:0.14187614619731903
epoch£º47	 i:2 	 global-step:942	 l-p:0.16347679495811462
epoch£º47	 i:3 	 global-step:943	 l-p:0.13055606186389923
epoch£º47	 i:4 	 global-step:944	 l-p:0.13770505785942078
epoch£º47	 i:5 	 global-step:945	 l-p:0.11224420368671417
epoch£º47	 i:6 	 global-step:946	 l-p:0.13481609523296356
epoch£º47	 i:7 	 global-step:947	 l-p:0.13515259325504303
epoch£º47	 i:8 	 global-step:948	 l-p:0.1337815374135971
epoch£º47	 i:9 	 global-step:949	 l-p:0.1703687459230423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7858, 5.1238, 5.0934],
        [4.7858, 4.9662, 4.9001],
        [4.7858, 4.7858, 4.7858],
        [4.7858, 5.0157, 4.9540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.17284923791885376 
model_pd.l_d.mean(): -20.68732261657715 
model_pd.lagr.mean(): -20.51447296142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4347], device='cuda:0')), ('power', tensor([-21.5244], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.17284923791885376
epoch£º48	 i:1 	 global-step:961	 l-p:0.12777136266231537
epoch£º48	 i:2 	 global-step:962	 l-p:0.13522027432918549
epoch£º48	 i:3 	 global-step:963	 l-p:0.11988916248083115
epoch£º48	 i:4 	 global-step:964	 l-p:0.1376475989818573
epoch£º48	 i:5 	 global-step:965	 l-p:0.20585693418979645
epoch£º48	 i:6 	 global-step:966	 l-p:0.12332204729318619
epoch£º48	 i:7 	 global-step:967	 l-p:0.12908506393432617
epoch£º48	 i:8 	 global-step:968	 l-p:0.11771706491708755
epoch£º48	 i:9 	 global-step:969	 l-p:0.11680161952972412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9362, 4.9707, 4.9439],
        [4.9362, 4.9376, 4.9363],
        [4.9362, 5.0952, 5.0274],
        [4.9362, 4.9424, 4.9367]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.12528257071971893 
model_pd.l_d.mean(): -20.676597595214844 
model_pd.lagr.mean(): -20.551315307617188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3882], device='cuda:0')), ('power', tensor([-21.4653], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.12528257071971893
epoch£º49	 i:1 	 global-step:981	 l-p:0.13822820782661438
epoch£º49	 i:2 	 global-step:982	 l-p:0.15642087161540985
epoch£º49	 i:3 	 global-step:983	 l-p:0.11825971305370331
epoch£º49	 i:4 	 global-step:984	 l-p:0.14523407816886902
epoch£º49	 i:5 	 global-step:985	 l-p:0.1426229476928711
epoch£º49	 i:6 	 global-step:986	 l-p:0.13741986453533173
epoch£º49	 i:7 	 global-step:987	 l-p:0.18439194560050964
epoch£º49	 i:8 	 global-step:988	 l-p:0.11888476461172104
epoch£º49	 i:9 	 global-step:989	 l-p:0.13849444687366486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8264, 5.6596, 5.9924],
        [4.8264, 4.8265, 4.8264],
        [4.8264, 5.1448, 5.1057],
        [4.8264, 4.8282, 4.8265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.1373373419046402 
model_pd.l_d.mean(): -19.932411193847656 
model_pd.lagr.mean(): -19.795074462890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4906], device='cuda:0')), ('power', tensor([-20.8133], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.1373373419046402
epoch£º50	 i:1 	 global-step:1001	 l-p:0.13769298791885376
epoch£º50	 i:2 	 global-step:1002	 l-p:0.142582505941391
epoch£º50	 i:3 	 global-step:1003	 l-p:0.1397446095943451
epoch£º50	 i:4 	 global-step:1004	 l-p:0.12561777234077454
epoch£º50	 i:5 	 global-step:1005	 l-p:0.12176545709371567
epoch£º50	 i:6 	 global-step:1006	 l-p:0.13377420604228973
epoch£º50	 i:7 	 global-step:1007	 l-p:0.14555427432060242
epoch£º50	 i:8 	 global-step:1008	 l-p:0.12408789992332458
epoch£º50	 i:9 	 global-step:1009	 l-p:0.13597320020198822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9639, 4.9639, 4.9639],
        [4.9639, 5.5696, 5.6896],
        [4.9639, 6.2923, 7.1554],
        [4.9639, 4.9640, 4.9639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.11435069143772125 
model_pd.l_d.mean(): -18.53336524963379 
model_pd.lagr.mean(): -18.41901397705078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5101], device='cuda:0')), ('power', tensor([-19.4082], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.11435069143772125
epoch£º51	 i:1 	 global-step:1021	 l-p:0.10959924757480621
epoch£º51	 i:2 	 global-step:1022	 l-p:0.11614032834768295
epoch£º51	 i:3 	 global-step:1023	 l-p:0.12675291299819946
epoch£º51	 i:4 	 global-step:1024	 l-p:0.12326550483703613
epoch£º51	 i:5 	 global-step:1025	 l-p:0.12459132075309753
epoch£º51	 i:6 	 global-step:1026	 l-p:0.13788661360740662
epoch£º51	 i:7 	 global-step:1027	 l-p:0.18677236139774323
epoch£º51	 i:8 	 global-step:1028	 l-p:0.15303950011730194
epoch£º51	 i:9 	 global-step:1029	 l-p:0.15377657115459442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7097, 5.2888, 5.4120],
        [4.7097, 5.1261, 5.1396],
        [4.7097, 4.7582, 4.7236],
        [4.7097, 4.7097, 4.7097]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.1430409997701645 
model_pd.l_d.mean(): -20.38134765625 
model_pd.lagr.mean(): -20.238306045532227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4676], device='cuda:0')), ('power', tensor([-21.2468], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.1430409997701645
epoch£º52	 i:1 	 global-step:1041	 l-p:0.15623614192008972
epoch£º52	 i:2 	 global-step:1042	 l-p:0.13896937668323517
epoch£º52	 i:3 	 global-step:1043	 l-p:0.14757315814495087
epoch£º52	 i:4 	 global-step:1044	 l-p:0.03257664665579796
epoch£º52	 i:5 	 global-step:1045	 l-p:0.14540013670921326
epoch£º52	 i:6 	 global-step:1046	 l-p:0.16257567703723907
epoch£º52	 i:7 	 global-step:1047	 l-p:-0.93214350938797
epoch£º52	 i:8 	 global-step:1048	 l-p:0.17384618520736694
epoch£º52	 i:9 	 global-step:1049	 l-p:0.1067822277545929
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6565, 4.6645, 4.6573],
        [4.6565, 4.6566, 4.6565],
        [4.6565, 4.6571, 4.6565],
        [4.6565, 4.6570, 4.6565]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.15208736062049866 
model_pd.l_d.mean(): -18.91732406616211 
model_pd.lagr.mean(): -18.765235900878906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5585], device='cuda:0')), ('power', tensor([-19.8496], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.15208736062049866
epoch£º53	 i:1 	 global-step:1061	 l-p:0.13184751570224762
epoch£º53	 i:2 	 global-step:1062	 l-p:0.1321941316127777
epoch£º53	 i:3 	 global-step:1063	 l-p:0.6495746970176697
epoch£º53	 i:4 	 global-step:1064	 l-p:0.12160001695156097
epoch£º53	 i:5 	 global-step:1065	 l-p:0.11829868704080582
epoch£º53	 i:6 	 global-step:1066	 l-p:0.12338767945766449
epoch£º53	 i:7 	 global-step:1067	 l-p:0.12010180205106735
epoch£º53	 i:8 	 global-step:1068	 l-p:0.13307274878025055
epoch£º53	 i:9 	 global-step:1069	 l-p:0.1354483813047409
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0276, 5.0276, 5.0276],
        [5.0276, 5.7461, 5.9528],
        [5.0276, 5.7983, 6.0506],
        [5.0276, 5.1352, 5.0756]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.12763440608978271 
model_pd.l_d.mean(): -20.427297592163086 
model_pd.lagr.mean(): -20.299663543701172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3862], device='cuda:0')), ('power', tensor([-21.2093], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.12763440608978271
epoch£º54	 i:1 	 global-step:1081	 l-p:0.12217056006193161
epoch£º54	 i:2 	 global-step:1082	 l-p:0.1259171962738037
epoch£º54	 i:3 	 global-step:1083	 l-p:0.11591538786888123
epoch£º54	 i:4 	 global-step:1084	 l-p:0.12452248483896255
epoch£º54	 i:5 	 global-step:1085	 l-p:0.13933897018432617
epoch£º54	 i:6 	 global-step:1086	 l-p:0.007500710431486368
epoch£º54	 i:7 	 global-step:1087	 l-p:0.1552942842245102
epoch£º54	 i:8 	 global-step:1088	 l-p:0.1319757103919983
epoch£º54	 i:9 	 global-step:1089	 l-p:0.1410582959651947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5530,  0.4539,  1.0000,  0.3726,
          1.0000,  0.8208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7399,  0.6692,  1.0000,  0.6053,
          1.0000,  0.9045, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5585,  0.4600,  1.0000,  0.3788,
          1.0000,  0.8235, 31.6228]], device='cuda:0')
 pt:tensor([[4.8630, 5.5984, 5.8383],
        [4.8630, 5.8996, 6.4419],
        [4.8630, 5.2967, 5.3115],
        [4.8630, 5.6075, 5.8555]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.15535786747932434 
model_pd.l_d.mean(): -19.877473831176758 
model_pd.lagr.mean(): -19.722116470336914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4834], device='cuda:0')), ('power', tensor([-20.7498], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.15535786747932434
epoch£º55	 i:1 	 global-step:1101	 l-p:-1.0356324911117554
epoch£º55	 i:2 	 global-step:1102	 l-p:0.14927536249160767
epoch£º55	 i:3 	 global-step:1103	 l-p:0.15566280484199524
epoch£º55	 i:4 	 global-step:1104	 l-p:0.10966117680072784
epoch£º55	 i:5 	 global-step:1105	 l-p:0.11469388753175735
epoch£º55	 i:6 	 global-step:1106	 l-p:0.11336985975503922
epoch£º55	 i:7 	 global-step:1107	 l-p:0.12109442800283432
epoch£º55	 i:8 	 global-step:1108	 l-p:0.11315376311540604
epoch£º55	 i:9 	 global-step:1109	 l-p:0.14950522780418396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9288, 6.2384, 7.0891],
        [4.9288, 4.9289, 4.9288],
        [4.9288, 5.1675, 5.1050],
        [4.9288, 5.0056, 4.9572]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.1787838488817215 
model_pd.l_d.mean(): -19.84010887145996 
model_pd.lagr.mean(): -19.661325454711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4742], device='cuda:0')), ('power', tensor([-20.7023], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.1787838488817215
epoch£º56	 i:1 	 global-step:1121	 l-p:0.13520097732543945
epoch£º56	 i:2 	 global-step:1122	 l-p:0.12750093638896942
epoch£º56	 i:3 	 global-step:1123	 l-p:0.12373380362987518
epoch£º56	 i:4 	 global-step:1124	 l-p:0.12157826870679855
epoch£º56	 i:5 	 global-step:1125	 l-p:0.12857340276241302
epoch£º56	 i:6 	 global-step:1126	 l-p:0.1336425542831421
epoch£º56	 i:7 	 global-step:1127	 l-p:0.14273397624492645
epoch£º56	 i:8 	 global-step:1128	 l-p:0.16250461339950562
epoch£º56	 i:9 	 global-step:1129	 l-p:0.14107368886470795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8344, 5.8146, 6.3017],
        [4.8344, 5.9296, 6.5439],
        [4.8344, 4.8344, 4.8344],
        [4.8344, 4.9005, 4.8571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.16008998453617096 
model_pd.l_d.mean(): -20.181180953979492 
model_pd.lagr.mean(): -20.02109146118164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4718], device='cuda:0')), ('power', tensor([-21.0472], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.16008998453617096
epoch£º57	 i:1 	 global-step:1141	 l-p:0.13855642080307007
epoch£º57	 i:2 	 global-step:1142	 l-p:0.16261686384677887
epoch£º57	 i:3 	 global-step:1143	 l-p:0.06003190949559212
epoch£º57	 i:4 	 global-step:1144	 l-p:0.12505139410495758
epoch£º57	 i:5 	 global-step:1145	 l-p:0.14716506004333496
epoch£º57	 i:6 	 global-step:1146	 l-p:0.14769525825977325
epoch£º57	 i:7 	 global-step:1147	 l-p:0.11327199637889862
epoch£º57	 i:8 	 global-step:1148	 l-p:0.12699121236801147
epoch£º57	 i:9 	 global-step:1149	 l-p:0.1140241026878357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9302, 5.2420, 5.1987],
        [4.9302, 5.1485, 5.0835],
        [4.9302, 6.1832, 6.9661],
        [4.9302, 4.9302, 4.9302]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.13071653246879578 
model_pd.l_d.mean(): -20.451658248901367 
model_pd.lagr.mean(): -20.320941925048828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4154], device='cuda:0')), ('power', tensor([-21.2643], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.13071653246879578
epoch£º58	 i:1 	 global-step:1161	 l-p:0.13971036672592163
epoch£º58	 i:2 	 global-step:1162	 l-p:0.12397567182779312
epoch£º58	 i:3 	 global-step:1163	 l-p:0.12997892498970032
epoch£º58	 i:4 	 global-step:1164	 l-p:0.14994046092033386
epoch£º58	 i:5 	 global-step:1165	 l-p:0.13225191831588745
epoch£º58	 i:6 	 global-step:1166	 l-p:0.1340317279100418
epoch£º58	 i:7 	 global-step:1167	 l-p:0.14073821902275085
epoch£º58	 i:8 	 global-step:1168	 l-p:0.15375523269176483
epoch£º58	 i:9 	 global-step:1169	 l-p:0.04063798859715462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6297, 4.9493, 4.9222],
        [4.6297, 4.6317, 4.6298],
        [4.6297, 4.6297, 4.6297],
        [4.6297, 4.7830, 4.7216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.13011544942855835 
model_pd.l_d.mean(): -18.68512725830078 
model_pd.lagr.mean(): -18.555011749267578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5757], device='cuda:0')), ('power', tensor([-19.6308], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.13011544942855835
epoch£º59	 i:1 	 global-step:1181	 l-p:0.14477653801441193
epoch£º59	 i:2 	 global-step:1182	 l-p:0.16381607949733734
epoch£º59	 i:3 	 global-step:1183	 l-p:0.22349630296230316
epoch£º59	 i:4 	 global-step:1184	 l-p:0.14975328743457794
epoch£º59	 i:5 	 global-step:1185	 l-p:0.17610235512256622
epoch£º59	 i:6 	 global-step:1186	 l-p:0.13960258662700653
epoch£º59	 i:7 	 global-step:1187	 l-p:0.12437897175550461
epoch£º59	 i:8 	 global-step:1188	 l-p:0.13224823772907257
epoch£º59	 i:9 	 global-step:1189	 l-p:0.13542141020298004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9808, 5.0885, 5.0298],
        [4.9808, 4.9808, 4.9808],
        [4.9808, 4.9828, 4.9808],
        [4.9808, 5.3254, 5.2926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.1586160808801651 
model_pd.l_d.mean(): -18.05537223815918 
model_pd.lagr.mean(): -17.89675521850586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5649], device='cuda:0')), ('power', tensor([-18.9781], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.1586160808801651
epoch£º60	 i:1 	 global-step:1201	 l-p:0.1340503692626953
epoch£º60	 i:2 	 global-step:1202	 l-p:0.11630325019359589
epoch£º60	 i:3 	 global-step:1203	 l-p:0.11693160980939865
epoch£º60	 i:4 	 global-step:1204	 l-p:0.13140569627285004
epoch£º60	 i:5 	 global-step:1205	 l-p:0.12279334664344788
epoch£º60	 i:6 	 global-step:1206	 l-p:0.12751145660877228
epoch£º60	 i:7 	 global-step:1207	 l-p:0.1147933378815651
epoch£º60	 i:8 	 global-step:1208	 l-p:0.14725838601589203
epoch£º60	 i:9 	 global-step:1209	 l-p:0.11725645512342453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8354, 4.8354, 4.8354],
        [4.8354, 5.4566, 5.6058],
        [4.8354, 4.8419, 4.8360],
        [4.8354, 5.5440, 5.7667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.15280722081661224 
model_pd.l_d.mean(): -19.077335357666016 
model_pd.lagr.mean(): -18.924528121948242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5089], device='cuda:0')), ('power', tensor([-19.9611], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.15280722081661224
epoch£º61	 i:1 	 global-step:1221	 l-p:0.14592862129211426
epoch£º61	 i:2 	 global-step:1222	 l-p:0.12451345473527908
epoch£º61	 i:3 	 global-step:1223	 l-p:0.17252002656459808
epoch£º61	 i:4 	 global-step:1224	 l-p:0.13715368509292603
epoch£º61	 i:5 	 global-step:1225	 l-p:0.11607103049755096
epoch£º61	 i:6 	 global-step:1226	 l-p:0.13367533683776855
epoch£º61	 i:7 	 global-step:1227	 l-p:0.1272994726896286
epoch£º61	 i:8 	 global-step:1228	 l-p:0.135645791888237
epoch£º61	 i:9 	 global-step:1229	 l-p:0.1461818814277649
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8446, 4.8762, 4.8515],
        [4.8446, 5.7739, 6.2091],
        [4.8446, 4.9756, 4.9136],
        [4.8446, 5.0579, 4.9952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.15713156759738922 
model_pd.l_d.mean(): -19.79358673095703 
model_pd.lagr.mean(): -19.636455535888672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5026], device='cuda:0')), ('power', tensor([-20.6842], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.15713156759738922
epoch£º62	 i:1 	 global-step:1241	 l-p:0.13051356375217438
epoch£º62	 i:2 	 global-step:1242	 l-p:0.10838466137647629
epoch£º62	 i:3 	 global-step:1243	 l-p:0.1230689212679863
epoch£º62	 i:4 	 global-step:1244	 l-p:0.20358844101428986
epoch£º62	 i:5 	 global-step:1245	 l-p:0.12748923897743225
epoch£º62	 i:6 	 global-step:1246	 l-p:0.1358506679534912
epoch£º62	 i:7 	 global-step:1247	 l-p:0.14141535758972168
epoch£º62	 i:8 	 global-step:1248	 l-p:0.1400785744190216
epoch£º62	 i:9 	 global-step:1249	 l-p:0.13906972110271454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9073, 4.9099, 4.9074],
        [4.9073, 6.2424, 7.1342],
        [4.9073, 6.2471, 7.1448],
        [4.9073, 4.9073, 4.9073]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.13424299657344818 
model_pd.l_d.mean(): -19.88812828063965 
model_pd.lagr.mean(): -19.75388526916504 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4665], device='cuda:0')), ('power', tensor([-20.7432], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.13424299657344818
epoch£º63	 i:1 	 global-step:1261	 l-p:0.15174919366836548
epoch£º63	 i:2 	 global-step:1262	 l-p:0.07273723185062408
epoch£º63	 i:3 	 global-step:1263	 l-p:0.15375779569149017
epoch£º63	 i:4 	 global-step:1264	 l-p:0.11572718620300293
epoch£º63	 i:5 	 global-step:1265	 l-p:0.1383800506591797
epoch£º63	 i:6 	 global-step:1266	 l-p:0.1421515941619873
epoch£º63	 i:7 	 global-step:1267	 l-p:0.17358461022377014
epoch£º63	 i:8 	 global-step:1268	 l-p:0.11361268907785416
epoch£º63	 i:9 	 global-step:1269	 l-p:2.115670919418335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6897, 4.6913, 4.6897],
        [4.6897, 4.6897, 4.6897],
        [4.6897, 4.7159, 4.6950],
        [4.6897, 4.6897, 4.6897]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.129761204123497 
model_pd.l_d.mean(): -19.628812789916992 
model_pd.lagr.mean(): -19.499052047729492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5419], device='cuda:0')), ('power', tensor([-20.5571], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.129761204123497
epoch£º64	 i:1 	 global-step:1281	 l-p:0.1455845683813095
epoch£º64	 i:2 	 global-step:1282	 l-p:0.1897893100976944
epoch£º64	 i:3 	 global-step:1283	 l-p:0.20318195223808289
epoch£º64	 i:4 	 global-step:1284	 l-p:0.09818867594003677
epoch£º64	 i:5 	 global-step:1285	 l-p:0.16379089653491974
epoch£º64	 i:6 	 global-step:1286	 l-p:0.13702701032161713
epoch£º64	 i:7 	 global-step:1287	 l-p:0.10069157183170319
epoch£º64	 i:8 	 global-step:1288	 l-p:0.14261527359485626
epoch£º64	 i:9 	 global-step:1289	 l-p:0.11549004167318344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9464, 4.9469, 4.9464],
        [4.9464, 4.9464, 4.9464],
        [4.9464, 6.0186, 6.5938],
        [4.9464, 5.0665, 5.0056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.12497755140066147 
model_pd.l_d.mean(): -19.849607467651367 
model_pd.lagr.mean(): -19.72463035583496 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4640], device='cuda:0')), ('power', tensor([-20.7013], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.12497755140066147
epoch£º65	 i:1 	 global-step:1301	 l-p:0.13952234387397766
epoch£º65	 i:2 	 global-step:1302	 l-p:0.13534896075725555
epoch£º65	 i:3 	 global-step:1303	 l-p:0.13732405006885529
epoch£º65	 i:4 	 global-step:1304	 l-p:0.12193255871534348
epoch£º65	 i:5 	 global-step:1305	 l-p:0.17988356947898865
epoch£º65	 i:6 	 global-step:1306	 l-p:0.1364271491765976
epoch£º65	 i:7 	 global-step:1307	 l-p:0.14618629217147827
epoch£º65	 i:8 	 global-step:1308	 l-p:0.14925359189510345
epoch£º65	 i:9 	 global-step:1309	 l-p:0.31518808007240295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7078, 4.7078, 4.7078],
        [4.7078, 5.6954, 6.2163],
        [4.7078, 5.2357, 5.3273],
        [4.7078, 4.7315, 4.7123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.14089685678482056 
model_pd.l_d.mean(): -20.30460548400879 
model_pd.lagr.mean(): -20.163707733154297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4912], device='cuda:0')), ('power', tensor([-21.1930], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.14089685678482056
epoch£º66	 i:1 	 global-step:1321	 l-p:0.1392405927181244
epoch£º66	 i:2 	 global-step:1322	 l-p:0.13163729012012482
epoch£º66	 i:3 	 global-step:1323	 l-p:0.12313001602888107
epoch£º66	 i:4 	 global-step:1324	 l-p:0.16344277560710907
epoch£º66	 i:5 	 global-step:1325	 l-p:0.1326449066400528
epoch£º66	 i:6 	 global-step:1326	 l-p:0.13550744950771332
epoch£º66	 i:7 	 global-step:1327	 l-p:0.17552444338798523
epoch£º66	 i:8 	 global-step:1328	 l-p:0.1292826384305954
epoch£º66	 i:9 	 global-step:1329	 l-p:0.15075787901878357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9135, 4.9135, 4.9135],
        [4.9135, 4.9135, 4.9135],
        [4.9135, 5.8940, 6.3761],
        [4.9135, 5.6866, 5.9620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.16386637091636658 
model_pd.l_d.mean(): -19.842605590820312 
model_pd.lagr.mean(): -19.678739547729492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4755], device='cuda:0')), ('power', tensor([-20.7062], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.16386637091636658
epoch£º67	 i:1 	 global-step:1341	 l-p:0.11248449236154556
epoch£º67	 i:2 	 global-step:1342	 l-p:0.1285218596458435
epoch£º67	 i:3 	 global-step:1343	 l-p:0.12928134202957153
epoch£º67	 i:4 	 global-step:1344	 l-p:0.13667702674865723
epoch£º67	 i:5 	 global-step:1345	 l-p:0.2082529067993164
epoch£º67	 i:6 	 global-step:1346	 l-p:0.13390876352787018
epoch£º67	 i:7 	 global-step:1347	 l-p:0.12883619964122772
epoch£º67	 i:8 	 global-step:1348	 l-p:0.14497031271457672
epoch£º67	 i:9 	 global-step:1349	 l-p:0.11003785580396652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8873, 6.1528, 6.9659],
        [4.8873, 6.2097, 7.0931],
        [4.8873, 4.8971, 4.8884],
        [4.8873, 5.9363, 6.4973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.1209840402007103 
model_pd.l_d.mean(): -18.9372615814209 
model_pd.lagr.mean(): -18.8162784576416 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5415], device='cuda:0')), ('power', tensor([-19.8522], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.1209840402007103
epoch£º68	 i:1 	 global-step:1361	 l-p:0.36786699295043945
epoch£º68	 i:2 	 global-step:1362	 l-p:0.15059208869934082
epoch£º68	 i:3 	 global-step:1363	 l-p:0.11869903653860092
epoch£º68	 i:4 	 global-step:1364	 l-p:0.1274632066488266
epoch£º68	 i:5 	 global-step:1365	 l-p:0.12340762466192245
epoch£º68	 i:6 	 global-step:1366	 l-p:0.13037332892417908
epoch£º68	 i:7 	 global-step:1367	 l-p:0.15457329154014587
epoch£º68	 i:8 	 global-step:1368	 l-p:0.11792800575494766
epoch£º68	 i:9 	 global-step:1369	 l-p:0.15866343677043915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8449, 5.0097, 4.9457],
        [4.8449, 5.9188, 6.5154],
        [4.8449, 5.0673, 5.0072],
        [4.8449, 4.8449, 4.8449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.15082918107509613 
model_pd.l_d.mean(): -18.46632957458496 
model_pd.lagr.mean(): -18.315500259399414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5681], device='cuda:0')), ('power', tensor([-19.4000], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.15082918107509613
epoch£º69	 i:1 	 global-step:1381	 l-p:0.1301330029964447
epoch£º69	 i:2 	 global-step:1382	 l-p:-1.2334823608398438
epoch£º69	 i:3 	 global-step:1383	 l-p:0.15293973684310913
epoch£º69	 i:4 	 global-step:1384	 l-p:0.11988089233636856
epoch£º69	 i:5 	 global-step:1385	 l-p:0.1241200789809227
epoch£º69	 i:6 	 global-step:1386	 l-p:0.11886384338140488
epoch£º69	 i:7 	 global-step:1387	 l-p:0.1322626918554306
epoch£º69	 i:8 	 global-step:1388	 l-p:0.12689632177352905
epoch£º69	 i:9 	 global-step:1389	 l-p:0.12079111486673355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9366, 6.3239, 7.2802],
        [4.9366, 4.9643, 4.9422],
        [4.9366, 4.9366, 4.9366],
        [4.9366, 6.2341, 7.0784]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.10369464755058289 
model_pd.l_d.mean(): -17.842063903808594 
model_pd.lagr.mean(): -17.73836898803711 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5746], device='cuda:0')), ('power', tensor([-18.7708], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.10369464755058289
epoch£º70	 i:1 	 global-step:1401	 l-p:0.1261817365884781
epoch£º70	 i:2 	 global-step:1402	 l-p:0.12830811738967896
epoch£º70	 i:3 	 global-step:1403	 l-p:0.14015284180641174
epoch£º70	 i:4 	 global-step:1404	 l-p:0.15187521278858185
epoch£º70	 i:5 	 global-step:1405	 l-p:0.18333874642848969
epoch£º70	 i:6 	 global-step:1406	 l-p:0.16357211768627167
epoch£º70	 i:7 	 global-step:1407	 l-p:0.0994306355714798
epoch£º70	 i:8 	 global-step:1408	 l-p:0.12622712552547455
epoch£º70	 i:9 	 global-step:1409	 l-p:0.14934304356575012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8291, 5.0207, 4.9579],
        [4.8291, 4.8322, 4.8293],
        [4.8291, 4.8291, 4.8291],
        [4.8291, 5.4334, 5.5749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.13691985607147217 
model_pd.l_d.mean(): -20.163827896118164 
model_pd.lagr.mean(): -20.02690887451172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4759], device='cuda:0')), ('power', tensor([-21.0338], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.13691985607147217
epoch£º71	 i:1 	 global-step:1421	 l-p:0.12872716784477234
epoch£º71	 i:2 	 global-step:1422	 l-p:0.0898936465382576
epoch£º71	 i:3 	 global-step:1423	 l-p:0.12103605270385742
epoch£º71	 i:4 	 global-step:1424	 l-p:0.15930886566638947
epoch£º71	 i:5 	 global-step:1425	 l-p:0.13836175203323364
epoch£º71	 i:6 	 global-step:1426	 l-p:0.1402266025543213
epoch£º71	 i:7 	 global-step:1427	 l-p:0.15915735065937042
epoch£º71	 i:8 	 global-step:1428	 l-p:0.14289617538452148
epoch£º71	 i:9 	 global-step:1429	 l-p:0.1542574167251587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8530, 4.8530, 4.8530],
        [4.8530, 4.8531, 4.8530],
        [4.8530, 4.8575, 4.8534],
        [4.8530, 4.8531, 4.8530]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.12221154570579529 
model_pd.l_d.mean(): -19.043914794921875 
model_pd.lagr.mean(): -18.921703338623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4902], device='cuda:0')), ('power', tensor([-19.9077], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.12221154570579529
epoch£º72	 i:1 	 global-step:1441	 l-p:0.14671678841114044
epoch£º72	 i:2 	 global-step:1442	 l-p:0.1358988881111145
epoch£º72	 i:3 	 global-step:1443	 l-p:0.14229992032051086
epoch£º72	 i:4 	 global-step:1444	 l-p:0.14747342467308044
epoch£º72	 i:5 	 global-step:1445	 l-p:0.11708597093820572
epoch£º72	 i:6 	 global-step:1446	 l-p:0.15363918244838715
epoch£º72	 i:7 	 global-step:1447	 l-p:0.13288752734661102
epoch£º72	 i:8 	 global-step:1448	 l-p:0.1558111011981964
epoch£º72	 i:9 	 global-step:1449	 l-p:-0.08720813691616058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8933, 4.8933, 4.8933],
        [4.8933, 5.0509, 4.9869],
        [4.8933, 5.8630, 6.3406],
        [4.8933, 5.1147, 5.0539]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.1314225047826767 
model_pd.l_d.mean(): -19.15789222717285 
model_pd.lagr.mean(): -19.026470184326172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4689], device='cuda:0')), ('power', tensor([-20.0018], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.1314225047826767
epoch£º73	 i:1 	 global-step:1461	 l-p:0.14315608143806458
epoch£º73	 i:2 	 global-step:1462	 l-p:0.20595349371433258
epoch£º73	 i:3 	 global-step:1463	 l-p:0.11429788172245026
epoch£º73	 i:4 	 global-step:1464	 l-p:0.12791049480438232
epoch£º73	 i:5 	 global-step:1465	 l-p:0.12018946558237076
epoch£º73	 i:6 	 global-step:1466	 l-p:0.13284575939178467
epoch£º73	 i:7 	 global-step:1467	 l-p:0.14369936287403107
epoch£º73	 i:8 	 global-step:1468	 l-p:0.10545701533555984
epoch£º73	 i:9 	 global-step:1469	 l-p:0.12764282524585724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9139, 4.9319, 4.9167],
        [4.9139, 5.2843, 5.2704],
        [4.9139, 5.6455, 5.8873],
        [4.9139, 5.1350, 5.0738]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.13360625505447388 
model_pd.l_d.mean(): -19.59657096862793 
model_pd.lagr.mean(): -19.46296501159668 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4987], device='cuda:0')), ('power', tensor([-20.4795], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.13360625505447388
epoch£º74	 i:1 	 global-step:1481	 l-p:0.11731621623039246
epoch£º74	 i:2 	 global-step:1482	 l-p:0.14399291574954987
epoch£º74	 i:3 	 global-step:1483	 l-p:0.13353215157985687
epoch£º74	 i:4 	 global-step:1484	 l-p:0.12479081749916077
epoch£º74	 i:5 	 global-step:1485	 l-p:0.17726390063762665
epoch£º74	 i:6 	 global-step:1486	 l-p:0.18185950815677643
epoch£º74	 i:7 	 global-step:1487	 l-p:0.10620992630720139
epoch£º74	 i:8 	 global-step:1488	 l-p:0.16122661530971527
epoch£º74	 i:9 	 global-step:1489	 l-p:0.13233964145183563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8988, 5.2322, 5.2032],
        [4.8988, 5.1090, 5.0471],
        [4.8988, 4.8988, 4.8988],
        [4.8988, 5.6822, 5.9739]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.13600605726242065 
model_pd.l_d.mean(): -19.93600845336914 
model_pd.lagr.mean(): -19.800003051757812 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4664], device='cuda:0')), ('power', tensor([-20.7919], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.13600605726242065
epoch£º75	 i:1 	 global-step:1501	 l-p:0.12752200663089752
epoch£º75	 i:2 	 global-step:1502	 l-p:0.1397332400083542
epoch£º75	 i:3 	 global-step:1503	 l-p:0.14123590290546417
epoch£º75	 i:4 	 global-step:1504	 l-p:0.13728517293930054
epoch£º75	 i:5 	 global-step:1505	 l-p:0.11924751102924347
epoch£º75	 i:6 	 global-step:1506	 l-p:0.1624266654253006
epoch£º75	 i:7 	 global-step:1507	 l-p:0.13571153581142426
epoch£º75	 i:8 	 global-step:1508	 l-p:0.09755260497331619
epoch£º75	 i:9 	 global-step:1509	 l-p:0.14521478116512299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8382, 4.8387, 4.8382],
        [4.8382, 4.8382, 4.8382],
        [4.8382, 5.5727, 5.8270],
        [4.8382, 5.0656, 5.0078]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.12407083064317703 
model_pd.l_d.mean(): -19.261337280273438 
model_pd.lagr.mean(): -19.137266159057617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5277], device='cuda:0')), ('power', tensor([-20.1680], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.12407083064317703
epoch£º76	 i:1 	 global-step:1521	 l-p:0.13845497369766235
epoch£º76	 i:2 	 global-step:1522	 l-p:0.26796314120292664
epoch£º76	 i:3 	 global-step:1523	 l-p:0.12710760533809662
epoch£º76	 i:4 	 global-step:1524	 l-p:0.1196078211069107
epoch£º76	 i:5 	 global-step:1525	 l-p:0.11825967580080032
epoch£º76	 i:6 	 global-step:1526	 l-p:0.11106591671705246
epoch£º76	 i:7 	 global-step:1527	 l-p:0.11725747585296631
epoch£º76	 i:8 	 global-step:1528	 l-p:0.12485742568969727
epoch£º76	 i:9 	 global-step:1529	 l-p:0.16207973659038544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9392, 4.9563, 4.9418],
        [4.9392, 4.9396, 4.9392],
        [4.9392, 4.9811, 4.9502],
        [4.9392, 5.0402, 4.9849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.12608705461025238 
model_pd.l_d.mean(): -20.28995704650879 
model_pd.lagr.mean(): -20.163869857788086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4306], device='cuda:0')), ('power', tensor([-21.1153], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.12608705461025238
epoch£º77	 i:1 	 global-step:1541	 l-p:0.11670615524053574
epoch£º77	 i:2 	 global-step:1542	 l-p:0.1503300815820694
epoch£º77	 i:3 	 global-step:1543	 l-p:0.10294175148010254
epoch£º77	 i:4 	 global-step:1544	 l-p:0.136651873588562
epoch£º77	 i:5 	 global-step:1545	 l-p:0.18643154203891754
epoch£º77	 i:6 	 global-step:1546	 l-p:0.12610670924186707
epoch£º77	 i:7 	 global-step:1547	 l-p:0.1726389080286026
epoch£º77	 i:8 	 global-step:1548	 l-p:0.1901843547821045
epoch£º77	 i:9 	 global-step:1549	 l-p:0.13949179649353027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8195, 5.4109, 5.5467],
        [4.8195, 5.6876, 6.0738],
        [4.8195, 4.9196, 4.8656],
        [4.8195, 4.8702, 4.8349]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.14249175786972046 
model_pd.l_d.mean(): -19.762420654296875 
model_pd.lagr.mean(): -19.61992835998535 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5105], device='cuda:0')), ('power', tensor([-20.6607], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.14249175786972046
epoch£º78	 i:1 	 global-step:1561	 l-p:0.13443462550640106
epoch£º78	 i:2 	 global-step:1562	 l-p:0.16616849601268768
epoch£º78	 i:3 	 global-step:1563	 l-p:0.13384804129600525
epoch£º78	 i:4 	 global-step:1564	 l-p:0.16234908998012543
epoch£º78	 i:5 	 global-step:1565	 l-p:0.12497907131910324
epoch£º78	 i:6 	 global-step:1566	 l-p:0.0867711678147316
epoch£º78	 i:7 	 global-step:1567	 l-p:0.13335508108139038
epoch£º78	 i:8 	 global-step:1568	 l-p:0.14581002295017242
epoch£º78	 i:9 	 global-step:1569	 l-p:0.14126138389110565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8202, 5.4614, 5.6377],
        [4.8202, 4.8203, 4.8202],
        [4.8202, 4.8218, 4.8203],
        [4.8202, 4.8245, 4.8205]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.14820577204227448 
model_pd.l_d.mean(): -20.290102005004883 
model_pd.lagr.mean(): -20.141895294189453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4700], device='cuda:0')), ('power', tensor([-21.1563], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.14820577204227448
epoch£º79	 i:1 	 global-step:1581	 l-p:0.17376689612865448
epoch£º79	 i:2 	 global-step:1582	 l-p:0.13713113963603973
epoch£º79	 i:3 	 global-step:1583	 l-p:0.13736775517463684
epoch£º79	 i:4 	 global-step:1584	 l-p:0.12785975635051727
epoch£º79	 i:5 	 global-step:1585	 l-p:0.19562137126922607
epoch£º79	 i:6 	 global-step:1586	 l-p:0.23356837034225464
epoch£º79	 i:7 	 global-step:1587	 l-p:0.14394326508045197
epoch£º79	 i:8 	 global-step:1588	 l-p:0.15501265227794647
epoch£º79	 i:9 	 global-step:1589	 l-p:0.10060714185237885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8186, 6.1566, 7.0821],
        [4.8186, 4.8347, 4.8210],
        [4.8186, 4.8186, 4.8186],
        [4.8186, 5.7545, 6.2120]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.16898775100708008 
model_pd.l_d.mean(): -18.92485809326172 
model_pd.lagr.mean(): -18.755870819091797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5187], device='cuda:0')), ('power', tensor([-19.8160], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.16898775100708008
epoch£º80	 i:1 	 global-step:1601	 l-p:0.12668274343013763
epoch£º80	 i:2 	 global-step:1602	 l-p:0.13870404660701752
epoch£º80	 i:3 	 global-step:1603	 l-p:0.12022989243268967
epoch£º80	 i:4 	 global-step:1604	 l-p:0.21688400208950043
epoch£º80	 i:5 	 global-step:1605	 l-p:0.1303628385066986
epoch£º80	 i:6 	 global-step:1606	 l-p:0.12778179347515106
epoch£º80	 i:7 	 global-step:1607	 l-p:0.13124686479568481
epoch£º80	 i:8 	 global-step:1608	 l-p:0.12617312371730804
epoch£º80	 i:9 	 global-step:1609	 l-p:0.13020414113998413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9240, 4.9299, 4.9245],
        [4.9240, 4.9240, 4.9240],
        [4.9240, 5.8905, 6.3652],
        [4.9240, 4.9943, 4.9497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.12173101305961609 
model_pd.l_d.mean(): -20.057641983032227 
model_pd.lagr.mean(): -19.935911178588867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4577], device='cuda:0')), ('power', tensor([-20.9068], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.12173101305961609
epoch£º81	 i:1 	 global-step:1621	 l-p:0.14275726675987244
epoch£º81	 i:2 	 global-step:1622	 l-p:0.1306273192167282
epoch£º81	 i:3 	 global-step:1623	 l-p:0.15365344285964966
epoch£º81	 i:4 	 global-step:1624	 l-p:0.16366222500801086
epoch£º81	 i:5 	 global-step:1625	 l-p:0.3576163351535797
epoch£º81	 i:6 	 global-step:1626	 l-p:0.13239188492298126
epoch£º81	 i:7 	 global-step:1627	 l-p:-0.12055496126413345
epoch£º81	 i:8 	 global-step:1628	 l-p:0.8029142022132874
epoch£º81	 i:9 	 global-step:1629	 l-p:0.1356772780418396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7377, 4.8035, 4.7617],
        [4.7377, 5.3809, 5.5688],
        [4.7377, 4.9379, 4.8798],
        [4.7377, 5.7665, 6.3387]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.21993142366409302 
model_pd.l_d.mean(): -20.132558822631836 
model_pd.lagr.mean(): -19.912628173828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5059], device='cuda:0')), ('power', tensor([-21.0330], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.21993142366409302
epoch£º82	 i:1 	 global-step:1641	 l-p:0.12980695068836212
epoch£º82	 i:2 	 global-step:1642	 l-p:0.15229947865009308
epoch£º82	 i:3 	 global-step:1643	 l-p:0.16252893209457397
epoch£º82	 i:4 	 global-step:1644	 l-p:0.1354241967201233
epoch£º82	 i:5 	 global-step:1645	 l-p:0.13671456277370453
epoch£º82	 i:6 	 global-step:1646	 l-p:0.14887744188308716
epoch£º82	 i:7 	 global-step:1647	 l-p:0.13629558682441711
epoch£º82	 i:8 	 global-step:1648	 l-p:0.12808452546596527
epoch£º82	 i:9 	 global-step:1649	 l-p:0.13668112456798553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9572, 5.8774, 6.3001],
        [4.9572, 5.1926, 5.1342],
        [4.9572, 4.9578, 4.9572],
        [4.9572, 5.4633, 5.5274]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.11839060485363007 
model_pd.l_d.mean(): -19.09731674194336 
model_pd.lagr.mean(): -18.978925704956055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4957], device='cuda:0')), ('power', tensor([-19.9678], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.11839060485363007
epoch£º83	 i:1 	 global-step:1661	 l-p:0.10585285723209381
epoch£º83	 i:2 	 global-step:1662	 l-p:0.13651229441165924
epoch£º83	 i:3 	 global-step:1663	 l-p:0.13026389479637146
epoch£º83	 i:4 	 global-step:1664	 l-p:0.12666834890842438
epoch£º83	 i:5 	 global-step:1665	 l-p:0.12581804394721985
epoch£º83	 i:6 	 global-step:1666	 l-p:0.13185568153858185
epoch£º83	 i:7 	 global-step:1667	 l-p:0.15344229340553284
epoch£º83	 i:8 	 global-step:1668	 l-p:-0.5629900097846985
epoch£º83	 i:9 	 global-step:1669	 l-p:0.1373457908630371
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8858, 4.8860, 4.8858],
        [4.8858, 4.8942, 4.8867],
        [4.8858, 4.9589, 4.9135],
        [4.8858, 5.1153, 5.0580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.13545916974544525 
model_pd.l_d.mean(): -20.12385368347168 
model_pd.lagr.mean(): -19.988393783569336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4722], device='cuda:0')), ('power', tensor([-20.9892], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.13545916974544525
epoch£º84	 i:1 	 global-step:1681	 l-p:0.1225399300456047
epoch£º84	 i:2 	 global-step:1682	 l-p:0.13937796652317047
epoch£º84	 i:3 	 global-step:1683	 l-p:0.14818337559700012
epoch£º84	 i:4 	 global-step:1684	 l-p:0.12514440715312958
epoch£º84	 i:5 	 global-step:1685	 l-p:0.14622311294078827
epoch£º84	 i:6 	 global-step:1686	 l-p:0.18305689096450806
epoch£º84	 i:7 	 global-step:1687	 l-p:0.06393515318632126
epoch£º84	 i:8 	 global-step:1688	 l-p:0.13630664348602295
epoch£º84	 i:9 	 global-step:1689	 l-p:0.13465848565101624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6507,  0.5638,  1.0000,  0.4886,
          1.0000,  0.8665, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[4.8298, 5.5561, 5.8084],
        [4.8298, 5.2920, 5.3378],
        [4.8298, 5.6605, 6.0117],
        [4.8298, 5.1531, 5.1259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.16135993599891663 
model_pd.l_d.mean(): -19.855619430541992 
model_pd.lagr.mean(): -19.694259643554688 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5066], device='cuda:0')), ('power', tensor([-20.7516], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.16135993599891663
epoch£º85	 i:1 	 global-step:1701	 l-p:0.13553205132484436
epoch£º85	 i:2 	 global-step:1702	 l-p:0.09099498391151428
epoch£º85	 i:3 	 global-step:1703	 l-p:0.15790341794490814
epoch£º85	 i:4 	 global-step:1704	 l-p:0.1527712047100067
epoch£º85	 i:5 	 global-step:1705	 l-p:0.1667390614748001
epoch£º85	 i:6 	 global-step:1706	 l-p:0.11853965371847153
epoch£º85	 i:7 	 global-step:1707	 l-p:0.11984745413064957
epoch£º85	 i:8 	 global-step:1708	 l-p:0.13867101073265076
epoch£º85	 i:9 	 global-step:1709	 l-p:0.12432832270860672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9116, 5.7653, 6.1289],
        [4.9116, 4.9212, 4.9127],
        [4.9116, 4.9117, 4.9116],
        [4.9116, 5.2227, 5.1875]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.43656906485557556 
model_pd.l_d.mean(): -17.214256286621094 
model_pd.lagr.mean(): -16.777687072753906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6587], device='cuda:0')), ('power', tensor([-18.2184], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.43656906485557556
epoch£º86	 i:1 	 global-step:1721	 l-p:0.11653287708759308
epoch£º86	 i:2 	 global-step:1722	 l-p:0.130994975566864
epoch£º86	 i:3 	 global-step:1723	 l-p:0.1401173323392868
epoch£º86	 i:4 	 global-step:1724	 l-p:0.12523874640464783
epoch£º86	 i:5 	 global-step:1725	 l-p:0.12507331371307373
epoch£º86	 i:6 	 global-step:1726	 l-p:0.1303197294473648
epoch£º86	 i:7 	 global-step:1727	 l-p:0.13566894829273224
epoch£º86	 i:8 	 global-step:1728	 l-p:0.14014723896980286
epoch£º86	 i:9 	 global-step:1729	 l-p:0.189630389213562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6958, 4.7833, 4.7343],
        [4.6958, 4.6998, 4.6961],
        [4.6958, 4.6962, 4.6958],
        [4.6958, 5.1780, 5.2478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.1505107283592224 
model_pd.l_d.mean(): -19.512592315673828 
model_pd.lagr.mean(): -19.36208152770996 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5771], device='cuda:0')), ('power', tensor([-20.4752], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.1505107283592224
epoch£º87	 i:1 	 global-step:1741	 l-p:0.13564473390579224
epoch£º87	 i:2 	 global-step:1742	 l-p:0.13549721240997314
epoch£º87	 i:3 	 global-step:1743	 l-p:0.16933505237102509
epoch£º87	 i:4 	 global-step:1744	 l-p:0.03602367267012596
epoch£º87	 i:5 	 global-step:1745	 l-p:-0.06204862520098686
epoch£º87	 i:6 	 global-step:1746	 l-p:0.13015082478523254
epoch£º87	 i:7 	 global-step:1747	 l-p:0.20438647270202637
epoch£º87	 i:8 	 global-step:1748	 l-p:0.1473495066165924
epoch£º87	 i:9 	 global-step:1749	 l-p:0.14569474756717682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8984, 4.8984, 4.8984],
        [4.8984, 5.1920, 5.1516],
        [4.8984, 4.9993, 4.9451],
        [4.8984, 4.9036, 4.8988]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.15165109932422638 
model_pd.l_d.mean(): -20.48173713684082 
model_pd.lagr.mean(): -20.33008575439453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4267], device='cuda:0')), ('power', tensor([-21.3067], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.15165109932422638
epoch£º88	 i:1 	 global-step:1761	 l-p:0.126090869307518
epoch£º88	 i:2 	 global-step:1762	 l-p:0.12855689227581024
epoch£º88	 i:3 	 global-step:1763	 l-p:0.13853149116039276
epoch£º88	 i:4 	 global-step:1764	 l-p:0.13096068799495697
epoch£º88	 i:5 	 global-step:1765	 l-p:0.14677377045154572
epoch£º88	 i:6 	 global-step:1766	 l-p:0.14818689227104187
epoch£º88	 i:7 	 global-step:1767	 l-p:0.14774079620838165
epoch£º88	 i:8 	 global-step:1768	 l-p:0.11605419963598251
epoch£º88	 i:9 	 global-step:1769	 l-p:0.11776711791753769
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7831, 5.7478, 6.2463],
        [4.7831, 4.8071, 4.7879],
        [4.7831, 4.8155, 4.7908],
        [4.7831, 4.9324, 4.8724]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.11368479579687119 
model_pd.l_d.mean(): -19.204280853271484 
model_pd.lagr.mean(): -19.090595245361328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5817], device='cuda:0')), ('power', tensor([-20.1658], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.11368479579687119
epoch£º89	 i:1 	 global-step:1781	 l-p:0.15314838290214539
epoch£º89	 i:2 	 global-step:1782	 l-p:0.1324307918548584
epoch£º89	 i:3 	 global-step:1783	 l-p:0.14957739412784576
epoch£º89	 i:4 	 global-step:1784	 l-p:0.21208888292312622
epoch£º89	 i:5 	 global-step:1785	 l-p:0.14058688282966614
epoch£º89	 i:6 	 global-step:1786	 l-p:0.14483366906642914
epoch£º89	 i:7 	 global-step:1787	 l-p:-0.5807159543037415
epoch£º89	 i:8 	 global-step:1788	 l-p:0.11752905696630478
epoch£º89	 i:9 	 global-step:1789	 l-p:0.12719370424747467
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9849, 6.0989, 6.7304],
        [4.9849, 4.9849, 4.9848],
        [4.9849, 4.9849, 4.9848],
        [4.9849, 5.0228, 4.9944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.16279923915863037 
model_pd.l_d.mean(): -19.22429847717285 
model_pd.lagr.mean(): -19.061498641967773 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4657], device='cuda:0')), ('power', tensor([-20.0661], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.16279923915863037
epoch£º90	 i:1 	 global-step:1801	 l-p:0.11141849309206009
epoch£º90	 i:2 	 global-step:1802	 l-p:0.12177577614784241
epoch£º90	 i:3 	 global-step:1803	 l-p:0.1186537891626358
epoch£º90	 i:4 	 global-step:1804	 l-p:0.11989372223615646
epoch£º90	 i:5 	 global-step:1805	 l-p:0.1357615441083908
epoch£º90	 i:6 	 global-step:1806	 l-p:0.12247100472450256
epoch£º90	 i:7 	 global-step:1807	 l-p:0.15550005435943604
epoch£º90	 i:8 	 global-step:1808	 l-p:0.1457693874835968
epoch£º90	 i:9 	 global-step:1809	 l-p:0.13796338438987732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7803, 4.7803, 4.7803],
        [4.7803, 4.7807, 4.7803],
        [4.7803, 4.7862, 4.7808],
        [4.7803, 5.5333, 5.8193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.13191816210746765 
model_pd.l_d.mean(): -19.168643951416016 
model_pd.lagr.mean(): -19.036725997924805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5006], device='cuda:0')), ('power', tensor([-20.0456], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.13191816210746765
epoch£º91	 i:1 	 global-step:1821	 l-p:0.18529599905014038
epoch£º91	 i:2 	 global-step:1822	 l-p:0.2659907042980194
epoch£º91	 i:3 	 global-step:1823	 l-p:0.13596861064434052
epoch£º91	 i:4 	 global-step:1824	 l-p:0.141145721077919
epoch£º91	 i:5 	 global-step:1825	 l-p:0.1656055599451065
epoch£º91	 i:6 	 global-step:1826	 l-p:0.10238032042980194
epoch£º91	 i:7 	 global-step:1827	 l-p:0.1297387033700943
epoch£º91	 i:8 	 global-step:1828	 l-p:0.13120855391025543
epoch£º91	 i:9 	 global-step:1829	 l-p:0.1539440155029297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8941, 4.8941, 4.8941],
        [4.8941, 5.9321, 6.4946],
        [4.8941, 5.0397, 4.9786],
        [4.8941, 4.9530, 4.9139]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.13306215405464172 
model_pd.l_d.mean(): -19.45956802368164 
model_pd.lagr.mean(): -19.326505661010742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5361], device='cuda:0')), ('power', tensor([-20.3787], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.13306215405464172
epoch£º92	 i:1 	 global-step:1841	 l-p:0.13222327828407288
epoch£º92	 i:2 	 global-step:1842	 l-p:0.12015756219625473
epoch£º92	 i:3 	 global-step:1843	 l-p:-0.6382240653038025
epoch£º92	 i:4 	 global-step:1844	 l-p:0.13794195652008057
epoch£º92	 i:5 	 global-step:1845	 l-p:0.14407791197299957
epoch£º92	 i:6 	 global-step:1846	 l-p:0.154526486992836
epoch£º92	 i:7 	 global-step:1847	 l-p:0.14241930842399597
epoch£º92	 i:8 	 global-step:1848	 l-p:0.1054496094584465
epoch£º92	 i:9 	 global-step:1849	 l-p:0.1278807371854782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9151, 4.9151, 4.9151],
        [4.9151, 5.2812, 5.2708],
        [4.9151, 5.3151, 5.3218],
        [4.9151, 5.1894, 5.1436]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.12549303472042084 
model_pd.l_d.mean(): -19.516698837280273 
model_pd.lagr.mean(): -19.391206741333008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4816], device='cuda:0')), ('power', tensor([-20.3804], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.12549303472042084
epoch£º93	 i:1 	 global-step:1861	 l-p:0.12939797341823578
epoch£º93	 i:2 	 global-step:1862	 l-p:0.12386499345302582
epoch£º93	 i:3 	 global-step:1863	 l-p:0.14073337614536285
epoch£º93	 i:4 	 global-step:1864	 l-p:0.15236608684062958
epoch£º93	 i:5 	 global-step:1865	 l-p:0.182074174284935
epoch£º93	 i:6 	 global-step:1866	 l-p:0.14184056222438812
epoch£º93	 i:7 	 global-step:1867	 l-p:0.14880527555942535
epoch£º93	 i:8 	 global-step:1868	 l-p:-0.140828937292099
epoch£º93	 i:9 	 global-step:1869	 l-p:0.12688636779785156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9764, 5.5207, 5.6144],
        [4.9764, 4.9764, 4.9764],
        [4.9764, 5.0208, 4.9888],
        [4.9764, 4.9909, 4.9785]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.1499616652727127 
model_pd.l_d.mean(): -20.534929275512695 
model_pd.lagr.mean(): -20.384967803955078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3970], device='cuda:0')), ('power', tensor([-21.3301], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.1499616652727127
epoch£º94	 i:1 	 global-step:1881	 l-p:0.11379992961883545
epoch£º94	 i:2 	 global-step:1882	 l-p:0.1773577481508255
epoch£º94	 i:3 	 global-step:1883	 l-p:0.12487872689962387
epoch£º94	 i:4 	 global-step:1884	 l-p:0.12431785464286804
epoch£º94	 i:5 	 global-step:1885	 l-p:0.11674704402685165
epoch£º94	 i:6 	 global-step:1886	 l-p:0.11379043012857437
epoch£º94	 i:7 	 global-step:1887	 l-p:0.12135353684425354
epoch£º94	 i:8 	 global-step:1888	 l-p:0.13112349808216095
epoch£º94	 i:9 	 global-step:1889	 l-p:0.1585284322500229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8891, 4.8892, 4.8891],
        [4.8891, 4.8891, 4.8891],
        [4.8891, 4.8905, 4.8892],
        [4.8891, 4.9045, 4.8914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.12313012778759003 
model_pd.l_d.mean(): -19.1104679107666 
model_pd.lagr.mean(): -18.987337112426758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5234], device='cuda:0')), ('power', tensor([-20.0099], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.12313012778759003
epoch£º95	 i:1 	 global-step:1901	 l-p:0.1487773060798645
epoch£º95	 i:2 	 global-step:1902	 l-p:0.13629047572612762
epoch£º95	 i:3 	 global-step:1903	 l-p:0.12910816073417664
epoch£º95	 i:4 	 global-step:1904	 l-p:0.1271217167377472
epoch£º95	 i:5 	 global-step:1905	 l-p:0.2103470116853714
epoch£º95	 i:6 	 global-step:1906	 l-p:0.13453668355941772
epoch£º95	 i:7 	 global-step:1907	 l-p:0.13363222777843475
epoch£º95	 i:8 	 global-step:1908	 l-p:0.13422752916812897
epoch£º95	 i:9 	 global-step:1909	 l-p:0.28690671920776367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8113, 5.9460, 6.6353],
        [4.8113, 4.8119, 4.8113],
        [4.8113, 5.5592, 5.8395],
        [4.8113, 4.8113, 4.8113]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.10279190540313721 
model_pd.l_d.mean(): -18.21607208251953 
model_pd.lagr.mean(): -18.113279342651367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6397], device='cuda:0')), ('power', tensor([-19.2193], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.10279190540313721
epoch£º96	 i:1 	 global-step:1921	 l-p:0.1472555249929428
epoch£º96	 i:2 	 global-step:1922	 l-p:0.12060422450304031
epoch£º96	 i:3 	 global-step:1923	 l-p:0.13362108170986176
epoch£º96	 i:4 	 global-step:1924	 l-p:0.1382628083229065
epoch£º96	 i:5 	 global-step:1925	 l-p:0.13662320375442505
epoch£º96	 i:6 	 global-step:1926	 l-p:0.1819237321615219
epoch£º96	 i:7 	 global-step:1927	 l-p:0.1205591931939125
epoch£º96	 i:8 	 global-step:1928	 l-p:0.14104799926280975
epoch£º96	 i:9 	 global-step:1929	 l-p:0.12763097882270813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0220, 5.5009, 5.5484],
        [5.0220, 5.8777, 6.2356],
        [5.0220, 5.0220, 5.0220],
        [5.0220, 6.3316, 7.1909]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.15318012237548828 
model_pd.l_d.mean(): -20.143735885620117 
model_pd.lagr.mean(): -19.990554809570312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4198], device='cuda:0')), ('power', tensor([-20.9552], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.15318012237548828
epoch£º97	 i:1 	 global-step:1941	 l-p:0.12259446829557419
epoch£º97	 i:2 	 global-step:1942	 l-p:0.10759726911783218
epoch£º97	 i:3 	 global-step:1943	 l-p:0.1467529833316803
epoch£º97	 i:4 	 global-step:1944	 l-p:0.1794508844614029
epoch£º97	 i:5 	 global-step:1945	 l-p:0.13726310431957245
epoch£º97	 i:6 	 global-step:1946	 l-p:0.11438826471567154
epoch£º97	 i:7 	 global-step:1947	 l-p:0.1308915913105011
epoch£º97	 i:8 	 global-step:1948	 l-p:0.14155788719654083
epoch£º97	 i:9 	 global-step:1949	 l-p:0.1258934587240219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7536, 5.1185, 5.1182],
        [4.7536, 4.7536, 4.7536],
        [4.7536, 4.7931, 4.7644],
        [4.7536, 5.3507, 5.5067]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.13104747235774994 
model_pd.l_d.mean(): -20.154743194580078 
model_pd.lagr.mean(): -20.02369499206543 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5045], device='cuda:0')), ('power', tensor([-21.0542], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.13104747235774994
epoch£º98	 i:1 	 global-step:1961	 l-p:0.1467527151107788
epoch£º98	 i:2 	 global-step:1962	 l-p:0.12730050086975098
epoch£º98	 i:3 	 global-step:1963	 l-p:-0.20573967695236206
epoch£º98	 i:4 	 global-step:1964	 l-p:0.16805097460746765
epoch£º98	 i:5 	 global-step:1965	 l-p:0.17553141713142395
epoch£º98	 i:6 	 global-step:1966	 l-p:0.1355501115322113
epoch£º98	 i:7 	 global-step:1967	 l-p:0.14521512389183044
epoch£º98	 i:8 	 global-step:1968	 l-p:0.1231636106967926
epoch£º98	 i:9 	 global-step:1969	 l-p:0.1371961534023285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9218, 4.9218, 4.9218],
        [4.9218, 4.9219, 4.9218],
        [4.9218, 5.3827, 5.4269],
        [4.9218, 4.9218, 4.9218]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.1454961746931076 
model_pd.l_d.mean(): -19.24591064453125 
model_pd.lagr.mean(): -19.100414276123047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4575], device='cuda:0')), ('power', tensor([-20.0796], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.1454961746931076
epoch£º99	 i:1 	 global-step:1981	 l-p:0.14154782891273499
epoch£º99	 i:2 	 global-step:1982	 l-p:0.1125311329960823
epoch£º99	 i:3 	 global-step:1983	 l-p:0.14440171420574188
epoch£º99	 i:4 	 global-step:1984	 l-p:0.12545621395111084
epoch£º99	 i:5 	 global-step:1985	 l-p:-0.020261559635400772
epoch£º99	 i:6 	 global-step:1986	 l-p:0.12308813631534576
epoch£º99	 i:7 	 global-step:1987	 l-p:0.16015911102294922
epoch£º99	 i:8 	 global-step:1988	 l-p:0.12655586004257202
epoch£º99	 i:9 	 global-step:1989	 l-p:0.139334574341774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8812, 4.8818, 4.8812],
        [4.8812, 5.3794, 5.4513],
        [4.8812, 4.9688, 4.9193],
        [4.8812, 4.8812, 4.8812]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.1611335277557373 
model_pd.l_d.mean(): -20.57787322998047 
model_pd.lagr.mean(): -20.41674041748047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4220], device='cuda:0')), ('power', tensor([-21.3997], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.1611335277557373
epoch£º100	 i:1 	 global-step:2001	 l-p:0.1330791562795639
epoch£º100	 i:2 	 global-step:2002	 l-p:0.10963107645511627
epoch£º100	 i:3 	 global-step:2003	 l-p:0.11720351129770279
epoch£º100	 i:4 	 global-step:2004	 l-p:0.13889098167419434
epoch£º100	 i:5 	 global-step:2005	 l-p:0.143341526389122
epoch£º100	 i:6 	 global-step:2006	 l-p:0.1375276893377304
epoch£º100	 i:7 	 global-step:2007	 l-p:0.47709736227989197
epoch£º100	 i:8 	 global-step:2008	 l-p:0.7777506709098816
epoch£º100	 i:9 	 global-step:2009	 l-p:0.166670560836792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8264, 4.8854, 4.8469],
        [4.8264, 4.8265, 4.8264],
        [4.8264, 4.8285, 4.8265],
        [4.8264, 4.8264, 4.8264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.13523080945014954 
model_pd.l_d.mean(): -19.943328857421875 
model_pd.lagr.mean(): -19.80809783935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4805], device='cuda:0')), ('power', tensor([-20.8139], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.13523080945014954
epoch£º101	 i:1 	 global-step:2021	 l-p:0.11621108651161194
epoch£º101	 i:2 	 global-step:2022	 l-p:0.10479407757520676
epoch£º101	 i:3 	 global-step:2023	 l-p:-0.06051577255129814
epoch£º101	 i:4 	 global-step:2024	 l-p:0.13449501991271973
epoch£º101	 i:5 	 global-step:2025	 l-p:0.13281668722629547
epoch£º101	 i:6 	 global-step:2026	 l-p:0.11729691177606583
epoch£º101	 i:7 	 global-step:2027	 l-p:0.12430443614721298
epoch£º101	 i:8 	 global-step:2028	 l-p:0.14241719245910645
epoch£º101	 i:9 	 global-step:2029	 l-p:0.1373659074306488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5760,  0.4793,  1.0000,  0.3988,
          1.0000,  0.8321, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5791,  0.4826,  1.0000,  0.4023,
          1.0000,  0.8335, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9034,  0.8733,  1.0000,  0.8442,
          1.0000,  0.9667, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[4.9909, 5.7091, 5.9456],
        [4.9909, 5.7140, 5.9547],
        [4.9909, 6.2236, 6.9995],
        [4.9909, 5.1622, 5.1000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.11455139517784119 
model_pd.l_d.mean(): -19.91588592529297 
model_pd.lagr.mean(): -19.801334381103516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4601], device='cuda:0')), ('power', tensor([-20.7648], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.11455139517784119
epoch£º102	 i:1 	 global-step:2041	 l-p:0.16053588688373566
epoch£º102	 i:2 	 global-step:2042	 l-p:-0.022148890420794487
epoch£º102	 i:3 	 global-step:2043	 l-p:0.14532455801963806
epoch£º102	 i:4 	 global-step:2044	 l-p:0.13984428346157074
epoch£º102	 i:5 	 global-step:2045	 l-p:0.13473854959011078
epoch£º102	 i:6 	 global-step:2046	 l-p:0.14285026490688324
epoch£º102	 i:7 	 global-step:2047	 l-p:0.12777705490589142
epoch£º102	 i:8 	 global-step:2048	 l-p:0.14536559581756592
epoch£º102	 i:9 	 global-step:2049	 l-p:0.2454947978258133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6799, 4.7704, 4.7218],
        [4.6799, 5.0051, 4.9911],
        [4.6799, 4.7642, 4.7173],
        [4.6799, 4.6898, 4.6812]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.12515445053577423 
model_pd.l_d.mean(): -19.7410888671875 
model_pd.lagr.mean(): -19.615934371948242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5633], device='cuda:0')), ('power', tensor([-20.6936], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.12515445053577423
epoch£º103	 i:1 	 global-step:2061	 l-p:0.08715227246284485
epoch£º103	 i:2 	 global-step:2062	 l-p:0.15404637157917023
epoch£º103	 i:3 	 global-step:2063	 l-p:0.163038969039917
epoch£º103	 i:4 	 global-step:2064	 l-p:0.132613405585289
epoch£º103	 i:5 	 global-step:2065	 l-p:0.13881385326385498
epoch£º103	 i:6 	 global-step:2066	 l-p:0.13010886311531067
epoch£º103	 i:7 	 global-step:2067	 l-p:0.1515791118144989
epoch£º103	 i:8 	 global-step:2068	 l-p:0.12999442219734192
epoch£º103	 i:9 	 global-step:2069	 l-p:0.12749212980270386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9762, 4.9780, 4.9763],
        [4.9762, 5.1235, 5.0625],
        [4.9762, 5.0344, 4.9958],
        [4.9762, 5.9084, 6.3535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.12842687964439392 
model_pd.l_d.mean(): -20.549699783325195 
model_pd.lagr.mean(): -20.42127227783203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3946], device='cuda:0')), ('power', tensor([-21.3427], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.12842687964439392
epoch£º104	 i:1 	 global-step:2081	 l-p:0.27631261944770813
epoch£º104	 i:2 	 global-step:2082	 l-p:0.15158015489578247
epoch£º104	 i:3 	 global-step:2083	 l-p:0.15599635243415833
epoch£º104	 i:4 	 global-step:2084	 l-p:0.12848973274230957
epoch£º104	 i:5 	 global-step:2085	 l-p:0.12579147517681122
epoch£º104	 i:6 	 global-step:2086	 l-p:0.12470553070306778
epoch£º104	 i:7 	 global-step:2087	 l-p:0.143047496676445
epoch£º104	 i:8 	 global-step:2088	 l-p:0.12391063570976257
epoch£º104	 i:9 	 global-step:2089	 l-p:0.13200080394744873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8467, 4.8467, 4.8467],
        [4.8467, 4.8467, 4.8467],
        [4.8467, 5.0496, 4.9932],
        [4.8467, 5.3681, 5.4611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.1587708592414856 
model_pd.l_d.mean(): -20.185226440429688 
model_pd.lagr.mean(): -20.02645492553711 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4673], device='cuda:0')), ('power', tensor([-21.0467], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.1587708592414856
epoch£º105	 i:1 	 global-step:2101	 l-p:0.16085006296634674
epoch£º105	 i:2 	 global-step:2102	 l-p:0.12373700737953186
epoch£º105	 i:3 	 global-step:2103	 l-p:0.14029577374458313
epoch£º105	 i:4 	 global-step:2104	 l-p:0.14363114535808563
epoch£º105	 i:5 	 global-step:2105	 l-p:0.18810546398162842
epoch£º105	 i:6 	 global-step:2106	 l-p:0.16599562764167786
epoch£º105	 i:7 	 global-step:2107	 l-p:0.17462576925754547
epoch£º105	 i:8 	 global-step:2108	 l-p:0.07794997841119766
epoch£º105	 i:9 	 global-step:2109	 l-p:0.12919855117797852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9053, 4.9081, 4.9054],
        [4.9053, 5.6099, 5.8457],
        [4.9053, 4.9236, 4.9084],
        [4.9053, 5.8172, 6.2526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.13373249769210815 
model_pd.l_d.mean(): -18.371156692504883 
model_pd.lagr.mean(): -18.237424850463867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5652], device='cuda:0')), ('power', tensor([-19.3000], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.13373249769210815
epoch£º106	 i:1 	 global-step:2121	 l-p:0.14318829774856567
epoch£º106	 i:2 	 global-step:2122	 l-p:0.13310763239860535
epoch£º106	 i:3 	 global-step:2123	 l-p:0.13347280025482178
epoch£º106	 i:4 	 global-step:2124	 l-p:0.12373135983943939
epoch£º106	 i:5 	 global-step:2125	 l-p:0.129064679145813
epoch£º106	 i:6 	 global-step:2126	 l-p:-1.6993516683578491
epoch£º106	 i:7 	 global-step:2127	 l-p:0.13456477224826813
epoch£º106	 i:8 	 global-step:2128	 l-p:0.1324400156736374
epoch£º106	 i:9 	 global-step:2129	 l-p:0.11528050154447556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9065, 4.9091, 4.9067],
        [4.9065, 4.9116, 4.9069],
        [4.9065, 5.9336, 6.4918],
        [4.9065, 5.7484, 6.1133]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.14432714879512787 
model_pd.l_d.mean(): -19.220705032348633 
model_pd.lagr.mean(): -19.076377868652344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5040], device='cuda:0')), ('power', tensor([-20.1022], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.14432714879512787
epoch£º107	 i:1 	 global-step:2141	 l-p:0.12064360082149506
epoch£º107	 i:2 	 global-step:2142	 l-p:-0.11079796403646469
epoch£º107	 i:3 	 global-step:2143	 l-p:0.12227296829223633
epoch£º107	 i:4 	 global-step:2144	 l-p:0.12821200489997864
epoch£º107	 i:5 	 global-step:2145	 l-p:0.13112863898277283
epoch£º107	 i:6 	 global-step:2146	 l-p:0.1704164296388626
epoch£º107	 i:7 	 global-step:2147	 l-p:0.14917628467082977
epoch£º107	 i:8 	 global-step:2148	 l-p:0.12664735317230225
epoch£º107	 i:9 	 global-step:2149	 l-p:0.13808168470859528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8255, 4.8356, 4.8267],
        [4.8255, 4.8256, 4.8255],
        [4.8255, 4.9202, 4.8696],
        [4.8255, 4.8255, 4.8255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.13060440123081207 
model_pd.l_d.mean(): -18.703575134277344 
model_pd.lagr.mean(): -18.57297134399414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5405], device='cuda:0')), ('power', tensor([-19.6132], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.13060440123081207
epoch£º108	 i:1 	 global-step:2161	 l-p:0.16381953656673431
epoch£º108	 i:2 	 global-step:2162	 l-p:0.10568245500326157
epoch£º108	 i:3 	 global-step:2163	 l-p:0.13614320755004883
epoch£º108	 i:4 	 global-step:2164	 l-p:0.1389225721359253
epoch£º108	 i:5 	 global-step:2165	 l-p:0.15474238991737366
epoch£º108	 i:6 	 global-step:2166	 l-p:0.12774038314819336
epoch£º108	 i:7 	 global-step:2167	 l-p:0.1266641914844513
epoch£º108	 i:8 	 global-step:2168	 l-p:0.13278645277023315
epoch£º108	 i:9 	 global-step:2169	 l-p:0.1613413542509079
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9348, 5.0183, 4.9702],
        [4.9348, 4.9363, 4.9349],
        [4.9348, 6.1931, 7.0144],
        [4.9348, 4.9348, 4.9348]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.11570655554533005 
model_pd.l_d.mean(): -19.854354858398438 
model_pd.lagr.mean(): -19.7386474609375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4730], device='cuda:0')), ('power', tensor([-20.7155], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.11570655554533005
epoch£º109	 i:1 	 global-step:2181	 l-p:0.13407264649868011
epoch£º109	 i:2 	 global-step:2182	 l-p:0.13086362183094025
epoch£º109	 i:3 	 global-step:2183	 l-p:0.14565223455429077
epoch£º109	 i:4 	 global-step:2184	 l-p:0.32376641035079956
epoch£º109	 i:5 	 global-step:2185	 l-p:0.13928447663784027
epoch£º109	 i:6 	 global-step:2186	 l-p:0.14242668449878693
epoch£º109	 i:7 	 global-step:2187	 l-p:0.12323939055204391
epoch£º109	 i:8 	 global-step:2188	 l-p:0.13373611867427826
epoch£º109	 i:9 	 global-step:2189	 l-p:0.11671863496303558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9473, 4.9478, 4.9473],
        [4.9473, 4.9473, 4.9473],
        [4.9473, 4.9483, 4.9473],
        [4.9473, 4.9678, 4.9511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.4301843047142029 
model_pd.l_d.mean(): -20.64302635192871 
model_pd.lagr.mean(): -20.21284294128418 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3933], device='cuda:0')), ('power', tensor([-21.4364], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.4301843047142029
epoch£º110	 i:1 	 global-step:2201	 l-p:0.13120509684085846
epoch£º110	 i:2 	 global-step:2202	 l-p:0.09824391454458237
epoch£º110	 i:3 	 global-step:2203	 l-p:0.13433896005153656
epoch£º110	 i:4 	 global-step:2204	 l-p:0.14354786276817322
epoch£º110	 i:5 	 global-step:2205	 l-p:0.13454003632068634
epoch£º110	 i:6 	 global-step:2206	 l-p:0.1387406885623932
epoch£º110	 i:7 	 global-step:2207	 l-p:0.13304837048053741
epoch£º110	 i:8 	 global-step:2208	 l-p:0.22281008958816528
epoch£º110	 i:9 	 global-step:2209	 l-p:0.21226529777050018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8684, 4.8727, 4.8687],
        [4.8684, 5.5470, 5.7669],
        [4.8684, 5.3822, 5.4709],
        [4.8684, 5.7335, 6.1294]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.1693091094493866 
model_pd.l_d.mean(): -19.927440643310547 
model_pd.lagr.mean(): -19.75813102722168 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4912], device='cuda:0')), ('power', tensor([-20.8088], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.1693091094493866
epoch£º111	 i:1 	 global-step:2221	 l-p:0.13469845056533813
epoch£º111	 i:2 	 global-step:2222	 l-p:0.37045130133628845
epoch£º111	 i:3 	 global-step:2223	 l-p:0.12724408507347107
epoch£º111	 i:4 	 global-step:2224	 l-p:0.11790977418422699
epoch£º111	 i:5 	 global-step:2225	 l-p:0.12331700325012207
epoch£º111	 i:6 	 global-step:2226	 l-p:0.11520052701234818
epoch£º111	 i:7 	 global-step:2227	 l-p:0.14073994755744934
epoch£º111	 i:8 	 global-step:2228	 l-p:0.13631176948547363
epoch£º111	 i:9 	 global-step:2229	 l-p:0.11079834401607513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9584, 5.0225, 4.9817],
        [4.9584, 5.0205, 4.9805],
        [4.9584, 4.9585, 4.9584],
        [4.9584, 4.9805, 4.9627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.14768677949905396 
model_pd.l_d.mean(): -20.386571884155273 
model_pd.lagr.mean(): -20.2388858795166 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4249], device='cuda:0')), ('power', tensor([-21.2079], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.14768677949905396
epoch£º112	 i:1 	 global-step:2241	 l-p:-0.08561042696237564
epoch£º112	 i:2 	 global-step:2242	 l-p:0.15538515150547028
epoch£º112	 i:3 	 global-step:2243	 l-p:0.13483312726020813
epoch£º112	 i:4 	 global-step:2244	 l-p:0.1344587653875351
epoch£º112	 i:5 	 global-step:2245	 l-p:0.15390773117542267
epoch£º112	 i:6 	 global-step:2246	 l-p:0.11893389374017715
epoch£º112	 i:7 	 global-step:2247	 l-p:0.16327811777591705
epoch£º112	 i:8 	 global-step:2248	 l-p:0.13059383630752563
epoch£º112	 i:9 	 global-step:2249	 l-p:0.1330137550830841
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8155, 4.8155, 4.8155],
        [4.8155, 5.4855, 5.7048],
        [4.8155, 4.8756, 4.8371],
        [4.8155, 4.8155, 4.8155]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.09461746364831924 
model_pd.l_d.mean(): -19.342649459838867 
model_pd.lagr.mean(): -19.248031616210938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5209], device='cuda:0')), ('power', tensor([-20.2439], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.09461746364831924
epoch£º113	 i:1 	 global-step:2261	 l-p:0.13559295237064362
epoch£º113	 i:2 	 global-step:2262	 l-p:0.12899146974086761
epoch£º113	 i:3 	 global-step:2263	 l-p:0.1423051804304123
epoch£º113	 i:4 	 global-step:2264	 l-p:0.13763508200645447
epoch£º113	 i:5 	 global-step:2265	 l-p:0.12665152549743652
epoch£º113	 i:6 	 global-step:2266	 l-p:0.303034245967865
epoch£º113	 i:7 	 global-step:2267	 l-p:0.13657121360301971
epoch£º113	 i:8 	 global-step:2268	 l-p:0.14618781208992004
epoch£º113	 i:9 	 global-step:2269	 l-p:0.5625269412994385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8173, 4.8173, 4.8173],
        [4.8173, 4.8188, 4.8174],
        [4.8173, 4.8181, 4.8173],
        [4.8173, 5.6866, 6.0957]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.12508508563041687 
model_pd.l_d.mean(): -19.43882179260254 
model_pd.lagr.mean(): -19.313735961914062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5487], device='cuda:0')), ('power', tensor([-20.3706], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.12508508563041687
epoch£º114	 i:1 	 global-step:2281	 l-p:0.1405564248561859
epoch£º114	 i:2 	 global-step:2282	 l-p:0.14101621508598328
epoch£º114	 i:3 	 global-step:2283	 l-p:0.15525655448436737
epoch£º114	 i:4 	 global-step:2284	 l-p:0.1303487867116928
epoch£º114	 i:5 	 global-step:2285	 l-p:0.13000814616680145
epoch£º114	 i:6 	 global-step:2286	 l-p:0.12732309103012085
epoch£º114	 i:7 	 global-step:2287	 l-p:0.13285541534423828
epoch£º114	 i:8 	 global-step:2288	 l-p:0.08132471144199371
epoch£º114	 i:9 	 global-step:2289	 l-p:0.13581673800945282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8624, 4.8624, 4.8624],
        [4.8624, 4.8624, 4.8624],
        [4.8624, 4.8624, 4.8624],
        [4.8624, 5.0558, 4.9996]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.1271410882472992 
model_pd.l_d.mean(): -20.103097915649414 
model_pd.lagr.mean(): -19.975955963134766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4756], device='cuda:0')), ('power', tensor([-20.9717], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.1271410882472992
epoch£º115	 i:1 	 global-step:2301	 l-p:0.11588626354932785
epoch£º115	 i:2 	 global-step:2302	 l-p:0.12097127735614777
epoch£º115	 i:3 	 global-step:2303	 l-p:0.5434354543685913
epoch£º115	 i:4 	 global-step:2304	 l-p:0.17073027789592743
epoch£º115	 i:5 	 global-step:2305	 l-p:0.13094092905521393
epoch£º115	 i:6 	 global-step:2306	 l-p:0.1363850235939026
epoch£º115	 i:7 	 global-step:2307	 l-p:0.13609975576400757
epoch£º115	 i:8 	 global-step:2308	 l-p:0.3736826479434967
epoch£º115	 i:9 	 global-step:2309	 l-p:0.19024482369422913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7837, 4.8588, 4.8148],
        [4.7837, 4.8181, 4.7927],
        [4.7837, 4.7837, 4.7837],
        [4.7837, 4.9754, 4.9211]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.11764808744192123 
model_pd.l_d.mean(): -19.593963623046875 
model_pd.lagr.mean(): -19.476316452026367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5360], device='cuda:0')), ('power', tensor([-20.5155], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.11764808744192123
epoch£º116	 i:1 	 global-step:2321	 l-p:0.14063838124275208
epoch£º116	 i:2 	 global-step:2322	 l-p:0.33596548438072205
epoch£º116	 i:3 	 global-step:2323	 l-p:0.15048083662986755
epoch£º116	 i:4 	 global-step:2324	 l-p:0.1023564413189888
epoch£º116	 i:5 	 global-step:2325	 l-p:0.17233432829380035
epoch£º116	 i:6 	 global-step:2326	 l-p:0.15219159424304962
epoch£º116	 i:7 	 global-step:2327	 l-p:0.13702242076396942
epoch£º116	 i:8 	 global-step:2328	 l-p:0.10875166952610016
epoch£º116	 i:9 	 global-step:2329	 l-p:0.13266469538211823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0349, 5.0359, 5.0350],
        [5.0349, 5.0968, 5.0568],
        [5.0349, 5.0349, 5.0349],
        [5.0349, 5.0349, 5.0349]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.12902949750423431 
model_pd.l_d.mean(): -20.59890365600586 
model_pd.lagr.mean(): -20.469873428344727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3750], device='cuda:0')), ('power', tensor([-21.3725], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.12902949750423431
epoch£º117	 i:1 	 global-step:2341	 l-p:0.13329635560512543
epoch£º117	 i:2 	 global-step:2342	 l-p:0.1338067501783371
epoch£º117	 i:3 	 global-step:2343	 l-p:0.1141880676150322
epoch£º117	 i:4 	 global-step:2344	 l-p:0.141922265291214
epoch£º117	 i:5 	 global-step:2345	 l-p:0.12403329461812973
epoch£º117	 i:6 	 global-step:2346	 l-p:0.2564142942428589
epoch£º117	 i:7 	 global-step:2347	 l-p:0.13965383172035217
epoch£º117	 i:8 	 global-step:2348	 l-p:-0.9660914540290833
epoch£º117	 i:9 	 global-step:2349	 l-p:0.153713658452034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7193, 5.8697, 6.6082],
        [4.7193, 5.0131, 4.9883],
        [4.7193, 4.7383, 4.7229],
        [4.7193, 4.7193, 4.7193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.12176653742790222 
model_pd.l_d.mean(): -19.58464241027832 
model_pd.lagr.mean(): -19.462875366210938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5592], device='cuda:0')), ('power', tensor([-20.5301], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.12176653742790222
epoch£º118	 i:1 	 global-step:2361	 l-p:0.14085011184215546
epoch£º118	 i:2 	 global-step:2362	 l-p:0.14085732400417328
epoch£º118	 i:3 	 global-step:2363	 l-p:0.19289115071296692
epoch£º118	 i:4 	 global-step:2364	 l-p:0.14375463128089905
epoch£º118	 i:5 	 global-step:2365	 l-p:0.15095458924770355
epoch£º118	 i:6 	 global-step:2366	 l-p:0.15045788884162903
epoch£º118	 i:7 	 global-step:2367	 l-p:0.6111000180244446
epoch£º118	 i:8 	 global-step:2368	 l-p:0.3166162073612213
epoch£º118	 i:9 	 global-step:2369	 l-p:0.1734507530927658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9001, 5.0931, 5.0365],
        [4.9001, 4.9079, 4.9010],
        [4.9001, 5.4349, 5.5395],
        [4.9001, 5.5705, 5.7839]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.16443920135498047 
model_pd.l_d.mean(): -20.55699920654297 
model_pd.lagr.mean(): -20.392559051513672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4229], device='cuda:0')), ('power', tensor([-21.3794], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.16443920135498047
epoch£º119	 i:1 	 global-step:2381	 l-p:0.1323997676372528
epoch£º119	 i:2 	 global-step:2382	 l-p:0.12263710051774979
epoch£º119	 i:3 	 global-step:2383	 l-p:0.11799008399248123
epoch£º119	 i:4 	 global-step:2384	 l-p:0.11435823142528534
epoch£º119	 i:5 	 global-step:2385	 l-p:0.11652535945177078
epoch£º119	 i:6 	 global-step:2386	 l-p:0.12513510882854462
epoch£º119	 i:7 	 global-step:2387	 l-p:0.1409754604101181
epoch£º119	 i:8 	 global-step:2388	 l-p:-0.09321311861276627
epoch£º119	 i:9 	 global-step:2389	 l-p:0.14657701551914215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9100, 4.9101, 4.9100],
        [4.9100, 4.9102, 4.9100],
        [4.9100, 4.9109, 4.9101],
        [4.9100, 5.3161, 5.3362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.1313050091266632 
model_pd.l_d.mean(): -20.14545249938965 
model_pd.lagr.mean(): -20.01414680480957 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4632], device='cuda:0')), ('power', tensor([-21.0019], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.1313050091266632
epoch£º120	 i:1 	 global-step:2401	 l-p:0.1262289434671402
epoch£º120	 i:2 	 global-step:2402	 l-p:0.12779484689235687
epoch£º120	 i:3 	 global-step:2403	 l-p:0.1521161049604416
epoch£º120	 i:4 	 global-step:2404	 l-p:0.14730100333690643
epoch£º120	 i:5 	 global-step:2405	 l-p:0.126935675740242
epoch£º120	 i:6 	 global-step:2406	 l-p:0.14552675187587738
epoch£º120	 i:7 	 global-step:2407	 l-p:0.08779452741146088
epoch£º120	 i:8 	 global-step:2408	 l-p:0.07648927718400955
epoch£º120	 i:9 	 global-step:2409	 l-p:0.030426377430558205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7523, 4.9324, 4.8783],
        [4.7523, 4.7780, 4.7581],
        [4.7523, 4.7523, 4.7523],
        [4.7523, 4.7528, 4.7523]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.14620289206504822 
model_pd.l_d.mean(): -19.357053756713867 
model_pd.lagr.mean(): -19.210851669311523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4942], device='cuda:0')), ('power', tensor([-20.2308], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.14620289206504822
epoch£º121	 i:1 	 global-step:2421	 l-p:0.10382141917943954
epoch£º121	 i:2 	 global-step:2422	 l-p:0.1325506716966629
epoch£º121	 i:3 	 global-step:2423	 l-p:0.1333770900964737
epoch£º121	 i:4 	 global-step:2424	 l-p:0.18889693915843964
epoch£º121	 i:5 	 global-step:2425	 l-p:0.12660065293312073
epoch£º121	 i:6 	 global-step:2426	 l-p:0.1365177482366562
epoch£º121	 i:7 	 global-step:2427	 l-p:0.15841910243034363
epoch£º121	 i:8 	 global-step:2428	 l-p:0.12420737743377686
epoch£º121	 i:9 	 global-step:2429	 l-p:0.1317305862903595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9523, 4.9788, 4.9581],
        [4.9523, 4.9619, 4.9535],
        [4.9523, 4.9524, 4.9523],
        [4.9523, 4.9591, 4.9530]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.13255339860916138 
model_pd.l_d.mean(): -18.468402862548828 
model_pd.lagr.mean(): -18.33584976196289 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5176], device='cuda:0')), ('power', tensor([-19.3499], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.13255339860916138
epoch£º122	 i:1 	 global-step:2441	 l-p:0.11412828415632248
epoch£º122	 i:2 	 global-step:2442	 l-p:0.1497354656457901
epoch£º122	 i:3 	 global-step:2443	 l-p:0.1358906775712967
epoch£º122	 i:4 	 global-step:2444	 l-p:0.13467782735824585
epoch£º122	 i:5 	 global-step:2445	 l-p:0.11205245554447174
epoch£º122	 i:6 	 global-step:2446	 l-p:0.14578209817409515
epoch£º122	 i:7 	 global-step:2447	 l-p:0.13209982216358185
epoch£º122	 i:8 	 global-step:2448	 l-p:0.14210693538188934
epoch£º122	 i:9 	 global-step:2449	 l-p:0.13630832731723785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7553, 4.7558, 4.7553],
        [4.7553, 4.8058, 4.7722],
        [4.7553, 4.7609, 4.7559],
        [4.7553, 4.7553, 4.7553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.1418212503194809 
model_pd.l_d.mean(): -20.368589401245117 
model_pd.lagr.mean(): -20.226768493652344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4872], device='cuda:0')), ('power', tensor([-21.2541], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.1418212503194809
epoch£º123	 i:1 	 global-step:2461	 l-p:0.13768336176872253
epoch£º123	 i:2 	 global-step:2462	 l-p:0.2163134664297104
epoch£º123	 i:3 	 global-step:2463	 l-p:0.14473184943199158
epoch£º123	 i:4 	 global-step:2464	 l-p:0.07654765248298645
epoch£º123	 i:5 	 global-step:2465	 l-p:0.14734816551208496
epoch£º123	 i:6 	 global-step:2466	 l-p:0.21830765902996063
epoch£º123	 i:7 	 global-step:2467	 l-p:0.14760775864124298
epoch£º123	 i:8 	 global-step:2468	 l-p:-0.5415633320808411
epoch£º123	 i:9 	 global-step:2469	 l-p:0.08989737182855606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6816, 4.6819, 4.6816],
        [4.6816, 5.3729, 5.6324],
        [4.6816, 4.7131, 4.6897],
        [4.6816, 4.7285, 4.6968]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.21345901489257812 
model_pd.l_d.mean(): -19.983291625976562 
model_pd.lagr.mean(): -19.769832611083984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5286], device='cuda:0')), ('power', tensor([-20.9044], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.21345901489257812
epoch£º124	 i:1 	 global-step:2481	 l-p:-2.2296693325042725
epoch£º124	 i:2 	 global-step:2482	 l-p:0.17310328781604767
epoch£º124	 i:3 	 global-step:2483	 l-p:0.13000181317329407
epoch£º124	 i:4 	 global-step:2484	 l-p:0.1379079669713974
epoch£º124	 i:5 	 global-step:2485	 l-p:0.10202695429325104
epoch£º124	 i:6 	 global-step:2486	 l-p:0.11595729738473892
epoch£º124	 i:7 	 global-step:2487	 l-p:0.10599296540021896
epoch£º124	 i:8 	 global-step:2488	 l-p:0.11509328335523605
epoch£º124	 i:9 	 global-step:2489	 l-p:0.11084854602813721
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3182, 5.3334, 5.3205],
        [5.3182, 6.7445, 7.7061],
        [5.3182, 5.3374, 5.3215],
        [5.3182, 5.7763, 5.8026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.10472127050161362 
model_pd.l_d.mean(): -18.867963790893555 
model_pd.lagr.mean(): -18.763242721557617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3911], device='cuda:0')), ('power', tensor([-19.6258], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.10472127050161362
epoch£º125	 i:1 	 global-step:2501	 l-p:0.11111393570899963
epoch£º125	 i:2 	 global-step:2502	 l-p:0.1113719642162323
epoch£º125	 i:3 	 global-step:2503	 l-p:0.1273907721042633
epoch£º125	 i:4 	 global-step:2504	 l-p:0.11360418796539307
epoch£º125	 i:5 	 global-step:2505	 l-p:0.12803946435451508
epoch£º125	 i:6 	 global-step:2506	 l-p:0.12337283790111542
epoch£º125	 i:7 	 global-step:2507	 l-p:0.18978901207447052
epoch£º125	 i:8 	 global-step:2508	 l-p:0.1430024355649948
epoch£º125	 i:9 	 global-step:2509	 l-p:0.157185360789299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6962, 5.3588, 5.5910],
        [4.6962, 5.1141, 5.1571],
        [4.6962, 4.6962, 4.6962],
        [4.6962, 4.7012, 4.6966]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.14176620543003082 
model_pd.l_d.mean(): -20.530872344970703 
model_pd.lagr.mean(): -20.38910675048828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4757], device='cuda:0')), ('power', tensor([-21.4075], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.14176620543003082
epoch£º126	 i:1 	 global-step:2521	 l-p:0.1637873351573944
epoch£º126	 i:2 	 global-step:2522	 l-p:0.16905677318572998
epoch£º126	 i:3 	 global-step:2523	 l-p:0.17666000127792358
epoch£º126	 i:4 	 global-step:2524	 l-p:0.14755214750766754
epoch£º126	 i:5 	 global-step:2525	 l-p:0.15634267032146454
epoch£º126	 i:6 	 global-step:2526	 l-p:0.11422953009605408
epoch£º126	 i:7 	 global-step:2527	 l-p:0.17043323814868927
epoch£º126	 i:8 	 global-step:2528	 l-p:-0.06515255570411682
epoch£º126	 i:9 	 global-step:2529	 l-p:-0.5185613632202148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2616,  0.1673,  1.0000,  0.1070,
          1.0000,  0.6396, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6901,  0.6098,  1.0000,  0.5389,
          1.0000,  0.8837, 31.6228]], device='cuda:0')
 pt:tensor([[4.6959, 4.8052, 4.7543],
        [4.6959, 4.7707, 4.7279],
        [4.6959, 4.8807, 4.8291],
        [4.6959, 5.4680, 5.8033]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.09281951934099197 
model_pd.l_d.mean(): -19.574377059936523 
model_pd.lagr.mean(): -19.481557846069336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5353], device='cuda:0')), ('power', tensor([-20.4949], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.09281951934099197
epoch£º127	 i:1 	 global-step:2541	 l-p:0.21945057809352875
epoch£º127	 i:2 	 global-step:2542	 l-p:0.13661977648735046
epoch£º127	 i:3 	 global-step:2543	 l-p:0.13982611894607544
epoch£º127	 i:4 	 global-step:2544	 l-p:0.11282375454902649
epoch£º127	 i:5 	 global-step:2545	 l-p:0.142952099442482
epoch£º127	 i:6 	 global-step:2546	 l-p:0.11857909709215164
epoch£º127	 i:7 	 global-step:2547	 l-p:0.11662494391202927
epoch£º127	 i:8 	 global-step:2548	 l-p:0.11878547072410583
epoch£º127	 i:9 	 global-step:2549	 l-p:0.1120586022734642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2023, 5.6782, 5.7234],
        [5.2023, 5.3898, 5.3274],
        [5.2023, 5.2052, 5.2025],
        [5.2023, 5.2030, 5.2023]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.09645652770996094 
model_pd.l_d.mean(): -19.50374984741211 
model_pd.lagr.mean(): -19.40729331970215 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4358], device='cuda:0')), ('power', tensor([-20.3198], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.09645652770996094
epoch£º128	 i:1 	 global-step:2561	 l-p:0.12358741462230682
epoch£º128	 i:2 	 global-step:2562	 l-p:0.12217824906110764
epoch£º128	 i:3 	 global-step:2563	 l-p:0.20868343114852905
epoch£º128	 i:4 	 global-step:2564	 l-p:0.14288531243801117
epoch£º128	 i:5 	 global-step:2565	 l-p:0.1457899510860443
epoch£º128	 i:6 	 global-step:2566	 l-p:0.1657789647579193
epoch£º128	 i:7 	 global-step:2567	 l-p:0.1275172084569931
epoch£º128	 i:8 	 global-step:2568	 l-p:0.14621232450008392
epoch£º128	 i:9 	 global-step:2569	 l-p:0.042536038905382156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7409, 4.7416, 4.7409],
        [4.7409, 4.7573, 4.7438],
        [4.7409, 4.7409, 4.7409],
        [4.7409, 4.8654, 4.8120]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.122541643679142 
model_pd.l_d.mean(): -19.828149795532227 
model_pd.lagr.mean(): -19.705608367919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5523], device='cuda:0')), ('power', tensor([-20.7709], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.122541643679142
epoch£º129	 i:1 	 global-step:2581	 l-p:0.33786818385124207
epoch£º129	 i:2 	 global-step:2582	 l-p:0.14745348691940308
epoch£º129	 i:3 	 global-step:2583	 l-p:0.1577061116695404
epoch£º129	 i:4 	 global-step:2584	 l-p:0.13060951232910156
epoch£º129	 i:5 	 global-step:2585	 l-p:0.12578070163726807
epoch£º129	 i:6 	 global-step:2586	 l-p:0.15259769558906555
epoch£º129	 i:7 	 global-step:2587	 l-p:0.14920112490653992
epoch£º129	 i:8 	 global-step:2588	 l-p:0.14381198585033417
epoch£º129	 i:9 	 global-step:2589	 l-p:0.12665057182312012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9159, 5.0551, 4.9982],
        [4.9159, 4.9159, 4.9159],
        [4.9159, 4.9159, 4.9159],
        [4.9159, 4.9169, 4.9159]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.025404412299394608 
model_pd.l_d.mean(): -18.60327911376953 
model_pd.lagr.mean(): -18.5778751373291 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5293], device='cuda:0')), ('power', tensor([-19.4993], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.025404412299394608
epoch£º130	 i:1 	 global-step:2601	 l-p:0.15718449652194977
epoch£º130	 i:2 	 global-step:2602	 l-p:0.1308944821357727
epoch£º130	 i:3 	 global-step:2603	 l-p:0.13636435568332672
epoch£º130	 i:4 	 global-step:2604	 l-p:0.126901313662529
epoch£º130	 i:5 	 global-step:2605	 l-p:0.15089957416057587
epoch£º130	 i:6 	 global-step:2606	 l-p:0.1229451522231102
epoch£º130	 i:7 	 global-step:2607	 l-p:0.20119552314281464
epoch£º130	 i:8 	 global-step:2608	 l-p:0.12892630696296692
epoch£º130	 i:9 	 global-step:2609	 l-p:0.13687573373317719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9100, 4.9100, 4.9100],
        [4.9100, 4.9848, 4.9408],
        [4.9100, 4.9101, 4.9100],
        [4.9100, 4.9100, 4.9100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.14009274542331696 
model_pd.l_d.mean(): -19.560813903808594 
model_pd.lagr.mean(): -19.42072105407715 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4999], device='cuda:0')), ('power', tensor([-20.4443], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.14009274542331696
epoch£º131	 i:1 	 global-step:2621	 l-p:0.16242513060569763
epoch£º131	 i:2 	 global-step:2622	 l-p:0.15224970877170563
epoch£º131	 i:3 	 global-step:2623	 l-p:0.08232873678207397
epoch£º131	 i:4 	 global-step:2624	 l-p:0.1256909817457199
epoch£º131	 i:5 	 global-step:2625	 l-p:0.14866870641708374
epoch£º131	 i:6 	 global-step:2626	 l-p:0.13244310021400452
epoch£º131	 i:7 	 global-step:2627	 l-p:0.15719296038150787
epoch£º131	 i:8 	 global-step:2628	 l-p:0.12485689669847488
epoch£º131	 i:9 	 global-step:2629	 l-p:0.1539871245622635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9374, 4.9374, 4.9374],
        [4.9374, 6.0227, 6.6542],
        [4.9374, 4.9374, 4.9374],
        [4.9374, 5.3614, 5.3938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.11696414649486542 
model_pd.l_d.mean(): -18.966934204101562 
model_pd.lagr.mean(): -18.8499698638916 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5279], device='cuda:0')), ('power', tensor([-19.8684], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.11696414649486542
epoch£º132	 i:1 	 global-step:2641	 l-p:0.11800500005483627
epoch£º132	 i:2 	 global-step:2642	 l-p:0.12227363139390945
epoch£º132	 i:3 	 global-step:2643	 l-p:0.1284971982240677
epoch£º132	 i:4 	 global-step:2644	 l-p:0.14034713804721832
epoch£º132	 i:5 	 global-step:2645	 l-p:0.13031207025051117
epoch£º132	 i:6 	 global-step:2646	 l-p:0.1328715831041336
epoch£º132	 i:7 	 global-step:2647	 l-p:-0.19374778866767883
epoch£º132	 i:8 	 global-step:2648	 l-p:0.12544625997543335
epoch£º132	 i:9 	 global-step:2649	 l-p:0.16980771720409393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9641, 4.9641, 4.9641],
        [4.9641, 4.9641, 4.9641],
        [4.9641, 5.2692, 5.2417],
        [4.9641, 5.0749, 5.0212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.14210744202136993 
model_pd.l_d.mean(): -18.43720817565918 
model_pd.lagr.mean(): -18.295101165771484 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5124], device='cuda:0')), ('power', tensor([-19.3127], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.14210744202136993
epoch£º133	 i:1 	 global-step:2661	 l-p:0.12322454899549484
epoch£º133	 i:2 	 global-step:2662	 l-p:0.1352553367614746
epoch£º133	 i:3 	 global-step:2663	 l-p:0.13401657342910767
epoch£º133	 i:4 	 global-step:2664	 l-p:0.13327601552009583
epoch£º133	 i:5 	 global-step:2665	 l-p:0.12331661581993103
epoch£º133	 i:6 	 global-step:2666	 l-p:0.15417428314685822
epoch£º133	 i:7 	 global-step:2667	 l-p:0.10319801419973373
epoch£º133	 i:8 	 global-step:2668	 l-p:0.15837308764457703
epoch£º133	 i:9 	 global-step:2669	 l-p:0.13632750511169434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9537, 5.2429, 5.2102],
        [4.9537, 5.0321, 4.9868],
        [4.9537, 4.9692, 4.9563],
        [4.9537, 4.9562, 4.9539]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.13941453397274017 
model_pd.l_d.mean(): -20.17563819885254 
model_pd.lagr.mean(): -20.036224365234375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4421], device='cuda:0')), ('power', tensor([-21.0109], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.13941453397274017
epoch£º134	 i:1 	 global-step:2681	 l-p:0.12912617623806
epoch£º134	 i:2 	 global-step:2682	 l-p:0.13009434938430786
epoch£º134	 i:3 	 global-step:2683	 l-p:0.20380719006061554
epoch£º134	 i:4 	 global-step:2684	 l-p:0.12816210091114044
epoch£º134	 i:5 	 global-step:2685	 l-p:0.11584366858005524
epoch£º134	 i:6 	 global-step:2686	 l-p:0.1313006728887558
epoch£º134	 i:7 	 global-step:2687	 l-p:0.11744365096092224
epoch£º134	 i:8 	 global-step:2688	 l-p:0.11537977308034897
epoch£º134	 i:9 	 global-step:2689	 l-p:0.1507451832294464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9433, 4.9433, 4.9433],
        [4.9433, 6.0500, 6.7067],
        [4.9433, 5.0671, 5.0118],
        [4.9433, 5.8600, 6.3110]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.12583211064338684 
model_pd.l_d.mean(): -18.602764129638672 
model_pd.lagr.mean(): -18.476932525634766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5507], device='cuda:0')), ('power', tensor([-19.5210], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.12583211064338684
epoch£º135	 i:1 	 global-step:2701	 l-p:0.1012360081076622
epoch£º135	 i:2 	 global-step:2702	 l-p:0.12522323429584503
epoch£º135	 i:3 	 global-step:2703	 l-p:0.17937739193439484
epoch£º135	 i:4 	 global-step:2704	 l-p:0.1287069469690323
epoch£º135	 i:5 	 global-step:2705	 l-p:0.15487010776996613
epoch£º135	 i:6 	 global-step:2706	 l-p:0.16504716873168945
epoch£º135	 i:7 	 global-step:2707	 l-p:0.11500675231218338
epoch£º135	 i:8 	 global-step:2708	 l-p:0.12809547781944275
epoch£º135	 i:9 	 global-step:2709	 l-p:0.1397278904914856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9491, 4.9492, 4.9491],
        [4.9491, 5.0921, 5.0352],
        [4.9491, 4.9498, 4.9491],
        [4.9491, 5.1212, 5.0643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.13986515998840332 
model_pd.l_d.mean(): -20.76788902282715 
model_pd.lagr.mean(): -20.628023147583008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3830], device='cuda:0')), ('power', tensor([-21.5529], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.13986515998840332
epoch£º136	 i:1 	 global-step:2721	 l-p:0.13961434364318848
epoch£º136	 i:2 	 global-step:2722	 l-p:0.15271472930908203
epoch£º136	 i:3 	 global-step:2723	 l-p:0.11970333009958267
epoch£º136	 i:4 	 global-step:2724	 l-p:0.1463686227798462
epoch£º136	 i:5 	 global-step:2725	 l-p:0.115297332406044
epoch£º136	 i:6 	 global-step:2726	 l-p:0.1739617884159088
epoch£º136	 i:7 	 global-step:2727	 l-p:0.1339719444513321
epoch£º136	 i:8 	 global-step:2728	 l-p:0.17495042085647583
epoch£º136	 i:9 	 global-step:2729	 l-p:0.11609138548374176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9656, 4.9656, 4.9656],
        [4.9656, 4.9820, 4.9684],
        [4.9656, 4.9656, 4.9656],
        [4.9656, 5.9701, 6.5117]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): -0.2822629511356354 
model_pd.l_d.mean(): -20.475366592407227 
model_pd.lagr.mean(): -20.75762939453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4164], device='cuda:0')), ('power', tensor([-21.2895], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:-0.2822629511356354
epoch£º137	 i:1 	 global-step:2741	 l-p:0.13479919731616974
epoch£º137	 i:2 	 global-step:2742	 l-p:0.11641594022512436
epoch£º137	 i:3 	 global-step:2743	 l-p:0.12558208405971527
epoch£º137	 i:4 	 global-step:2744	 l-p:0.11685483902692795
epoch£º137	 i:5 	 global-step:2745	 l-p:0.1309947520494461
epoch£º137	 i:6 	 global-step:2746	 l-p:0.11402095854282379
epoch£º137	 i:7 	 global-step:2747	 l-p:0.10933367162942886
epoch£º137	 i:8 	 global-step:2748	 l-p:0.14167669415473938
epoch£º137	 i:9 	 global-step:2749	 l-p:0.14591717720031738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9466, 5.0165, 4.9744],
        [4.9466, 4.9466, 4.9466],
        [4.9466, 4.9466, 4.9466],
        [4.9466, 4.9466, 4.9466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.13191936910152435 
model_pd.l_d.mean(): -20.481063842773438 
model_pd.lagr.mean(): -20.349143981933594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4150], device='cuda:0')), ('power', tensor([-21.2938], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.13191936910152435
epoch£º138	 i:1 	 global-step:2761	 l-p:0.12639765441417694
epoch£º138	 i:2 	 global-step:2762	 l-p:0.1390611082315445
epoch£º138	 i:3 	 global-step:2763	 l-p:0.24270617961883545
epoch£º138	 i:4 	 global-step:2764	 l-p:0.15009553730487823
epoch£º138	 i:5 	 global-step:2765	 l-p:0.1330632120370865
epoch£º138	 i:6 	 global-step:2766	 l-p:0.04191388562321663
epoch£º138	 i:7 	 global-step:2767	 l-p:-0.21970337629318237
epoch£º138	 i:8 	 global-step:2768	 l-p:0.1351184844970703
epoch£º138	 i:9 	 global-step:2769	 l-p:0.19326533377170563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9249, 5.9112, 6.4406],
        [4.9249, 6.1012, 6.8434],
        [4.9249, 4.9249, 4.9249],
        [4.9249, 4.9270, 4.9250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.060169026255607605 
model_pd.l_d.mean(): -18.568058013916016 
model_pd.lagr.mean(): -18.507888793945312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5337], device='cuda:0')), ('power', tensor([-19.4681], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.060169026255607605
epoch£º139	 i:1 	 global-step:2781	 l-p:0.14582718908786774
epoch£º139	 i:2 	 global-step:2782	 l-p:0.1303030103445053
epoch£º139	 i:3 	 global-step:2783	 l-p:0.13308174908161163
epoch£º139	 i:4 	 global-step:2784	 l-p:0.1270158588886261
epoch£º139	 i:5 	 global-step:2785	 l-p:0.1152954250574112
epoch£º139	 i:6 	 global-step:2786	 l-p:0.13211382925510406
epoch£º139	 i:7 	 global-step:2787	 l-p:0.11907300353050232
epoch£º139	 i:8 	 global-step:2788	 l-p:0.1624944657087326
epoch£º139	 i:9 	 global-step:2789	 l-p:0.15729738771915436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8889, 5.9202, 6.5045],
        [4.8889, 4.9225, 4.8978],
        [4.8889, 5.4863, 5.6495],
        [4.8889, 5.9396, 6.5453]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.14048518240451813 
model_pd.l_d.mean(): -19.9287052154541 
model_pd.lagr.mean(): -19.788219451904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4915], device='cuda:0')), ('power', tensor([-20.8104], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.14048518240451813
epoch£º140	 i:1 	 global-step:2801	 l-p:0.13618017733097076
epoch£º140	 i:2 	 global-step:2802	 l-p:0.1342807114124298
epoch£º140	 i:3 	 global-step:2803	 l-p:0.17512527108192444
epoch£º140	 i:4 	 global-step:2804	 l-p:0.11239739507436752
epoch£º140	 i:5 	 global-step:2805	 l-p:0.1312285214662552
epoch£º140	 i:6 	 global-step:2806	 l-p:0.13852480053901672
epoch£º140	 i:7 	 global-step:2807	 l-p:0.138691708445549
epoch£º140	 i:8 	 global-step:2808	 l-p:0.12668690085411072
epoch£º140	 i:9 	 global-step:2809	 l-p:0.12977167963981628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0322, 5.0323, 5.0322],
        [5.0322, 5.0323, 5.0322],
        [5.0322, 5.0322, 5.0322],
        [5.0322, 5.1609, 5.1045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.13614650070667267 
model_pd.l_d.mean(): -19.98208999633789 
model_pd.lagr.mean(): -19.845943450927734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4394], device='cuda:0')), ('power', tensor([-20.8108], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.13614650070667267
epoch£º141	 i:1 	 global-step:2821	 l-p:0.11856989562511444
epoch£º141	 i:2 	 global-step:2822	 l-p:0.3323909342288971
epoch£º141	 i:3 	 global-step:2823	 l-p:0.1277736872434616
epoch£º141	 i:4 	 global-step:2824	 l-p:0.13526593148708344
epoch£º141	 i:5 	 global-step:2825	 l-p:0.12895570695400238
epoch£º141	 i:6 	 global-step:2826	 l-p:0.12923283874988556
epoch£º141	 i:7 	 global-step:2827	 l-p:0.12512455880641937
epoch£º141	 i:8 	 global-step:2828	 l-p:0.006515130866318941
epoch£º141	 i:9 	 global-step:2829	 l-p:0.18718266487121582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7221, 4.8146, 4.7675],
        [4.7221, 4.8806, 4.8282],
        [4.7221, 4.8640, 4.8113],
        [4.7221, 4.7225, 4.7221]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.2943900525569916 
model_pd.l_d.mean(): -20.10194206237793 
model_pd.lagr.mean(): -19.807552337646484 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5206], device='cuda:0')), ('power', tensor([-21.0170], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.2943900525569916
epoch£º142	 i:1 	 global-step:2841	 l-p:0.16025912761688232
epoch£º142	 i:2 	 global-step:2842	 l-p:0.14169947803020477
epoch£º142	 i:3 	 global-step:2843	 l-p:0.6408146023750305
epoch£º142	 i:4 	 global-step:2844	 l-p:0.14986640214920044
epoch£º142	 i:5 	 global-step:2845	 l-p:0.15084820985794067
epoch£º142	 i:6 	 global-step:2846	 l-p:0.11263937503099442
epoch£º142	 i:7 	 global-step:2847	 l-p:0.1183956116437912
epoch£º142	 i:8 	 global-step:2848	 l-p:0.12860740721225739
epoch£º142	 i:9 	 global-step:2849	 l-p:0.11324945092201233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1462, 5.1462, 5.1462],
        [5.1462, 5.1462, 5.1462],
        [5.1462, 5.1463, 5.1462],
        [5.1462, 5.9120, 6.1954]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.11867476999759674 
model_pd.l_d.mean(): -18.80428123474121 
model_pd.lagr.mean(): -18.685606002807617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4661], device='cuda:0')), ('power', tensor([-19.6387], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.11867476999759674
epoch£º143	 i:1 	 global-step:2861	 l-p:0.11474044620990753
epoch£º143	 i:2 	 global-step:2862	 l-p:0.11817200481891632
epoch£º143	 i:3 	 global-step:2863	 l-p:0.13474223017692566
epoch£º143	 i:4 	 global-step:2864	 l-p:0.13219420611858368
epoch£º143	 i:5 	 global-step:2865	 l-p:0.1265331208705902
epoch£º143	 i:6 	 global-step:2866	 l-p:0.16579318046569824
epoch£º143	 i:7 	 global-step:2867	 l-p:0.14155298471450806
epoch£º143	 i:8 	 global-step:2868	 l-p:0.1003701388835907
epoch£º143	 i:9 	 global-step:2869	 l-p:0.163052499294281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7797, 5.9506, 6.7155],
        [4.7797, 5.3297, 5.4672],
        [4.7797, 4.8703, 4.8233],
        [4.7797, 4.7850, 4.7803]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.13472509384155273 
model_pd.l_d.mean(): -19.389169692993164 
model_pd.lagr.mean(): -19.254444122314453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5130], device='cuda:0')), ('power', tensor([-20.2830], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.13472509384155273
epoch£º144	 i:1 	 global-step:2881	 l-p:0.13452579081058502
epoch£º144	 i:2 	 global-step:2882	 l-p:0.2111060917377472
epoch£º144	 i:3 	 global-step:2883	 l-p:-0.009303970262408257
epoch£º144	 i:4 	 global-step:2884	 l-p:0.24555730819702148
epoch£º144	 i:5 	 global-step:2885	 l-p:0.15093164145946503
epoch£º144	 i:6 	 global-step:2886	 l-p:0.14586254954338074
epoch£º144	 i:7 	 global-step:2887	 l-p:0.09365858137607574
epoch£º144	 i:8 	 global-step:2888	 l-p:0.16042408347129822
epoch£º144	 i:9 	 global-step:2889	 l-p:0.1243952140212059
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9828, 5.6828, 5.9261],
        [4.9828, 5.7767, 6.1064],
        [4.9828, 4.9913, 4.9838],
        [4.9828, 6.0489, 6.6593]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.1255674660205841 
model_pd.l_d.mean(): -20.323780059814453 
model_pd.lagr.mean(): -20.198211669921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4303], device='cuda:0')), ('power', tensor([-21.1495], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.1255674660205841
epoch£º145	 i:1 	 global-step:2901	 l-p:-2.0737972259521484
epoch£º145	 i:2 	 global-step:2902	 l-p:0.1242600604891777
epoch£º145	 i:3 	 global-step:2903	 l-p:0.12902985513210297
epoch£º145	 i:4 	 global-step:2904	 l-p:0.1388099044561386
epoch£º145	 i:5 	 global-step:2905	 l-p:0.11724286526441574
epoch£º145	 i:6 	 global-step:2906	 l-p:0.14128904044628143
epoch£º145	 i:7 	 global-step:2907	 l-p:0.10773243010044098
epoch£º145	 i:8 	 global-step:2908	 l-p:0.147119402885437
epoch£º145	 i:9 	 global-step:2909	 l-p:0.14483948051929474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8819, 4.8819, 4.8819],
        [4.8819, 4.9128, 4.8897],
        [4.8819, 5.1297, 5.0891],
        [4.8819, 4.9402, 4.9032]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.15658687055110931 
model_pd.l_d.mean(): -19.482593536376953 
model_pd.lagr.mean(): -19.326005935668945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4993], device='cuda:0')), ('power', tensor([-20.3641], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.15658687055110931
epoch£º146	 i:1 	 global-step:2921	 l-p:0.1703900694847107
epoch£º146	 i:2 	 global-step:2922	 l-p:0.13860614597797394
epoch£º146	 i:3 	 global-step:2923	 l-p:0.134846031665802
epoch£º146	 i:4 	 global-step:2924	 l-p:-1.7519468069076538
epoch£º146	 i:5 	 global-step:2925	 l-p:0.1389637142419815
epoch£º146	 i:6 	 global-step:2926	 l-p:0.1581028550863266
epoch£º146	 i:7 	 global-step:2927	 l-p:0.13855187594890594
epoch£º146	 i:8 	 global-step:2928	 l-p:0.1587139517068863
epoch£º146	 i:9 	 global-step:2929	 l-p:0.17896276712417603
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8391, 5.0073, 4.9536],
        [4.8391, 4.8391, 4.8391],
        [4.8391, 4.8827, 4.8527],
        [4.8391, 5.0871, 5.0481]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.13568659126758575 
model_pd.l_d.mean(): -20.61569595336914 
model_pd.lagr.mean(): -20.480009078979492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4361], device='cuda:0')), ('power', tensor([-21.4529], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.13568659126758575
epoch£º147	 i:1 	 global-step:2941	 l-p:0.1300099790096283
epoch£º147	 i:2 	 global-step:2942	 l-p:0.13319794833660126
epoch£º147	 i:3 	 global-step:2943	 l-p:0.1640089452266693
epoch£º147	 i:4 	 global-step:2944	 l-p:0.13979099690914154
epoch£º147	 i:5 	 global-step:2945	 l-p:0.14804141223430634
epoch£º147	 i:6 	 global-step:2946	 l-p:0.05160145461559296
epoch£º147	 i:7 	 global-step:2947	 l-p:0.11923226714134216
epoch£º147	 i:8 	 global-step:2948	 l-p:0.1412106454372406
epoch£º147	 i:9 	 global-step:2949	 l-p:0.12225174903869629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0396, 5.0607, 5.0438],
        [5.0396, 5.0493, 5.0408],
        [5.0396, 6.1464, 6.7946],
        [5.0396, 5.0396, 5.0396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.12211044877767563 
model_pd.l_d.mean(): -20.333650588989258 
model_pd.lagr.mean(): -20.21154022216797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4060], device='cuda:0')), ('power', tensor([-21.1344], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.12211044877767563
epoch£º148	 i:1 	 global-step:2961	 l-p:0.1463373899459839
epoch£º148	 i:2 	 global-step:2962	 l-p:0.12663836777210236
epoch£º148	 i:3 	 global-step:2963	 l-p:0.16389359533786774
epoch£º148	 i:4 	 global-step:2964	 l-p:0.1356256753206253
epoch£º148	 i:5 	 global-step:2965	 l-p:0.11808489263057709
epoch£º148	 i:6 	 global-step:2966	 l-p:0.14149828255176544
epoch£º148	 i:7 	 global-step:2967	 l-p:0.10527406632900238
epoch£º148	 i:8 	 global-step:2968	 l-p:0.12073013186454773
epoch£º148	 i:9 	 global-step:2969	 l-p:0.14026662707328796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8322, 4.8921, 4.8547],
        [4.8322, 4.8606, 4.8392],
        [4.8322, 5.8513, 6.4364],
        [4.8322, 5.2747, 5.3323]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.21223041415214539 
model_pd.l_d.mean(): -18.60541343688965 
model_pd.lagr.mean(): -18.3931827545166 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5853], device='cuda:0')), ('power', tensor([-19.5595], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.21223041415214539
epoch£º149	 i:1 	 global-step:2981	 l-p:0.14358125627040863
epoch£º149	 i:2 	 global-step:2982	 l-p:0.25129860639572144
epoch£º149	 i:3 	 global-step:2983	 l-p:0.14014750719070435
epoch£º149	 i:4 	 global-step:2984	 l-p:0.10948871076107025
epoch£º149	 i:5 	 global-step:2985	 l-p:0.14346343278884888
epoch£º149	 i:6 	 global-step:2986	 l-p:0.13817311823368073
epoch£º149	 i:7 	 global-step:2987	 l-p:0.1873553842306137
epoch£º149	 i:8 	 global-step:2988	 l-p:0.1167837530374527
epoch£º149	 i:9 	 global-step:2989	 l-p:0.14817014336585999
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8474, 5.1334, 5.1076],
        [4.8474, 5.2568, 5.2930],
        [4.8474, 5.1272, 5.0991],
        [4.8474, 4.8492, 4.8475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.13506759703159332 
model_pd.l_d.mean(): -18.513137817382812 
model_pd.lagr.mean(): -18.378070831298828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5547], device='cuda:0')), ('power', tensor([-19.4338], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.13506759703159332
epoch£º150	 i:1 	 global-step:3001	 l-p:0.17710202932357788
epoch£º150	 i:2 	 global-step:3002	 l-p:0.07122781872749329
epoch£º150	 i:3 	 global-step:3003	 l-p:0.12189620733261108
epoch£º150	 i:4 	 global-step:3004	 l-p:0.12104124575853348
epoch£º150	 i:5 	 global-step:3005	 l-p:0.12670934200286865
epoch£º150	 i:6 	 global-step:3006	 l-p:0.12315423786640167
epoch£º150	 i:7 	 global-step:3007	 l-p:0.127155601978302
epoch£º150	 i:8 	 global-step:3008	 l-p:0.12825964391231537
epoch£º150	 i:9 	 global-step:3009	 l-p:0.1634376049041748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9275, 5.2746, 5.2721],
        [4.9275, 4.9275, 4.9275],
        [4.9275, 5.6932, 6.0056],
        [4.9275, 5.3931, 5.4598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.12991982698440552 
model_pd.l_d.mean(): -20.571439743041992 
model_pd.lagr.mean(): -20.44152069091797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4166], device='cuda:0')), ('power', tensor([-21.3876], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.12991982698440552
epoch£º151	 i:1 	 global-step:3021	 l-p:0.14067557454109192
epoch£º151	 i:2 	 global-step:3022	 l-p:0.216729536652565
epoch£º151	 i:3 	 global-step:3023	 l-p:0.015997055917978287
epoch£º151	 i:4 	 global-step:3024	 l-p:0.3210037648677826
epoch£º151	 i:5 	 global-step:3025	 l-p:0.1580210030078888
epoch£º151	 i:6 	 global-step:3026	 l-p:0.14703014492988586
epoch£º151	 i:7 	 global-step:3027	 l-p:0.062000565230846405
epoch£º151	 i:8 	 global-step:3028	 l-p:0.14358222484588623
epoch£º151	 i:9 	 global-step:3029	 l-p:0.12895508110523224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2225,  0.1348,  1.0000,  0.0817,
          1.0000,  0.6059, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228]], device='cuda:0')
 pt:tensor([[4.9232, 5.0553, 5.0008],
        [4.9232, 5.8538, 6.3311],
        [4.9232, 5.0946, 5.0399],
        [4.9232, 5.0897, 5.0347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.08862683176994324 
model_pd.l_d.mean(): -19.69965934753418 
model_pd.lagr.mean(): -19.611032485961914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5195], device='cuda:0')), ('power', tensor([-20.6060], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.08862683176994324
epoch£º152	 i:1 	 global-step:3041	 l-p:0.11843951046466827
epoch£º152	 i:2 	 global-step:3042	 l-p:0.12977595627307892
epoch£º152	 i:3 	 global-step:3043	 l-p:0.12515053153038025
epoch£º152	 i:4 	 global-step:3044	 l-p:0.13371820747852325
epoch£º152	 i:5 	 global-step:3045	 l-p:0.12178874760866165
epoch£º152	 i:6 	 global-step:3046	 l-p:0.1278415024280548
epoch£º152	 i:7 	 global-step:3047	 l-p:0.1415853500366211
epoch£º152	 i:8 	 global-step:3048	 l-p:0.1600467711687088
epoch£º152	 i:9 	 global-step:3049	 l-p:0.12454143166542053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9725, 4.9763, 4.9728],
        [4.9725, 4.9726, 4.9725],
        [4.9725, 4.9827, 4.9739],
        [4.9725, 4.9725, 4.9725]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.1731683313846588 
model_pd.l_d.mean(): -19.22088623046875 
model_pd.lagr.mean(): -19.047718048095703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4593], device='cuda:0')), ('power', tensor([-20.0560], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.1731683313846588
epoch£º153	 i:1 	 global-step:3061	 l-p:0.1291615515947342
epoch£º153	 i:2 	 global-step:3062	 l-p:0.05123897269368172
epoch£º153	 i:3 	 global-step:3063	 l-p:0.1410142332315445
epoch£º153	 i:4 	 global-step:3064	 l-p:0.13661162555217743
epoch£º153	 i:5 	 global-step:3065	 l-p:0.11786261200904846
epoch£º153	 i:6 	 global-step:3066	 l-p:0.14326177537441254
epoch£º153	 i:7 	 global-step:3067	 l-p:0.12165284901857376
epoch£º153	 i:8 	 global-step:3068	 l-p:0.14098848402500153
epoch£º153	 i:9 	 global-step:3069	 l-p:0.17211344838142395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7427, 5.3099, 5.4690],
        [4.7427, 4.8748, 4.8228],
        [4.7427, 4.7637, 4.7471],
        [4.7427, 4.7465, 4.7430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.14126120507717133 
model_pd.l_d.mean(): -18.595197677612305 
model_pd.lagr.mean(): -18.453935623168945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5887], device='cuda:0')), ('power', tensor([-19.5527], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.14126120507717133
epoch£º154	 i:1 	 global-step:3081	 l-p:0.13037525117397308
epoch£º154	 i:2 	 global-step:3082	 l-p:0.19854655861854553
epoch£º154	 i:3 	 global-step:3083	 l-p:0.1395404040813446
epoch£º154	 i:4 	 global-step:3084	 l-p:0.12053892016410828
epoch£º154	 i:5 	 global-step:3085	 l-p:0.1540290117263794
epoch£º154	 i:6 	 global-step:3086	 l-p:0.13376037776470184
epoch£º154	 i:7 	 global-step:3087	 l-p:0.1418742537498474
epoch£º154	 i:8 	 global-step:3088	 l-p:0.4737595021724701
epoch£º154	 i:9 	 global-step:3089	 l-p:0.1311841905117035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0442, 5.6797, 5.8661],
        [5.0442, 5.5915, 5.7085],
        [5.0442, 5.0859, 5.0565],
        [5.0442, 5.3629, 5.3429]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.13061369955539703 
model_pd.l_d.mean(): -19.79477882385254 
model_pd.lagr.mean(): -19.664165496826172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4491], device='cuda:0')), ('power', tensor([-20.6301], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.13061369955539703
epoch£º155	 i:1 	 global-step:3101	 l-p:0.3139098584651947
epoch£º155	 i:2 	 global-step:3102	 l-p:0.13165782392024994
epoch£º155	 i:3 	 global-step:3103	 l-p:0.1262715756893158
epoch£º155	 i:4 	 global-step:3104	 l-p:0.11895377933979034
epoch£º155	 i:5 	 global-step:3105	 l-p:0.15506669878959656
epoch£º155	 i:6 	 global-step:3106	 l-p:0.12806177139282227
epoch£º155	 i:7 	 global-step:3107	 l-p:0.1709849089384079
epoch£º155	 i:8 	 global-step:3108	 l-p:0.12235676497220993
epoch£º155	 i:9 	 global-step:3109	 l-p:0.13808046281337738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9160,  0.8896,  1.0000,  0.8640,
          1.0000,  0.9712, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228]], device='cuda:0')
 pt:tensor([[4.8869, 5.9743, 6.6295],
        [4.8869, 5.9974, 6.6788],
        [4.8869, 5.0330, 4.9787],
        [4.8869, 5.9710, 6.6225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.12627336382865906 
model_pd.l_d.mean(): -19.824609756469727 
model_pd.lagr.mean(): -19.698335647583008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5021], device='cuda:0')), ('power', tensor([-20.7154], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.12627336382865906
epoch£º156	 i:1 	 global-step:3121	 l-p:0.15085947513580322
epoch£º156	 i:2 	 global-step:3122	 l-p:0.12483720481395721
epoch£º156	 i:3 	 global-step:3123	 l-p:0.0719422772526741
epoch£º156	 i:4 	 global-step:3124	 l-p:0.14377722144126892
epoch£º156	 i:5 	 global-step:3125	 l-p:0.13755886256694794
epoch£º156	 i:6 	 global-step:3126	 l-p:0.1522367000579834
epoch£º156	 i:7 	 global-step:3127	 l-p:0.1294809877872467
epoch£º156	 i:8 	 global-step:3128	 l-p:0.12161919474601746
epoch£º156	 i:9 	 global-step:3129	 l-p:0.1456623375415802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1482,  0.0784,  1.0000,  0.0415,
          1.0000,  0.5292, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228]], device='cuda:0')
 pt:tensor([[4.9446, 5.0055, 4.9675],
        [4.9446, 5.4287, 5.5090],
        [4.9446, 5.8185, 6.2373],
        [4.9446, 5.1697, 5.1231]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.08174566179513931 
model_pd.l_d.mean(): -20.010238647460938 
model_pd.lagr.mean(): -19.92849349975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4786], device='cuda:0')), ('power', tensor([-20.8801], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.08174566179513931
epoch£º157	 i:1 	 global-step:3141	 l-p:0.11531098932027817
epoch£º157	 i:2 	 global-step:3142	 l-p:0.1698661744594574
epoch£º157	 i:3 	 global-step:3143	 l-p:0.1275646686553955
epoch£º157	 i:4 	 global-step:3144	 l-p:0.14593231678009033
epoch£º157	 i:5 	 global-step:3145	 l-p:0.20318883657455444
epoch£º157	 i:6 	 global-step:3146	 l-p:0.13395951688289642
epoch£º157	 i:7 	 global-step:3147	 l-p:0.1563970297574997
epoch£º157	 i:8 	 global-step:3148	 l-p:0.141708105802536
epoch£º157	 i:9 	 global-step:3149	 l-p:0.21375665068626404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9010, 4.9010, 4.9010],
        [4.9010, 4.9087, 4.9019],
        [4.9010, 4.9010, 4.9010],
        [4.9010, 5.2945, 5.3203]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.1473015993833542 
model_pd.l_d.mean(): -19.428543090820312 
model_pd.lagr.mean(): -19.28124237060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4816], device='cuda:0')), ('power', tensor([-20.2906], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.1473015993833542
epoch£º158	 i:1 	 global-step:3161	 l-p:0.13415612280368805
epoch£º158	 i:2 	 global-step:3162	 l-p:0.12071563303470612
epoch£º158	 i:3 	 global-step:3163	 l-p:0.119676873087883
epoch£º158	 i:4 	 global-step:3164	 l-p:0.12494034320116043
epoch£º158	 i:5 	 global-step:3165	 l-p:0.1288205087184906
epoch£º158	 i:6 	 global-step:3166	 l-p:0.14894914627075195
epoch£º158	 i:7 	 global-step:3167	 l-p:0.1391138881444931
epoch£º158	 i:8 	 global-step:3168	 l-p:0.13660237193107605
epoch£º158	 i:9 	 global-step:3169	 l-p:0.22044143080711365
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8477, 4.9810, 4.9278],
        [4.8477, 4.9055, 4.8691],
        [4.8477, 4.9473, 4.8982],
        [4.8477, 4.8794, 4.8560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.24038255214691162 
model_pd.l_d.mean(): -20.31931495666504 
model_pd.lagr.mean(): -20.07893180847168 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4756], device='cuda:0')), ('power', tensor([-21.1919], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.24038255214691162
epoch£º159	 i:1 	 global-step:3181	 l-p:0.13721820712089539
epoch£º159	 i:2 	 global-step:3182	 l-p:0.13479788601398468
epoch£º159	 i:3 	 global-step:3183	 l-p:-0.12629060447216034
epoch£º159	 i:4 	 global-step:3184	 l-p:0.15141290426254272
epoch£º159	 i:5 	 global-step:3185	 l-p:0.1312706023454666
epoch£º159	 i:6 	 global-step:3186	 l-p:0.1171879917383194
epoch£º159	 i:7 	 global-step:3187	 l-p:0.13445812463760376
epoch£º159	 i:8 	 global-step:3188	 l-p:0.12308468669652939
epoch£º159	 i:9 	 global-step:3189	 l-p:0.1776576191186905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8854, 4.9397, 4.9047],
        [4.8854, 5.5741, 5.8250],
        [4.8854, 4.8854, 4.8854],
        [4.8854, 4.8859, 4.8854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.18677742779254913 
model_pd.l_d.mean(): -20.718996047973633 
model_pd.lagr.mean(): -20.53221893310547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4133], device='cuda:0')), ('power', tensor([-21.5345], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.18677742779254913
epoch£º160	 i:1 	 global-step:3201	 l-p:0.11581452190876007
epoch£º160	 i:2 	 global-step:3202	 l-p:0.124587781727314
epoch£º160	 i:3 	 global-step:3203	 l-p:0.13099534809589386
epoch£º160	 i:4 	 global-step:3204	 l-p:0.13994251191616058
epoch£º160	 i:5 	 global-step:3205	 l-p:-0.21122778952121735
epoch£º160	 i:6 	 global-step:3206	 l-p:0.12787029147148132
epoch£º160	 i:7 	 global-step:3207	 l-p:0.1324501931667328
epoch£º160	 i:8 	 global-step:3208	 l-p:0.1338607519865036
epoch£º160	 i:9 	 global-step:3209	 l-p:0.15751293301582336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2302,  0.1411,  1.0000,  0.0865,
          1.0000,  0.6129, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1536,  0.0823,  1.0000,  0.0441,
          1.0000,  0.5356, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228]], device='cuda:0')
 pt:tensor([[4.9399, 5.0717, 5.0174],
        [4.9399, 5.0024, 4.9638],
        [4.9399, 5.5120, 5.6594],
        [4.9399, 5.5860, 5.7943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.16051407158374786 
model_pd.l_d.mean(): -20.704622268676758 
model_pd.lagr.mean(): -20.54410743713379 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4006], device='cuda:0')), ('power', tensor([-21.5067], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.16051407158374786
epoch£º161	 i:1 	 global-step:3221	 l-p:0.131182000041008
epoch£º161	 i:2 	 global-step:3222	 l-p:0.29004111886024475
epoch£º161	 i:3 	 global-step:3223	 l-p:0.11710994690656662
epoch£º161	 i:4 	 global-step:3224	 l-p:0.03606708347797394
epoch£º161	 i:5 	 global-step:3225	 l-p:0.19051805138587952
epoch£º161	 i:6 	 global-step:3226	 l-p:0.1305287927389145
epoch£º161	 i:7 	 global-step:3227	 l-p:0.14530259370803833
epoch£º161	 i:8 	 global-step:3228	 l-p:0.16932949423789978
epoch£º161	 i:9 	 global-step:3229	 l-p:0.16564024984836578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6488, 5.1075, 5.1950],
        [4.6488, 4.6493, 4.6488],
        [4.6488, 4.7230, 4.6816],
        [4.6488, 5.2229, 5.4010]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.12029161304235458 
model_pd.l_d.mean(): -20.17251968383789 
model_pd.lagr.mean(): -20.052228927612305 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5505], device='cuda:0')), ('power', tensor([-21.1199], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.12029161304235458
epoch£º162	 i:1 	 global-step:3241	 l-p:0.15348243713378906
epoch£º162	 i:2 	 global-step:3242	 l-p:0.15924693644046783
epoch£º162	 i:3 	 global-step:3243	 l-p:0.15792283415794373
epoch£º162	 i:4 	 global-step:3244	 l-p:0.04624898359179497
epoch£º162	 i:5 	 global-step:3245	 l-p:0.12786923348903656
epoch£º162	 i:6 	 global-step:3246	 l-p:0.18761226534843445
epoch£º162	 i:7 	 global-step:3247	 l-p:0.07215982675552368
epoch£º162	 i:8 	 global-step:3248	 l-p:0.3285839855670929
epoch£º162	 i:9 	 global-step:3249	 l-p:0.1245175451040268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9598, 5.8245, 6.2346],
        [4.9598, 4.9598, 4.9598],
        [4.9598, 5.2732, 5.2564],
        [4.9598, 4.9598, 4.9598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.14264750480651855 
model_pd.l_d.mean(): -18.397737503051758 
model_pd.lagr.mean(): -18.255090713500977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5616], device='cuda:0')), ('power', tensor([-19.3234], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.14264750480651855
epoch£º163	 i:1 	 global-step:3261	 l-p:0.11653215438127518
epoch£º163	 i:2 	 global-step:3262	 l-p:0.11873915046453476
epoch£º163	 i:3 	 global-step:3263	 l-p:0.15873588621616364
epoch£º163	 i:4 	 global-step:3264	 l-p:0.13054868578910828
epoch£º163	 i:5 	 global-step:3265	 l-p:0.13630154728889465
epoch£º163	 i:6 	 global-step:3266	 l-p:0.1160990297794342
epoch£º163	 i:7 	 global-step:3267	 l-p:0.13939113914966583
epoch£º163	 i:8 	 global-step:3268	 l-p:0.12521575391292572
epoch£º163	 i:9 	 global-step:3269	 l-p:0.13109610974788666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9667, 5.1861, 5.1384],
        [4.9667, 4.9707, 4.9671],
        [4.9667, 4.9675, 4.9668],
        [4.9667, 4.9891, 4.9715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.12684156000614166 
model_pd.l_d.mean(): -20.00238609313965 
model_pd.lagr.mean(): -19.87554359436035 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4593], device='cuda:0')), ('power', tensor([-20.8521], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.12684156000614166
epoch£º164	 i:1 	 global-step:3281	 l-p:0.12776346504688263
epoch£º164	 i:2 	 global-step:3282	 l-p:0.14595593512058258
epoch£º164	 i:3 	 global-step:3283	 l-p:0.11097553372383118
epoch£º164	 i:4 	 global-step:3284	 l-p:0.14655715227127075
epoch£º164	 i:5 	 global-step:3285	 l-p:0.12940388917922974
epoch£º164	 i:6 	 global-step:3286	 l-p:0.12951509654521942
epoch£º164	 i:7 	 global-step:3287	 l-p:0.14458614587783813
epoch£º164	 i:8 	 global-step:3288	 l-p:0.1499103307723999
epoch£º164	 i:9 	 global-step:3289	 l-p:0.14764690399169922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8978, 4.8978, 4.8978],
        [4.8978, 5.6363, 5.9333],
        [4.8978, 5.4146, 5.5247],
        [4.8978, 5.0782, 5.0259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.1308986097574234 
model_pd.l_d.mean(): -20.044029235839844 
model_pd.lagr.mean(): -19.913129806518555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4774], device='cuda:0')), ('power', tensor([-20.9133], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.1308986097574234
epoch£º165	 i:1 	 global-step:3301	 l-p:0.1320672333240509
epoch£º165	 i:2 	 global-step:3302	 l-p:0.1626777946949005
epoch£º165	 i:3 	 global-step:3303	 l-p:0.14320753514766693
epoch£º165	 i:4 	 global-step:3304	 l-p:-0.022542640566825867
epoch£º165	 i:5 	 global-step:3305	 l-p:0.3691575527191162
epoch£º165	 i:6 	 global-step:3306	 l-p:0.12381221354007721
epoch£º165	 i:7 	 global-step:3307	 l-p:0.1419675201177597
epoch£º165	 i:8 	 global-step:3308	 l-p:-0.0331370010972023
epoch£º165	 i:9 	 global-step:3309	 l-p:0.1300966739654541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8856, 4.8871, 4.8857],
        [4.8856, 4.9521, 4.9122],
        [4.8856, 4.9613, 4.9182],
        [4.8856, 4.8856, 4.8856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.14085695147514343 
model_pd.l_d.mean(): -18.828521728515625 
model_pd.lagr.mean(): -18.687664031982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5416], device='cuda:0')), ('power', tensor([-19.7416], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.14085695147514343
epoch£º166	 i:1 	 global-step:3321	 l-p:0.13018007576465607
epoch£º166	 i:2 	 global-step:3322	 l-p:0.14248226583003998
epoch£º166	 i:3 	 global-step:3323	 l-p:0.1410445123910904
epoch£º166	 i:4 	 global-step:3324	 l-p:0.12556493282318115
epoch£º166	 i:5 	 global-step:3325	 l-p:0.110594242811203
epoch£º166	 i:6 	 global-step:3326	 l-p:0.12316251546144485
epoch£º166	 i:7 	 global-step:3327	 l-p:0.12944965064525604
epoch£º166	 i:8 	 global-step:3328	 l-p:0.11851336807012558
epoch£º166	 i:9 	 global-step:3329	 l-p:0.5260277986526489
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0103, 5.0103, 5.0103],
        [5.0103, 5.1612, 5.1055],
        [5.0103, 6.2028, 6.9613],
        [5.0103, 5.0218, 5.0120]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.13463887572288513 
model_pd.l_d.mean(): -19.303730010986328 
model_pd.lagr.mean(): -19.169090270996094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4810], device='cuda:0')), ('power', tensor([-20.1628], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.13463887572288513
epoch£º167	 i:1 	 global-step:3341	 l-p:0.12992708384990692
epoch£º167	 i:2 	 global-step:3342	 l-p:0.1270671784877777
epoch£º167	 i:3 	 global-step:3343	 l-p:0.1389654576778412
epoch£º167	 i:4 	 global-step:3344	 l-p:0.13274388015270233
epoch£º167	 i:5 	 global-step:3345	 l-p:0.12151391804218292
epoch£º167	 i:6 	 global-step:3346	 l-p:0.4343430697917938
epoch£º167	 i:7 	 global-step:3347	 l-p:0.07806149870157242
epoch£º167	 i:8 	 global-step:3348	 l-p:0.055211201310157776
epoch£º167	 i:9 	 global-step:3349	 l-p:0.12085284292697906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8224, 4.8229, 4.8224],
        [4.8224, 5.8337, 6.4204],
        [4.8224, 4.8224, 4.8224],
        [4.8224, 4.8224, 4.8224]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.1285671591758728 
model_pd.l_d.mean(): -18.50955581665039 
model_pd.lagr.mean(): -18.38098907470703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5759], device='cuda:0')), ('power', tensor([-19.4521], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.1285671591758728
epoch£º168	 i:1 	 global-step:3361	 l-p:0.1269046813249588
epoch£º168	 i:2 	 global-step:3362	 l-p:0.14674247801303864
epoch£º168	 i:3 	 global-step:3363	 l-p:0.14480631053447723
epoch£º168	 i:4 	 global-step:3364	 l-p:0.14134523272514343
epoch£º168	 i:5 	 global-step:3365	 l-p:0.11977994441986084
epoch£º168	 i:6 	 global-step:3366	 l-p:0.11623001098632812
epoch£º168	 i:7 	 global-step:3367	 l-p:0.14006727933883667
epoch£º168	 i:8 	 global-step:3368	 l-p:0.15061786770820618
epoch£º168	 i:9 	 global-step:3369	 l-p:0.12784546613693237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8985, 4.8987, 4.8985],
        [4.8985, 4.9166, 4.9020],
        [4.8985, 5.0277, 4.9744],
        [4.8985, 4.9483, 4.9153]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.13707441091537476 
model_pd.l_d.mean(): -20.550640106201172 
model_pd.lagr.mean(): -20.41356658935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4272], device='cuda:0')), ('power', tensor([-21.3774], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.13707441091537476
epoch£º169	 i:1 	 global-step:3381	 l-p:0.14157477021217346
epoch£º169	 i:2 	 global-step:3382	 l-p:0.13314244151115417
epoch£º169	 i:3 	 global-step:3383	 l-p:-0.1876201182603836
epoch£º169	 i:4 	 global-step:3384	 l-p:0.1355348378419876
epoch£º169	 i:5 	 global-step:3385	 l-p:0.09304666519165039
epoch£º169	 i:6 	 global-step:3386	 l-p:0.03840412199497223
epoch£º169	 i:7 	 global-step:3387	 l-p:0.15152761340141296
epoch£º169	 i:8 	 global-step:3388	 l-p:0.12490642070770264
epoch£º169	 i:9 	 global-step:3389	 l-p:0.14755713939666748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9601, 5.1542, 5.1027],
        [4.9601, 4.9609, 4.9601],
        [4.9601, 4.9834, 4.9651],
        [4.9601, 4.9607, 4.9601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.14967510104179382 
model_pd.l_d.mean(): -18.643831253051758 
model_pd.lagr.mean(): -18.494155883789062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5315], device='cuda:0')), ('power', tensor([-19.5429], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.14967510104179382
epoch£º170	 i:1 	 global-step:3401	 l-p:0.13907352089881897
epoch£º170	 i:2 	 global-step:3402	 l-p:0.13736660778522491
epoch£º170	 i:3 	 global-step:3403	 l-p:0.12428320944309235
epoch£º170	 i:4 	 global-step:3404	 l-p:0.12301449477672577
epoch£º170	 i:5 	 global-step:3405	 l-p:0.3005020320415497
epoch£º170	 i:6 	 global-step:3406	 l-p:0.11999039351940155
epoch£º170	 i:7 	 global-step:3407	 l-p:0.12206236273050308
epoch£º170	 i:8 	 global-step:3408	 l-p:0.1278916448354721
epoch£º170	 i:9 	 global-step:3409	 l-p:0.15006256103515625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9603, 5.4463, 5.5317],
        [4.9603, 4.9616, 4.9604],
        [4.9603, 4.9849, 4.9658],
        [4.9603, 4.9603, 4.9603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.11712026596069336 
model_pd.l_d.mean(): -18.537031173706055 
model_pd.lagr.mean(): -18.419910430908203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5485], device='cuda:0')), ('power', tensor([-19.4518], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.11712026596069336
epoch£º171	 i:1 	 global-step:3421	 l-p:0.12483041733503342
epoch£º171	 i:2 	 global-step:3422	 l-p:0.1664636731147766
epoch£º171	 i:3 	 global-step:3423	 l-p:0.1444704681634903
epoch£º171	 i:4 	 global-step:3424	 l-p:0.0778638944029808
epoch£º171	 i:5 	 global-step:3425	 l-p:0.11658232659101486
epoch£º171	 i:6 	 global-step:3426	 l-p:0.13424564898014069
epoch£º171	 i:7 	 global-step:3427	 l-p:0.2401965707540512
epoch£º171	 i:8 	 global-step:3428	 l-p:0.13771477341651917
epoch£º171	 i:9 	 global-step:3429	 l-p:0.15457783639431
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8199, 4.8204, 4.8200],
        [4.8199, 4.8252, 4.8205],
        [4.8199, 5.0659, 5.0309],
        [4.8199, 4.8199, 4.8199]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.11449600011110306 
model_pd.l_d.mean(): -20.62439727783203 
model_pd.lagr.mean(): -20.50990104675293 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4473], device='cuda:0')), ('power', tensor([-21.4733], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.11449600011110306
epoch£º172	 i:1 	 global-step:3441	 l-p:0.18389369547367096
epoch£º172	 i:2 	 global-step:3442	 l-p:0.09504890441894531
epoch£º172	 i:3 	 global-step:3443	 l-p:-1.0386042594909668
epoch£º172	 i:4 	 global-step:3444	 l-p:0.12588798999786377
epoch£º172	 i:5 	 global-step:3445	 l-p:0.13554921746253967
epoch£º172	 i:6 	 global-step:3446	 l-p:0.1393984705209732
epoch£º172	 i:7 	 global-step:3447	 l-p:0.1744910329580307
epoch£º172	 i:8 	 global-step:3448	 l-p:0.12032045423984528
epoch£º172	 i:9 	 global-step:3449	 l-p:0.12997741997241974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2106,  0.1253,  1.0000,  0.0745,
          1.0000,  0.5949, 31.6228]], device='cuda:0')
 pt:tensor([[5.1254, 6.2466, 6.9083],
        [5.1254, 5.2239, 5.1731],
        [5.1254, 5.8157, 6.0485],
        [5.1254, 5.2359, 5.1826]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.11865239590406418 
model_pd.l_d.mean(): -19.316242218017578 
model_pd.lagr.mean(): -19.197589874267578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4771], device='cuda:0')), ('power', tensor([-20.1716], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.11865239590406418
epoch£º173	 i:1 	 global-step:3461	 l-p:0.10033416748046875
epoch£º173	 i:2 	 global-step:3462	 l-p:0.1300189197063446
epoch£º173	 i:3 	 global-step:3463	 l-p:0.14521074295043945
epoch£º173	 i:4 	 global-step:3464	 l-p:0.1267080008983612
epoch£º173	 i:5 	 global-step:3465	 l-p:0.14462321996688843
epoch£º173	 i:6 	 global-step:3466	 l-p:0.1343546062707901
epoch£º173	 i:7 	 global-step:3467	 l-p:0.10543140769004822
epoch£º173	 i:8 	 global-step:3468	 l-p:0.10149982571601868
epoch£º173	 i:9 	 global-step:3469	 l-p:0.14369843900203705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6634, 5.4109, 5.7510],
        [4.6634, 4.6745, 4.6650],
        [4.6634, 4.7277, 4.6890],
        [4.6634, 4.7803, 4.7304]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.1581508368253708 
model_pd.l_d.mean(): -20.09773063659668 
model_pd.lagr.mean(): -19.939579010009766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5430], device='cuda:0')), ('power', tensor([-21.0360], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.1581508368253708
epoch£º174	 i:1 	 global-step:3481	 l-p:0.13909578323364258
epoch£º174	 i:2 	 global-step:3482	 l-p:0.10326255857944489
epoch£º174	 i:3 	 global-step:3483	 l-p:0.15481853485107422
epoch£º174	 i:4 	 global-step:3484	 l-p:0.15464891493320465
epoch£º174	 i:5 	 global-step:3485	 l-p:0.3072664141654968
epoch£º174	 i:6 	 global-step:3486	 l-p:0.1660870760679245
epoch£º174	 i:7 	 global-step:3487	 l-p:0.1338622272014618
epoch£º174	 i:8 	 global-step:3488	 l-p:0.1264512985944748
epoch£º174	 i:9 	 global-step:3489	 l-p:0.11638621985912323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1786, 5.1812, 5.1787],
        [5.1786, 5.1901, 5.1802],
        [5.1786, 5.2328, 5.1969],
        [5.1786, 5.2625, 5.2153]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.12009971588850021 
model_pd.l_d.mean(): -20.433345794677734 
model_pd.lagr.mean(): -20.31324577331543 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3486], device='cuda:0')), ('power', tensor([-21.1765], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.12009971588850021
epoch£º175	 i:1 	 global-step:3501	 l-p:0.11928584426641464
epoch£º175	 i:2 	 global-step:3502	 l-p:0.15308333933353424
epoch£º175	 i:3 	 global-step:3503	 l-p:0.17580509185791016
epoch£º175	 i:4 	 global-step:3504	 l-p:0.13002581894397736
epoch£º175	 i:5 	 global-step:3505	 l-p:0.11786679178476334
epoch£º175	 i:6 	 global-step:3506	 l-p:0.11526830494403839
epoch£º175	 i:7 	 global-step:3507	 l-p:0.12464997917413712
epoch£º175	 i:8 	 global-step:3508	 l-p:0.1261734515428543
epoch£º175	 i:9 	 global-step:3509	 l-p:0.1252153068780899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9098, 4.9098, 4.9098],
        [4.9098, 4.9275, 4.9131],
        [4.9098, 5.4311, 5.5481],
        [4.9098, 5.1124, 5.0639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.09986858069896698 
model_pd.l_d.mean(): -19.159143447875977 
model_pd.lagr.mean(): -19.059274673461914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5124], device='cuda:0')), ('power', tensor([-20.0481], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.09986858069896698
epoch£º176	 i:1 	 global-step:3521	 l-p:0.17920637130737305
epoch£º176	 i:2 	 global-step:3522	 l-p:0.13702592253684998
epoch£º176	 i:3 	 global-step:3523	 l-p:0.1364450305700302
epoch£º176	 i:4 	 global-step:3524	 l-p:0.09027751535177231
epoch£º176	 i:5 	 global-step:3525	 l-p:0.1903495192527771
epoch£º176	 i:6 	 global-step:3526	 l-p:0.1339425891637802
epoch£º176	 i:7 	 global-step:3527	 l-p:0.13098503649234772
epoch£º176	 i:8 	 global-step:3528	 l-p:0.1533469557762146
epoch£º176	 i:9 	 global-step:3529	 l-p:0.14749778807163239
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8623, 5.9445, 6.6069],
        [4.8623, 4.8629, 4.8623],
        [4.8623, 4.9598, 4.9109],
        [4.8623, 4.8625, 4.8623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.10611479729413986 
model_pd.l_d.mean(): -19.665966033935547 
model_pd.lagr.mean(): -19.559850692749023 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5110], device='cuda:0')), ('power', tensor([-20.5630], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.10611479729413986
epoch£º177	 i:1 	 global-step:3541	 l-p:0.12885801494121552
epoch£º177	 i:2 	 global-step:3542	 l-p:0.15985116362571716
epoch£º177	 i:3 	 global-step:3543	 l-p:0.10898877680301666
epoch£º177	 i:4 	 global-step:3544	 l-p:0.13846485316753387
epoch£º177	 i:5 	 global-step:3545	 l-p:0.10921275615692139
epoch£º177	 i:6 	 global-step:3546	 l-p:0.13302819430828094
epoch£º177	 i:7 	 global-step:3547	 l-p:0.12833167612552643
epoch£º177	 i:8 	 global-step:3548	 l-p:0.157134547829628
epoch£º177	 i:9 	 global-step:3549	 l-p:0.4516797661781311
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8525, 4.8527, 4.8525],
        [4.8525, 4.8525, 4.8525],
        [4.8525, 4.8525, 4.8525],
        [4.8525, 4.9162, 4.8770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): -1.0232150554656982 
model_pd.l_d.mean(): -18.163206100463867 
model_pd.lagr.mean(): -19.186420440673828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5643], device='cuda:0')), ('power', tensor([-19.0873], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:-1.0232150554656982
epoch£º178	 i:1 	 global-step:3561	 l-p:0.15542499721050262
epoch£º178	 i:2 	 global-step:3562	 l-p:0.14136654138565063
epoch£º178	 i:3 	 global-step:3563	 l-p:0.11585019528865814
epoch£º178	 i:4 	 global-step:3564	 l-p:0.13169461488723755
epoch£º178	 i:5 	 global-step:3565	 l-p:0.15180423855781555
epoch£º178	 i:6 	 global-step:3566	 l-p:0.12268077582120895
epoch£º178	 i:7 	 global-step:3567	 l-p:0.19858847558498383
epoch£º178	 i:8 	 global-step:3568	 l-p:0.11940772831439972
epoch£º178	 i:9 	 global-step:3569	 l-p:0.2541449964046478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9121, 4.9131, 4.9121],
        [4.9121, 4.9335, 4.9164],
        [4.9121, 4.9322, 4.9160],
        [4.9121, 4.9125, 4.9121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.15105925500392914 
model_pd.l_d.mean(): -20.384044647216797 
model_pd.lagr.mean(): -20.23298454284668 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4428], device='cuda:0')), ('power', tensor([-21.2238], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.15105925500392914
epoch£º179	 i:1 	 global-step:3581	 l-p:0.20853617787361145
epoch£º179	 i:2 	 global-step:3582	 l-p:0.12464186549186707
epoch£º179	 i:3 	 global-step:3583	 l-p:0.12562181055545807
epoch£º179	 i:4 	 global-step:3584	 l-p:0.158307284116745
epoch£º179	 i:5 	 global-step:3585	 l-p:0.13346686959266663
epoch£º179	 i:6 	 global-step:3586	 l-p:0.28718096017837524
epoch£º179	 i:7 	 global-step:3587	 l-p:0.13058115541934967
epoch£º179	 i:8 	 global-step:3588	 l-p:0.11701638251543045
epoch£º179	 i:9 	 global-step:3589	 l-p:-0.12632600963115692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8123, 4.8129, 4.8123],
        [4.8123, 4.8127, 4.8123],
        [4.8123, 4.9897, 4.9394],
        [4.8123, 4.8123, 4.8123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.13293418288230896 
model_pd.l_d.mean(): -20.748525619506836 
model_pd.lagr.mean(): -20.615591049194336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4357], device='cuda:0')), ('power', tensor([-21.5878], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.13293418288230896
epoch£º180	 i:1 	 global-step:3601	 l-p:0.1617542803287506
epoch£º180	 i:2 	 global-step:3602	 l-p:0.09531696140766144
epoch£º180	 i:3 	 global-step:3603	 l-p:0.09378103911876678
epoch£º180	 i:4 	 global-step:3604	 l-p:0.11027677357196808
epoch£º180	 i:5 	 global-step:3605	 l-p:0.21166624128818512
epoch£º180	 i:6 	 global-step:3606	 l-p:0.08095022290945053
epoch£º180	 i:7 	 global-step:3607	 l-p:0.13230426609516144
epoch£º180	 i:8 	 global-step:3608	 l-p:0.13938741385936737
epoch£º180	 i:9 	 global-step:3609	 l-p:0.13366194069385529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9725, 4.9790, 4.9732],
        [4.9725, 5.0452, 5.0023],
        [4.9725, 5.3678, 5.3969],
        [4.9725, 5.6330, 5.8590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.1442728042602539 
model_pd.l_d.mean(): -20.305810928344727 
model_pd.lagr.mean(): -20.161537170410156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4402], device='cuda:0')), ('power', tensor([-21.1415], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.1442728042602539
epoch£º181	 i:1 	 global-step:3621	 l-p:0.17047318816184998
epoch£º181	 i:2 	 global-step:3622	 l-p:0.11988332122564316
epoch£º181	 i:3 	 global-step:3623	 l-p:0.1034807339310646
epoch£º181	 i:4 	 global-step:3624	 l-p:0.13222669064998627
epoch£º181	 i:5 	 global-step:3625	 l-p:0.10389237105846405
epoch£º181	 i:6 	 global-step:3626	 l-p:0.1311623454093933
epoch£º181	 i:7 	 global-step:3627	 l-p:0.1250760853290558
epoch£º181	 i:8 	 global-step:3628	 l-p:0.16185103356838226
epoch£º181	 i:9 	 global-step:3629	 l-p:0.1851235181093216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9492, 4.9570, 4.9501],
        [4.9492, 4.9979, 4.9648],
        [4.9492, 5.0694, 5.0161],
        [4.9492, 4.9492, 4.9492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.12389960139989853 
model_pd.l_d.mean(): -20.09598159790039 
model_pd.lagr.mean(): -19.972082138061523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4366], device='cuda:0')), ('power', tensor([-20.9239], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.12389960139989853
epoch£º182	 i:1 	 global-step:3641	 l-p:0.1425870656967163
epoch£º182	 i:2 	 global-step:3642	 l-p:0.1285216063261032
epoch£º182	 i:3 	 global-step:3643	 l-p:0.12190350890159607
epoch£º182	 i:4 	 global-step:3644	 l-p:0.24599803984165192
epoch£º182	 i:5 	 global-step:3645	 l-p:0.0875132605433464
epoch£º182	 i:6 	 global-step:3646	 l-p:0.1872163563966751
epoch£º182	 i:7 	 global-step:3647	 l-p:0.21141494810581207
epoch£º182	 i:8 	 global-step:3648	 l-p:0.1425837129354477
epoch£º182	 i:9 	 global-step:3649	 l-p:0.11313716322183609
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0789, 5.3957, 5.3791],
        [5.0789, 5.2184, 5.1622],
        [5.0789, 5.0870, 5.0799],
        [5.0789, 5.8080, 6.0841]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.12538251280784607 
model_pd.l_d.mean(): -20.176179885864258 
model_pd.lagr.mean(): -20.050796508789062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4124], device='cuda:0')), ('power', tensor([-20.9806], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.12538251280784607
epoch£º183	 i:1 	 global-step:3661	 l-p:0.12486825883388519
epoch£º183	 i:2 	 global-step:3662	 l-p:0.1288655698299408
epoch£º183	 i:3 	 global-step:3663	 l-p:0.12330478429794312
epoch£º183	 i:4 	 global-step:3664	 l-p:0.1713303178548813
epoch£º183	 i:5 	 global-step:3665	 l-p:0.21947802603244781
epoch£º183	 i:6 	 global-step:3666	 l-p:0.13720163702964783
epoch£º183	 i:7 	 global-step:3667	 l-p:0.26310795545578003
epoch£º183	 i:8 	 global-step:3668	 l-p:0.11445882171392441
epoch£º183	 i:9 	 global-step:3669	 l-p:0.12567290663719177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9519, 5.0213, 4.9794],
        [4.9519, 5.7272, 6.0607],
        [4.9519, 4.9840, 4.9599],
        [4.9519, 4.9519, 4.9519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.13699615001678467 
model_pd.l_d.mean(): -18.871442794799805 
model_pd.lagr.mean(): -18.734447479248047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5184], device='cuda:0')), ('power', tensor([-19.7612], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.13699615001678467
epoch£º184	 i:1 	 global-step:3681	 l-p:0.15108907222747803
epoch£º184	 i:2 	 global-step:3682	 l-p:0.12737801671028137
epoch£º184	 i:3 	 global-step:3683	 l-p:0.12049198150634766
epoch£º184	 i:4 	 global-step:3684	 l-p:0.12847281992435455
epoch£º184	 i:5 	 global-step:3685	 l-p:0.09954355657100677
epoch£º184	 i:6 	 global-step:3686	 l-p:0.13395701348781586
epoch£º184	 i:7 	 global-step:3687	 l-p:0.12063653767108917
epoch£º184	 i:8 	 global-step:3688	 l-p:0.11419675499200821
epoch£º184	 i:9 	 global-step:3689	 l-p:0.09945552796125412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9261, 5.0404, 4.9878],
        [4.9261, 4.9261, 4.9261],
        [4.9261, 4.9400, 4.9283],
        [4.9261, 5.6483, 5.9357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.12988707423210144 
model_pd.l_d.mean(): -19.21930694580078 
model_pd.lagr.mean(): -19.089420318603516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5234], device='cuda:0')), ('power', tensor([-20.1208], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.12988707423210144
epoch£º185	 i:1 	 global-step:3701	 l-p:0.2642887532711029
epoch£º185	 i:2 	 global-step:3702	 l-p:0.1377105712890625
epoch£º185	 i:3 	 global-step:3703	 l-p:0.15080229938030243
epoch£º185	 i:4 	 global-step:3704	 l-p:0.14563164114952087
epoch£º185	 i:5 	 global-step:3705	 l-p:0.021751880645751953
epoch£º185	 i:6 	 global-step:3706	 l-p:0.14312222599983215
epoch£º185	 i:7 	 global-step:3707	 l-p:0.16118012368679047
epoch£º185	 i:8 	 global-step:3708	 l-p:0.11478063464164734
epoch£º185	 i:9 	 global-step:3709	 l-p:0.4355970621109009
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9150, 4.9150, 4.9150],
        [4.9150, 5.2614, 5.2675],
        [4.9150, 5.2192, 5.2047],
        [4.9150, 5.0166, 4.9660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.13095560669898987 
model_pd.l_d.mean(): -20.057323455810547 
model_pd.lagr.mean(): -19.926368713378906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4810], device='cuda:0')), ('power', tensor([-20.9305], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.13095560669898987
epoch£º186	 i:1 	 global-step:3721	 l-p:0.13619299232959747
epoch£º186	 i:2 	 global-step:3722	 l-p:0.1755809187889099
epoch£º186	 i:3 	 global-step:3723	 l-p:0.17569108307361603
epoch£º186	 i:4 	 global-step:3724	 l-p:0.10395137220621109
epoch£º186	 i:5 	 global-step:3725	 l-p:0.11252698302268982
epoch£º186	 i:6 	 global-step:3726	 l-p:0.13256214559078217
epoch£º186	 i:7 	 global-step:3727	 l-p:0.13747775554656982
epoch£º186	 i:8 	 global-step:3728	 l-p:0.11996076256036758
epoch£º186	 i:9 	 global-step:3729	 l-p:0.11548928916454315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1270, 5.1271, 5.1270],
        [5.1270, 5.3045, 5.2484],
        [5.1270, 5.4417, 5.4228],
        [5.1270, 5.1270, 5.1270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.18660464882850647 
model_pd.l_d.mean(): -19.242067337036133 
model_pd.lagr.mean(): -19.055461883544922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4505], device='cuda:0')), ('power', tensor([-20.0685], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.18660464882850647
epoch£º187	 i:1 	 global-step:3741	 l-p:0.14012131094932556
epoch£º187	 i:2 	 global-step:3742	 l-p:0.11662321537733078
epoch£º187	 i:3 	 global-step:3743	 l-p:0.13602694869041443
epoch£º187	 i:4 	 global-step:3744	 l-p:0.1394788920879364
epoch£º187	 i:5 	 global-step:3745	 l-p:0.12679699063301086
epoch£º187	 i:6 	 global-step:3746	 l-p:0.0743054449558258
epoch£º187	 i:7 	 global-step:3747	 l-p:0.10616910457611084
epoch£º187	 i:8 	 global-step:3748	 l-p:0.13249114155769348
epoch£º187	 i:9 	 global-step:3749	 l-p:0.14597462117671967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6720, 5.3556, 5.6397],
        [4.6720, 4.7308, 4.6925],
        [4.6720, 4.6720, 4.6720],
        [4.6720, 5.3445, 5.6185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.15283744037151337 
model_pd.l_d.mean(): -20.53059959411621 
model_pd.lagr.mean(): -20.377761840820312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5062], device='cuda:0')), ('power', tensor([-21.4388], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.15283744037151337
epoch£º188	 i:1 	 global-step:3761	 l-p:0.13374513387680054
epoch£º188	 i:2 	 global-step:3762	 l-p:0.13187269866466522
epoch£º188	 i:3 	 global-step:3763	 l-p:0.14359505474567413
epoch£º188	 i:4 	 global-step:3764	 l-p:0.09594901651144028
epoch£º188	 i:5 	 global-step:3765	 l-p:0.18785211443901062
epoch£º188	 i:6 	 global-step:3766	 l-p:0.16751810908317566
epoch£º188	 i:7 	 global-step:3767	 l-p:0.10882391035556793
epoch£º188	 i:8 	 global-step:3768	 l-p:0.17112398147583008
epoch£º188	 i:9 	 global-step:3769	 l-p:0.11645647138357162
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0586, 6.0570, 6.6020],
        [5.0586, 6.0732, 6.6352],
        [5.0586, 5.0587, 5.0586],
        [5.0586, 5.0586, 5.0586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.10192830115556717 
model_pd.l_d.mean(): -19.516151428222656 
model_pd.lagr.mean(): -19.414222717285156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4626], device='cuda:0')), ('power', tensor([-20.3602], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.10192830115556717
epoch£º189	 i:1 	 global-step:3781	 l-p:0.18755367398262024
epoch£º189	 i:2 	 global-step:3782	 l-p:0.11339431256055832
epoch£º189	 i:3 	 global-step:3783	 l-p:0.12508371472358704
epoch£º189	 i:4 	 global-step:3784	 l-p:0.1212652176618576
epoch£º189	 i:5 	 global-step:3785	 l-p:0.10332086682319641
epoch£º189	 i:6 	 global-step:3786	 l-p:0.1217687577009201
epoch£º189	 i:7 	 global-step:3787	 l-p:0.10959209501743317
epoch£º189	 i:8 	 global-step:3788	 l-p:0.11679238826036453
epoch£º189	 i:9 	 global-step:3789	 l-p:0.1326596587896347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0394, 5.0394, 5.0394],
        [5.0394, 5.0394, 5.0394],
        [5.0394, 5.0394, 5.0394],
        [5.0394, 5.0532, 5.0414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.13203538954257965 
model_pd.l_d.mean(): -20.05708885192871 
model_pd.lagr.mean(): -19.925052642822266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4309], device='cuda:0')), ('power', tensor([-20.8785], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.13203538954257965
epoch£º190	 i:1 	 global-step:3801	 l-p:0.13823740184307098
epoch£º190	 i:2 	 global-step:3802	 l-p:0.13227833807468414
epoch£º190	 i:3 	 global-step:3803	 l-p:0.0678800493478775
epoch£º190	 i:4 	 global-step:3804	 l-p:0.23529797792434692
epoch£º190	 i:5 	 global-step:3805	 l-p:0.13851699233055115
epoch£º190	 i:6 	 global-step:3806	 l-p:0.20072457194328308
epoch£º190	 i:7 	 global-step:3807	 l-p:0.15133582055568695
epoch£º190	 i:8 	 global-step:3808	 l-p:0.15051396191120148
epoch£º190	 i:9 	 global-step:3809	 l-p:0.1575237661600113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6857, 5.5155, 5.9404],
        [4.6857, 4.7229, 4.6948],
        [4.6857, 4.6857, 4.6857],
        [4.6857, 4.6857, 4.6857]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.16406476497650146 
model_pd.l_d.mean(): -18.69449806213379 
model_pd.lagr.mean(): -18.530433654785156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6502], device='cuda:0')), ('power', tensor([-19.7175], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.16406476497650146
epoch£º191	 i:1 	 global-step:3821	 l-p:0.19882161915302277
epoch£º191	 i:2 	 global-step:3822	 l-p:-0.00491828890517354
epoch£º191	 i:3 	 global-step:3823	 l-p:0.13489167392253876
epoch£º191	 i:4 	 global-step:3824	 l-p:0.15919998288154602
epoch£º191	 i:5 	 global-step:3825	 l-p:0.10903774201869965
epoch£º191	 i:6 	 global-step:3826	 l-p:0.12160332500934601
epoch£º191	 i:7 	 global-step:3827	 l-p:0.141847163438797
epoch£º191	 i:8 	 global-step:3828	 l-p:0.15519019961357117
epoch£º191	 i:9 	 global-step:3829	 l-p:0.1296604424715042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9348, 4.9499, 4.9370],
        [4.9348, 4.9438, 4.9358],
        [4.9348, 5.7622, 6.1510],
        [4.9348, 4.9565, 4.9388]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.19457915425300598 
model_pd.l_d.mean(): -20.043554306030273 
model_pd.lagr.mean(): -19.848974227905273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4746], device='cuda:0')), ('power', tensor([-20.9100], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.19457915425300598
epoch£º192	 i:1 	 global-step:3841	 l-p:0.14732813835144043
epoch£º192	 i:2 	 global-step:3842	 l-p:0.5153611898422241
epoch£º192	 i:3 	 global-step:3843	 l-p:0.12873004376888275
epoch£º192	 i:4 	 global-step:3844	 l-p:0.5321869254112244
epoch£º192	 i:5 	 global-step:3845	 l-p:0.13041122257709503
epoch£º192	 i:6 	 global-step:3846	 l-p:0.14437609910964966
epoch£º192	 i:7 	 global-step:3847	 l-p:0.15465298295021057
epoch£º192	 i:8 	 global-step:3848	 l-p:0.12705948948860168
epoch£º192	 i:9 	 global-step:3849	 l-p:0.13140355050563812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9437, 5.7082, 6.0357],
        [4.9437, 4.9573, 4.9456],
        [4.9437, 4.9453, 4.9438],
        [4.9437, 4.9438, 4.9437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.135921910405159 
model_pd.l_d.mean(): -20.381765365600586 
model_pd.lagr.mean(): -20.2458438873291 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4425], device='cuda:0')), ('power', tensor([-21.2212], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.135921910405159
epoch£º193	 i:1 	 global-step:3861	 l-p:0.13772831857204437
epoch£º193	 i:2 	 global-step:3862	 l-p:0.06048175320029259
epoch£º193	 i:3 	 global-step:3863	 l-p:-0.06720613688230515
epoch£º193	 i:4 	 global-step:3864	 l-p:0.13291814923286438
epoch£º193	 i:5 	 global-step:3865	 l-p:0.1468183994293213
epoch£º193	 i:6 	 global-step:3866	 l-p:0.1380959451198578
epoch£º193	 i:7 	 global-step:3867	 l-p:0.10964253544807434
epoch£º193	 i:8 	 global-step:3868	 l-p:0.1508144736289978
epoch£º193	 i:9 	 global-step:3869	 l-p:0.11369497328996658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7170, 4.7170, 4.7170],
        [4.7170, 5.1835, 5.2811],
        [4.7170, 4.8720, 4.8203],
        [4.7170, 4.7395, 4.7206]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.09995801001787186 
model_pd.l_d.mean(): -20.758270263671875 
model_pd.lagr.mean(): -20.65831184387207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4698], device='cuda:0')), ('power', tensor([-21.6330], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.09995801001787186
epoch£º194	 i:1 	 global-step:3881	 l-p:0.1436469554901123
epoch£º194	 i:2 	 global-step:3882	 l-p:-0.0046983337961137295
epoch£º194	 i:3 	 global-step:3883	 l-p:0.1332329660654068
epoch£º194	 i:4 	 global-step:3884	 l-p:0.13552813231945038
epoch£º194	 i:5 	 global-step:3885	 l-p:0.11620041728019714
epoch£º194	 i:6 	 global-step:3886	 l-p:0.12080449610948563
epoch£º194	 i:7 	 global-step:3887	 l-p:0.1400792896747589
epoch£º194	 i:8 	 global-step:3888	 l-p:0.12981104850769043
epoch£º194	 i:9 	 global-step:3889	 l-p:0.4411938786506653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1154, 5.1831, 5.1408],
        [5.1154, 5.6189, 5.7124],
        [5.1154, 5.1177, 5.1156],
        [5.1154, 5.5214, 5.5514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.11424747854471207 
model_pd.l_d.mean(): -20.142311096191406 
model_pd.lagr.mean(): -20.02806282043457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4099], device='cuda:0')), ('power', tensor([-20.9435], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.11424747854471207
epoch£º195	 i:1 	 global-step:3901	 l-p:0.11829634755849838
epoch£º195	 i:2 	 global-step:3902	 l-p:0.11932618916034698
epoch£º195	 i:3 	 global-step:3903	 l-p:0.11979842931032181
epoch£º195	 i:4 	 global-step:3904	 l-p:0.15927356481552124
epoch£º195	 i:5 	 global-step:3905	 l-p:0.13395582139492035
epoch£º195	 i:6 	 global-step:3906	 l-p:0.14193305373191833
epoch£º195	 i:7 	 global-step:3907	 l-p:0.23344947397708893
epoch£º195	 i:8 	 global-step:3908	 l-p:0.11524955183267593
epoch£º195	 i:9 	 global-step:3909	 l-p:0.12448085844516754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7868, 4.7869, 4.7868],
        [4.7868, 4.7907, 4.7870],
        [4.7868, 4.7869, 4.7868],
        [4.7868, 4.7896, 4.7870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.13850067555904388 
model_pd.l_d.mean(): -19.46961784362793 
model_pd.lagr.mean(): -19.331117630004883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5764], device='cuda:0')), ('power', tensor([-20.4307], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.13850067555904388
epoch£º196	 i:1 	 global-step:3921	 l-p:0.10520309209823608
epoch£º196	 i:2 	 global-step:3922	 l-p:0.09514027088880539
epoch£º196	 i:3 	 global-step:3923	 l-p:0.10520666837692261
epoch£º196	 i:4 	 global-step:3924	 l-p:0.1315615177154541
epoch£º196	 i:5 	 global-step:3925	 l-p:0.15588517487049103
epoch£º196	 i:6 	 global-step:3926	 l-p:0.1238616406917572
epoch£º196	 i:7 	 global-step:3927	 l-p:0.12602950632572174
epoch£º196	 i:8 	 global-step:3928	 l-p:0.13520948588848114
epoch£º196	 i:9 	 global-step:3929	 l-p:0.11927498877048492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9530, 4.9532, 4.9530],
        [4.9530, 5.3565, 5.3953],
        [4.9530, 4.9530, 4.9530],
        [4.9530, 4.9653, 4.9544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.16879768669605255 
model_pd.l_d.mean(): -18.917903900146484 
model_pd.lagr.mean(): -18.74910545349121 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5329], device='cuda:0')), ('power', tensor([-19.8236], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.16879768669605255
epoch£º197	 i:1 	 global-step:3941	 l-p:0.1844060868024826
epoch£º197	 i:2 	 global-step:3942	 l-p:0.1476554274559021
epoch£º197	 i:3 	 global-step:3943	 l-p:0.11310407519340515
epoch£º197	 i:4 	 global-step:3944	 l-p:-0.009368538856506348
epoch£º197	 i:5 	 global-step:3945	 l-p:0.1344660222530365
epoch£º197	 i:6 	 global-step:3946	 l-p:0.12944574654102325
epoch£º197	 i:7 	 global-step:3947	 l-p:0.13435377180576324
epoch£º197	 i:8 	 global-step:3948	 l-p:0.13490085303783417
epoch£º197	 i:9 	 global-step:3949	 l-p:0.12213002145290375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9477, 4.9657, 4.9504],
        [4.9477, 4.9478, 4.9477],
        [4.9477, 4.9477, 4.9477],
        [4.9477, 4.9483, 4.9477]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.1293489933013916 
model_pd.l_d.mean(): -20.370914459228516 
model_pd.lagr.mean(): -20.241565704345703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4349], device='cuda:0')), ('power', tensor([-21.2022], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.1293489933013916
epoch£º198	 i:1 	 global-step:3961	 l-p:0.140707865357399
epoch£º198	 i:2 	 global-step:3962	 l-p:0.15250736474990845
epoch£º198	 i:3 	 global-step:3963	 l-p:0.14583992958068848
epoch£º198	 i:4 	 global-step:3964	 l-p:0.11236163973808289
epoch£º198	 i:5 	 global-step:3965	 l-p:0.1288379579782486
epoch£º198	 i:6 	 global-step:3966	 l-p:0.1398562788963318
epoch£º198	 i:7 	 global-step:3967	 l-p:0.23098129034042358
epoch£º198	 i:8 	 global-step:3968	 l-p:0.13350996375083923
epoch£º198	 i:9 	 global-step:3969	 l-p:0.12272335588932037
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0250, 5.0257, 5.0251],
        [5.0250, 5.0348, 5.0261],
        [5.0250, 5.0250, 5.0250],
        [5.0250, 5.1373, 5.0831]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.15698710083961487 
model_pd.l_d.mean(): -18.896095275878906 
model_pd.lagr.mean(): -18.73910903930664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4858], device='cuda:0')), ('power', tensor([-19.7526], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.15698710083961487
epoch£º199	 i:1 	 global-step:3981	 l-p:0.11568882316350937
epoch£º199	 i:2 	 global-step:3982	 l-p:0.11794587224721909
epoch£º199	 i:3 	 global-step:3983	 l-p:0.12516115605831146
epoch£º199	 i:4 	 global-step:3984	 l-p:0.1358565092086792
epoch£º199	 i:5 	 global-step:3985	 l-p:0.1476796269416809
epoch£º199	 i:6 	 global-step:3986	 l-p:0.11206094175577164
epoch£º199	 i:7 	 global-step:3987	 l-p:0.11619697511196136
epoch£º199	 i:8 	 global-step:3988	 l-p:0.11312820017337799
epoch£º199	 i:9 	 global-step:3989	 l-p:0.16382834315299988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9870, 4.9870, 4.9870],
        [4.9870, 4.9873, 4.9870],
        [4.9870, 4.9975, 4.9881],
        [4.9870, 5.3112, 5.3046]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.149900883436203 
model_pd.l_d.mean(): -20.37551498413086 
model_pd.lagr.mean(): -20.225614547729492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4263], device='cuda:0')), ('power', tensor([-21.1980], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.149900883436203
epoch£º200	 i:1 	 global-step:4001	 l-p:0.14234428107738495
epoch£º200	 i:2 	 global-step:4002	 l-p:0.12806855142116547
epoch£º200	 i:3 	 global-step:4003	 l-p:0.4331264793872833
epoch£º200	 i:4 	 global-step:4004	 l-p:0.1365167647600174
epoch£º200	 i:5 	 global-step:4005	 l-p:-0.013344001024961472
epoch£º200	 i:6 	 global-step:4006	 l-p:0.17492391169071198
epoch£º200	 i:7 	 global-step:4007	 l-p:0.1763264387845993
epoch£º200	 i:8 	 global-step:4008	 l-p:0.12275380641222
epoch£º200	 i:9 	 global-step:4009	 l-p:0.020399579778313637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8833, 4.8833, 4.8833],
        [4.8833, 5.3382, 5.4160],
        [4.8833, 4.8833, 4.8833],
        [4.8833, 4.9672, 4.9189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): -0.9654993414878845 
model_pd.l_d.mean(): -18.408706665039062 
model_pd.lagr.mean(): -19.37420654296875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5712], device='cuda:0')), ('power', tensor([-19.3446], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:-0.9654993414878845
epoch£º201	 i:1 	 global-step:4021	 l-p:0.11127127707004547
epoch£º201	 i:2 	 global-step:4022	 l-p:0.09077794849872589
epoch£º201	 i:3 	 global-step:4023	 l-p:0.13344021141529083
epoch£º201	 i:4 	 global-step:4024	 l-p:0.1577962040901184
epoch£º201	 i:5 	 global-step:4025	 l-p:0.13730068504810333
epoch£º201	 i:6 	 global-step:4026	 l-p:0.12528963387012482
epoch£º201	 i:7 	 global-step:4027	 l-p:0.1407727599143982
epoch£º201	 i:8 	 global-step:4028	 l-p:0.12519234418869019
epoch£º201	 i:9 	 global-step:4029	 l-p:0.15375934541225433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9831, 4.9833, 4.9831],
        [4.9831, 4.9832, 4.9831],
        [4.9831, 5.0669, 5.0185],
        [4.9831, 4.9944, 4.9843]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.1377558708190918 
model_pd.l_d.mean(): -19.908266067504883 
model_pd.lagr.mean(): -19.770509719848633 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4591], device='cuda:0')), ('power', tensor([-20.7561], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.1377558708190918
epoch£º202	 i:1 	 global-step:4041	 l-p:0.15997432172298431
epoch£º202	 i:2 	 global-step:4042	 l-p:0.1322253942489624
epoch£º202	 i:3 	 global-step:4043	 l-p:0.1518082469701767
epoch£º202	 i:4 	 global-step:4044	 l-p:0.2112661451101303
epoch£º202	 i:5 	 global-step:4045	 l-p:0.1455349177122116
epoch£º202	 i:6 	 global-step:4046	 l-p:0.13165530562400818
epoch£º202	 i:7 	 global-step:4047	 l-p:0.1327333301305771
epoch£º202	 i:8 	 global-step:4048	 l-p:-0.015080356039106846
epoch£º202	 i:9 	 global-step:4049	 l-p:0.19598381221294403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8551, 5.3798, 5.5114],
        [4.8551, 4.9377, 4.8896],
        [4.8551, 4.9053, 4.8694],
        [4.8551, 4.8552, 4.8551]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.021244144067168236 
model_pd.l_d.mean(): -18.781518936157227 
model_pd.lagr.mean(): -18.76027488708496 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5364], device='cuda:0')), ('power', tensor([-19.6883], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.021244144067168236
epoch£º203	 i:1 	 global-step:4061	 l-p:0.10287759453058243
epoch£º203	 i:2 	 global-step:4062	 l-p:1.6678173542022705
epoch£º203	 i:3 	 global-step:4063	 l-p:0.1366012692451477
epoch£º203	 i:4 	 global-step:4064	 l-p:0.47031310200691223
epoch£º203	 i:5 	 global-step:4065	 l-p:0.1758785992860794
epoch£º203	 i:6 	 global-step:4066	 l-p:0.1316995471715927
epoch£º203	 i:7 	 global-step:4067	 l-p:0.13315820693969727
epoch£º203	 i:8 	 global-step:4068	 l-p:0.1250009685754776
epoch£º203	 i:9 	 global-step:4069	 l-p:0.1325351446866989
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9901, 5.5023, 5.6136],
        [4.9901, 4.9901, 4.9901],
        [4.9901, 5.4927, 5.5969],
        [4.9901, 5.0965, 5.0429]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.12091199308633804 
model_pd.l_d.mean(): -20.203079223632812 
model_pd.lagr.mean(): -20.08216667175293 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4536], device='cuda:0')), ('power', tensor([-21.0506], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.12091199308633804
epoch£º204	 i:1 	 global-step:4081	 l-p:0.14883573353290558
epoch£º204	 i:2 	 global-step:4082	 l-p:0.24169687926769257
epoch£º204	 i:3 	 global-step:4083	 l-p:0.22831977903842926
epoch£º204	 i:4 	 global-step:4084	 l-p:0.1425533890724182
epoch£º204	 i:5 	 global-step:4085	 l-p:0.10153493285179138
epoch£º204	 i:6 	 global-step:4086	 l-p:0.14099372923374176
epoch£º204	 i:7 	 global-step:4087	 l-p:0.1471787542104721
epoch£º204	 i:8 	 global-step:4088	 l-p:0.1424056440591812
epoch£º204	 i:9 	 global-step:4089	 l-p:0.133544921875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9506, 5.0149, 4.9728],
        [4.9506, 4.9506, 4.9506],
        [4.9506, 4.9507, 4.9506],
        [4.9506, 4.9506, 4.9506]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.19398686289787292 
model_pd.l_d.mean(): -19.536449432373047 
model_pd.lagr.mean(): -19.34246253967285 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5061], device='cuda:0')), ('power', tensor([-20.4260], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.19398686289787292
epoch£º205	 i:1 	 global-step:4101	 l-p:0.1368970274925232
epoch£º205	 i:2 	 global-step:4102	 l-p:0.1255238950252533
epoch£º205	 i:3 	 global-step:4103	 l-p:0.1599498689174652
epoch£º205	 i:4 	 global-step:4104	 l-p:0.15701459348201752
epoch£º205	 i:5 	 global-step:4105	 l-p:0.07770705223083496
epoch£º205	 i:6 	 global-step:4106	 l-p:0.14299900829792023
epoch£º205	 i:7 	 global-step:4107	 l-p:0.17094816267490387
epoch£º205	 i:8 	 global-step:4108	 l-p:0.11196785420179367
epoch£º205	 i:9 	 global-step:4109	 l-p:0.12261684238910675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0432, 5.0810, 5.0521],
        [5.0432, 5.9892, 6.4857],
        [5.0432, 5.6082, 5.7556],
        [5.0432, 5.0432, 5.0432]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.13776086270809174 
model_pd.l_d.mean(): -20.06077766418457 
model_pd.lagr.mean(): -19.923017501831055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4598], device='cuda:0')), ('power', tensor([-20.9121], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.13776086270809174
epoch£º206	 i:1 	 global-step:4121	 l-p:0.08731824904680252
epoch£º206	 i:2 	 global-step:4122	 l-p:0.12079597264528275
epoch£º206	 i:3 	 global-step:4123	 l-p:0.1482069194316864
epoch£º206	 i:4 	 global-step:4124	 l-p:0.11399832367897034
epoch£º206	 i:5 	 global-step:4125	 l-p:0.1601574718952179
epoch£º206	 i:6 	 global-step:4126	 l-p:0.13503126800060272
epoch£º206	 i:7 	 global-step:4127	 l-p:0.11255929619073868
epoch£º206	 i:8 	 global-step:4128	 l-p:0.16605117917060852
epoch£º206	 i:9 	 global-step:4129	 l-p:0.12842965126037598
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9811, 5.0013, 4.9840],
        [4.9811, 4.9811, 4.9811],
        [4.9811, 5.2620, 5.2365],
        [4.9811, 5.1484, 5.0928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.1315881460905075 
model_pd.l_d.mean(): -19.68427848815918 
model_pd.lagr.mean(): -19.552690505981445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4570], device='cuda:0')), ('power', tensor([-20.5256], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.1315881460905075
epoch£º207	 i:1 	 global-step:4141	 l-p:0.12575583159923553
epoch£º207	 i:2 	 global-step:4142	 l-p:0.203689306974411
epoch£º207	 i:3 	 global-step:4143	 l-p:0.16162416338920593
epoch£º207	 i:4 	 global-step:4144	 l-p:0.16553397476673126
epoch£º207	 i:5 	 global-step:4145	 l-p:0.13596545159816742
epoch£º207	 i:6 	 global-step:4146	 l-p:0.12281845510005951
epoch£º207	 i:7 	 global-step:4147	 l-p:0.524108350276947
epoch£º207	 i:8 	 global-step:4148	 l-p:0.12709753215312958
epoch£º207	 i:9 	 global-step:4149	 l-p:0.1344974935054779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9005, 4.9005, 4.9005],
        [4.9005, 5.3822, 5.4785],
        [4.9005, 5.0267, 4.9710],
        [4.9005, 4.9005, 4.9005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.13222132623195648 
model_pd.l_d.mean(): -20.70064353942871 
model_pd.lagr.mean(): -20.568422317504883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4169], device='cuda:0')), ('power', tensor([-21.5195], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:0.13222132623195648
epoch£º208	 i:1 	 global-step:4161	 l-p:-0.35648027062416077
epoch£º208	 i:2 	 global-step:4162	 l-p:0.12474947422742844
epoch£º208	 i:3 	 global-step:4163	 l-p:0.12846757471561432
epoch£º208	 i:4 	 global-step:4164	 l-p:0.13547082245349884
epoch£º208	 i:5 	 global-step:4165	 l-p:0.14204929769039154
epoch£º208	 i:6 	 global-step:4166	 l-p:0.1836460828781128
epoch£º208	 i:7 	 global-step:4167	 l-p:0.09077133238315582
epoch£º208	 i:8 	 global-step:4168	 l-p:0.15095274150371552
epoch£º208	 i:9 	 global-step:4169	 l-p:0.1493612825870514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228]], device='cuda:0')
 pt:tensor([[4.8919, 5.2278, 5.2321],
        [4.8919, 5.8172, 6.3138],
        [4.8919, 4.9407, 4.9050],
        [4.8919, 5.1403, 5.1054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): -1.0015698671340942 
model_pd.l_d.mean(): -20.07986068725586 
model_pd.lagr.mean(): -21.081430435180664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5037], device='cuda:0')), ('power', tensor([-20.9771], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:-1.0015698671340942
epoch£º209	 i:1 	 global-step:4181	 l-p:0.13578417897224426
epoch£º209	 i:2 	 global-step:4182	 l-p:0.1278972029685974
epoch£º209	 i:3 	 global-step:4183	 l-p:0.13942968845367432
epoch£º209	 i:4 	 global-step:4184	 l-p:0.14195102453231812
epoch£º209	 i:5 	 global-step:4185	 l-p:0.09925948828458786
epoch£º209	 i:6 	 global-step:4186	 l-p:0.13355472683906555
epoch£º209	 i:7 	 global-step:4187	 l-p:0.18920084834098816
epoch£º209	 i:8 	 global-step:4188	 l-p:0.15185068547725677
epoch£º209	 i:9 	 global-step:4189	 l-p:0.12344680726528168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0191, 5.0191, 5.0191],
        [5.0191, 5.0191, 5.0191],
        [5.0191, 5.1716, 5.1145],
        [5.0191, 5.0191, 5.0191]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.16602537035942078 
model_pd.l_d.mean(): -20.458444595336914 
model_pd.lagr.mean(): -20.29241943359375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4148], device='cuda:0')), ('power', tensor([-21.2706], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.16602537035942078
epoch£º210	 i:1 	 global-step:4201	 l-p:0.13764214515686035
epoch£º210	 i:2 	 global-step:4202	 l-p:0.14752401411533356
epoch£º210	 i:3 	 global-step:4203	 l-p:0.12527574598789215
epoch£º210	 i:4 	 global-step:4204	 l-p:0.1359919160604477
epoch£º210	 i:5 	 global-step:4205	 l-p:0.12317115813493729
epoch£º210	 i:6 	 global-step:4206	 l-p:0.11877831816673279
epoch£º210	 i:7 	 global-step:4207	 l-p:0.1241176575422287
epoch£º210	 i:8 	 global-step:4208	 l-p:-0.011692485772073269
epoch£º210	 i:9 	 global-step:4209	 l-p:0.12791408598423004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0468, 5.4434, 5.4736],
        [5.0468, 5.0469, 5.0468],
        [5.0468, 5.1837, 5.1262],
        [5.0468, 5.0469, 5.0468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.1409481018781662 
model_pd.l_d.mean(): -19.41339111328125 
model_pd.lagr.mean(): -19.272443771362305 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4525], device='cuda:0')), ('power', tensor([-20.2451], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.1409481018781662
epoch£º211	 i:1 	 global-step:4221	 l-p:0.12847711145877838
epoch£º211	 i:2 	 global-step:4222	 l-p:0.15929014980793
epoch£º211	 i:3 	 global-step:4223	 l-p:0.18130986392498016
epoch£º211	 i:4 	 global-step:4224	 l-p:0.12600992619991302
epoch£º211	 i:5 	 global-step:4225	 l-p:0.1647753119468689
epoch£º211	 i:6 	 global-step:4226	 l-p:0.11535781621932983
epoch£º211	 i:7 	 global-step:4227	 l-p:0.1193205788731575
epoch£º211	 i:8 	 global-step:4228	 l-p:0.12533296644687653
epoch£º211	 i:9 	 global-step:4229	 l-p:0.13402272760868073
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9596, 5.4233, 5.5028],
        [4.9596, 4.9597, 4.9596],
        [4.9596, 4.9596, 4.9596],
        [4.9596, 5.3931, 5.4520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.1386972963809967 
model_pd.l_d.mean(): -20.784170150756836 
model_pd.lagr.mean(): -20.64547348022461 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3910], device='cuda:0')), ('power', tensor([-21.5778], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.1386972963809967
epoch£º212	 i:1 	 global-step:4241	 l-p:0.11900753527879715
epoch£º212	 i:2 	 global-step:4242	 l-p:0.14550498127937317
epoch£º212	 i:3 	 global-step:4243	 l-p:0.12124317139387131
epoch£º212	 i:4 	 global-step:4244	 l-p:0.3409577012062073
epoch£º212	 i:5 	 global-step:4245	 l-p:0.1519308090209961
epoch£º212	 i:6 	 global-step:4246	 l-p:0.12060786783695221
epoch£º212	 i:7 	 global-step:4247	 l-p:0.204930379986763
epoch£º212	 i:8 	 global-step:4248	 l-p:-0.2369784116744995
epoch£º212	 i:9 	 global-step:4249	 l-p:0.10969015210866928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7807, 4.7810, 4.7807],
        [4.7807, 4.7807, 4.7807],
        [4.7807, 4.9228, 4.8675],
        [4.7807, 4.7867, 4.7805]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.12226790934801102 
model_pd.l_d.mean(): -19.372295379638672 
model_pd.lagr.mean(): -19.25002670288086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5782], device='cuda:0')), ('power', tensor([-20.3334], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.12226790934801102
epoch£º213	 i:1 	 global-step:4261	 l-p:0.13224664330482483
epoch£º213	 i:2 	 global-step:4262	 l-p:0.13859139382839203
epoch£º213	 i:3 	 global-step:4263	 l-p:0.13350948691368103
epoch£º213	 i:4 	 global-step:4264	 l-p:0.0005538653931580484
epoch£º213	 i:5 	 global-step:4265	 l-p:0.14267517626285553
epoch£º213	 i:6 	 global-step:4266	 l-p:0.1333789974451065
epoch£º213	 i:7 	 global-step:4267	 l-p:0.11877773702144623
epoch£º213	 i:8 	 global-step:4268	 l-p:0.34355440735816956
epoch£º213	 i:9 	 global-step:4269	 l-p:0.07507199048995972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8467, 4.8467, 4.8467],
        [4.8467, 4.8712, 4.8496],
        [4.8467, 4.9866, 4.9305],
        [4.8467, 4.8467, 4.8467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.13872084021568298 
model_pd.l_d.mean(): -20.4435977935791 
model_pd.lagr.mean(): -20.30487632751465 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4627], device='cuda:0')), ('power', tensor([-21.3051], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.13872084021568298
epoch£º214	 i:1 	 global-step:4281	 l-p:0.12212901562452316
epoch£º214	 i:2 	 global-step:4282	 l-p:0.08178960531949997
epoch£º214	 i:3 	 global-step:4283	 l-p:0.17714080214500427
epoch£º214	 i:4 	 global-step:4284	 l-p:0.1359238177537918
epoch£º214	 i:5 	 global-step:4285	 l-p:0.2292921394109726
epoch£º214	 i:6 	 global-step:4286	 l-p:0.16399472951889038
epoch£º214	 i:7 	 global-step:4287	 l-p:0.25507065653800964
epoch£º214	 i:8 	 global-step:4288	 l-p:0.1405000537633896
epoch£º214	 i:9 	 global-step:4289	 l-p:0.16677667200565338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0259, 5.1828, 5.1255],
        [5.0259, 5.0261, 5.0259],
        [5.0259, 5.0373, 5.0268],
        [5.0259, 5.0259, 5.0259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.1518053412437439 
model_pd.l_d.mean(): -20.35440444946289 
model_pd.lagr.mean(): -20.202598571777344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4285], device='cuda:0')), ('power', tensor([-21.1789], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.1518053412437439
epoch£º215	 i:1 	 global-step:4301	 l-p:0.12910565733909607
epoch£º215	 i:2 	 global-step:4302	 l-p:0.14362280070781708
epoch£º215	 i:3 	 global-step:4303	 l-p:0.11831315606832504
epoch£º215	 i:4 	 global-step:4304	 l-p:-0.17843876779079437
epoch£º215	 i:5 	 global-step:4305	 l-p:0.12627266347408295
epoch£º215	 i:6 	 global-step:4306	 l-p:0.1432904750108719
epoch£º215	 i:7 	 global-step:4307	 l-p:0.12001532316207886
epoch£º215	 i:8 	 global-step:4308	 l-p:0.1303953230381012
epoch£º215	 i:9 	 global-step:4309	 l-p:0.11262558400630951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0548, 5.0555, 5.0549],
        [5.0548, 5.3978, 5.3980],
        [5.0548, 5.0698, 5.0564],
        [5.0548, 5.0548, 5.0548]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.11976512521505356 
model_pd.l_d.mean(): -18.969911575317383 
model_pd.lagr.mean(): -18.850147247314453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4678], device='cuda:0')), ('power', tensor([-19.8092], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.11976512521505356
epoch£º216	 i:1 	 global-step:4321	 l-p:0.11983329802751541
epoch£º216	 i:2 	 global-step:4322	 l-p:0.167430117726326
epoch£º216	 i:3 	 global-step:4323	 l-p:0.15326951444149017
epoch£º216	 i:4 	 global-step:4324	 l-p:0.16329017281532288
epoch£º216	 i:5 	 global-step:4325	 l-p:0.2090384066104889
epoch£º216	 i:6 	 global-step:4326	 l-p:0.09203238040208817
epoch£º216	 i:7 	 global-step:4327	 l-p:0.12789231538772583
epoch£º216	 i:8 	 global-step:4328	 l-p:0.12441422790288925
epoch£º216	 i:9 	 global-step:4329	 l-p:0.16318859159946442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9173, 4.9245, 4.9174],
        [4.9173, 5.3450, 5.4034],
        [4.9173, 5.0954, 5.0416],
        [4.9173, 4.9173, 4.9173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.13661199808120728 
model_pd.l_d.mean(): -20.476776123046875 
model_pd.lagr.mean(): -20.340164184570312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4318], device='cuda:0')), ('power', tensor([-21.3069], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.13661199808120728
epoch£º217	 i:1 	 global-step:4341	 l-p:0.1105036810040474
epoch£º217	 i:2 	 global-step:4342	 l-p:-0.18365615606307983
epoch£º217	 i:3 	 global-step:4343	 l-p:0.19058507680892944
epoch£º217	 i:4 	 global-step:4344	 l-p:0.1832370162010193
epoch£º217	 i:5 	 global-step:4345	 l-p:0.13209185004234314
epoch£º217	 i:6 	 global-step:4346	 l-p:0.1350746750831604
epoch£º217	 i:7 	 global-step:4347	 l-p:0.4341675341129303
epoch£º217	 i:8 	 global-step:4348	 l-p:0.21927589178085327
epoch£º217	 i:9 	 global-step:4349	 l-p:0.13794632256031036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9782, 4.9804, 4.9782],
        [4.9782, 4.9782, 4.9782],
        [4.9782, 4.9782, 4.9782],
        [4.9782, 4.9782, 4.9782]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.12182299047708511 
model_pd.l_d.mean(): -20.492816925048828 
model_pd.lagr.mean(): -20.370994567871094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4170], device='cuda:0')), ('power', tensor([-21.3079], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.12182299047708511
epoch£º218	 i:1 	 global-step:4361	 l-p:0.13193626701831818
epoch£º218	 i:2 	 global-step:4362	 l-p:0.13290056586265564
epoch£º218	 i:3 	 global-step:4363	 l-p:0.16811344027519226
epoch£º218	 i:4 	 global-step:4364	 l-p:0.12555429339408875
epoch£º218	 i:5 	 global-step:4365	 l-p:0.23147627711296082
epoch£º218	 i:6 	 global-step:4366	 l-p:0.9141172766685486
epoch£º218	 i:7 	 global-step:4367	 l-p:0.12666302919387817
epoch£º218	 i:8 	 global-step:4368	 l-p:0.23224163055419922
epoch£º218	 i:9 	 global-step:4369	 l-p:0.09689048677682877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9816, 4.9853, 4.9817],
        [4.9816, 4.9905, 4.9820],
        [4.9816, 5.1322, 5.0748],
        [4.9816, 5.9866, 6.5544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.12192782759666443 
model_pd.l_d.mean(): -18.708419799804688 
model_pd.lagr.mean(): -18.58649253845215 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5135], device='cuda:0')), ('power', tensor([-19.5901], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.12192782759666443
epoch£º219	 i:1 	 global-step:4381	 l-p:0.08202649652957916
epoch£º219	 i:2 	 global-step:4382	 l-p:0.14535406231880188
epoch£º219	 i:3 	 global-step:4383	 l-p:0.11946150660514832
epoch£º219	 i:4 	 global-step:4384	 l-p:0.17930442094802856
epoch£º219	 i:5 	 global-step:4385	 l-p:0.11975225061178207
epoch£º219	 i:6 	 global-step:4386	 l-p:0.15497896075248718
epoch£º219	 i:7 	 global-step:4387	 l-p:0.13275665044784546
epoch£º219	 i:8 	 global-step:4388	 l-p:0.12108819931745529
epoch£º219	 i:9 	 global-step:4389	 l-p:0.13391855359077454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0007, 5.0008, 5.0007],
        [5.0007, 5.0585, 5.0179],
        [5.0007, 5.0007, 5.0007],
        [5.0007, 5.0109, 5.0012]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.16247476637363434 
model_pd.l_d.mean(): -20.20012664794922 
model_pd.lagr.mean(): -20.03765106201172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4368], device='cuda:0')), ('power', tensor([-21.0303], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.16247476637363434
epoch£º220	 i:1 	 global-step:4401	 l-p:0.1306866705417633
epoch£º220	 i:2 	 global-step:4402	 l-p:0.10809756815433502
epoch£º220	 i:3 	 global-step:4403	 l-p:0.1376744508743286
epoch£º220	 i:4 	 global-step:4404	 l-p:0.14568109810352325
epoch£º220	 i:5 	 global-step:4405	 l-p:-6.337597370147705
epoch£º220	 i:6 	 global-step:4406	 l-p:0.47547435760498047
epoch£º220	 i:7 	 global-step:4407	 l-p:0.1353539228439331
epoch£º220	 i:8 	 global-step:4408	 l-p:0.14577224850654602
epoch£º220	 i:9 	 global-step:4409	 l-p:0.1529320627450943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8793, 5.3159, 5.3830],
        [4.8793, 5.1484, 5.1218],
        [4.8793, 4.9792, 4.9252],
        [4.8793, 4.8793, 4.8793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.14664731919765472 
model_pd.l_d.mean(): -20.013370513916016 
model_pd.lagr.mean(): -19.866724014282227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4912], device='cuda:0')), ('power', tensor([-20.8964], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.14664731919765472
epoch£º221	 i:1 	 global-step:4421	 l-p:0.015482439659535885
epoch£º221	 i:2 	 global-step:4422	 l-p:0.13350540399551392
epoch£º221	 i:3 	 global-step:4423	 l-p:0.1463233083486557
epoch£º221	 i:4 	 global-step:4424	 l-p:0.04205890744924545
epoch£º221	 i:5 	 global-step:4425	 l-p:2.4794552326202393
epoch£º221	 i:6 	 global-step:4426	 l-p:0.12656307220458984
epoch£º221	 i:7 	 global-step:4427	 l-p:0.13820375502109528
epoch£º221	 i:8 	 global-step:4428	 l-p:0.1802394837141037
epoch£º221	 i:9 	 global-step:4429	 l-p:0.13509772717952728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228]], device='cuda:0')
 pt:tensor([[4.8695, 5.8419, 6.3922],
        [4.8695, 5.3769, 5.4961],
        [4.8695, 5.7985, 6.3034],
        [4.8695, 5.3348, 5.4229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.1212625503540039 
model_pd.l_d.mean(): -19.447132110595703 
model_pd.lagr.mean(): -19.325870513916016 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5658], device='cuda:0')), ('power', tensor([-20.3968], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:0.1212625503540039
epoch£º222	 i:1 	 global-step:4441	 l-p:0.12235479056835175
epoch£º222	 i:2 	 global-step:4442	 l-p:0.15247473120689392
epoch£º222	 i:3 	 global-step:4443	 l-p:0.1546577662229538
epoch£º222	 i:4 	 global-step:4444	 l-p:0.1327861249446869
epoch£º222	 i:5 	 global-step:4445	 l-p:0.1385764330625534
epoch£º222	 i:6 	 global-step:4446	 l-p:0.3598559498786926
epoch£º222	 i:7 	 global-step:4447	 l-p:0.11278968304395676
epoch£º222	 i:8 	 global-step:4448	 l-p:0.08983520418405533
epoch£º222	 i:9 	 global-step:4449	 l-p:-0.030570486560463905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7686, 4.7686, 4.7686],
        [4.7686, 4.8641, 4.8104],
        [4.7686, 4.9429, 4.8904],
        [4.7686, 4.7748, 4.7678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): -0.06867055594921112 
model_pd.l_d.mean(): -20.595172882080078 
model_pd.lagr.mean(): -20.663843154907227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4811], device='cuda:0')), ('power', tensor([-21.4786], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:-0.06867055594921112
epoch£º223	 i:1 	 global-step:4461	 l-p:0.14684642851352692
epoch£º223	 i:2 	 global-step:4462	 l-p:0.13786408305168152
epoch£º223	 i:3 	 global-step:4463	 l-p:0.12719470262527466
epoch£º223	 i:4 	 global-step:4464	 l-p:0.12468947470188141
epoch£º223	 i:5 	 global-step:4465	 l-p:0.27735766768455505
epoch£º223	 i:6 	 global-step:4466	 l-p:-0.1309351623058319
epoch£º223	 i:7 	 global-step:4467	 l-p:0.7219337224960327
epoch£º223	 i:8 	 global-step:4468	 l-p:0.13721291720867157
epoch£º223	 i:9 	 global-step:4469	 l-p:0.18058109283447266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0230, 5.0230, 5.0230],
        [5.0230, 5.0230, 5.0230],
        [5.0230, 5.0509, 5.0270],
        [5.0230, 5.0235, 5.0230]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.12277062237262726 
model_pd.l_d.mean(): -20.41596031188965 
model_pd.lagr.mean(): -20.293190002441406 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4029], device='cuda:0')), ('power', tensor([-21.2151], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.12277062237262726
epoch£º224	 i:1 	 global-step:4481	 l-p:0.12418311834335327
epoch£º224	 i:2 	 global-step:4482	 l-p:0.13500678539276123
epoch£º224	 i:3 	 global-step:4483	 l-p:0.14859777688980103
epoch£º224	 i:4 	 global-step:4484	 l-p:0.1424669772386551
epoch£º224	 i:5 	 global-step:4485	 l-p:0.07543064653873444
epoch£º224	 i:6 	 global-step:4486	 l-p:0.15399430692195892
epoch£º224	 i:7 	 global-step:4487	 l-p:0.1522327959537506
epoch£º224	 i:8 	 global-step:4488	 l-p:0.10779208689928055
epoch£º224	 i:9 	 global-step:4489	 l-p:0.12078994512557983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1076, 5.1076, 5.1076],
        [5.1076, 5.1076, 5.1077],
        [5.1076, 5.1077, 5.1076],
        [5.1076, 5.2423, 5.1833]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.11575576663017273 
model_pd.l_d.mean(): -19.580219268798828 
model_pd.lagr.mean(): -19.46446418762207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4345], device='cuda:0')), ('power', tensor([-20.3963], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.11575576663017273
epoch£º225	 i:1 	 global-step:4501	 l-p:-0.11343518644571304
epoch£º225	 i:2 	 global-step:4502	 l-p:0.12227877229452133
epoch£º225	 i:3 	 global-step:4503	 l-p:0.1656164824962616
epoch£º225	 i:4 	 global-step:4504	 l-p:0.1191563755273819
epoch£º225	 i:5 	 global-step:4505	 l-p:0.16888993978500366
epoch£º225	 i:6 	 global-step:4506	 l-p:0.11876393109560013
epoch£º225	 i:7 	 global-step:4507	 l-p:0.1257798969745636
epoch£º225	 i:8 	 global-step:4508	 l-p:0.1690584123134613
epoch£º225	 i:9 	 global-step:4509	 l-p:0.1285160779953003
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9584, 5.1114, 5.0537],
        [4.9584, 5.4793, 5.6018],
        [4.9584, 5.0192, 4.9763],
        [4.9584, 4.9584, 4.9584]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.1258934587240219 
model_pd.l_d.mean(): -20.438114166259766 
model_pd.lagr.mean(): -20.31222152709961 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4337], device='cuda:0')), ('power', tensor([-21.2695], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.1258934587240219
epoch£º226	 i:1 	 global-step:4521	 l-p:0.269795686006546
epoch£º226	 i:2 	 global-step:4522	 l-p:0.09752652049064636
epoch£º226	 i:3 	 global-step:4523	 l-p:-10.877138137817383
epoch£º226	 i:4 	 global-step:4524	 l-p:0.15135188400745392
epoch£º226	 i:5 	 global-step:4525	 l-p:0.19725137948989868
epoch£º226	 i:6 	 global-step:4526	 l-p:0.12662440538406372
epoch£º226	 i:7 	 global-step:4527	 l-p:0.11506223678588867
epoch£º226	 i:8 	 global-step:4528	 l-p:0.14475318789482117
epoch£º226	 i:9 	 global-step:4529	 l-p:0.24280986189842224
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9684, 5.0780, 5.0216],
        [4.9684, 4.9691, 4.9684],
        [4.9684, 4.9880, 4.9697],
        [4.9684, 4.9684, 4.9684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.14997100830078125 
model_pd.l_d.mean(): -19.8452205657959 
model_pd.lagr.mean(): -19.695249557495117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4615], device='cuda:0')), ('power', tensor([-20.6943], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.14997100830078125
epoch£º227	 i:1 	 global-step:4541	 l-p:0.12103486061096191
epoch£º227	 i:2 	 global-step:4542	 l-p:0.1311475783586502
epoch£º227	 i:3 	 global-step:4543	 l-p:0.17050401866436005
epoch£º227	 i:4 	 global-step:4544	 l-p:0.3304779827594757
epoch£º227	 i:5 	 global-step:4545	 l-p:0.2389073520898819
epoch£º227	 i:6 	 global-step:4546	 l-p:0.12781690061092377
epoch£º227	 i:7 	 global-step:4547	 l-p:0.12480185180902481
epoch£º227	 i:8 	 global-step:4548	 l-p:0.1275434046983719
epoch£º227	 i:9 	 global-step:4549	 l-p:0.17714592814445496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0147, 5.0147, 5.0147],
        [5.0147, 5.0147, 5.0147],
        [5.0147, 5.0147, 5.0147],
        [5.0147, 5.0424, 5.0182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.1322604864835739 
model_pd.l_d.mean(): -20.363231658935547 
model_pd.lagr.mean(): -20.23097038269043 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4296], device='cuda:0')), ('power', tensor([-21.1889], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.1322604864835739
epoch£º228	 i:1 	 global-step:4561	 l-p:0.12922494113445282
epoch£º228	 i:2 	 global-step:4562	 l-p:0.13870923221111298
epoch£º228	 i:3 	 global-step:4563	 l-p:0.1296795904636383
epoch£º228	 i:4 	 global-step:4564	 l-p:0.09451521933078766
epoch£º228	 i:5 	 global-step:4565	 l-p:0.1943158358335495
epoch£º228	 i:6 	 global-step:4566	 l-p:0.13404473662376404
epoch£º228	 i:7 	 global-step:4567	 l-p:0.14173412322998047
epoch£º228	 i:8 	 global-step:4568	 l-p:0.17163263261318207
epoch£º228	 i:9 	 global-step:4569	 l-p:0.17456656694412231
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9871, 4.9871, 4.9871],
        [4.9871, 5.5401, 5.6853],
        [4.9871, 4.9989, 4.9872],
        [4.9871, 4.9944, 4.9869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.13648316264152527 
model_pd.l_d.mean(): -18.4968204498291 
model_pd.lagr.mean(): -18.36033821105957 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5421], device='cuda:0')), ('power', tensor([-19.4042], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.13648316264152527
epoch£º229	 i:1 	 global-step:4581	 l-p:0.1392616182565689
epoch£º229	 i:2 	 global-step:4582	 l-p:0.15683580935001373
epoch£º229	 i:3 	 global-step:4583	 l-p:0.16954493522644043
epoch£º229	 i:4 	 global-step:4584	 l-p:0.11708437651395798
epoch£º229	 i:5 	 global-step:4585	 l-p:0.13284890353679657
epoch£º229	 i:6 	 global-step:4586	 l-p:-0.1780916154384613
epoch£º229	 i:7 	 global-step:4587	 l-p:0.12126760929822922
epoch£º229	 i:8 	 global-step:4588	 l-p:0.11190208792686462
epoch£º229	 i:9 	 global-step:4589	 l-p:0.12726938724517822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1466,  0.0773,  1.0000,  0.0408,
          1.0000,  0.5273, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2503,  1.0000,  0.1770,
          1.0000,  0.7073, 31.6228]], device='cuda:0')
 pt:tensor([[5.0990, 5.1191, 5.1009],
        [5.0990, 6.0789, 6.6050],
        [5.0990, 5.1274, 5.1030],
        [5.0990, 5.3012, 5.2474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.10717368125915527 
model_pd.l_d.mean(): -18.889888763427734 
model_pd.lagr.mean(): -18.78271484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4891], device='cuda:0')), ('power', tensor([-19.7497], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.10717368125915527
epoch£º230	 i:1 	 global-step:4601	 l-p:0.13160592317581177
epoch£º230	 i:2 	 global-step:4602	 l-p:0.12163163721561432
epoch£º230	 i:3 	 global-step:4603	 l-p:0.1526997685432434
epoch£º230	 i:4 	 global-step:4604	 l-p:0.16783244907855988
epoch£º230	 i:5 	 global-step:4605	 l-p:0.13026796281337738
epoch£º230	 i:6 	 global-step:4606	 l-p:0.23137348890304565
epoch£º230	 i:7 	 global-step:4607	 l-p:0.15215764939785004
epoch£º230	 i:8 	 global-step:4608	 l-p:0.14098161458969116
epoch£º230	 i:9 	 global-step:4609	 l-p:-0.0761246532201767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8842, 4.8842, 4.8842],
        [4.8842, 4.8841, 4.8841],
        [4.8842, 5.7575, 6.2046],
        [4.8842, 4.9289, 4.8925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.12919707596302032 
model_pd.l_d.mean(): -19.981151580810547 
model_pd.lagr.mean(): -19.85195541381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5091], device='cuda:0')), ('power', tensor([-20.8821], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.12919707596302032
epoch£º231	 i:1 	 global-step:4621	 l-p:0.14285904169082642
epoch£º231	 i:2 	 global-step:4622	 l-p:0.1256943941116333
epoch£º231	 i:3 	 global-step:4623	 l-p:0.13455656170845032
epoch£º231	 i:4 	 global-step:4624	 l-p:0.13842938840389252
epoch£º231	 i:5 	 global-step:4625	 l-p:0.3079688251018524
epoch£º231	 i:6 	 global-step:4626	 l-p:0.13673806190490723
epoch£º231	 i:7 	 global-step:4627	 l-p:0.07030197232961655
epoch£º231	 i:8 	 global-step:4628	 l-p:0.04993479698896408
epoch£º231	 i:9 	 global-step:4629	 l-p:0.16953763365745544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7808, 4.7807, 4.7808],
        [4.7808, 4.8008, 4.7800],
        [4.7808, 5.1412, 5.1661],
        [4.7808, 4.7810, 4.7805]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.12502799928188324 
model_pd.l_d.mean(): -20.516740798950195 
model_pd.lagr.mean(): -20.391712188720703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4872], device='cuda:0')), ('power', tensor([-21.4050], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.12502799928188324
epoch£º232	 i:1 	 global-step:4641	 l-p:0.18829770386219025
epoch£º232	 i:2 	 global-step:4642	 l-p:0.05244201049208641
epoch£º232	 i:3 	 global-step:4643	 l-p:0.1401890516281128
epoch£º232	 i:4 	 global-step:4644	 l-p:0.13818876445293427
epoch£º232	 i:5 	 global-step:4645	 l-p:0.13287092745304108
epoch£º232	 i:6 	 global-step:4646	 l-p:0.1463051736354828
epoch£º232	 i:7 	 global-step:4647	 l-p:0.12307921051979065
epoch£º232	 i:8 	 global-step:4648	 l-p:0.11851533502340317
epoch£º232	 i:9 	 global-step:4649	 l-p:0.12075615674257278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0691, 5.1096, 5.0769],
        [5.0691, 5.0861, 5.0700],
        [5.0691, 5.3699, 5.3496],
        [5.0691, 5.8996, 6.2809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.06831656396389008 
model_pd.l_d.mean(): -19.83868408203125 
model_pd.lagr.mean(): -19.770366668701172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4739], device='cuda:0')), ('power', tensor([-20.7004], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.06831656396389008
epoch£º233	 i:1 	 global-step:4661	 l-p:0.13592351973056793
epoch£º233	 i:2 	 global-step:4662	 l-p:0.10976757109165192
epoch£º233	 i:3 	 global-step:4663	 l-p:0.13210679590702057
epoch£º233	 i:4 	 global-step:4664	 l-p:0.1362837255001068
epoch£º233	 i:5 	 global-step:4665	 l-p:0.1531524360179901
epoch£º233	 i:6 	 global-step:4666	 l-p:0.14581002295017242
epoch£º233	 i:7 	 global-step:4667	 l-p:0.1349681317806244
epoch£º233	 i:8 	 global-step:4668	 l-p:0.12039996683597565
epoch£º233	 i:9 	 global-step:4669	 l-p:0.12992143630981445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9686, 4.9701, 4.9684],
        [4.9686, 4.9755, 4.9681],
        [4.9686, 4.9803, 4.9682],
        [4.9686, 4.9686, 4.9686]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.12340205907821655 
model_pd.l_d.mean(): -19.866798400878906 
model_pd.lagr.mean(): -19.743396759033203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4943], device='cuda:0')), ('power', tensor([-20.7502], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.12340205907821655
epoch£º234	 i:1 	 global-step:4681	 l-p:3.143864154815674
epoch£º234	 i:2 	 global-step:4682	 l-p:0.12512442469596863
epoch£º234	 i:3 	 global-step:4683	 l-p:0.1297541707754135
epoch£º234	 i:4 	 global-step:4684	 l-p:0.21136736869812012
epoch£º234	 i:5 	 global-step:4685	 l-p:0.05556195229291916
epoch£º234	 i:6 	 global-step:4686	 l-p:0.13102269172668457
epoch£º234	 i:7 	 global-step:4687	 l-p:0.1465424746274948
epoch£º234	 i:8 	 global-step:4688	 l-p:0.14146821200847626
epoch£º234	 i:9 	 global-step:4689	 l-p:0.0928119421005249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7780, 5.4058, 5.6349],
        [4.7780, 4.7780, 4.7780],
        [4.7780, 4.7785, 4.7774],
        [4.7780, 4.7780, 4.7780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.13864165544509888 
model_pd.l_d.mean(): -20.38974952697754 
model_pd.lagr.mean(): -20.251108169555664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4840], device='cuda:0')), ('power', tensor([-21.2724], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.13864165544509888
epoch£º235	 i:1 	 global-step:4701	 l-p:-0.055349454283714294
epoch£º235	 i:2 	 global-step:4702	 l-p:0.13554701209068298
epoch£º235	 i:3 	 global-step:4703	 l-p:0.06365153193473816
epoch£º235	 i:4 	 global-step:4704	 l-p:0.16508503258228302
epoch£º235	 i:5 	 global-step:4705	 l-p:0.18509946763515472
epoch£º235	 i:6 	 global-step:4706	 l-p:0.1310737580060959
epoch£º235	 i:7 	 global-step:4707	 l-p:0.14072421193122864
epoch£º235	 i:8 	 global-step:4708	 l-p:0.12388040125370026
epoch£º235	 i:9 	 global-step:4709	 l-p:0.1225254163146019
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9012, 5.1039, 5.0539],
        [4.9012, 4.9011, 4.9011],
        [4.9012, 5.0309, 4.9719],
        [4.9012, 4.9211, 4.9010]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.13083235919475555 
model_pd.l_d.mean(): -20.332727432250977 
model_pd.lagr.mean(): -20.201894760131836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4619], device='cuda:0')), ('power', tensor([-21.1913], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.13083235919475555
epoch£º236	 i:1 	 global-step:4721	 l-p:0.13487769663333893
epoch£º236	 i:2 	 global-step:4722	 l-p:0.13491125404834747
epoch£º236	 i:3 	 global-step:4723	 l-p:0.13838954269886017
epoch£º236	 i:4 	 global-step:4724	 l-p:0.3495209515094757
epoch£º236	 i:5 	 global-step:4725	 l-p:0.1118578091263771
epoch£º236	 i:6 	 global-step:4726	 l-p:0.14896489679813385
epoch£º236	 i:7 	 global-step:4727	 l-p:0.11129796504974365
epoch£º236	 i:8 	 global-step:4728	 l-p:0.15168753266334534
epoch£º236	 i:9 	 global-step:4729	 l-p:0.12658299505710602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8330, 5.5596, 5.8717],
        [4.8330, 4.9070, 4.8564],
        [4.8330, 4.8330, 4.8330],
        [4.8330, 4.9552, 4.8965]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.04777902364730835 
model_pd.l_d.mean(): -19.427013397216797 
model_pd.lagr.mean(): -19.379234313964844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5405], device='cuda:0')), ('power', tensor([-20.3501], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.04777902364730835
epoch£º237	 i:1 	 global-step:4741	 l-p:0.13201260566711426
epoch£º237	 i:2 	 global-step:4742	 l-p:-0.09443119913339615
epoch£º237	 i:3 	 global-step:4743	 l-p:0.14609093964099884
epoch£º237	 i:4 	 global-step:4744	 l-p:0.1270064264535904
epoch£º237	 i:5 	 global-step:4745	 l-p:0.18383581936359406
epoch£º237	 i:6 	 global-step:4746	 l-p:0.1233091726899147
epoch£º237	 i:7 	 global-step:4747	 l-p:0.14339153468608856
epoch£º237	 i:8 	 global-step:4748	 l-p:0.1304531991481781
epoch£º237	 i:9 	 global-step:4749	 l-p:0.11947286128997803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9950, 5.8815, 6.3275],
        [4.9950, 5.0418, 5.0041],
        [4.9950, 5.5012, 5.6109],
        [4.9950, 4.9950, 4.9950]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.1304837316274643 
model_pd.l_d.mean(): -17.157419204711914 
model_pd.lagr.mean(): -17.026935577392578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6305], device='cuda:0')), ('power', tensor([-18.1313], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.1304837316274643
epoch£º238	 i:1 	 global-step:4761	 l-p:0.18830204010009766
epoch£º238	 i:2 	 global-step:4762	 l-p:0.12629811465740204
epoch£º238	 i:3 	 global-step:4763	 l-p:0.1323828101158142
epoch£º238	 i:4 	 global-step:4764	 l-p:0.15574197471141815
epoch£º238	 i:5 	 global-step:4765	 l-p:0.1919531226158142
epoch£º238	 i:6 	 global-step:4766	 l-p:0.09112657606601715
epoch£º238	 i:7 	 global-step:4767	 l-p:0.1764581948518753
epoch£º238	 i:8 	 global-step:4768	 l-p:0.12379828095436096
epoch£º238	 i:9 	 global-step:4769	 l-p:0.12608329951763153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[4.9829, 5.4745, 5.5744],
        [4.9829, 5.7775, 6.1355],
        [4.9829, 5.0664, 5.0136],
        [4.9829, 5.3361, 5.3461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.15071716904640198 
model_pd.l_d.mean(): -20.18355941772461 
model_pd.lagr.mean(): -20.0328426361084 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4660], device='cuda:0')), ('power', tensor([-21.0437], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.15071716904640198
epoch£º239	 i:1 	 global-step:4781	 l-p:0.10841367393732071
epoch£º239	 i:2 	 global-step:4782	 l-p:0.12734416127204895
epoch£º239	 i:3 	 global-step:4783	 l-p:0.13462531566619873
epoch£º239	 i:4 	 global-step:4784	 l-p:1.2822211980819702
epoch£º239	 i:5 	 global-step:4785	 l-p:0.13059067726135254
epoch£º239	 i:6 	 global-step:4786	 l-p:0.12045158445835114
epoch£º239	 i:7 	 global-step:4787	 l-p:0.15586760640144348
epoch£º239	 i:8 	 global-step:4788	 l-p:0.10754011571407318
epoch£º239	 i:9 	 global-step:4789	 l-p:0.09895936399698257
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7921, 5.1077, 5.1066],
        [4.7921, 4.8220, 4.7922],
        [4.7921, 4.8506, 4.8048],
        [4.7921, 4.7919, 4.7921]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): -0.9136518836021423 
model_pd.l_d.mean(): -20.546100616455078 
model_pd.lagr.mean(): -21.459753036499023 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4647], device='cuda:0')), ('power', tensor([-21.4116], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:-0.9136518836021423
epoch£º240	 i:1 	 global-step:4801	 l-p:0.15910901129245758
epoch£º240	 i:2 	 global-step:4802	 l-p:0.1390843689441681
epoch£º240	 i:3 	 global-step:4803	 l-p:0.05397215113043785
epoch£º240	 i:4 	 global-step:4804	 l-p:0.12907211482524872
epoch£º240	 i:5 	 global-step:4805	 l-p:0.125472292304039
epoch£º240	 i:6 	 global-step:4806	 l-p:0.14421655237674713
epoch£º240	 i:7 	 global-step:4807	 l-p:0.1512557566165924
epoch£º240	 i:8 	 global-step:4808	 l-p:0.1973538100719452
epoch£º240	 i:9 	 global-step:4809	 l-p:0.008631753735244274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9266, 4.9733, 4.9345],
        [4.9266, 4.9266, 4.9266],
        [4.9266, 5.4231, 5.5310],
        [4.9266, 5.6748, 5.9961]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.3756121098995209 
model_pd.l_d.mean(): -20.28714942932129 
model_pd.lagr.mean(): -19.911537170410156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4601], device='cuda:0')), ('power', tensor([-21.1430], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.3756121098995209
epoch£º241	 i:1 	 global-step:4821	 l-p:0.39904701709747314
epoch£º241	 i:2 	 global-step:4822	 l-p:0.12032550573348999
epoch£º241	 i:3 	 global-step:4823	 l-p:0.15624794363975525
epoch£º241	 i:4 	 global-step:4824	 l-p:0.13435295224189758
epoch£º241	 i:5 	 global-step:4825	 l-p:0.1279127150774002
epoch£º241	 i:6 	 global-step:4826	 l-p:0.11859073489904404
epoch£º241	 i:7 	 global-step:4827	 l-p:0.11963212490081787
epoch£º241	 i:8 	 global-step:4828	 l-p:0.13469333946704865
epoch£º241	 i:9 	 global-step:4829	 l-p:0.0792682021856308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0484, 5.3387, 5.3146],
        [5.0484, 5.0639, 5.0482],
        [5.0484, 5.0484, 5.0484],
        [5.0484, 5.7434, 6.0029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.14164382219314575 
model_pd.l_d.mean(): -20.119447708129883 
model_pd.lagr.mean(): -19.97780418395996 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4415], device='cuda:0')), ('power', tensor([-20.9530], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.14164382219314575
epoch£º242	 i:1 	 global-step:4841	 l-p:0.12539692223072052
epoch£º242	 i:2 	 global-step:4842	 l-p:0.16282877326011658
epoch£º242	 i:3 	 global-step:4843	 l-p:0.12737245857715607
epoch£º242	 i:4 	 global-step:4844	 l-p:0.15166255831718445
epoch£º242	 i:5 	 global-step:4845	 l-p:0.13436172902584076
epoch£º242	 i:6 	 global-step:4846	 l-p:0.09824660420417786
epoch£º242	 i:7 	 global-step:4847	 l-p:0.13467727601528168
epoch£º242	 i:8 	 global-step:4848	 l-p:0.12513205409049988
epoch£º242	 i:9 	 global-step:4849	 l-p:0.3323570191860199
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9603, 4.9830, 4.9602],
        [4.9603, 4.9602, 4.9603],
        [4.9603, 4.9603, 4.9603],
        [4.9603, 5.5687, 5.7626]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.15633288025856018 
model_pd.l_d.mean(): -19.906442642211914 
model_pd.lagr.mean(): -19.750110626220703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4571], device='cuda:0')), ('power', tensor([-20.7521], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.15633288025856018
epoch£º243	 i:1 	 global-step:4861	 l-p:0.4862186312675476
epoch£º243	 i:2 	 global-step:4862	 l-p:0.1300586611032486
epoch£º243	 i:3 	 global-step:4863	 l-p:0.14039871096611023
epoch£º243	 i:4 	 global-step:4864	 l-p:0.12132573872804642
epoch£º243	 i:5 	 global-step:4865	 l-p:0.11900615692138672
epoch£º243	 i:6 	 global-step:4866	 l-p:0.20017272233963013
epoch£º243	 i:7 	 global-step:4867	 l-p:-0.06920282542705536
epoch£º243	 i:8 	 global-step:4868	 l-p:0.1312282383441925
epoch£º243	 i:9 	 global-step:4869	 l-p:0.1314067542552948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1771,  0.0994,  1.0000,  0.0558,
          1.0000,  0.5615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[4.8841, 4.8948, 4.8819],
        [4.8841, 5.1590, 5.1342],
        [4.8841, 4.9131, 4.8844],
        [4.8841, 5.4200, 5.5619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.052475616335868835 
model_pd.l_d.mean(): -20.33234405517578 
model_pd.lagr.mean(): -20.279869079589844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4760], device='cuda:0')), ('power', tensor([-21.2056], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.052475616335868835
epoch£º244	 i:1 	 global-step:4881	 l-p:0.13903558254241943
epoch£º244	 i:2 	 global-step:4882	 l-p:0.11366221308708191
epoch£º244	 i:3 	 global-step:4883	 l-p:0.12877357006072998
epoch£º244	 i:4 	 global-step:4884	 l-p:0.1246817335486412
epoch£º244	 i:5 	 global-step:4885	 l-p:-0.15329016745090485
epoch£º244	 i:6 	 global-step:4886	 l-p:0.13442392647266388
epoch£º244	 i:7 	 global-step:4887	 l-p:0.21510940790176392
epoch£º244	 i:8 	 global-step:4888	 l-p:0.07746226340532303
epoch£º244	 i:9 	 global-step:4889	 l-p:0.15610381960868835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8797, 4.8797, 4.8797],
        [4.8797, 5.0194, 4.9593],
        [4.8797, 5.1603, 5.1382],
        [4.8797, 4.8797, 4.8797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.05861934646964073 
model_pd.l_d.mean(): -19.844511032104492 
model_pd.lagr.mean(): -19.785892486572266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5247], device='cuda:0')), ('power', tensor([-20.7590], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.05861934646964073
epoch£º245	 i:1 	 global-step:4901	 l-p:0.19817641377449036
epoch£º245	 i:2 	 global-step:4902	 l-p:1.4244781732559204
epoch£º245	 i:3 	 global-step:4903	 l-p:0.14640212059020996
epoch£º245	 i:4 	 global-step:4904	 l-p:0.11519422382116318
epoch£º245	 i:5 	 global-step:4905	 l-p:0.1313541978597641
epoch£º245	 i:6 	 global-step:4906	 l-p:0.07715974748134613
epoch£º245	 i:7 	 global-step:4907	 l-p:0.1661115288734436
epoch£º245	 i:8 	 global-step:4908	 l-p:0.12388253957033157
epoch£º245	 i:9 	 global-step:4909	 l-p:0.12311319261789322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1391, 5.6229, 5.7064],
        [5.1391, 5.1391, 5.1391],
        [5.1391, 5.1887, 5.1495],
        [5.1391, 5.1391, 5.1391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.12024638056755066 
model_pd.l_d.mean(): -19.303958892822266 
model_pd.lagr.mean(): -19.183712005615234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4700], device='cuda:0')), ('power', tensor([-20.1518], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.12024638056755066
epoch£º246	 i:1 	 global-step:4921	 l-p:0.12976126372814178
epoch£º246	 i:2 	 global-step:4922	 l-p:0.12727247178554535
epoch£º246	 i:3 	 global-step:4923	 l-p:0.13703405857086182
epoch£º246	 i:4 	 global-step:4924	 l-p:0.13115954399108887
epoch£º246	 i:5 	 global-step:4925	 l-p:0.12983648478984833
epoch£º246	 i:6 	 global-step:4926	 l-p:0.04100450500845909
epoch£º246	 i:7 	 global-step:4927	 l-p:0.1397753357887268
epoch£º246	 i:8 	 global-step:4928	 l-p:0.156073197722435
epoch£º246	 i:9 	 global-step:4929	 l-p:0.12098313122987747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0535, 5.0534, 5.0535],
        [5.0535, 5.0602, 5.0524],
        [5.0535, 5.1396, 5.0850],
        [5.0535, 5.0534, 5.0534]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.07674083858728409 
model_pd.l_d.mean(): -19.796863555908203 
model_pd.lagr.mean(): -19.720123291015625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4406], device='cuda:0')), ('power', tensor([-20.6234], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.07674083858728409
epoch£º247	 i:1 	 global-step:4941	 l-p:0.1346123218536377
epoch£º247	 i:2 	 global-step:4942	 l-p:0.16774769127368927
epoch£º247	 i:3 	 global-step:4943	 l-p:0.1382330060005188
epoch£º247	 i:4 	 global-step:4944	 l-p:0.14068195223808289
epoch£º247	 i:5 	 global-step:4945	 l-p:0.1861397922039032
epoch£º247	 i:6 	 global-step:4946	 l-p:0.12790687382221222
epoch£º247	 i:7 	 global-step:4947	 l-p:0.1291295737028122
epoch£º247	 i:8 	 global-step:4948	 l-p:0.11909322440624237
epoch£º247	 i:9 	 global-step:4949	 l-p:0.11439887434244156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0656, 5.0656, 5.0656],
        [5.0656, 5.0656, 5.0656],
        [5.0656, 5.0718, 5.0646],
        [5.0656, 5.0656, 5.0656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.09719060361385345 
model_pd.l_d.mean(): -20.446945190429688 
model_pd.lagr.mean(): -20.349754333496094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4085], device='cuda:0')), ('power', tensor([-21.2524], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.09719060361385345
epoch£º248	 i:1 	 global-step:4961	 l-p:0.13810525834560394
epoch£º248	 i:2 	 global-step:4962	 l-p:0.12294593453407288
epoch£º248	 i:3 	 global-step:4963	 l-p:0.16896595060825348
epoch£º248	 i:4 	 global-step:4964	 l-p:0.12654510140419006
epoch£º248	 i:5 	 global-step:4965	 l-p:0.1356525719165802
epoch£º248	 i:6 	 global-step:4966	 l-p:0.13203759491443634
epoch£º248	 i:7 	 global-step:4967	 l-p:0.1412394940853119
epoch£º248	 i:8 	 global-step:4968	 l-p:0.12164250016212463
epoch£º248	 i:9 	 global-step:4969	 l-p:-0.5186927318572998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2392,  0.1485,  1.0000,  0.0922,
          1.0000,  0.6208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228]], device='cuda:0')
 pt:tensor([[4.9106, 5.3008, 5.3371],
        [4.9106, 5.3492, 5.4173],
        [4.9106, 4.9738, 4.9253],
        [4.9106, 5.6441, 5.9544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): -0.019548578187823296 
model_pd.l_d.mean(): -20.512964248657227 
model_pd.lagr.mean(): -20.532512664794922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4488], device='cuda:0')), ('power', tensor([-21.3613], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:-0.019548578187823296
epoch£º249	 i:1 	 global-step:4981	 l-p:0.15193095803260803
epoch£º249	 i:2 	 global-step:4982	 l-p:0.12276966124773026
epoch£º249	 i:3 	 global-step:4983	 l-p:0.18113599717617035
epoch£º249	 i:4 	 global-step:4984	 l-p:0.21188820898532867
epoch£º249	 i:5 	 global-step:4985	 l-p:0.13339807093143463
epoch£º249	 i:6 	 global-step:4986	 l-p:0.10743793100118637
epoch£º249	 i:7 	 global-step:4987	 l-p:0.12464210391044617
epoch£º249	 i:8 	 global-step:4988	 l-p:0.15062718093395233
epoch£º249	 i:9 	 global-step:4989	 l-p:0.6588129997253418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9441, 5.1162, 5.0578],
        [4.9441, 4.9441, 4.9441],
        [4.9441, 4.9529, 4.9417],
        [4.9441, 4.9441, 4.9441]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.16078875958919525 
model_pd.l_d.mean(): -19.36861801147461 
model_pd.lagr.mean(): -19.207828521728516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5163], device='cuda:0')), ('power', tensor([-20.2656], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.16078875958919525
epoch£º250	 i:1 	 global-step:5001	 l-p:0.16916589438915253
epoch£º250	 i:2 	 global-step:5002	 l-p:0.12778079509735107
epoch£º250	 i:3 	 global-step:5003	 l-p:0.1399690955877304
epoch£º250	 i:4 	 global-step:5004	 l-p:0.12805986404418945
epoch£º250	 i:5 	 global-step:5005	 l-p:0.12631715834140778
epoch£º250	 i:6 	 global-step:5006	 l-p:0.1433933526277542
epoch£º250	 i:7 	 global-step:5007	 l-p:0.12415851652622223
epoch£º250	 i:8 	 global-step:5008	 l-p:0.1625635176897049
epoch£º250	 i:9 	 global-step:5009	 l-p:0.0873926505446434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0583, 5.2116, 5.1499],
        [5.0583, 5.0582, 5.0583],
        [5.0583, 5.1328, 5.0810],
        [5.0583, 5.0583, 5.0583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.18354786932468414 
model_pd.l_d.mean(): -19.99768829345703 
model_pd.lagr.mean(): -19.81414031982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4560], device='cuda:0')), ('power', tensor([-20.8439], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.18354786932468414
epoch£º251	 i:1 	 global-step:5021	 l-p:0.13440774381160736
epoch£º251	 i:2 	 global-step:5022	 l-p:0.12275492399930954
epoch£º251	 i:3 	 global-step:5023	 l-p:0.11940170079469681
epoch£º251	 i:4 	 global-step:5024	 l-p:0.14187532663345337
epoch£º251	 i:5 	 global-step:5025	 l-p:0.1156817376613617
epoch£º251	 i:6 	 global-step:5026	 l-p:0.07701857388019562
epoch£º251	 i:7 	 global-step:5027	 l-p:0.12564729154109955
epoch£º251	 i:8 	 global-step:5028	 l-p:0.12699109315872192
epoch£º251	 i:9 	 global-step:5029	 l-p:0.1303187906742096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0265, 5.0264, 5.0265],
        [5.0265, 5.9340, 6.3980],
        [5.0265, 5.0973, 5.0463],
        [5.0265, 5.1850, 5.1239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.12805093824863434 
model_pd.l_d.mean(): -19.935758590698242 
model_pd.lagr.mean(): -19.807706832885742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4861], device='cuda:0')), ('power', tensor([-20.8120], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.12805093824863434
epoch£º252	 i:1 	 global-step:5041	 l-p:0.21437567472457886
epoch£º252	 i:2 	 global-step:5042	 l-p:0.1270633190870285
epoch£º252	 i:3 	 global-step:5043	 l-p:0.10367550700902939
epoch£º252	 i:4 	 global-step:5044	 l-p:0.17792601883411407
epoch£º252	 i:5 	 global-step:5045	 l-p:0.12621867656707764
epoch£º252	 i:6 	 global-step:5046	 l-p:-0.5913622975349426
epoch£º252	 i:7 	 global-step:5047	 l-p:0.1340906172990799
epoch£º252	 i:8 	 global-step:5048	 l-p:0.12990641593933105
epoch£º252	 i:9 	 global-step:5049	 l-p:0.07406110316514969
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228]], device='cuda:0')
 pt:tensor([[4.8457, 5.6747, 6.0847],
        [4.8457, 4.9118, 4.8605],
        [4.8457, 4.9037, 4.8557],
        [4.8457, 5.2340, 5.2731]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.13332663476467133 
model_pd.l_d.mean(): -20.43732452392578 
model_pd.lagr.mean(): -20.303997039794922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4710], device='cuda:0')), ('power', tensor([-21.3074], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.13332663476467133
epoch£º253	 i:1 	 global-step:5061	 l-p:0.20587100088596344
epoch£º253	 i:2 	 global-step:5062	 l-p:0.109428271651268
epoch£º253	 i:3 	 global-step:5063	 l-p:0.137241393327713
epoch£º253	 i:4 	 global-step:5064	 l-p:0.06381094455718994
epoch£º253	 i:5 	 global-step:5065	 l-p:0.14052270352840424
epoch£º253	 i:6 	 global-step:5066	 l-p:0.13074451684951782
epoch£º253	 i:7 	 global-step:5067	 l-p:0.21951311826705933
epoch£º253	 i:8 	 global-step:5068	 l-p:0.1417054980993271
epoch£º253	 i:9 	 global-step:5069	 l-p:0.12430603057146072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0299, 5.0380, 5.0279],
        [5.0299, 5.0682, 5.0330],
        [5.0299, 5.0299, 5.0299],
        [5.0299, 5.0299, 5.0299]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.15237943828105927 
model_pd.l_d.mean(): -20.03833770751953 
model_pd.lagr.mean(): -19.885957717895508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4539], device='cuda:0')), ('power', tensor([-20.8832], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.15237943828105927
epoch£º254	 i:1 	 global-step:5081	 l-p:0.09436162561178207
epoch£º254	 i:2 	 global-step:5082	 l-p:0.14092136919498444
epoch£º254	 i:3 	 global-step:5083	 l-p:0.13026757538318634
epoch£º254	 i:4 	 global-step:5084	 l-p:0.10339852422475815
epoch£º254	 i:5 	 global-step:5085	 l-p:0.11587411910295486
epoch£º254	 i:6 	 global-step:5086	 l-p:0.11366122961044312
epoch£º254	 i:7 	 global-step:5087	 l-p:0.14507466554641724
epoch£º254	 i:8 	 global-step:5088	 l-p:0.1480766087770462
epoch£º254	 i:9 	 global-step:5089	 l-p:0.1479644775390625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9847,  0.9796,  1.0000,  0.9746,
          1.0000,  0.9949, 31.6228]], device='cuda:0')
 pt:tensor([[5.0736, 5.9772, 6.4313],
        [5.0736, 5.2042, 5.1416],
        [5.0736, 5.5145, 5.5737],
        [5.0736, 6.1334, 6.7482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.15257899463176727 
model_pd.l_d.mean(): -18.685977935791016 
model_pd.lagr.mean(): -18.53339958190918 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5105], device='cuda:0')), ('power', tensor([-19.5642], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.15257899463176727
epoch£º255	 i:1 	 global-step:5101	 l-p:0.11521374434232712
epoch£º255	 i:2 	 global-step:5102	 l-p:0.09048984944820404
epoch£º255	 i:3 	 global-step:5103	 l-p:0.12309007346630096
epoch£º255	 i:4 	 global-step:5104	 l-p:0.18265612423419952
epoch£º255	 i:5 	 global-step:5105	 l-p:0.12698140740394592
epoch£º255	 i:6 	 global-step:5106	 l-p:0.11899783462285995
epoch£º255	 i:7 	 global-step:5107	 l-p:0.3055981695652008
epoch£º255	 i:8 	 global-step:5108	 l-p:0.20945599675178528
epoch£º255	 i:9 	 global-step:5109	 l-p:0.14586137235164642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9239, 4.9478, 4.9210],
        [4.9239, 4.9236, 4.9230],
        [4.9239, 4.9239, 4.9239],
        [4.9239, 5.2714, 5.2814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.1471710354089737 
model_pd.l_d.mean(): -20.47719383239746 
model_pd.lagr.mean(): -20.33002281188965 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4442], device='cuda:0')), ('power', tensor([-21.3201], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.1471710354089737
epoch£º256	 i:1 	 global-step:5121	 l-p:-0.1163884699344635
epoch£º256	 i:2 	 global-step:5122	 l-p:0.13579049706459045
epoch£º256	 i:3 	 global-step:5123	 l-p:0.13519372045993805
epoch£º256	 i:4 	 global-step:5124	 l-p:0.11776560544967651
epoch£º256	 i:5 	 global-step:5125	 l-p:0.13520696759223938
epoch£º256	 i:6 	 global-step:5126	 l-p:0.14165909588336945
epoch£º256	 i:7 	 global-step:5127	 l-p:0.3777635395526886
epoch£º256	 i:8 	 global-step:5128	 l-p:0.12349111586809158
epoch£º256	 i:9 	 global-step:5129	 l-p:-0.6627379655838013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8773, 4.8773, 4.8773],
        [4.8773, 4.8773, 4.8773],
        [4.8773, 5.0419, 4.9823],
        [4.8773, 4.8786, 4.8749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.1246998980641365 
model_pd.l_d.mean(): -19.457435607910156 
model_pd.lagr.mean(): -19.332735061645508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4968], device='cuda:0')), ('power', tensor([-20.3358], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.1246998980641365
epoch£º257	 i:1 	 global-step:5141	 l-p:0.14080654084682465
epoch£º257	 i:2 	 global-step:5142	 l-p:0.12464053928852081
epoch£º257	 i:3 	 global-step:5143	 l-p:0.06611447036266327
epoch£º257	 i:4 	 global-step:5144	 l-p:0.16921691596508026
epoch£º257	 i:5 	 global-step:5145	 l-p:0.12570710480213165
epoch£º257	 i:6 	 global-step:5146	 l-p:0.1290862113237381
epoch£º257	 i:7 	 global-step:5147	 l-p:0.16356605291366577
epoch£º257	 i:8 	 global-step:5148	 l-p:0.2559705376625061
epoch£º257	 i:9 	 global-step:5149	 l-p:0.1314341127872467
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0327, 5.0325, 5.0327],
        [5.0327, 5.0603, 5.0318],
        [5.0327, 5.0327, 5.0327],
        [5.0327, 5.4049, 5.4236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.12395907938480377 
model_pd.l_d.mean(): -19.930845260620117 
model_pd.lagr.mean(): -19.806886672973633 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4606], device='cuda:0')), ('power', tensor([-20.7806], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.12395907938480377
epoch£º258	 i:1 	 global-step:5161	 l-p:0.12685053050518036
epoch£º258	 i:2 	 global-step:5162	 l-p:0.19509460031986237
epoch£º258	 i:3 	 global-step:5163	 l-p:0.11557569354772568
epoch£º258	 i:4 	 global-step:5164	 l-p:0.12056779861450195
epoch£º258	 i:5 	 global-step:5165	 l-p:0.17956188321113586
epoch£º258	 i:6 	 global-step:5166	 l-p:0.19138336181640625
epoch£º258	 i:7 	 global-step:5167	 l-p:0.12343385070562363
epoch£º258	 i:8 	 global-step:5168	 l-p:0.1194930449128151
epoch£º258	 i:9 	 global-step:5169	 l-p:0.14034174382686615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0223, 5.3910, 5.4082],
        [5.0223, 5.0223, 5.0223],
        [5.0223, 5.0222, 5.0223],
        [5.0223, 5.0867, 5.0368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.11502673476934433 
model_pd.l_d.mean(): -18.827856063842773 
model_pd.lagr.mean(): -18.71282958984375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4956], device='cuda:0')), ('power', tensor([-19.6933], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:0.11502673476934433
epoch£º259	 i:1 	 global-step:5181	 l-p:0.09859514236450195
epoch£º259	 i:2 	 global-step:5182	 l-p:0.15009137988090515
epoch£º259	 i:3 	 global-step:5183	 l-p:0.1433582305908203
epoch£º259	 i:4 	 global-step:5184	 l-p:0.2458173930644989
epoch£º259	 i:5 	 global-step:5185	 l-p:0.2655360698699951
epoch£º259	 i:6 	 global-step:5186	 l-p:0.1185016855597496
epoch£º259	 i:7 	 global-step:5187	 l-p:0.12348458915948868
epoch£º259	 i:8 	 global-step:5188	 l-p:0.25512951612472534
epoch£º259	 i:9 	 global-step:5189	 l-p:0.14506274461746216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9453, 5.4709, 5.6007],
        [4.9453, 5.2039, 5.1694],
        [4.9453, 4.9453, 4.9453],
        [4.9453, 5.0156, 4.9623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.15394176542758942 
model_pd.l_d.mean(): -19.979021072387695 
model_pd.lagr.mean(): -19.8250789642334 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4994], device='cuda:0')), ('power', tensor([-20.8698], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.15394176542758942
epoch£º260	 i:1 	 global-step:5201	 l-p:0.47952142357826233
epoch£º260	 i:2 	 global-step:5202	 l-p:0.1696988046169281
epoch£º260	 i:3 	 global-step:5203	 l-p:0.10825148969888687
epoch£º260	 i:4 	 global-step:5204	 l-p:-0.3647119700908661
epoch£º260	 i:5 	 global-step:5205	 l-p:0.13639427721500397
epoch£º260	 i:6 	 global-step:5206	 l-p:0.13439930975437164
epoch£º260	 i:7 	 global-step:5207	 l-p:0.18027503788471222
epoch£º260	 i:8 	 global-step:5208	 l-p:0.02468832954764366
epoch£º260	 i:9 	 global-step:5209	 l-p:0.13418330252170563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9263, 4.9263, 4.9263],
        [4.9263, 5.2330, 5.2209],
        [4.9263, 5.2739, 5.2838],
        [4.9263, 4.9287, 4.9234]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): -0.6394806504249573 
model_pd.l_d.mean(): -19.924501419067383 
model_pd.lagr.mean(): -20.563982009887695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4677], device='cuda:0')), ('power', tensor([-20.7815], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:-0.6394806504249573
epoch£º261	 i:1 	 global-step:5221	 l-p:0.11193760484457016
epoch£º261	 i:2 	 global-step:5222	 l-p:-0.03531140089035034
epoch£º261	 i:3 	 global-step:5223	 l-p:0.1380598098039627
epoch£º261	 i:4 	 global-step:5224	 l-p:0.4236427843570709
epoch£º261	 i:5 	 global-step:5225	 l-p:0.14585702121257782
epoch£º261	 i:6 	 global-step:5226	 l-p:0.1275557577610016
epoch£º261	 i:7 	 global-step:5227	 l-p:0.14458060264587402
epoch£º261	 i:8 	 global-step:5228	 l-p:0.154999777674675
epoch£º261	 i:9 	 global-step:5229	 l-p:0.14005602896213531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0217, 5.0213, 5.0211],
        [5.0217, 5.0296, 5.0185],
        [5.0217, 5.0212, 5.0216],
        [5.0217, 5.0213, 5.0216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.12536455690860748 
model_pd.l_d.mean(): -19.394817352294922 
model_pd.lagr.mean(): -19.269453048706055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4841], device='cuda:0')), ('power', tensor([-20.2589], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.12536455690860748
epoch£º262	 i:1 	 global-step:5241	 l-p:0.13050052523612976
epoch£º262	 i:2 	 global-step:5242	 l-p:0.14052210748195648
epoch£º262	 i:3 	 global-step:5243	 l-p:0.3778339922428131
epoch£º262	 i:4 	 global-step:5244	 l-p:-0.22420845925807953
epoch£º262	 i:5 	 global-step:5245	 l-p:0.15360446274280548
epoch£º262	 i:6 	 global-step:5246	 l-p:0.14976046979427338
epoch£º262	 i:7 	 global-step:5247	 l-p:0.132289320230484
epoch£º262	 i:8 	 global-step:5248	 l-p:0.19960792362689972
epoch£º262	 i:9 	 global-step:5249	 l-p:0.12779244780540466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0461, 5.5222, 5.6079],
        [5.0461, 5.3343, 5.3087],
        [5.0461, 5.0478, 5.0445],
        [5.0461, 5.4648, 5.5114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.1264086663722992 
model_pd.l_d.mean(): -19.186458587646484 
model_pd.lagr.mean(): -19.060049057006836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4787], device='cuda:0')), ('power', tensor([-20.0411], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.1264086663722992
epoch£º263	 i:1 	 global-step:5261	 l-p:0.17710551619529724
epoch£º263	 i:2 	 global-step:5262	 l-p:0.1238715723156929
epoch£º263	 i:3 	 global-step:5263	 l-p:0.12991935014724731
epoch£º263	 i:4 	 global-step:5264	 l-p:0.1444152444601059
epoch£º263	 i:5 	 global-step:5265	 l-p:0.1302071362733841
epoch£º263	 i:6 	 global-step:5266	 l-p:0.14641137421131134
epoch£º263	 i:7 	 global-step:5267	 l-p:0.1292240470647812
epoch£º263	 i:8 	 global-step:5268	 l-p:0.06735055893659592
epoch£º263	 i:9 	 global-step:5269	 l-p:0.12374218553304672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0908, 5.0905, 5.0905],
        [5.0908, 5.5934, 5.6949],
        [5.0908, 5.8578, 6.1795],
        [5.0908, 5.4342, 5.4338]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.14783675968647003 
model_pd.l_d.mean(): -20.330286026000977 
model_pd.lagr.mean(): -20.182449340820312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4159], device='cuda:0')), ('power', tensor([-21.1412], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.14783675968647003
epoch£º264	 i:1 	 global-step:5281	 l-p:0.12939824163913727
epoch£º264	 i:2 	 global-step:5282	 l-p:0.10625190287828445
epoch£º264	 i:3 	 global-step:5283	 l-p:0.12226833403110504
epoch£º264	 i:4 	 global-step:5284	 l-p:0.14627180993556976
epoch£º264	 i:5 	 global-step:5285	 l-p:0.14629924297332764
epoch£º264	 i:6 	 global-step:5286	 l-p:0.2886917293071747
epoch£º264	 i:7 	 global-step:5287	 l-p:0.5649314522743225
epoch£º264	 i:8 	 global-step:5288	 l-p:0.15530671179294586
epoch£º264	 i:9 	 global-step:5289	 l-p:0.12135005742311478
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0120, 5.8376, 6.2238],
        [5.0120, 5.1570, 5.0931],
        [5.0120, 5.0120, 5.0120],
        [5.0120, 5.0120, 5.0120]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.11401596665382385 
model_pd.l_d.mean(): -19.64740753173828 
model_pd.lagr.mean(): -19.53339195251465 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5045], device='cuda:0')), ('power', tensor([-20.5373], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.11401596665382385
epoch£º265	 i:1 	 global-step:5301	 l-p:0.14533279836177826
epoch£º265	 i:2 	 global-step:5302	 l-p:0.13455531001091003
epoch£º265	 i:3 	 global-step:5303	 l-p:0.22035156190395355
epoch£º265	 i:4 	 global-step:5304	 l-p:0.09048739075660706
epoch£º265	 i:5 	 global-step:5305	 l-p:0.1561814248561859
epoch£º265	 i:6 	 global-step:5306	 l-p:0.1295701116323471
epoch£º265	 i:7 	 global-step:5307	 l-p:0.1319742202758789
epoch£º265	 i:8 	 global-step:5308	 l-p:0.14498181641101837
epoch£º265	 i:9 	 global-step:5309	 l-p:0.12083201855421066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0619, 5.0614, 5.0617],
        [5.0619, 5.0617, 5.0618],
        [5.0619, 5.0626, 5.0604],
        [5.0619, 5.0618, 5.0619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.1236046776175499 
model_pd.l_d.mean(): -20.188905715942383 
model_pd.lagr.mean(): -20.0653018951416 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4371], device='cuda:0')), ('power', tensor([-21.0191], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.1236046776175499
epoch£º266	 i:1 	 global-step:5321	 l-p:0.13010691106319427
epoch£º266	 i:2 	 global-step:5322	 l-p:0.12524515390396118
epoch£º266	 i:3 	 global-step:5323	 l-p:0.2913995385169983
epoch£º266	 i:4 	 global-step:5324	 l-p:0.1479979157447815
epoch£º266	 i:5 	 global-step:5325	 l-p:0.35381120443344116
epoch£º266	 i:6 	 global-step:5326	 l-p:0.13896901905536652
epoch£º266	 i:7 	 global-step:5327	 l-p:0.18825550377368927
epoch£º266	 i:8 	 global-step:5328	 l-p:0.12947969138622284
epoch£º266	 i:9 	 global-step:5329	 l-p:0.13281096518039703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0523, 5.0523, 5.0523],
        [5.0523, 5.1388, 5.0800],
        [5.0523, 5.0523, 5.0523],
        [5.0523, 5.0519, 5.0514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.12930259108543396 
model_pd.l_d.mean(): -19.80105209350586 
model_pd.lagr.mean(): -19.671749114990234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4823], device='cuda:0')), ('power', tensor([-20.6709], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.12930259108543396
epoch£º267	 i:1 	 global-step:5341	 l-p:0.1445162147283554
epoch£º267	 i:2 	 global-step:5342	 l-p:0.11061027646064758
epoch£º267	 i:3 	 global-step:5343	 l-p:0.1337405890226364
epoch£º267	 i:4 	 global-step:5344	 l-p:0.11930474638938904
epoch£º267	 i:5 	 global-step:5345	 l-p:0.18211054801940918
epoch£º267	 i:6 	 global-step:5346	 l-p:0.3251946270465851
epoch£º267	 i:7 	 global-step:5347	 l-p:0.12439835071563721
epoch£º267	 i:8 	 global-step:5348	 l-p:0.13482291996479034
epoch£º267	 i:9 	 global-step:5349	 l-p:0.1770467907190323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9794, 4.9793, 4.9794],
        [4.9794, 5.1817, 5.1269],
        [4.9794, 4.9784, 4.9783],
        [4.9794, 4.9794, 4.9794]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.1500478833913803 
model_pd.l_d.mean(): -20.079076766967773 
model_pd.lagr.mean(): -19.92902946472168 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4763], device='cuda:0')), ('power', tensor([-20.9478], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.1500478833913803
epoch£º268	 i:1 	 global-step:5361	 l-p:0.13865330815315247
epoch£º268	 i:2 	 global-step:5362	 l-p:0.15357857942581177
epoch£º268	 i:3 	 global-step:5363	 l-p:0.12294217944145203
epoch£º268	 i:4 	 global-step:5364	 l-p:3.8158552646636963
epoch£º268	 i:5 	 global-step:5365	 l-p:-0.3522905707359314
epoch£º268	 i:6 	 global-step:5366	 l-p:0.252508282661438
epoch£º268	 i:7 	 global-step:5367	 l-p:0.13396479189395905
epoch£º268	 i:8 	 global-step:5368	 l-p:0.15697327256202698
epoch£º268	 i:9 	 global-step:5369	 l-p:0.12851493060588837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0208, 5.0206, 5.0208],
        [5.0208, 5.0200, 5.0205],
        [5.0208, 5.0297, 5.0161],
        [5.0208, 5.0231, 5.0178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.19165927171707153 
model_pd.l_d.mean(): -20.26429557800293 
model_pd.lagr.mean(): -20.072635650634766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4546], device='cuda:0')), ('power', tensor([-21.1141], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.19165927171707153
epoch£º269	 i:1 	 global-step:5381	 l-p:0.22320720553398132
epoch£º269	 i:2 	 global-step:5382	 l-p:0.1394740790128708
epoch£º269	 i:3 	 global-step:5383	 l-p:0.1144542321562767
epoch£º269	 i:4 	 global-step:5384	 l-p:0.1227710098028183
epoch£º269	 i:5 	 global-step:5385	 l-p:0.1221914291381836
epoch£º269	 i:6 	 global-step:5386	 l-p:0.10637526214122772
epoch£º269	 i:7 	 global-step:5387	 l-p:0.17242519557476044
epoch£º269	 i:8 	 global-step:5388	 l-p:0.13902421295642853
epoch£º269	 i:9 	 global-step:5389	 l-p:0.09885915368795395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9837, 4.9827, 4.9835],
        [4.9837, 5.2370, 5.1984],
        [4.9837, 4.9835, 4.9837],
        [4.9837, 5.0913, 5.0274]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.13212262094020844 
model_pd.l_d.mean(): -20.46936798095703 
model_pd.lagr.mean(): -20.33724594116211 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4291], device='cuda:0')), ('power', tensor([-21.2965], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.13212262094020844
epoch£º270	 i:1 	 global-step:5401	 l-p:0.3772518038749695
epoch£º270	 i:2 	 global-step:5402	 l-p:1.1274253129959106
epoch£º270	 i:3 	 global-step:5403	 l-p:0.15267349779605865
epoch£º270	 i:4 	 global-step:5404	 l-p:0.1400010585784912
epoch£º270	 i:5 	 global-step:5405	 l-p:0.2834213972091675
epoch£º270	 i:6 	 global-step:5406	 l-p:0.1678546518087387
epoch£º270	 i:7 	 global-step:5407	 l-p:0.12920591235160828
epoch£º270	 i:8 	 global-step:5408	 l-p:0.12981660664081573
epoch£º270	 i:9 	 global-step:5409	 l-p:0.1540401577949524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9507, 4.9507, 4.9507],
        [4.9507, 5.0381, 4.9771],
        [4.9507, 5.3370, 5.3688],
        [4.9507, 5.6667, 5.9583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): -0.11588550359010696 
model_pd.l_d.mean(): -20.308591842651367 
model_pd.lagr.mean(): -20.424476623535156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4653], device='cuda:0')), ('power', tensor([-21.1702], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:-0.11588550359010696
epoch£º271	 i:1 	 global-step:5421	 l-p:0.11784597486257553
epoch£º271	 i:2 	 global-step:5422	 l-p:0.15270936489105225
epoch£º271	 i:3 	 global-step:5423	 l-p:0.15517465770244598
epoch£º271	 i:4 	 global-step:5424	 l-p:0.13786916434764862
epoch£º271	 i:5 	 global-step:5425	 l-p:0.12209849059581757
epoch£º271	 i:6 	 global-step:5426	 l-p:0.1488543152809143
epoch£º271	 i:7 	 global-step:5427	 l-p:0.13171662390232086
epoch£º271	 i:8 	 global-step:5428	 l-p:-0.01209163200110197
epoch£º271	 i:9 	 global-step:5429	 l-p:0.1436920017004013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9241, 4.9222, 4.9230],
        [4.9241, 5.3857, 5.4699],
        [4.9241, 4.9231, 4.9211],
        [4.9241, 4.9225, 4.9236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): -0.1509779691696167 
model_pd.l_d.mean(): -19.066787719726562 
model_pd.lagr.mean(): -19.21776580810547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5182], device='cuda:0')), ('power', tensor([-19.9601], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:-0.1509779691696167
epoch£º272	 i:1 	 global-step:5441	 l-p:0.15634967386722565
epoch£º272	 i:2 	 global-step:5442	 l-p:0.117885060608387
epoch£º272	 i:3 	 global-step:5443	 l-p:0.10193536430597305
epoch£º272	 i:4 	 global-step:5444	 l-p:0.14313848316669464
epoch£º272	 i:5 	 global-step:5445	 l-p:0.12857800722122192
epoch£º272	 i:6 	 global-step:5446	 l-p:-0.03279741853475571
epoch£º272	 i:7 	 global-step:5447	 l-p:0.13520364463329315
epoch£º272	 i:8 	 global-step:5448	 l-p:0.22673511505126953
epoch£º272	 i:9 	 global-step:5449	 l-p:0.14435750246047974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9089, 4.9632, 4.9122],
        [4.9089, 4.9083, 4.9088],
        [4.9089, 4.9084, 4.9049],
        [4.9089, 4.9069, 4.9079]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.22498410940170288 
model_pd.l_d.mean(): -18.36911392211914 
model_pd.lagr.mean(): -18.14413070678711 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5711], device='cuda:0')), ('power', tensor([-19.3041], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.22498410940170288
epoch£º273	 i:1 	 global-step:5461	 l-p:0.13891611993312836
epoch£º273	 i:2 	 global-step:5462	 l-p:0.14426502585411072
epoch£º273	 i:3 	 global-step:5463	 l-p:0.06657139956951141
epoch£º273	 i:4 	 global-step:5464	 l-p:0.12148501724004745
epoch£º273	 i:5 	 global-step:5465	 l-p:0.1222868487238884
epoch£º273	 i:6 	 global-step:5466	 l-p:0.14670000970363617
epoch£º273	 i:7 	 global-step:5467	 l-p:0.13551998138427734
epoch£º273	 i:8 	 global-step:5468	 l-p:0.13483278453350067
epoch£º273	 i:9 	 global-step:5469	 l-p:0.1927763670682907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8607, 5.0537, 4.9978],
        [4.8607, 5.5441, 5.8166],
        [4.8607, 4.8598, 4.8606],
        [4.8607, 5.5422, 5.8131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.03372512385249138 
model_pd.l_d.mean(): -20.522764205932617 
model_pd.lagr.mean(): -20.489038467407227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4618], device='cuda:0')), ('power', tensor([-21.3848], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.03372512385249138
epoch£º274	 i:1 	 global-step:5481	 l-p:0.13579459488391876
epoch£º274	 i:2 	 global-step:5482	 l-p:0.18633750081062317
epoch£º274	 i:3 	 global-step:5483	 l-p:0.21966896951198578
epoch£º274	 i:4 	 global-step:5484	 l-p:0.1448211520910263
epoch£º274	 i:5 	 global-step:5485	 l-p:0.13754235208034515
epoch£º274	 i:6 	 global-step:5486	 l-p:0.12226106971502304
epoch£º274	 i:7 	 global-step:5487	 l-p:0.12019263952970505
epoch£º274	 i:8 	 global-step:5488	 l-p:0.17990867793560028
epoch£º274	 i:9 	 global-step:5489	 l-p:0.5616133809089661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0111, 5.2926, 5.2647],
        [5.0111, 5.3164, 5.2997],
        [5.0111, 5.0111, 5.0111],
        [5.0111, 5.0096, 5.0102]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.14492300152778625 
model_pd.l_d.mean(): -20.585424423217773 
model_pd.lagr.mean(): -20.440502166748047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4055], device='cuda:0')), ('power', tensor([-21.3903], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.14492300152778625
epoch£º275	 i:1 	 global-step:5501	 l-p:0.1270105540752411
epoch£º275	 i:2 	 global-step:5502	 l-p:0.11955483257770538
epoch£º275	 i:3 	 global-step:5503	 l-p:0.10980001091957092
epoch£º275	 i:4 	 global-step:5504	 l-p:0.1544945240020752
epoch£º275	 i:5 	 global-step:5505	 l-p:0.15507936477661133
epoch£º275	 i:6 	 global-step:5506	 l-p:0.11543040722608566
epoch£º275	 i:7 	 global-step:5507	 l-p:0.10602792352437973
epoch£º275	 i:8 	 global-step:5508	 l-p:0.1613820195198059
epoch£º275	 i:9 	 global-step:5509	 l-p:0.1325344443321228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0191, 5.6696, 5.8960],
        [5.0191, 5.0176, 5.0180],
        [5.0191, 5.0191, 5.0191],
        [5.0191, 5.0187, 5.0191]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.13001441955566406 
model_pd.l_d.mean(): -20.64908218383789 
model_pd.lagr.mean(): -20.519067764282227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3970], device='cuda:0')), ('power', tensor([-21.4464], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.13001441955566406
epoch£º276	 i:1 	 global-step:5521	 l-p:0.16228574514389038
epoch£º276	 i:2 	 global-step:5522	 l-p:0.011526228860020638
epoch£º276	 i:3 	 global-step:5523	 l-p:0.14321820437908173
epoch£º276	 i:4 	 global-step:5524	 l-p:0.11107159405946732
epoch£º276	 i:5 	 global-step:5525	 l-p:0.10616898536682129
epoch£º276	 i:6 	 global-step:5526	 l-p:-0.09759780764579773
epoch£º276	 i:7 	 global-step:5527	 l-p:0.12670771777629852
epoch£º276	 i:8 	 global-step:5528	 l-p:0.12426572293043137
epoch£º276	 i:9 	 global-step:5529	 l-p:0.12633737921714783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7761, 5.1973, 5.2643],
        [4.7761, 5.2782, 5.4060],
        [4.7761, 4.7761, 4.7761],
        [4.7761, 5.0957, 5.0970]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.17745059728622437 
model_pd.l_d.mean(): -20.092798233032227 
model_pd.lagr.mean(): -19.915348052978516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5401], device='cuda:0')), ('power', tensor([-21.0279], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.17745059728622437
epoch£º277	 i:1 	 global-step:5541	 l-p:0.1484997421503067
epoch£º277	 i:2 	 global-step:5542	 l-p:0.13718611001968384
epoch£º277	 i:3 	 global-step:5543	 l-p:0.16046756505966187
epoch£º277	 i:4 	 global-step:5544	 l-p:0.06722637265920639
epoch£º277	 i:5 	 global-step:5545	 l-p:0.1209852546453476
epoch£º277	 i:6 	 global-step:5546	 l-p:0.14822980761528015
epoch£º277	 i:7 	 global-step:5547	 l-p:-0.39257052540779114
epoch£º277	 i:8 	 global-step:5548	 l-p:0.13424000144004822
epoch£º277	 i:9 	 global-step:5549	 l-p:0.13912367820739746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9147, 4.9130, 4.9143],
        [4.9147, 4.9124, 4.9118],
        [4.9147, 4.9255, 4.9053],
        [4.9147, 5.4132, 5.5258]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.1301163285970688 
model_pd.l_d.mean(): -19.02388572692871 
model_pd.lagr.mean(): -18.893770217895508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5170], device='cuda:0')), ('power', tensor([-19.9151], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.1301163285970688
epoch£º278	 i:1 	 global-step:5561	 l-p:-0.06285103410482407
epoch£º278	 i:2 	 global-step:5562	 l-p:0.33637481927871704
epoch£º278	 i:3 	 global-step:5563	 l-p:0.1156734824180603
epoch£º278	 i:4 	 global-step:5564	 l-p:0.1429608166217804
epoch£º278	 i:5 	 global-step:5565	 l-p:0.11730062961578369
epoch£º278	 i:6 	 global-step:5566	 l-p:0.1354997456073761
epoch£º278	 i:7 	 global-step:5567	 l-p:0.1183091551065445
epoch£º278	 i:8 	 global-step:5568	 l-p:0.15633262693881989
epoch£º278	 i:9 	 global-step:5569	 l-p:0.1263757348060608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0676, 5.1085, 5.0662],
        [5.0676, 5.0681, 5.0644],
        [5.0676, 5.5541, 5.6461],
        [5.0676, 5.0994, 5.0632]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.13247911632061005 
model_pd.l_d.mean(): -20.3149356842041 
model_pd.lagr.mean(): -20.182456970214844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4214], device='cuda:0')), ('power', tensor([-21.1313], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.13247911632061005
epoch£º279	 i:1 	 global-step:5581	 l-p:0.21204738318920135
epoch£º279	 i:2 	 global-step:5582	 l-p:0.10827852785587311
epoch£º279	 i:3 	 global-step:5583	 l-p:0.10297776758670807
epoch£º279	 i:4 	 global-step:5584	 l-p:0.14386257529258728
epoch£º279	 i:5 	 global-step:5585	 l-p:0.19698308408260345
epoch£º279	 i:6 	 global-step:5586	 l-p:0.2101261466741562
epoch£º279	 i:7 	 global-step:5587	 l-p:0.13006716966629028
epoch£º279	 i:8 	 global-step:5588	 l-p:0.14278031885623932
epoch£º279	 i:9 	 global-step:5589	 l-p:0.13830910623073578
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0411, 5.0408, 5.0411],
        [5.0411, 5.0407, 5.0377],
        [5.0411, 5.0495, 5.0341],
        [5.0411, 5.0731, 5.0358]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.12617959082126617 
model_pd.l_d.mean(): -19.889829635620117 
model_pd.lagr.mean(): -19.76365089416504 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4372], device='cuda:0')), ('power', tensor([-20.7146], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.12617959082126617
epoch£º280	 i:1 	 global-step:5601	 l-p:0.15201054513454437
epoch£º280	 i:2 	 global-step:5602	 l-p:0.14776606857776642
epoch£º280	 i:3 	 global-step:5603	 l-p:0.1302562803030014
epoch£º280	 i:4 	 global-step:5604	 l-p:0.1312895119190216
epoch£º280	 i:5 	 global-step:5605	 l-p:0.049576833844184875
epoch£º280	 i:6 	 global-step:5606	 l-p:0.05711909011006355
epoch£º280	 i:7 	 global-step:5607	 l-p:0.1219261884689331
epoch£º280	 i:8 	 global-step:5608	 l-p:0.12958183884620667
epoch£º280	 i:9 	 global-step:5609	 l-p:0.05387406796216965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9041, 4.9942, 4.9294],
        [4.9041, 4.9039, 4.9041],
        [4.9041, 4.9015, 4.9032],
        [4.9041, 4.9067, 4.8954]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.06598906219005585 
model_pd.l_d.mean(): -20.443771362304688 
model_pd.lagr.mean(): -20.377782821655273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4622], device='cuda:0')), ('power', tensor([-21.3048], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.06598906219005585
epoch£º281	 i:1 	 global-step:5621	 l-p:0.03697909787297249
epoch£º281	 i:2 	 global-step:5622	 l-p:0.15118210017681122
epoch£º281	 i:3 	 global-step:5623	 l-p:0.16376739740371704
epoch£º281	 i:4 	 global-step:5624	 l-p:-0.2908496856689453
epoch£º281	 i:5 	 global-step:5625	 l-p:0.11595024168491364
epoch£º281	 i:6 	 global-step:5626	 l-p:0.1527113914489746
epoch£º281	 i:7 	 global-step:5627	 l-p:0.13444900512695312
epoch£º281	 i:8 	 global-step:5628	 l-p:0.13016162812709808
epoch£º281	 i:9 	 global-step:5629	 l-p:0.1197492778301239
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1456,  0.0766,  1.0000,  0.0403,
          1.0000,  0.5261, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228]], device='cuda:0')
 pt:tensor([[4.9961, 5.0023, 4.9880],
        [4.9961, 5.1220, 5.0542],
        [4.9961, 5.0061, 4.9872],
        [4.9961, 5.6606, 5.9018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.09722646325826645 
model_pd.l_d.mean(): -18.434186935424805 
model_pd.lagr.mean(): -18.336959838867188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5897], device='cuda:0')), ('power', tensor([-19.3897], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.09722646325826645
epoch£º282	 i:1 	 global-step:5641	 l-p:0.13501308858394623
epoch£º282	 i:2 	 global-step:5642	 l-p:0.18394385278224945
epoch£º282	 i:3 	 global-step:5643	 l-p:0.23673854768276215
epoch£º282	 i:4 	 global-step:5644	 l-p:0.1839739978313446
epoch£º282	 i:5 	 global-step:5645	 l-p:0.1498584747314453
epoch£º282	 i:6 	 global-step:5646	 l-p:0.11401752382516861
epoch£º282	 i:7 	 global-step:5647	 l-p:0.15787191689014435
epoch£º282	 i:8 	 global-step:5648	 l-p:0.13257096707820892
epoch£º282	 i:9 	 global-step:5649	 l-p:0.12879329919815063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0576, 5.3021, 5.2569],
        [5.0576, 5.7387, 5.9881],
        [5.0576, 5.3021, 5.2569],
        [5.0576, 5.0566, 5.0574]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.09458576887845993 
model_pd.l_d.mean(): -19.917869567871094 
model_pd.lagr.mean(): -19.823284149169922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4699], device='cuda:0')), ('power', tensor([-20.7770], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.09458576887845993
epoch£º283	 i:1 	 global-step:5661	 l-p:0.12262453138828278
epoch£º283	 i:2 	 global-step:5662	 l-p:0.12774346768856049
epoch£º283	 i:3 	 global-step:5663	 l-p:0.1477413922548294
epoch£º283	 i:4 	 global-step:5664	 l-p:0.3020564913749695
epoch£º283	 i:5 	 global-step:5665	 l-p:0.1346311867237091
epoch£º283	 i:6 	 global-step:5666	 l-p:0.12509693205356598
epoch£º283	 i:7 	 global-step:5667	 l-p:1.5199707746505737
epoch£º283	 i:8 	 global-step:5668	 l-p:0.18396945297718048
epoch£º283	 i:9 	 global-step:5669	 l-p:0.47458434104919434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0079, 5.0057, 5.0070],
        [5.0079, 5.0064, 5.0076],
        [5.0079, 5.0078, 5.0079],
        [5.0079, 5.1444, 5.0763]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.18379323184490204 
model_pd.l_d.mean(): -20.594820022583008 
model_pd.lagr.mean(): -20.411026000976562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4130], device='cuda:0')), ('power', tensor([-21.4077], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.18379323184490204
epoch£º284	 i:1 	 global-step:5681	 l-p:0.17380020022392273
epoch£º284	 i:2 	 global-step:5682	 l-p:0.13568590581417084
epoch£º284	 i:3 	 global-step:5683	 l-p:0.14986032247543335
epoch£º284	 i:4 	 global-step:5684	 l-p:0.12198643386363983
epoch£º284	 i:5 	 global-step:5685	 l-p:0.007832727394998074
epoch£º284	 i:6 	 global-step:5686	 l-p:0.1273113638162613
epoch£º284	 i:7 	 global-step:5687	 l-p:0.13740108907222748
epoch£º284	 i:8 	 global-step:5688	 l-p:0.11661679297685623
epoch£º284	 i:9 	 global-step:5689	 l-p:0.1011829748749733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1267, 5.1267, 5.1267],
        [5.1267, 5.1253, 5.1262],
        [5.1267, 5.1363, 5.1200],
        [5.1267, 5.3632, 5.3132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.11146785318851471 
model_pd.l_d.mean(): -20.294116973876953 
model_pd.lagr.mean(): -20.182649612426758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4071], device='cuda:0')), ('power', tensor([-21.0953], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.11146785318851471
epoch£º285	 i:1 	 global-step:5701	 l-p:0.1259840726852417
epoch£º285	 i:2 	 global-step:5702	 l-p:0.13499616086483002
epoch£º285	 i:3 	 global-step:5703	 l-p:0.12192859500646591
epoch£º285	 i:4 	 global-step:5704	 l-p:0.1280660629272461
epoch£º285	 i:5 	 global-step:5705	 l-p:-0.18227586150169373
epoch£º285	 i:6 	 global-step:5706	 l-p:0.07623124122619629
epoch£º285	 i:7 	 global-step:5707	 l-p:0.004801339935511351
epoch£º285	 i:8 	 global-step:5708	 l-p:0.19618694484233856
epoch£º285	 i:9 	 global-step:5709	 l-p:0.12710770964622498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9440, 4.9407, 4.9422],
        [4.9440, 4.9440, 4.9440],
        [4.9440, 4.9440, 4.9440],
        [4.9440, 4.9432, 4.9439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.13102325797080994 
model_pd.l_d.mean(): -20.464075088500977 
model_pd.lagr.mean(): -20.333051681518555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4416], device='cuda:0')), ('power', tensor([-21.3041], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:0.13102325797080994
epoch£º286	 i:1 	 global-step:5721	 l-p:0.13206109404563904
epoch£º286	 i:2 	 global-step:5722	 l-p:0.2998473346233368
epoch£º286	 i:3 	 global-step:5723	 l-p:0.12465085834264755
epoch£º286	 i:4 	 global-step:5724	 l-p:0.08622020483016968
epoch£º286	 i:5 	 global-step:5725	 l-p:0.02595951408147812
epoch£º286	 i:6 	 global-step:5726	 l-p:0.13115762174129486
epoch£º286	 i:7 	 global-step:5727	 l-p:0.1351747065782547
epoch£º286	 i:8 	 global-step:5728	 l-p:0.14075204730033875
epoch£º286	 i:9 	 global-step:5729	 l-p:0.13858334720134735
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0084, 5.1973, 5.1359],
        [5.0084, 5.2750, 5.2396],
        [5.0084, 5.0302, 4.9982],
        [5.0084, 5.0060, 5.0049]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.14021426439285278 
model_pd.l_d.mean(): -20.26691246032715 
model_pd.lagr.mean(): -20.126697540283203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4479], device='cuda:0')), ('power', tensor([-21.1098], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:0.14021426439285278
epoch£º287	 i:1 	 global-step:5741	 l-p:0.12813223898410797
epoch£º287	 i:2 	 global-step:5742	 l-p:0.18460875749588013
epoch£º287	 i:3 	 global-step:5743	 l-p:-0.009333948604762554
epoch£º287	 i:4 	 global-step:5744	 l-p:0.13312721252441406
epoch£º287	 i:5 	 global-step:5745	 l-p:0.1453508734703064
epoch£º287	 i:6 	 global-step:5746	 l-p:0.13961488008499146
epoch£º287	 i:7 	 global-step:5747	 l-p:0.14255040884017944
epoch£º287	 i:8 	 global-step:5748	 l-p:0.14298973977565765
epoch£º287	 i:9 	 global-step:5749	 l-p:-0.04320140928030014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9436, 4.9402, 4.9420],
        [4.9436, 4.9436, 4.9436],
        [4.9436, 4.9431, 4.9436],
        [4.9436, 5.0765, 5.0074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.1922348141670227 
model_pd.l_d.mean(): -20.408401489257812 
model_pd.lagr.mean(): -20.216167449951172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4558], device='cuda:0')), ('power', tensor([-21.2621], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.1922348141670227
epoch£º288	 i:1 	 global-step:5761	 l-p:0.14470304548740387
epoch£º288	 i:2 	 global-step:5762	 l-p:0.1462683230638504
epoch£º288	 i:3 	 global-step:5763	 l-p:0.4404197335243225
epoch£º288	 i:4 	 global-step:5764	 l-p:0.14035066962242126
epoch£º288	 i:5 	 global-step:5765	 l-p:0.13215574622154236
epoch£º288	 i:6 	 global-step:5766	 l-p:0.1475933939218521
epoch£º288	 i:7 	 global-step:5767	 l-p:0.12404003739356995
epoch£º288	 i:8 	 global-step:5768	 l-p:0.09964345395565033
epoch£º288	 i:9 	 global-step:5769	 l-p:0.4033123552799225
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9937, 5.3246, 5.3211],
        [4.9937, 4.9933, 4.9937],
        [4.9937, 4.9934, 4.9937],
        [4.9937, 4.9907, 4.9923]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.11718018352985382 
model_pd.l_d.mean(): -18.875717163085938 
model_pd.lagr.mean(): -18.75853729248047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5385], device='cuda:0')), ('power', tensor([-19.7864], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.11718018352985382
epoch£º289	 i:1 	 global-step:5781	 l-p:0.09212086349725723
epoch£º289	 i:2 	 global-step:5782	 l-p:0.8745065331459045
epoch£º289	 i:3 	 global-step:5783	 l-p:0.13638919591903687
epoch£º289	 i:4 	 global-step:5784	 l-p:0.15119023621082306
epoch£º289	 i:5 	 global-step:5785	 l-p:0.7717042565345764
epoch£º289	 i:6 	 global-step:5786	 l-p:0.17787033319473267
epoch£º289	 i:7 	 global-step:5787	 l-p:0.14325536787509918
epoch£º289	 i:8 	 global-step:5788	 l-p:0.1228499561548233
epoch£º289	 i:9 	 global-step:5789	 l-p:0.14161525666713715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9999, 4.9999, 4.9999],
        [4.9999, 4.9987, 4.9997],
        [4.9999, 4.9999, 4.9999],
        [4.9999, 5.0180, 4.9879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.1520167738199234 
model_pd.l_d.mean(): -20.448678970336914 
model_pd.lagr.mean(): -20.296661376953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4192], device='cuda:0')), ('power', tensor([-21.2652], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.1520167738199234
epoch£º290	 i:1 	 global-step:5801	 l-p:0.1283344328403473
epoch£º290	 i:2 	 global-step:5802	 l-p:0.1275797337293625
epoch£º290	 i:3 	 global-step:5803	 l-p:1.629350185394287
epoch£º290	 i:4 	 global-step:5804	 l-p:0.1293828785419464
epoch£º290	 i:5 	 global-step:5805	 l-p:0.1281529814004898
epoch£º290	 i:6 	 global-step:5806	 l-p:0.1414773315191269
epoch£º290	 i:7 	 global-step:5807	 l-p:0.24910295009613037
epoch£º290	 i:8 	 global-step:5808	 l-p:0.15178513526916504
epoch£º290	 i:9 	 global-step:5809	 l-p:0.14878524839878082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8557, 4.8787, 4.8393],
        [4.8557, 5.0781, 5.0290],
        [4.8557, 4.8513, 4.8539],
        [4.8557, 4.8557, 4.8557]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.0988149642944336 
model_pd.l_d.mean(): -20.74976348876953 
model_pd.lagr.mean(): -20.65094757080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4329], device='cuda:0')), ('power', tensor([-21.5861], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.0988149642944336
epoch£º291	 i:1 	 global-step:5821	 l-p:0.11735524982213974
epoch£º291	 i:2 	 global-step:5822	 l-p:0.12541277706623077
epoch£º291	 i:3 	 global-step:5823	 l-p:0.17260554432868958
epoch£º291	 i:4 	 global-step:5824	 l-p:0.1304166615009308
epoch£º291	 i:5 	 global-step:5825	 l-p:-0.0034362601581960917
epoch£º291	 i:6 	 global-step:5826	 l-p:0.1901952177286148
epoch£º291	 i:7 	 global-step:5827	 l-p:0.16753001511096954
epoch£º291	 i:8 	 global-step:5828	 l-p:0.10249074548482895
epoch£º291	 i:9 	 global-step:5829	 l-p:0.1334293633699417
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1037, 5.1037, 5.1037],
        [5.1037, 5.1021, 5.1033],
        [5.1037, 5.1035, 5.1037],
        [5.1037, 5.1037, 5.1037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.129460871219635 
model_pd.l_d.mean(): -19.890756607055664 
model_pd.lagr.mean(): -19.761295318603516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4364], device='cuda:0')), ('power', tensor([-20.7147], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.129460871219635
epoch£º292	 i:1 	 global-step:5841	 l-p:0.1248905137181282
epoch£º292	 i:2 	 global-step:5842	 l-p:0.11861993372440338
epoch£º292	 i:3 	 global-step:5843	 l-p:0.11697525531053543
epoch£º292	 i:4 	 global-step:5844	 l-p:0.11583258956670761
epoch£º292	 i:5 	 global-step:5845	 l-p:0.162039652466774
epoch£º292	 i:6 	 global-step:5846	 l-p:0.11563975363969803
epoch£º292	 i:7 	 global-step:5847	 l-p:0.18716533482074738
epoch£º292	 i:8 	 global-step:5848	 l-p:0.12851175665855408
epoch£º292	 i:9 	 global-step:5849	 l-p:0.12631307542324066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2542,  0.1610,  1.0000,  0.1020,
          1.0000,  0.6334, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1464,  0.0772,  1.0000,  0.0407,
          1.0000,  0.5270, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228]], device='cuda:0')
 pt:tensor([[5.0771, 5.1315, 5.0771],
        [5.0771, 5.1288, 5.0757],
        [5.0771, 5.0819, 5.0678],
        [5.0771, 5.0780, 5.0697]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.12762078642845154 
model_pd.l_d.mean(): -20.443872451782227 
model_pd.lagr.mean(): -20.316251754760742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3974], device='cuda:0')), ('power', tensor([-21.2378], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.12762078642845154
epoch£º293	 i:1 	 global-step:5861	 l-p:0.11508169025182724
epoch£º293	 i:2 	 global-step:5862	 l-p:0.20865432918071747
epoch£º293	 i:3 	 global-step:5863	 l-p:0.28232342004776
epoch£º293	 i:4 	 global-step:5864	 l-p:0.13102492690086365
epoch£º293	 i:5 	 global-step:5865	 l-p:0.12131179869174957
epoch£º293	 i:6 	 global-step:5866	 l-p:0.33620014786720276
epoch£º293	 i:7 	 global-step:5867	 l-p:0.12250519543886185
epoch£º293	 i:8 	 global-step:5868	 l-p:0.14573724567890167
epoch£º293	 i:9 	 global-step:5869	 l-p:0.20867472887039185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228]], device='cuda:0')
 pt:tensor([[4.9822, 4.9882, 4.9694],
        [4.9822, 5.2581, 5.2269],
        [4.9822, 5.3301, 5.3364],
        [4.9822, 5.0479, 4.9865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.15443745255470276 
model_pd.l_d.mean(): -19.329200744628906 
model_pd.lagr.mean(): -19.174762725830078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4872], device='cuda:0')), ('power', tensor([-20.1952], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.15443745255470276
epoch£º294	 i:1 	 global-step:5881	 l-p:0.14025337994098663
epoch£º294	 i:2 	 global-step:5882	 l-p:0.21951261162757874
epoch£º294	 i:3 	 global-step:5883	 l-p:0.12642626464366913
epoch£º294	 i:4 	 global-step:5884	 l-p:0.1075286939740181
epoch£º294	 i:5 	 global-step:5885	 l-p:0.21866507828235626
epoch£º294	 i:6 	 global-step:5886	 l-p:0.14762412011623383
epoch£º294	 i:7 	 global-step:5887	 l-p:0.1108039990067482
epoch£º294	 i:8 	 global-step:5888	 l-p:0.12482217699289322
epoch£º294	 i:9 	 global-step:5889	 l-p:0.19357305765151978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0334, 5.0585, 5.0210],
        [5.0334, 5.0333, 5.0334],
        [5.0334, 5.0327, 5.0333],
        [5.0334, 5.0334, 5.0334]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.1370864063501358 
model_pd.l_d.mean(): -20.68130874633789 
model_pd.lagr.mean(): -20.544221878051758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3943], device='cuda:0')), ('power', tensor([-21.4764], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.1370864063501358
epoch£º295	 i:1 	 global-step:5901	 l-p:0.1223149374127388
epoch£º295	 i:2 	 global-step:5902	 l-p:0.40018516778945923
epoch£º295	 i:3 	 global-step:5903	 l-p:0.2274051457643509
epoch£º295	 i:4 	 global-step:5904	 l-p:0.2747364044189453
epoch£º295	 i:5 	 global-step:5905	 l-p:0.13493597507476807
epoch£º295	 i:6 	 global-step:5906	 l-p:0.14186961948871613
epoch£º295	 i:7 	 global-step:5907	 l-p:0.12290934473276138
epoch£º295	 i:8 	 global-step:5908	 l-p:0.13509251177310944
epoch£º295	 i:9 	 global-step:5909	 l-p:0.13579557836055756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0881, 5.1292, 5.0813],
        [5.0881, 5.0861, 5.0877],
        [5.0881, 5.0878, 5.0881],
        [5.0881, 5.0881, 5.0881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.151725634932518 
model_pd.l_d.mean(): -20.302330017089844 
model_pd.lagr.mean(): -20.150604248046875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4184], device='cuda:0')), ('power', tensor([-21.1153], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.151725634932518
epoch£º296	 i:1 	 global-step:5921	 l-p:0.13234379887580872
epoch£º296	 i:2 	 global-step:5922	 l-p:0.13735561072826385
epoch£º296	 i:3 	 global-step:5923	 l-p:0.22892145812511444
epoch£º296	 i:4 	 global-step:5924	 l-p:0.21427971124649048
epoch£º296	 i:5 	 global-step:5925	 l-p:0.10937881469726562
epoch£º296	 i:6 	 global-step:5926	 l-p:0.12976643443107605
epoch£º296	 i:7 	 global-step:5927	 l-p:0.14107835292816162
epoch£º296	 i:8 	 global-step:5928	 l-p:0.13850967586040497
epoch£º296	 i:9 	 global-step:5929	 l-p:0.11621540039777756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8969, 5.5555, 5.8027],
        [4.8969, 4.9069, 4.8786],
        [4.8969, 4.8922, 4.8951],
        [4.8969, 4.8965, 4.8969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.07455657422542572 
model_pd.l_d.mean(): -19.371135711669922 
model_pd.lagr.mean(): -19.296579360961914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5428], device='cuda:0')), ('power', tensor([-20.2956], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.07455657422542572
epoch£º297	 i:1 	 global-step:5941	 l-p:0.1292889416217804
epoch£º297	 i:2 	 global-step:5942	 l-p:0.1422942727804184
epoch£º297	 i:3 	 global-step:5943	 l-p:0.140681192278862
epoch£º297	 i:4 	 global-step:5944	 l-p:0.14234724640846252
epoch£º297	 i:5 	 global-step:5945	 l-p:0.15443946421146393
epoch£º297	 i:6 	 global-step:5946	 l-p:0.0824594646692276
epoch£º297	 i:7 	 global-step:5947	 l-p:0.12431139498949051
epoch£º297	 i:8 	 global-step:5948	 l-p:0.14273172616958618
epoch£º297	 i:9 	 global-step:5949	 l-p:0.16376008093357086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7988, 4.7988, 4.7988],
        [4.7988, 4.7908, 4.7885],
        [4.7988, 4.7987, 4.7988],
        [4.7988, 5.5967, 5.9839]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.14081619679927826 
model_pd.l_d.mean(): -19.684066772460938 
model_pd.lagr.mean(): -19.543251037597656 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5312], device='cuda:0')), ('power', tensor([-20.6024], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.14081619679927826
epoch£º298	 i:1 	 global-step:5961	 l-p:0.518333375453949
epoch£º298	 i:2 	 global-step:5962	 l-p:0.15195152163505554
epoch£º298	 i:3 	 global-step:5963	 l-p:0.128072127699852
epoch£º298	 i:4 	 global-step:5964	 l-p:0.16518065333366394
epoch£º298	 i:5 	 global-step:5965	 l-p:0.12497613579034805
epoch£º298	 i:6 	 global-step:5966	 l-p:1.242445468902588
epoch£º298	 i:7 	 global-step:5967	 l-p:0.1468435674905777
epoch£º298	 i:8 	 global-step:5968	 l-p:0.1270144134759903
epoch£º298	 i:9 	 global-step:5969	 l-p:0.17811614274978638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0763, 5.0732, 5.0753],
        [5.0763, 5.0834, 5.0637],
        [5.0763, 5.1256, 5.0713],
        [5.0763, 5.2094, 5.1373]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.11821399629116058 
model_pd.l_d.mean(): -20.294919967651367 
model_pd.lagr.mean(): -20.176706314086914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4290], device='cuda:0')), ('power', tensor([-21.1187], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.11821399629116058
epoch£º299	 i:1 	 global-step:5981	 l-p:0.1600969135761261
epoch£º299	 i:2 	 global-step:5982	 l-p:0.17762213945388794
epoch£º299	 i:3 	 global-step:5983	 l-p:0.1265932023525238
epoch£º299	 i:4 	 global-step:5984	 l-p:0.13182927668094635
epoch£º299	 i:5 	 global-step:5985	 l-p:0.14573609828948975
epoch£º299	 i:6 	 global-step:5986	 l-p:0.1333589404821396
epoch£º299	 i:7 	 global-step:5987	 l-p:1.393989086151123
epoch£º299	 i:8 	 global-step:5988	 l-p:0.0999954417347908
epoch£º299	 i:9 	 global-step:5989	 l-p:-3.039419174194336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9823, 5.6524, 5.9005],
        [4.9823, 4.9768, 4.9767],
        [4.9823, 4.9823, 4.9823],
        [4.9823, 4.9768, 4.9770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.18694421648979187 
model_pd.l_d.mean(): -20.50767707824707 
model_pd.lagr.mean(): -20.32073211669922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4308], device='cuda:0')), ('power', tensor([-21.3373], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.18694421648979187
epoch£º300	 i:1 	 global-step:6001	 l-p:0.637811541557312
epoch£º300	 i:2 	 global-step:6002	 l-p:0.1385660469532013
epoch£º300	 i:3 	 global-step:6003	 l-p:0.13513371348381042
epoch£º300	 i:4 	 global-step:6004	 l-p:0.1341594159603119
epoch£º300	 i:5 	 global-step:6005	 l-p:0.12755151093006134
epoch£º300	 i:6 	 global-step:6006	 l-p:0.11849282681941986
epoch£º300	 i:7 	 global-step:6007	 l-p:0.20514875650405884
epoch£º300	 i:8 	 global-step:6008	 l-p:0.1269408017396927
epoch£º300	 i:9 	 global-step:6009	 l-p:0.14032495021820068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9859, 4.9873, 4.9717],
        [4.9859, 4.9859, 4.9859],
        [4.9859, 5.1315, 5.0598],
        [4.9859, 4.9859, 4.9859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): -1.577186107635498 
model_pd.l_d.mean(): -19.079757690429688 
model_pd.lagr.mean(): -20.656944274902344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5100], device='cuda:0')), ('power', tensor([-19.9648], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:-1.577186107635498
epoch£º301	 i:1 	 global-step:6021	 l-p:0.1242353767156601
epoch£º301	 i:2 	 global-step:6022	 l-p:0.11635511368513107
epoch£º301	 i:3 	 global-step:6023	 l-p:0.1442878097295761
epoch£º301	 i:4 	 global-step:6024	 l-p:0.08521748334169388
epoch£º301	 i:5 	 global-step:6025	 l-p:0.13673041760921478
epoch£º301	 i:6 	 global-step:6026	 l-p:0.13990019261837006
epoch£º301	 i:7 	 global-step:6027	 l-p:1.6864609718322754
epoch£º301	 i:8 	 global-step:6028	 l-p:0.24173064529895782
epoch£º301	 i:9 	 global-step:6029	 l-p:0.044940367341041565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8795, 4.8823, 4.8595],
        [4.8795, 4.8789, 4.8795],
        [4.8795, 5.4518, 5.6271],
        [4.8795, 4.8790, 4.8795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.1443406045436859 
model_pd.l_d.mean(): -20.57065773010254 
model_pd.lagr.mean(): -20.42631721496582 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-21.4226], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.1443406045436859
epoch£º302	 i:1 	 global-step:6041	 l-p:0.18067462742328644
epoch£º302	 i:2 	 global-step:6042	 l-p:0.12157472223043442
epoch£º302	 i:3 	 global-step:6043	 l-p:0.07636269181966782
epoch£º302	 i:4 	 global-step:6044	 l-p:0.14944443106651306
epoch£º302	 i:5 	 global-step:6045	 l-p:0.13575978577136993
epoch£º302	 i:6 	 global-step:6046	 l-p:1.7245862483978271
epoch£º302	 i:7 	 global-step:6047	 l-p:0.14841708540916443
epoch£º302	 i:8 	 global-step:6048	 l-p:0.14777080714702606
epoch£º302	 i:9 	 global-step:6049	 l-p:0.12224897742271423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8781, 4.8781, 4.8781],
        [4.8781, 4.8772, 4.8593],
        [4.8781, 5.6648, 6.0314],
        [4.8781, 4.9037, 4.8568]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.14290647208690643 
model_pd.l_d.mean(): -19.014141082763672 
model_pd.lagr.mean(): -18.871234893798828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5330], device='cuda:0')), ('power', tensor([-19.9217], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.14290647208690643
epoch£º303	 i:1 	 global-step:6061	 l-p:0.18238069117069244
epoch£º303	 i:2 	 global-step:6062	 l-p:0.1403772383928299
epoch£º303	 i:3 	 global-step:6063	 l-p:-0.11564092338085175
epoch£º303	 i:4 	 global-step:6064	 l-p:0.29905861616134644
epoch£º303	 i:5 	 global-step:6065	 l-p:0.09733913093805313
epoch£º303	 i:6 	 global-step:6066	 l-p:0.14272059500217438
epoch£º303	 i:7 	 global-step:6067	 l-p:0.13313370943069458
epoch£º303	 i:8 	 global-step:6068	 l-p:0.14110642671585083
epoch£º303	 i:9 	 global-step:6069	 l-p:0.09844633936882019
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1380, 5.1380, 5.1380],
        [5.1380, 5.1380, 5.1380],
        [5.1380, 5.1375, 5.1380],
        [5.1380, 5.1378, 5.1380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.12620866298675537 
model_pd.l_d.mean(): -18.921218872070312 
model_pd.lagr.mean(): -18.79500961303711 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4559], device='cuda:0')), ('power', tensor([-19.7472], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.12620866298675537
epoch£º304	 i:1 	 global-step:6081	 l-p:0.11102049797773361
epoch£º304	 i:2 	 global-step:6082	 l-p:0.12704256176948547
epoch£º304	 i:3 	 global-step:6083	 l-p:0.14106106758117676
epoch£º304	 i:4 	 global-step:6084	 l-p:0.15247932076454163
epoch£º304	 i:5 	 global-step:6085	 l-p:0.16114407777786255
epoch£º304	 i:6 	 global-step:6086	 l-p:0.062103189527988434
epoch£º304	 i:7 	 global-step:6087	 l-p:0.14248719811439514
epoch£º304	 i:8 	 global-step:6088	 l-p:0.12487739324569702
epoch£º304	 i:9 	 global-step:6089	 l-p:0.1309957057237625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1260, 5.1717, 5.1178],
        [5.1260, 5.1259, 5.1260],
        [5.1260, 5.8181, 6.0709],
        [5.1260, 5.1270, 5.1145]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.13406285643577576 
model_pd.l_d.mean(): -19.993947982788086 
model_pd.lagr.mean(): -19.85988426208496 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4458], device='cuda:0')), ('power', tensor([-20.8296], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.13406285643577576
epoch£º305	 i:1 	 global-step:6101	 l-p:0.1286042034626007
epoch£º305	 i:2 	 global-step:6102	 l-p:0.09970288723707199
epoch£º305	 i:3 	 global-step:6103	 l-p:0.3361775875091553
epoch£º305	 i:4 	 global-step:6104	 l-p:0.3450908958911896
epoch£º305	 i:5 	 global-step:6105	 l-p:0.13160787522792816
epoch£º305	 i:6 	 global-step:6106	 l-p:0.14088860154151917
epoch£º305	 i:7 	 global-step:6107	 l-p:-7.351775169372559
epoch£º305	 i:8 	 global-step:6108	 l-p:0.11750224977731705
epoch£º305	 i:9 	 global-step:6109	 l-p:0.11894950270652771
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9769, 4.9769, 4.9769],
        [4.9769, 5.5981, 5.8048],
        [4.9769, 4.9769, 4.9769],
        [4.9769, 5.0614, 4.9896]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): -0.9326809644699097 
model_pd.l_d.mean(): -18.893205642700195 
model_pd.lagr.mean(): -19.825885772705078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5388], device='cuda:0')), ('power', tensor([-19.8045], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:-0.9326809644699097
epoch£º306	 i:1 	 global-step:6121	 l-p:0.12897337973117828
epoch£º306	 i:2 	 global-step:6122	 l-p:0.13749860227108002
epoch£º306	 i:3 	 global-step:6123	 l-p:0.16581310331821442
epoch£º306	 i:4 	 global-step:6124	 l-p:0.17063692212104797
epoch£º306	 i:5 	 global-step:6125	 l-p:0.14825713634490967
epoch£º306	 i:6 	 global-step:6126	 l-p:0.15809886157512665
epoch£º306	 i:7 	 global-step:6127	 l-p:0.12790749967098236
epoch£º306	 i:8 	 global-step:6128	 l-p:0.11145726591348648
epoch£º306	 i:9 	 global-step:6129	 l-p:0.09607958793640137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9075, 4.9075, 4.9075],
        [4.9075, 4.8988, 4.8985],
        [4.9075, 5.3042, 5.3446],
        [4.9075, 4.8989, 4.8999]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.14754660427570343 
model_pd.l_d.mean(): -20.451629638671875 
model_pd.lagr.mean(): -20.3040828704834 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-21.3145], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.14754660427570343
epoch£º307	 i:1 	 global-step:6141	 l-p:0.13754723966121674
epoch£º307	 i:2 	 global-step:6142	 l-p:0.12194175273180008
epoch£º307	 i:3 	 global-step:6143	 l-p:0.14095452427864075
epoch£º307	 i:4 	 global-step:6144	 l-p:0.13836835324764252
epoch£º307	 i:5 	 global-step:6145	 l-p:0.10223331302404404
epoch£º307	 i:6 	 global-step:6146	 l-p:0.23076140880584717
epoch£º307	 i:7 	 global-step:6147	 l-p:0.13559986650943756
epoch£º307	 i:8 	 global-step:6148	 l-p:0.14767856895923615
epoch£º307	 i:9 	 global-step:6149	 l-p:0.1241956502199173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9370, 4.9327, 4.9360],
        [4.9370, 4.9314, 4.9351],
        [4.9370, 4.9290, 4.9256],
        [4.9370, 5.0094, 4.9386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.12775884568691254 
model_pd.l_d.mean(): -20.80028533935547 
model_pd.lagr.mean(): -20.672527313232422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4045], device='cuda:0')), ('power', tensor([-21.6082], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.12775884568691254
epoch£º308	 i:1 	 global-step:6161	 l-p:0.06683795899152756
epoch£º308	 i:2 	 global-step:6162	 l-p:0.131455659866333
epoch£º308	 i:3 	 global-step:6163	 l-p:0.1414342224597931
epoch£º308	 i:4 	 global-step:6164	 l-p:0.1456030309200287
epoch£º308	 i:5 	 global-step:6165	 l-p:0.17292383313179016
epoch£º308	 i:6 	 global-step:6166	 l-p:-0.04528186842799187
epoch£º308	 i:7 	 global-step:6167	 l-p:0.2034830003976822
epoch£º308	 i:8 	 global-step:6168	 l-p:0.12796711921691895
epoch£º308	 i:9 	 global-step:6169	 l-p:0.13206958770751953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9581, 4.9634, 4.9355],
        [4.9581, 5.1847, 5.1314],
        [4.9581, 4.9580, 4.9581],
        [4.9581, 5.3185, 5.3325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.19954589009284973 
model_pd.l_d.mean(): -18.91013526916504 
model_pd.lagr.mean(): -18.710588455200195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5300], device='cuda:0')), ('power', tensor([-19.8126], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.19954589009284973
epoch£º309	 i:1 	 global-step:6181	 l-p:-0.15787550806999207
epoch£º309	 i:2 	 global-step:6182	 l-p:0.14268489181995392
epoch£º309	 i:3 	 global-step:6183	 l-p:0.1255868822336197
epoch£º309	 i:4 	 global-step:6184	 l-p:0.13863617181777954
epoch£º309	 i:5 	 global-step:6185	 l-p:0.1358463019132614
epoch£º309	 i:6 	 global-step:6186	 l-p:0.1408572942018509
epoch£º309	 i:7 	 global-step:6187	 l-p:0.3493350148200989
epoch£º309	 i:8 	 global-step:6188	 l-p:0.2508029341697693
epoch£º309	 i:9 	 global-step:6189	 l-p:0.13565297424793243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0851, 5.0818, 5.0844],
        [5.0851, 5.0850, 5.0851],
        [5.0851, 5.3195, 5.2661],
        [5.0851, 5.5980, 5.7082]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.18316181004047394 
model_pd.l_d.mean(): -20.575090408325195 
model_pd.lagr.mean(): -20.39192771911621 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3967], device='cuda:0')), ('power', tensor([-21.3707], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.18316181004047394
epoch£º310	 i:1 	 global-step:6201	 l-p:0.0830637514591217
epoch£º310	 i:2 	 global-step:6202	 l-p:0.12323230504989624
epoch£º310	 i:3 	 global-step:6203	 l-p:0.11441940069198608
epoch£º310	 i:4 	 global-step:6204	 l-p:0.14562681317329407
epoch£º310	 i:5 	 global-step:6205	 l-p:0.13548021018505096
epoch£º310	 i:6 	 global-step:6206	 l-p:0.14604197442531586
epoch£º310	 i:7 	 global-step:6207	 l-p:0.12969155609607697
epoch£º310	 i:8 	 global-step:6208	 l-p:0.10264778137207031
epoch£º310	 i:9 	 global-step:6209	 l-p:0.13257554173469543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2563,  0.1628,  1.0000,  0.1034,
          1.0000,  0.6352, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[5.1132, 5.1563, 5.1007],
        [5.1132, 5.1526, 5.0992],
        [5.1132, 5.1210, 5.0953],
        [5.1132, 5.1807, 5.1140]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.11363054066896439 
model_pd.l_d.mean(): -18.916181564331055 
model_pd.lagr.mean(): -18.80255126953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4778], device='cuda:0')), ('power', tensor([-19.7648], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.11363054066896439
epoch£º311	 i:1 	 global-step:6221	 l-p:0.18563678860664368
epoch£º311	 i:2 	 global-step:6222	 l-p:0.15489035844802856
epoch£º311	 i:3 	 global-step:6223	 l-p:0.12950167059898376
epoch£º311	 i:4 	 global-step:6224	 l-p:0.12005596607923508
epoch£º311	 i:5 	 global-step:6225	 l-p:0.11064665764570236
epoch£º311	 i:6 	 global-step:6226	 l-p:0.13470065593719482
epoch£º311	 i:7 	 global-step:6227	 l-p:0.20001371204853058
epoch£º311	 i:8 	 global-step:6228	 l-p:0.14131002128124237
epoch£º311	 i:9 	 global-step:6229	 l-p:0.13278023898601532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0469, 5.0871, 5.0307],
        [5.0469, 5.0438, 5.0463],
        [5.0469, 5.0469, 5.0469],
        [5.0469, 5.5253, 5.6129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.22778110206127167 
model_pd.l_d.mean(): -20.58000946044922 
model_pd.lagr.mean(): -20.35222816467285 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4068], device='cuda:0')), ('power', tensor([-21.3861], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:0.22778110206127167
epoch£º312	 i:1 	 global-step:6241	 l-p:0.1000860407948494
epoch£º312	 i:2 	 global-step:6242	 l-p:0.10529433935880661
epoch£º312	 i:3 	 global-step:6243	 l-p:0.170231431722641
epoch£º312	 i:4 	 global-step:6244	 l-p:0.1417539119720459
epoch£º312	 i:5 	 global-step:6245	 l-p:-0.059171583503484726
epoch£º312	 i:6 	 global-step:6246	 l-p:0.15051190555095673
epoch£º312	 i:7 	 global-step:6247	 l-p:0.1308632791042328
epoch£º312	 i:8 	 global-step:6248	 l-p:0.1364530473947525
epoch£º312	 i:9 	 global-step:6249	 l-p:0.015914957970380783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9528, 4.9433, 4.9443],
        [4.9528, 5.5259, 5.6948],
        [4.9528, 4.9850, 4.9298],
        [4.9528, 4.9475, 4.9513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.15298952162265778 
model_pd.l_d.mean(): -20.52174949645996 
model_pd.lagr.mean(): -20.368759155273438 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4418], device='cuda:0')), ('power', tensor([-21.3631], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.15298952162265778
epoch£º313	 i:1 	 global-step:6261	 l-p:0.13553793728351593
epoch£º313	 i:2 	 global-step:6262	 l-p:-0.03865234926342964
epoch£º313	 i:3 	 global-step:6263	 l-p:0.7367061972618103
epoch£º313	 i:4 	 global-step:6264	 l-p:0.1546139121055603
epoch£º313	 i:5 	 global-step:6265	 l-p:0.12468934059143066
epoch£º313	 i:6 	 global-step:6266	 l-p:0.23373430967330933
epoch£º313	 i:7 	 global-step:6267	 l-p:0.13651227951049805
epoch£º313	 i:8 	 global-step:6268	 l-p:0.10005263239145279
epoch£º313	 i:9 	 global-step:6269	 l-p:0.07559792697429657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8843, 4.8843, 4.8843],
        [4.8843, 4.8773, 4.8819],
        [4.8843, 4.8738, 4.8675],
        [4.8843, 4.8786, 4.8604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.09477831423282623 
model_pd.l_d.mean(): -19.849533081054688 
model_pd.lagr.mean(): -19.7547550201416 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5428], device='cuda:0')), ('power', tensor([-20.7829], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.09477831423282623
epoch£º314	 i:1 	 global-step:6281	 l-p:0.14411096274852753
epoch£º314	 i:2 	 global-step:6282	 l-p:0.1708548367023468
epoch£º314	 i:3 	 global-step:6283	 l-p:0.13008038699626923
epoch£º314	 i:4 	 global-step:6284	 l-p:0.05924247205257416
epoch£º314	 i:5 	 global-step:6285	 l-p:-0.004850363824516535
epoch£º314	 i:6 	 global-step:6286	 l-p:0.14269645512104034
epoch£º314	 i:7 	 global-step:6287	 l-p:0.1311320960521698
epoch£º314	 i:8 	 global-step:6288	 l-p:0.11617656797170639
epoch£º314	 i:9 	 global-step:6289	 l-p:0.14345058798789978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0934, 5.3023, 5.2394],
        [5.0934, 5.1236, 5.0740],
        [5.0934, 5.0934, 5.0934],
        [5.0934, 5.5116, 5.5544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.13035565614700317 
model_pd.l_d.mean(): -20.051822662353516 
model_pd.lagr.mean(): -19.921466827392578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4342], device='cuda:0')), ('power', tensor([-20.8765], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.13035565614700317
epoch£º315	 i:1 	 global-step:6301	 l-p:0.14134365320205688
epoch£º315	 i:2 	 global-step:6302	 l-p:0.11137060075998306
epoch£º315	 i:3 	 global-step:6303	 l-p:0.1298026293516159
epoch£º315	 i:4 	 global-step:6304	 l-p:0.1336374431848526
epoch£º315	 i:5 	 global-step:6305	 l-p:0.2135787159204483
epoch£º315	 i:6 	 global-step:6306	 l-p:0.2250269055366516
epoch£º315	 i:7 	 global-step:6307	 l-p:0.15600821375846863
epoch£º315	 i:8 	 global-step:6308	 l-p:0.11970081180334091
epoch£º315	 i:9 	 global-step:6309	 l-p:0.13527435064315796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0114, 5.0068, 5.0102],
        [5.0114, 5.0105, 5.0113],
        [5.0114, 5.0614, 4.9970],
        [5.0114, 5.0023, 5.0020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.13764797151088715 
model_pd.l_d.mean(): -20.795198440551758 
model_pd.lagr.mean(): -20.657550811767578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3845], device='cuda:0')), ('power', tensor([-21.5823], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.13764797151088715
epoch£º316	 i:1 	 global-step:6321	 l-p:0.03331506624817848
epoch£º316	 i:2 	 global-step:6322	 l-p:0.21588483452796936
epoch£º316	 i:3 	 global-step:6323	 l-p:0.14165885746479034
epoch£º316	 i:4 	 global-step:6324	 l-p:0.118743397295475
epoch£º316	 i:5 	 global-step:6325	 l-p:0.143889382481575
epoch£º316	 i:6 	 global-step:6326	 l-p:0.0908803716301918
epoch£º316	 i:7 	 global-step:6327	 l-p:0.23008204996585846
epoch£º316	 i:8 	 global-step:6328	 l-p:0.12785764038562775
epoch£º316	 i:9 	 global-step:6329	 l-p:0.018695110455155373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8443, 4.8313, 4.8316],
        [4.8443, 4.8436, 4.8443],
        [4.8443, 4.8395, 4.8150],
        [4.8443, 4.8354, 4.8189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.1098284125328064 
model_pd.l_d.mean(): -20.212331771850586 
model_pd.lagr.mean(): -20.102502822875977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5100], device='cuda:0')), ('power', tensor([-21.1185], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.1098284125328064
epoch£º317	 i:1 	 global-step:6341	 l-p:0.14434315264225006
epoch£º317	 i:2 	 global-step:6342	 l-p:0.1283539980649948
epoch£º317	 i:3 	 global-step:6343	 l-p:0.06867425888776779
epoch£º317	 i:4 	 global-step:6344	 l-p:0.14020507037639618
epoch£º317	 i:5 	 global-step:6345	 l-p:0.3812471330165863
epoch£º317	 i:6 	 global-step:6346	 l-p:0.1252792775630951
epoch£º317	 i:7 	 global-step:6347	 l-p:0.12402896583080292
epoch£º317	 i:8 	 global-step:6348	 l-p:0.14232318103313446
epoch£º317	 i:9 	 global-step:6349	 l-p:0.12552958726882935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9749, 4.9649, 4.9624],
        [4.9749, 4.9748, 4.9749],
        [4.9749, 4.9699, 4.9736],
        [4.9749, 4.9745, 4.9749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.2023753672838211 
model_pd.l_d.mean(): -20.07090950012207 
model_pd.lagr.mean(): -19.868534088134766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4783], device='cuda:0')), ('power', tensor([-20.9416], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.2023753672838211
epoch£º318	 i:1 	 global-step:6361	 l-p:0.13606247305870056
epoch£º318	 i:2 	 global-step:6362	 l-p:0.12253623455762863
epoch£º318	 i:3 	 global-step:6363	 l-p:0.1556786149740219
epoch£º318	 i:4 	 global-step:6364	 l-p:0.12850314378738403
epoch£º318	 i:5 	 global-step:6365	 l-p:0.34391555190086365
epoch£º318	 i:6 	 global-step:6366	 l-p:0.680836021900177
epoch£º318	 i:7 	 global-step:6367	 l-p:0.12608079612255096
epoch£º318	 i:8 	 global-step:6368	 l-p:0.12014823406934738
epoch£º318	 i:9 	 global-step:6369	 l-p:0.12285889685153961
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1207, 5.1230, 5.1010],
        [5.1207, 5.1466, 5.0995],
        [5.1207, 5.1207, 5.1207],
        [5.1207, 5.1207, 5.1207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.16117633879184723 
model_pd.l_d.mean(): -20.33989715576172 
model_pd.lagr.mean(): -20.178720474243164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4146], device='cuda:0')), ('power', tensor([-21.1497], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.16117633879184723
epoch£º319	 i:1 	 global-step:6381	 l-p:0.11462530493736267
epoch£º319	 i:2 	 global-step:6382	 l-p:0.12566788494586945
epoch£º319	 i:3 	 global-step:6383	 l-p:0.13377532362937927
epoch£º319	 i:4 	 global-step:6384	 l-p:0.13946223258972168
epoch£º319	 i:5 	 global-step:6385	 l-p:0.12536725401878357
epoch£º319	 i:6 	 global-step:6386	 l-p:0.005970344413071871
epoch£º319	 i:7 	 global-step:6387	 l-p:0.05117223039269447
epoch£º319	 i:8 	 global-step:6388	 l-p:0.19945137202739716
epoch£º319	 i:9 	 global-step:6389	 l-p:0.42101046442985535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9517, 4.9634, 4.9213],
        [4.9517, 5.0083, 4.9376],
        [4.9517, 4.9956, 4.9303],
        [4.9517, 4.9493, 4.9514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.1857907474040985 
model_pd.l_d.mean(): -20.017223358154297 
model_pd.lagr.mean(): -19.831432342529297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4552], device='cuda:0')), ('power', tensor([-20.8630], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.1857907474040985
epoch£º320	 i:1 	 global-step:6401	 l-p:0.1492859125137329
epoch£º320	 i:2 	 global-step:6402	 l-p:0.10506483167409897
epoch£º320	 i:3 	 global-step:6403	 l-p:0.46320900321006775
epoch£º320	 i:4 	 global-step:6404	 l-p:0.12473126500844955
epoch£º320	 i:5 	 global-step:6405	 l-p:0.15248693525791168
epoch£º320	 i:6 	 global-step:6406	 l-p:0.12915350496768951
epoch£º320	 i:7 	 global-step:6407	 l-p:0.22620616853237152
epoch£º320	 i:8 	 global-step:6408	 l-p:0.2189425379037857
epoch£º320	 i:9 	 global-step:6409	 l-p:0.141307532787323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0857, 5.0779, 5.0814],
        [5.0857, 5.4153, 5.4044],
        [5.0857, 5.0827, 5.0852],
        [5.0857, 5.0855, 5.0857]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.11611102521419525 
model_pd.l_d.mean(): -19.41951560974121 
model_pd.lagr.mean(): -19.303403854370117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4740], device='cuda:0')), ('power', tensor([-20.2736], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.11611102521419525
epoch£º321	 i:1 	 global-step:6421	 l-p:0.23269957304000854
epoch£º321	 i:2 	 global-step:6422	 l-p:0.11193368583917618
epoch£º321	 i:3 	 global-step:6423	 l-p:0.11741607636213303
epoch£º321	 i:4 	 global-step:6424	 l-p:0.12217433005571365
epoch£º321	 i:5 	 global-step:6425	 l-p:0.19045522809028625
epoch£º321	 i:6 	 global-step:6426	 l-p:0.12168744951486588
epoch£º321	 i:7 	 global-step:6427	 l-p:0.18436728417873383
epoch£º321	 i:8 	 global-step:6428	 l-p:0.1348443627357483
epoch£º321	 i:9 	 global-step:6429	 l-p:0.14438170194625854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0472, 5.8912, 6.2912],
        [5.0472, 5.0471, 5.0472],
        [5.0472, 5.0469, 5.0472],
        [5.0472, 5.0472, 5.0472]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.15927328169345856 
model_pd.l_d.mean(): -20.46190643310547 
model_pd.lagr.mean(): -20.30263328552246 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4168], device='cuda:0')), ('power', tensor([-21.2762], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.15927328169345856
epoch£º322	 i:1 	 global-step:6441	 l-p:0.14182505011558533
epoch£º322	 i:2 	 global-step:6442	 l-p:0.11698953062295914
epoch£º322	 i:3 	 global-step:6443	 l-p:0.2402617186307907
epoch£º322	 i:4 	 global-step:6444	 l-p:0.11308126151561737
epoch£º322	 i:5 	 global-step:6445	 l-p:0.1458934247493744
epoch£º322	 i:6 	 global-step:6446	 l-p:0.13463816046714783
epoch£º322	 i:7 	 global-step:6447	 l-p:0.2041269987821579
epoch£º322	 i:8 	 global-step:6448	 l-p:0.08819441497325897
epoch£º322	 i:9 	 global-step:6449	 l-p:0.10799660533666611
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9091, 4.9091, 4.9091],
        [4.9091, 4.9054, 4.9085],
        [4.9091, 5.0406, 4.9604],
        [4.9091, 4.8977, 4.9027]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.0969146341085434 
model_pd.l_d.mean(): -20.246227264404297 
model_pd.lagr.mean(): -20.14931297302246 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4893], device='cuda:0')), ('power', tensor([-21.1316], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.0969146341085434
epoch£º323	 i:1 	 global-step:6461	 l-p:0.1915043741464615
epoch£º323	 i:2 	 global-step:6462	 l-p:0.10640545934438705
epoch£º323	 i:3 	 global-step:6463	 l-p:0.4330037236213684
epoch£º323	 i:4 	 global-step:6464	 l-p:0.13577543199062347
epoch£º323	 i:5 	 global-step:6465	 l-p:0.12603391706943512
epoch£º323	 i:6 	 global-step:6466	 l-p:0.12476001679897308
epoch£º323	 i:7 	 global-step:6467	 l-p:0.12110128253698349
epoch£º323	 i:8 	 global-step:6468	 l-p:0.11953794211149216
epoch£º323	 i:9 	 global-step:6469	 l-p:0.13223594427108765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1474, 5.1474, 5.1474],
        [5.1474, 5.1474, 5.1474],
        [5.1474, 5.1419, 5.1457],
        [5.1474, 5.1488, 5.1256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.1495700180530548 
model_pd.l_d.mean(): -19.536041259765625 
model_pd.lagr.mean(): -19.386470794677734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5015], device='cuda:0')), ('power', tensor([-20.4208], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.1495700180530548
epoch£º324	 i:1 	 global-step:6481	 l-p:0.12331579625606537
epoch£º324	 i:2 	 global-step:6482	 l-p:0.1664828062057495
epoch£º324	 i:3 	 global-step:6483	 l-p:0.13139966130256653
epoch£º324	 i:4 	 global-step:6484	 l-p:0.09023258090019226
epoch£º324	 i:5 	 global-step:6485	 l-p:0.14394792914390564
epoch£º324	 i:6 	 global-step:6486	 l-p:0.12660053372383118
epoch£º324	 i:7 	 global-step:6487	 l-p:0.12496750801801682
epoch£º324	 i:8 	 global-step:6488	 l-p:-0.028692148625850677
epoch£º324	 i:9 	 global-step:6489	 l-p:0.19537471234798431
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9623, 4.9792, 4.9286],
        [4.9623, 4.9554, 4.9603],
        [4.9623, 4.9621, 4.9623],
        [4.9623, 5.0502, 4.9695]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.12827324867248535 
model_pd.l_d.mean(): -19.13180160522461 
model_pd.lagr.mean(): -19.003528594970703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5035], device='cuda:0')), ('power', tensor([-20.0111], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.12827324867248535
epoch£º325	 i:1 	 global-step:6501	 l-p:0.07422390580177307
epoch£º325	 i:2 	 global-step:6502	 l-p:0.18687888979911804
epoch£º325	 i:3 	 global-step:6503	 l-p:0.13064612448215485
epoch£º325	 i:4 	 global-step:6504	 l-p:0.1234203577041626
epoch£º325	 i:5 	 global-step:6505	 l-p:0.13484248518943787
epoch£º325	 i:6 	 global-step:6506	 l-p:0.34534820914268494
epoch£º325	 i:7 	 global-step:6507	 l-p:0.09769776463508606
epoch£º325	 i:8 	 global-step:6508	 l-p:0.06274029612541199
epoch£º325	 i:9 	 global-step:6509	 l-p:0.11828294396400452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9796, 5.0566, 4.9774],
        [4.9796, 4.9788, 4.9795],
        [4.9796, 5.7554, 6.0989],
        [4.9796, 5.0433, 4.9673]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.12736698985099792 
model_pd.l_d.mean(): -20.141937255859375 
model_pd.lagr.mean(): -20.014570236206055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4696], device='cuda:0')), ('power', tensor([-21.0049], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.12736698985099792
epoch£º326	 i:1 	 global-step:6521	 l-p:0.12643669545650482
epoch£º326	 i:2 	 global-step:6522	 l-p:0.14311456680297852
epoch£º326	 i:3 	 global-step:6523	 l-p:4.0978899002075195
epoch£º326	 i:4 	 global-step:6524	 l-p:0.1528070569038391
epoch£º326	 i:5 	 global-step:6525	 l-p:0.205793559551239
epoch£º326	 i:6 	 global-step:6526	 l-p:0.12336502969264984
epoch£º326	 i:7 	 global-step:6527	 l-p:0.13419581949710846
epoch£º326	 i:8 	 global-step:6528	 l-p:0.11338087171316147
epoch£º326	 i:9 	 global-step:6529	 l-p:0.13361987471580505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8858, 5.6449, 5.9836],
        [4.8858, 4.8857, 4.8858],
        [4.8858, 4.8700, 4.8656],
        [4.8858, 4.8849, 4.8858]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.13428105413913727 
model_pd.l_d.mean(): -20.004215240478516 
model_pd.lagr.mean(): -19.86993408203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5014], device='cuda:0')), ('power', tensor([-20.8976], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.13428105413913727
epoch£º327	 i:1 	 global-step:6541	 l-p:0.029413267970085144
epoch£º327	 i:2 	 global-step:6542	 l-p:0.12871690094470978
epoch£º327	 i:3 	 global-step:6543	 l-p:0.12507453560829163
epoch£º327	 i:4 	 global-step:6544	 l-p:0.09248080849647522
epoch£º327	 i:5 	 global-step:6545	 l-p:0.16664688289165497
epoch£º327	 i:6 	 global-step:6546	 l-p:0.22112998366355896
epoch£º327	 i:7 	 global-step:6547	 l-p:0.13432714343070984
epoch£º327	 i:8 	 global-step:6548	 l-p:0.13522382080554962
epoch£º327	 i:9 	 global-step:6549	 l-p:0.1309618502855301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9554, 4.9503, 4.9544],
        [4.9554, 4.9533, 4.9552],
        [4.9554, 5.0191, 4.9415],
        [4.9554, 5.4241, 5.5110]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.05869479849934578 
model_pd.l_d.mean(): -19.95766258239746 
model_pd.lagr.mean(): -19.898967742919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5021], device='cuda:0')), ('power', tensor([-20.8509], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.05869479849934578
epoch£º328	 i:1 	 global-step:6561	 l-p:0.12893539667129517
epoch£º328	 i:2 	 global-step:6562	 l-p:0.1151949092745781
epoch£º328	 i:3 	 global-step:6563	 l-p:0.14036139845848083
epoch£º328	 i:4 	 global-step:6564	 l-p:0.30823612213134766
epoch£º328	 i:5 	 global-step:6565	 l-p:0.11493778228759766
epoch£º328	 i:6 	 global-step:6566	 l-p:-0.029834851622581482
epoch£º328	 i:7 	 global-step:6567	 l-p:0.17896004021167755
epoch£º328	 i:8 	 global-step:6568	 l-p:0.13053037226200104
epoch£º328	 i:9 	 global-step:6569	 l-p:0.1059698760509491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1131, 5.1131, 5.1131],
        [5.1131, 5.3629, 5.3115],
        [5.1131, 5.1124, 5.1131],
        [5.1131, 5.1024, 5.1001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.1291862577199936 
model_pd.l_d.mean(): -18.886363983154297 
model_pd.lagr.mean(): -18.757177352905273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5145], device='cuda:0')), ('power', tensor([-19.7724], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.1291862577199936
epoch£º329	 i:1 	 global-step:6581	 l-p:0.11103682965040207
epoch£º329	 i:2 	 global-step:6582	 l-p:0.11477169394493103
epoch£º329	 i:3 	 global-step:6583	 l-p:0.13889993727207184
epoch£º329	 i:4 	 global-step:6584	 l-p:0.1145232766866684
epoch£º329	 i:5 	 global-step:6585	 l-p:0.1506558358669281
epoch£º329	 i:6 	 global-step:6586	 l-p:0.123267263174057
epoch£º329	 i:7 	 global-step:6587	 l-p:0.1748812049627304
epoch£º329	 i:8 	 global-step:6588	 l-p:0.10291469097137451
epoch£º329	 i:9 	 global-step:6589	 l-p:0.881841778755188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0223, 5.0216, 5.0222],
        [5.0223, 5.0213, 5.0222],
        [5.0223, 5.0217, 5.0222],
        [5.0223, 5.0223, 5.0223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.12645146250724792 
model_pd.l_d.mean(): -20.358062744140625 
model_pd.lagr.mean(): -20.231611251831055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4326], device='cuda:0')), ('power', tensor([-21.1868], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.12645146250724792
epoch£º330	 i:1 	 global-step:6601	 l-p:0.15188449621200562
epoch£º330	 i:2 	 global-step:6602	 l-p:0.14355319738388062
epoch£º330	 i:3 	 global-step:6603	 l-p:-0.09315899759531021
epoch£º330	 i:4 	 global-step:6604	 l-p:0.1528943032026291
epoch£º330	 i:5 	 global-step:6605	 l-p:1.2072198390960693
epoch£º330	 i:6 	 global-step:6606	 l-p:0.13454309105873108
epoch£º330	 i:7 	 global-step:6607	 l-p:0.11961201578378677
epoch£º330	 i:8 	 global-step:6608	 l-p:0.1275150328874588
epoch£º330	 i:9 	 global-step:6609	 l-p:0.15078727900981903
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7909, 4.8194, 4.7479],
        [4.7909, 5.4546, 5.7171],
        [4.7909, 4.7878, 4.7906],
        [4.7909, 5.3520, 5.5265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.09983853995800018 
model_pd.l_d.mean(): -20.13469696044922 
model_pd.lagr.mean(): -20.03485870361328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5360], device='cuda:0')), ('power', tensor([-21.0663], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.09983853995800018
epoch£º331	 i:1 	 global-step:6621	 l-p:0.14033596217632294
epoch£º331	 i:2 	 global-step:6622	 l-p:0.1660444438457489
epoch£º331	 i:3 	 global-step:6623	 l-p:0.134233295917511
epoch£º331	 i:4 	 global-step:6624	 l-p:0.16561278700828552
epoch£º331	 i:5 	 global-step:6625	 l-p:0.24700050055980682
epoch£º331	 i:6 	 global-step:6626	 l-p:0.13504479825496674
epoch£º331	 i:7 	 global-step:6627	 l-p:0.14367133378982544
epoch£º331	 i:8 	 global-step:6628	 l-p:0.13072335720062256
epoch£º331	 i:9 	 global-step:6629	 l-p:0.12148287147283554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0336, 5.0416, 4.9979],
        [5.0336, 5.1895, 5.1096],
        [5.0336, 5.0855, 5.0119],
        [5.0336, 5.0313, 5.0334]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.13186274468898773 
model_pd.l_d.mean(): -20.146644592285156 
model_pd.lagr.mean(): -20.014781951904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4698], device='cuda:0')), ('power', tensor([-21.0099], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.13186274468898773
epoch£º332	 i:1 	 global-step:6641	 l-p:0.1273752748966217
epoch£º332	 i:2 	 global-step:6642	 l-p:-0.1154974177479744
epoch£º332	 i:3 	 global-step:6643	 l-p:0.11221130192279816
epoch£º332	 i:4 	 global-step:6644	 l-p:0.0406595878303051
epoch£º332	 i:5 	 global-step:6645	 l-p:0.15553876757621765
epoch£º332	 i:6 	 global-step:6646	 l-p:0.13686010241508484
epoch£º332	 i:7 	 global-step:6647	 l-p:0.1925467997789383
epoch£º332	 i:8 	 global-step:6648	 l-p:0.06438452750444412
epoch£º332	 i:9 	 global-step:6649	 l-p:0.12603090703487396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9469, 4.9469, 4.9469],
        [4.9469, 5.1568, 5.0927],
        [4.9469, 4.9469, 4.9469],
        [4.9469, 4.9469, 4.9469]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.2330619990825653 
model_pd.l_d.mean(): -20.24652671813965 
model_pd.lagr.mean(): -20.013463973999023 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4690], device='cuda:0')), ('power', tensor([-21.1109], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.2330619990825653
epoch£º333	 i:1 	 global-step:6661	 l-p:0.12756292521953583
epoch£º333	 i:2 	 global-step:6662	 l-p:0.12693633139133453
epoch£º333	 i:3 	 global-step:6663	 l-p:0.04579772427678108
epoch£º333	 i:4 	 global-step:6664	 l-p:0.20562241971492767
epoch£º333	 i:5 	 global-step:6665	 l-p:0.14369891583919525
epoch£º333	 i:6 	 global-step:6666	 l-p:0.12327232211828232
epoch£º333	 i:7 	 global-step:6667	 l-p:1.229311466217041
epoch£º333	 i:8 	 global-step:6668	 l-p:0.1350281536579132
epoch£º333	 i:9 	 global-step:6669	 l-p:0.46607276797294617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0648, 5.0644, 5.0647],
        [5.0648, 5.0626, 5.0645],
        [5.0648, 5.0950, 5.0327],
        [5.0648, 5.0601, 5.0336]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.1437079906463623 
model_pd.l_d.mean(): -20.702774047851562 
model_pd.lagr.mean(): -20.559066772460938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3856], device='cuda:0')), ('power', tensor([-21.4893], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.1437079906463623
epoch£º334	 i:1 	 global-step:6681	 l-p:0.14370611310005188
epoch£º334	 i:2 	 global-step:6682	 l-p:0.13726414740085602
epoch£º334	 i:3 	 global-step:6683	 l-p:0.2044687271118164
epoch£º334	 i:4 	 global-step:6684	 l-p:0.1294116973876953
epoch£º334	 i:5 	 global-step:6685	 l-p:0.16800159215927124
epoch£º334	 i:6 	 global-step:6686	 l-p:0.12994998693466187
epoch£º334	 i:7 	 global-step:6687	 l-p:0.1597096174955368
epoch£º334	 i:8 	 global-step:6688	 l-p:0.12461118400096893
epoch£º334	 i:9 	 global-step:6689	 l-p:0.11038022488355637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1434, 5.1322, 5.1254],
        [5.1434, 5.9016, 6.2104],
        [5.1434, 5.1434, 5.1434],
        [5.1434, 5.9830, 6.3669]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.10070959478616714 
model_pd.l_d.mean(): -19.094409942626953 
model_pd.lagr.mean(): -18.99370002746582 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4990], device='cuda:0')), ('power', tensor([-19.9683], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.10070959478616714
epoch£º335	 i:1 	 global-step:6701	 l-p:0.13015373051166534
epoch£º335	 i:2 	 global-step:6702	 l-p:0.17012141644954681
epoch£º335	 i:3 	 global-step:6703	 l-p:0.16130757331848145
epoch£º335	 i:4 	 global-step:6704	 l-p:0.13611553609371185
epoch£º335	 i:5 	 global-step:6705	 l-p:0.1443232148885727
epoch£º335	 i:6 	 global-step:6706	 l-p:-0.28964194655418396
epoch£º335	 i:7 	 global-step:6707	 l-p:0.12177734076976776
epoch£º335	 i:8 	 global-step:6708	 l-p:0.1383780986070633
epoch£º335	 i:9 	 global-step:6709	 l-p:0.12472269684076309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2039,  0.1200,  1.0000,  0.0706,
          1.0000,  0.5886, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1532,  0.0820,  1.0000,  0.0439,
          1.0000,  0.5351, 31.6228]], device='cuda:0')
 pt:tensor([[5.0152, 4.9994, 4.9964],
        [5.0152, 5.0090, 4.9797],
        [5.0152, 5.2394, 5.1786],
        [5.0152, 5.0000, 4.9925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.11919619888067245 
model_pd.l_d.mean(): -19.23280906677246 
model_pd.lagr.mean(): -19.11361312866211 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4675], device='cuda:0')), ('power', tensor([-20.0767], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.11919619888067245
epoch£º336	 i:1 	 global-step:6721	 l-p:0.07048778235912323
epoch£º336	 i:2 	 global-step:6722	 l-p:0.1317676156759262
epoch£º336	 i:3 	 global-step:6723	 l-p:0.09402970969676971
epoch£º336	 i:4 	 global-step:6724	 l-p:0.142170250415802
epoch£º336	 i:5 	 global-step:6725	 l-p:0.13368457555770874
epoch£º336	 i:6 	 global-step:6726	 l-p:0.9629346132278442
epoch£º336	 i:7 	 global-step:6727	 l-p:0.12056140601634979
epoch£º336	 i:8 	 global-step:6728	 l-p:0.1423770934343338
epoch£º336	 i:9 	 global-step:6729	 l-p:-0.8149916529655457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8866, 5.7473, 6.1809],
        [4.8866, 4.8678, 4.8720],
        [4.8866, 4.9549, 4.8690],
        [4.8866, 5.3053, 5.3606]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.13545453548431396 
model_pd.l_d.mean(): -18.321659088134766 
model_pd.lagr.mean(): -18.18620491027832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5666], device='cuda:0')), ('power', tensor([-19.2512], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.13545453548431396
epoch£º337	 i:1 	 global-step:6741	 l-p:0.25843945145606995
epoch£º337	 i:2 	 global-step:6742	 l-p:0.0204519871622324
epoch£º337	 i:3 	 global-step:6743	 l-p:0.13076110184192657
epoch£º337	 i:4 	 global-step:6744	 l-p:0.10102906823158264
epoch£º337	 i:5 	 global-step:6745	 l-p:0.12955348193645477
epoch£º337	 i:6 	 global-step:6746	 l-p:0.12853948771953583
epoch£º337	 i:7 	 global-step:6747	 l-p:0.14758571982383728
epoch£º337	 i:8 	 global-step:6748	 l-p:0.14573535323143005
epoch£º337	 i:9 	 global-step:6749	 l-p:0.15776899456977844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9417, 4.9234, 4.9133],
        [4.9417, 4.9264, 4.9331],
        [4.9417, 4.9376, 4.9411],
        [4.9417, 4.9417, 4.9417]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.12395971268415451 
model_pd.l_d.mean(): -20.11937713623047 
model_pd.lagr.mean(): -19.99541664123535 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4898], device='cuda:0')), ('power', tensor([-21.0028], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.12395971268415451
epoch£º338	 i:1 	 global-step:6761	 l-p:0.12229882180690765
epoch£º338	 i:2 	 global-step:6762	 l-p:0.09170637279748917
epoch£º338	 i:3 	 global-step:6763	 l-p:0.1307210624217987
epoch£º338	 i:4 	 global-step:6764	 l-p:-0.5472745895385742
epoch£º338	 i:5 	 global-step:6765	 l-p:0.1544952392578125
epoch£º338	 i:6 	 global-step:6766	 l-p:0.10008331388235092
epoch£º338	 i:7 	 global-step:6767	 l-p:0.16950593888759613
epoch£º338	 i:8 	 global-step:6768	 l-p:0.10858224332332611
epoch£º338	 i:9 	 global-step:6769	 l-p:0.14903514087200165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0490, 5.6074, 5.7550],
        [5.0490, 5.0327, 5.0326],
        [5.0490, 5.0349, 5.0403],
        [5.0490, 5.0490, 5.0490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.1250862330198288 
model_pd.l_d.mean(): -20.251562118530273 
model_pd.lagr.mean(): -20.126476287841797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4488], device='cuda:0')), ('power', tensor([-21.0951], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.1250862330198288
epoch£º339	 i:1 	 global-step:6781	 l-p:0.1367771029472351
epoch£º339	 i:2 	 global-step:6782	 l-p:0.1298702508211136
epoch£º339	 i:3 	 global-step:6783	 l-p:3.6357498168945312
epoch£º339	 i:4 	 global-step:6784	 l-p:0.1577295958995819
epoch£º339	 i:5 	 global-step:6785	 l-p:-0.0416342169046402
epoch£º339	 i:6 	 global-step:6786	 l-p:0.14544135332107544
epoch£º339	 i:7 	 global-step:6787	 l-p:0.12566187977790833
epoch£º339	 i:8 	 global-step:6788	 l-p:0.15243379771709442
epoch£º339	 i:9 	 global-step:6789	 l-p:-9.82070541381836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0734, 5.4494, 5.4641],
        [5.0734, 5.0715, 5.0732],
        [5.0734, 5.0733, 5.0734],
        [5.0734, 5.0585, 5.0486]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.12134790420532227 
model_pd.l_d.mean(): -19.78070068359375 
model_pd.lagr.mean(): -19.659353256225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4791], device='cuda:0')), ('power', tensor([-20.6468], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.12134790420532227
epoch£º340	 i:1 	 global-step:6801	 l-p:0.1340397596359253
epoch£º340	 i:2 	 global-step:6802	 l-p:0.2538619041442871
epoch£º340	 i:3 	 global-step:6803	 l-p:0.1394600123167038
epoch£º340	 i:4 	 global-step:6804	 l-p:0.1264442503452301
epoch£º340	 i:5 	 global-step:6805	 l-p:0.11026781797409058
epoch£º340	 i:6 	 global-step:6806	 l-p:0.20733679831027985
epoch£º340	 i:7 	 global-step:6807	 l-p:0.12318626046180725
epoch£º340	 i:8 	 global-step:6808	 l-p:0.1765725314617157
epoch£º340	 i:9 	 global-step:6809	 l-p:0.1168648898601532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1295, 5.1288, 5.1294],
        [5.1295, 5.1243, 5.0952],
        [5.1295, 5.6030, 5.6798],
        [5.1295, 5.6206, 5.7102]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.12895619869232178 
model_pd.l_d.mean(): -19.941883087158203 
model_pd.lagr.mean(): -19.81292724609375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4605], device='cuda:0')), ('power', tensor([-20.7917], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.12895619869232178
epoch£º341	 i:1 	 global-step:6821	 l-p:0.1443304866552353
epoch£º341	 i:2 	 global-step:6822	 l-p:0.13637712597846985
epoch£º341	 i:3 	 global-step:6823	 l-p:0.11921849101781845
epoch£º341	 i:4 	 global-step:6824	 l-p:0.12960931658744812
epoch£º341	 i:5 	 global-step:6825	 l-p:0.12088309228420258
epoch£º341	 i:6 	 global-step:6826	 l-p:0.007187905255705118
epoch£º341	 i:7 	 global-step:6827	 l-p:0.14743094146251678
epoch£º341	 i:8 	 global-step:6828	 l-p:0.09508588910102844
epoch£º341	 i:9 	 global-step:6829	 l-p:0.3345767855644226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9341, 4.9341, 4.9341],
        [4.9341, 5.4529, 5.5790],
        [4.9341, 5.6484, 5.9395],
        [4.9341, 5.4538, 5.5806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.14836706221103668 
model_pd.l_d.mean(): -20.561180114746094 
model_pd.lagr.mean(): -20.412813186645508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4485], device='cuda:0')), ('power', tensor([-21.4102], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.14836706221103668
epoch£º342	 i:1 	 global-step:6841	 l-p:0.5103751420974731
epoch£º342	 i:2 	 global-step:6842	 l-p:0.14263494312763214
epoch£º342	 i:3 	 global-step:6843	 l-p:0.12596261501312256
epoch£º342	 i:4 	 global-step:6844	 l-p:0.133395716547966
epoch£º342	 i:5 	 global-step:6845	 l-p:0.3186298608779907
epoch£º342	 i:6 	 global-step:6846	 l-p:0.047569260001182556
epoch£º342	 i:7 	 global-step:6847	 l-p:0.08289065212011337
epoch£º342	 i:8 	 global-step:6848	 l-p:0.06982564181089401
epoch£º342	 i:9 	 global-step:6849	 l-p:0.13743279874324799
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0527, 5.6048, 5.7467],
        [5.0527, 5.0527, 5.0527],
        [5.0527, 5.7950, 6.0984],
        [5.0527, 5.0396, 5.0465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.13640539348125458 
model_pd.l_d.mean(): -20.589139938354492 
model_pd.lagr.mean(): -20.452733993530273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4047], device='cuda:0')), ('power', tensor([-21.3933], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.13640539348125458
epoch£º343	 i:1 	 global-step:6861	 l-p:0.13230085372924805
epoch£º343	 i:2 	 global-step:6862	 l-p:0.13317866623401642
epoch£º343	 i:3 	 global-step:6863	 l-p:0.1233573853969574
epoch£º343	 i:4 	 global-step:6864	 l-p:0.13264121115207672
epoch£º343	 i:5 	 global-step:6865	 l-p:-0.0652778148651123
epoch£º343	 i:6 	 global-step:6866	 l-p:0.13071952760219574
epoch£º343	 i:7 	 global-step:6867	 l-p:-0.785496175289154
epoch£º343	 i:8 	 global-step:6868	 l-p:0.12766508758068085
epoch£º343	 i:9 	 global-step:6869	 l-p:0.13037054240703583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228]], device='cuda:0')
 pt:tensor([[4.9952, 5.7172, 6.0085],
        [4.9952, 5.5489, 5.6970],
        [4.9952, 4.9795, 4.9576],
        [4.9952, 4.9782, 4.9597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.14524666965007782 
model_pd.l_d.mean(): -20.615793228149414 
model_pd.lagr.mean(): -20.47054672241211 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4187], device='cuda:0')), ('power', tensor([-21.4349], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.14524666965007782
epoch£º344	 i:1 	 global-step:6881	 l-p:0.11184713244438171
epoch£º344	 i:2 	 global-step:6882	 l-p:0.13258729875087738
epoch£º344	 i:3 	 global-step:6883	 l-p:0.12786594033241272
epoch£º344	 i:4 	 global-step:6884	 l-p:0.10230923444032669
epoch£º344	 i:5 	 global-step:6885	 l-p:0.15071140229701996
epoch£º344	 i:6 	 global-step:6886	 l-p:0.13313715159893036
epoch£º344	 i:7 	 global-step:6887	 l-p:0.08925765752792358
epoch£º344	 i:8 	 global-step:6888	 l-p:0.14800521731376648
epoch£º344	 i:9 	 global-step:6889	 l-p:-0.20438407361507416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9002, 4.8888, 4.8966],
        [4.9002, 5.5994, 5.8804],
        [4.9002, 4.8864, 4.8948],
        [4.9002, 4.9561, 4.8683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.12366670370101929 
model_pd.l_d.mean(): -19.352432250976562 
model_pd.lagr.mean(): -19.2287654876709 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4971], device='cuda:0')), ('power', tensor([-20.2291], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.12366670370101929
epoch£º345	 i:1 	 global-step:6901	 l-p:0.13964290916919708
epoch£º345	 i:2 	 global-step:6902	 l-p:0.11924455314874649
epoch£º345	 i:3 	 global-step:6903	 l-p:0.14549995958805084
epoch£º345	 i:4 	 global-step:6904	 l-p:0.11782094091176987
epoch£º345	 i:5 	 global-step:6905	 l-p:4.220385551452637
epoch£º345	 i:6 	 global-step:6906	 l-p:0.08327134698629379
epoch£º345	 i:7 	 global-step:6907	 l-p:0.18227557837963104
epoch£º345	 i:8 	 global-step:6908	 l-p:0.13218814134597778
epoch£º345	 i:9 	 global-step:6909	 l-p:0.12327311933040619
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0552, 5.0486, 5.0538],
        [5.0552, 5.0551, 5.0552],
        [5.0552, 5.0399, 5.0468],
        [5.0552, 5.1697, 5.0793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.1438828557729721 
model_pd.l_d.mean(): -19.7391414642334 
model_pd.lagr.mean(): -19.595258712768555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5066], device='cuda:0')), ('power', tensor([-20.6329], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.1438828557729721
epoch£º346	 i:1 	 global-step:6921	 l-p:0.1307821422815323
epoch£º346	 i:2 	 global-step:6922	 l-p:0.14052465558052063
epoch£º346	 i:3 	 global-step:6923	 l-p:0.21368812024593353
epoch£º346	 i:4 	 global-step:6924	 l-p:0.130241259932518
epoch£º346	 i:5 	 global-step:6925	 l-p:0.1052543967962265
epoch£º346	 i:6 	 global-step:6926	 l-p:0.12169992178678513
epoch£º346	 i:7 	 global-step:6927	 l-p:0.12889936566352844
epoch£º346	 i:8 	 global-step:6928	 l-p:0.13574056327342987
epoch£º346	 i:9 	 global-step:6929	 l-p:-0.05840477719902992
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0020, 5.0020, 5.0020],
        [5.0020, 5.0711, 4.9820],
        [5.0020, 5.7835, 6.1275],
        [5.0020, 4.9989, 5.0017]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.12156684696674347 
model_pd.l_d.mean(): -19.81184959411621 
model_pd.lagr.mean(): -19.690282821655273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5083], device='cuda:0')), ('power', tensor([-20.7088], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:0.12156684696674347
epoch£º347	 i:1 	 global-step:6941	 l-p:0.08824155479669571
epoch£º347	 i:2 	 global-step:6942	 l-p:0.09221168607473373
epoch£º347	 i:3 	 global-step:6943	 l-p:0.0908110961318016
epoch£º347	 i:4 	 global-step:6944	 l-p:0.127938911318779
epoch£º347	 i:5 	 global-step:6945	 l-p:0.17513370513916016
epoch£º347	 i:6 	 global-step:6946	 l-p:0.305067777633667
epoch£º347	 i:7 	 global-step:6947	 l-p:0.12592874467372894
epoch£º347	 i:8 	 global-step:6948	 l-p:0.11960925906896591
epoch£º347	 i:9 	 global-step:6949	 l-p:0.13722917437553406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0346, 5.0140, 5.0145],
        [5.0346, 5.0192, 5.0268],
        [5.0346, 5.0145, 5.0171],
        [5.0346, 5.0346, 5.0346]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.13712598383426666 
model_pd.l_d.mean(): -20.264883041381836 
model_pd.lagr.mean(): -20.127756118774414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4374], device='cuda:0')), ('power', tensor([-21.0968], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.13712598383426666
epoch£º348	 i:1 	 global-step:6961	 l-p:0.047636259347200394
epoch£º348	 i:2 	 global-step:6962	 l-p:0.16593047976493835
epoch£º348	 i:3 	 global-step:6963	 l-p:0.19478601217269897
epoch£º348	 i:4 	 global-step:6964	 l-p:0.12802816927433014
epoch£º348	 i:5 	 global-step:6965	 l-p:0.11886733770370483
epoch£º348	 i:6 	 global-step:6966	 l-p:0.14339111745357513
epoch£º348	 i:7 	 global-step:6967	 l-p:1.515536904335022
epoch£º348	 i:8 	 global-step:6968	 l-p:0.1364797055721283
epoch£º348	 i:9 	 global-step:6969	 l-p:-0.001566049992106855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0138, 5.0138, 5.0138],
        [5.0138, 5.0128, 5.0138],
        [5.0138, 5.0125, 5.0138],
        [5.0138, 4.9980, 5.0060]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.13541173934936523 
model_pd.l_d.mean(): -18.753395080566406 
model_pd.lagr.mean(): -18.617982864379883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5361], device='cuda:0')), ('power', tensor([-19.6593], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.13541173934936523
epoch£º349	 i:1 	 global-step:6981	 l-p:0.12554673850536346
epoch£º349	 i:2 	 global-step:6982	 l-p:0.12823913991451263
epoch£º349	 i:3 	 global-step:6983	 l-p:0.14511151611804962
epoch£º349	 i:4 	 global-step:6984	 l-p:0.12161491066217422
epoch£º349	 i:5 	 global-step:6985	 l-p:0.1414073407649994
epoch£º349	 i:6 	 global-step:6986	 l-p:0.15278711915016174
epoch£º349	 i:7 	 global-step:6987	 l-p:0.4640209972858429
epoch£º349	 i:8 	 global-step:6988	 l-p:0.13476687669754028
epoch£º349	 i:9 	 global-step:6989	 l-p:0.053515899926424026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8622, 4.8608, 4.8621],
        [4.8622, 5.3080, 5.3837],
        [4.8622, 5.1672, 5.1484],
        [4.8622, 5.3168, 5.3991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.19665828347206116 
model_pd.l_d.mean(): -20.32274055480957 
model_pd.lagr.mean(): -20.126081466674805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5010], device='cuda:0')), ('power', tensor([-21.2216], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.19665828347206116
epoch£º350	 i:1 	 global-step:7001	 l-p:0.09616772085428238
epoch£º350	 i:2 	 global-step:7002	 l-p:0.04767609387636185
epoch£º350	 i:3 	 global-step:7003	 l-p:0.11544615775346756
epoch£º350	 i:4 	 global-step:7004	 l-p:0.16968093812465668
epoch£º350	 i:5 	 global-step:7005	 l-p:0.13124659657478333
epoch£º350	 i:6 	 global-step:7006	 l-p:0.11879619210958481
epoch£º350	 i:7 	 global-step:7007	 l-p:1.9507780075073242
epoch£º350	 i:8 	 global-step:7008	 l-p:0.13226985931396484
epoch£º350	 i:9 	 global-step:7009	 l-p:0.12037967145442963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0771, 5.0574, 5.0608],
        [5.0771, 5.0564, 5.0510],
        [5.0771, 5.2325, 5.1456],
        [5.0771, 5.2515, 5.1691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.3148767054080963 
model_pd.l_d.mean(): -20.003297805786133 
model_pd.lagr.mean(): -19.68842124938965 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4734], device='cuda:0')), ('power', tensor([-20.8676], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:0.3148767054080963
epoch£º351	 i:1 	 global-step:7021	 l-p:0.23601822555065155
epoch£º351	 i:2 	 global-step:7022	 l-p:0.09991991519927979
epoch£º351	 i:3 	 global-step:7023	 l-p:0.13289014995098114
epoch£º351	 i:4 	 global-step:7024	 l-p:0.11989086866378784
epoch£º351	 i:5 	 global-step:7025	 l-p:0.12804125249385834
epoch£º351	 i:6 	 global-step:7026	 l-p:0.14340835809707642
epoch£º351	 i:7 	 global-step:7027	 l-p:0.11614177376031876
epoch£º351	 i:8 	 global-step:7028	 l-p:0.2978193461894989
epoch£º351	 i:9 	 global-step:7029	 l-p:0.12624627351760864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0369, 5.0169, 5.0228],
        [5.0369, 5.1167, 5.0244],
        [5.0369, 5.0154, 5.0022],
        [5.0369, 5.2448, 5.1731]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): -0.02329936996102333 
model_pd.l_d.mean(): -20.794654846191406 
model_pd.lagr.mean(): -20.817955017089844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3829], device='cuda:0')), ('power', tensor([-21.5801], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:-0.02329936996102333
epoch£º352	 i:1 	 global-step:7041	 l-p:0.15429414808750153
epoch£º352	 i:2 	 global-step:7042	 l-p:0.13455234467983246
epoch£º352	 i:3 	 global-step:7043	 l-p:0.13904529809951782
epoch£º352	 i:4 	 global-step:7044	 l-p:0.13594549894332886
epoch£º352	 i:5 	 global-step:7045	 l-p:0.16822965443134308
epoch£º352	 i:6 	 global-step:7046	 l-p:0.176927387714386
epoch£º352	 i:7 	 global-step:7047	 l-p:0.11011525988578796
epoch£º352	 i:8 	 global-step:7048	 l-p:0.13134387135505676
epoch£º352	 i:9 	 global-step:7049	 l-p:0.1152610257267952
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1860, 5.1810, 5.1852],
        [5.1860, 5.1841, 5.1859],
        [5.1860, 5.1843, 5.1859],
        [5.1860, 5.1860, 5.1860]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.12279810756444931 
model_pd.l_d.mean(): -20.2752628326416 
model_pd.lagr.mean(): -20.152463912963867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3997], device='cuda:0')), ('power', tensor([-21.0684], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.12279810756444931
epoch£º353	 i:1 	 global-step:7061	 l-p:0.12834586203098297
epoch£º353	 i:2 	 global-step:7062	 l-p:0.14315520226955414
epoch£º353	 i:3 	 global-step:7063	 l-p:0.13671015202999115
epoch£º353	 i:4 	 global-step:7064	 l-p:0.1065300703048706
epoch£º353	 i:5 	 global-step:7065	 l-p:0.1549007147550583
epoch£º353	 i:6 	 global-step:7066	 l-p:0.1422642171382904
epoch£º353	 i:7 	 global-step:7067	 l-p:0.1397303193807602
epoch£º353	 i:8 	 global-step:7068	 l-p:-1.9710148572921753
epoch£º353	 i:9 	 global-step:7069	 l-p:0.1176479309797287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0235, 5.0044, 5.0123],
        [5.0235, 5.2824, 5.2319],
        [5.0235, 5.8384, 6.2102],
        [5.0235, 5.0179, 5.0226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.12673503160476685 
model_pd.l_d.mean(): -20.583749771118164 
model_pd.lagr.mean(): -20.457014083862305 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4006], device='cuda:0')), ('power', tensor([-21.3835], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.12673503160476685
epoch£º354	 i:1 	 global-step:7081	 l-p:0.26388728618621826
epoch£º354	 i:2 	 global-step:7082	 l-p:0.08031095564365387
epoch£º354	 i:3 	 global-step:7083	 l-p:0.1839635670185089
epoch£º354	 i:4 	 global-step:7084	 l-p:0.14142902195453644
epoch£º354	 i:5 	 global-step:7085	 l-p:0.13550035655498505
epoch£º354	 i:6 	 global-step:7086	 l-p:0.14448274672031403
epoch£º354	 i:7 	 global-step:7087	 l-p:0.1391918957233429
epoch£º354	 i:8 	 global-step:7088	 l-p:0.12475356459617615
epoch£º354	 i:9 	 global-step:7089	 l-p:-0.007296700496226549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9837, 5.7912, 6.1603],
        [4.9837, 4.9697, 4.9786],
        [4.9837, 4.9804, 4.9834],
        [4.9837, 4.9825, 4.9837]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.1315654069185257 
model_pd.l_d.mean(): -19.987823486328125 
model_pd.lagr.mean(): -19.856258392333984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4758], device='cuda:0')), ('power', tensor([-20.8544], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.1315654069185257
epoch£º355	 i:1 	 global-step:7101	 l-p:0.12904462218284607
epoch£º355	 i:2 	 global-step:7102	 l-p:0.3052542507648468
epoch£º355	 i:3 	 global-step:7103	 l-p:0.12469262629747391
epoch£º355	 i:4 	 global-step:7104	 l-p:0.1491788923740387
epoch£º355	 i:5 	 global-step:7105	 l-p:0.16926299035549164
epoch£º355	 i:6 	 global-step:7106	 l-p:0.1509292721748352
epoch£º355	 i:7 	 global-step:7107	 l-p:0.08658580482006073
epoch£º355	 i:8 	 global-step:7108	 l-p:0.14047487080097198
epoch£º355	 i:9 	 global-step:7109	 l-p:0.09073811024427414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8278, 4.7959, 4.7956],
        [4.8278, 5.2263, 5.2697],
        [4.8278, 4.8278, 4.8278],
        [4.8278, 4.7955, 4.7931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.16339854896068573 
model_pd.l_d.mean(): -19.734956741333008 
model_pd.lagr.mean(): -19.571557998657227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5175], device='cuda:0')), ('power', tensor([-20.6400], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.16339854896068573
epoch£º356	 i:1 	 global-step:7121	 l-p:0.15056075155735016
epoch£º356	 i:2 	 global-step:7122	 l-p:0.14757929742336273
epoch£º356	 i:3 	 global-step:7123	 l-p:0.19706130027770996
epoch£º356	 i:4 	 global-step:7124	 l-p:0.07768870890140533
epoch£º356	 i:5 	 global-step:7125	 l-p:0.10827726870775223
epoch£º356	 i:6 	 global-step:7126	 l-p:0.18055827915668488
epoch£º356	 i:7 	 global-step:7127	 l-p:0.06555987894535065
epoch£º356	 i:8 	 global-step:7128	 l-p:0.08532530814409256
epoch£º356	 i:9 	 global-step:7129	 l-p:0.14696408808231354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9395, 5.6815, 5.9949],
        [4.9395, 4.9377, 4.9394],
        [4.9395, 4.9311, 4.9378],
        [4.9395, 4.9137, 4.8885]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 0.49690213799476624 
model_pd.l_d.mean(): -19.850553512573242 
model_pd.lagr.mean(): -19.35365104675293 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5374], device='cuda:0')), ('power', tensor([-20.7783], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:0.49690213799476624
epoch£º357	 i:1 	 global-step:7141	 l-p:0.1531895250082016
epoch£º357	 i:2 	 global-step:7142	 l-p:0.13576142489910126
epoch£º357	 i:3 	 global-step:7143	 l-p:0.292935311794281
epoch£º357	 i:4 	 global-step:7144	 l-p:0.0456063412129879
epoch£º357	 i:5 	 global-step:7145	 l-p:0.1364157497882843
epoch£º357	 i:6 	 global-step:7146	 l-p:0.13080807030200958
epoch£º357	 i:7 	 global-step:7147	 l-p:0.12610039114952087
epoch£º357	 i:8 	 global-step:7148	 l-p:0.11488895118236542
epoch£º357	 i:9 	 global-step:7149	 l-p:0.12749409675598145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9548, 4.9540, 4.9548],
        [4.9548, 4.9548, 4.9548],
        [4.9548, 4.9513, 4.8891],
        [4.9548, 5.5295, 5.6969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.07508239895105362 
model_pd.l_d.mean(): -20.4558162689209 
model_pd.lagr.mean(): -20.380733489990234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4525], device='cuda:0')), ('power', tensor([-21.3070], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.07508239895105362
epoch£º358	 i:1 	 global-step:7161	 l-p:0.022129425778985023
epoch£º358	 i:2 	 global-step:7162	 l-p:0.1546323597431183
epoch£º358	 i:3 	 global-step:7163	 l-p:0.13281236588954926
epoch£º358	 i:4 	 global-step:7164	 l-p:0.09373238682746887
epoch£º358	 i:5 	 global-step:7165	 l-p:0.12962527573108673
epoch£º358	 i:6 	 global-step:7166	 l-p:0.1475345939397812
epoch£º358	 i:7 	 global-step:7167	 l-p:0.13403552770614624
epoch£º358	 i:8 	 global-step:7168	 l-p:0.3189575970172882
epoch£º358	 i:9 	 global-step:7169	 l-p:0.19580909609794617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9728, 4.9540, 4.9640],
        [4.9728, 4.9728, 4.9728],
        [4.9728, 4.9557, 4.9115],
        [4.9728, 4.9728, 4.9728]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.1417706161737442 
model_pd.l_d.mean(): -20.129348754882812 
model_pd.lagr.mean(): -19.987577438354492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4770], device='cuda:0')), ('power', tensor([-20.9998], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.1417706161737442
epoch£º359	 i:1 	 global-step:7181	 l-p:0.12845294177532196
epoch£º359	 i:2 	 global-step:7182	 l-p:0.13240976631641388
epoch£º359	 i:3 	 global-step:7183	 l-p:0.12434054166078568
epoch£º359	 i:4 	 global-step:7184	 l-p:-1.7778496742248535
epoch£º359	 i:5 	 global-step:7185	 l-p:0.14505739510059357
epoch£º359	 i:6 	 global-step:7186	 l-p:0.3163791298866272
epoch£º359	 i:7 	 global-step:7187	 l-p:0.09696996212005615
epoch£º359	 i:8 	 global-step:7188	 l-p:0.1730356216430664
epoch£º359	 i:9 	 global-step:7189	 l-p:0.14312924444675446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8936, 4.8623, 4.8404],
        [4.8936, 4.8628, 4.8386],
        [4.8936, 4.8611, 4.8579],
        [4.8936, 4.8936, 4.8936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.1324431151151657 
model_pd.l_d.mean(): -19.976381301879883 
model_pd.lagr.mean(): -19.84393882751465 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5272], device='cuda:0')), ('power', tensor([-20.8960], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.1324431151151657
epoch£º360	 i:1 	 global-step:7201	 l-p:0.11617463827133179
epoch£º360	 i:2 	 global-step:7202	 l-p:0.1953914910554886
epoch£º360	 i:3 	 global-step:7203	 l-p:0.05474286898970604
epoch£º360	 i:4 	 global-step:7204	 l-p:0.17690511047840118
epoch£º360	 i:5 	 global-step:7205	 l-p:0.15491054952144623
epoch£º360	 i:6 	 global-step:7206	 l-p:0.14782677590847015
epoch£º360	 i:7 	 global-step:7207	 l-p:0.0863790512084961
epoch£º360	 i:8 	 global-step:7208	 l-p:0.1309477686882019
epoch£º360	 i:9 	 global-step:7209	 l-p:0.1399003565311432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8672, 4.8672, 4.8672],
        [4.8672, 4.8672, 4.8672],
        [4.8672, 4.8670, 4.8672],
        [4.8672, 4.8656, 4.8671]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.17173856496810913 
model_pd.l_d.mean(): -20.47639274597168 
model_pd.lagr.mean(): -20.304655075073242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4877], device='cuda:0')), ('power', tensor([-21.3644], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.17173856496810913
epoch£º361	 i:1 	 global-step:7221	 l-p:0.18508069217205048
epoch£º361	 i:2 	 global-step:7222	 l-p:1.108981966972351
epoch£º361	 i:3 	 global-step:7223	 l-p:0.07616522908210754
epoch£º361	 i:4 	 global-step:7224	 l-p:0.11147499084472656
epoch£º361	 i:5 	 global-step:7225	 l-p:0.11447244137525558
epoch£º361	 i:6 	 global-step:7226	 l-p:0.12765458226203918
epoch£º361	 i:7 	 global-step:7227	 l-p:0.13176178932189941
epoch£º361	 i:8 	 global-step:7228	 l-p:0.6697561740875244
epoch£º361	 i:9 	 global-step:7229	 l-p:0.24072185158729553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1136, 5.0959, 5.0616],
        [5.1136, 5.1135, 5.1136],
        [5.1136, 5.1135, 5.1136],
        [5.1136, 5.5512, 5.6013]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.12969881296157837 
model_pd.l_d.mean(): -20.1932430267334 
model_pd.lagr.mean(): -20.06354331970215 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4370], device='cuda:0')), ('power', tensor([-21.0235], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.12969881296157837
epoch£º362	 i:1 	 global-step:7241	 l-p:0.17805586755275726
epoch£º362	 i:2 	 global-step:7242	 l-p:0.13305453956127167
epoch£º362	 i:3 	 global-step:7243	 l-p:0.13565683364868164
epoch£º362	 i:4 	 global-step:7244	 l-p:0.16604070365428925
epoch£º362	 i:5 	 global-step:7245	 l-p:0.12957467138767242
epoch£º362	 i:6 	 global-step:7246	 l-p:0.13942405581474304
epoch£º362	 i:7 	 global-step:7247	 l-p:0.1330011934041977
epoch£º362	 i:8 	 global-step:7248	 l-p:0.12269630283117294
epoch£º362	 i:9 	 global-step:7249	 l-p:0.13632866740226746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1303, 5.1051, 5.1039],
        [5.1303, 5.1297, 5.1303],
        [5.1303, 5.1289, 5.1302],
        [5.1303, 5.1244, 5.0718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.19604042172431946 
model_pd.l_d.mean(): -20.531389236450195 
model_pd.lagr.mean(): -20.33534812927246 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3913], device='cuda:0')), ('power', tensor([-21.3206], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.19604042172431946
epoch£º363	 i:1 	 global-step:7261	 l-p:0.15056738257408142
epoch£º363	 i:2 	 global-step:7262	 l-p:0.08434487879276276
epoch£º363	 i:3 	 global-step:7263	 l-p:0.1146346852183342
epoch£º363	 i:4 	 global-step:7264	 l-p:0.12737241387367249
epoch£º363	 i:5 	 global-step:7265	 l-p:0.14283697307109833
epoch£º363	 i:6 	 global-step:7266	 l-p:0.1167459636926651
epoch£º363	 i:7 	 global-step:7267	 l-p:0.1580268293619156
epoch£º363	 i:8 	 global-step:7268	 l-p:0.12051942199468613
epoch£º363	 i:9 	 global-step:7269	 l-p:0.12390624731779099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1237, 6.0252, 6.4656],
        [5.1237, 5.0978, 5.0969],
        [5.1237, 5.1236, 5.1237],
        [5.1237, 5.1269, 5.0627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.1492297649383545 
model_pd.l_d.mean(): -19.18453025817871 
model_pd.lagr.mean(): -19.035301208496094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4681], device='cuda:0')), ('power', tensor([-20.0281], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.1492297649383545
epoch£º364	 i:1 	 global-step:7281	 l-p:0.1216544434428215
epoch£º364	 i:2 	 global-step:7282	 l-p:-0.1725756675004959
epoch£º364	 i:3 	 global-step:7283	 l-p:0.11971092224121094
epoch£º364	 i:4 	 global-step:7284	 l-p:0.1387491524219513
epoch£º364	 i:5 	 global-step:7285	 l-p:0.09231521189212799
epoch£º364	 i:6 	 global-step:7286	 l-p:0.06248830631375313
epoch£º364	 i:7 	 global-step:7287	 l-p:0.17702975869178772
epoch£º364	 i:8 	 global-step:7288	 l-p:0.13269411027431488
epoch£º364	 i:9 	 global-step:7289	 l-p:0.04214395582675934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8745, 4.8745, 4.8745],
        [4.8745, 4.8477, 4.7993],
        [4.8745, 4.8386, 4.8158],
        [4.8745, 4.8679, 4.8735]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.09412578493356705 
model_pd.l_d.mean(): -20.609399795532227 
model_pd.lagr.mean(): -20.515274047851562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4618], device='cuda:0')), ('power', tensor([-21.4731], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.09412578493356705
epoch£º365	 i:1 	 global-step:7301	 l-p:0.17349973320960999
epoch£º365	 i:2 	 global-step:7302	 l-p:0.1486501395702362
epoch£º365	 i:3 	 global-step:7303	 l-p:0.10600507259368896
epoch£º365	 i:4 	 global-step:7304	 l-p:0.14362798631191254
epoch£º365	 i:5 	 global-step:7305	 l-p:0.13603851199150085
epoch£º365	 i:6 	 global-step:7306	 l-p:0.4252977669239044
epoch£º365	 i:7 	 global-step:7307	 l-p:0.10811704397201538
epoch£º365	 i:8 	 global-step:7308	 l-p:0.12224296480417252
epoch£º365	 i:9 	 global-step:7309	 l-p:0.1330946683883667
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9717, 4.9678, 4.9713],
        [4.9717, 5.6557, 5.9127],
        [4.9717, 5.7447, 6.0810],
        [4.9717, 4.9685, 4.9714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.11464639753103256 
model_pd.l_d.mean(): -18.548322677612305 
model_pd.lagr.mean(): -18.43367576599121 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6046], device='cuda:0')), ('power', tensor([-19.5214], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.11464639753103256
epoch£º366	 i:1 	 global-step:7321	 l-p:0.1344970166683197
epoch£º366	 i:2 	 global-step:7322	 l-p:0.13346727192401886
epoch£º366	 i:3 	 global-step:7323	 l-p:0.1349463313817978
epoch£º366	 i:4 	 global-step:7324	 l-p:0.25531086325645447
epoch£º366	 i:5 	 global-step:7325	 l-p:0.13126976788043976
epoch£º366	 i:6 	 global-step:7326	 l-p:0.010510282590985298
epoch£º366	 i:7 	 global-step:7327	 l-p:0.13753217458724976
epoch£º366	 i:8 	 global-step:7328	 l-p:-0.08817443996667862
epoch£º366	 i:9 	 global-step:7329	 l-p:0.12380380183458328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228]], device='cuda:0')
 pt:tensor([[5.1011, 5.8432, 6.1381],
        [5.1011, 5.0833, 5.0397],
        [5.1011, 5.0729, 5.0570],
        [5.1011, 5.1824, 5.0815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.19786149263381958 
model_pd.l_d.mean(): -19.183940887451172 
model_pd.lagr.mean(): -18.986080169677734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4974], device='cuda:0')), ('power', tensor([-20.0579], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.19786149263381958
epoch£º367	 i:1 	 global-step:7341	 l-p:0.14188118278980255
epoch£º367	 i:2 	 global-step:7342	 l-p:0.1399233192205429
epoch£º367	 i:3 	 global-step:7343	 l-p:0.1440165936946869
epoch£º367	 i:4 	 global-step:7344	 l-p:0.13067331910133362
epoch£º367	 i:5 	 global-step:7345	 l-p:0.13002637028694153
epoch£º367	 i:6 	 global-step:7346	 l-p:0.14319385588169098
epoch£º367	 i:7 	 global-step:7347	 l-p:0.12046529352664948
epoch£º367	 i:8 	 global-step:7348	 l-p:0.1006714403629303
epoch£º367	 i:9 	 global-step:7349	 l-p:0.12957869470119476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2141, 5.2141, 5.2141],
        [5.2141, 5.2141, 5.2141],
        [5.2141, 5.2118, 5.2139],
        [5.2141, 5.2571, 5.1675]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.15725946426391602 
model_pd.l_d.mean(): -20.138973236083984 
model_pd.lagr.mean(): -19.981714248657227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4214], device='cuda:0')), ('power', tensor([-20.9520], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.15725946426391602
epoch£º368	 i:1 	 global-step:7361	 l-p:0.08747220039367676
epoch£º368	 i:2 	 global-step:7362	 l-p:0.12410219013690948
epoch£º368	 i:3 	 global-step:7363	 l-p:0.20917759835720062
epoch£º368	 i:4 	 global-step:7364	 l-p:0.11632604151964188
epoch£º368	 i:5 	 global-step:7365	 l-p:0.13578277826309204
epoch£º368	 i:6 	 global-step:7366	 l-p:0.15826964378356934
epoch£º368	 i:7 	 global-step:7367	 l-p:0.11758096516132355
epoch£º368	 i:8 	 global-step:7368	 l-p:0.1090308353304863
epoch£º368	 i:9 	 global-step:7369	 l-p:0.18947036564350128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0218, 5.1234, 5.0191],
        [5.0218, 5.0199, 5.0217],
        [5.0218, 5.0218, 5.0218],
        [5.0218, 5.7383, 6.0179]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.1503552943468094 
model_pd.l_d.mean(): -19.580562591552734 
model_pd.lagr.mean(): -19.430208206176758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5405], device='cuda:0')), ('power', tensor([-20.5066], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.1503552943468094
epoch£º369	 i:1 	 global-step:7381	 l-p:0.12369455397129059
epoch£º369	 i:2 	 global-step:7382	 l-p:0.08888589590787888
epoch£º369	 i:3 	 global-step:7383	 l-p:0.17814548313617706
epoch£º369	 i:4 	 global-step:7384	 l-p:0.12784847617149353
epoch£º369	 i:5 	 global-step:7385	 l-p:0.123514324426651
epoch£º369	 i:6 	 global-step:7386	 l-p:0.17002731561660767
epoch£º369	 i:7 	 global-step:7387	 l-p:0.09552127867937088
epoch£º369	 i:8 	 global-step:7388	 l-p:-0.38205328583717346
epoch£º369	 i:9 	 global-step:7389	 l-p:0.12790267169475555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0231, 5.0560, 4.9596],
        [5.0231, 5.0205, 5.0229],
        [5.0231, 5.5285, 5.6332],
        [5.0231, 4.9894, 4.9894]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.1320759356021881 
model_pd.l_d.mean(): -17.821063995361328 
model_pd.lagr.mean(): -17.688987731933594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5622], device='cuda:0')), ('power', tensor([-18.7366], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.1320759356021881
epoch£º370	 i:1 	 global-step:7401	 l-p:0.13678331673145294
epoch£º370	 i:2 	 global-step:7402	 l-p:0.08936753869056702
epoch£º370	 i:3 	 global-step:7403	 l-p:0.13842462003231049
epoch£º370	 i:4 	 global-step:7404	 l-p:0.1334584802389145
epoch£º370	 i:5 	 global-step:7405	 l-p:0.13481874763965607
epoch£º370	 i:6 	 global-step:7406	 l-p:0.1524398922920227
epoch£º370	 i:7 	 global-step:7407	 l-p:0.11671438068151474
epoch£º370	 i:8 	 global-step:7408	 l-p:-0.40907859802246094
epoch£º370	 i:9 	 global-step:7409	 l-p:0.12221533805131912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0400, 5.0400, 5.0400],
        [5.0400, 5.0387, 5.0399],
        [5.0400, 5.7507, 6.0231],
        [5.0400, 5.0281, 5.0370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): -0.058060139417648315 
model_pd.l_d.mean(): -20.125123977661133 
model_pd.lagr.mean(): -20.183183670043945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4686], device='cuda:0')), ('power', tensor([-20.9868], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:-0.058060139417648315
epoch£º371	 i:1 	 global-step:7421	 l-p:0.12915287911891937
epoch£º371	 i:2 	 global-step:7422	 l-p:0.1213209480047226
epoch£º371	 i:3 	 global-step:7423	 l-p:0.145761176943779
epoch£º371	 i:4 	 global-step:7424	 l-p:0.11172275245189667
epoch£º371	 i:5 	 global-step:7425	 l-p:0.0037199114449322224
epoch£º371	 i:6 	 global-step:7426	 l-p:0.12023961544036865
epoch£º371	 i:7 	 global-step:7427	 l-p:0.20287786424160004
epoch£º371	 i:8 	 global-step:7428	 l-p:0.13034164905548096
epoch£º371	 i:9 	 global-step:7429	 l-p:0.12255928665399551
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8515, 5.6272, 5.9769],
        [4.8515, 4.8515, 4.8515],
        [4.8515, 4.8500, 4.8514],
        [4.8515, 4.8492, 4.7590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.13042762875556946 
model_pd.l_d.mean(): -19.27580451965332 
model_pd.lagr.mean(): -19.145376205444336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5540], device='cuda:0')), ('power', tensor([-20.2101], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.13042762875556946
epoch£º372	 i:1 	 global-step:7441	 l-p:0.0710558295249939
epoch£º372	 i:2 	 global-step:7442	 l-p:0.13452458381652832
epoch£º372	 i:3 	 global-step:7443	 l-p:0.15807391703128815
epoch£º372	 i:4 	 global-step:7444	 l-p:0.18096673488616943
epoch£º372	 i:5 	 global-step:7445	 l-p:0.13163302838802338
epoch£º372	 i:6 	 global-step:7446	 l-p:0.20257790386676788
epoch£º372	 i:7 	 global-step:7447	 l-p:0.1611459106206894
epoch£º372	 i:8 	 global-step:7448	 l-p:0.14109136164188385
epoch£º372	 i:9 	 global-step:7449	 l-p:0.07718978822231293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8683, 5.5163, 5.7504],
        [4.8683, 4.8483, 4.8612],
        [4.8683, 4.8683, 4.8683],
        [4.8683, 4.9002, 4.7937]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.14259715378284454 
model_pd.l_d.mean(): -18.877222061157227 
model_pd.lagr.mean(): -18.7346248626709 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5709], device='cuda:0')), ('power', tensor([-19.8215], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.14259715378284454
epoch£º373	 i:1 	 global-step:7461	 l-p:-0.16171060502529144
epoch£º373	 i:2 	 global-step:7462	 l-p:0.07569481432437897
epoch£º373	 i:3 	 global-step:7463	 l-p:0.154330775141716
epoch£º373	 i:4 	 global-step:7464	 l-p:0.11731196939945221
epoch£º373	 i:5 	 global-step:7465	 l-p:0.13164125382900238
epoch£º373	 i:6 	 global-step:7466	 l-p:0.1429465264081955
epoch£º373	 i:7 	 global-step:7467	 l-p:0.1283787339925766
epoch£º373	 i:8 	 global-step:7468	 l-p:0.14484244585037231
epoch£º373	 i:9 	 global-step:7469	 l-p:0.1004355251789093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9826, 4.9735, 4.8958],
        [4.9826, 4.9561, 4.9686],
        [4.9826, 4.9620, 4.9745],
        [4.9826, 5.7356, 6.0506]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.09199028462171555 
model_pd.l_d.mean(): -20.442577362060547 
model_pd.lagr.mean(): -20.350587844848633 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4464], device='cuda:0')), ('power', tensor([-21.2872], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.09199028462171555
epoch£º374	 i:1 	 global-step:7481	 l-p:0.21017812192440033
epoch£º374	 i:2 	 global-step:7482	 l-p:0.05327879264950752
epoch£º374	 i:3 	 global-step:7483	 l-p:0.12480657547712326
epoch£º374	 i:4 	 global-step:7484	 l-p:0.13689008355140686
epoch£º374	 i:5 	 global-step:7485	 l-p:0.1340133100748062
epoch£º374	 i:6 	 global-step:7486	 l-p:0.06872910261154175
epoch£º374	 i:7 	 global-step:7487	 l-p:0.10640233010053635
epoch£º374	 i:8 	 global-step:7488	 l-p:0.13926926255226135
epoch£º374	 i:9 	 global-step:7489	 l-p:0.0280231274664402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9264, 4.9222, 4.9260],
        [4.9264, 4.9264, 4.9264],
        [4.9264, 4.9215, 4.9258],
        [4.9264, 4.9263, 4.9264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.06184741482138634 
model_pd.l_d.mean(): -20.305500030517578 
model_pd.lagr.mean(): -20.24365234375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4582], device='cuda:0')), ('power', tensor([-21.1597], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:0.06184741482138634
epoch£º375	 i:1 	 global-step:7501	 l-p:0.249947652220726
epoch£º375	 i:2 	 global-step:7502	 l-p:0.14692020416259766
epoch£º375	 i:3 	 global-step:7503	 l-p:0.11799316853284836
epoch£º375	 i:4 	 global-step:7504	 l-p:0.1263958215713501
epoch£º375	 i:5 	 global-step:7505	 l-p:0.13131120800971985
epoch£º375	 i:6 	 global-step:7506	 l-p:0.17586033046245575
epoch£º375	 i:7 	 global-step:7507	 l-p:0.1310124397277832
epoch£º375	 i:8 	 global-step:7508	 l-p:0.14489606022834778
epoch£º375	 i:9 	 global-step:7509	 l-p:0.1159466877579689
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0811, 5.0500, 5.0582],
        [5.0811, 5.1185, 5.0179],
        [5.0811, 5.0810, 5.0811],
        [5.0811, 5.0543, 5.0080]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.10969747602939606 
model_pd.l_d.mean(): -19.520320892333984 
model_pd.lagr.mean(): -19.41062355041504 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4936], device='cuda:0')), ('power', tensor([-20.3965], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.10969747602939606
epoch£º376	 i:1 	 global-step:7521	 l-p:0.16507141292095184
epoch£º376	 i:2 	 global-step:7522	 l-p:0.16139526665210724
epoch£º376	 i:3 	 global-step:7523	 l-p:0.13195548951625824
epoch£º376	 i:4 	 global-step:7524	 l-p:0.2813446819782257
epoch£º376	 i:5 	 global-step:7525	 l-p:0.13529108464717865
epoch£º376	 i:6 	 global-step:7526	 l-p:0.13314710557460785
epoch£º376	 i:7 	 global-step:7527	 l-p:0.10950201004743576
epoch£º376	 i:8 	 global-step:7528	 l-p:0.13168111443519592
epoch£º376	 i:9 	 global-step:7529	 l-p:-0.27671027183532715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0720, 5.0489, 4.9928],
        [5.0720, 5.0407, 5.0497],
        [5.0720, 5.0443, 5.0558],
        [5.0720, 5.7534, 5.9961]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): -0.07974326610565186 
model_pd.l_d.mean(): -20.118207931518555 
model_pd.lagr.mean(): -20.19795036315918 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4423], device='cuda:0')), ('power', tensor([-20.9525], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:-0.07974326610565186
epoch£º377	 i:1 	 global-step:7541	 l-p:-0.004448785912245512
epoch£º377	 i:2 	 global-step:7542	 l-p:0.16316154599189758
epoch£º377	 i:3 	 global-step:7543	 l-p:0.20953771471977234
epoch£º377	 i:4 	 global-step:7544	 l-p:0.1141853779554367
epoch£º377	 i:5 	 global-step:7545	 l-p:0.12708865106105804
epoch£º377	 i:6 	 global-step:7546	 l-p:0.14038047194480896
epoch£º377	 i:7 	 global-step:7547	 l-p:0.1303827315568924
epoch£º377	 i:8 	 global-step:7548	 l-p:0.06480690091848373
epoch£º377	 i:9 	 global-step:7549	 l-p:0.12035388499498367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2409, 5.2672, 5.1773],
        [5.2409, 5.2409, 5.2409],
        [5.2409, 5.2099, 5.2026],
        [5.2409, 5.2170, 5.2268]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.13505078852176666 
model_pd.l_d.mean(): -19.737707138061523 
model_pd.lagr.mean(): -19.6026554107666 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4545], device='cuda:0')), ('power', tensor([-20.5776], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.13505078852176666
epoch£º378	 i:1 	 global-step:7561	 l-p:0.10042580217123032
epoch£º378	 i:2 	 global-step:7562	 l-p:0.13089263439178467
epoch£º378	 i:3 	 global-step:7563	 l-p:0.13465671241283417
epoch£º378	 i:4 	 global-step:7564	 l-p:0.10561948269605637
epoch£º378	 i:5 	 global-step:7565	 l-p:0.15892422199249268
epoch£º378	 i:6 	 global-step:7566	 l-p:0.14426448941230774
epoch£º378	 i:7 	 global-step:7567	 l-p:0.12388923019170761
epoch£º378	 i:8 	 global-step:7568	 l-p:0.12384752184152603
epoch£º378	 i:9 	 global-step:7569	 l-p:0.13402608036994934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0211, 4.9832, 4.9495],
        [5.0211, 5.0211, 5.0211],
        [5.0211, 5.0112, 5.0191],
        [5.0211, 4.9923, 4.9369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.14494767785072327 
model_pd.l_d.mean(): -18.960756301879883 
model_pd.lagr.mean(): -18.81580924987793 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5299], device='cuda:0')), ('power', tensor([-19.8642], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.14494767785072327
epoch£º379	 i:1 	 global-step:7581	 l-p:0.08968350291252136
epoch£º379	 i:2 	 global-step:7582	 l-p:0.11684146523475647
epoch£º379	 i:3 	 global-step:7583	 l-p:-0.007744235917925835
epoch£º379	 i:4 	 global-step:7584	 l-p:0.13271063566207886
epoch£º379	 i:5 	 global-step:7585	 l-p:0.13499626517295837
epoch£º379	 i:6 	 global-step:7586	 l-p:-0.8448874354362488
epoch£º379	 i:7 	 global-step:7587	 l-p:0.13110733032226562
epoch£º379	 i:8 	 global-step:7588	 l-p:0.14226345717906952
epoch£º379	 i:9 	 global-step:7589	 l-p:0.11732883006334305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8258, 4.9715, 4.8690],
        [4.8258, 4.8113, 4.8223],
        [4.8258, 4.8255, 4.8258],
        [4.8258, 4.8228, 4.8256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.1505623608827591 
model_pd.l_d.mean(): -20.244394302368164 
model_pd.lagr.mean(): -20.09383201599121 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5061], device='cuda:0')), ('power', tensor([-21.1472], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.1505623608827591
epoch£º380	 i:1 	 global-step:7601	 l-p:0.17048586905002594
epoch£º380	 i:2 	 global-step:7602	 l-p:0.12599599361419678
epoch£º380	 i:3 	 global-step:7603	 l-p:0.12482602894306183
epoch£º380	 i:4 	 global-step:7604	 l-p:0.11966300755739212
epoch£º380	 i:5 	 global-step:7605	 l-p:0.15881498157978058
epoch£º380	 i:6 	 global-step:7606	 l-p:0.10167859494686127
epoch£º380	 i:7 	 global-step:7607	 l-p:0.031119002029299736
epoch£º380	 i:8 	 global-step:7608	 l-p:0.03489382565021515
epoch£º380	 i:9 	 global-step:7609	 l-p:-0.2720048129558563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7195, 4.7194, 4.7195],
        [4.7195, 5.1008, 5.1350],
        [4.7195, 4.6797, 4.6957],
        [4.7195, 4.6607, 4.6435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.6169676184654236 
model_pd.l_d.mean(): -19.953832626342773 
model_pd.lagr.mean(): -19.336864471435547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6011], device='cuda:0')), ('power', tensor([-20.9495], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.6169676184654236
epoch£º381	 i:1 	 global-step:7621	 l-p:0.2789321541786194
epoch£º381	 i:2 	 global-step:7622	 l-p:0.1070151999592781
epoch£º381	 i:3 	 global-step:7623	 l-p:0.10401049256324768
epoch£º381	 i:4 	 global-step:7624	 l-p:-0.12486094981431961
epoch£º381	 i:5 	 global-step:7625	 l-p:0.1283448338508606
epoch£º381	 i:6 	 global-step:7626	 l-p:0.13046690821647644
epoch£º381	 i:7 	 global-step:7627	 l-p:0.24307982623577118
epoch£º381	 i:8 	 global-step:7628	 l-p:0.11525039374828339
epoch£º381	 i:9 	 global-step:7629	 l-p:0.14209316670894623
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1054, 5.0665, 5.0647],
        [5.1054, 5.1051, 5.1054],
        [5.1054, 5.1047, 5.1054],
        [5.1054, 5.0662, 5.0444]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.13104404509067535 
model_pd.l_d.mean(): -20.632190704345703 
model_pd.lagr.mean(): -20.50114631652832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3783], device='cuda:0')), ('power', tensor([-21.4098], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.13104404509067535
epoch£º382	 i:1 	 global-step:7641	 l-p:0.1622208058834076
epoch£º382	 i:2 	 global-step:7642	 l-p:0.15826092660427094
epoch£º382	 i:3 	 global-step:7643	 l-p:0.1373172104358673
epoch£º382	 i:4 	 global-step:7644	 l-p:0.038853928446769714
epoch£º382	 i:5 	 global-step:7645	 l-p:0.11619631201028824
epoch£º382	 i:6 	 global-step:7646	 l-p:0.08761211484670639
epoch£º382	 i:7 	 global-step:7647	 l-p:0.19289731979370117
epoch£º382	 i:8 	 global-step:7648	 l-p:0.0366317443549633
epoch£º382	 i:9 	 global-step:7649	 l-p:0.11019381880760193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1092, 5.1063, 5.1090],
        [5.1092, 5.1010, 5.0208],
        [5.1092, 5.1092, 5.1092],
        [5.1092, 5.0775, 5.0884]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.12754085659980774 
model_pd.l_d.mean(): -20.0332088470459 
model_pd.lagr.mean(): -19.905668258666992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4497], device='cuda:0')), ('power', tensor([-20.8735], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.12754085659980774
epoch£º383	 i:1 	 global-step:7661	 l-p:0.3207423985004425
epoch£º383	 i:2 	 global-step:7662	 l-p:0.12940886616706848
epoch£º383	 i:3 	 global-step:7663	 l-p:0.12121248990297318
epoch£º383	 i:4 	 global-step:7664	 l-p:0.08311380445957184
epoch£º383	 i:5 	 global-step:7665	 l-p:0.13088054955005646
epoch£º383	 i:6 	 global-step:7666	 l-p:0.13997015357017517
epoch£º383	 i:7 	 global-step:7667	 l-p:0.13361917436122894
epoch£º383	 i:8 	 global-step:7668	 l-p:0.1543746143579483
epoch£º383	 i:9 	 global-step:7669	 l-p:0.2579326927661896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0269, 5.5503, 5.6658],
        [5.0269, 4.9867, 4.9456],
        [5.0269, 5.2723, 5.2059],
        [5.0269, 5.0019, 5.0160]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.21899470686912537 
model_pd.l_d.mean(): -20.59142303466797 
model_pd.lagr.mean(): -20.37242889404297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4287], device='cuda:0')), ('power', tensor([-21.4205], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.21899470686912537
epoch£º384	 i:1 	 global-step:7681	 l-p:0.15122167766094208
epoch£º384	 i:2 	 global-step:7682	 l-p:0.13082221150398254
epoch£º384	 i:3 	 global-step:7683	 l-p:0.12184807658195496
epoch£º384	 i:4 	 global-step:7684	 l-p:0.13640394806861877
epoch£º384	 i:5 	 global-step:7685	 l-p:0.10176530480384827
epoch£º384	 i:6 	 global-step:7686	 l-p:0.09957000613212585
epoch£º384	 i:7 	 global-step:7687	 l-p:0.6709146499633789
epoch£º384	 i:8 	 global-step:7688	 l-p:0.10677455365657806
epoch£º384	 i:9 	 global-step:7689	 l-p:0.12891513109207153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0784, 5.0703, 4.9852],
        [5.0784, 5.0764, 5.0783],
        [5.0784, 5.0784, 5.0784],
        [5.0784, 5.5766, 5.6688]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.1314667910337448 
model_pd.l_d.mean(): -20.055444717407227 
model_pd.lagr.mean(): -19.923978805541992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4656], device='cuda:0')), ('power', tensor([-20.9127], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.1314667910337448
epoch£º385	 i:1 	 global-step:7701	 l-p:0.11898848414421082
epoch£º385	 i:2 	 global-step:7702	 l-p:0.13755513727664948
epoch£º385	 i:3 	 global-step:7703	 l-p:0.09360936284065247
epoch£º385	 i:4 	 global-step:7704	 l-p:0.10708858072757721
epoch£º385	 i:5 	 global-step:7705	 l-p:0.13835878670215607
epoch£º385	 i:6 	 global-step:7706	 l-p:0.02691616676747799
epoch£º385	 i:7 	 global-step:7707	 l-p:0.10436613857746124
epoch£º385	 i:8 	 global-step:7708	 l-p:0.1518276482820511
epoch£º385	 i:9 	 global-step:7709	 l-p:0.19189360737800598
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8893, 4.8489, 4.8622],
        [4.8893, 5.0772, 4.9870],
        [4.8893, 4.8893, 4.8893],
        [4.8893, 4.8572, 4.8732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.15281786024570465 
model_pd.l_d.mean(): -20.005773544311523 
model_pd.lagr.mean(): -19.852954864501953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5407], device='cuda:0')), ('power', tensor([-20.9399], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.15281786024570465
epoch£º386	 i:1 	 global-step:7721	 l-p:0.12329701334238052
epoch£º386	 i:2 	 global-step:7722	 l-p:0.15606163442134857
epoch£º386	 i:3 	 global-step:7723	 l-p:1.419249415397644
epoch£º386	 i:4 	 global-step:7724	 l-p:0.0769483745098114
epoch£º386	 i:5 	 global-step:7725	 l-p:0.10222622007131577
epoch£º386	 i:6 	 global-step:7726	 l-p:0.11489511281251907
epoch£º386	 i:7 	 global-step:7727	 l-p:0.12533007562160492
epoch£º386	 i:8 	 global-step:7728	 l-p:0.2142905443906784
epoch£º386	 i:9 	 global-step:7729	 l-p:0.14821882545948029
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2453, 5.2240, 5.2363],
        [5.2453, 5.7284, 5.7978],
        [5.2453, 5.2448, 5.2453],
        [5.2453, 5.2129, 5.2203]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.1265685260295868 
model_pd.l_d.mean(): -20.443344116210938 
model_pd.lagr.mean(): -20.316776275634766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3729], device='cuda:0')), ('power', tensor([-21.2118], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.1265685260295868
epoch£º387	 i:1 	 global-step:7741	 l-p:0.12616021931171417
epoch£º387	 i:2 	 global-step:7742	 l-p:0.12210007756948471
epoch£º387	 i:3 	 global-step:7743	 l-p:0.2728199064731598
epoch£º387	 i:4 	 global-step:7744	 l-p:0.11412815004587173
epoch£º387	 i:5 	 global-step:7745	 l-p:0.12682747840881348
epoch£º387	 i:6 	 global-step:7746	 l-p:0.10467611998319626
epoch£º387	 i:7 	 global-step:7747	 l-p:0.12812364101409912
epoch£º387	 i:8 	 global-step:7748	 l-p:0.13373355567455292
epoch£º387	 i:9 	 global-step:7749	 l-p:0.1113046258687973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2119, 5.2090, 5.2117],
        [5.2119, 5.1895, 5.2023],
        [5.2119, 5.2084, 5.2116],
        [5.2119, 5.2015, 5.1254]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.10681132972240448 
model_pd.l_d.mean(): -19.267282485961914 
model_pd.lagr.mean(): -19.160470962524414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5248], device='cuda:0')), ('power', tensor([-20.1712], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.10681132972240448
epoch£º388	 i:1 	 global-step:7761	 l-p:0.21370582282543182
epoch£º388	 i:2 	 global-step:7762	 l-p:0.48696741461753845
epoch£º388	 i:3 	 global-step:7763	 l-p:0.17029690742492676
epoch£º388	 i:4 	 global-step:7764	 l-p:0.13243182003498077
epoch£º388	 i:5 	 global-step:7765	 l-p:0.1556546986103058
epoch£º388	 i:6 	 global-step:7766	 l-p:-7.64493989944458
epoch£º388	 i:7 	 global-step:7767	 l-p:0.15550044178962708
epoch£º388	 i:8 	 global-step:7768	 l-p:0.134937584400177
epoch£º388	 i:9 	 global-step:7769	 l-p:0.13094033300876617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9008, 4.9422, 4.8213],
        [4.9008, 4.8558, 4.7928],
        [4.9008, 4.9006, 4.9008],
        [4.9008, 4.8603, 4.8744]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.13411954045295715 
model_pd.l_d.mean(): -19.253873825073242 
model_pd.lagr.mean(): -19.119754791259766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5552], device='cuda:0')), ('power', tensor([-20.1889], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.13411954045295715
epoch£º389	 i:1 	 global-step:7781	 l-p:0.1179758608341217
epoch£º389	 i:2 	 global-step:7782	 l-p:0.15862974524497986
epoch£º389	 i:3 	 global-step:7783	 l-p:0.15156526863574982
epoch£º389	 i:4 	 global-step:7784	 l-p:0.0957440435886383
epoch£º389	 i:5 	 global-step:7785	 l-p:0.1583566665649414
epoch£º389	 i:6 	 global-step:7786	 l-p:0.170045405626297
epoch£º389	 i:7 	 global-step:7787	 l-p:0.15595775842666626
epoch£º389	 i:8 	 global-step:7788	 l-p:0.10407764464616776
epoch£º389	 i:9 	 global-step:7789	 l-p:0.08134526759386063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0274, 5.0272, 5.0274],
        [5.0274, 4.9780, 4.9587],
        [5.0274, 5.5565, 5.6751],
        [5.0274, 5.0272, 5.0274]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.05385855957865715 
model_pd.l_d.mean(): -20.504928588867188 
model_pd.lagr.mean(): -20.45107078552246 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4348], device='cuda:0')), ('power', tensor([-21.3387], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.05385855957865715
epoch£º390	 i:1 	 global-step:7801	 l-p:0.11473367363214493
epoch£º390	 i:2 	 global-step:7802	 l-p:0.12208835035562515
epoch£º390	 i:3 	 global-step:7803	 l-p:0.1378384530544281
epoch£º390	 i:4 	 global-step:7804	 l-p:0.1188087910413742
epoch£º390	 i:5 	 global-step:7805	 l-p:0.13664406538009644
epoch£º390	 i:6 	 global-step:7806	 l-p:0.11641930043697357
epoch£º390	 i:7 	 global-step:7807	 l-p:0.12783105671405792
epoch£º390	 i:8 	 global-step:7808	 l-p:0.1409866064786911
epoch£º390	 i:9 	 global-step:7809	 l-p:0.13331808149814606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1595, 5.1515, 5.1582],
        [5.1595, 5.1516, 5.0647],
        [5.1595, 5.1335, 5.0671],
        [5.1595, 5.1558, 5.1592]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.15074662864208221 
model_pd.l_d.mean(): -20.50202178955078 
model_pd.lagr.mean(): -20.351274490356445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3911], device='cuda:0')), ('power', tensor([-21.2905], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.15074662864208221
epoch£º391	 i:1 	 global-step:7821	 l-p:0.46626976132392883
epoch£º391	 i:2 	 global-step:7822	 l-p:0.13208040595054626
epoch£º391	 i:3 	 global-step:7823	 l-p:0.12125503271818161
epoch£º391	 i:4 	 global-step:7824	 l-p:0.11379930377006531
epoch£º391	 i:5 	 global-step:7825	 l-p:0.13412907719612122
epoch£º391	 i:6 	 global-step:7826	 l-p:0.10812506079673767
epoch£º391	 i:7 	 global-step:7827	 l-p:0.12358963489532471
epoch£º391	 i:8 	 global-step:7828	 l-p:0.12711061537265778
epoch£º391	 i:9 	 global-step:7829	 l-p:0.23722802102565765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8266, 4.7860, 4.8032],
        [4.8266, 4.8263, 4.8266],
        [4.8266, 5.5467, 5.8419],
        [4.8266, 4.8266, 4.8266]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.13307973742485046 
model_pd.l_d.mean(): -19.76738929748535 
model_pd.lagr.mean(): -19.634309768676758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5181], device='cuda:0')), ('power', tensor([-20.6736], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.13307973742485046
epoch£º392	 i:1 	 global-step:7841	 l-p:0.1604236513376236
epoch£º392	 i:2 	 global-step:7842	 l-p:0.1543540507555008
epoch£º392	 i:3 	 global-step:7843	 l-p:0.19146084785461426
epoch£º392	 i:4 	 global-step:7844	 l-p:0.17843998968601227
epoch£º392	 i:5 	 global-step:7845	 l-p:0.01891779527068138
epoch£º392	 i:6 	 global-step:7846	 l-p:0.15277208387851715
epoch£º392	 i:7 	 global-step:7847	 l-p:0.014682173728942871
epoch£º392	 i:8 	 global-step:7848	 l-p:0.1316254436969757
epoch£º392	 i:9 	 global-step:7849	 l-p:0.11643261462450027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9789, 5.5834, 5.7657],
        [4.9789, 4.9789, 4.9789],
        [4.9789, 5.1737, 5.0820],
        [4.9789, 4.9789, 4.9789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): -0.45621880888938904 
model_pd.l_d.mean(): -20.174724578857422 
model_pd.lagr.mean(): -20.630943298339844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4972], device='cuda:0')), ('power', tensor([-21.0670], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:-0.45621880888938904
epoch£º393	 i:1 	 global-step:7861	 l-p:0.17057166993618011
epoch£º393	 i:2 	 global-step:7862	 l-p:0.15978451073169708
epoch£º393	 i:3 	 global-step:7863	 l-p:0.11934816092252731
epoch£º393	 i:4 	 global-step:7864	 l-p:0.25902384519577026
epoch£º393	 i:5 	 global-step:7865	 l-p:0.12562806904315948
epoch£º393	 i:6 	 global-step:7866	 l-p:0.2288312315940857
epoch£º393	 i:7 	 global-step:7867	 l-p:0.13955169916152954
epoch£º393	 i:8 	 global-step:7868	 l-p:0.13125422596931458
epoch£º393	 i:9 	 global-step:7869	 l-p:0.11626895517110825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1330, 5.0963, 5.1086],
        [5.1330, 5.1326, 5.1330],
        [5.1330, 5.0971, 5.0380],
        [5.1330, 5.8676, 6.1463]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.15246830880641937 
model_pd.l_d.mean(): -20.380420684814453 
model_pd.lagr.mean(): -20.22795295715332 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4120], device='cuda:0')), ('power', tensor([-21.1882], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.15246830880641937
epoch£º394	 i:1 	 global-step:7881	 l-p:0.11484599858522415
epoch£º394	 i:2 	 global-step:7882	 l-p:0.10652501881122589
epoch£º394	 i:3 	 global-step:7883	 l-p:0.04430866613984108
epoch£º394	 i:4 	 global-step:7884	 l-p:0.07207582145929337
epoch£º394	 i:5 	 global-step:7885	 l-p:0.044139184057712555
epoch£º394	 i:6 	 global-step:7886	 l-p:0.14570198953151703
epoch£º394	 i:7 	 global-step:7887	 l-p:0.13173922896385193
epoch£º394	 i:8 	 global-step:7888	 l-p:0.14753466844558716
epoch£º394	 i:9 	 global-step:7889	 l-p:0.14212556183338165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0756, 5.0740, 5.0755],
        [5.0756, 5.0293, 5.0337],
        [5.0756, 5.0756, 5.0756],
        [5.0756, 5.0377, 5.0513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.11461879312992096 
model_pd.l_d.mean(): -19.937274932861328 
model_pd.lagr.mean(): -19.822656631469727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4837], device='cuda:0')), ('power', tensor([-20.8110], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.11461879312992096
epoch£º395	 i:1 	 global-step:7901	 l-p:0.12032154947519302
epoch£º395	 i:2 	 global-step:7902	 l-p:0.13655208051204681
epoch£º395	 i:3 	 global-step:7903	 l-p:0.15239831805229187
epoch£º395	 i:4 	 global-step:7904	 l-p:0.06857974827289581
epoch£º395	 i:5 	 global-step:7905	 l-p:0.06309721618890762
epoch£º395	 i:6 	 global-step:7906	 l-p:0.14681152999401093
epoch£º395	 i:7 	 global-step:7907	 l-p:0.1181594654917717
epoch£º395	 i:8 	 global-step:7908	 l-p:0.1819673478603363
epoch£º395	 i:9 	 global-step:7909	 l-p:0.17279914021492004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8894, 5.0591, 4.9573],
        [4.8894, 5.1179, 5.0427],
        [4.8894, 4.8698, 4.8839],
        [4.8894, 4.8773, 4.8871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.1404617428779602 
model_pd.l_d.mean(): -20.903610229492188 
model_pd.lagr.mean(): -20.76314926147461 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4218], device='cuda:0')), ('power', tensor([-21.7314], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.1404617428779602
epoch£º396	 i:1 	 global-step:7921	 l-p:0.10677717626094818
epoch£º396	 i:2 	 global-step:7922	 l-p:0.127262681722641
epoch£º396	 i:3 	 global-step:7923	 l-p:0.14699910581111908
epoch£º396	 i:4 	 global-step:7924	 l-p:0.09749504923820496
epoch£º396	 i:5 	 global-step:7925	 l-p:0.17366142570972443
epoch£º396	 i:6 	 global-step:7926	 l-p:0.12322720885276794
epoch£º396	 i:7 	 global-step:7927	 l-p:0.039237603545188904
epoch£º396	 i:8 	 global-step:7928	 l-p:0.1521793156862259
epoch£º396	 i:9 	 global-step:7929	 l-p:0.09980998933315277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0008, 5.6833, 5.9281],
        [5.0008, 4.9952, 5.0002],
        [5.0008, 4.9527, 4.9617],
        [5.0008, 4.9557, 4.9679]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.11243278533220291 
model_pd.l_d.mean(): -19.145370483398438 
model_pd.lagr.mean(): -19.03293800354004 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5827], device='cuda:0')), ('power', tensor([-20.1069], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.11243278533220291
epoch£º397	 i:1 	 global-step:7941	 l-p:0.33222705125808716
epoch£º397	 i:2 	 global-step:7942	 l-p:0.15811416506767273
epoch£º397	 i:3 	 global-step:7943	 l-p:0.10430315881967545
epoch£º397	 i:4 	 global-step:7944	 l-p:0.29594871401786804
epoch£º397	 i:5 	 global-step:7945	 l-p:0.16257961094379425
epoch£º397	 i:6 	 global-step:7946	 l-p:-2.0525949001312256
epoch£º397	 i:7 	 global-step:7947	 l-p:0.1340702474117279
epoch£º397	 i:8 	 global-step:7948	 l-p:0.12769785523414612
epoch£º397	 i:9 	 global-step:7949	 l-p:0.604159951210022
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1473, 5.3978, 5.3261],
        [5.1473, 5.1472, 5.1473],
        [5.1473, 5.1457, 5.1472],
        [5.1473, 5.3273, 5.2259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.2596411108970642 
model_pd.l_d.mean(): -19.158512115478516 
model_pd.lagr.mean(): -18.89887046813965 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4989], device='cuda:0')), ('power', tensor([-20.0334], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:0.2596411108970642
epoch£º398	 i:1 	 global-step:7961	 l-p:0.10700608789920807
epoch£º398	 i:2 	 global-step:7962	 l-p:0.19617809355258942
epoch£º398	 i:3 	 global-step:7963	 l-p:0.14235173165798187
epoch£º398	 i:4 	 global-step:7964	 l-p:0.14190910756587982
epoch£º398	 i:5 	 global-step:7965	 l-p:0.11740896850824356
epoch£º398	 i:6 	 global-step:7966	 l-p:0.1368161290884018
epoch£º398	 i:7 	 global-step:7967	 l-p:0.12617962062358856
epoch£º398	 i:8 	 global-step:7968	 l-p:0.12536107003688812
epoch£º398	 i:9 	 global-step:7969	 l-p:0.11806825548410416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0686, 5.0682, 5.0686],
        [5.0686, 5.0236, 5.0344],
        [5.0686, 5.1500, 5.0254],
        [5.0686, 5.0327, 4.9555]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.11571173369884491 
model_pd.l_d.mean(): -19.88314437866211 
model_pd.lagr.mean(): -19.767433166503906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4781], device='cuda:0')), ('power', tensor([-20.7501], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.11571173369884491
epoch£º399	 i:1 	 global-step:7981	 l-p:0.1023339256644249
epoch£º399	 i:2 	 global-step:7982	 l-p:0.12644386291503906
epoch£º399	 i:3 	 global-step:7983	 l-p:0.01783073879778385
epoch£º399	 i:4 	 global-step:7984	 l-p:0.13474240899085999
epoch£º399	 i:5 	 global-step:7985	 l-p:0.14800818264484406
epoch£º399	 i:6 	 global-step:7986	 l-p:0.14409790933132172
epoch£º399	 i:7 	 global-step:7987	 l-p:0.15950311720371246
epoch£º399	 i:8 	 global-step:7988	 l-p:0.18799655139446259
epoch£º399	 i:9 	 global-step:7989	 l-p:0.04222534969449043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9244, 4.9024, 4.7939],
        [4.9244, 5.0245, 4.8991],
        [4.9244, 4.9072, 4.9202],
        [4.9244, 4.8934, 4.7924]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.1466883271932602 
model_pd.l_d.mean(): -19.50509262084961 
model_pd.lagr.mean(): -19.3584041595459 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5118], device='cuda:0')), ('power', tensor([-20.3999], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.1466883271932602
epoch£º400	 i:1 	 global-step:8001	 l-p:0.11018750816583633
epoch£º400	 i:2 	 global-step:8002	 l-p:0.12100650370121002
epoch£º400	 i:3 	 global-step:8003	 l-p:0.14314419031143188
epoch£º400	 i:4 	 global-step:8004	 l-p:0.14526128768920898
epoch£º400	 i:5 	 global-step:8005	 l-p:-1.153822660446167
epoch£º400	 i:6 	 global-step:8006	 l-p:0.1440250724554062
epoch£º400	 i:7 	 global-step:8007	 l-p:-0.0040235803462564945
epoch£º400	 i:8 	 global-step:8008	 l-p:0.10985975712537766
epoch£º400	 i:9 	 global-step:8009	 l-p:0.13145576417446136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9944, 5.1521, 5.0424],
        [4.9944, 4.9942, 4.9944],
        [4.9944, 5.0994, 4.9754],
        [4.9944, 5.2930, 5.2508]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.14756622910499573 
model_pd.l_d.mean(): -19.337316513061523 
model_pd.lagr.mean(): -19.18975067138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5812], device='cuda:0')), ('power', tensor([-20.3008], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.14756622910499573
epoch£º401	 i:1 	 global-step:8021	 l-p:0.15102076530456543
epoch£º401	 i:2 	 global-step:8022	 l-p:0.14674769341945648
epoch£º401	 i:3 	 global-step:8023	 l-p:0.2780066132545471
epoch£º401	 i:4 	 global-step:8024	 l-p:0.1140189990401268
epoch£º401	 i:5 	 global-step:8025	 l-p:-0.0017143702134490013
epoch£º401	 i:6 	 global-step:8026	 l-p:0.14729058742523193
epoch£º401	 i:7 	 global-step:8027	 l-p:0.11551867425441742
epoch£º401	 i:8 	 global-step:8028	 l-p:0.08877186477184296
epoch£º401	 i:9 	 global-step:8029	 l-p:0.14859531819820404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9982, 4.9982, 4.9982],
        [4.9982, 4.9977, 4.9982],
        [4.9982, 4.9496, 4.9617],
        [4.9982, 4.9982, 4.9982]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.1352529674768448 
model_pd.l_d.mean(): -20.585691452026367 
model_pd.lagr.mean(): -20.450437545776367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4217], device='cuda:0')), ('power', tensor([-21.4074], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.1352529674768448
epoch£º402	 i:1 	 global-step:8041	 l-p:0.13848456740379333
epoch£º402	 i:2 	 global-step:8042	 l-p:0.13537472486495972
epoch£º402	 i:3 	 global-step:8043	 l-p:0.0946834459900856
epoch£º402	 i:4 	 global-step:8044	 l-p:0.11918734014034271
epoch£º402	 i:5 	 global-step:8045	 l-p:0.005501770880073309
epoch£º402	 i:6 	 global-step:8046	 l-p:0.14035752415657043
epoch£º402	 i:7 	 global-step:8047	 l-p:0.16335994005203247
epoch£º402	 i:8 	 global-step:8048	 l-p:0.12939685583114624
epoch£º402	 i:9 	 global-step:8049	 l-p:0.1548749953508377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0569, 5.0156, 5.0311],
        [5.0569, 5.0569, 5.0569],
        [5.0569, 5.0185, 5.0350],
        [5.0569, 5.0409, 5.0531]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.1566184014081955 
model_pd.l_d.mean(): -20.350128173828125 
model_pd.lagr.mean(): -20.193510055541992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4288], device='cuda:0')), ('power', tensor([-21.1748], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.1566184014081955
epoch£º403	 i:1 	 global-step:8061	 l-p:0.12363287061452866
epoch£º403	 i:2 	 global-step:8062	 l-p:0.13205796480178833
epoch£º403	 i:3 	 global-step:8063	 l-p:0.12547209858894348
epoch£º403	 i:4 	 global-step:8064	 l-p:0.017163557931780815
epoch£º403	 i:5 	 global-step:8065	 l-p:0.13588720560073853
epoch£º403	 i:6 	 global-step:8066	 l-p:0.12155143171548843
epoch£º403	 i:7 	 global-step:8067	 l-p:0.11589179933071136
epoch£º403	 i:8 	 global-step:8068	 l-p:0.12113670259714127
epoch£º403	 i:9 	 global-step:8069	 l-p:0.035402894020080566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1045, 5.2264, 5.1060],
        [5.1045, 5.5785, 5.6466],
        [5.1045, 5.0757, 5.0923],
        [5.1045, 5.0996, 5.1041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.15098397433757782 
model_pd.l_d.mean(): -19.021299362182617 
model_pd.lagr.mean(): -18.870315551757812 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5279], device='cuda:0')), ('power', tensor([-19.9237], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.15098397433757782
epoch£º404	 i:1 	 global-step:8081	 l-p:0.1130143254995346
epoch£º404	 i:2 	 global-step:8082	 l-p:0.055198028683662415
epoch£º404	 i:3 	 global-step:8083	 l-p:0.1276874542236328
epoch£º404	 i:4 	 global-step:8084	 l-p:0.120073102414608
epoch£º404	 i:5 	 global-step:8085	 l-p:0.007332963868975639
epoch£º404	 i:6 	 global-step:8086	 l-p:0.15829095244407654
epoch£º404	 i:7 	 global-step:8087	 l-p:0.1208174005150795
epoch£º404	 i:8 	 global-step:8088	 l-p:-0.1841210573911667
epoch£º404	 i:9 	 global-step:8089	 l-p:0.1483667939901352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1059, 5.0879, 4.9881],
        [5.1059, 5.1059, 5.1059],
        [5.1059, 5.1059, 5.1059],
        [5.1059, 5.0517, 5.0127]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.10061720013618469 
model_pd.l_d.mean(): -20.20567512512207 
model_pd.lagr.mean(): -20.105058670043945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4505], device='cuda:0')), ('power', tensor([-21.0501], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.10061720013618469
epoch£º405	 i:1 	 global-step:8101	 l-p:0.1706784963607788
epoch£º405	 i:2 	 global-step:8102	 l-p:0.10529825091362
epoch£º405	 i:3 	 global-step:8103	 l-p:0.03064887970685959
epoch£º405	 i:4 	 global-step:8104	 l-p:0.03989298641681671
epoch£º405	 i:5 	 global-step:8105	 l-p:0.06138453260064125
epoch£º405	 i:6 	 global-step:8106	 l-p:0.15815764665603638
epoch£º405	 i:7 	 global-step:8107	 l-p:0.15108254551887512
epoch£º405	 i:8 	 global-step:8108	 l-p:0.12591682374477386
epoch£º405	 i:9 	 global-step:8109	 l-p:0.13392995297908783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0921, 5.0448, 5.0556],
        [5.0921, 5.0729, 5.0867],
        [5.0921, 5.0364, 4.9989],
        [5.0921, 5.2511, 5.1402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.03535008430480957 
model_pd.l_d.mean(): -19.531713485717773 
model_pd.lagr.mean(): -19.496362686157227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4920], device='cuda:0')), ('power', tensor([-20.4065], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:0.03535008430480957
epoch£º406	 i:1 	 global-step:8121	 l-p:0.09994418919086456
epoch£º406	 i:2 	 global-step:8122	 l-p:0.1305740475654602
epoch£º406	 i:3 	 global-step:8123	 l-p:0.17629964649677277
epoch£º406	 i:4 	 global-step:8124	 l-p:0.15230467915534973
epoch£º406	 i:5 	 global-step:8125	 l-p:2.4920380115509033
epoch£º406	 i:6 	 global-step:8126	 l-p:0.12063869833946228
epoch£º406	 i:7 	 global-step:8127	 l-p:0.08101104199886322
epoch£º406	 i:8 	 global-step:8128	 l-p:0.12743522226810455
epoch£º406	 i:9 	 global-step:8129	 l-p:0.1293642520904541
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0575, 5.0513, 5.0568],
        [5.0575, 5.0520, 5.0570],
        [5.0575, 5.1375, 5.0091],
        [5.0575, 5.0392, 4.9343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.1748456358909607 
model_pd.l_d.mean(): -20.483957290649414 
model_pd.lagr.mean(): -20.309112548828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4246], device='cuda:0')), ('power', tensor([-21.3068], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.1748456358909607
epoch£º407	 i:1 	 global-step:8141	 l-p:0.12725219130516052
epoch£º407	 i:2 	 global-step:8142	 l-p:0.15438874065876007
epoch£º407	 i:3 	 global-step:8143	 l-p:0.14203578233718872
epoch£º407	 i:4 	 global-step:8144	 l-p:-0.12916016578674316
epoch£º407	 i:5 	 global-step:8145	 l-p:0.14659908413887024
epoch£º407	 i:6 	 global-step:8146	 l-p:0.1407177746295929
epoch£º407	 i:7 	 global-step:8147	 l-p:0.423032283782959
epoch£º407	 i:8 	 global-step:8148	 l-p:0.1155642420053482
epoch£º407	 i:9 	 global-step:8149	 l-p:0.12211375683546066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1545, 5.1243, 5.1408],
        [5.1545, 5.1278, 5.1439],
        [5.1545, 5.1023, 5.0625],
        [5.1545, 5.2293, 5.1026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.37625598907470703 
model_pd.l_d.mean(): -20.418195724487305 
model_pd.lagr.mean(): -20.04193878173828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4172], device='cuda:0')), ('power', tensor([-21.2321], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.37625598907470703
epoch£º408	 i:1 	 global-step:8161	 l-p:0.13941672444343567
epoch£º408	 i:2 	 global-step:8162	 l-p:0.14561985433101654
epoch£º408	 i:3 	 global-step:8163	 l-p:0.13876155018806458
epoch£º408	 i:4 	 global-step:8164	 l-p:0.2604884207248688
epoch£º408	 i:5 	 global-step:8165	 l-p:0.2024560570716858
epoch£º408	 i:6 	 global-step:8166	 l-p:0.1198720708489418
epoch£º408	 i:7 	 global-step:8167	 l-p:0.12140888720750809
epoch£º408	 i:8 	 global-step:8168	 l-p:0.12866367399692535
epoch£º408	 i:9 	 global-step:8169	 l-p:0.09321631491184235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1666, 5.1464, 5.1605],
        [5.1666, 5.1444, 5.1592],
        [5.1666, 5.1649, 5.1665],
        [5.1666, 5.1654, 5.1666]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.12794868648052216 
model_pd.l_d.mean(): -20.43768882751465 
model_pd.lagr.mean(): -20.30974006652832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3847], device='cuda:0')), ('power', tensor([-21.2183], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.12794868648052216
epoch£º409	 i:1 	 global-step:8181	 l-p:0.1639404445886612
epoch£º409	 i:2 	 global-step:8182	 l-p:0.27958935499191284
epoch£º409	 i:3 	 global-step:8183	 l-p:0.14058992266654968
epoch£º409	 i:4 	 global-step:8184	 l-p:0.13988584280014038
epoch£º409	 i:5 	 global-step:8185	 l-p:0.12325820326805115
epoch£º409	 i:6 	 global-step:8186	 l-p:-0.0724169984459877
epoch£º409	 i:7 	 global-step:8187	 l-p:-0.35633593797683716
epoch£º409	 i:8 	 global-step:8188	 l-p:0.14227136969566345
epoch£º409	 i:9 	 global-step:8189	 l-p:0.11605679243803024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1271, 5.0958, 5.1128],
        [5.1271, 5.0761, 5.0816],
        [5.1271, 5.1212, 5.1264],
        [5.1271, 5.1271, 5.1271]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.15234710276126862 
model_pd.l_d.mean(): -20.301485061645508 
model_pd.lagr.mean(): -20.149137496948242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4334], device='cuda:0')), ('power', tensor([-21.1300], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.15234710276126862
epoch£º410	 i:1 	 global-step:8201	 l-p:0.13231849670410156
epoch£º410	 i:2 	 global-step:8202	 l-p:0.09575188159942627
epoch£º410	 i:3 	 global-step:8203	 l-p:0.12824490666389465
epoch£º410	 i:4 	 global-step:8204	 l-p:0.2892919182777405
epoch£º410	 i:5 	 global-step:8205	 l-p:0.13082349300384521
epoch£º410	 i:6 	 global-step:8206	 l-p:0.08471798896789551
epoch£º410	 i:7 	 global-step:8207	 l-p:0.12921178340911865
epoch£º410	 i:8 	 global-step:8208	 l-p:0.14491349458694458
epoch£º410	 i:9 	 global-step:8209	 l-p:0.07826811820268631
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0183, 5.0183, 5.0183],
        [5.0183, 5.0176, 5.0182],
        [5.0183, 5.0181, 5.0182],
        [5.0183, 5.0170, 5.0182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.33793312311172485 
model_pd.l_d.mean(): -20.204580307006836 
model_pd.lagr.mean(): -19.866647720336914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4760], device='cuda:0')), ('power', tensor([-21.0754], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.33793312311172485
epoch£º411	 i:1 	 global-step:8221	 l-p:0.13491643965244293
epoch£º411	 i:2 	 global-step:8222	 l-p:0.15032166242599487
epoch£º411	 i:3 	 global-step:8223	 l-p:0.1715448945760727
epoch£º411	 i:4 	 global-step:8224	 l-p:0.08703716099262238
epoch£º411	 i:5 	 global-step:8225	 l-p:0.31869372725486755
epoch£º411	 i:6 	 global-step:8226	 l-p:0.07389995455741882
epoch£º411	 i:7 	 global-step:8227	 l-p:0.12181854248046875
epoch£º411	 i:8 	 global-step:8228	 l-p:0.13493633270263672
epoch£º411	 i:9 	 global-step:8229	 l-p:0.14176228642463684
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9873, 4.9812, 4.9866],
        [4.9873, 5.7506, 6.0641],
        [4.9873, 4.9873, 4.9873],
        [4.9873, 4.9316, 4.9405]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.27689430117607117 
model_pd.l_d.mean(): -19.0340518951416 
model_pd.lagr.mean(): -18.757158279418945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5347], device='cuda:0')), ('power', tensor([-19.9438], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.27689430117607117
epoch£º412	 i:1 	 global-step:8241	 l-p:0.10786069929599762
epoch£º412	 i:2 	 global-step:8242	 l-p:0.1365358829498291
epoch£º412	 i:3 	 global-step:8243	 l-p:0.1459439992904663
epoch£º412	 i:4 	 global-step:8244	 l-p:0.1223430410027504
epoch£º412	 i:5 	 global-step:8245	 l-p:0.15660762786865234
epoch£º412	 i:6 	 global-step:8246	 l-p:0.09162120521068573
epoch£º412	 i:7 	 global-step:8247	 l-p:0.0774708166718483
epoch£º412	 i:8 	 global-step:8248	 l-p:0.16368049383163452
epoch£º412	 i:9 	 global-step:8249	 l-p:0.11860156804323196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9298, 4.8823, 4.9001],
        [4.9298, 4.9298, 4.9298],
        [4.9298, 5.6122, 5.8601],
        [4.9298, 4.9298, 4.9298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.14947859942913055 
model_pd.l_d.mean(): -19.783796310424805 
model_pd.lagr.mean(): -19.63431739807129 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5238], device='cuda:0')), ('power', tensor([-20.6962], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.14947859942913055
epoch£º413	 i:1 	 global-step:8261	 l-p:0.14724254608154297
epoch£º413	 i:2 	 global-step:8262	 l-p:0.13506722450256348
epoch£º413	 i:3 	 global-step:8263	 l-p:0.07509433478116989
epoch£º413	 i:4 	 global-step:8264	 l-p:0.11934863030910492
epoch£º413	 i:5 	 global-step:8265	 l-p:0.18149840831756592
epoch£º413	 i:6 	 global-step:8266	 l-p:0.16932673752307892
epoch£º413	 i:7 	 global-step:8267	 l-p:0.15460795164108276
epoch£º413	 i:8 	 global-step:8268	 l-p:0.02428385242819786
epoch£º413	 i:9 	 global-step:8269	 l-p:0.1303098499774933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8959, 4.8920, 4.8956],
        [4.8959, 5.0009, 4.8720],
        [4.8959, 4.8959, 4.8959],
        [4.8959, 4.8705, 4.8878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.08838855475187302 
model_pd.l_d.mean(): -19.407611846923828 
model_pd.lagr.mean(): -19.319223403930664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5473], device='cuda:0')), ('power', tensor([-20.3374], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.08838855475187302
epoch£º414	 i:1 	 global-step:8281	 l-p:0.1442074179649353
epoch£º414	 i:2 	 global-step:8282	 l-p:0.17176656424999237
epoch£º414	 i:3 	 global-step:8283	 l-p:0.1251939982175827
epoch£º414	 i:4 	 global-step:8284	 l-p:0.15044070780277252
epoch£º414	 i:5 	 global-step:8285	 l-p:0.17326326668262482
epoch£º414	 i:6 	 global-step:8286	 l-p:0.08781692385673523
epoch£º414	 i:7 	 global-step:8287	 l-p:0.1042872965335846
epoch£º414	 i:8 	 global-step:8288	 l-p:0.13386069238185883
epoch£º414	 i:9 	 global-step:8289	 l-p:0.1594831794500351
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9424, 5.0227, 4.8893],
        [4.9424, 5.3992, 5.4645],
        [4.9424, 4.9719, 4.8371],
        [4.9424, 4.9280, 4.9395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.1059059426188469 
model_pd.l_d.mean(): -19.7439022064209 
model_pd.lagr.mean(): -19.637996673583984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5658], device='cuda:0')), ('power', tensor([-20.6991], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.1059059426188469
epoch£º415	 i:1 	 global-step:8301	 l-p:0.12435819208621979
epoch£º415	 i:2 	 global-step:8302	 l-p:0.15788249671459198
epoch£º415	 i:3 	 global-step:8303	 l-p:0.12926040589809418
epoch£º415	 i:4 	 global-step:8304	 l-p:0.14127729833126068
epoch£º415	 i:5 	 global-step:8305	 l-p:-0.16426105797290802
epoch£º415	 i:6 	 global-step:8306	 l-p:0.1337866634130478
epoch£º415	 i:7 	 global-step:8307	 l-p:0.13288888335227966
epoch£º415	 i:8 	 global-step:8308	 l-p:0.05140439420938492
epoch£º415	 i:9 	 global-step:8309	 l-p:0.12783287465572357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9665, 4.9343, 4.9533],
        [4.9665, 4.9665, 4.9665],
        [4.9665, 5.0343, 4.8996],
        [4.9665, 4.9665, 4.9665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.16937316954135895 
model_pd.l_d.mean(): -20.496328353881836 
model_pd.lagr.mean(): -20.326955795288086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4643], device='cuda:0')), ('power', tensor([-21.3605], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.16937316954135895
epoch£º416	 i:1 	 global-step:8321	 l-p:-0.6272544860839844
epoch£º416	 i:2 	 global-step:8322	 l-p:0.07289235293865204
epoch£º416	 i:3 	 global-step:8323	 l-p:0.11385690420866013
epoch£º416	 i:4 	 global-step:8324	 l-p:0.12394780665636063
epoch£º416	 i:5 	 global-step:8325	 l-p:0.08260960131883621
epoch£º416	 i:6 	 global-step:8326	 l-p:0.11013460904359818
epoch£º416	 i:7 	 global-step:8327	 l-p:0.12478765100240707
epoch£º416	 i:8 	 global-step:8328	 l-p:0.11906594783067703
epoch£º416	 i:9 	 global-step:8329	 l-p:0.12730437517166138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0336, 5.0251, 5.0325],
        [5.0336, 5.0160, 5.0293],
        [5.0336, 4.9877, 5.0044],
        [5.0336, 5.0336, 5.0336]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.3238266706466675 
model_pd.l_d.mean(): -20.266319274902344 
model_pd.lagr.mean(): -19.942493438720703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4600], device='cuda:0')), ('power', tensor([-21.1218], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.3238266706466675
epoch£º417	 i:1 	 global-step:8341	 l-p:0.14023765921592712
epoch£º417	 i:2 	 global-step:8342	 l-p:0.12867887318134308
epoch£º417	 i:3 	 global-step:8343	 l-p:0.12010467052459717
epoch£º417	 i:4 	 global-step:8344	 l-p:0.12414184212684631
epoch£º417	 i:5 	 global-step:8345	 l-p:0.14396294951438904
epoch£º417	 i:6 	 global-step:8346	 l-p:0.04226449504494667
epoch£º417	 i:7 	 global-step:8347	 l-p:0.10461509227752686
epoch£º417	 i:8 	 global-step:8348	 l-p:0.14022983610630035
epoch£º417	 i:9 	 global-step:8349	 l-p:0.14476734399795532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9512, 4.9511, 4.9512],
        [4.9512, 4.9511, 4.9512],
        [4.9512, 5.1333, 5.0295],
        [4.9512, 4.8846, 4.8796]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): -0.17615512013435364 
model_pd.l_d.mean(): -19.48790168762207 
model_pd.lagr.mean(): -19.6640567779541 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5183], device='cuda:0')), ('power', tensor([-20.3891], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:-0.17615512013435364
epoch£º418	 i:1 	 global-step:8361	 l-p:0.1503455489873886
epoch£º418	 i:2 	 global-step:8362	 l-p:0.10799436271190643
epoch£º418	 i:3 	 global-step:8363	 l-p:0.13656902313232422
epoch£º418	 i:4 	 global-step:8364	 l-p:0.14741607010364532
epoch£º418	 i:5 	 global-step:8365	 l-p:0.11841947585344315
epoch£º418	 i:6 	 global-step:8366	 l-p:0.12024947255849838
epoch£º418	 i:7 	 global-step:8367	 l-p:0.17970581352710724
epoch£º418	 i:8 	 global-step:8368	 l-p:0.11527599394321442
epoch£º418	 i:9 	 global-step:8369	 l-p:0.12906315922737122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9603, 4.8938, 4.8892],
        [4.9603, 4.9856, 4.8502],
        [4.9603, 4.9232, 4.8163],
        [4.9603, 4.9510, 4.9590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.1534685343503952 
model_pd.l_d.mean(): -20.157297134399414 
model_pd.lagr.mean(): -20.003828048706055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5052], device='cuda:0')), ('power', tensor([-21.0575], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.1534685343503952
epoch£º419	 i:1 	 global-step:8381	 l-p:0.12988686561584473
epoch£º419	 i:2 	 global-step:8382	 l-p:0.11528315395116806
epoch£º419	 i:3 	 global-step:8383	 l-p:0.1326857954263687
epoch£º419	 i:4 	 global-step:8384	 l-p:0.1401558667421341
epoch£º419	 i:5 	 global-step:8385	 l-p:0.08188533782958984
epoch£º419	 i:6 	 global-step:8386	 l-p:-0.03583642840385437
epoch£º419	 i:7 	 global-step:8387	 l-p:0.12757757306098938
epoch£º419	 i:8 	 global-step:8388	 l-p:0.13911393284797668
epoch£º419	 i:9 	 global-step:8389	 l-p:0.2885027825832367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9894, 5.1288, 5.0082],
        [4.9894, 4.9278, 4.9322],
        [4.9894, 4.9207, 4.8845],
        [4.9894, 4.9889, 4.9893]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.0704454779624939 
model_pd.l_d.mean(): -19.484416961669922 
model_pd.lagr.mean(): -19.413970947265625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5036], device='cuda:0')), ('power', tensor([-20.3703], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.0704454779624939
epoch£º420	 i:1 	 global-step:8401	 l-p:0.12913179397583008
epoch£º420	 i:2 	 global-step:8402	 l-p:0.1344718188047409
epoch£º420	 i:3 	 global-step:8403	 l-p:0.13240541517734528
epoch£º420	 i:4 	 global-step:8404	 l-p:0.13039545714855194
epoch£º420	 i:5 	 global-step:8405	 l-p:0.2452300637960434
epoch£º420	 i:6 	 global-step:8406	 l-p:0.07555650174617767
epoch£º420	 i:7 	 global-step:8407	 l-p:0.12484091520309448
epoch£º420	 i:8 	 global-step:8408	 l-p:0.10981973260641098
epoch£º420	 i:9 	 global-step:8409	 l-p:0.23155297338962555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0638, 5.0025, 4.9494],
        [5.0638, 5.6748, 5.8500],
        [5.0638, 5.0279, 5.0468],
        [5.0638, 5.0575, 5.0631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.0986921414732933 
model_pd.l_d.mean(): -20.152925491333008 
model_pd.lagr.mean(): -20.05423355102539 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4617], device='cuda:0')), ('power', tensor([-21.0079], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.0986921414732933
epoch£º421	 i:1 	 global-step:8421	 l-p:0.12552839517593384
epoch£º421	 i:2 	 global-step:8422	 l-p:0.1154196485877037
epoch£º421	 i:3 	 global-step:8423	 l-p:0.13689981400966644
epoch£º421	 i:4 	 global-step:8424	 l-p:0.17563161253929138
epoch£º421	 i:5 	 global-step:8425	 l-p:0.06610873341560364
epoch£º421	 i:6 	 global-step:8426	 l-p:0.9600642919540405
epoch£º421	 i:7 	 global-step:8427	 l-p:0.14188599586486816
epoch£º421	 i:8 	 global-step:8428	 l-p:0.1346924602985382
epoch£º421	 i:9 	 global-step:8429	 l-p:0.1261848509311676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0813, 5.0662, 5.0781],
        [5.0813, 5.0813, 5.0813],
        [5.0813, 5.0806, 5.0813],
        [5.0813, 5.0986, 4.9693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.26193925738334656 
model_pd.l_d.mean(): -20.1154727935791 
model_pd.lagr.mean(): -19.853532791137695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4681], device='cuda:0')), ('power', tensor([-20.9764], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.26193925738334656
epoch£º422	 i:1 	 global-step:8441	 l-p:0.12924499809741974
epoch£º422	 i:2 	 global-step:8442	 l-p:0.11772819608449936
epoch£º422	 i:3 	 global-step:8443	 l-p:0.1228032335639
epoch£º422	 i:4 	 global-step:8444	 l-p:0.16117976605892181
epoch£º422	 i:5 	 global-step:8445	 l-p:0.090763621032238
epoch£º422	 i:6 	 global-step:8446	 l-p:0.08449719101190567
epoch£º422	 i:7 	 global-step:8447	 l-p:0.006347055081278086
epoch£º422	 i:8 	 global-step:8448	 l-p:0.15804162621498108
epoch£º422	 i:9 	 global-step:8449	 l-p:0.12385564297437668
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1458, 5.1458, 5.1458],
        [5.1458, 5.2204, 5.0881],
        [5.1458, 5.1285, 5.1415],
        [5.1458, 5.1210, 5.0180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.10774539411067963 
model_pd.l_d.mean(): -19.94565773010254 
model_pd.lagr.mean(): -19.83791160583496 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4661], device='cuda:0')), ('power', tensor([-20.8014], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.10774539411067963
epoch£º423	 i:1 	 global-step:8461	 l-p:17.573589324951172
epoch£º423	 i:2 	 global-step:8462	 l-p:0.3000760078430176
epoch£º423	 i:3 	 global-step:8463	 l-p:0.14090189337730408
epoch£º423	 i:4 	 global-step:8464	 l-p:0.190316841006279
epoch£º423	 i:5 	 global-step:8465	 l-p:0.11375478655099869
epoch£º423	 i:6 	 global-step:8466	 l-p:0.13419398665428162
epoch£º423	 i:7 	 global-step:8467	 l-p:0.1316891461610794
epoch£º423	 i:8 	 global-step:8468	 l-p:0.12369762361049652
epoch£º423	 i:9 	 global-step:8469	 l-p:0.12078098207712173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1833, 5.1260, 5.1214],
        [5.1833, 5.1810, 5.1831],
        [5.1833, 5.4571, 5.3910],
        [5.1833, 5.1833, 5.1833]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.12872079014778137 
model_pd.l_d.mean(): -19.990497589111328 
model_pd.lagr.mean(): -19.86177635192871 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4618], device='cuda:0')), ('power', tensor([-20.8426], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.12872079014778137
epoch£º424	 i:1 	 global-step:8481	 l-p:0.1334753781557083
epoch£º424	 i:2 	 global-step:8482	 l-p:-0.7220032215118408
epoch£º424	 i:3 	 global-step:8483	 l-p:0.10306339710950851
epoch£º424	 i:4 	 global-step:8484	 l-p:0.04906661808490753
epoch£º424	 i:5 	 global-step:8485	 l-p:0.14395225048065186
epoch£º424	 i:6 	 global-step:8486	 l-p:0.13338741660118103
epoch£º424	 i:7 	 global-step:8487	 l-p:0.09109244495630264
epoch£º424	 i:8 	 global-step:8488	 l-p:-0.004129895940423012
epoch£º424	 i:9 	 global-step:8489	 l-p:0.3013266623020172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0343, 4.9746, 4.9815],
        [5.0343, 4.9703, 4.9121],
        [5.0343, 5.1783, 5.0578],
        [5.0343, 5.0343, 5.0343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.1692701131105423 
model_pd.l_d.mean(): -19.364709854125977 
model_pd.lagr.mean(): -19.1954402923584 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4849], device='cuda:0')), ('power', tensor([-20.2290], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.1692701131105423
epoch£º425	 i:1 	 global-step:8501	 l-p:0.142689511179924
epoch£º425	 i:2 	 global-step:8502	 l-p:0.25570929050445557
epoch£º425	 i:3 	 global-step:8503	 l-p:-0.3005426526069641
epoch£º425	 i:4 	 global-step:8504	 l-p:0.09037797898054123
epoch£º425	 i:5 	 global-step:8505	 l-p:0.11829771101474762
epoch£º425	 i:6 	 global-step:8506	 l-p:0.11883998662233353
epoch£º425	 i:7 	 global-step:8507	 l-p:0.009915396571159363
epoch£º425	 i:8 	 global-step:8508	 l-p:0.07224202156066895
epoch£º425	 i:9 	 global-step:8509	 l-p:0.12274842709302902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1242, 5.1159, 5.1231],
        [5.1242, 5.1235, 5.1242],
        [5.1242, 5.1217, 5.1241],
        [5.1242, 5.4782, 5.4601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): -0.9631015658378601 
model_pd.l_d.mean(): -18.896442413330078 
model_pd.lagr.mean(): -19.85954475402832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5284], device='cuda:0')), ('power', tensor([-19.7970], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:-0.9631015658378601
epoch£º426	 i:1 	 global-step:8521	 l-p:0.1845170110464096
epoch£º426	 i:2 	 global-step:8522	 l-p:0.13564184308052063
epoch£º426	 i:3 	 global-step:8523	 l-p:0.14466822147369385
epoch£º426	 i:4 	 global-step:8524	 l-p:0.12259730696678162
epoch£º426	 i:5 	 global-step:8525	 l-p:0.12622590363025665
epoch£º426	 i:6 	 global-step:8526	 l-p:0.12424755096435547
epoch£º426	 i:7 	 global-step:8527	 l-p:0.0989033505320549
epoch£º426	 i:8 	 global-step:8528	 l-p:0.1519378274679184
epoch£º426	 i:9 	 global-step:8529	 l-p:0.11367754638195038
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0686, 5.0627, 5.0680],
        [5.0686, 5.0659, 5.0685],
        [5.0686, 5.0215, 5.0387],
        [5.0686, 5.0656, 4.9402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.16057471930980682 
model_pd.l_d.mean(): -19.65741539001465 
model_pd.lagr.mean(): -19.496841430664062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4467], device='cuda:0')), ('power', tensor([-20.4877], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.16057471930980682
epoch£º427	 i:1 	 global-step:8541	 l-p:0.10213089734315872
epoch£º427	 i:2 	 global-step:8542	 l-p:0.2519867718219757
epoch£º427	 i:3 	 global-step:8543	 l-p:0.11555004119873047
epoch£º427	 i:4 	 global-step:8544	 l-p:0.11993341147899628
epoch£º427	 i:5 	 global-step:8545	 l-p:-0.0022113847080618143
epoch£º427	 i:6 	 global-step:8546	 l-p:0.17334921658039093
epoch£º427	 i:7 	 global-step:8547	 l-p:0.0742095559835434
epoch£º427	 i:8 	 global-step:8548	 l-p:0.1539682000875473
epoch£º427	 i:9 	 global-step:8549	 l-p:0.14079095423221588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6811,  0.5993,  1.0000,  0.5273,
          1.0000,  0.8799, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[5.0023, 5.4520, 5.5060],
        [5.0023, 4.9342, 4.8766],
        [5.0023, 5.3180, 5.2813],
        [5.0023, 4.9420, 4.8633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.25298309326171875 
model_pd.l_d.mean(): -20.442729949951172 
model_pd.lagr.mean(): -20.189746856689453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4534], device='cuda:0')), ('power', tensor([-21.2945], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.25298309326171875
epoch£º428	 i:1 	 global-step:8561	 l-p:0.11909374594688416
epoch£º428	 i:2 	 global-step:8562	 l-p:0.10626305639743805
epoch£º428	 i:3 	 global-step:8563	 l-p:0.13380970060825348
epoch£º428	 i:4 	 global-step:8564	 l-p:0.12260463088750839
epoch£º428	 i:5 	 global-step:8565	 l-p:0.006186304148286581
epoch£º428	 i:6 	 global-step:8566	 l-p:0.16020435094833374
epoch£º428	 i:7 	 global-step:8567	 l-p:0.1316821128129959
epoch£º428	 i:8 	 global-step:8568	 l-p:0.171896830201149
epoch£º428	 i:9 	 global-step:8569	 l-p:0.11789003014564514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9563, 4.9059, 4.8035],
        [4.9563, 4.9562, 4.9563],
        [4.9563, 5.1251, 5.0130],
        [4.9563, 5.6240, 5.8540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.11032647639513016 
model_pd.l_d.mean(): -19.253742218017578 
model_pd.lagr.mean(): -19.143415451049805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5908], device='cuda:0')), ('power', tensor([-20.2257], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.11032647639513016
epoch£º429	 i:1 	 global-step:8581	 l-p:0.10573118180036545
epoch£º429	 i:2 	 global-step:8582	 l-p:0.1502126157283783
epoch£º429	 i:3 	 global-step:8583	 l-p:0.15662209689617157
epoch£º429	 i:4 	 global-step:8584	 l-p:0.16795678436756134
epoch£º429	 i:5 	 global-step:8585	 l-p:-0.011916255578398705
epoch£º429	 i:6 	 global-step:8586	 l-p:0.11027906090021133
epoch£º429	 i:7 	 global-step:8587	 l-p:0.11201648414134979
epoch£º429	 i:8 	 global-step:8588	 l-p:0.07626602798700333
epoch£º429	 i:9 	 global-step:8589	 l-p:0.11241397261619568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9906, 4.9590, 4.9785],
        [4.9906, 5.6647, 5.8970],
        [4.9906, 4.9294, 4.9400],
        [4.9906, 4.9892, 4.9905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.275077223777771 
model_pd.l_d.mean(): -19.196962356567383 
model_pd.lagr.mean(): -18.921884536743164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5935], device='cuda:0')), ('power', tensor([-20.1707], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.275077223777771
epoch£º430	 i:1 	 global-step:8601	 l-p:0.14836427569389343
epoch£º430	 i:2 	 global-step:8602	 l-p:0.13858604431152344
epoch£º430	 i:3 	 global-step:8603	 l-p:0.14887908101081848
epoch£º430	 i:4 	 global-step:8604	 l-p:0.13143028318881989
epoch£º430	 i:5 	 global-step:8605	 l-p:0.12164241820573807
epoch£º430	 i:6 	 global-step:8606	 l-p:0.11942777037620544
epoch£º430	 i:7 	 global-step:8607	 l-p:0.12093213945627213
epoch£º430	 i:8 	 global-step:8608	 l-p:0.11243705451488495
epoch£º430	 i:9 	 global-step:8609	 l-p:0.14787796139717102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9326, 5.0334, 4.8983],
        [4.9326, 5.5968, 5.8254],
        [4.9326, 4.9201, 4.9305],
        [4.9326, 4.8700, 4.7791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.03547333553433418 
model_pd.l_d.mean(): -19.95919418334961 
model_pd.lagr.mean(): -19.923721313476562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5245], device='cuda:0')), ('power', tensor([-20.8756], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.03547333553433418
epoch£º431	 i:1 	 global-step:8621	 l-p:0.10675740987062454
epoch£º431	 i:2 	 global-step:8622	 l-p:0.1372305303812027
epoch£º431	 i:3 	 global-step:8623	 l-p:0.13183607161045074
epoch£º431	 i:4 	 global-step:8624	 l-p:0.1645098775625229
epoch£º431	 i:5 	 global-step:8625	 l-p:0.0828743502497673
epoch£º431	 i:6 	 global-step:8626	 l-p:0.12729190289974213
epoch£º431	 i:7 	 global-step:8627	 l-p:0.13854941725730896
epoch£º431	 i:8 	 global-step:8628	 l-p:0.14052651822566986
epoch£º431	 i:9 	 global-step:8629	 l-p:0.1734614074230194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9375, 5.5299, 5.6983],
        [4.9375, 4.9374, 4.9375],
        [4.9375, 4.9188, 4.9331],
        [4.9375, 4.9035, 4.9241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.1445455402135849 
model_pd.l_d.mean(): -20.023855209350586 
model_pd.lagr.mean(): -19.879310607910156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5198], device='cuda:0')), ('power', tensor([-20.9366], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.1445455402135849
epoch£º432	 i:1 	 global-step:8641	 l-p:0.153810054063797
epoch£º432	 i:2 	 global-step:8642	 l-p:0.0978313460946083
epoch£º432	 i:3 	 global-step:8643	 l-p:0.12267076224088669
epoch£º432	 i:4 	 global-step:8644	 l-p:0.11246978491544724
epoch£º432	 i:5 	 global-step:8645	 l-p:0.13225945830345154
epoch£º432	 i:6 	 global-step:8646	 l-p:0.13715490698814392
epoch£º432	 i:7 	 global-step:8647	 l-p:0.0440542958676815
epoch£º432	 i:8 	 global-step:8648	 l-p:0.27963608503341675
epoch£º432	 i:9 	 global-step:8649	 l-p:0.14577503502368927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0283, 4.9577, 4.9050],
        [5.0283, 5.2115, 5.1035],
        [5.0283, 5.0283, 5.0283],
        [5.0283, 5.0023, 5.0200]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.13972970843315125 
model_pd.l_d.mean(): -19.942052841186523 
model_pd.lagr.mean(): -19.802322387695312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4993], device='cuda:0')), ('power', tensor([-20.8321], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.13972970843315125
epoch£º433	 i:1 	 global-step:8661	 l-p:0.1448621302843094
epoch£º433	 i:2 	 global-step:8662	 l-p:0.13168072700500488
epoch£º433	 i:3 	 global-step:8663	 l-p:0.36490383744239807
epoch£º433	 i:4 	 global-step:8664	 l-p:0.11350134760141373
epoch£º433	 i:5 	 global-step:8665	 l-p:0.049716223031282425
epoch£º433	 i:6 	 global-step:8666	 l-p:-0.031071742996573448
epoch£º433	 i:7 	 global-step:8667	 l-p:0.18727168440818787
epoch£º433	 i:8 	 global-step:8668	 l-p:0.14588139951229095
epoch£º433	 i:9 	 global-step:8669	 l-p:0.13885033130645752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6197,  0.5284,  1.0000,  0.4505,
          1.0000,  0.8526, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[5.0677, 5.0356, 4.9224],
        [5.0677, 5.3109, 5.2299],
        [5.0677, 4.9985, 4.9869],
        [5.0677, 4.9989, 4.9486]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.14526169002056122 
model_pd.l_d.mean(): -20.72434425354004 
model_pd.lagr.mean(): -20.579082489013672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4023], device='cuda:0')), ('power', tensor([-21.5286], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.14526169002056122
epoch£º434	 i:1 	 global-step:8681	 l-p:0.14633744955062866
epoch£º434	 i:2 	 global-step:8682	 l-p:0.11791028827428818
epoch£º434	 i:3 	 global-step:8683	 l-p:0.5022810101509094
epoch£º434	 i:4 	 global-step:8684	 l-p:0.045591697096824646
epoch£º434	 i:5 	 global-step:8685	 l-p:0.19644835591316223
epoch£º434	 i:6 	 global-step:8686	 l-p:0.1219341829419136
epoch£º434	 i:7 	 global-step:8687	 l-p:0.1511315554380417
epoch£º434	 i:8 	 global-step:8688	 l-p:0.1401742398738861
epoch£º434	 i:9 	 global-step:8689	 l-p:0.0986679419875145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0053, 5.0051, 5.0053],
        [5.0053, 5.0053, 5.0053],
        [5.0053, 5.0052, 5.0053],
        [5.0053, 5.5940, 5.7534]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.1537531316280365 
model_pd.l_d.mean(): -20.65770721435547 
model_pd.lagr.mean(): -20.50395393371582 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4203], device='cuda:0')), ('power', tensor([-21.4793], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.1537531316280365
epoch£º435	 i:1 	 global-step:8701	 l-p:0.09317976981401443
epoch£º435	 i:2 	 global-step:8702	 l-p:0.10865972191095352
epoch£º435	 i:3 	 global-step:8703	 l-p:0.12591218948364258
epoch£º435	 i:4 	 global-step:8704	 l-p:0.13639430701732635
epoch£º435	 i:5 	 global-step:8705	 l-p:0.09835837781429291
epoch£º435	 i:6 	 global-step:8706	 l-p:0.08129255473613739
epoch£º435	 i:7 	 global-step:8707	 l-p:0.17909018695354462
epoch£º435	 i:8 	 global-step:8708	 l-p:0.16956782341003418
epoch£º435	 i:9 	 global-step:8709	 l-p:0.13185131549835205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9253, 5.4648, 5.5911],
        [4.9253, 5.2165, 5.1669],
        [4.9253, 4.9171, 4.9243],
        [4.9253, 4.9930, 4.8495]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.10606233030557632 
model_pd.l_d.mean(): -20.32535171508789 
model_pd.lagr.mean(): -20.219289779663086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5114], device='cuda:0')), ('power', tensor([-21.2351], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.10606233030557632
epoch£º436	 i:1 	 global-step:8721	 l-p:0.10231157392263412
epoch£º436	 i:2 	 global-step:8722	 l-p:0.0955645814538002
epoch£º436	 i:3 	 global-step:8723	 l-p:-0.27882811427116394
epoch£º436	 i:4 	 global-step:8724	 l-p:0.12165113538503647
epoch£º436	 i:5 	 global-step:8725	 l-p:0.13866743445396423
epoch£º436	 i:6 	 global-step:8726	 l-p:0.14829811453819275
epoch£º436	 i:7 	 global-step:8727	 l-p:0.16188167035579681
epoch£º436	 i:8 	 global-step:8728	 l-p:0.14305931329727173
epoch£º436	 i:9 	 global-step:8729	 l-p:0.11397896707057953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0234, 5.0194, 5.0231],
        [5.0234, 5.7753, 6.0703],
        [5.0234, 5.2063, 5.0972],
        [5.0234, 5.2378, 5.1428]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.056001100689172745 
model_pd.l_d.mean(): -19.320993423461914 
model_pd.lagr.mean(): -19.264991760253906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4860], device='cuda:0')), ('power', tensor([-20.1856], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.056001100689172745
epoch£º437	 i:1 	 global-step:8741	 l-p:0.38906970620155334
epoch£º437	 i:2 	 global-step:8742	 l-p:0.17413249611854553
epoch£º437	 i:3 	 global-step:8743	 l-p:0.08013720065355301
epoch£º437	 i:4 	 global-step:8744	 l-p:0.05592953413724899
epoch£º437	 i:5 	 global-step:8745	 l-p:0.10672225803136826
epoch£º437	 i:6 	 global-step:8746	 l-p:0.13799728453159332
epoch£º437	 i:7 	 global-step:8747	 l-p:0.12600232660770416
epoch£º437	 i:8 	 global-step:8748	 l-p:0.1667889803647995
epoch£º437	 i:9 	 global-step:8749	 l-p:0.11026614159345627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2520, 5.1948, 5.1415],
        [5.2520, 5.3172, 5.1818],
        [5.2520, 5.2279, 5.2441],
        [5.2520, 5.2520, 5.2520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.12323468923568726 
model_pd.l_d.mean(): -19.226585388183594 
model_pd.lagr.mean(): -19.103351593017578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4196], device='cuda:0')), ('power', tensor([-20.0207], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.12323468923568726
epoch£º438	 i:1 	 global-step:8761	 l-p:0.13091184198856354
epoch£º438	 i:2 	 global-step:8762	 l-p:0.18007342517375946
epoch£º438	 i:3 	 global-step:8763	 l-p:0.10519470274448395
epoch£º438	 i:4 	 global-step:8764	 l-p:0.0796419084072113
epoch£º438	 i:5 	 global-step:8765	 l-p:0.1319820135831833
epoch£º438	 i:6 	 global-step:8766	 l-p:0.17589277029037476
epoch£º438	 i:7 	 global-step:8767	 l-p:0.11755689233541489
epoch£º438	 i:8 	 global-step:8768	 l-p:0.1396993100643158
epoch£º438	 i:9 	 global-step:8769	 l-p:0.11781714111566544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2423, 5.2124, 5.1111],
        [5.2423, 5.2330, 5.1156],
        [5.2423, 5.2368, 5.1172],
        [5.2423, 5.2421, 5.2423]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.08694391697645187 
model_pd.l_d.mean(): -20.59040641784668 
model_pd.lagr.mean(): -20.503461837768555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3572], device='cuda:0')), ('power', tensor([-21.3454], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.08694391697645187
epoch£º439	 i:1 	 global-step:8781	 l-p:0.11773151159286499
epoch£º439	 i:2 	 global-step:8782	 l-p:0.20668408274650574
epoch£º439	 i:3 	 global-step:8783	 l-p:0.11663727462291718
epoch£º439	 i:4 	 global-step:8784	 l-p:0.128544420003891
epoch£º439	 i:5 	 global-step:8785	 l-p:0.5907698273658752
epoch£º439	 i:6 	 global-step:8786	 l-p:0.132443368434906
epoch£º439	 i:7 	 global-step:8787	 l-p:0.18419112265110016
epoch£º439	 i:8 	 global-step:8788	 l-p:0.14258724451065063
epoch£º439	 i:9 	 global-step:8789	 l-p:0.12440580129623413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1203, 5.0947, 5.1120],
        [5.1203, 5.0997, 5.1148],
        [5.1203, 5.0510, 5.0351],
        [5.1203, 5.2932, 5.1786]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.058389123529195786 
model_pd.l_d.mean(): -19.22385025024414 
model_pd.lagr.mean(): -19.16546058654785 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4734], device='cuda:0')), ('power', tensor([-20.0737], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.058389123529195786
epoch£º440	 i:1 	 global-step:8801	 l-p:0.1262938678264618
epoch£º440	 i:2 	 global-step:8802	 l-p:0.10942013561725616
epoch£º440	 i:3 	 global-step:8803	 l-p:0.15464378893375397
epoch£º440	 i:4 	 global-step:8804	 l-p:0.1419801563024521
epoch£º440	 i:5 	 global-step:8805	 l-p:0.1332179456949234
epoch£º440	 i:6 	 global-step:8806	 l-p:-13.888969421386719
epoch£º440	 i:7 	 global-step:8807	 l-p:0.08892125636339188
epoch£º440	 i:8 	 global-step:8808	 l-p:0.14692798256874084
epoch£º440	 i:9 	 global-step:8809	 l-p:0.13911208510398865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0543, 5.0463, 5.0533],
        [5.0543, 5.2118, 5.0918],
        [5.0543, 5.0535, 5.0543],
        [5.0543, 4.9924, 4.9088]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.04759584367275238 
model_pd.l_d.mean(): -19.01170539855957 
model_pd.lagr.mean(): -18.964109420776367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5404], device='cuda:0')), ('power', tensor([-19.9269], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.04759584367275238
epoch£º441	 i:1 	 global-step:8821	 l-p:0.13789914548397064
epoch£º441	 i:2 	 global-step:8822	 l-p:0.9656317830085754
epoch£º441	 i:3 	 global-step:8823	 l-p:0.14716626703739166
epoch£º441	 i:4 	 global-step:8824	 l-p:-0.013790030032396317
epoch£º441	 i:5 	 global-step:8825	 l-p:0.13255199790000916
epoch£º441	 i:6 	 global-step:8826	 l-p:0.12555471062660217
epoch£º441	 i:7 	 global-step:8827	 l-p:0.16089759767055511
epoch£º441	 i:8 	 global-step:8828	 l-p:0.11394807696342468
epoch£º441	 i:9 	 global-step:8829	 l-p:0.12160033732652664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1838, 5.1823, 5.1837],
        [5.1838, 5.6067, 5.6285],
        [5.1838, 5.2283, 5.0901],
        [5.1838, 5.1644, 5.1788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.12305296957492828 
model_pd.l_d.mean(): -20.117015838623047 
model_pd.lagr.mean(): -19.99396324157715 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4117], device='cuda:0')), ('power', tensor([-20.9196], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.12305296957492828
epoch£º442	 i:1 	 global-step:8841	 l-p:0.1299331784248352
epoch£º442	 i:2 	 global-step:8842	 l-p:0.35875019431114197
epoch£º442	 i:3 	 global-step:8843	 l-p:0.14836888015270233
epoch£º442	 i:4 	 global-step:8844	 l-p:0.1485155075788498
epoch£º442	 i:5 	 global-step:8845	 l-p:-0.2251087874174118
epoch£º442	 i:6 	 global-step:8846	 l-p:0.1354273110628128
epoch£º442	 i:7 	 global-step:8847	 l-p:0.13767211139202118
epoch£º442	 i:8 	 global-step:8848	 l-p:0.11739765852689743
epoch£º442	 i:9 	 global-step:8849	 l-p:-0.9261994957923889
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1701, 5.1340, 5.1537],
        [5.1701, 5.1694, 5.1701],
        [5.1701, 5.1691, 5.1701],
        [5.1701, 5.1233, 5.1415]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.11283879727125168 
model_pd.l_d.mean(): -19.88865089416504 
model_pd.lagr.mean(): -19.77581214904785 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4227], device='cuda:0')), ('power', tensor([-20.6983], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.11283879727125168
epoch£º443	 i:1 	 global-step:8861	 l-p:0.13304099440574646
epoch£º443	 i:2 	 global-step:8862	 l-p:0.12813472747802734
epoch£º443	 i:3 	 global-step:8863	 l-p:-1.0755157470703125
epoch£º443	 i:4 	 global-step:8864	 l-p:0.15813684463500977
epoch£º443	 i:5 	 global-step:8865	 l-p:0.13718007504940033
epoch£º443	 i:6 	 global-step:8866	 l-p:0.11637132614850998
epoch£º443	 i:7 	 global-step:8867	 l-p:0.02733413688838482
epoch£º443	 i:8 	 global-step:8868	 l-p:0.12755808234214783
epoch£º443	 i:9 	 global-step:8869	 l-p:0.49176040291786194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0729, 5.2180, 5.0927],
        [5.0729, 5.0246, 5.0446],
        [5.0729, 4.9986, 4.9563],
        [5.0729, 5.1611, 5.0212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): 7.499884605407715 
model_pd.l_d.mean(): -18.824939727783203 
model_pd.lagr.mean(): -11.325055122375488 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5340], device='cuda:0')), ('power', tensor([-19.7300], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:7.499884605407715
epoch£º444	 i:1 	 global-step:8881	 l-p:0.1415732502937317
epoch£º444	 i:2 	 global-step:8882	 l-p:0.13368333876132965
epoch£º444	 i:3 	 global-step:8883	 l-p:0.12706054747104645
epoch£º444	 i:4 	 global-step:8884	 l-p:0.7785126566886902
epoch£º444	 i:5 	 global-step:8885	 l-p:0.12827810645103455
epoch£º444	 i:6 	 global-step:8886	 l-p:0.14730268716812134
epoch£º444	 i:7 	 global-step:8887	 l-p:0.15720616281032562
epoch£º444	 i:8 	 global-step:8888	 l-p:0.16594642400741577
epoch£º444	 i:9 	 global-step:8889	 l-p:-0.16705085337162018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9753, 4.9749, 4.9753],
        [4.9753, 4.9000, 4.8263],
        [4.9753, 4.9646, 4.9738],
        [4.9753, 4.9025, 4.9055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): -0.22654172778129578 
model_pd.l_d.mean(): -20.47662353515625 
model_pd.lagr.mean(): -20.70316505432129 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4636], device='cuda:0')), ('power', tensor([-21.3396], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:-0.22654172778129578
epoch£º445	 i:1 	 global-step:8901	 l-p:0.1405852884054184
epoch£º445	 i:2 	 global-step:8902	 l-p:0.10262129455804825
epoch£º445	 i:3 	 global-step:8903	 l-p:0.008141702972352505
epoch£º445	 i:4 	 global-step:8904	 l-p:0.136104553937912
epoch£º445	 i:5 	 global-step:8905	 l-p:0.166814386844635
epoch£º445	 i:6 	 global-step:8906	 l-p:0.10872757434844971
epoch£º445	 i:7 	 global-step:8907	 l-p:0.11620637774467468
epoch£º445	 i:8 	 global-step:8908	 l-p:0.13282427191734314
epoch£º445	 i:9 	 global-step:8909	 l-p:0.12853646278381348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[5.0029, 4.9292, 4.8559],
        [5.0029, 5.4566, 5.5107],
        [5.0029, 4.9602, 4.8405],
        [5.0029, 5.4827, 5.5559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.11155588179826736 
model_pd.l_d.mean(): -20.043258666992188 
model_pd.lagr.mean(): -19.931703567504883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4991], device='cuda:0')), ('power', tensor([-20.9349], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.11155588179826736
epoch£º446	 i:1 	 global-step:8921	 l-p:0.14054082334041595
epoch£º446	 i:2 	 global-step:8922	 l-p:0.1310524344444275
epoch£º446	 i:3 	 global-step:8923	 l-p:0.09916073828935623
epoch£º446	 i:4 	 global-step:8924	 l-p:0.14096961915493011
epoch£º446	 i:5 	 global-step:8925	 l-p:0.11818668246269226
epoch£º446	 i:6 	 global-step:8926	 l-p:0.1419566124677658
epoch£º446	 i:7 	 global-step:8927	 l-p:0.148492231965065
epoch£º446	 i:8 	 global-step:8928	 l-p:0.04658140614628792
epoch£º446	 i:9 	 global-step:8929	 l-p:0.07693032175302505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9699, 4.9649, 4.9695],
        [4.9699, 4.9303, 4.9526],
        [4.9699, 4.9010, 4.8081],
        [4.9699, 4.9033, 4.8064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.10360297560691833 
model_pd.l_d.mean(): -20.230825424194336 
model_pd.lagr.mean(): -20.127222061157227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4915], device='cuda:0')), ('power', tensor([-21.1182], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.10360297560691833
epoch£º447	 i:1 	 global-step:8941	 l-p:0.14438396692276
epoch£º447	 i:2 	 global-step:8942	 l-p:0.14985515177249908
epoch£º447	 i:3 	 global-step:8943	 l-p:0.13674892485141754
epoch£º447	 i:4 	 global-step:8944	 l-p:0.13414181768894196
epoch£º447	 i:5 	 global-step:8945	 l-p:-0.013776635751128197
epoch£º447	 i:6 	 global-step:8946	 l-p:0.1475512534379959
epoch£º447	 i:7 	 global-step:8947	 l-p:0.09930220991373062
epoch£º447	 i:8 	 global-step:8948	 l-p:0.09820928424596786
epoch£º447	 i:9 	 global-step:8949	 l-p:0.11875095218420029
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9804, 5.5482, 5.6907],
        [4.9804, 5.3651, 5.3713],
        [4.9804, 4.9509, 4.9706],
        [4.9804, 4.9803, 4.9804]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.13660511374473572 
model_pd.l_d.mean(): -19.665224075317383 
model_pd.lagr.mean(): -19.52861976623535 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5325], device='cuda:0')), ('power', tensor([-20.5845], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.13660511374473572
epoch£º448	 i:1 	 global-step:8961	 l-p:0.09933570772409439
epoch£º448	 i:2 	 global-step:8962	 l-p:0.14760129153728485
epoch£º448	 i:3 	 global-step:8963	 l-p:0.011210317723453045
epoch£º448	 i:4 	 global-step:8964	 l-p:0.09638171643018723
epoch£º448	 i:5 	 global-step:8965	 l-p:0.08857128024101257
epoch£º448	 i:6 	 global-step:8966	 l-p:0.13647694885730743
epoch£º448	 i:7 	 global-step:8967	 l-p:0.13320039212703705
epoch£º448	 i:8 	 global-step:8968	 l-p:0.2192915678024292
epoch£º448	 i:9 	 global-step:8969	 l-p:0.13734939694404602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9337, 4.9314, 4.9335],
        [4.9337, 4.8497, 4.8349],
        [4.9337, 4.8925, 4.9155],
        [4.9337, 4.8836, 4.9066]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.11432158946990967 
model_pd.l_d.mean(): -20.60822105407715 
model_pd.lagr.mean(): -20.493900299072266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4642], device='cuda:0')), ('power', tensor([-21.4743], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.11432158946990967
epoch£º449	 i:1 	 global-step:8981	 l-p:0.1088709905743599
epoch£º449	 i:2 	 global-step:8982	 l-p:0.1699809432029724
epoch£º449	 i:3 	 global-step:8983	 l-p:0.1732780933380127
epoch£º449	 i:4 	 global-step:8984	 l-p:0.15968510508537292
epoch£º449	 i:5 	 global-step:8985	 l-p:0.0638061910867691
epoch£º449	 i:6 	 global-step:8986	 l-p:0.14131921529769897
epoch£º449	 i:7 	 global-step:8987	 l-p:0.12237008661031723
epoch£º449	 i:8 	 global-step:8988	 l-p:0.17771588265895844
epoch£º449	 i:9 	 global-step:8989	 l-p:0.110650435090065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9691, 4.9686, 4.9691],
        [4.9691, 4.9689, 4.9691],
        [4.9691, 4.9658, 4.9689],
        [4.9691, 5.2129, 5.1315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.09133155643939972 
model_pd.l_d.mean(): -20.51902961730957 
model_pd.lagr.mean(): -20.427698135375977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4656], device='cuda:0')), ('power', tensor([-21.3850], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.09133155643939972
epoch£º450	 i:1 	 global-step:9001	 l-p:0.15698108077049255
epoch£º450	 i:2 	 global-step:9002	 l-p:0.12009664624929428
epoch£º450	 i:3 	 global-step:9003	 l-p:0.12764815986156464
epoch£º450	 i:4 	 global-step:9004	 l-p:0.07847528904676437
epoch£º450	 i:5 	 global-step:9005	 l-p:-0.22702988982200623
epoch£º450	 i:6 	 global-step:9006	 l-p:0.1763937771320343
epoch£º450	 i:7 	 global-step:9007	 l-p:0.14883384108543396
epoch£º450	 i:8 	 global-step:9008	 l-p:0.09493112564086914
epoch£º450	 i:9 	 global-step:9009	 l-p:0.12102328985929489
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0159, 4.9927, 5.0097],
        [5.0159, 4.9618, 4.9828],
        [5.0159, 5.0158, 5.0159],
        [5.0159, 5.0157, 5.0159]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.13441655039787292 
model_pd.l_d.mean(): -18.626672744750977 
model_pd.lagr.mean(): -18.49225616455078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5848], device='cuda:0')), ('power', tensor([-19.5807], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.13441655039787292
epoch£º451	 i:1 	 global-step:9021	 l-p:0.10687363147735596
epoch£º451	 i:2 	 global-step:9022	 l-p:0.14121344685554504
epoch£º451	 i:3 	 global-step:9023	 l-p:0.15706922113895416
epoch£º451	 i:4 	 global-step:9024	 l-p:0.07429686188697815
epoch£º451	 i:5 	 global-step:9025	 l-p:0.11938934028148651
epoch£º451	 i:6 	 global-step:9026	 l-p:-0.7180320024490356
epoch£º451	 i:7 	 global-step:9027	 l-p:0.14923898875713348
epoch£º451	 i:8 	 global-step:9028	 l-p:0.13485437631607056
epoch£º451	 i:9 	 global-step:9029	 l-p:0.04949990659952164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0246, 4.9598, 4.9746],
        [5.0246, 5.0232, 5.0245],
        [5.0246, 5.0218, 5.0244],
        [5.0246, 5.0246, 5.0246]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.2433670163154602 
model_pd.l_d.mean(): -19.93687629699707 
model_pd.lagr.mean(): -19.693510055541992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5032], device='cuda:0')), ('power', tensor([-20.8308], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.2433670163154602
epoch£º452	 i:1 	 global-step:9041	 l-p:-0.4716132879257202
epoch£º452	 i:2 	 global-step:9042	 l-p:0.10854759067296982
epoch£º452	 i:3 	 global-step:9043	 l-p:0.12606750428676605
epoch£º452	 i:4 	 global-step:9044	 l-p:0.1276082843542099
epoch£º452	 i:5 	 global-step:9045	 l-p:0.07282495498657227
epoch£º452	 i:6 	 global-step:9046	 l-p:0.1212310940027237
epoch£º452	 i:7 	 global-step:9047	 l-p:0.03840748593211174
epoch£º452	 i:8 	 global-step:9048	 l-p:0.1339375525712967
epoch£º452	 i:9 	 global-step:9049	 l-p:0.13633543252944946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0706, 5.0692, 5.0706],
        [5.0706, 5.0706, 5.0706],
        [5.0706, 5.0685, 5.0705],
        [5.0706, 5.0324, 5.0542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.2477380484342575 
model_pd.l_d.mean(): -20.529212951660156 
model_pd.lagr.mean(): -20.281475067138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4298], device='cuda:0')), ('power', tensor([-21.3582], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.2477380484342575
epoch£º453	 i:1 	 global-step:9061	 l-p:0.14295846223831177
epoch£º453	 i:2 	 global-step:9062	 l-p:0.06645752489566803
epoch£º453	 i:3 	 global-step:9063	 l-p:0.07810674607753754
epoch£º453	 i:4 	 global-step:9064	 l-p:0.1265348643064499
epoch£º453	 i:5 	 global-step:9065	 l-p:0.12966766953468323
epoch£º453	 i:6 	 global-step:9066	 l-p:0.2641119658946991
epoch£º453	 i:7 	 global-step:9067	 l-p:0.07560692727565765
epoch£º453	 i:8 	 global-step:9068	 l-p:0.11020982265472412
epoch£º453	 i:9 	 global-step:9069	 l-p:0.13920530676841736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1318, 5.0612, 5.0618],
        [5.1318, 5.2659, 5.1337],
        [5.1318, 5.1290, 5.1316],
        [5.1318, 5.1197, 5.1298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.1428460031747818 
model_pd.l_d.mean(): -20.23798942565918 
model_pd.lagr.mean(): -20.095144271850586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4137], device='cuda:0')), ('power', tensor([-21.0449], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.1428460031747818
epoch£º454	 i:1 	 global-step:9081	 l-p:-0.14468620717525482
epoch£º454	 i:2 	 global-step:9082	 l-p:0.17001785337924957
epoch£º454	 i:3 	 global-step:9083	 l-p:0.14679047465324402
epoch£º454	 i:4 	 global-step:9084	 l-p:0.1566806137561798
epoch£º454	 i:5 	 global-step:9085	 l-p:0.12258466333150864
epoch£º454	 i:6 	 global-step:9086	 l-p:0.11642705649137497
epoch£º454	 i:7 	 global-step:9087	 l-p:0.09890542179346085
epoch£º454	 i:8 	 global-step:9088	 l-p:-0.01784658432006836
epoch£º454	 i:9 	 global-step:9089	 l-p:0.13322857022285461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0272, 4.9493, 4.8726],
        [5.0272, 5.0267, 5.0272],
        [5.0272, 5.0270, 5.0272],
        [5.0272, 5.0272, 5.0272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.1302034556865692 
model_pd.l_d.mean(): -20.0535831451416 
model_pd.lagr.mean(): -19.92337989807129 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4910], device='cuda:0')), ('power', tensor([-20.9371], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.1302034556865692
epoch£º455	 i:1 	 global-step:9101	 l-p:0.12079516798257828
epoch£º455	 i:2 	 global-step:9102	 l-p:0.1518988162279129
epoch£º455	 i:3 	 global-step:9103	 l-p:0.12639421224594116
epoch£º455	 i:4 	 global-step:9104	 l-p:0.09823019802570343
epoch£º455	 i:5 	 global-step:9105	 l-p:0.11409682780504227
epoch£º455	 i:6 	 global-step:9106	 l-p:0.12815044820308685
epoch£º455	 i:7 	 global-step:9107	 l-p:0.28161948919296265
epoch£º455	 i:8 	 global-step:9108	 l-p:0.17168134450912476
epoch£º455	 i:9 	 global-step:9109	 l-p:0.1299145370721817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8453, 4.8453, 4.8453],
        [4.8453, 4.7742, 4.7932],
        [4.8453, 4.8447, 4.8453],
        [4.8453, 4.8423, 4.8451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.11558938771486282 
model_pd.l_d.mean(): -20.286415100097656 
model_pd.lagr.mean(): -20.170825958251953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5095], device='cuda:0')), ('power', tensor([-21.1934], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.11558938771486282
epoch£º456	 i:1 	 global-step:9121	 l-p:0.16484226286411285
epoch£º456	 i:2 	 global-step:9122	 l-p:0.15402241051197052
epoch£º456	 i:3 	 global-step:9123	 l-p:0.6396273374557495
epoch£º456	 i:4 	 global-step:9124	 l-p:0.13574253022670746
epoch£º456	 i:5 	 global-step:9125	 l-p:0.18485915660858154
epoch£º456	 i:6 	 global-step:9126	 l-p:0.14318819344043732
epoch£º456	 i:7 	 global-step:9127	 l-p:0.22097022831439972
epoch£º456	 i:8 	 global-step:9128	 l-p:0.14973662793636322
epoch£º456	 i:9 	 global-step:9129	 l-p:0.09778129309415817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9134, 5.0761, 4.9541],
        [4.9134, 4.8386, 4.8517],
        [4.9134, 4.9092, 4.7551],
        [4.9134, 4.9134, 4.9134]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.2029964029788971 
model_pd.l_d.mean(): -20.72028160095215 
model_pd.lagr.mean(): -20.517284393310547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4559], device='cuda:0')), ('power', tensor([-21.5800], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.2029964029788971
epoch£º457	 i:1 	 global-step:9141	 l-p:0.08576323091983795
epoch£º457	 i:2 	 global-step:9142	 l-p:0.14601710438728333
epoch£º457	 i:3 	 global-step:9143	 l-p:0.14025148749351501
epoch£º457	 i:4 	 global-step:9144	 l-p:0.1068374440073967
epoch£º457	 i:5 	 global-step:9145	 l-p:0.1537671685218811
epoch£º457	 i:6 	 global-step:9146	 l-p:0.12422449886798859
epoch£º457	 i:7 	 global-step:9147	 l-p:0.11760281026363373
epoch£º457	 i:8 	 global-step:9148	 l-p:0.03369438275694847
epoch£º457	 i:9 	 global-step:9149	 l-p:0.12461476027965546
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0757, 5.0757, 5.0757],
        [5.0757, 5.0328, 5.0554],
        [5.0757, 5.0757, 5.0757],
        [5.0757, 5.0756, 5.0757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.14530393481254578 
model_pd.l_d.mean(): -20.560373306274414 
model_pd.lagr.mean(): -20.415069580078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4243], device='cuda:0')), ('power', tensor([-21.3843], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.14530393481254578
epoch£º458	 i:1 	 global-step:9161	 l-p:0.11836637556552887
epoch£º458	 i:2 	 global-step:9162	 l-p:0.13222090899944305
epoch£º458	 i:3 	 global-step:9163	 l-p:0.1859372854232788
epoch£º458	 i:4 	 global-step:9164	 l-p:0.11472955346107483
epoch£º458	 i:5 	 global-step:9165	 l-p:0.1257137507200241
epoch£º458	 i:6 	 global-step:9166	 l-p:-0.018289703875780106
epoch£º458	 i:7 	 global-step:9167	 l-p:0.13231070339679718
epoch£º458	 i:8 	 global-step:9168	 l-p:0.12251906096935272
epoch£º458	 i:9 	 global-step:9169	 l-p:0.23004965484142303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1191, 5.1191, 5.1191],
        [5.1191, 5.2479, 5.1125],
        [5.1191, 5.8660, 6.1438],
        [5.1191, 5.1191, 5.1191]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.139081671833992 
model_pd.l_d.mean(): -20.061124801635742 
model_pd.lagr.mean(): -19.922042846679688 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4201], device='cuda:0')), ('power', tensor([-20.8714], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.139081671833992
epoch£º459	 i:1 	 global-step:9181	 l-p:0.1434451788663864
epoch£º459	 i:2 	 global-step:9182	 l-p:0.11550208181142807
epoch£º459	 i:3 	 global-step:9183	 l-p:0.24704581499099731
epoch£º459	 i:4 	 global-step:9184	 l-p:0.0733208954334259
epoch£º459	 i:5 	 global-step:9185	 l-p:0.05779556557536125
epoch£º459	 i:6 	 global-step:9186	 l-p:0.13593332469463348
epoch£º459	 i:7 	 global-step:9187	 l-p:0.11304061859846115
epoch£º459	 i:8 	 global-step:9188	 l-p:0.15135355293750763
epoch£º459	 i:9 	 global-step:9189	 l-p:0.04155294969677925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0450, 5.0352, 5.0437],
        [5.0450, 4.9652, 4.8898],
        [5.0450, 5.7071, 5.9187],
        [5.0450, 5.0696, 4.9175]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.1231393963098526 
model_pd.l_d.mean(): -18.63924217224121 
model_pd.lagr.mean(): -18.516101837158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5678], device='cuda:0')), ('power', tensor([-19.5759], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.1231393963098526
epoch£º460	 i:1 	 global-step:9201	 l-p:0.1305035501718521
epoch£º460	 i:2 	 global-step:9202	 l-p:0.12591490149497986
epoch£º460	 i:3 	 global-step:9203	 l-p:0.2493450939655304
epoch£º460	 i:4 	 global-step:9204	 l-p:0.10850546509027481
epoch£º460	 i:5 	 global-step:9205	 l-p:0.1453040987253189
epoch£º460	 i:6 	 global-step:9206	 l-p:0.14399734139442444
epoch£º460	 i:7 	 global-step:9207	 l-p:0.13801352679729462
epoch£º460	 i:8 	 global-step:9208	 l-p:0.1013275757431984
epoch£º460	 i:9 	 global-step:9209	 l-p:0.11090633273124695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9496, 4.9475, 4.9495],
        [4.9496, 4.8590, 4.7925],
        [4.9496, 4.9019, 4.9264],
        [4.9496, 5.0243, 4.8727]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.1685386300086975 
model_pd.l_d.mean(): -18.435441970825195 
model_pd.lagr.mean(): -18.266902923583984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5807], device='cuda:0')), ('power', tensor([-19.3816], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.1685386300086975
epoch£º461	 i:1 	 global-step:9221	 l-p:0.03958277031779289
epoch£º461	 i:2 	 global-step:9222	 l-p:0.11167533695697784
epoch£º461	 i:3 	 global-step:9223	 l-p:0.1595221310853958
epoch£º461	 i:4 	 global-step:9224	 l-p:0.13876372575759888
epoch£º461	 i:5 	 global-step:9225	 l-p:0.12977610528469086
epoch£º461	 i:6 	 global-step:9226	 l-p:0.15945041179656982
epoch£º461	 i:7 	 global-step:9227	 l-p:0.07794924825429916
epoch£º461	 i:8 	 global-step:9228	 l-p:0.0975128561258316
epoch£º461	 i:9 	 global-step:9229	 l-p:0.11670015007257462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0163, 4.9690, 4.8384],
        [5.0163, 4.9436, 4.8422],
        [5.0163, 4.9742, 4.9977],
        [5.0163, 4.9747, 4.9982]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.09198428690433502 
model_pd.l_d.mean(): -20.30522918701172 
model_pd.lagr.mean(): -20.213245391845703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4716], device='cuda:0')), ('power', tensor([-21.1734], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.09198428690433502
epoch£º462	 i:1 	 global-step:9241	 l-p:0.28979262709617615
epoch£º462	 i:2 	 global-step:9242	 l-p:0.14558430016040802
epoch£º462	 i:3 	 global-step:9243	 l-p:0.09646064788103104
epoch£º462	 i:4 	 global-step:9244	 l-p:-0.2715211510658264
epoch£º462	 i:5 	 global-step:9245	 l-p:0.11929444968700409
epoch£º462	 i:6 	 global-step:9246	 l-p:0.1505829244852066
epoch£º462	 i:7 	 global-step:9247	 l-p:0.05167968571186066
epoch£º462	 i:8 	 global-step:9248	 l-p:0.0904022604227066
epoch£º462	 i:9 	 global-step:9249	 l-p:0.13993433117866516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0828, 5.0812, 5.0827],
        [5.0828, 5.0717, 5.0812],
        [5.0828, 5.0729, 5.0815],
        [5.0828, 5.0827, 5.0828]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.177259162068367 
model_pd.l_d.mean(): -18.085472106933594 
model_pd.lagr.mean(): -17.908212661743164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5948], device='cuda:0')), ('power', tensor([-19.0397], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.177259162068367
epoch£º463	 i:1 	 global-step:9261	 l-p:0.5433557629585266
epoch£º463	 i:2 	 global-step:9262	 l-p:0.14415155351161957
epoch£º463	 i:3 	 global-step:9263	 l-p:0.023517651483416557
epoch£º463	 i:4 	 global-step:9264	 l-p:0.1259482353925705
epoch£º463	 i:5 	 global-step:9265	 l-p:0.1175771951675415
epoch£º463	 i:6 	 global-step:9266	 l-p:0.12260888516902924
epoch£º463	 i:7 	 global-step:9267	 l-p:0.12925934791564941
epoch£º463	 i:8 	 global-step:9268	 l-p:0.05666089057922363
epoch£º463	 i:9 	 global-step:9269	 l-p:0.11752137541770935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1229, 5.1192, 5.1226],
        [5.1229, 5.2050, 5.0569],
        [5.1229, 5.3198, 5.2086],
        [5.1229, 5.1229, 5.1229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): -0.03953847289085388 
model_pd.l_d.mean(): -20.418365478515625 
model_pd.lagr.mean(): -20.457904815673828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4235], device='cuda:0')), ('power', tensor([-21.2388], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:-0.03953847289085388
epoch£º464	 i:1 	 global-step:9281	 l-p:0.17740893363952637
epoch£º464	 i:2 	 global-step:9282	 l-p:0.05346978083252907
epoch£º464	 i:3 	 global-step:9283	 l-p:0.13191600143909454
epoch£º464	 i:4 	 global-step:9284	 l-p:0.11152925342321396
epoch£º464	 i:5 	 global-step:9285	 l-p:0.12089996039867401
epoch£º464	 i:6 	 global-step:9286	 l-p:0.22135458886623383
epoch£º464	 i:7 	 global-step:9287	 l-p:0.14280004799365997
epoch£º464	 i:8 	 global-step:9288	 l-p:0.1706760823726654
epoch£º464	 i:9 	 global-step:9289	 l-p:0.13191819190979004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1164, 5.1164, 5.1164],
        [5.1164, 5.1149, 5.1163],
        [5.1164, 5.0958, 5.1114],
        [5.1164, 5.0332, 4.9887]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.1282246708869934 
model_pd.l_d.mean(): -19.96349334716797 
model_pd.lagr.mean(): -19.835268020629883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4851], device='cuda:0')), ('power', tensor([-20.8392], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.1282246708869934
epoch£º465	 i:1 	 global-step:9301	 l-p:0.2527223825454712
epoch£º465	 i:2 	 global-step:9302	 l-p:0.1226467490196228
epoch£º465	 i:3 	 global-step:9303	 l-p:0.13258124887943268
epoch£º465	 i:4 	 global-step:9304	 l-p:0.11957108974456787
epoch£º465	 i:5 	 global-step:9305	 l-p:0.12820294499397278
epoch£º465	 i:6 	 global-step:9306	 l-p:0.13426461815834045
epoch£º465	 i:7 	 global-step:9307	 l-p:0.15568912029266357
epoch£º465	 i:8 	 global-step:9308	 l-p:0.14167359471321106
epoch£º465	 i:9 	 global-step:9309	 l-p:0.09453817456960678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0158, 4.9298, 4.9199],
        [5.0158, 4.9251, 4.8823],
        [5.0158, 4.9997, 5.0127],
        [5.0158, 5.0045, 4.8534]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.1373610645532608 
model_pd.l_d.mean(): -18.829866409301758 
model_pd.lagr.mean(): -18.6925048828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5443], device='cuda:0')), ('power', tensor([-19.7457], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.1373610645532608
epoch£º466	 i:1 	 global-step:9321	 l-p:0.12498971074819565
epoch£º466	 i:2 	 global-step:9322	 l-p:0.1366037279367447
epoch£º466	 i:3 	 global-step:9323	 l-p:0.1293065845966339
epoch£º466	 i:4 	 global-step:9324	 l-p:0.5489362478256226
epoch£º466	 i:5 	 global-step:9325	 l-p:0.042763371020555496
epoch£º466	 i:6 	 global-step:9326	 l-p:0.11268861591815948
epoch£º466	 i:7 	 global-step:9327	 l-p:0.07657420635223389
epoch£º466	 i:8 	 global-step:9328	 l-p:0.12467242777347565
epoch£º466	 i:9 	 global-step:9329	 l-p:0.1105099469423294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0634, 5.0633, 5.0634],
        [5.0634, 5.1889, 5.0497],
        [5.0634, 5.0393, 5.0570],
        [5.0634, 4.9758, 4.9474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.12486582249403 
model_pd.l_d.mean(): -20.16621208190918 
model_pd.lagr.mean(): -20.041345596313477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4646], device='cuda:0')), ('power', tensor([-21.0245], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.12486582249403
epoch£º467	 i:1 	 global-step:9341	 l-p:0.18326112627983093
epoch£º467	 i:2 	 global-step:9342	 l-p:0.28732651472091675
epoch£º467	 i:3 	 global-step:9343	 l-p:0.1206369549036026
epoch£º467	 i:4 	 global-step:9344	 l-p:0.14304985105991364
epoch£º467	 i:5 	 global-step:9345	 l-p:0.5055229663848877
epoch£º467	 i:6 	 global-step:9346	 l-p:0.04725237563252449
epoch£º467	 i:7 	 global-step:9347	 l-p:0.04537391662597656
epoch£º467	 i:8 	 global-step:9348	 l-p:0.11637573689222336
epoch£º467	 i:9 	 global-step:9349	 l-p:0.11352716386318207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1928, 5.1588, 5.1799],
        [5.1928, 5.3508, 5.2223],
        [5.1928, 5.1391, 5.1596],
        [5.1928, 5.8690, 6.0790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.38964831829071045 
model_pd.l_d.mean(): -20.356204986572266 
model_pd.lagr.mean(): -19.966556549072266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4155], device='cuda:0')), ('power', tensor([-21.1672], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.38964831829071045
epoch£º468	 i:1 	 global-step:9361	 l-p:0.10627850145101547
epoch£º468	 i:2 	 global-step:9362	 l-p:0.1262563318014145
epoch£º468	 i:3 	 global-step:9363	 l-p:0.2044529914855957
epoch£º468	 i:4 	 global-step:9364	 l-p:0.1449737548828125
epoch£º468	 i:5 	 global-step:9365	 l-p:0.14372913539409637
epoch£º468	 i:6 	 global-step:9366	 l-p:0.12943106889724731
epoch£º468	 i:7 	 global-step:9367	 l-p:0.1112133115530014
epoch£º468	 i:8 	 global-step:9368	 l-p:-0.16200695931911469
epoch£º468	 i:9 	 global-step:9369	 l-p:0.11320589482784271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1464, 5.1028, 5.1257],
        [5.1464, 5.1464, 5.1464],
        [5.1464, 5.1434, 5.1462],
        [5.1464, 5.1464, 5.1464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.09365865588188171 
model_pd.l_d.mean(): -20.311790466308594 
model_pd.lagr.mean(): -20.21813201904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4108], device='cuda:0')), ('power', tensor([-21.1171], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.09365865588188171
epoch£º469	 i:1 	 global-step:9381	 l-p:0.08586642146110535
epoch£º469	 i:2 	 global-step:9382	 l-p:0.1291126012802124
epoch£º469	 i:3 	 global-step:9383	 l-p:0.1353779137134552
epoch£º469	 i:4 	 global-step:9384	 l-p:-0.0029444503597915173
epoch£º469	 i:5 	 global-step:9385	 l-p:0.7852225303649902
epoch£º469	 i:6 	 global-step:9386	 l-p:0.18808266520500183
epoch£º469	 i:7 	 global-step:9387	 l-p:0.1388315111398697
epoch£º469	 i:8 	 global-step:9388	 l-p:0.10780569165945053
epoch£º469	 i:9 	 global-step:9389	 l-p:0.11701896041631699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0431, 5.0429, 5.0431],
        [5.0431, 4.9556, 4.9415],
        [5.0431, 4.9871, 5.0104],
        [5.0431, 5.0430, 5.0431]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.12229246646165848 
model_pd.l_d.mean(): -20.429630279541016 
model_pd.lagr.mean(): -20.30733871459961 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4387], device='cuda:0')), ('power', tensor([-21.2661], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.12229246646165848
epoch£º470	 i:1 	 global-step:9401	 l-p:0.10468553006649017
epoch£º470	 i:2 	 global-step:9402	 l-p:0.13359200954437256
epoch£º470	 i:3 	 global-step:9403	 l-p:0.06734270602464676
epoch£º470	 i:4 	 global-step:9404	 l-p:0.15311363339424133
epoch£º470	 i:5 	 global-step:9405	 l-p:0.09420184791088104
epoch£º470	 i:6 	 global-step:9406	 l-p:0.16592197120189667
epoch£º470	 i:7 	 global-step:9407	 l-p:0.15194793045520782
epoch£º470	 i:8 	 global-step:9408	 l-p:0.09157812595367432
epoch£º470	 i:9 	 global-step:9409	 l-p:-0.4097455143928528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0150, 5.0150, 5.0150],
        [5.0150, 5.0150, 5.0150],
        [5.0150, 5.0150, 5.0150],
        [5.0150, 5.0150, 5.0150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.575373113155365 
model_pd.l_d.mean(): -19.907222747802734 
model_pd.lagr.mean(): -19.331850051879883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5130], device='cuda:0')), ('power', tensor([-20.8108], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.575373113155365
epoch£º471	 i:1 	 global-step:9421	 l-p:0.1429527848958969
epoch£º471	 i:2 	 global-step:9422	 l-p:-0.5201643109321594
epoch£º471	 i:3 	 global-step:9423	 l-p:0.1392592340707779
epoch£º471	 i:4 	 global-step:9424	 l-p:-0.061538465321063995
epoch£º471	 i:5 	 global-step:9425	 l-p:0.13867974281311035
epoch£º471	 i:6 	 global-step:9426	 l-p:0.127081498503685
epoch£º471	 i:7 	 global-step:9427	 l-p:0.1332331895828247
epoch£º471	 i:8 	 global-step:9428	 l-p:0.0699428841471672
epoch£º471	 i:9 	 global-step:9429	 l-p:0.13757027685642242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0781, 5.0071, 4.9015],
        [5.0781, 5.0567, 5.0730],
        [5.0781, 5.4178, 5.3843],
        [5.0781, 5.0781, 5.0781]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.12853576242923737 
model_pd.l_d.mean(): -20.45637321472168 
model_pd.lagr.mean(): -20.327836990356445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4311], device='cuda:0')), ('power', tensor([-21.2854], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.12853576242923737
epoch£º472	 i:1 	 global-step:9441	 l-p:0.3357212245464325
epoch£º472	 i:2 	 global-step:9442	 l-p:0.1270216852426529
epoch£º472	 i:3 	 global-step:9443	 l-p:0.13724759221076965
epoch£º472	 i:4 	 global-step:9444	 l-p:0.14333198964595795
epoch£º472	 i:5 	 global-step:9445	 l-p:0.12301556020975113
epoch£º472	 i:6 	 global-step:9446	 l-p:0.08435515314340591
epoch£º472	 i:7 	 global-step:9447	 l-p:0.11427430808544159
epoch£º472	 i:8 	 global-step:9448	 l-p:0.1963970959186554
epoch£º472	 i:9 	 global-step:9449	 l-p:0.1257859617471695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9283, 4.8279, 4.7768],
        [4.9283, 4.9283, 4.9283],
        [4.9283, 4.9283, 4.9283],
        [4.9283, 4.9047, 4.9225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.12507624924182892 
model_pd.l_d.mean(): -19.523271560668945 
model_pd.lagr.mean(): -19.398195266723633 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5990], device='cuda:0')), ('power', tensor([-20.5088], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.12507624924182892
epoch£º473	 i:1 	 global-step:9461	 l-p:0.12682341039180756
epoch£º473	 i:2 	 global-step:9462	 l-p:0.1867922693490982
epoch£º473	 i:3 	 global-step:9463	 l-p:0.11726968735456467
epoch£º473	 i:4 	 global-step:9464	 l-p:0.13379508256912231
epoch£º473	 i:5 	 global-step:9465	 l-p:0.14875033497810364
epoch£º473	 i:6 	 global-step:9466	 l-p:0.1744144707918167
epoch£º473	 i:7 	 global-step:9467	 l-p:0.14808763563632965
epoch£º473	 i:8 	 global-step:9468	 l-p:0.1163734570145607
epoch£º473	 i:9 	 global-step:9469	 l-p:0.15662530064582825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9084, 4.9062, 4.9083],
        [4.9084, 4.9084, 4.9084],
        [4.9084, 4.9084, 4.9084],
        [4.9084, 4.8426, 4.8667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.1271364539861679 
model_pd.l_d.mean(): -20.11220359802246 
model_pd.lagr.mean(): -19.98506736755371 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5261], device='cuda:0')), ('power', tensor([-21.0332], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.1271364539861679
epoch£º474	 i:1 	 global-step:9481	 l-p:0.15021516382694244
epoch£º474	 i:2 	 global-step:9482	 l-p:0.1521003395318985
epoch£º474	 i:3 	 global-step:9483	 l-p:0.16524845361709595
epoch£º474	 i:4 	 global-step:9484	 l-p:0.14923866093158722
epoch£º474	 i:5 	 global-step:9485	 l-p:0.12172038108110428
epoch£º474	 i:6 	 global-step:9486	 l-p:0.128493070602417
epoch£º474	 i:7 	 global-step:9487	 l-p:0.1729227602481842
epoch£º474	 i:8 	 global-step:9488	 l-p:0.11098650097846985
epoch£º474	 i:9 	 global-step:9489	 l-p:0.01610974222421646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0044, 4.9272, 4.9408],
        [5.0044, 4.9123, 4.8984],
        [5.0044, 5.0044, 5.0044],
        [5.0044, 5.0044, 5.0044]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.14190524816513062 
model_pd.l_d.mean(): -19.8895263671875 
model_pd.lagr.mean(): -19.747621536254883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5238], device='cuda:0')), ('power', tensor([-20.8040], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.14190524816513062
epoch£º475	 i:1 	 global-step:9501	 l-p:0.15108248591423035
epoch£º475	 i:2 	 global-step:9502	 l-p:0.13670334219932556
epoch£º475	 i:3 	 global-step:9503	 l-p:0.09227978438138962
epoch£º475	 i:4 	 global-step:9504	 l-p:0.10441234707832336
epoch£º475	 i:5 	 global-step:9505	 l-p:0.18174847960472107
epoch£º475	 i:6 	 global-step:9506	 l-p:0.14928370714187622
epoch£º475	 i:7 	 global-step:9507	 l-p:0.19182327389717102
epoch£º475	 i:8 	 global-step:9508	 l-p:0.287456214427948
epoch£º475	 i:9 	 global-step:9509	 l-p:0.09847011417150497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1612, 5.5400, 5.5270],
        [5.1612, 5.1612, 5.1612],
        [5.1612, 5.4718, 5.4160],
        [5.1612, 5.1103, 5.1334]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.11992108821868896 
model_pd.l_d.mean(): -20.294404983520508 
model_pd.lagr.mean(): -20.174484252929688 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4231], device='cuda:0')), ('power', tensor([-21.1122], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.11992108821868896
epoch£º476	 i:1 	 global-step:9521	 l-p:0.1283240020275116
epoch£º476	 i:2 	 global-step:9522	 l-p:0.21038749814033508
epoch£º476	 i:3 	 global-step:9523	 l-p:0.13366828858852386
epoch£º476	 i:4 	 global-step:9524	 l-p:0.14775723218917847
epoch£º476	 i:5 	 global-step:9525	 l-p:0.136975958943367
epoch£º476	 i:6 	 global-step:9526	 l-p:-0.17844001948833466
epoch£º476	 i:7 	 global-step:9527	 l-p:0.12220263481140137
epoch£º476	 i:8 	 global-step:9528	 l-p:0.06816602498292923
epoch£º476	 i:9 	 global-step:9529	 l-p:0.12423475831747055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1221, 5.1212, 5.1221],
        [5.1221, 5.2193, 5.0695],
        [5.1221, 5.0770, 5.1011],
        [5.1221, 5.0331, 4.9821]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.12894248962402344 
model_pd.l_d.mean(): -19.183765411376953 
model_pd.lagr.mean(): -19.05482292175293 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5022], device='cuda:0')), ('power', tensor([-20.0627], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.12894248962402344
epoch£º477	 i:1 	 global-step:9541	 l-p:0.13471220433712006
epoch£º477	 i:2 	 global-step:9542	 l-p:0.11034955084323883
epoch£º477	 i:3 	 global-step:9543	 l-p:0.13520433008670807
epoch£º477	 i:4 	 global-step:9544	 l-p:0.11183372139930725
epoch£º477	 i:5 	 global-step:9545	 l-p:0.03827934339642525
epoch£º477	 i:6 	 global-step:9546	 l-p:-0.03590187430381775
epoch£º477	 i:7 	 global-step:9547	 l-p:0.13198548555374146
epoch£º477	 i:8 	 global-step:9548	 l-p:0.16022935509681702
epoch£º477	 i:9 	 global-step:9549	 l-p:0.16239063441753387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9457, 4.9457, 4.9457],
        [4.9457, 5.3455, 5.3580],
        [4.9457, 4.9457, 4.9457],
        [4.9457, 4.8624, 4.7443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.09413477033376694 
model_pd.l_d.mean(): -18.607196807861328 
model_pd.lagr.mean(): -18.5130615234375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5951], device='cuda:0')), ('power', tensor([-19.5715], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.09413477033376694
epoch£º478	 i:1 	 global-step:9561	 l-p:0.16655845940113068
epoch£º478	 i:2 	 global-step:9562	 l-p:0.11588544398546219
epoch£º478	 i:3 	 global-step:9563	 l-p:0.19026561081409454
epoch£º478	 i:4 	 global-step:9564	 l-p:0.1347070336341858
epoch£º478	 i:5 	 global-step:9565	 l-p:0.13994625210762024
epoch£º478	 i:6 	 global-step:9566	 l-p:0.13478070497512817
epoch£º478	 i:7 	 global-step:9567	 l-p:0.16085375845432281
epoch£º478	 i:8 	 global-step:9568	 l-p:0.12908339500427246
epoch£º478	 i:9 	 global-step:9569	 l-p:0.15389956533908844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9039, 5.2805, 5.2791],
        [4.9039, 4.8865, 4.9006],
        [4.9039, 4.8145, 4.6968],
        [4.9039, 4.8292, 4.8509]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.1235930547118187 
model_pd.l_d.mean(): -19.735454559326172 
model_pd.lagr.mean(): -19.611862182617188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5541], device='cuda:0')), ('power', tensor([-20.6784], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.1235930547118187
epoch£º479	 i:1 	 global-step:9581	 l-p:0.12266261130571365
epoch£º479	 i:2 	 global-step:9582	 l-p:0.15470434725284576
epoch£º479	 i:3 	 global-step:9583	 l-p:0.10103144496679306
epoch£º479	 i:4 	 global-step:9584	 l-p:0.15537434816360474
epoch£º479	 i:5 	 global-step:9585	 l-p:0.18871083855628967
epoch£º479	 i:6 	 global-step:9586	 l-p:0.16856738924980164
epoch£º479	 i:7 	 global-step:9587	 l-p:0.19766011834144592
epoch£º479	 i:8 	 global-step:9588	 l-p:-0.2136213779449463
epoch£º479	 i:9 	 global-step:9589	 l-p:0.21686215698719025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8772, 4.8771, 4.8772],
        [4.8772, 4.7771, 4.7702],
        [4.8772, 4.8771, 4.8772],
        [4.8772, 4.8114, 4.8377]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.12579268217086792 
model_pd.l_d.mean(): -19.195974349975586 
model_pd.lagr.mean(): -19.070180892944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5428], device='cuda:0')), ('power', tensor([-20.1171], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.12579268217086792
epoch£º480	 i:1 	 global-step:9601	 l-p:0.1349216252565384
epoch£º480	 i:2 	 global-step:9602	 l-p:0.16729770600795746
epoch£º480	 i:3 	 global-step:9603	 l-p:0.14914894104003906
epoch£º480	 i:4 	 global-step:9604	 l-p:0.07709436863660812
epoch£º480	 i:5 	 global-step:9605	 l-p:0.1722107231616974
epoch£º480	 i:6 	 global-step:9606	 l-p:0.15158255398273468
epoch£º480	 i:7 	 global-step:9607	 l-p:0.14427991211414337
epoch£º480	 i:8 	 global-step:9608	 l-p:0.10889207571744919
epoch£º480	 i:9 	 global-step:9609	 l-p:0.09899665415287018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0373, 5.0015, 5.0247],
        [5.0373, 5.0373, 5.0373],
        [5.0373, 5.0329, 5.0370],
        [5.0373, 5.0373, 5.0373]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.13673824071884155 
model_pd.l_d.mean(): -20.660776138305664 
model_pd.lagr.mean(): -20.524038314819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4280], device='cuda:0')), ('power', tensor([-21.4904], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.13673824071884155
epoch£º481	 i:1 	 global-step:9621	 l-p:0.13710448145866394
epoch£º481	 i:2 	 global-step:9622	 l-p:0.141805037856102
epoch£º481	 i:3 	 global-step:9623	 l-p:0.1636931151151657
epoch£º481	 i:4 	 global-step:9624	 l-p:-0.021063003689050674
epoch£º481	 i:5 	 global-step:9625	 l-p:0.14203953742980957
epoch£º481	 i:6 	 global-step:9626	 l-p:0.1443507820367813
epoch£º481	 i:7 	 global-step:9627	 l-p:0.13618245720863342
epoch£º481	 i:8 	 global-step:9628	 l-p:0.07338836789131165
epoch£º481	 i:9 	 global-step:9629	 l-p:0.14051935076713562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9707, 4.9472, 4.9651],
        [4.9707, 4.9195, 4.9461],
        [4.9707, 4.8756, 4.7795],
        [4.9707, 5.0286, 4.8656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.08311042934656143 
model_pd.l_d.mean(): -20.74418067932129 
model_pd.lagr.mean(): -20.661069869995117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4301], device='cuda:0')), ('power', tensor([-21.5775], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.08311042934656143
epoch£º482	 i:1 	 global-step:9641	 l-p:0.11342697590589523
epoch£º482	 i:2 	 global-step:9642	 l-p:0.09841681271791458
epoch£º482	 i:3 	 global-step:9643	 l-p:0.08762533217668533
epoch£º482	 i:4 	 global-step:9644	 l-p:0.17670905590057373
epoch£º482	 i:5 	 global-step:9645	 l-p:0.11683820188045502
epoch£º482	 i:6 	 global-step:9646	 l-p:0.139151468873024
epoch£º482	 i:7 	 global-step:9647	 l-p:0.12805446982383728
epoch£º482	 i:8 	 global-step:9648	 l-p:0.12149378657341003
epoch£º482	 i:9 	 global-step:9649	 l-p:0.0867886170744896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0968, 5.0968, 5.0968],
        [5.0968, 5.1785, 5.0225],
        [5.0968, 5.0964, 5.0968],
        [5.0968, 5.0968, 5.0968]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): -0.3040260672569275 
model_pd.l_d.mean(): -18.57601547241211 
model_pd.lagr.mean(): -18.880041122436523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5544], device='cuda:0')), ('power', tensor([-19.4976], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:-0.3040260672569275
epoch£º483	 i:1 	 global-step:9661	 l-p:0.19470787048339844
epoch£º483	 i:2 	 global-step:9662	 l-p:0.14857524633407593
epoch£º483	 i:3 	 global-step:9663	 l-p:0.11969270557165146
epoch£º483	 i:4 	 global-step:9664	 l-p:0.13468831777572632
epoch£º483	 i:5 	 global-step:9665	 l-p:-0.02947346121072769
epoch£º483	 i:6 	 global-step:9666	 l-p:0.12069641053676605
epoch£º483	 i:7 	 global-step:9667	 l-p:0.10756412148475647
epoch£º483	 i:8 	 global-step:9668	 l-p:0.14822639524936676
epoch£º483	 i:9 	 global-step:9669	 l-p:0.10704634338617325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[5.1098, 5.3305, 5.2254],
        [5.1098, 5.0247, 4.9336],
        [5.1098, 5.0649, 4.9235],
        [5.1098, 5.3560, 5.2639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.12813042104244232 
model_pd.l_d.mean(): -19.478994369506836 
model_pd.lagr.mean(): -19.35086441040039 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5404], device='cuda:0')), ('power', tensor([-20.4030], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.12813042104244232
epoch£º484	 i:1 	 global-step:9681	 l-p:0.13837984204292297
epoch£º484	 i:2 	 global-step:9682	 l-p:0.19939973950386047
epoch£º484	 i:3 	 global-step:9683	 l-p:0.021015409380197525
epoch£º484	 i:4 	 global-step:9684	 l-p:0.06697806715965271
epoch£º484	 i:5 	 global-step:9685	 l-p:0.6292110085487366
epoch£º484	 i:6 	 global-step:9686	 l-p:0.13155004382133484
epoch£º484	 i:7 	 global-step:9687	 l-p:0.12743528187274933
epoch£º484	 i:8 	 global-step:9688	 l-p:0.1467471867799759
epoch£º484	 i:9 	 global-step:9689	 l-p:0.1237427145242691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0521, 5.0191, 5.0413],
        [5.0521, 4.9710, 4.9821],
        [5.0521, 5.0521, 5.0521],
        [5.0521, 5.0521, 5.0521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.1292915940284729 
model_pd.l_d.mean(): -20.9454345703125 
model_pd.lagr.mean(): -20.816143035888672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3664], device='cuda:0')), ('power', tensor([-21.7166], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.1292915940284729
epoch£º485	 i:1 	 global-step:9701	 l-p:0.07901705801486969
epoch£º485	 i:2 	 global-step:9702	 l-p:0.11873941123485565
epoch£º485	 i:3 	 global-step:9703	 l-p:0.12610213458538055
epoch£º485	 i:4 	 global-step:9704	 l-p:0.025291213765740395
epoch£º485	 i:5 	 global-step:9705	 l-p:0.12605029344558716
epoch£º485	 i:6 	 global-step:9706	 l-p:0.1603326052427292
epoch£º485	 i:7 	 global-step:9707	 l-p:0.21732200682163239
epoch£º485	 i:8 	 global-step:9708	 l-p:0.1552201360464096
epoch£º485	 i:9 	 global-step:9709	 l-p:0.26655659079551697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8947, 4.8318, 4.6792],
        [4.8947, 4.7862, 4.7544],
        [4.8947, 5.0446, 4.9095],
        [4.8947, 4.8326, 4.8601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.1554071009159088 
model_pd.l_d.mean(): -19.87462615966797 
model_pd.lagr.mean(): -19.719219207763672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5613], device='cuda:0')), ('power', tensor([-20.8276], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.1554071009159088
epoch£º486	 i:1 	 global-step:9721	 l-p:0.12145648896694183
epoch£º486	 i:2 	 global-step:9722	 l-p:0.1419598013162613
epoch£º486	 i:3 	 global-step:9723	 l-p:0.20218335092067719
epoch£º486	 i:4 	 global-step:9724	 l-p:0.1267877221107483
epoch£º486	 i:5 	 global-step:9725	 l-p:0.12200126051902771
epoch£º486	 i:6 	 global-step:9726	 l-p:0.16486108303070068
epoch£º486	 i:7 	 global-step:9727	 l-p:0.21133507788181305
epoch£º486	 i:8 	 global-step:9728	 l-p:0.0953906774520874
epoch£º486	 i:9 	 global-step:9729	 l-p:0.13090433180332184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9416, 4.8357, 4.8024],
        [4.9416, 4.9414, 4.9416],
        [4.9416, 4.9416, 4.9416],
        [4.9416, 4.9553, 4.7857]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.11517460644245148 
model_pd.l_d.mean(): -19.997846603393555 
model_pd.lagr.mean(): -19.882671356201172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5145], device='cuda:0')), ('power', tensor([-20.9047], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.11517460644245148
epoch£º487	 i:1 	 global-step:9741	 l-p:0.1720031201839447
epoch£º487	 i:2 	 global-step:9742	 l-p:0.1099698543548584
epoch£º487	 i:3 	 global-step:9743	 l-p:-0.07276325672864914
epoch£º487	 i:4 	 global-step:9744	 l-p:0.15938296914100647
epoch£º487	 i:5 	 global-step:9745	 l-p:0.10452643781900406
epoch£º487	 i:6 	 global-step:9746	 l-p:0.13562650978565216
epoch£º487	 i:7 	 global-step:9747	 l-p:0.14048472046852112
epoch£º487	 i:8 	 global-step:9748	 l-p:0.1281139850616455
epoch£º487	 i:9 	 global-step:9749	 l-p:0.12714803218841553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0978, 5.0966, 5.0977],
        [5.0978, 5.0361, 4.9041],
        [5.0978, 5.0754, 5.0924],
        [5.0978, 5.4585, 5.4344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.13123512268066406 
model_pd.l_d.mean(): -20.22261619567871 
model_pd.lagr.mean(): -20.091381072998047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4612], device='cuda:0')), ('power', tensor([-21.0784], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.13123512268066406
epoch£º488	 i:1 	 global-step:9761	 l-p:-0.020102210342884064
epoch£º488	 i:2 	 global-step:9762	 l-p:0.1494525820016861
epoch£º488	 i:3 	 global-step:9763	 l-p:0.10302379727363586
epoch£º488	 i:4 	 global-step:9764	 l-p:0.14574380218982697
epoch£º488	 i:5 	 global-step:9765	 l-p:0.2307671308517456
epoch£º488	 i:6 	 global-step:9766	 l-p:-0.11789590865373611
epoch£º488	 i:7 	 global-step:9767	 l-p:0.08803229033946991
epoch£º488	 i:8 	 global-step:9768	 l-p:0.1064239889383316
epoch£º488	 i:9 	 global-step:9769	 l-p:0.14937442541122437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0623, 5.0289, 5.0514],
        [5.0623, 5.0619, 5.0623],
        [5.0623, 5.0259, 5.0494],
        [5.0623, 5.0623, 5.0623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.10270539671182632 
model_pd.l_d.mean(): -20.55097770690918 
model_pd.lagr.mean(): -20.448272705078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4180], device='cuda:0')), ('power', tensor([-21.3682], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.10270539671182632
epoch£º489	 i:1 	 global-step:9781	 l-p:0.11765982210636139
epoch£º489	 i:2 	 global-step:9782	 l-p:0.270241916179657
epoch£º489	 i:3 	 global-step:9783	 l-p:0.09352624416351318
epoch£º489	 i:4 	 global-step:9784	 l-p:0.1530923843383789
epoch£º489	 i:5 	 global-step:9785	 l-p:0.1504896879196167
epoch£º489	 i:6 	 global-step:9786	 l-p:0.13219992816448212
epoch£º489	 i:7 	 global-step:9787	 l-p:0.1345696896314621
epoch£º489	 i:8 	 global-step:9788	 l-p:0.6261585354804993
epoch£º489	 i:9 	 global-step:9789	 l-p:0.12360549718141556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0897, 5.0898, 5.0897],
        [5.0897, 5.0893, 5.0897],
        [5.0897, 5.0890, 5.0897],
        [5.0897, 5.0898, 5.0897]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.1337781548500061 
model_pd.l_d.mean(): -20.648731231689453 
model_pd.lagr.mean(): -20.51495361328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4042], device='cuda:0')), ('power', tensor([-21.4534], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.1337781548500061
epoch£º490	 i:1 	 global-step:9801	 l-p:0.10744386166334152
epoch£º490	 i:2 	 global-step:9802	 l-p:0.14725930988788605
epoch£º490	 i:3 	 global-step:9803	 l-p:0.09477557241916656
epoch£º490	 i:4 	 global-step:9804	 l-p:-0.18864277005195618
epoch£º490	 i:5 	 global-step:9805	 l-p:0.07453528046607971
epoch£º490	 i:6 	 global-step:9806	 l-p:0.0933971181511879
epoch£º490	 i:7 	 global-step:9807	 l-p:0.10633259266614914
epoch£º490	 i:8 	 global-step:9808	 l-p:0.18436355888843536
epoch£º490	 i:9 	 global-step:9809	 l-p:0.125235453248024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1554, 5.5898, 5.6120],
        [5.1554, 5.0623, 5.0375],
        [5.1554, 5.1173, 5.1409],
        [5.1554, 5.3445, 5.2222]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.2493119239807129 
model_pd.l_d.mean(): -19.26199722290039 
model_pd.lagr.mean(): -19.012685775756836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4700], device='cuda:0')), ('power', tensor([-20.1090], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.2493119239807129
epoch£º491	 i:1 	 global-step:9821	 l-p:0.12319215387105942
epoch£º491	 i:2 	 global-step:9822	 l-p:0.12313903868198395
epoch£º491	 i:3 	 global-step:9823	 l-p:0.12925678491592407
epoch£º491	 i:4 	 global-step:9824	 l-p:0.22345435619354248
epoch£º491	 i:5 	 global-step:9825	 l-p:0.2183399498462677
epoch£º491	 i:6 	 global-step:9826	 l-p:0.12664252519607544
epoch£º491	 i:7 	 global-step:9827	 l-p:0.12144692987203598
epoch£º491	 i:8 	 global-step:9828	 l-p:0.11483405530452728
epoch£º491	 i:9 	 global-step:9829	 l-p:0.11646531522274017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2937, 5.2936, 5.2937],
        [5.2937, 5.2435, 5.1188],
        [5.2937, 5.2864, 5.2929],
        [5.2937, 5.2883, 5.2932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.17695102095603943 
model_pd.l_d.mean(): -20.450714111328125 
model_pd.lagr.mean(): -20.27376365661621 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3819], device='cuda:0')), ('power', tensor([-21.2287], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.17695102095603943
epoch£º492	 i:1 	 global-step:9841	 l-p:0.12130054086446762
epoch£º492	 i:2 	 global-step:9842	 l-p:0.06831812113523483
epoch£º492	 i:3 	 global-step:9843	 l-p:0.15573300421237946
epoch£º492	 i:4 	 global-step:9844	 l-p:0.1195116937160492
epoch£º492	 i:5 	 global-step:9845	 l-p:0.13731351494789124
epoch£º492	 i:6 	 global-step:9846	 l-p:0.1296747624874115
epoch£º492	 i:7 	 global-step:9847	 l-p:0.11825675517320633
epoch£º492	 i:8 	 global-step:9848	 l-p:0.13110803067684174
epoch£º492	 i:9 	 global-step:9849	 l-p:0.1316162645816803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2493, 6.0224, 6.3049],
        [5.2493, 5.2490, 5.2493],
        [5.2493, 5.1630, 5.1492],
        [5.2493, 5.8461, 5.9832]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.09494979679584503 
model_pd.l_d.mean(): -17.472299575805664 
model_pd.lagr.mean(): -17.377349853515625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5948], device='cuda:0')), ('power', tensor([-18.4150], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.09494979679584503
epoch£º493	 i:1 	 global-step:9861	 l-p:0.11182434111833572
epoch£º493	 i:2 	 global-step:9862	 l-p:0.1297520250082016
epoch£º493	 i:3 	 global-step:9863	 l-p:0.148659810423851
epoch£º493	 i:4 	 global-step:9864	 l-p:0.12868350744247437
epoch£º493	 i:5 	 global-step:9865	 l-p:-0.12374412268400192
epoch£º493	 i:6 	 global-step:9866	 l-p:0.11514299362897873
epoch£º493	 i:7 	 global-step:9867	 l-p:0.13425444066524506
epoch£º493	 i:8 	 global-step:9868	 l-p:-0.17642192542552948
epoch£º493	 i:9 	 global-step:9869	 l-p:0.18884356319904327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0836, 5.0075, 4.8819],
        [5.0836, 5.7235, 5.9064],
        [5.0836, 5.0004, 5.0122],
        [5.0836, 5.0656, 5.0801]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.1190417930483818 
model_pd.l_d.mean(): -18.080839157104492 
model_pd.lagr.mean(): -17.9617977142334 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6056], device='cuda:0')), ('power', tensor([-19.0461], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.1190417930483818
epoch£º494	 i:1 	 global-step:9881	 l-p:0.15079638361930847
epoch£º494	 i:2 	 global-step:9882	 l-p:0.06692080199718475
epoch£º494	 i:3 	 global-step:9883	 l-p:0.09765983372926712
epoch£º494	 i:4 	 global-step:9884	 l-p:0.14268125593662262
epoch£º494	 i:5 	 global-step:9885	 l-p:0.25269895792007446
epoch£º494	 i:6 	 global-step:9886	 l-p:0.07416354864835739
epoch£º494	 i:7 	 global-step:9887	 l-p:0.13887141644954681
epoch£º494	 i:8 	 global-step:9888	 l-p:0.07193729281425476
epoch£º494	 i:9 	 global-step:9889	 l-p:0.14437173306941986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1458, 5.0553, 5.0499],
        [5.1458, 5.1452, 5.1457],
        [5.1458, 5.4713, 5.4210],
        [5.1458, 5.1344, 5.1442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.1297917366027832 
model_pd.l_d.mean(): -20.266633987426758 
model_pd.lagr.mean(): -20.136842727661133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4288], device='cuda:0')), ('power', tensor([-21.0898], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.1297917366027832
epoch£º495	 i:1 	 global-step:9901	 l-p:0.12682978808879852
epoch£º495	 i:2 	 global-step:9902	 l-p:0.13608866930007935
epoch£º495	 i:3 	 global-step:9903	 l-p:0.12563861906528473
epoch£º495	 i:4 	 global-step:9904	 l-p:-0.062238823622465134
epoch£º495	 i:5 	 global-step:9905	 l-p:0.109260693192482
epoch£º495	 i:6 	 global-step:9906	 l-p:0.5132853388786316
epoch£º495	 i:7 	 global-step:9907	 l-p:0.12282900512218475
epoch£º495	 i:8 	 global-step:9908	 l-p:0.139801487326622
epoch£º495	 i:9 	 global-step:9909	 l-p:0.11516746133565903
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0532, 5.0532, 5.0532],
        [5.0532, 4.9907, 5.0168],
        [5.0532, 5.0532, 5.0532],
        [5.0532, 5.3527, 5.2900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.13719843327999115 
model_pd.l_d.mean(): -20.42935562133789 
model_pd.lagr.mean(): -20.292158126831055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4435], device='cuda:0')), ('power', tensor([-21.2707], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.13719843327999115
epoch£º496	 i:1 	 global-step:9921	 l-p:0.12646500766277313
epoch£º496	 i:2 	 global-step:9922	 l-p:0.09924398362636566
epoch£º496	 i:3 	 global-step:9923	 l-p:0.003953761886805296
epoch£º496	 i:4 	 global-step:9924	 l-p:0.11319854855537415
epoch£º496	 i:5 	 global-step:9925	 l-p:0.1284913569688797
epoch£º496	 i:6 	 global-step:9926	 l-p:0.13321180641651154
epoch£º496	 i:7 	 global-step:9927	 l-p:0.10763870179653168
epoch£º496	 i:8 	 global-step:9928	 l-p:0.21745732426643372
epoch£º496	 i:9 	 global-step:9929	 l-p:0.13479377329349518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9582, 4.8560, 4.7470],
        [4.9582, 4.9582, 4.9582],
        [4.9582, 4.9581, 4.9582],
        [4.9582, 4.9565, 4.9581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.15388083457946777 
model_pd.l_d.mean(): -20.172597885131836 
model_pd.lagr.mean(): -20.01871681213379 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5134], device='cuda:0')), ('power', tensor([-21.0815], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.15388083457946777
epoch£º497	 i:1 	 global-step:9941	 l-p:0.11522003263235092
epoch£º497	 i:2 	 global-step:9942	 l-p:0.1233462318778038
epoch£º497	 i:3 	 global-step:9943	 l-p:0.11963877081871033
epoch£º497	 i:4 	 global-step:9944	 l-p:0.13257278501987457
epoch£º497	 i:5 	 global-step:9945	 l-p:0.19449679553508759
epoch£º497	 i:6 	 global-step:9946	 l-p:0.16135896742343903
epoch£º497	 i:7 	 global-step:9947	 l-p:0.15106235444545746
epoch£º497	 i:8 	 global-step:9948	 l-p:0.11303960531949997
epoch£º497	 i:9 	 global-step:9949	 l-p:0.04226705804467201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8510, 4.8016, 4.8304],
        [4.8510, 5.1865, 5.1565],
        [4.8510, 4.9522, 4.7937],
        [4.8510, 4.8430, 4.8502]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.2876553237438202 
model_pd.l_d.mean(): -20.255746841430664 
model_pd.lagr.mean(): -19.96809196472168 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5292], device='cuda:0')), ('power', tensor([-21.1826], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.2876553237438202
epoch£º498	 i:1 	 global-step:9961	 l-p:0.19299820065498352
epoch£º498	 i:2 	 global-step:9962	 l-p:0.1267789751291275
epoch£º498	 i:3 	 global-step:9963	 l-p:0.20093606412410736
epoch£º498	 i:4 	 global-step:9964	 l-p:0.24770350754261017
epoch£º498	 i:5 	 global-step:9965	 l-p:0.14140601456165314
epoch£º498	 i:6 	 global-step:9966	 l-p:0.17799516022205353
epoch£º498	 i:7 	 global-step:9967	 l-p:0.143021821975708
epoch£º498	 i:8 	 global-step:9968	 l-p:0.09401929378509521
epoch£º498	 i:9 	 global-step:9969	 l-p:0.11110906302928925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0339, 5.0331, 5.0338],
        [5.0339, 5.3925, 5.3678],
        [5.0339, 5.0339, 5.0339],
        [5.0339, 5.0339, 5.0339]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.15824095904827118 
model_pd.l_d.mean(): -20.828693389892578 
model_pd.lagr.mean(): -20.670452117919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3927], device='cuda:0')), ('power', tensor([-21.6249], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.15824095904827118
epoch£º499	 i:1 	 global-step:9981	 l-p:0.10538409650325775
epoch£º499	 i:2 	 global-step:9982	 l-p:0.07115671783685684
epoch£º499	 i:3 	 global-step:9983	 l-p:0.13965967297554016
epoch£º499	 i:4 	 global-step:9984	 l-p:0.22010734677314758
epoch£º499	 i:5 	 global-step:9985	 l-p:0.043833985924720764
epoch£º499	 i:6 	 global-step:9986	 l-p:0.11927948892116547
epoch£º499	 i:7 	 global-step:9987	 l-p:0.01613558642566204
epoch£º499	 i:8 	 global-step:9988	 l-p:0.12612862884998322
epoch£º499	 i:9 	 global-step:9989	 l-p:0.12002994865179062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2190, 5.1443, 5.1599],
        [5.2190, 5.1406, 5.0331],
        [5.2190, 5.1751, 5.2001],
        [5.2190, 5.3371, 5.1859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): 0.6060642600059509 
model_pd.l_d.mean(): -20.59968376159668 
model_pd.lagr.mean(): -19.993619918823242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3819], device='cuda:0')), ('power', tensor([-21.3804], device='cuda:0'))])
epoch£º500	 i:0 	 global-step:10000	 l-p:0.6060642600059509
epoch£º500	 i:1 	 global-step:10001	 l-p:0.12131521105766296
epoch£º500	 i:2 	 global-step:10002	 l-p:0.2601773142814636
epoch£º500	 i:3 	 global-step:10003	 l-p:0.11048972606658936
epoch£º500	 i:4 	 global-step:10004	 l-p:0.11062648892402649
epoch£º500	 i:5 	 global-step:10005	 l-p:0.11704560369253159
epoch£º500	 i:6 	 global-step:10006	 l-p:0.1131814569234848
epoch£º500	 i:7 	 global-step:10007	 l-p:0.13177084922790527
epoch£º500	 i:8 	 global-step:10008	 l-p:0.29427871108055115
epoch£º500	 i:9 	 global-step:10009	 l-p:0.12733590602874756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1939, 5.1057, 5.1053],
        [5.1939, 5.1162, 5.0014],
        [5.1939, 5.1329, 4.9981],
        [5.1939, 5.1677, 5.1869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): -0.21759748458862305 
model_pd.l_d.mean(): -19.790424346923828 
model_pd.lagr.mean(): -20.00802230834961 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4637], device='cuda:0')), ('power', tensor([-20.6408], device='cuda:0'))])
epoch£º501	 i:0 	 global-step:10020	 l-p:-0.21759748458862305
epoch£º501	 i:1 	 global-step:10021	 l-p:0.1457924097776413
epoch£º501	 i:2 	 global-step:10022	 l-p:0.05359527841210365
epoch£º501	 i:3 	 global-step:10023	 l-p:0.10591301321983337
epoch£º501	 i:4 	 global-step:10024	 l-p:0.13201218843460083
epoch£º501	 i:5 	 global-step:10025	 l-p:0.08204860240221024
epoch£º501	 i:6 	 global-step:10026	 l-p:0.11176757514476776
epoch£º501	 i:7 	 global-step:10027	 l-p:0.12887965142726898
epoch£º501	 i:8 	 global-step:10028	 l-p:-0.069646455347538
epoch£º501	 i:9 	 global-step:10029	 l-p:0.12918417155742645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0741, 4.9684, 4.8988],
        [5.0741, 5.0740, 5.0741],
        [5.0741, 5.4309, 5.4024],
        [5.0741, 5.0738, 5.0741]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.1371016502380371 
model_pd.l_d.mean(): -19.4372501373291 
model_pd.lagr.mean(): -19.300148010253906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5082], device='cuda:0')), ('power', tensor([-20.3271], device='cuda:0'))])
epoch£º502	 i:0 	 global-step:10040	 l-p:0.1371016502380371
epoch£º502	 i:1 	 global-step:10041	 l-p:-0.09478425979614258
epoch£º502	 i:2 	 global-step:10042	 l-p:0.16713348031044006
epoch£º502	 i:3 	 global-step:10043	 l-p:0.14796024560928345
epoch£º502	 i:4 	 global-step:10044	 l-p:0.1226554811000824
epoch£º502	 i:5 	 global-step:10045	 l-p:0.1553935706615448
epoch£º502	 i:6 	 global-step:10046	 l-p:0.048707541078329086
epoch£º502	 i:7 	 global-step:10047	 l-p:0.13668356835842133
epoch£º502	 i:8 	 global-step:10048	 l-p:0.13532572984695435
epoch£º502	 i:9 	 global-step:10049	 l-p:0.12084344774484634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9556, 4.9555, 4.9556],
        [4.9556, 5.2586, 5.2007],
        [4.9556, 4.9556, 4.9556],
        [4.9556, 4.9309, 4.9499]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.16011933982372284 
model_pd.l_d.mean(): -19.661279678344727 
model_pd.lagr.mean(): -19.50115966796875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5123], device='cuda:0')), ('power', tensor([-20.5595], device='cuda:0'))])
epoch£º503	 i:0 	 global-step:10060	 l-p:0.16011933982372284
epoch£º503	 i:1 	 global-step:10061	 l-p:0.11949460953474045
epoch£º503	 i:2 	 global-step:10062	 l-p:0.12139824777841568
epoch£º503	 i:3 	 global-step:10063	 l-p:0.13710948824882507
epoch£º503	 i:4 	 global-step:10064	 l-p:0.09975085407495499
epoch£º503	 i:5 	 global-step:10065	 l-p:0.21107585728168488
epoch£º503	 i:6 	 global-step:10066	 l-p:0.211099773645401
epoch£º503	 i:7 	 global-step:10067	 l-p:0.42545774579048157
epoch£º503	 i:8 	 global-step:10068	 l-p:0.13182418048381805
epoch£º503	 i:9 	 global-step:10069	 l-p:0.09722324460744858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8994, 4.8877, 4.8979],
        [4.8994, 4.7874, 4.6727],
        [4.8994, 4.8678, 4.8905],
        [4.8994, 4.7976, 4.8049]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): 0.1019306480884552 
model_pd.l_d.mean(): -20.2900390625 
model_pd.lagr.mean(): -20.188108444213867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5214], device='cuda:0')), ('power', tensor([-21.2095], device='cuda:0'))])
epoch£º504	 i:0 	 global-step:10080	 l-p:0.1019306480884552
epoch£º504	 i:1 	 global-step:10081	 l-p:0.11568471789360046
epoch£º504	 i:2 	 global-step:10082	 l-p:0.19432859122753143
epoch£º504	 i:3 	 global-step:10083	 l-p:0.386395126581192
epoch£º504	 i:4 	 global-step:10084	 l-p:0.1648368388414383
epoch£º504	 i:5 	 global-step:10085	 l-p:0.13112393021583557
epoch£º504	 i:6 	 global-step:10086	 l-p:0.13242360949516296
epoch£º504	 i:7 	 global-step:10087	 l-p:0.15445545315742493
epoch£º504	 i:8 	 global-step:10088	 l-p:0.16327635943889618
epoch£º504	 i:9 	 global-step:10089	 l-p:0.15846262872219086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9603, 4.8502, 4.8355],
        [4.9603, 4.9086, 4.9375],
        [4.9603, 4.9557, 4.9600],
        [4.9603, 4.8904, 4.9184]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.1912604421377182 
model_pd.l_d.mean(): -20.479177474975586 
model_pd.lagr.mean(): -20.28791618347168 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4719], device='cuda:0')), ('power', tensor([-21.3509], device='cuda:0'))])
epoch£º505	 i:0 	 global-step:10100	 l-p:0.1912604421377182
epoch£º505	 i:1 	 global-step:10101	 l-p:0.1572176069021225
epoch£º505	 i:2 	 global-step:10102	 l-p:0.1069369837641716
epoch£º505	 i:3 	 global-step:10103	 l-p:0.1487828940153122
epoch£º505	 i:4 	 global-step:10104	 l-p:0.08029596507549286
epoch£º505	 i:5 	 global-step:10105	 l-p:0.13221314549446106
epoch£º505	 i:6 	 global-step:10106	 l-p:-0.3178713619709015
epoch£º505	 i:7 	 global-step:10107	 l-p:0.14141608774662018
epoch£º505	 i:8 	 global-step:10108	 l-p:0.11139198392629623
epoch£º505	 i:9 	 global-step:10109	 l-p:0.07661780714988708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1368, 5.1258, 5.1354],
        [5.1368, 5.8195, 6.0309],
        [5.1368, 5.6712, 5.7646],
        [5.1368, 5.0616, 4.9284]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.07164380699396133 
model_pd.l_d.mean(): -19.55683135986328 
model_pd.lagr.mean(): -19.485187530517578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5061], device='cuda:0')), ('power', tensor([-20.4467], device='cuda:0'))])
epoch£º506	 i:0 	 global-step:10120	 l-p:0.07164380699396133
epoch£º506	 i:1 	 global-step:10121	 l-p:0.05856835097074509
epoch£º506	 i:2 	 global-step:10122	 l-p:0.17400166392326355
epoch£º506	 i:3 	 global-step:10123	 l-p:0.10885560512542725
epoch£º506	 i:4 	 global-step:10124	 l-p:0.2142229825258255
epoch£º506	 i:5 	 global-step:10125	 l-p:0.11180009692907333
epoch£º506	 i:6 	 global-step:10126	 l-p:0.14076687395572662
epoch£º506	 i:7 	 global-step:10127	 l-p:0.10339494794607162
epoch£º506	 i:8 	 global-step:10128	 l-p:0.11577006429433823
epoch£º506	 i:9 	 global-step:10129	 l-p:0.1293489784002304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2748, 5.2318, 5.2568],
        [5.2748, 5.1796, 5.1330],
        [5.2748, 5.1865, 5.1007],
        [5.2748, 5.9386, 6.1244]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.13559071719646454 
model_pd.l_d.mean(): -20.17947769165039 
model_pd.lagr.mean(): -20.043886184692383 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4176], device='cuda:0')), ('power', tensor([-20.9893], device='cuda:0'))])
epoch£º507	 i:0 	 global-step:10140	 l-p:0.13559071719646454
epoch£º507	 i:1 	 global-step:10141	 l-p:0.12261143326759338
epoch£º507	 i:2 	 global-step:10142	 l-p:0.11915275454521179
epoch£º507	 i:3 	 global-step:10143	 l-p:0.16776622831821442
epoch£º507	 i:4 	 global-step:10144	 l-p:0.1209651380777359
epoch£º507	 i:5 	 global-step:10145	 l-p:0.05948346108198166
epoch£º507	 i:6 	 global-step:10146	 l-p:0.05107445269823074
epoch£º507	 i:7 	 global-step:10147	 l-p:0.17467249929904938
epoch£º507	 i:8 	 global-step:10148	 l-p:-0.009156245738267899
epoch£º507	 i:9 	 global-step:10149	 l-p:0.12083221226930618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1883, 5.1648, 5.1827],
        [5.1883, 5.1860, 5.1881],
        [5.1883, 5.1877, 5.1882],
        [5.1883, 5.1985, 5.0289]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.09729092568159103 
model_pd.l_d.mean(): -19.311124801635742 
model_pd.lagr.mean(): -19.213834762573242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4918], device='cuda:0')), ('power', tensor([-20.1816], device='cuda:0'))])
epoch£º508	 i:0 	 global-step:10160	 l-p:0.09729092568159103
epoch£º508	 i:1 	 global-step:10161	 l-p:0.1322816014289856
epoch£º508	 i:2 	 global-step:10162	 l-p:0.1980857104063034
epoch£º508	 i:3 	 global-step:10163	 l-p:-0.06047584488987923
epoch£º508	 i:4 	 global-step:10164	 l-p:-0.009323358535766602
epoch£º508	 i:5 	 global-step:10165	 l-p:0.14378994703292847
epoch£º508	 i:6 	 global-step:10166	 l-p:-0.0771169438958168
epoch£º508	 i:7 	 global-step:10167	 l-p:0.109686940908432
epoch£º508	 i:8 	 global-step:10168	 l-p:0.1460132598876953
epoch£º508	 i:9 	 global-step:10169	 l-p:0.1377432644367218
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2130, 5.4550, 5.3523],
        [5.2130, 5.2125, 5.2130],
        [5.2130, 5.9426, 6.1871],
        [5.2130, 5.1738, 5.1984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): 0.36079806089401245 
model_pd.l_d.mean(): -19.725828170776367 
model_pd.lagr.mean(): -19.36503028869629 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4501], device='cuda:0')), ('power', tensor([-20.5608], device='cuda:0'))])
epoch£º509	 i:0 	 global-step:10180	 l-p:0.36079806089401245
epoch£º509	 i:1 	 global-step:10181	 l-p:0.43189969658851624
epoch£º509	 i:2 	 global-step:10182	 l-p:0.11861368268728256
epoch£º509	 i:3 	 global-step:10183	 l-p:0.1386953443288803
epoch£º509	 i:4 	 global-step:10184	 l-p:0.09538813680410385
epoch£º509	 i:5 	 global-step:10185	 l-p:0.1236572414636612
epoch£º509	 i:6 	 global-step:10186	 l-p:0.11486836522817612
epoch£º509	 i:7 	 global-step:10187	 l-p:-0.000579566927626729
epoch£º509	 i:8 	 global-step:10188	 l-p:0.10836105048656464
epoch£º509	 i:9 	 global-step:10189	 l-p:0.14143802225589752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1457, 5.1443, 5.1457],
        [5.1457, 5.2735, 5.1215],
        [5.1457, 5.3103, 5.1722],
        [5.1457, 5.1549, 4.9825]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.11667590588331223 
model_pd.l_d.mean(): -18.554452896118164 
model_pd.lagr.mean(): -18.437776565551758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4705], device='cuda:0')), ('power', tensor([-19.3887], device='cuda:0'))])
epoch£º510	 i:0 	 global-step:10200	 l-p:0.11667590588331223
epoch£º510	 i:1 	 global-step:10201	 l-p:0.12193094193935394
epoch£º510	 i:2 	 global-step:10202	 l-p:0.13071972131729126
epoch£º510	 i:3 	 global-step:10203	 l-p:0.010288504883646965
epoch£º510	 i:4 	 global-step:10204	 l-p:0.12689325213432312
epoch£º510	 i:5 	 global-step:10205	 l-p:0.13529625535011292
epoch£º510	 i:6 	 global-step:10206	 l-p:0.14576557278633118
epoch£º510	 i:7 	 global-step:10207	 l-p:0.12962675094604492
epoch£º510	 i:8 	 global-step:10208	 l-p:0.1572316735982895
epoch£º510	 i:9 	 global-step:10209	 l-p:0.13237690925598145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9631, 4.8885, 4.9162],
        [4.9631, 5.0093, 4.8321],
        [4.9631, 4.9282, 4.9524],
        [4.9631, 4.9631, 4.9631]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): 0.1660183072090149 
model_pd.l_d.mean(): -19.93268585205078 
model_pd.lagr.mean(): -19.76666831970215 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5410], device='cuda:0')), ('power', tensor([-20.8657], device='cuda:0'))])
epoch£º511	 i:0 	 global-step:10220	 l-p:0.1660183072090149
epoch£º511	 i:1 	 global-step:10221	 l-p:0.1340494006872177
epoch£º511	 i:2 	 global-step:10222	 l-p:0.12448754906654358
epoch£º511	 i:3 	 global-step:10223	 l-p:0.16087208688259125
epoch£º511	 i:4 	 global-step:10224	 l-p:0.13287830352783203
epoch£º511	 i:5 	 global-step:10225	 l-p:0.1702774614095688
epoch£º511	 i:6 	 global-step:10226	 l-p:0.1266380101442337
epoch£º511	 i:7 	 global-step:10227	 l-p:0.15663599967956543
epoch£º511	 i:8 	 global-step:10228	 l-p:0.06165045127272606
epoch£º511	 i:9 	 global-step:10229	 l-p:0.14696934819221497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9560, 4.9440, 4.9544],
        [4.9560, 4.9130, 4.9403],
        [4.9560, 4.9826, 4.8017],
        [4.9560, 4.8530, 4.8589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.16253821551799774 
model_pd.l_d.mean(): -20.490144729614258 
model_pd.lagr.mean(): -20.327606201171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4597], device='cuda:0')), ('power', tensor([-21.3494], device='cuda:0'))])
epoch£º512	 i:0 	 global-step:10240	 l-p:0.16253821551799774
epoch£º512	 i:1 	 global-step:10241	 l-p:0.10152234137058258
epoch£º512	 i:2 	 global-step:10242	 l-p:0.2056700587272644
epoch£º512	 i:3 	 global-step:10243	 l-p:0.14078551530838013
epoch£º512	 i:4 	 global-step:10244	 l-p:0.11456609517335892
epoch£º512	 i:5 	 global-step:10245	 l-p:0.11456096172332764
epoch£º512	 i:6 	 global-step:10246	 l-p:0.1306464672088623
epoch£º512	 i:7 	 global-step:10247	 l-p:0.14593075215816498
epoch£º512	 i:8 	 global-step:10248	 l-p:0.16326867043972015
epoch£º512	 i:9 	 global-step:10249	 l-p:0.24043181538581848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9298, 4.8083, 4.7739],
        [4.9298, 5.2355, 5.1785],
        [4.9298, 4.8144, 4.8008],
        [4.9298, 4.9275, 4.9297]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.15169711410999298 
model_pd.l_d.mean(): -20.32392120361328 
model_pd.lagr.mean(): -20.172224044799805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5058], device='cuda:0')), ('power', tensor([-21.2278], device='cuda:0'))])
epoch£º513	 i:0 	 global-step:10260	 l-p:0.15169711410999298
epoch£º513	 i:1 	 global-step:10261	 l-p:0.16393598914146423
epoch£º513	 i:2 	 global-step:10262	 l-p:0.13731203973293304
epoch£º513	 i:3 	 global-step:10263	 l-p:0.1334470957517624
epoch£º513	 i:4 	 global-step:10264	 l-p:0.173614040017128
epoch£º513	 i:5 	 global-step:10265	 l-p:0.12399774044752121
epoch£º513	 i:6 	 global-step:10266	 l-p:0.1235080137848854
epoch£º513	 i:7 	 global-step:10267	 l-p:0.12658265233039856
epoch£º513	 i:8 	 global-step:10268	 l-p:0.03696554899215698
epoch£º513	 i:9 	 global-step:10269	 l-p:0.08542787283658981
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0707, 5.6162, 5.7207],
        [5.0707, 5.3950, 5.3429],
        [5.0707, 4.9686, 4.9664],
        [5.0707, 5.0704, 5.0707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.14187027513980865 
model_pd.l_d.mean(): -20.28295135498047 
model_pd.lagr.mean(): -20.141080856323242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4515], device='cuda:0')), ('power', tensor([-21.1298], device='cuda:0'))])
epoch£º514	 i:0 	 global-step:10280	 l-p:0.14187027513980865
epoch£º514	 i:1 	 global-step:10281	 l-p:0.14445705711841583
epoch£º514	 i:2 	 global-step:10282	 l-p:0.15826521813869476
epoch£º514	 i:3 	 global-step:10283	 l-p:-0.02927527390420437
epoch£º514	 i:4 	 global-step:10284	 l-p:0.0630515068769455
epoch£º514	 i:5 	 global-step:10285	 l-p:0.12964294850826263
epoch£º514	 i:6 	 global-step:10286	 l-p:0.11450927704572678
epoch£º514	 i:7 	 global-step:10287	 l-p:0.13917885720729828
epoch£º514	 i:8 	 global-step:10288	 l-p:0.12034576386213303
epoch£º514	 i:9 	 global-step:10289	 l-p:0.13323000073432922
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0471, 5.3319, 5.2563],
        [5.0471, 5.2823, 5.1782],
        [5.0471, 4.9941, 5.0231],
        [5.0471, 5.0123, 4.8378]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): 0.005930749233812094 
model_pd.l_d.mean(): -18.911067962646484 
model_pd.lagr.mean(): -18.90513801574707 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5578], device='cuda:0')), ('power', tensor([-19.8424], device='cuda:0'))])
epoch£º515	 i:0 	 global-step:10300	 l-p:0.005930749233812094
epoch£º515	 i:1 	 global-step:10301	 l-p:0.10314472019672394
epoch£º515	 i:2 	 global-step:10302	 l-p:0.11559135466814041
epoch£º515	 i:3 	 global-step:10303	 l-p:0.15641865134239197
epoch£º515	 i:4 	 global-step:10304	 l-p:0.1326589584350586
epoch£º515	 i:5 	 global-step:10305	 l-p:0.13338221609592438
epoch£º515	 i:6 	 global-step:10306	 l-p:0.0938018411397934
epoch£º515	 i:7 	 global-step:10307	 l-p:0.23661178350448608
epoch£º515	 i:8 	 global-step:10308	 l-p:0.2365877479314804
epoch£º515	 i:9 	 global-step:10309	 l-p:0.1472553163766861
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9183, 4.9085, 4.9172],
        [4.9183, 4.8026, 4.7937],
        [4.9183, 5.1635, 5.0693],
        [4.9183, 4.9003, 4.9151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 0.2925586700439453 
model_pd.l_d.mean(): -19.849315643310547 
model_pd.lagr.mean(): -19.5567569732666 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5602], device='cuda:0')), ('power', tensor([-20.8007], device='cuda:0'))])
epoch£º516	 i:0 	 global-step:10320	 l-p:0.2925586700439453
epoch£º516	 i:1 	 global-step:10321	 l-p:0.1967233568429947
epoch£º516	 i:2 	 global-step:10322	 l-p:0.1396350860595703
epoch£º516	 i:3 	 global-step:10323	 l-p:0.12964364886283875
epoch£º516	 i:4 	 global-step:10324	 l-p:0.09696093946695328
epoch£º516	 i:5 	 global-step:10325	 l-p:0.17722377181053162
epoch£º516	 i:6 	 global-step:10326	 l-p:0.10852926969528198
epoch£º516	 i:7 	 global-step:10327	 l-p:0.15422143042087555
epoch£º516	 i:8 	 global-step:10328	 l-p:0.1323065310716629
epoch£º516	 i:9 	 global-step:10329	 l-p:0.14137183129787445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9557, 4.9481, 4.9550],
        [4.9557, 4.8424, 4.7210],
        [4.9557, 4.8354, 4.7341],
        [4.9557, 4.9557, 4.9557]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.10613253712654114 
model_pd.l_d.mean(): -19.896846771240234 
model_pd.lagr.mean(): -19.790714263916016 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5026], device='cuda:0')), ('power', tensor([-20.7894], device='cuda:0'))])
epoch£º517	 i:0 	 global-step:10340	 l-p:0.10613253712654114
epoch£º517	 i:1 	 global-step:10341	 l-p:0.1311895102262497
epoch£º517	 i:2 	 global-step:10342	 l-p:0.1811794936656952
epoch£º517	 i:3 	 global-step:10343	 l-p:0.1519177258014679
epoch£º517	 i:4 	 global-step:10344	 l-p:0.12556692957878113
epoch£º517	 i:5 	 global-step:10345	 l-p:0.12974081933498383
epoch£º517	 i:6 	 global-step:10346	 l-p:0.10204780101776123
epoch£º517	 i:7 	 global-step:10347	 l-p:-0.32298046350479126
epoch£º517	 i:8 	 global-step:10348	 l-p:0.08158660680055618
epoch£º517	 i:9 	 global-step:10349	 l-p:0.13845187425613403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6054,  0.5121,  1.0000,  0.4332,
          1.0000,  0.8459, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4016,  0.2963,  1.0000,  0.2186,
          1.0000,  0.7378, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1448,  0.0760,  1.0000,  0.0399,
          1.0000,  0.5251, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228]], device='cuda:0')
 pt:tensor([[5.1622, 5.3217, 5.1788],
        [5.1622, 5.1095, 4.9497],
        [5.1622, 5.0840, 5.1060],
        [5.1622, 5.0530, 5.0090]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.11508025974035263 
model_pd.l_d.mean(): -20.137544631958008 
model_pd.lagr.mean(): -20.022464752197266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4594], device='cuda:0')), ('power', tensor([-20.9899], device='cuda:0'))])
epoch£º518	 i:0 	 global-step:10360	 l-p:0.11508025974035263
epoch£º518	 i:1 	 global-step:10361	 l-p:0.0350722037255764
epoch£º518	 i:2 	 global-step:10362	 l-p:0.01763404719531536
epoch£º518	 i:3 	 global-step:10363	 l-p:0.12148088961839676
epoch£º518	 i:4 	 global-step:10364	 l-p:0.1892869472503662
epoch£º518	 i:5 	 global-step:10365	 l-p:0.13509872555732727
epoch£º518	 i:6 	 global-step:10366	 l-p:0.09047704190015793
epoch£º518	 i:7 	 global-step:10367	 l-p:0.12464601546525955
epoch£º518	 i:8 	 global-step:10368	 l-p:0.12004804611206055
epoch£º518	 i:9 	 global-step:10369	 l-p:0.2247096747159958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2448, 5.2420, 5.2446],
        [5.2448, 5.9208, 6.1160],
        [5.2448, 5.2417, 5.2446],
        [5.2448, 5.1803, 5.2063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.12612883746623993 
model_pd.l_d.mean(): -20.250385284423828 
model_pd.lagr.mean(): -20.124256134033203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4038], device='cuda:0')), ('power', tensor([-21.0473], device='cuda:0'))])
epoch£º519	 i:0 	 global-step:10380	 l-p:0.12612883746623993
epoch£º519	 i:1 	 global-step:10381	 l-p:0.17020323872566223
epoch£º519	 i:2 	 global-step:10382	 l-p:0.11038292944431305
epoch£º519	 i:3 	 global-step:10383	 l-p:0.1492401510477066
epoch£º519	 i:4 	 global-step:10384	 l-p:-0.02568761818110943
epoch£º519	 i:5 	 global-step:10385	 l-p:-0.0744498074054718
epoch£º519	 i:6 	 global-step:10386	 l-p:0.12003636360168457
epoch£º519	 i:7 	 global-step:10387	 l-p:0.0895986258983612
epoch£º519	 i:8 	 global-step:10388	 l-p:0.11380916088819504
epoch£º519	 i:9 	 global-step:10389	 l-p:0.11792951077222824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1545, 5.1545, 5.1545],
        [5.1545, 5.6768, 5.7570],
        [5.1545, 5.1545, 5.1545],
        [5.1545, 5.1472, 5.1538]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.13504621386528015 
model_pd.l_d.mean(): -19.415740966796875 
model_pd.lagr.mean(): -19.28069496154785 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4474], device='cuda:0')), ('power', tensor([-20.2422], device='cuda:0'))])
epoch£º520	 i:0 	 global-step:10400	 l-p:0.13504621386528015
epoch£º520	 i:1 	 global-step:10401	 l-p:0.07766858488321304
epoch£º520	 i:2 	 global-step:10402	 l-p:0.1219383180141449
epoch£º520	 i:3 	 global-step:10403	 l-p:-0.005904183257371187
epoch£º520	 i:4 	 global-step:10404	 l-p:0.1744682639837265
epoch£º520	 i:5 	 global-step:10405	 l-p:0.47205108404159546
epoch£º520	 i:6 	 global-step:10406	 l-p:0.13381440937519073
epoch£º520	 i:7 	 global-step:10407	 l-p:0.13045421242713928
epoch£º520	 i:8 	 global-step:10408	 l-p:0.14161014556884766
epoch£º520	 i:9 	 global-step:10409	 l-p:0.17036934196949005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0640, 4.9462, 4.8751],
        [5.0640, 5.0361, 5.0569],
        [5.0640, 5.6393, 5.7658],
        [5.0640, 4.9484, 4.9165]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.13590648770332336 
model_pd.l_d.mean(): -20.572031021118164 
model_pd.lagr.mean(): -20.436124801635742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4218], device='cuda:0')), ('power', tensor([-21.3936], device='cuda:0'))])
epoch£º521	 i:0 	 global-step:10420	 l-p:0.13590648770332336
epoch£º521	 i:1 	 global-step:10421	 l-p:0.12842586636543274
epoch£º521	 i:2 	 global-step:10422	 l-p:0.0174888726323843
epoch£º521	 i:3 	 global-step:10423	 l-p:0.1561359018087387
epoch£º521	 i:4 	 global-step:10424	 l-p:0.11681149899959564
epoch£º521	 i:5 	 global-step:10425	 l-p:0.16017167270183563
epoch£º521	 i:6 	 global-step:10426	 l-p:0.15181566774845123
epoch£º521	 i:7 	 global-step:10427	 l-p:0.14751183986663818
epoch£º521	 i:8 	 global-step:10428	 l-p:0.11888490617275238
epoch£º521	 i:9 	 global-step:10429	 l-p:0.2901073694229126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9150, 4.9112, 4.9148],
        [4.9150, 4.9150, 4.9150],
        [4.9150, 4.9062, 4.9141],
        [4.9150, 4.9051, 4.9139]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): 0.1178334578871727 
model_pd.l_d.mean(): -20.59200096130371 
model_pd.lagr.mean(): -20.474166870117188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4807], device='cuda:0')), ('power', tensor([-21.4750], device='cuda:0'))])
epoch£º522	 i:0 	 global-step:10440	 l-p:0.1178334578871727
epoch£º522	 i:1 	 global-step:10441	 l-p:0.2113228142261505
epoch£º522	 i:2 	 global-step:10442	 l-p:0.265674889087677
epoch£º522	 i:3 	 global-step:10443	 l-p:0.1500454843044281
epoch£º522	 i:4 	 global-step:10444	 l-p:0.18127399682998657
epoch£º522	 i:5 	 global-step:10445	 l-p:0.16732284426689148
epoch£º522	 i:6 	 global-step:10446	 l-p:0.07425892353057861
epoch£º522	 i:7 	 global-step:10447	 l-p:0.10914615541696548
epoch£º522	 i:8 	 global-step:10448	 l-p:0.13219167292118073
epoch£º522	 i:9 	 global-step:10449	 l-p:0.10972166806459427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0541, 5.0359, 5.0508],
        [5.0541, 5.0541, 5.0541],
        [5.0541, 5.6234, 5.7454],
        [5.0541, 5.0539, 5.0541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.10304149240255356 
model_pd.l_d.mean(): -18.145103454589844 
model_pd.lagr.mean(): -18.042062759399414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6124], device='cuda:0')), ('power', tensor([-19.1187], device='cuda:0'))])
epoch£º523	 i:0 	 global-step:10460	 l-p:0.10304149240255356
epoch£º523	 i:1 	 global-step:10461	 l-p:0.09076749533414841
epoch£º523	 i:2 	 global-step:10462	 l-p:0.3159256875514984
epoch£º523	 i:3 	 global-step:10463	 l-p:0.14744022488594055
epoch£º523	 i:4 	 global-step:10464	 l-p:0.13399122655391693
epoch£º523	 i:5 	 global-step:10465	 l-p:-0.058500152081251144
epoch£º523	 i:6 	 global-step:10466	 l-p:0.15881411731243134
epoch£º523	 i:7 	 global-step:10467	 l-p:0.1467439979314804
epoch£º523	 i:8 	 global-step:10468	 l-p:0.14115199446678162
epoch£º523	 i:9 	 global-step:10469	 l-p:0.1374877691268921
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0211, 5.0099, 5.0197],
        [5.0211, 5.2863, 5.1978],
        [5.0211, 5.0208, 5.0211],
        [5.0211, 5.0104, 5.0198]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.18929274380207062 
model_pd.l_d.mean(): -20.159847259521484 
model_pd.lagr.mean(): -19.97055435180664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4804], device='cuda:0')), ('power', tensor([-21.0344], device='cuda:0'))])
epoch£º524	 i:0 	 global-step:10480	 l-p:0.18929274380207062
epoch£º524	 i:1 	 global-step:10481	 l-p:0.1255819946527481
epoch£º524	 i:2 	 global-step:10482	 l-p:0.008506698533892632
epoch£º524	 i:3 	 global-step:10483	 l-p:0.1038486436009407
epoch£º524	 i:4 	 global-step:10484	 l-p:0.13372865319252014
epoch£º524	 i:5 	 global-step:10485	 l-p:0.13178594410419464
epoch£º524	 i:6 	 global-step:10486	 l-p:0.131626695394516
epoch£º524	 i:7 	 global-step:10487	 l-p:0.12421146780252457
epoch£º524	 i:8 	 global-step:10488	 l-p:0.14894156157970428
epoch£º524	 i:9 	 global-step:10489	 l-p:0.11310894042253494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9960, 4.9937, 4.9959],
        [4.9960, 4.9960, 4.9960],
        [4.9960, 4.9928, 4.9959],
        [4.9960, 4.8703, 4.8217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.16373157501220703 
model_pd.l_d.mean(): -19.569862365722656 
model_pd.lagr.mean(): -19.406131744384766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5165], device='cuda:0')), ('power', tensor([-20.4707], device='cuda:0'))])
epoch£º525	 i:0 	 global-step:10500	 l-p:0.16373157501220703
epoch£º525	 i:1 	 global-step:10501	 l-p:0.07717728614807129
epoch£º525	 i:2 	 global-step:10502	 l-p:0.12332858890295029
epoch£º525	 i:3 	 global-step:10503	 l-p:0.12498906999826431
epoch£º525	 i:4 	 global-step:10504	 l-p:0.12343009561300278
epoch£º525	 i:5 	 global-step:10505	 l-p:0.1245906874537468
epoch£º525	 i:6 	 global-step:10506	 l-p:0.1871522217988968
epoch£º525	 i:7 	 global-step:10507	 l-p:0.17295832931995392
epoch£º525	 i:8 	 global-step:10508	 l-p:0.13538476824760437
epoch£º525	 i:9 	 global-step:10509	 l-p:0.10617612302303314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0011, 4.9899, 4.9997],
        [5.0011, 4.8856, 4.8770],
        [5.0011, 5.0608, 4.8805],
        [5.0011, 5.1325, 4.9757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.1524917036294937 
model_pd.l_d.mean(): -20.49757957458496 
model_pd.lagr.mean(): -20.3450870513916 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4709], device='cuda:0')), ('power', tensor([-21.3686], device='cuda:0'))])
epoch£º526	 i:0 	 global-step:10520	 l-p:0.1524917036294937
epoch£º526	 i:1 	 global-step:10521	 l-p:0.14445897936820984
epoch£º526	 i:2 	 global-step:10522	 l-p:0.06114435940980911
epoch£º526	 i:3 	 global-step:10523	 l-p:0.12603864073753357
epoch£º526	 i:4 	 global-step:10524	 l-p:0.1238868236541748
epoch£º526	 i:5 	 global-step:10525	 l-p:0.13708564639091492
epoch£º526	 i:6 	 global-step:10526	 l-p:0.18633723258972168
epoch£º526	 i:7 	 global-step:10527	 l-p:0.13769376277923584
epoch£º526	 i:8 	 global-step:10528	 l-p:0.4022962152957916
epoch£º526	 i:9 	 global-step:10529	 l-p:0.13372798264026642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1770, 5.1770, 5.1770],
        [5.1770, 5.0753, 4.9608],
        [5.1770, 5.1770, 5.1770],
        [5.1770, 5.1764, 5.1770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.12700867652893066 
model_pd.l_d.mean(): -20.33858299255371 
model_pd.lagr.mean(): -20.21157455444336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4134], device='cuda:0')), ('power', tensor([-21.1471], device='cuda:0'))])
epoch£º527	 i:0 	 global-step:10540	 l-p:0.12700867652893066
epoch£º527	 i:1 	 global-step:10541	 l-p:0.1334981471300125
epoch£º527	 i:2 	 global-step:10542	 l-p:0.15150845050811768
epoch£º527	 i:3 	 global-step:10543	 l-p:0.13968786597251892
epoch£º527	 i:4 	 global-step:10544	 l-p:-0.6170409917831421
epoch£º527	 i:5 	 global-step:10545	 l-p:0.1322510838508606
epoch£º527	 i:6 	 global-step:10546	 l-p:0.13471448421478271
epoch£º527	 i:7 	 global-step:10547	 l-p:-0.28173908591270447
epoch£º527	 i:8 	 global-step:10548	 l-p:0.12722252309322357
epoch£º527	 i:9 	 global-step:10549	 l-p:0.14006881415843964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0407, 4.9954, 5.0239],
        [5.0407, 5.0340, 5.0401],
        [5.0407, 5.3403, 5.2707],
        [5.0407, 5.3405, 5.2709]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): 0.051743779331445694 
model_pd.l_d.mean(): -19.629030227661133 
model_pd.lagr.mean(): -19.577285766601562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5262], device='cuda:0')), ('power', tensor([-20.5410], device='cuda:0'))])
epoch£º528	 i:0 	 global-step:10560	 l-p:0.051743779331445694
epoch£º528	 i:1 	 global-step:10561	 l-p:0.12281282246112823
epoch£º528	 i:2 	 global-step:10562	 l-p:0.1319073587656021
epoch£º528	 i:3 	 global-step:10563	 l-p:0.16107545793056488
epoch£º528	 i:4 	 global-step:10564	 l-p:0.12484528869390488
epoch£º528	 i:5 	 global-step:10565	 l-p:0.13174904882907867
epoch£º528	 i:6 	 global-step:10566	 l-p:0.16433613002300262
epoch£º528	 i:7 	 global-step:10567	 l-p:0.1573721319437027
epoch£º528	 i:8 	 global-step:10568	 l-p:0.1353689581155777
epoch£º528	 i:9 	 global-step:10569	 l-p:0.16127391159534454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3405,  0.2378,  1.0000,  0.1660,
          1.0000,  0.6983, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6844,  0.6031,  1.0000,  0.5315,
          1.0000,  0.8812, 31.6228]], device='cuda:0')
 pt:tensor([[4.9506, 4.8286, 4.7006],
        [4.9506, 4.8952, 4.7082],
        [4.9506, 4.8716, 4.9021],
        [4.9506, 5.1486, 5.0232]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): 0.1089671328663826 
model_pd.l_d.mean(): -20.531204223632812 
model_pd.lagr.mean(): -20.422237396240234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4809], device='cuda:0')), ('power', tensor([-21.4132], device='cuda:0'))])
epoch£º529	 i:0 	 global-step:10580	 l-p:0.1089671328663826
epoch£º529	 i:1 	 global-step:10581	 l-p:0.10093247890472412
epoch£º529	 i:2 	 global-step:10582	 l-p:0.1444026529788971
epoch£º529	 i:3 	 global-step:10583	 l-p:0.14197178184986115
epoch£º529	 i:4 	 global-step:10584	 l-p:0.2019820362329483
epoch£º529	 i:5 	 global-step:10585	 l-p:0.07932807505130768
epoch£º529	 i:6 	 global-step:10586	 l-p:0.1583728790283203
epoch£º529	 i:7 	 global-step:10587	 l-p:0.15612562000751495
epoch£º529	 i:8 	 global-step:10588	 l-p:0.15589012205600739
epoch£º529	 i:9 	 global-step:10589	 l-p:0.14220714569091797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0123, 5.0123, 5.0123],
        [5.0123, 4.8832, 4.8123],
        [5.0123, 5.0123, 5.0123],
        [5.0123, 5.1171, 4.9485]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.13521312177181244 
model_pd.l_d.mean(): -20.17461395263672 
model_pd.lagr.mean(): -20.039400100708008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4934], device='cuda:0')), ('power', tensor([-21.0629], device='cuda:0'))])
epoch£º530	 i:0 	 global-step:10600	 l-p:0.13521312177181244
epoch£º530	 i:1 	 global-step:10601	 l-p:0.1277771294116974
epoch£º530	 i:2 	 global-step:10602	 l-p:0.12387694418430328
epoch£º530	 i:3 	 global-step:10603	 l-p:0.09461299329996109
epoch£º530	 i:4 	 global-step:10604	 l-p:-0.14768069982528687
epoch£º530	 i:5 	 global-step:10605	 l-p:0.13354691863059998
epoch£º530	 i:6 	 global-step:10606	 l-p:0.1652143895626068
epoch£º530	 i:7 	 global-step:10607	 l-p:0.10835739225149155
epoch£º530	 i:8 	 global-step:10608	 l-p:0.13428530097007751
epoch£º530	 i:9 	 global-step:10609	 l-p:0.0663997232913971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1046, 5.0996, 5.1043],
        [5.1046, 5.1046, 5.1046],
        [5.1046, 5.1046, 5.1046],
        [5.1046, 5.1046, 5.1046]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.14272372424602509 
model_pd.l_d.mean(): -19.267929077148438 
model_pd.lagr.mean(): -19.125205993652344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4748], device='cuda:0')), ('power', tensor([-20.1200], device='cuda:0'))])
epoch£º531	 i:0 	 global-step:10620	 l-p:0.14272372424602509
epoch£º531	 i:1 	 global-step:10621	 l-p:0.126563161611557
epoch£º531	 i:2 	 global-step:10622	 l-p:0.1461459845304489
epoch£º531	 i:3 	 global-step:10623	 l-p:0.1352456510066986
epoch£º531	 i:4 	 global-step:10624	 l-p:0.07833029329776764
epoch£º531	 i:5 	 global-step:10625	 l-p:0.20260123908519745
epoch£º531	 i:6 	 global-step:10626	 l-p:-2.654003143310547
epoch£º531	 i:7 	 global-step:10627	 l-p:0.1342116743326187
epoch£º531	 i:8 	 global-step:10628	 l-p:0.13566482067108154
epoch£º531	 i:9 	 global-step:10629	 l-p:0.13312502205371857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1729, 5.1845, 4.9999],
        [5.1729, 5.1269, 5.1551],
        [5.1729, 5.2533, 5.0785],
        [5.1729, 5.1701, 5.1727]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): 0.05198412761092186 
model_pd.l_d.mean(): -19.266094207763672 
model_pd.lagr.mean(): -19.214109420776367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4746], device='cuda:0')), ('power', tensor([-20.1179], device='cuda:0'))])
epoch£º532	 i:0 	 global-step:10640	 l-p:0.05198412761092186
epoch£º532	 i:1 	 global-step:10641	 l-p:0.13117733597755432
epoch£º532	 i:2 	 global-step:10642	 l-p:0.21432453393936157
epoch£º532	 i:3 	 global-step:10643	 l-p:0.12510566413402557
epoch£º532	 i:4 	 global-step:10644	 l-p:0.0010975885670632124
epoch£º532	 i:5 	 global-step:10645	 l-p:0.15429769456386566
epoch£º532	 i:6 	 global-step:10646	 l-p:0.11755378544330597
epoch£º532	 i:7 	 global-step:10647	 l-p:0.11488529294729233
epoch£º532	 i:8 	 global-step:10648	 l-p:0.10785988718271255
epoch£º532	 i:9 	 global-step:10649	 l-p:0.28994885087013245
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2551, 5.5717, 5.5029],
        [5.2551, 5.2519, 5.2549],
        [5.2551, 5.2312, 5.2496],
        [5.2551, 5.1428, 5.0983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.12366578727960587 
model_pd.l_d.mean(): -19.562061309814453 
model_pd.lagr.mean(): -19.438396453857422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4284], device='cuda:0')), ('power', tensor([-20.3716], device='cuda:0'))])
epoch£º533	 i:0 	 global-step:10660	 l-p:0.12366578727960587
epoch£º533	 i:1 	 global-step:10661	 l-p:0.1158253625035286
epoch£º533	 i:2 	 global-step:10662	 l-p:0.11632556468248367
epoch£º533	 i:3 	 global-step:10663	 l-p:0.14305077493190765
epoch£º533	 i:4 	 global-step:10664	 l-p:-0.010195082984864712
epoch£º533	 i:5 	 global-step:10665	 l-p:0.13694125413894653
epoch£º533	 i:6 	 global-step:10666	 l-p:0.0997047945857048
epoch£º533	 i:7 	 global-step:10667	 l-p:0.09867584705352783
epoch£º533	 i:8 	 global-step:10668	 l-p:0.13899819552898407
epoch£º533	 i:9 	 global-step:10669	 l-p:-0.22800233960151672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1371, 5.0176, 4.9837],
        [5.1371, 5.0581, 4.8971],
        [5.1371, 5.1291, 5.1363],
        [5.1371, 5.0161, 4.9361]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): 0.0872945785522461 
model_pd.l_d.mean(): -19.804777145385742 
model_pd.lagr.mean(): -19.717483520507812 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5025], device='cuda:0')), ('power', tensor([-20.6956], device='cuda:0'))])
epoch£º534	 i:0 	 global-step:10680	 l-p:0.0872945785522461
epoch£º534	 i:1 	 global-step:10681	 l-p:0.13829433917999268
epoch£º534	 i:2 	 global-step:10682	 l-p:0.12654340267181396
epoch£º534	 i:3 	 global-step:10683	 l-p:0.12861418724060059
epoch£º534	 i:4 	 global-step:10684	 l-p:0.11256150901317596
epoch£º534	 i:5 	 global-step:10685	 l-p:-0.37437668442726135
epoch£º534	 i:6 	 global-step:10686	 l-p:0.205900639295578
epoch£º534	 i:7 	 global-step:10687	 l-p:0.11706231534481049
epoch£º534	 i:8 	 global-step:10688	 l-p:0.14852137863636017
epoch£º534	 i:9 	 global-step:10689	 l-p:0.1574964076280594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0467, 5.0465, 5.0467],
        [5.0467, 5.0258, 5.0427],
        [5.0467, 5.0467, 5.0467],
        [5.0467, 5.0016, 5.0304]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): -0.12171002477407455 
model_pd.l_d.mean(): -20.169586181640625 
model_pd.lagr.mean(): -20.291296005249023 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4628], device='cuda:0')), ('power', tensor([-21.0261], device='cuda:0'))])
epoch£º535	 i:0 	 global-step:10700	 l-p:-0.12171002477407455
epoch£º535	 i:1 	 global-step:10701	 l-p:0.1435987651348114
epoch£º535	 i:2 	 global-step:10702	 l-p:0.1542908251285553
epoch£º535	 i:3 	 global-step:10703	 l-p:0.16128318011760712
epoch£º535	 i:4 	 global-step:10704	 l-p:0.12377042323350906
epoch£º535	 i:5 	 global-step:10705	 l-p:0.15037937462329865
epoch£º535	 i:6 	 global-step:10706	 l-p:0.1282230019569397
epoch£º535	 i:7 	 global-step:10707	 l-p:0.11177118867635727
epoch£º535	 i:8 	 global-step:10708	 l-p:0.10103540122509003
epoch£º535	 i:9 	 global-step:10709	 l-p:0.13809768855571747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9895, 4.9783, 4.9882],
        [4.9895, 4.9840, 4.9891],
        [4.9895, 4.9793, 4.9883],
        [4.9895, 5.0801, 4.9041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.15788182616233826 
model_pd.l_d.mean(): -19.418458938598633 
model_pd.lagr.mean(): -19.260576248168945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5340], device='cuda:0')), ('power', tensor([-20.3346], device='cuda:0'))])
epoch£º536	 i:0 	 global-step:10720	 l-p:0.15788182616233826
epoch£º536	 i:1 	 global-step:10721	 l-p:0.07983050495386124
epoch£º536	 i:2 	 global-step:10722	 l-p:0.051872678101062775
epoch£º536	 i:3 	 global-step:10723	 l-p:0.10638287663459778
epoch£º536	 i:4 	 global-step:10724	 l-p:0.11233899742364883
epoch£º536	 i:5 	 global-step:10725	 l-p:0.10863090306520462
epoch£º536	 i:6 	 global-step:10726	 l-p:0.13948798179626465
epoch£º536	 i:7 	 global-step:10727	 l-p:0.15402193367481232
epoch£º536	 i:8 	 global-step:10728	 l-p:0.13047634065151215
epoch£º536	 i:9 	 global-step:10729	 l-p:0.1076519787311554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1224, 5.1223, 5.1224],
        [5.1224, 4.9974, 4.9434],
        [5.1224, 4.9984, 4.9550],
        [5.1224, 5.1224, 5.1224]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): 0.10221248120069504 
model_pd.l_d.mean(): -20.66615104675293 
model_pd.lagr.mean(): -20.56393814086914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3862], device='cuda:0')), ('power', tensor([-21.4526], device='cuda:0'))])
epoch£º537	 i:0 	 global-step:10740	 l-p:0.10221248120069504
epoch£º537	 i:1 	 global-step:10741	 l-p:0.12428952753543854
epoch£º537	 i:2 	 global-step:10742	 l-p:0.19505523145198822
epoch£º537	 i:3 	 global-step:10743	 l-p:0.06986898928880692
epoch£º537	 i:4 	 global-step:10744	 l-p:0.13412714004516602
epoch£º537	 i:5 	 global-step:10745	 l-p:0.05197206884622574
epoch£º537	 i:6 	 global-step:10746	 l-p:0.12468985468149185
epoch£º537	 i:7 	 global-step:10747	 l-p:0.10208024084568024
epoch£º537	 i:8 	 global-step:10748	 l-p:0.1286468505859375
epoch£º537	 i:9 	 global-step:10749	 l-p:0.16346149146556854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2235, 5.1914, 5.2144],
        [5.2235, 5.2230, 5.2235],
        [5.2235, 5.2235, 5.2235],
        [5.2235, 5.1723, 5.2016]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.1288013607263565 
model_pd.l_d.mean(): -20.454036712646484 
model_pd.lagr.mean(): -20.32523536682129 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3942], device='cuda:0')), ('power', tensor([-21.2447], device='cuda:0'))])
epoch£º538	 i:0 	 global-step:10760	 l-p:0.1288013607263565
epoch£º538	 i:1 	 global-step:10761	 l-p:-0.019017331302165985
epoch£º538	 i:2 	 global-step:10762	 l-p:0.13316960632801056
epoch£º538	 i:3 	 global-step:10763	 l-p:0.11128333956003189
epoch£º538	 i:4 	 global-step:10764	 l-p:-0.031050043180584908
epoch£º538	 i:5 	 global-step:10765	 l-p:0.16300487518310547
epoch£º538	 i:6 	 global-step:10766	 l-p:0.13019098341464996
epoch£º538	 i:7 	 global-step:10767	 l-p:0.2220877856016159
epoch£º538	 i:8 	 global-step:10768	 l-p:0.1328304409980774
epoch£º538	 i:9 	 global-step:10769	 l-p:-0.036463700234889984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2298, 5.1475, 5.1714],
        [5.2298, 5.2298, 5.2298],
        [5.2298, 5.2170, 5.2280],
        [5.2298, 5.2297, 5.2298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.11360687017440796 
model_pd.l_d.mean(): -20.319862365722656 
model_pd.lagr.mean(): -20.206254959106445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4161], device='cuda:0')), ('power', tensor([-21.1308], device='cuda:0'))])
epoch£º539	 i:0 	 global-step:10780	 l-p:0.11360687017440796
epoch£º539	 i:1 	 global-step:10781	 l-p:1.047983169555664
epoch£º539	 i:2 	 global-step:10782	 l-p:0.12734900414943695
epoch£º539	 i:3 	 global-step:10783	 l-p:0.1494171917438507
epoch£º539	 i:4 	 global-step:10784	 l-p:0.12316032499074936
epoch£º539	 i:5 	 global-step:10785	 l-p:0.12373200803995132
epoch£º539	 i:6 	 global-step:10786	 l-p:0.11577586084604263
epoch£º539	 i:7 	 global-step:10787	 l-p:0.1176820918917656
epoch£º539	 i:8 	 global-step:10788	 l-p:0.18756362795829773
epoch£º539	 i:9 	 global-step:10789	 l-p:0.04250550642609596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1728, 5.0756, 4.9322],
        [5.1728, 5.1728, 5.1728],
        [5.1728, 5.0673, 5.0717],
        [5.1728, 5.3032, 5.1412]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.16259992122650146 
model_pd.l_d.mean(): -19.141231536865234 
model_pd.lagr.mean(): -18.9786319732666 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5440], device='cuda:0')), ('power', tensor([-20.0626], device='cuda:0'))])
epoch£º540	 i:0 	 global-step:10800	 l-p:0.16259992122650146
epoch£º540	 i:1 	 global-step:10801	 l-p:0.1885485053062439
epoch£º540	 i:2 	 global-step:10802	 l-p:0.13501985371112823
epoch£º540	 i:3 	 global-step:10803	 l-p:0.12487932294607162
epoch£º540	 i:4 	 global-step:10804	 l-p:0.0667831152677536
epoch£º540	 i:5 	 global-step:10805	 l-p:0.12106728553771973
epoch£º540	 i:6 	 global-step:10806	 l-p:0.12595410645008087
epoch£º540	 i:7 	 global-step:10807	 l-p:0.13349507749080658
epoch£º540	 i:8 	 global-step:10808	 l-p:0.13630051910877228
epoch£º540	 i:9 	 global-step:10809	 l-p:0.4420301020145416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1044, 5.0675, 5.0932],
        [5.1044, 5.0750, 5.0970],
        [5.1044, 5.5891, 5.6389],
        [5.1044, 5.1044, 5.1045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.11531630903482437 
model_pd.l_d.mean(): -19.0722713470459 
model_pd.lagr.mean(): -18.956954956054688 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4899], device='cuda:0')), ('power', tensor([-19.9363], device='cuda:0'))])
epoch£º541	 i:0 	 global-step:10820	 l-p:0.11531630903482437
epoch£º541	 i:1 	 global-step:10821	 l-p:0.1386047899723053
epoch£º541	 i:2 	 global-step:10822	 l-p:-0.2626045346260071
epoch£º541	 i:3 	 global-step:10823	 l-p:0.09353407472372055
epoch£º541	 i:4 	 global-step:10824	 l-p:0.15101824700832367
epoch£º541	 i:5 	 global-step:10825	 l-p:0.13815335929393768
epoch£º541	 i:6 	 global-step:10826	 l-p:0.12094821780920029
epoch£º541	 i:7 	 global-step:10827	 l-p:0.10845182090997696
epoch£º541	 i:8 	 global-step:10828	 l-p:0.12417241185903549
epoch£º541	 i:9 	 global-step:10829	 l-p:0.15818174183368683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9411, 4.9411, 4.9411],
        [4.9411, 4.8043, 4.6829],
        [4.9411, 5.5488, 5.7025],
        [4.9411, 4.8290, 4.6629]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.15941612422466278 
model_pd.l_d.mean(): -18.43804168701172 
model_pd.lagr.mean(): -18.27862548828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5901], device='cuda:0')), ('power', tensor([-19.3940], device='cuda:0'))])
epoch£º542	 i:0 	 global-step:10840	 l-p:0.15941612422466278
epoch£º542	 i:1 	 global-step:10841	 l-p:0.1804133504629135
epoch£º542	 i:2 	 global-step:10842	 l-p:0.13479049503803253
epoch£º542	 i:3 	 global-step:10843	 l-p:0.28573745489120483
epoch£º542	 i:4 	 global-step:10844	 l-p:0.09689155220985413
epoch£º542	 i:5 	 global-step:10845	 l-p:0.14009101688861847
epoch£º542	 i:6 	 global-step:10846	 l-p:0.1358150690793991
epoch£º542	 i:7 	 global-step:10847	 l-p:0.1639070212841034
epoch£º542	 i:8 	 global-step:10848	 l-p:0.14850619435310364
epoch£º542	 i:9 	 global-step:10849	 l-p:0.13118159770965576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9671, 4.8529, 4.6922],
        [4.9671, 5.5230, 5.6341],
        [4.9671, 4.8876, 4.9205],
        [4.9671, 4.9657, 4.9670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.13504819571971893 
model_pd.l_d.mean(): -19.932146072387695 
model_pd.lagr.mean(): -19.79709815979004 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5120], device='cuda:0')), ('power', tensor([-20.8352], device='cuda:0'))])
epoch£º543	 i:0 	 global-step:10860	 l-p:0.13504819571971893
epoch£º543	 i:1 	 global-step:10861	 l-p:0.08592766523361206
epoch£º543	 i:2 	 global-step:10862	 l-p:0.16075968742370605
epoch£º543	 i:3 	 global-step:10863	 l-p:0.1347913146018982
epoch£º543	 i:4 	 global-step:10864	 l-p:0.12821201980113983
epoch£º543	 i:5 	 global-step:10865	 l-p:0.12193101644515991
epoch£º543	 i:6 	 global-step:10866	 l-p:0.15445400774478912
epoch£º543	 i:7 	 global-step:10867	 l-p:0.21887195110321045
epoch£º543	 i:8 	 global-step:10868	 l-p:0.2400364875793457
epoch£º543	 i:9 	 global-step:10869	 l-p:0.1992802768945694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9750, 4.9739, 4.9750],
        [4.9750, 4.8826, 4.7020],
        [4.9750, 4.9750, 4.9750],
        [4.9750, 5.3130, 5.2666]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.11179591715335846 
model_pd.l_d.mean(): -20.432748794555664 
model_pd.lagr.mean(): -20.320953369140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4619], device='cuda:0')), ('power', tensor([-21.2932], device='cuda:0'))])
epoch£º544	 i:0 	 global-step:10880	 l-p:0.11179591715335846
epoch£º544	 i:1 	 global-step:10881	 l-p:0.1365003138780594
epoch£º544	 i:2 	 global-step:10882	 l-p:0.10524610430002213
epoch£º544	 i:3 	 global-step:10883	 l-p:0.14750173687934875
epoch£º544	 i:4 	 global-step:10884	 l-p:0.13912582397460938
epoch£º544	 i:5 	 global-step:10885	 l-p:0.1394529491662979
epoch£º544	 i:6 	 global-step:10886	 l-p:0.0945424810051918
epoch£º544	 i:7 	 global-step:10887	 l-p:0.11626872420310974
epoch£º544	 i:8 	 global-step:10888	 l-p:0.1451585292816162
epoch£º544	 i:9 	 global-step:10889	 l-p:0.1306585818529129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0683, 5.0564, 5.0668],
        [5.0683, 5.0220, 5.0515],
        [5.0683, 4.9465, 4.9374],
        [5.0683, 4.9488, 4.9442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.09772119671106339 
model_pd.l_d.mean(): -20.591176986694336 
model_pd.lagr.mean(): -20.49345588684082 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4340], device='cuda:0')), ('power', tensor([-21.4257], device='cuda:0'))])
epoch£º545	 i:0 	 global-step:10900	 l-p:0.09772119671106339
epoch£º545	 i:1 	 global-step:10901	 l-p:0.14154784381389618
epoch£º545	 i:2 	 global-step:10902	 l-p:0.13145047426223755
epoch£º545	 i:3 	 global-step:10903	 l-p:0.11431745439767838
epoch£º545	 i:4 	 global-step:10904	 l-p:0.06852471083402634
epoch£º545	 i:5 	 global-step:10905	 l-p:0.4093243181705475
epoch£º545	 i:6 	 global-step:10906	 l-p:0.1485443115234375
epoch£º545	 i:7 	 global-step:10907	 l-p:0.09449156373739243
epoch£º545	 i:8 	 global-step:10908	 l-p:0.14836211502552032
epoch£º545	 i:9 	 global-step:10909	 l-p:0.11307605355978012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2063, 5.1800, 5.2002],
        [5.2063, 5.1812, 5.2006],
        [5.2063, 5.2062, 5.2063],
        [5.2063, 5.0917, 4.9773]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.12564411759376526 
model_pd.l_d.mean(): -19.64487648010254 
model_pd.lagr.mean(): -19.51923179626465 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3959], device='cuda:0')), ('power', tensor([-20.4223], device='cuda:0'))])
epoch£º546	 i:0 	 global-step:10920	 l-p:0.12564411759376526
epoch£º546	 i:1 	 global-step:10921	 l-p:0.13235248625278473
epoch£º546	 i:2 	 global-step:10922	 l-p:0.12684668600559235
epoch£º546	 i:3 	 global-step:10923	 l-p:0.06657865643501282
epoch£º546	 i:4 	 global-step:10924	 l-p:0.028717437759041786
epoch£º546	 i:5 	 global-step:10925	 l-p:0.14698901772499084
epoch£º546	 i:6 	 global-step:10926	 l-p:0.13686271011829376
epoch£º546	 i:7 	 global-step:10927	 l-p:0.1407153457403183
epoch£º546	 i:8 	 global-step:10928	 l-p:0.11886259913444519
epoch£º546	 i:9 	 global-step:10929	 l-p:0.12919463217258453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1840, 5.0888, 5.1082],
        [5.1840, 5.7103, 5.7849],
        [5.1840, 5.1830, 5.1840],
        [5.1840, 5.1840, 5.1840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.1197044625878334 
model_pd.l_d.mean(): -20.328330993652344 
model_pd.lagr.mean(): -20.20862579345703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4237], device='cuda:0')), ('power', tensor([-21.1473], device='cuda:0'))])
epoch£º547	 i:0 	 global-step:10940	 l-p:0.1197044625878334
epoch£º547	 i:1 	 global-step:10941	 l-p:0.1508655995130539
epoch£º547	 i:2 	 global-step:10942	 l-p:0.1139443963766098
epoch£º547	 i:3 	 global-step:10943	 l-p:0.09275712817907333
epoch£º547	 i:4 	 global-step:10944	 l-p:0.17226238548755646
epoch£º547	 i:5 	 global-step:10945	 l-p:0.1512567549943924
epoch£º547	 i:6 	 global-step:10946	 l-p:0.13814441859722137
epoch£º547	 i:7 	 global-step:10947	 l-p:0.04251563921570778
epoch£º547	 i:8 	 global-step:10948	 l-p:0.135061576962471
epoch£º547	 i:9 	 global-step:10949	 l-p:0.11518776416778564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0817, 5.0815, 5.0817],
        [5.0817, 4.9919, 5.0191],
        [5.0817, 5.0499, 5.0734],
        [5.0817, 5.0353, 5.0650]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.12508510053157806 
model_pd.l_d.mean(): -19.279916763305664 
model_pd.lagr.mean(): -19.154830932617188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5173], device='cuda:0')), ('power', tensor([-20.1763], device='cuda:0'))])
epoch£º548	 i:0 	 global-step:10960	 l-p:0.12508510053157806
epoch£º548	 i:1 	 global-step:10961	 l-p:0.1276005357503891
epoch£º548	 i:2 	 global-step:10962	 l-p:0.15334148705005646
epoch£º548	 i:3 	 global-step:10963	 l-p:0.12522272765636444
epoch£º548	 i:4 	 global-step:10964	 l-p:0.18296094238758087
epoch£º548	 i:5 	 global-step:10965	 l-p:0.1296512633562088
epoch£º548	 i:6 	 global-step:10966	 l-p:0.1355222463607788
epoch£º548	 i:7 	 global-step:10967	 l-p:0.10258457064628601
epoch£º548	 i:8 	 global-step:10968	 l-p:0.12120223790407181
epoch£º548	 i:9 	 global-step:10969	 l-p:0.10265658795833588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9904, 5.3111, 5.2516],
        [4.9904, 4.9851, 4.9900],
        [4.9904, 4.9822, 4.9896],
        [4.9904, 4.8518, 4.8142]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): 0.11532361805438995 
model_pd.l_d.mean(): -20.423622131347656 
model_pd.lagr.mean(): -20.308298110961914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4583], device='cuda:0')), ('power', tensor([-21.2802], device='cuda:0'))])
epoch£º549	 i:0 	 global-step:10980	 l-p:0.11532361805438995
epoch£º549	 i:1 	 global-step:10981	 l-p:0.15003947913646698
epoch£º549	 i:2 	 global-step:10982	 l-p:0.1605895608663559
epoch£º549	 i:3 	 global-step:10983	 l-p:0.13971692323684692
epoch£º549	 i:4 	 global-step:10984	 l-p:0.10348151624202728
epoch£º549	 i:5 	 global-step:10985	 l-p:0.11745317280292511
epoch£º549	 i:6 	 global-step:10986	 l-p:0.1763167381286621
epoch£º549	 i:7 	 global-step:10987	 l-p:0.14708967506885529
epoch£º549	 i:8 	 global-step:10988	 l-p:0.14648261666297913
epoch£º549	 i:9 	 global-step:10989	 l-p:0.1462412178516388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9513, 4.9165, 4.9419],
        [4.9513, 4.8102, 4.6859],
        [4.9513, 4.8042, 4.7343],
        [4.9513, 4.8945, 4.9279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.14427335560321808 
model_pd.l_d.mean(): -18.393856048583984 
model_pd.lagr.mean(): -18.249582290649414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5987], device='cuda:0')), ('power', tensor([-19.3579], device='cuda:0'))])
epoch£º550	 i:0 	 global-step:11000	 l-p:0.14427335560321808
epoch£º550	 i:1 	 global-step:11001	 l-p:0.1093873456120491
epoch£º550	 i:2 	 global-step:11002	 l-p:0.2414473444223404
epoch£º550	 i:3 	 global-step:11003	 l-p:0.1086849570274353
epoch£º550	 i:4 	 global-step:11004	 l-p:0.12096071243286133
epoch£º550	 i:5 	 global-step:11005	 l-p:0.13724994659423828
epoch£º550	 i:6 	 global-step:11006	 l-p:0.1643529236316681
epoch£º550	 i:7 	 global-step:11007	 l-p:0.13354690372943878
epoch£º550	 i:8 	 global-step:11008	 l-p:0.136298269033432
epoch£º550	 i:9 	 global-step:11009	 l-p:0.1192128136754036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0213, 4.9088, 4.9226],
        [5.0213, 5.1056, 4.9222],
        [5.0213, 5.5806, 5.6889],
        [5.0213, 5.0213, 5.0213]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.134134441614151 
model_pd.l_d.mean(): -18.041648864746094 
model_pd.lagr.mean(): -17.907514572143555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5926], device='cuda:0')), ('power', tensor([-18.9928], device='cuda:0'))])
epoch£º551	 i:0 	 global-step:11020	 l-p:0.134134441614151
epoch£º551	 i:1 	 global-step:11021	 l-p:0.14465147256851196
epoch£º551	 i:2 	 global-step:11022	 l-p:0.10749492049217224
epoch£º551	 i:3 	 global-step:11023	 l-p:0.16270984709262848
epoch£º551	 i:4 	 global-step:11024	 l-p:0.11653582006692886
epoch£º551	 i:5 	 global-step:11025	 l-p:0.15645523369312286
epoch£º551	 i:6 	 global-step:11026	 l-p:0.1413254737854004
epoch£º551	 i:7 	 global-step:11027	 l-p:0.1287311613559723
epoch£º551	 i:8 	 global-step:11028	 l-p:0.1310247927904129
epoch£º551	 i:9 	 global-step:11029	 l-p:0.13931599259376526
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9508, 4.8089, 4.6807],
        [4.9508, 4.9508, 4.9508],
        [4.9508, 4.9508, 4.9508],
        [4.9508, 4.9508, 4.9508]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.11790823936462402 
model_pd.l_d.mean(): -19.800016403198242 
model_pd.lagr.mean(): -19.68210792541504 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5324], device='cuda:0')), ('power', tensor([-20.7217], device='cuda:0'))])
epoch£º552	 i:0 	 global-step:11040	 l-p:0.11790823936462402
epoch£º552	 i:1 	 global-step:11041	 l-p:0.21098873019218445
epoch£º552	 i:2 	 global-step:11042	 l-p:0.30581051111221313
epoch£º552	 i:3 	 global-step:11043	 l-p:0.16168180108070374
epoch£º552	 i:4 	 global-step:11044	 l-p:0.13905653357505798
epoch£º552	 i:5 	 global-step:11045	 l-p:0.1215912327170372
epoch£º552	 i:6 	 global-step:11046	 l-p:0.1288343369960785
epoch£º552	 i:7 	 global-step:11047	 l-p:0.14003844559192657
epoch£º552	 i:8 	 global-step:11048	 l-p:0.1699405014514923
epoch£º552	 i:9 	 global-step:11049	 l-p:0.1861562728881836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9618, 4.9618, 4.9618],
        [4.9618, 4.8215, 4.6893],
        [4.9618, 5.4603, 5.5251],
        [4.9618, 4.9157, 4.9461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.21361152827739716 
model_pd.l_d.mean(): -20.10773277282715 
model_pd.lagr.mean(): -19.894121170043945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5360], device='cuda:0')), ('power', tensor([-21.0389], device='cuda:0'))])
epoch£º553	 i:0 	 global-step:11060	 l-p:0.21361152827739716
epoch£º553	 i:1 	 global-step:11061	 l-p:0.11570669710636139
epoch£º553	 i:2 	 global-step:11062	 l-p:0.10866083949804306
epoch£º553	 i:3 	 global-step:11063	 l-p:0.17002147436141968
epoch£º553	 i:4 	 global-step:11064	 l-p:0.13952277600765228
epoch£º553	 i:5 	 global-step:11065	 l-p:0.08122110366821289
epoch£º553	 i:6 	 global-step:11066	 l-p:0.13433873653411865
epoch£º553	 i:7 	 global-step:11067	 l-p:0.1353069245815277
epoch£º553	 i:8 	 global-step:11068	 l-p:0.10963335633277893
epoch£º553	 i:9 	 global-step:11069	 l-p:0.18836148083209991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0355, 5.0136, 5.0313],
        [5.0355, 5.0345, 5.0354],
        [5.0355, 5.0084, 5.0294],
        [5.0355, 4.9693, 4.7730]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): 0.08611314743757248 
model_pd.l_d.mean(): -19.869035720825195 
model_pd.lagr.mean(): -19.782922744750977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4631], device='cuda:0')), ('power', tensor([-20.7202], device='cuda:0'))])
epoch£º554	 i:0 	 global-step:11080	 l-p:0.08611314743757248
epoch£º554	 i:1 	 global-step:11081	 l-p:0.13001778721809387
epoch£º554	 i:2 	 global-step:11082	 l-p:0.09570668637752533
epoch£º554	 i:3 	 global-step:11083	 l-p:0.10934197902679443
epoch£º554	 i:4 	 global-step:11084	 l-p:0.14074759185314178
epoch£º554	 i:5 	 global-step:11085	 l-p:0.2769463360309601
epoch£º554	 i:6 	 global-step:11086	 l-p:0.12185248732566833
epoch£º554	 i:7 	 global-step:11087	 l-p:0.12579531967639923
epoch£º554	 i:8 	 global-step:11088	 l-p:0.12267657369375229
epoch£º554	 i:9 	 global-step:11089	 l-p:0.13639985024929047
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1549, 5.1549, 5.1549],
        [5.1549, 5.0790, 5.1110],
        [5.1549, 5.1420, 5.1532],
        [5.1549, 5.0643, 5.0911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.13863416016101837 
model_pd.l_d.mean(): -20.25864601135254 
model_pd.lagr.mean(): -20.120012283325195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4300], device='cuda:0')), ('power', tensor([-21.0828], device='cuda:0'))])
epoch£º555	 i:0 	 global-step:11100	 l-p:0.13863416016101837
epoch£º555	 i:1 	 global-step:11101	 l-p:0.09022693336009979
epoch£º555	 i:2 	 global-step:11102	 l-p:0.10681459307670593
epoch£º555	 i:3 	 global-step:11103	 l-p:0.17464129626750946
epoch£º555	 i:4 	 global-step:11104	 l-p:0.1820065677165985
epoch£º555	 i:5 	 global-step:11105	 l-p:-0.023019570857286453
epoch£º555	 i:6 	 global-step:11106	 l-p:0.14077094197273254
epoch£º555	 i:7 	 global-step:11107	 l-p:0.11108088493347168
epoch£º555	 i:8 	 global-step:11108	 l-p:0.1113949567079544
epoch£º555	 i:9 	 global-step:11109	 l-p:0.1280425488948822
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2959, 5.1758, 5.0794],
        [5.2959, 5.2959, 5.2959],
        [5.2959, 5.3693, 5.1850],
        [5.2959, 5.2941, 5.2958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): 0.15807293355464935 
model_pd.l_d.mean(): -20.39238739013672 
model_pd.lagr.mean(): -20.23431396484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3928], device='cuda:0')), ('power', tensor([-21.1805], device='cuda:0'))])
epoch£º556	 i:0 	 global-step:11120	 l-p:0.15807293355464935
epoch£º556	 i:1 	 global-step:11121	 l-p:0.21675385534763336
epoch£º556	 i:2 	 global-step:11122	 l-p:0.18955913186073303
epoch£º556	 i:3 	 global-step:11123	 l-p:0.11800333112478256
epoch£º556	 i:4 	 global-step:11124	 l-p:0.13365165889263153
epoch£º556	 i:5 	 global-step:11125	 l-p:0.14418193697929382
epoch£º556	 i:6 	 global-step:11126	 l-p:-24.97905731201172
epoch£º556	 i:7 	 global-step:11127	 l-p:0.12451322376728058
epoch£º556	 i:8 	 global-step:11128	 l-p:0.11302752047777176
epoch£º556	 i:9 	 global-step:11129	 l-p:0.13308051228523254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1922, 5.1819, 5.1910],
        [5.1922, 5.0999, 4.9320],
        [5.1922, 5.1333, 5.1656],
        [5.1922, 5.2536, 5.0638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.0915016233921051 
model_pd.l_d.mean(): -19.796653747558594 
model_pd.lagr.mean(): -19.70515251159668 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4899], device='cuda:0')), ('power', tensor([-20.6743], device='cuda:0'))])
epoch£º557	 i:0 	 global-step:11140	 l-p:0.0915016233921051
epoch£º557	 i:1 	 global-step:11141	 l-p:0.12815934419631958
epoch£º557	 i:2 	 global-step:11142	 l-p:-0.1278124451637268
epoch£º557	 i:3 	 global-step:11143	 l-p:0.12863923609256744
epoch£º557	 i:4 	 global-step:11144	 l-p:0.12814031541347504
epoch£º557	 i:5 	 global-step:11145	 l-p:0.1356797218322754
epoch£º557	 i:6 	 global-step:11146	 l-p:0.6734748482704163
epoch£º557	 i:7 	 global-step:11147	 l-p:0.13856440782546997
epoch£º557	 i:8 	 global-step:11148	 l-p:0.11664474010467529
epoch£º557	 i:9 	 global-step:11149	 l-p:0.1207084134221077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0703, 4.9268, 4.8451],
        [5.0703, 5.0711, 4.8671],
        [5.0703, 5.0405, 5.0632],
        [5.0703, 5.0633, 5.0697]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): 0.08330530673265457 
model_pd.l_d.mean(): -19.75857162475586 
model_pd.lagr.mean(): -19.67526626586914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4799], device='cuda:0')), ('power', tensor([-20.6251], device='cuda:0'))])
epoch£º558	 i:0 	 global-step:11160	 l-p:0.08330530673265457
epoch£º558	 i:1 	 global-step:11161	 l-p:0.04047402739524841
epoch£º558	 i:2 	 global-step:11162	 l-p:0.12592628598213196
epoch£º558	 i:3 	 global-step:11163	 l-p:0.13284166157245636
epoch£º558	 i:4 	 global-step:11164	 l-p:0.11315619945526123
epoch£º558	 i:5 	 global-step:11165	 l-p:0.13683810830116272
epoch£º558	 i:6 	 global-step:11166	 l-p:0.1243804320693016
epoch£º558	 i:7 	 global-step:11167	 l-p:0.15821950137615204
epoch£º558	 i:8 	 global-step:11168	 l-p:0.12795430421829224
epoch£º558	 i:9 	 global-step:11169	 l-p:0.1515345275402069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0118, 4.9334, 4.7338],
        [5.0118, 5.0024, 5.0109],
        [5.0118, 5.0118, 5.0118],
        [5.0118, 5.0111, 5.0118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.1431369185447693 
model_pd.l_d.mean(): -19.733108520507812 
model_pd.lagr.mean(): -19.5899715423584 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5621], device='cuda:0')), ('power', tensor([-20.6842], device='cuda:0'))])
epoch£º559	 i:0 	 global-step:11180	 l-p:0.1431369185447693
epoch£º559	 i:1 	 global-step:11181	 l-p:0.1506083458662033
epoch£º559	 i:2 	 global-step:11182	 l-p:0.10677702724933624
epoch£º559	 i:3 	 global-step:11183	 l-p:0.16344019770622253
epoch£º559	 i:4 	 global-step:11184	 l-p:0.17632049322128296
epoch£º559	 i:5 	 global-step:11185	 l-p:0.14449606835842133
epoch£º559	 i:6 	 global-step:11186	 l-p:0.17344123125076294
epoch£º559	 i:7 	 global-step:11187	 l-p:0.157577246427536
epoch£º559	 i:8 	 global-step:11188	 l-p:0.05978783592581749
epoch£º559	 i:9 	 global-step:11189	 l-p:0.11956688016653061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9770,  0.9695,  1.0000,  0.9620,
          1.0000,  0.9923, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9814,  0.9752,  1.0000,  0.9691,
          1.0000,  0.9938, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228]], device='cuda:0')
 pt:tensor([[5.0723, 5.6952, 5.8466],
        [5.0723, 4.9394, 4.7988],
        [5.0723, 5.7019, 5.8586],
        [5.0723, 5.0934, 4.8900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): 0.12010625749826431 
model_pd.l_d.mean(): -18.559450149536133 
model_pd.lagr.mean(): -18.43934440612793 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5483], device='cuda:0')), ('power', tensor([-19.4743], device='cuda:0'))])
epoch£º560	 i:0 	 global-step:11200	 l-p:0.12010625749826431
epoch£º560	 i:1 	 global-step:11201	 l-p:0.42627039551734924
epoch£º560	 i:2 	 global-step:11202	 l-p:0.08496066182851791
epoch£º560	 i:3 	 global-step:11203	 l-p:-0.04748866334557533
epoch£º560	 i:4 	 global-step:11204	 l-p:0.19118253886699677
epoch£º560	 i:5 	 global-step:11205	 l-p:0.11322399228811264
epoch£º560	 i:6 	 global-step:11206	 l-p:0.1346285194158554
epoch£º560	 i:7 	 global-step:11207	 l-p:7.580552577972412
epoch£º560	 i:8 	 global-step:11208	 l-p:0.14796283841133118
epoch£º560	 i:9 	 global-step:11209	 l-p:0.12139780074357986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2912, 5.2912, 5.2912],
        [5.2912, 5.1745, 5.0500],
        [5.2912, 5.2820, 5.2902],
        [5.2912, 5.3702, 5.1844]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): 0.20639266073703766 
model_pd.l_d.mean(): -19.530454635620117 
model_pd.lagr.mean(): -19.32406234741211 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4197], device='cuda:0')), ('power', tensor([-20.3304], device='cuda:0'))])
epoch£º561	 i:0 	 global-step:11220	 l-p:0.20639266073703766
epoch£º561	 i:1 	 global-step:11221	 l-p:0.20138871669769287
epoch£º561	 i:2 	 global-step:11222	 l-p:0.14093436300754547
epoch£º561	 i:3 	 global-step:11223	 l-p:0.1146291121840477
epoch£º561	 i:4 	 global-step:11224	 l-p:0.12746796011924744
epoch£º561	 i:5 	 global-step:11225	 l-p:0.12058892846107483
epoch£º561	 i:6 	 global-step:11226	 l-p:0.09308956563472748
epoch£º561	 i:7 	 global-step:11227	 l-p:0.13387656211853027
epoch£º561	 i:8 	 global-step:11228	 l-p:-0.40786710381507874
epoch£º561	 i:9 	 global-step:11229	 l-p:0.18446208536624908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2264, 5.1880, 5.2147],
        [5.2264, 5.2259, 5.2264],
        [5.2264, 5.2264, 5.2264],
        [5.2264, 5.1600, 4.9726]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.13581033051013947 
model_pd.l_d.mean(): -19.76959228515625 
model_pd.lagr.mean(): -19.63378143310547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4215], device='cuda:0')), ('power', tensor([-20.5758], device='cuda:0'))])
epoch£º562	 i:0 	 global-step:11240	 l-p:0.13581033051013947
epoch£º562	 i:1 	 global-step:11241	 l-p:0.12720955908298492
epoch£º562	 i:2 	 global-step:11242	 l-p:0.15674522519111633
epoch£º562	 i:3 	 global-step:11243	 l-p:0.06515033543109894
epoch£º562	 i:4 	 global-step:11244	 l-p:0.11334502696990967
epoch£º562	 i:5 	 global-step:11245	 l-p:0.13402807712554932
epoch£º562	 i:6 	 global-step:11246	 l-p:0.1398046463727951
epoch£º562	 i:7 	 global-step:11247	 l-p:0.13476447761058807
epoch£º562	 i:8 	 global-step:11248	 l-p:-0.020928438752889633
epoch£º562	 i:9 	 global-step:11249	 l-p:0.07258769124746323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0773, 5.0700, 5.0767],
        [5.0773, 4.9753, 5.0017],
        [5.0773, 5.0050, 5.0402],
        [5.0773, 5.0435, 5.0684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): 0.030364936217665672 
model_pd.l_d.mean(): -20.41696548461914 
model_pd.lagr.mean(): -20.386600494384766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4550], device='cuda:0')), ('power', tensor([-21.2700], device='cuda:0'))])
epoch£º563	 i:0 	 global-step:11260	 l-p:0.030364936217665672
epoch£º563	 i:1 	 global-step:11261	 l-p:0.14092250168323517
epoch£º563	 i:2 	 global-step:11262	 l-p:0.10369257628917694
epoch£º563	 i:3 	 global-step:11263	 l-p:0.15580199658870697
epoch£º563	 i:4 	 global-step:11264	 l-p:-0.031436365097761154
epoch£º563	 i:5 	 global-step:11265	 l-p:0.14083431661128998
epoch£º563	 i:6 	 global-step:11266	 l-p:0.13721005618572235
epoch£º563	 i:7 	 global-step:11267	 l-p:0.145769864320755
epoch£º563	 i:8 	 global-step:11268	 l-p:0.1095028817653656
epoch£º563	 i:9 	 global-step:11269	 l-p:0.12575587630271912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1715,  0.0953,  1.0000,  0.0530,
          1.0000,  0.5556, 31.6228]], device='cuda:0')
 pt:tensor([[5.0383, 4.9231, 4.7447],
        [5.0383, 5.0485, 4.8405],
        [5.0383, 4.8871, 4.8052],
        [5.0383, 4.9227, 4.9406]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.05697782337665558 
model_pd.l_d.mean(): -19.53923225402832 
model_pd.lagr.mean(): -19.482254028320312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4844], device='cuda:0')), ('power', tensor([-20.4063], device='cuda:0'))])
epoch£º564	 i:0 	 global-step:11280	 l-p:0.05697782337665558
epoch£º564	 i:1 	 global-step:11281	 l-p:0.2126501202583313
epoch£º564	 i:2 	 global-step:11282	 l-p:0.13405512273311615
epoch£º564	 i:3 	 global-step:11283	 l-p:0.14041239023208618
epoch£º564	 i:4 	 global-step:11284	 l-p:0.1916131228208542
epoch£º564	 i:5 	 global-step:11285	 l-p:0.10672219842672348
epoch£º564	 i:6 	 global-step:11286	 l-p:0.1310480237007141
epoch£º564	 i:7 	 global-step:11287	 l-p:0.12824854254722595
epoch£º564	 i:8 	 global-step:11288	 l-p:0.1327139139175415
epoch£º564	 i:9 	 global-step:11289	 l-p:0.1215558871626854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8939, 4.8886, 4.8935],
        [4.8939, 4.7854, 4.8153],
        [4.8939, 5.3793, 5.4340],
        [4.8939, 5.3845, 5.4430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.04419270530343056 
model_pd.l_d.mean(): -20.01165008544922 
model_pd.lagr.mean(): -19.967456817626953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5486], device='cuda:0')), ('power', tensor([-20.9541], device='cuda:0'))])
epoch£º565	 i:0 	 global-step:11300	 l-p:0.04419270530343056
epoch£º565	 i:1 	 global-step:11301	 l-p:0.23778091371059418
epoch£º565	 i:2 	 global-step:11302	 l-p:-0.01870403252542019
epoch£º565	 i:3 	 global-step:11303	 l-p:0.15824516117572784
epoch£º565	 i:4 	 global-step:11304	 l-p:0.49904951453208923
epoch£º565	 i:5 	 global-step:11305	 l-p:0.20693476498126984
epoch£º565	 i:6 	 global-step:11306	 l-p:0.164671391248703
epoch£º565	 i:7 	 global-step:11307	 l-p:0.17831401526927948
epoch£º565	 i:8 	 global-step:11308	 l-p:0.09369663149118423
epoch£º565	 i:9 	 global-step:11309	 l-p:0.10221447795629501
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0485, 5.0350, 5.0468],
        [5.0485, 5.0468, 5.0485],
        [5.0485, 4.9487, 4.9781],
        [5.0485, 4.9979, 5.0301]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.05958477780222893 
model_pd.l_d.mean(): -20.188175201416016 
model_pd.lagr.mean(): -20.128589630126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4793], device='cuda:0')), ('power', tensor([-21.0621], device='cuda:0'))])
epoch£º566	 i:0 	 global-step:11320	 l-p:0.05958477780222893
epoch£º566	 i:1 	 global-step:11321	 l-p:0.11796397715806961
epoch£º566	 i:2 	 global-step:11322	 l-p:0.09824559092521667
epoch£º566	 i:3 	 global-step:11323	 l-p:0.1535649299621582
epoch£º566	 i:4 	 global-step:11324	 l-p:0.1557009220123291
epoch£º566	 i:5 	 global-step:11325	 l-p:0.11550118774175644
epoch£º566	 i:6 	 global-step:11326	 l-p:0.1941516101360321
epoch£º566	 i:7 	 global-step:11327	 l-p:0.032626476138830185
epoch£º566	 i:8 	 global-step:11328	 l-p:0.0922183021903038
epoch£º566	 i:9 	 global-step:11329	 l-p:0.1297294795513153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228]], device='cuda:0')
 pt:tensor([[5.1865, 5.7932, 5.9232],
        [5.1865, 5.7258, 5.8042],
        [5.1865, 5.0689, 5.0758],
        [5.1865, 5.5361, 5.4812]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.14945797622203827 
model_pd.l_d.mean(): -18.383224487304688 
model_pd.lagr.mean(): -18.233766555786133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5501], device='cuda:0')), ('power', tensor([-19.2968], device='cuda:0'))])
epoch£º567	 i:0 	 global-step:11340	 l-p:0.14945797622203827
epoch£º567	 i:1 	 global-step:11341	 l-p:0.1360546052455902
epoch£º567	 i:2 	 global-step:11342	 l-p:0.17171016335487366
epoch£º567	 i:3 	 global-step:11343	 l-p:0.13059386610984802
epoch£º567	 i:4 	 global-step:11344	 l-p:-0.0006565380026586354
epoch£º567	 i:5 	 global-step:11345	 l-p:0.12520557641983032
epoch£º567	 i:6 	 global-step:11346	 l-p:-0.7196043133735657
epoch£º567	 i:7 	 global-step:11347	 l-p:0.1367778331041336
epoch£º567	 i:8 	 global-step:11348	 l-p:0.12967067956924438
epoch£º567	 i:9 	 global-step:11349	 l-p:-0.05800079554319382
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2513, 5.1305, 4.9942],
        [5.2513, 5.2423, 5.2504],
        [5.2513, 5.4661, 5.3309],
        [5.2513, 5.1398, 4.9878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): -0.0452740453183651 
model_pd.l_d.mean(): -19.879579544067383 
model_pd.lagr.mean(): -19.924854278564453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4570], device='cuda:0')), ('power', tensor([-20.7246], device='cuda:0'))])
epoch£º568	 i:0 	 global-step:11360	 l-p:-0.0452740453183651
epoch£º568	 i:1 	 global-step:11361	 l-p:0.1426510214805603
epoch£º568	 i:2 	 global-step:11362	 l-p:0.11095991730690002
epoch£º568	 i:3 	 global-step:11363	 l-p:0.11157884448766708
epoch£º568	 i:4 	 global-step:11364	 l-p:0.18844328820705414
epoch£º568	 i:5 	 global-step:11365	 l-p:0.11609241366386414
epoch£º568	 i:6 	 global-step:11366	 l-p:0.11294867098331451
epoch£º568	 i:7 	 global-step:11367	 l-p:0.0826694443821907
epoch£º568	 i:8 	 global-step:11368	 l-p:0.19327014684677124
epoch£º568	 i:9 	 global-step:11369	 l-p:0.04773145169019699
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2033, 5.2033, 5.2033],
        [5.2033, 5.1717, 5.1953],
        [5.2033, 5.7963, 5.9143],
        [5.2033, 5.2033, 5.2033]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.12007149308919907 
model_pd.l_d.mean(): -19.400882720947266 
model_pd.lagr.mean(): -19.280811309814453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4685], device='cuda:0')), ('power', tensor([-20.2489], device='cuda:0'))])
epoch£º569	 i:0 	 global-step:11380	 l-p:0.12007149308919907
epoch£º569	 i:1 	 global-step:11381	 l-p:0.1172342300415039
epoch£º569	 i:2 	 global-step:11382	 l-p:0.06752582639455795
epoch£º569	 i:3 	 global-step:11383	 l-p:0.11994872987270355
epoch£º569	 i:4 	 global-step:11384	 l-p:-0.012912082485854626
epoch£º569	 i:5 	 global-step:11385	 l-p:0.12472432106733322
epoch£º569	 i:6 	 global-step:11386	 l-p:0.12640777230262756
epoch£º569	 i:7 	 global-step:11387	 l-p:0.14075656235218048
epoch£º569	 i:8 	 global-step:11388	 l-p:0.15420792996883392
epoch£º569	 i:9 	 global-step:11389	 l-p:0.1216912493109703
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1841, 5.1738, 5.1829],
        [5.1841, 5.1841, 5.1841],
        [5.1841, 5.0493, 4.9277],
        [5.1841, 5.4925, 5.4113]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.12815944850444794 
model_pd.l_d.mean(): -19.911357879638672 
model_pd.lagr.mean(): -19.783199310302734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4456], device='cuda:0')), ('power', tensor([-20.7452], device='cuda:0'))])
epoch£º570	 i:0 	 global-step:11400	 l-p:0.12815944850444794
epoch£º570	 i:1 	 global-step:11401	 l-p:0.13481691479682922
epoch£º570	 i:2 	 global-step:11402	 l-p:0.23816947638988495
epoch£º570	 i:3 	 global-step:11403	 l-p:-0.023900236934423447
epoch£º570	 i:4 	 global-step:11404	 l-p:0.13286776840686798
epoch£º570	 i:5 	 global-step:11405	 l-p:0.15992914140224457
epoch£º570	 i:6 	 global-step:11406	 l-p:0.1412922590970993
epoch£º570	 i:7 	 global-step:11407	 l-p:0.13978637754917145
epoch£º570	 i:8 	 global-step:11408	 l-p:0.10827487707138062
epoch£º570	 i:9 	 global-step:11409	 l-p:0.260513573884964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9531, 4.7957, 4.7484],
        [4.9531, 4.9531, 4.9531],
        [4.9531, 4.9214, 4.9456],
        [4.9531, 4.9475, 4.9527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.23806947469711304 
model_pd.l_d.mean(): -19.229095458984375 
model_pd.lagr.mean(): -18.991025924682617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5328], device='cuda:0')), ('power', tensor([-20.1405], device='cuda:0'))])
epoch£º571	 i:0 	 global-step:11420	 l-p:0.23806947469711304
epoch£º571	 i:1 	 global-step:11421	 l-p:0.09498906880617142
epoch£º571	 i:2 	 global-step:11422	 l-p:0.12124425917863846
epoch£º571	 i:3 	 global-step:11423	 l-p:0.1855601966381073
epoch£º571	 i:4 	 global-step:11424	 l-p:0.12121421843767166
epoch£º571	 i:5 	 global-step:11425	 l-p:0.13131240010261536
epoch£º571	 i:6 	 global-step:11426	 l-p:0.1941956728696823
epoch£º571	 i:7 	 global-step:11427	 l-p:0.11745776981115341
epoch£º571	 i:8 	 global-step:11428	 l-p:0.19616565108299255
epoch£º571	 i:9 	 global-step:11429	 l-p:0.12542682886123657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9925, 4.9915, 4.9925],
        [4.9925, 5.5733, 5.6929],
        [4.9925, 5.4787, 5.5267],
        [4.9925, 4.9794, 4.9909]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.13176117837429047 
model_pd.l_d.mean(): -19.4333553314209 
model_pd.lagr.mean(): -19.301593780517578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5448], device='cuda:0')), ('power', tensor([-20.3610], device='cuda:0'))])
epoch£º572	 i:0 	 global-step:11440	 l-p:0.13176117837429047
epoch£º572	 i:1 	 global-step:11441	 l-p:0.11309618502855301
epoch£º572	 i:2 	 global-step:11442	 l-p:0.12535439431667328
epoch£º572	 i:3 	 global-step:11443	 l-p:0.18386703729629517
epoch£º572	 i:4 	 global-step:11444	 l-p:0.18935465812683105
epoch£º572	 i:5 	 global-step:11445	 l-p:0.13858263194561005
epoch£º572	 i:6 	 global-step:11446	 l-p:0.007427849806845188
epoch£º572	 i:7 	 global-step:11447	 l-p:-2.650738477706909
epoch£º572	 i:8 	 global-step:11448	 l-p:0.20769864320755005
epoch£º572	 i:9 	 global-step:11449	 l-p:0.20852211117744446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9517, 4.8926, 4.9283],
        [4.9517, 4.8043, 4.7914],
        [4.9517, 4.8700, 4.9080],
        [4.9517, 4.9514, 4.9517]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): 0.19343768060207367 
model_pd.l_d.mean(): -19.464513778686523 
model_pd.lagr.mean(): -19.271076202392578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5052], device='cuda:0')), ('power', tensor([-20.3518], device='cuda:0'))])
epoch£º573	 i:0 	 global-step:11460	 l-p:0.19343768060207367
epoch£º573	 i:1 	 global-step:11461	 l-p:0.1679431051015854
epoch£º573	 i:2 	 global-step:11462	 l-p:0.10716937482357025
epoch£º573	 i:3 	 global-step:11463	 l-p:0.10583163797855377
epoch£º573	 i:4 	 global-step:11464	 l-p:0.11410343647003174
epoch£º573	 i:5 	 global-step:11465	 l-p:0.0840657576918602
epoch£º573	 i:6 	 global-step:11466	 l-p:0.04090006649494171
epoch£º573	 i:7 	 global-step:11467	 l-p:0.13004542887210846
epoch£º573	 i:8 	 global-step:11468	 l-p:0.17886729538440704
epoch£º573	 i:9 	 global-step:11469	 l-p:0.107411228120327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1235, 4.9754, 4.8668],
        [5.1235, 5.1821, 4.9819],
        [5.1235, 5.1236, 5.1235],
        [5.1235, 5.1196, 5.1233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.13385982811450958 
model_pd.l_d.mean(): -20.165109634399414 
model_pd.lagr.mean(): -20.03125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4460], device='cuda:0')), ('power', tensor([-21.0041], device='cuda:0'))])
epoch£º574	 i:0 	 global-step:11480	 l-p:0.13385982811450958
epoch£º574	 i:1 	 global-step:11481	 l-p:0.1347019225358963
epoch£º574	 i:2 	 global-step:11482	 l-p:-0.018372897058725357
epoch£º574	 i:3 	 global-step:11483	 l-p:0.09288040548563004
epoch£º574	 i:4 	 global-step:11484	 l-p:0.06783156841993332
epoch£º574	 i:5 	 global-step:11485	 l-p:0.1465759575366974
epoch£º574	 i:6 	 global-step:11486	 l-p:0.1162726953625679
epoch£º574	 i:7 	 global-step:11487	 l-p:0.16060443222522736
epoch£º574	 i:8 	 global-step:11488	 l-p:0.14283223450183868
epoch£º574	 i:9 	 global-step:11489	 l-p:0.11212127655744553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1882, 5.1763, 5.1868],
        [5.1882, 5.0578, 5.0496],
        [5.1882, 5.1882, 5.1882],
        [5.1882, 5.1748, 5.1865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): 0.08004724234342575 
model_pd.l_d.mean(): -20.168643951416016 
model_pd.lagr.mean(): -20.08859634399414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-21.0263], device='cuda:0'))])
epoch£º575	 i:0 	 global-step:11500	 l-p:0.08004724234342575
epoch£º575	 i:1 	 global-step:11501	 l-p:0.08221843838691711
epoch£º575	 i:2 	 global-step:11502	 l-p:0.06587059050798416
epoch£º575	 i:3 	 global-step:11503	 l-p:0.11000917106866837
epoch£º575	 i:4 	 global-step:11504	 l-p:0.14664830267429352
epoch£º575	 i:5 	 global-step:11505	 l-p:0.12276013940572739
epoch£º575	 i:6 	 global-step:11506	 l-p:0.12084004282951355
epoch£º575	 i:7 	 global-step:11507	 l-p:0.14694316685199738
epoch£º575	 i:8 	 global-step:11508	 l-p:0.2039777934551239
epoch£º575	 i:9 	 global-step:11509	 l-p:0.1354132443666458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2323, 5.2320, 5.2323],
        [5.2323, 5.2288, 5.2321],
        [5.2323, 5.7613, 5.8271],
        [5.2323, 5.1811, 4.9789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): -0.03908485174179077 
model_pd.l_d.mean(): -20.11301612854004 
model_pd.lagr.mean(): -20.152101516723633 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4391], device='cuda:0')), ('power', tensor([-20.9439], device='cuda:0'))])
epoch£º576	 i:0 	 global-step:11520	 l-p:-0.03908485174179077
epoch£º576	 i:1 	 global-step:11521	 l-p:0.13144369423389435
epoch£º576	 i:2 	 global-step:11522	 l-p:0.1274162232875824
epoch£º576	 i:3 	 global-step:11523	 l-p:0.11299597471952438
epoch£º576	 i:4 	 global-step:11524	 l-p:0.1175486147403717
epoch£º576	 i:5 	 global-step:11525	 l-p:0.12248945981264114
epoch£º576	 i:6 	 global-step:11526	 l-p:0.09330032020807266
epoch£º576	 i:7 	 global-step:11527	 l-p:0.13700126111507416
epoch£º576	 i:8 	 global-step:11528	 l-p:0.7072522044181824
epoch£º576	 i:9 	 global-step:11529	 l-p:0.13865898549556732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2016, 5.1047, 5.1335],
        [5.2016, 5.2016, 5.2016],
        [5.2016, 5.2975, 5.1090],
        [5.2016, 5.2016, 5.2016]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): 0.00634267320856452 
model_pd.l_d.mean(): -20.48404312133789 
model_pd.lagr.mean(): -20.47770118713379 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4104], device='cuda:0')), ('power', tensor([-21.2922], device='cuda:0'))])
epoch£º577	 i:0 	 global-step:11540	 l-p:0.00634267320856452
epoch£º577	 i:1 	 global-step:11541	 l-p:0.14622704684734344
epoch£º577	 i:2 	 global-step:11542	 l-p:0.12231279164552689
epoch£º577	 i:3 	 global-step:11543	 l-p:0.1110014021396637
epoch£º577	 i:4 	 global-step:11544	 l-p:0.2335130274295807
epoch£º577	 i:5 	 global-step:11545	 l-p:0.13060680031776428
epoch£º577	 i:6 	 global-step:11546	 l-p:0.08123262971639633
epoch£º577	 i:7 	 global-step:11547	 l-p:0.11994751542806625
epoch£º577	 i:8 	 global-step:11548	 l-p:0.13971546292304993
epoch£º577	 i:9 	 global-step:11549	 l-p:0.19499702751636505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2183, 5.2183, 5.2183],
        [5.2183, 5.2946, 5.0998],
        [5.2183, 5.0789, 5.0449],
        [5.2183, 5.0912, 4.9421]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.23606637120246887 
model_pd.l_d.mean(): -20.326923370361328 
model_pd.lagr.mean(): -20.090856552124023 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4111], device='cuda:0')), ('power', tensor([-21.1328], device='cuda:0'))])
epoch£º578	 i:0 	 global-step:11560	 l-p:0.23606637120246887
epoch£º578	 i:1 	 global-step:11561	 l-p:0.05460579693317413
epoch£º578	 i:2 	 global-step:11562	 l-p:0.12133964151144028
epoch£º578	 i:3 	 global-step:11563	 l-p:0.10187115520238876
epoch£º578	 i:4 	 global-step:11564	 l-p:0.16449522972106934
epoch£º578	 i:5 	 global-step:11565	 l-p:0.18375897407531738
epoch£º578	 i:6 	 global-step:11566	 l-p:0.11783965677022934
epoch£º578	 i:7 	 global-step:11567	 l-p:0.11511685699224472
epoch£º578	 i:8 	 global-step:11568	 l-p:0.12309872359037399
epoch£º578	 i:9 	 global-step:11569	 l-p:0.13843902945518494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3836, 6.0506, 6.2133],
        [5.3836, 5.3145, 5.3474],
        [5.3836, 5.2693, 5.1309],
        [5.3836, 5.3328, 5.3638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.12021932005882263 
model_pd.l_d.mean(): -20.190860748291016 
model_pd.lagr.mean(): -20.070640563964844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3768], device='cuda:0')), ('power', tensor([-20.9587], device='cuda:0'))])
epoch£º579	 i:0 	 global-step:11580	 l-p:0.12021932005882263
epoch£º579	 i:1 	 global-step:11581	 l-p:0.15033084154129028
epoch£º579	 i:2 	 global-step:11582	 l-p:0.10917119681835175
epoch£º579	 i:3 	 global-step:11583	 l-p:0.14546611905097961
epoch£º579	 i:4 	 global-step:11584	 l-p:0.19834277033805847
epoch£º579	 i:5 	 global-step:11585	 l-p:0.13004030287265778
epoch£º579	 i:6 	 global-step:11586	 l-p:0.14801940321922302
epoch£º579	 i:7 	 global-step:11587	 l-p:0.10067497193813324
epoch£º579	 i:8 	 global-step:11588	 l-p:0.13325585424900055
epoch£º579	 i:9 	 global-step:11589	 l-p:0.16219615936279297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2318, 5.2117, 5.2283],
        [5.2318, 5.2261, 5.2314],
        [5.2318, 5.1872, 4.9810],
        [5.2318, 5.2062, 5.2264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.05320661887526512 
model_pd.l_d.mean(): -20.72937774658203 
model_pd.lagr.mean(): -20.676170349121094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3580], device='cuda:0')), ('power', tensor([-21.4878], device='cuda:0'))])
epoch£º580	 i:0 	 global-step:11600	 l-p:0.05320661887526512
epoch£º580	 i:1 	 global-step:11601	 l-p:0.13274922966957092
epoch£º580	 i:2 	 global-step:11602	 l-p:0.1251242607831955
epoch£º580	 i:3 	 global-step:11603	 l-p:0.08537919819355011
epoch£º580	 i:4 	 global-step:11604	 l-p:0.5112137794494629
epoch£º580	 i:5 	 global-step:11605	 l-p:0.15218345820903778
epoch£º580	 i:6 	 global-step:11606	 l-p:0.15224990248680115
epoch£º580	 i:7 	 global-step:11607	 l-p:0.24062518775463104
epoch£º580	 i:8 	 global-step:11608	 l-p:0.12242783606052399
epoch£º580	 i:9 	 global-step:11609	 l-p:0.11962608247995377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1181, 5.1181, 5.1181],
        [5.1181, 5.3235, 5.1820],
        [5.1181, 4.9693, 4.8405],
        [5.1181, 5.0228, 5.0558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.15583032369613647 
model_pd.l_d.mean(): -20.109222412109375 
model_pd.lagr.mean(): -19.953392028808594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4791], device='cuda:0')), ('power', tensor([-20.9815], device='cuda:0'))])
epoch£º581	 i:0 	 global-step:11620	 l-p:0.15583032369613647
epoch£º581	 i:1 	 global-step:11621	 l-p:0.09528392553329468
epoch£º581	 i:2 	 global-step:11622	 l-p:0.12399373948574066
epoch£º581	 i:3 	 global-step:11623	 l-p:0.13220080733299255
epoch£º581	 i:4 	 global-step:11624	 l-p:0.12809331715106964
epoch£º581	 i:5 	 global-step:11625	 l-p:0.09916664659976959
epoch£º581	 i:6 	 global-step:11626	 l-p:0.09778565913438797
epoch£º581	 i:7 	 global-step:11627	 l-p:0.18961933255195618
epoch£º581	 i:8 	 global-step:11628	 l-p:0.14899703860282898
epoch£º581	 i:9 	 global-step:11629	 l-p:0.14614595472812653
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9992, 4.9472, 4.9809],
        [4.9992, 5.4804, 5.5216],
        [4.9992, 4.9988, 4.9992],
        [4.9992, 4.9881, 4.9979]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.16103015840053558 
model_pd.l_d.mean(): -18.357324600219727 
model_pd.lagr.mean(): -18.1962947845459 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5815], device='cuda:0')), ('power', tensor([-19.3029], device='cuda:0'))])
epoch£º582	 i:0 	 global-step:11640	 l-p:0.16103015840053558
epoch£º582	 i:1 	 global-step:11641	 l-p:0.1618720144033432
epoch£º582	 i:2 	 global-step:11642	 l-p:0.12545977532863617
epoch£º582	 i:3 	 global-step:11643	 l-p:0.14167875051498413
epoch£º582	 i:4 	 global-step:11644	 l-p:0.15453331172466278
epoch£º582	 i:5 	 global-step:11645	 l-p:0.15368615090847015
epoch£º582	 i:6 	 global-step:11646	 l-p:0.10877037048339844
epoch£º582	 i:7 	 global-step:11647	 l-p:0.12211782485246658
epoch£º582	 i:8 	 global-step:11648	 l-p:0.11923688650131226
epoch£º582	 i:9 	 global-step:11649	 l-p:0.1391412615776062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0351, 5.0002, 5.0262],
        [5.0351, 5.1273, 4.9335],
        [5.0351, 5.1490, 4.9639],
        [5.0351, 4.8877, 4.7251]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.13654379546642303 
model_pd.l_d.mean(): -20.29815673828125 
model_pd.lagr.mean(): -20.16161346435547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4927], device='cuda:0')), ('power', tensor([-21.1881], device='cuda:0'))])
epoch£º583	 i:0 	 global-step:11660	 l-p:0.13654379546642303
epoch£º583	 i:1 	 global-step:11661	 l-p:0.14580368995666504
epoch£º583	 i:2 	 global-step:11662	 l-p:0.13252104818820953
epoch£º583	 i:3 	 global-step:11663	 l-p:0.10880973935127258
epoch£º583	 i:4 	 global-step:11664	 l-p:0.20641030371189117
epoch£º583	 i:5 	 global-step:11665	 l-p:0.1757703572511673
epoch£º583	 i:6 	 global-step:11666	 l-p:0.0932340994477272
epoch£º583	 i:7 	 global-step:11667	 l-p:0.12908941507339478
epoch£º583	 i:8 	 global-step:11668	 l-p:0.12700025737285614
epoch£º583	 i:9 	 global-step:11669	 l-p:0.18729552626609802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0116, 4.9902, 5.0079],
        [5.0116, 5.0116, 5.0116],
        [5.0116, 4.9554, 4.9905],
        [5.0116, 4.9444, 4.9820]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): 0.22355018556118011 
model_pd.l_d.mean(): -20.53264808654785 
model_pd.lagr.mean(): -20.309097290039062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4626], device='cuda:0')), ('power', tensor([-21.3958], device='cuda:0'))])
epoch£º584	 i:0 	 global-step:11680	 l-p:0.22355018556118011
epoch£º584	 i:1 	 global-step:11681	 l-p:0.12039151787757874
epoch£º584	 i:2 	 global-step:11682	 l-p:0.1467389613389969
epoch£º584	 i:3 	 global-step:11683	 l-p:0.13947458565235138
epoch£º584	 i:4 	 global-step:11684	 l-p:0.11481573432683945
epoch£º584	 i:5 	 global-step:11685	 l-p:0.038487546145915985
epoch£º584	 i:6 	 global-step:11686	 l-p:0.1410796344280243
epoch£º584	 i:7 	 global-step:11687	 l-p:0.18182039260864258
epoch£º584	 i:8 	 global-step:11688	 l-p:0.08228383958339691
epoch£º584	 i:9 	 global-step:11689	 l-p:0.10692492127418518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2158, 5.1429, 5.1788],
        [5.2158, 5.2085, 5.2152],
        [5.2158, 5.2142, 5.2157],
        [5.2158, 5.2063, 5.2148]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.2627411186695099 
model_pd.l_d.mean(): -20.263774871826172 
model_pd.lagr.mean(): -20.001033782958984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4072], device='cuda:0')), ('power', tensor([-21.0645], device='cuda:0'))])
epoch£º585	 i:0 	 global-step:11700	 l-p:0.2627411186695099
epoch£º585	 i:1 	 global-step:11701	 l-p:0.12084869295358658
epoch£º585	 i:2 	 global-step:11702	 l-p:0.18592551350593567
epoch£º585	 i:3 	 global-step:11703	 l-p:0.3381461203098297
epoch£º585	 i:4 	 global-step:11704	 l-p:0.10664433985948563
epoch£º585	 i:5 	 global-step:11705	 l-p:0.1178748831152916
epoch£º585	 i:6 	 global-step:11706	 l-p:0.13172748684883118
epoch£º585	 i:7 	 global-step:11707	 l-p:0.11207372695207596
epoch£º585	 i:8 	 global-step:11708	 l-p:0.12742409110069275
epoch£º585	 i:9 	 global-step:11709	 l-p:0.570923388004303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2766, 5.2760, 5.2766],
        [5.2766, 5.8475, 5.9396],
        [5.2766, 5.1517, 5.1533],
        [5.2766, 5.2457, 5.2691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.12808381021022797 
model_pd.l_d.mean(): -20.027624130249023 
model_pd.lagr.mean(): -19.899539947509766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4234], device='cuda:0')), ('power', tensor([-20.8407], device='cuda:0'))])
epoch£º586	 i:0 	 global-step:11720	 l-p:0.12808381021022797
epoch£º586	 i:1 	 global-step:11721	 l-p:0.13739082217216492
epoch£º586	 i:2 	 global-step:11722	 l-p:0.022387299686670303
epoch£º586	 i:3 	 global-step:11723	 l-p:0.15201400220394135
epoch£º586	 i:4 	 global-step:11724	 l-p:0.060008104890584946
epoch£º586	 i:5 	 global-step:11725	 l-p:0.1107291653752327
epoch£º586	 i:6 	 global-step:11726	 l-p:-1.6797341108322144
epoch£º586	 i:7 	 global-step:11727	 l-p:0.13275259733200073
epoch£º586	 i:8 	 global-step:11728	 l-p:0.11436769366264343
epoch£º586	 i:9 	 global-step:11729	 l-p:0.13952739536762238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1305, 4.9784, 4.8467],
        [5.1305, 5.2081, 5.0084],
        [5.1305, 5.1468, 4.9317],
        [5.1305, 5.0765, 5.1104]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.12324163317680359 
model_pd.l_d.mean(): -20.011890411376953 
model_pd.lagr.mean(): -19.888648986816406 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4850], device='cuda:0')), ('power', tensor([-20.8884], device='cuda:0'))])
epoch£º587	 i:0 	 global-step:11740	 l-p:0.12324163317680359
epoch£º587	 i:1 	 global-step:11741	 l-p:0.055675406008958817
epoch£º587	 i:2 	 global-step:11742	 l-p:-0.08120185881853104
epoch£º587	 i:3 	 global-step:11743	 l-p:0.12913371622562408
epoch£º587	 i:4 	 global-step:11744	 l-p:0.1328520029783249
epoch£º587	 i:5 	 global-step:11745	 l-p:0.19456039369106293
epoch£º587	 i:6 	 global-step:11746	 l-p:0.17043930292129517
epoch£º587	 i:7 	 global-step:11747	 l-p:0.17317496240139008
epoch£º587	 i:8 	 global-step:11748	 l-p:0.1266455501317978
epoch£º587	 i:9 	 global-step:11749	 l-p:0.3352818191051483
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9671, 4.9671, 4.9671],
        [4.9671, 4.8798, 4.9192],
        [4.9671, 4.7972, 4.7377],
        [4.9671, 4.8922, 4.9316]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.13510897755622864 
model_pd.l_d.mean(): -20.33121681213379 
model_pd.lagr.mean(): -20.196107864379883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4996], device='cuda:0')), ('power', tensor([-21.2288], device='cuda:0'))])
epoch£º588	 i:0 	 global-step:11760	 l-p:0.13510897755622864
epoch£º588	 i:1 	 global-step:11761	 l-p:0.11968600004911423
epoch£º588	 i:2 	 global-step:11762	 l-p:0.11806605756282806
epoch£º588	 i:3 	 global-step:11763	 l-p:0.12395749986171722
epoch£º588	 i:4 	 global-step:11764	 l-p:0.06644352525472641
epoch£º588	 i:5 	 global-step:11765	 l-p:0.17326900362968445
epoch£º588	 i:6 	 global-step:11766	 l-p:0.021827200427651405
epoch£º588	 i:7 	 global-step:11767	 l-p:0.09456583857536316
epoch£º588	 i:8 	 global-step:11768	 l-p:0.15424251556396484
epoch£º588	 i:9 	 global-step:11769	 l-p:0.07437073439359665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8545, 4.8393, 4.8525],
        [4.8545, 4.8536, 4.8545],
        [4.8545, 4.8094, 4.8411],
        [4.8545, 4.8545, 4.8545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 0.09625913202762604 
model_pd.l_d.mean(): -20.108600616455078 
model_pd.lagr.mean(): -20.012340545654297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5617], device='cuda:0')), ('power', tensor([-21.0664], device='cuda:0'))])
epoch£º589	 i:0 	 global-step:11780	 l-p:0.09625913202762604
epoch£º589	 i:1 	 global-step:11781	 l-p:0.13418738543987274
epoch£º589	 i:2 	 global-step:11782	 l-p:0.18493297696113586
epoch£º589	 i:3 	 global-step:11783	 l-p:0.12009385973215103
epoch£º589	 i:4 	 global-step:11784	 l-p:0.06526932865381241
epoch£º589	 i:5 	 global-step:11785	 l-p:0.24176697432994843
epoch£º589	 i:6 	 global-step:11786	 l-p:0.17006011307239532
epoch£º589	 i:7 	 global-step:11787	 l-p:0.14271976053714752
epoch£º589	 i:8 	 global-step:11788	 l-p:0.1548944115638733
epoch£º589	 i:9 	 global-step:11789	 l-p:0.13313619792461395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0549, 5.0205, 5.0463],
        [5.0549, 5.0138, 4.7892],
        [5.0549, 4.9981, 5.0334],
        [5.0549, 5.0449, 5.0539]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.16218338906764984 
model_pd.l_d.mean(): -20.199769973754883 
model_pd.lagr.mean(): -20.037586212158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4855], device='cuda:0')), ('power', tensor([-21.0804], device='cuda:0'))])
epoch£º590	 i:0 	 global-step:11800	 l-p:0.16218338906764984
epoch£º590	 i:1 	 global-step:11801	 l-p:0.13546232879161835
epoch£º590	 i:2 	 global-step:11802	 l-p:0.12665900588035583
epoch£º590	 i:3 	 global-step:11803	 l-p:0.13426201045513153
epoch£º590	 i:4 	 global-step:11804	 l-p:0.14095701277256012
epoch£º590	 i:5 	 global-step:11805	 l-p:0.13317088782787323
epoch£º590	 i:6 	 global-step:11806	 l-p:0.12831681966781616
epoch£º590	 i:7 	 global-step:11807	 l-p:0.08771687000989914
epoch£º590	 i:8 	 global-step:11808	 l-p:0.01887459307909012
epoch£º590	 i:9 	 global-step:11809	 l-p:0.1400875598192215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0544, 5.1259, 4.9217],
        [5.0544, 4.8907, 4.7620],
        [5.0544, 5.0543, 5.0544],
        [5.0544, 5.0540, 5.0544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.12509439885616302 
model_pd.l_d.mean(): -18.690555572509766 
model_pd.lagr.mean(): -18.565462112426758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5326], device='cuda:0')), ('power', tensor([-19.5917], device='cuda:0'))])
epoch£º591	 i:0 	 global-step:11820	 l-p:0.12509439885616302
epoch£º591	 i:1 	 global-step:11821	 l-p:0.17681284248828888
epoch£º591	 i:2 	 global-step:11822	 l-p:0.12094876915216446
epoch£º591	 i:3 	 global-step:11823	 l-p:0.12562938034534454
epoch£º591	 i:4 	 global-step:11824	 l-p:0.07393191009759903
epoch£º591	 i:5 	 global-step:11825	 l-p:0.11530065536499023
epoch£º591	 i:6 	 global-step:11826	 l-p:0.1034415140748024
epoch£º591	 i:7 	 global-step:11827	 l-p:0.1803210824728012
epoch£º591	 i:8 	 global-step:11828	 l-p:0.0952264666557312
epoch£º591	 i:9 	 global-step:11829	 l-p:0.13341832160949707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9731,  0.9643,  1.0000,  0.9556,
          1.0000,  0.9910, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228]], device='cuda:0')
 pt:tensor([[5.1081, 4.9529, 4.9187],
        [5.1081, 4.9528, 4.9184],
        [5.1081, 5.7045, 5.8236],
        [5.1081, 4.9466, 4.8852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.12723815441131592 
model_pd.l_d.mean(): -20.00261688232422 
model_pd.lagr.mean(): -19.87537956237793 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4480], device='cuda:0')), ('power', tensor([-20.8406], device='cuda:0'))])
epoch£º592	 i:0 	 global-step:11840	 l-p:0.12723815441131592
epoch£º592	 i:1 	 global-step:11841	 l-p:0.08515304327011108
epoch£º592	 i:2 	 global-step:11842	 l-p:-0.1270589828491211
epoch£º592	 i:3 	 global-step:11843	 l-p:0.14328397810459137
epoch£º592	 i:4 	 global-step:11844	 l-p:0.0359543077647686
epoch£º592	 i:5 	 global-step:11845	 l-p:0.12608493864536285
epoch£º592	 i:6 	 global-step:11846	 l-p:0.13945148885250092
epoch£º592	 i:7 	 global-step:11847	 l-p:0.13109101355075836
epoch£º592	 i:8 	 global-step:11848	 l-p:0.1735483855009079
epoch£º592	 i:9 	 global-step:11849	 l-p:0.12191744893789291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0787, 5.0077, 5.0461],
        [5.0787, 4.9700, 5.0019],
        [5.0787, 5.0787, 5.0787],
        [5.0787, 5.0787, 5.0787]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): 0.027401546016335487 
model_pd.l_d.mean(): -20.609312057495117 
model_pd.lagr.mean(): -20.581911087036133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4304], device='cuda:0')), ('power', tensor([-21.4405], device='cuda:0'))])
epoch£º593	 i:0 	 global-step:11860	 l-p:0.027401546016335487
epoch£º593	 i:1 	 global-step:11861	 l-p:0.10793212801218033
epoch£º593	 i:2 	 global-step:11862	 l-p:0.10933607071638107
epoch£º593	 i:3 	 global-step:11863	 l-p:0.18735668063163757
epoch£º593	 i:4 	 global-step:11864	 l-p:0.14097407460212708
epoch£º593	 i:5 	 global-step:11865	 l-p:0.1333690583705902
epoch£º593	 i:6 	 global-step:11866	 l-p:0.1299104541540146
epoch£º593	 i:7 	 global-step:11867	 l-p:0.17870303988456726
epoch£º593	 i:8 	 global-step:11868	 l-p:0.1519545167684555
epoch£º593	 i:9 	 global-step:11869	 l-p:0.10093424469232559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0709, 4.9692, 5.0046],
        [5.0709, 5.0708, 5.0709],
        [5.0709, 5.0097, 5.0464],
        [5.0709, 5.3166, 5.1944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.10457584261894226 
model_pd.l_d.mean(): -20.170255661010742 
model_pd.lagr.mean(): -20.0656795501709 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5029], device='cuda:0')), ('power', tensor([-21.0682], device='cuda:0'))])
epoch£º594	 i:0 	 global-step:11880	 l-p:0.10457584261894226
epoch£º594	 i:1 	 global-step:11881	 l-p:0.13019801676273346
epoch£º594	 i:2 	 global-step:11882	 l-p:0.113743856549263
epoch£º594	 i:3 	 global-step:11883	 l-p:0.1314549595117569
epoch£º594	 i:4 	 global-step:11884	 l-p:0.1076795756816864
epoch£º594	 i:5 	 global-step:11885	 l-p:-0.0013063334627076983
epoch£º594	 i:6 	 global-step:11886	 l-p:0.1374647170305252
epoch£º594	 i:7 	 global-step:11887	 l-p:0.174948051571846
epoch£º594	 i:8 	 global-step:11888	 l-p:0.12058742344379425
epoch£º594	 i:9 	 global-step:11889	 l-p:0.1370413601398468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0841, 5.3368, 5.2180],
        [5.0841, 4.9250, 4.8911],
        [5.0841, 5.0841, 5.0841],
        [5.0841, 5.0825, 5.0841]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.14270001649856567 
model_pd.l_d.mean(): -20.450593948364258 
model_pd.lagr.mean(): -20.307893753051758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4428], device='cuda:0')), ('power', tensor([-21.2917], device='cuda:0'))])
epoch£º595	 i:0 	 global-step:11900	 l-p:0.14270001649856567
epoch£º595	 i:1 	 global-step:11901	 l-p:0.11723005771636963
epoch£º595	 i:2 	 global-step:11902	 l-p:0.15864364802837372
epoch£º595	 i:3 	 global-step:11903	 l-p:0.1685076802968979
epoch£º595	 i:4 	 global-step:11904	 l-p:0.12082649022340775
epoch£º595	 i:5 	 global-step:11905	 l-p:0.12552489340305328
epoch£º595	 i:6 	 global-step:11906	 l-p:0.12309091538190842
epoch£º595	 i:7 	 global-step:11907	 l-p:0.11292053014039993
epoch£º595	 i:8 	 global-step:11908	 l-p:0.12000612169504166
epoch£º595	 i:9 	 global-step:11909	 l-p:0.11720337718725204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9988, 4.9612, 4.9891],
        [4.9988, 4.9980, 4.9988],
        [4.9988, 4.8883, 4.6672],
        [4.9988, 4.9988, 4.9988]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): 0.11266769468784332 
model_pd.l_d.mean(): -20.40874481201172 
model_pd.lagr.mean(): -20.296077728271484 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4671], device='cuda:0')), ('power', tensor([-21.2742], device='cuda:0'))])
epoch£º596	 i:0 	 global-step:11920	 l-p:0.11266769468784332
epoch£º596	 i:1 	 global-step:11921	 l-p:0.14433087408542633
epoch£º596	 i:2 	 global-step:11922	 l-p:0.15289124846458435
epoch£º596	 i:3 	 global-step:11923	 l-p:0.526833176612854
epoch£º596	 i:4 	 global-step:11924	 l-p:0.1225847601890564
epoch£º596	 i:5 	 global-step:11925	 l-p:6.83974027633667
epoch£º596	 i:6 	 global-step:11926	 l-p:0.3942627012729645
epoch£º596	 i:7 	 global-step:11927	 l-p:0.5310395956039429
epoch£º596	 i:8 	 global-step:11928	 l-p:0.07594707608222961
epoch£º596	 i:9 	 global-step:11929	 l-p:0.13151815533638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9458, 4.7907, 4.7887],
        [4.9458, 4.9206, 4.9411],
        [4.9458, 4.9458, 4.9458],
        [4.9458, 4.9442, 4.9458]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): 0.14847883582115173 
model_pd.l_d.mean(): -20.318803787231445 
model_pd.lagr.mean(): -20.170324325561523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4919], device='cuda:0')), ('power', tensor([-21.2082], device='cuda:0'))])
epoch£º597	 i:0 	 global-step:11940	 l-p:0.14847883582115173
epoch£º597	 i:1 	 global-step:11941	 l-p:0.1435750126838684
epoch£º597	 i:2 	 global-step:11942	 l-p:0.13201385736465454
epoch£º597	 i:3 	 global-step:11943	 l-p:0.028928130865097046
epoch£º597	 i:4 	 global-step:11944	 l-p:0.25437676906585693
epoch£º597	 i:5 	 global-step:11945	 l-p:-6.129584789276123
epoch£º597	 i:6 	 global-step:11946	 l-p:0.09054113179445267
epoch£º597	 i:7 	 global-step:11947	 l-p:0.18172508478164673
epoch£º597	 i:8 	 global-step:11948	 l-p:0.1342371255159378
epoch£º597	 i:9 	 global-step:11949	 l-p:0.15126799046993256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0227, 5.0219, 5.0226],
        [5.0227, 5.1385, 4.9482],
        [5.0227, 4.8910, 4.9117],
        [5.0227, 5.4376, 5.4249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.11600995808839798 
model_pd.l_d.mean(): -19.144832611083984 
model_pd.lagr.mean(): -19.02882194519043 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5406], device='cuda:0')), ('power', tensor([-20.0627], device='cuda:0'))])
epoch£º598	 i:0 	 global-step:11960	 l-p:0.11600995808839798
epoch£º598	 i:1 	 global-step:11961	 l-p:0.12114915251731873
epoch£º598	 i:2 	 global-step:11962	 l-p:0.10983788222074509
epoch£º598	 i:3 	 global-step:11963	 l-p:0.13130725920200348
epoch£º598	 i:4 	 global-step:11964	 l-p:0.11588062345981598
epoch£º598	 i:5 	 global-step:11965	 l-p:0.11905112862586975
epoch£º598	 i:6 	 global-step:11966	 l-p:0.1441756933927536
epoch£º598	 i:7 	 global-step:11967	 l-p:0.1385190635919571
epoch£º598	 i:8 	 global-step:11968	 l-p:0.15528647601604462
epoch£º598	 i:9 	 global-step:11969	 l-p:0.13155019283294678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0832, 5.0832, 5.0832],
        [5.0832, 4.9477, 4.9616],
        [5.0832, 5.3297, 5.2056],
        [5.0832, 5.0794, 5.0830]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): 0.14147982001304626 
model_pd.l_d.mean(): -19.68332862854004 
model_pd.lagr.mean(): -19.54184913635254 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5491], device='cuda:0')), ('power', tensor([-20.6201], device='cuda:0'))])
epoch£º599	 i:0 	 global-step:11980	 l-p:0.14147982001304626
epoch£º599	 i:1 	 global-step:11981	 l-p:0.15947777032852173
epoch£º599	 i:2 	 global-step:11982	 l-p:0.12245319038629532
epoch£º599	 i:3 	 global-step:11983	 l-p:0.1812121868133545
epoch£º599	 i:4 	 global-step:11984	 l-p:0.099954754114151
epoch£º599	 i:5 	 global-step:11985	 l-p:0.0824093148112297
epoch£º599	 i:6 	 global-step:11986	 l-p:0.13794414699077606
epoch£º599	 i:7 	 global-step:11987	 l-p:0.13677182793617249
epoch£º599	 i:8 	 global-step:11988	 l-p:0.12821358442306519
epoch£º599	 i:9 	 global-step:11989	 l-p:0.12495078891515732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0038, 5.0434, 4.8225],
        [5.0038, 5.0038, 5.0038],
        [5.0038, 5.0038, 5.0038],
        [5.0038, 5.0038, 5.0038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.1345507651567459 
model_pd.l_d.mean(): -19.49129295349121 
model_pd.lagr.mean(): -19.35674285888672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4912], device='cuda:0')), ('power', tensor([-20.3645], device='cuda:0'))])
epoch£º600	 i:0 	 global-step:12000	 l-p:0.1345507651567459
epoch£º600	 i:1 	 global-step:12001	 l-p:0.1556471735239029
epoch£º600	 i:2 	 global-step:12002	 l-p:0.2548123598098755
epoch£º600	 i:3 	 global-step:12003	 l-p:0.10098434239625931
epoch£º600	 i:4 	 global-step:12004	 l-p:0.11984990537166595
epoch£º600	 i:5 	 global-step:12005	 l-p:0.1480618417263031
epoch£º600	 i:6 	 global-step:12006	 l-p:0.25496646761894226
epoch£º600	 i:7 	 global-step:12007	 l-p:0.12294509261846542
epoch£º600	 i:8 	 global-step:12008	 l-p:0.13078083097934723
epoch£º600	 i:9 	 global-step:12009	 l-p:0.13167396187782288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0250, 4.9842, 4.7489],
        [5.0250, 4.8503, 4.7067],
        [5.0250, 5.0044, 5.0216],
        [5.0250, 5.0243, 5.0249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): 0.11877038329839706 
model_pd.l_d.mean(): -20.497604370117188 
model_pd.lagr.mean(): -20.378833770751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4422], device='cuda:0')), ('power', tensor([-21.3389], device='cuda:0'))])
epoch£º601	 i:0 	 global-step:12020	 l-p:0.11877038329839706
epoch£º601	 i:1 	 global-step:12021	 l-p:0.12705913186073303
epoch£º601	 i:2 	 global-step:12022	 l-p:0.19461968541145325
epoch£º601	 i:3 	 global-step:12023	 l-p:0.15508809685707092
epoch£º601	 i:4 	 global-step:12024	 l-p:0.13107830286026
epoch£º601	 i:5 	 global-step:12025	 l-p:0.11086232215166092
epoch£º601	 i:6 	 global-step:12026	 l-p:0.2015487551689148
epoch£º601	 i:7 	 global-step:12027	 l-p:0.10773542523384094
epoch£º601	 i:8 	 global-step:12028	 l-p:0.10363434255123138
epoch£º601	 i:9 	 global-step:12029	 l-p:0.12894119322299957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0499, 5.0499, 5.0499],
        [5.0499, 5.0790, 4.8556],
        [5.0499, 5.0257, 5.0454],
        [5.0499, 5.0353, 4.8031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): 0.13337646424770355 
model_pd.l_d.mean(): -20.24802589416504 
model_pd.lagr.mean(): -20.114648818969727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4806], device='cuda:0')), ('power', tensor([-21.1244], device='cuda:0'))])
epoch£º602	 i:0 	 global-step:12040	 l-p:0.13337646424770355
epoch£º602	 i:1 	 global-step:12041	 l-p:0.07094091176986694
epoch£º602	 i:2 	 global-step:12042	 l-p:0.11816461384296417
epoch£º602	 i:3 	 global-step:12043	 l-p:0.11205854266881943
epoch£º602	 i:4 	 global-step:12044	 l-p:0.10829374194145203
epoch£º602	 i:5 	 global-step:12045	 l-p:0.19474270939826965
epoch£º602	 i:6 	 global-step:12046	 l-p:0.12483804672956467
epoch£º602	 i:7 	 global-step:12047	 l-p:0.09039539098739624
epoch£º602	 i:8 	 global-step:12048	 l-p:0.14878427982330322
epoch£º602	 i:9 	 global-step:12049	 l-p:0.16226086020469666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0958, 4.9764, 5.0051],
        [5.0958, 5.0849, 5.0946],
        [5.0958, 5.0957, 5.0958],
        [5.0958, 4.9840, 5.0170]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.09043501317501068 
model_pd.l_d.mean(): -20.5990047454834 
model_pd.lagr.mean(): -20.508569717407227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4159], device='cuda:0')), ('power', tensor([-21.4149], device='cuda:0'))])
epoch£º603	 i:0 	 global-step:12060	 l-p:0.09043501317501068
epoch£º603	 i:1 	 global-step:12061	 l-p:0.17008450627326965
epoch£º603	 i:2 	 global-step:12062	 l-p:0.13329392671585083
epoch£º603	 i:3 	 global-step:12063	 l-p:0.12809473276138306
epoch£º603	 i:4 	 global-step:12064	 l-p:0.09578617662191391
epoch£º603	 i:5 	 global-step:12065	 l-p:0.12969760596752167
epoch£º603	 i:6 	 global-step:12066	 l-p:0.14080029726028442
epoch£º603	 i:7 	 global-step:12067	 l-p:-0.10378289222717285
epoch£º603	 i:8 	 global-step:12068	 l-p:0.11497405916452408
epoch£º603	 i:9 	 global-step:12069	 l-p:0.12202339619398117
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1191, 5.4059, 5.3033],
        [5.1191, 5.1191, 5.1191],
        [5.1191, 5.0544, 5.0923],
        [5.1191, 4.9522, 4.9018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): -0.9950602650642395 
model_pd.l_d.mean(): -20.370994567871094 
model_pd.lagr.mean(): -21.36605453491211 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4413], device='cuda:0')), ('power', tensor([-21.2090], device='cuda:0'))])
epoch£º604	 i:0 	 global-step:12080	 l-p:-0.9950602650642395
epoch£º604	 i:1 	 global-step:12081	 l-p:0.15525375306606293
epoch£º604	 i:2 	 global-step:12082	 l-p:0.11202802509069443
epoch£º604	 i:3 	 global-step:12083	 l-p:0.09217434376478195
epoch£º604	 i:4 	 global-step:12084	 l-p:0.1296020895242691
epoch£º604	 i:5 	 global-step:12085	 l-p:0.11262346059083939
epoch£º604	 i:6 	 global-step:12086	 l-p:-0.23152461647987366
epoch£º604	 i:7 	 global-step:12087	 l-p:0.111549012362957
epoch£º604	 i:8 	 global-step:12088	 l-p:0.14104045927524567
epoch£º604	 i:9 	 global-step:12089	 l-p:0.11984607577323914
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1679, 5.1378, 4.9091],
        [5.1679, 5.0627, 5.0968],
        [5.1679, 5.1140, 5.1487],
        [5.1679, 5.1240, 5.1548]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.11103413254022598 
model_pd.l_d.mean(): -20.267900466918945 
model_pd.lagr.mean(): -20.1568660736084 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4317], device='cuda:0')), ('power', tensor([-21.0940], device='cuda:0'))])
epoch£º605	 i:0 	 global-step:12100	 l-p:0.11103413254022598
epoch£º605	 i:1 	 global-step:12101	 l-p:0.12233084440231323
epoch£º605	 i:2 	 global-step:12102	 l-p:0.13801658153533936
epoch£º605	 i:3 	 global-step:12103	 l-p:0.12770724296569824
epoch£º605	 i:4 	 global-step:12104	 l-p:0.07309434562921524
epoch£º605	 i:5 	 global-step:12105	 l-p:0.14318275451660156
epoch£º605	 i:6 	 global-step:12106	 l-p:0.11243864893913269
epoch£º605	 i:7 	 global-step:12107	 l-p:0.7565840482711792
epoch£º605	 i:8 	 global-step:12108	 l-p:0.12579534947872162
epoch£º605	 i:9 	 global-step:12109	 l-p:0.028117507696151733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1122, 5.1122, 5.1122],
        [5.1122, 5.3667, 5.2447],
        [5.1122, 4.9445, 4.8951],
        [5.1122, 5.1119, 5.1122]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.11539546400308609 
model_pd.l_d.mean(): -20.53140640258789 
model_pd.lagr.mean(): -20.416011810302734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4254], device='cuda:0')), ('power', tensor([-21.3560], device='cuda:0'))])
epoch£º606	 i:0 	 global-step:12120	 l-p:0.11539546400308609
epoch£º606	 i:1 	 global-step:12121	 l-p:0.14135980606079102
epoch£º606	 i:2 	 global-step:12122	 l-p:0.12914346158504486
epoch£º606	 i:3 	 global-step:12123	 l-p:0.09211114794015884
epoch£º606	 i:4 	 global-step:12124	 l-p:0.14401063323020935
epoch£º606	 i:5 	 global-step:12125	 l-p:0.07779882848262787
epoch£º606	 i:6 	 global-step:12126	 l-p:0.14361515641212463
epoch£º606	 i:7 	 global-step:12127	 l-p:0.1452367603778839
epoch£º606	 i:8 	 global-step:12128	 l-p:0.13480502367019653
epoch£º606	 i:9 	 global-step:12129	 l-p:0.08086684346199036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0635, 5.0541, 5.0626],
        [5.0635, 4.8869, 4.8133],
        [5.0635, 5.0621, 5.0634],
        [5.0635, 4.8968, 4.8641]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.14713376760482788 
model_pd.l_d.mean(): -20.320655822753906 
model_pd.lagr.mean(): -20.17352294921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4690], device='cuda:0')), ('power', tensor([-21.1864], device='cuda:0'))])
epoch£º607	 i:0 	 global-step:12140	 l-p:0.14713376760482788
epoch£º607	 i:1 	 global-step:12141	 l-p:0.13019801676273346
epoch£º607	 i:2 	 global-step:12142	 l-p:0.1087188720703125
epoch£º607	 i:3 	 global-step:12143	 l-p:0.1482226699590683
epoch£º607	 i:4 	 global-step:12144	 l-p:0.16019222140312195
epoch£º607	 i:5 	 global-step:12145	 l-p:0.11385919898748398
epoch£º607	 i:6 	 global-step:12146	 l-p:0.09523129463195801
epoch£º607	 i:7 	 global-step:12147	 l-p:0.12458894401788712
epoch£º607	 i:8 	 global-step:12148	 l-p:0.1367560476064682
epoch£º607	 i:9 	 global-step:12149	 l-p:0.17794924974441528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0310, 5.0295, 5.0309],
        [5.0310, 5.0253, 5.0306],
        [5.0310, 4.9675, 5.0060],
        [5.0310, 5.2367, 5.0890]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.182001993060112 
model_pd.l_d.mean(): -19.4339599609375 
model_pd.lagr.mean(): -19.2519588470459 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5541], device='cuda:0')), ('power', tensor([-20.3713], device='cuda:0'))])
epoch£º608	 i:0 	 global-step:12160	 l-p:0.182001993060112
epoch£º608	 i:1 	 global-step:12161	 l-p:0.14540483057498932
epoch£º608	 i:2 	 global-step:12162	 l-p:0.14222022891044617
epoch£º608	 i:3 	 global-step:12163	 l-p:0.1818997710943222
epoch£º608	 i:4 	 global-step:12164	 l-p:0.07377716153860092
epoch£º608	 i:5 	 global-step:12165	 l-p:0.11872044950723648
epoch£º608	 i:6 	 global-step:12166	 l-p:0.13499535620212555
epoch£º608	 i:7 	 global-step:12167	 l-p:0.11717895418405533
epoch£º608	 i:8 	 global-step:12168	 l-p:0.15568752586841583
epoch£º608	 i:9 	 global-step:12169	 l-p:0.1336880624294281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0449, 5.0449, 5.0449],
        [5.0449, 5.0273, 5.0423],
        [5.0449, 5.2538, 5.1073],
        [5.0449, 4.9332, 4.9688]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): 0.14422136545181274 
model_pd.l_d.mean(): -19.100723266601562 
model_pd.lagr.mean(): -18.956501007080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5641], device='cuda:0')), ('power', tensor([-20.0421], device='cuda:0'))])
epoch£º609	 i:0 	 global-step:12180	 l-p:0.14422136545181274
epoch£º609	 i:1 	 global-step:12181	 l-p:0.1367475837469101
epoch£º609	 i:2 	 global-step:12182	 l-p:0.1573254019021988
epoch£º609	 i:3 	 global-step:12183	 l-p:0.13239876925945282
epoch£º609	 i:4 	 global-step:12184	 l-p:0.11516258120536804
epoch£º609	 i:5 	 global-step:12185	 l-p:0.142751082777977
epoch£º609	 i:6 	 global-step:12186	 l-p:0.16941870748996735
epoch£º609	 i:7 	 global-step:12187	 l-p:0.13716237246990204
epoch£º609	 i:8 	 global-step:12188	 l-p:0.10142400860786438
epoch£º609	 i:9 	 global-step:12189	 l-p:0.10715565085411072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0465, 4.9347, 4.9704],
        [5.0465, 5.0462, 5.0465],
        [5.0465, 5.0465, 5.0465],
        [5.0465, 5.0122, 5.0384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.08127758651971817 
model_pd.l_d.mean(): -20.251739501953125 
model_pd.lagr.mean(): -20.170461654663086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4733], device='cuda:0')), ('power', tensor([-21.1206], device='cuda:0'))])
epoch£º610	 i:0 	 global-step:12200	 l-p:0.08127758651971817
epoch£º610	 i:1 	 global-step:12201	 l-p:0.1431540697813034
epoch£º610	 i:2 	 global-step:12202	 l-p:0.12847724556922913
epoch£º610	 i:3 	 global-step:12203	 l-p:0.12468362599611282
epoch£º610	 i:4 	 global-step:12204	 l-p:0.12856867909431458
epoch£º610	 i:5 	 global-step:12205	 l-p:0.1292954534292221
epoch£º610	 i:6 	 global-step:12206	 l-p:0.0450819730758667
epoch£º610	 i:7 	 global-step:12207	 l-p:0.18703219294548035
epoch£º610	 i:8 	 global-step:12208	 l-p:0.15404000878334045
epoch£º610	 i:9 	 global-step:12209	 l-p:0.14430780708789825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0988, 4.9266, 4.8706],
        [5.0988, 5.3877, 5.2859],
        [5.0988, 5.6192, 5.6756],
        [5.0988, 5.0977, 5.0988]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.09664779156446457 
model_pd.l_d.mean(): -20.4281005859375 
model_pd.lagr.mean(): -20.331453323364258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4182], device='cuda:0')), ('power', tensor([-21.2432], device='cuda:0'))])
epoch£º611	 i:0 	 global-step:12220	 l-p:0.09664779156446457
epoch£º611	 i:1 	 global-step:12221	 l-p:0.11928606033325195
epoch£º611	 i:2 	 global-step:12222	 l-p:0.08653058111667633
epoch£º611	 i:3 	 global-step:12223	 l-p:0.15970365703105927
epoch£º611	 i:4 	 global-step:12224	 l-p:0.1395242065191269
epoch£º611	 i:5 	 global-step:12225	 l-p:0.11146938055753708
epoch£º611	 i:6 	 global-step:12226	 l-p:0.12743240594863892
epoch£º611	 i:7 	 global-step:12227	 l-p:0.1288544237613678
epoch£º611	 i:8 	 global-step:12228	 l-p:-0.25958141684532166
epoch£º611	 i:9 	 global-step:12229	 l-p:0.14833594858646393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1165, 5.0717, 4.8374],
        [5.1165, 5.1165, 5.1165],
        [5.1165, 4.9533, 4.9247],
        [5.1165, 5.1156, 5.1165]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.14370757341384888 
model_pd.l_d.mean(): -19.34954071044922 
model_pd.lagr.mean(): -19.205833435058594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4851], device='cuda:0')), ('power', tensor([-20.2138], device='cuda:0'))])
epoch£º612	 i:0 	 global-step:12240	 l-p:0.14370757341384888
epoch£º612	 i:1 	 global-step:12241	 l-p:0.129365012049675
epoch£º612	 i:2 	 global-step:12242	 l-p:0.09712377935647964
epoch£º612	 i:3 	 global-step:12243	 l-p:0.12963561713695526
epoch£º612	 i:4 	 global-step:12244	 l-p:0.10851926356554031
epoch£º612	 i:5 	 global-step:12245	 l-p:0.13362795114517212
epoch£º612	 i:6 	 global-step:12246	 l-p:0.1436934918165207
epoch£º612	 i:7 	 global-step:12247	 l-p:0.14542070031166077
epoch£º612	 i:8 	 global-step:12248	 l-p:0.19011083245277405
epoch£º612	 i:9 	 global-step:12249	 l-p:0.059623297303915024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0601, 5.0600, 5.0601],
        [5.0601, 5.0355, 5.0556],
        [5.0601, 5.0569, 5.0599],
        [5.0601, 5.0031, 5.0397]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 0.20171615481376648 
model_pd.l_d.mean(): -20.423656463623047 
model_pd.lagr.mean(): -20.221940994262695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4581], device='cuda:0')), ('power', tensor([-21.2800], device='cuda:0'))])
epoch£º613	 i:0 	 global-step:12260	 l-p:0.20171615481376648
epoch£º613	 i:1 	 global-step:12261	 l-p:0.1641875058412552
epoch£º613	 i:2 	 global-step:12262	 l-p:0.11995527893304825
epoch£º613	 i:3 	 global-step:12263	 l-p:0.0469842329621315
epoch£º613	 i:4 	 global-step:12264	 l-p:0.11407922208309174
epoch£º613	 i:5 	 global-step:12265	 l-p:0.12229692935943604
epoch£º613	 i:6 	 global-step:12266	 l-p:0.14099907875061035
epoch£º613	 i:7 	 global-step:12267	 l-p:0.09704042971134186
epoch£º613	 i:8 	 global-step:12268	 l-p:0.1074858009815216
epoch£º613	 i:9 	 global-step:12269	 l-p:0.1322711706161499
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0881, 4.9853, 4.7575],
        [5.0881, 4.9091, 4.8169],
        [5.0881, 5.0848, 5.0880],
        [5.0881, 5.0865, 5.0881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.13710395991802216 
model_pd.l_d.mean(): -20.281564712524414 
model_pd.lagr.mean(): -20.144460678100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4699], device='cuda:0')), ('power', tensor([-21.1475], device='cuda:0'))])
epoch£º614	 i:0 	 global-step:12280	 l-p:0.13710395991802216
epoch£º614	 i:1 	 global-step:12281	 l-p:0.14915479719638824
epoch£º614	 i:2 	 global-step:12282	 l-p:0.036287203431129456
epoch£º614	 i:3 	 global-step:12283	 l-p:0.12790648639202118
epoch£º614	 i:4 	 global-step:12284	 l-p:0.12902656197547913
epoch£º614	 i:5 	 global-step:12285	 l-p:0.1403914839029312
epoch£º614	 i:6 	 global-step:12286	 l-p:0.182022362947464
epoch£º614	 i:7 	 global-step:12287	 l-p:0.11127230525016785
epoch£º614	 i:8 	 global-step:12288	 l-p:0.15365946292877197
epoch£º614	 i:9 	 global-step:12289	 l-p:0.1223749965429306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0555, 4.8729, 4.7610],
        [5.0555, 4.8735, 4.7885],
        [5.0555, 5.0944, 4.8702],
        [5.0555, 5.0555, 5.0555]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.1336761862039566 
model_pd.l_d.mean(): -18.990203857421875 
model_pd.lagr.mean(): -18.85652732849121 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5202], device='cuda:0')), ('power', tensor([-19.8841], device='cuda:0'))])
epoch£º615	 i:0 	 global-step:12300	 l-p:0.1336761862039566
epoch£º615	 i:1 	 global-step:12301	 l-p:0.09929385781288147
epoch£º615	 i:2 	 global-step:12302	 l-p:0.13926251232624054
epoch£º615	 i:3 	 global-step:12303	 l-p:0.10511183738708496
epoch£º615	 i:4 	 global-step:12304	 l-p:0.13166546821594238
epoch£º615	 i:5 	 global-step:12305	 l-p:0.1305050104856491
epoch£º615	 i:6 	 global-step:12306	 l-p:0.14826679229736328
epoch£º615	 i:7 	 global-step:12307	 l-p:0.17693451046943665
epoch£º615	 i:8 	 global-step:12308	 l-p:0.13014869391918182
epoch£º615	 i:9 	 global-step:12309	 l-p:0.11782016605138779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0567, 5.0546, 5.0566],
        [5.0567, 5.0237, 5.0492],
        [5.0567, 5.2095, 5.0315],
        [5.0567, 4.9091, 4.7045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.1335132122039795 
model_pd.l_d.mean(): -18.593523025512695 
model_pd.lagr.mean(): -18.460010528564453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6019], device='cuda:0')), ('power', tensor([-19.5646], device='cuda:0'))])
epoch£º616	 i:0 	 global-step:12320	 l-p:0.1335132122039795
epoch£º616	 i:1 	 global-step:12321	 l-p:0.1571764349937439
epoch£º616	 i:2 	 global-step:12322	 l-p:0.15791969001293182
epoch£º616	 i:3 	 global-step:12323	 l-p:0.14858336746692657
epoch£º616	 i:4 	 global-step:12324	 l-p:0.07540673017501831
epoch£º616	 i:5 	 global-step:12325	 l-p:0.10754415392875671
epoch£º616	 i:6 	 global-step:12326	 l-p:0.13247863948345184
epoch£º616	 i:7 	 global-step:12327	 l-p:0.12464220076799393
epoch£º616	 i:8 	 global-step:12328	 l-p:0.13884209096431732
epoch£º616	 i:9 	 global-step:12329	 l-p:0.13041044771671295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0534, 4.9933, 5.0310],
        [5.0534, 4.9102, 4.9247],
        [5.0534, 5.5849, 5.6504],
        [5.0534, 4.9943, 5.0317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.1467934399843216 
model_pd.l_d.mean(): -20.097990036010742 
model_pd.lagr.mean(): -19.951196670532227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4804], device='cuda:0')), ('power', tensor([-20.9713], device='cuda:0'))])
epoch£º617	 i:0 	 global-step:12340	 l-p:0.1467934399843216
epoch£º617	 i:1 	 global-step:12341	 l-p:0.11305948346853256
epoch£º617	 i:2 	 global-step:12342	 l-p:0.1697060912847519
epoch£º617	 i:3 	 global-step:12343	 l-p:0.11709301918745041
epoch£º617	 i:4 	 global-step:12344	 l-p:0.1301933377981186
epoch£º617	 i:5 	 global-step:12345	 l-p:0.15307624638080597
epoch£º617	 i:6 	 global-step:12346	 l-p:0.12814916670322418
epoch£º617	 i:7 	 global-step:12347	 l-p:0.21174189448356628
epoch£º617	 i:8 	 global-step:12348	 l-p:0.11975844949483871
epoch£º617	 i:9 	 global-step:12349	 l-p:0.1449069231748581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0266, 4.8397, 4.7356],
        [5.0266, 4.9647, 5.0032],
        [5.0266, 5.5247, 5.5666],
        [5.0266, 4.8548, 4.8264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.12912717461585999 
model_pd.l_d.mean(): -20.140634536743164 
model_pd.lagr.mean(): -20.011507034301758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4888], device='cuda:0')), ('power', tensor([-21.0235], device='cuda:0'))])
epoch£º618	 i:0 	 global-step:12360	 l-p:0.12912717461585999
epoch£º618	 i:1 	 global-step:12361	 l-p:0.1712140291929245
epoch£º618	 i:2 	 global-step:12362	 l-p:0.10410778969526291
epoch£º618	 i:3 	 global-step:12363	 l-p:0.2696413993835449
epoch£º618	 i:4 	 global-step:12364	 l-p:0.12044533342123032
epoch£º618	 i:5 	 global-step:12365	 l-p:0.15993820130825043
epoch£º618	 i:6 	 global-step:12366	 l-p:0.13666193187236786
epoch£º618	 i:7 	 global-step:12367	 l-p:0.15951018035411835
epoch£º618	 i:8 	 global-step:12368	 l-p:0.08828576654195786
epoch£º618	 i:9 	 global-step:12369	 l-p:0.15350890159606934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0200, 4.9604, 4.9982],
        [5.0200, 4.9040, 4.9402],
        [5.0200, 4.8999, 4.9343],
        [5.0200, 5.0989, 4.8874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): 0.17219607532024384 
model_pd.l_d.mean(): -20.199954986572266 
model_pd.lagr.mean(): -20.027759552001953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5096], device='cuda:0')), ('power', tensor([-21.1055], device='cuda:0'))])
epoch£º619	 i:0 	 global-step:12380	 l-p:0.17219607532024384
epoch£º619	 i:1 	 global-step:12381	 l-p:0.15030154585838318
epoch£º619	 i:2 	 global-step:12382	 l-p:0.1203235611319542
epoch£º619	 i:3 	 global-step:12383	 l-p:0.08805111050605774
epoch£º619	 i:4 	 global-step:12384	 l-p:0.17757728695869446
epoch£º619	 i:5 	 global-step:12385	 l-p:0.10780855268239975
epoch£º619	 i:6 	 global-step:12386	 l-p:0.14408719539642334
epoch£º619	 i:7 	 global-step:12387	 l-p:0.11461971700191498
epoch£º619	 i:8 	 global-step:12388	 l-p:0.12424203753471375
epoch£º619	 i:9 	 global-step:12389	 l-p:0.16778358817100525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0645, 5.0417, 5.0606],
        [5.0645, 4.9236, 4.9407],
        [5.0645, 5.0645, 5.0645],
        [5.0645, 5.0272, 4.7873]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.1306166648864746 
model_pd.l_d.mean(): -20.70477867126465 
model_pd.lagr.mean(): -20.574161529541016 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4320], device='cuda:0')), ('power', tensor([-21.5393], device='cuda:0'))])
epoch£º620	 i:0 	 global-step:12400	 l-p:0.1306166648864746
epoch£º620	 i:1 	 global-step:12401	 l-p:0.176430806517601
epoch£º620	 i:2 	 global-step:12402	 l-p:0.12593211233615875
epoch£º620	 i:3 	 global-step:12403	 l-p:0.10230153799057007
epoch£º620	 i:4 	 global-step:12404	 l-p:0.1230224221944809
epoch£º620	 i:5 	 global-step:12405	 l-p:0.08963076025247574
epoch£º620	 i:6 	 global-step:12406	 l-p:0.14979606866836548
epoch£º620	 i:7 	 global-step:12407	 l-p:0.1351538747549057
epoch£º620	 i:8 	 global-step:12408	 l-p:0.1388946920633316
epoch£º620	 i:9 	 global-step:12409	 l-p:0.11995410919189453
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0755, 5.1246, 4.9024],
        [5.0755, 5.0746, 5.0755],
        [5.0755, 5.0755, 5.0755],
        [5.0755, 5.0755, 5.0755]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.14699454605579376 
model_pd.l_d.mean(): -20.706186294555664 
model_pd.lagr.mean(): -20.559192657470703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4081], device='cuda:0')), ('power', tensor([-21.5160], device='cuda:0'))])
epoch£º621	 i:0 	 global-step:12420	 l-p:0.14699454605579376
epoch£º621	 i:1 	 global-step:12421	 l-p:0.12578965723514557
epoch£º621	 i:2 	 global-step:12422	 l-p:0.12320175021886826
epoch£º621	 i:3 	 global-step:12423	 l-p:0.17567317187786102
epoch£º621	 i:4 	 global-step:12424	 l-p:0.1759050488471985
epoch£º621	 i:5 	 global-step:12425	 l-p:0.06777001172304153
epoch£º621	 i:6 	 global-step:12426	 l-p:0.11148113012313843
epoch£º621	 i:7 	 global-step:12427	 l-p:0.12563928961753845
epoch£º621	 i:8 	 global-step:12428	 l-p:0.14393073320388794
epoch£º621	 i:9 	 global-step:12429	 l-p:-0.026251908391714096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1077, 5.1077, 5.1077],
        [5.1077, 4.9338, 4.8837],
        [5.1077, 5.0039, 4.7747],
        [5.1077, 4.9564, 4.9580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.1551329344511032 
model_pd.l_d.mean(): -20.59609031677246 
model_pd.lagr.mean(): -20.44095802307129 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4216], device='cuda:0')), ('power', tensor([-21.4178], device='cuda:0'))])
epoch£º622	 i:0 	 global-step:12440	 l-p:0.1551329344511032
epoch£º622	 i:1 	 global-step:12441	 l-p:0.13004477322101593
epoch£º622	 i:2 	 global-step:12442	 l-p:0.1212020292878151
epoch£º622	 i:3 	 global-step:12443	 l-p:0.09539655596017838
epoch£º622	 i:4 	 global-step:12444	 l-p:0.1199793592095375
epoch£º622	 i:5 	 global-step:12445	 l-p:0.0027688932605087757
epoch£º622	 i:6 	 global-step:12446	 l-p:0.1549433022737503
epoch£º622	 i:7 	 global-step:12447	 l-p:0.14397487044334412
epoch£º622	 i:8 	 global-step:12448	 l-p:0.1203622967004776
epoch£º622	 i:9 	 global-step:12449	 l-p:0.11720307171344757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0695, 5.0356, 5.0616],
        [5.0695, 4.9561, 4.9924],
        [5.0695, 5.0631, 5.0691],
        [5.0695, 5.0694, 5.0695]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.14979511499404907 
model_pd.l_d.mean(): -20.1369571685791 
model_pd.lagr.mean(): -19.98716163635254 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4384], device='cuda:0')), ('power', tensor([-20.9676], device='cuda:0'))])
epoch£º623	 i:0 	 global-step:12460	 l-p:0.14979511499404907
epoch£º623	 i:1 	 global-step:12461	 l-p:0.1258433759212494
epoch£º623	 i:2 	 global-step:12462	 l-p:0.1675603836774826
epoch£º623	 i:3 	 global-step:12463	 l-p:0.2067905217409134
epoch£º623	 i:4 	 global-step:12464	 l-p:0.11816809326410294
epoch£º623	 i:5 	 global-step:12465	 l-p:0.1341291218996048
epoch£º623	 i:6 	 global-step:12466	 l-p:0.12796932458877563
epoch£º623	 i:7 	 global-step:12467	 l-p:0.1230962947010994
epoch£º623	 i:8 	 global-step:12468	 l-p:0.12405131012201309
epoch£º623	 i:9 	 global-step:12469	 l-p:0.12030603736639023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0287, 5.0203, 5.0280],
        [5.0287, 5.3820, 5.3219],
        [5.0287, 5.0270, 5.0287],
        [5.0287, 5.0287, 5.0287]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.1850118190050125 
model_pd.l_d.mean(): -20.343393325805664 
model_pd.lagr.mean(): -20.158382415771484 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4750], device='cuda:0')), ('power', tensor([-21.2158], device='cuda:0'))])
epoch£º624	 i:0 	 global-step:12480	 l-p:0.1850118190050125
epoch£º624	 i:1 	 global-step:12481	 l-p:0.1298886090517044
epoch£º624	 i:2 	 global-step:12482	 l-p:0.1519503891468048
epoch£º624	 i:3 	 global-step:12483	 l-p:0.10096949338912964
epoch£º624	 i:4 	 global-step:12484	 l-p:0.1920681744813919
epoch£º624	 i:5 	 global-step:12485	 l-p:0.1467500478029251
epoch£º624	 i:6 	 global-step:12486	 l-p:0.14754049479961395
epoch£º624	 i:7 	 global-step:12487	 l-p:0.0968189612030983
epoch£º624	 i:8 	 global-step:12488	 l-p:0.110732801258564
epoch£º624	 i:9 	 global-step:12489	 l-p:0.15727479755878448
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0574, 5.0574, 5.0574],
        [5.0574, 5.0450, 5.0561],
        [5.0574, 5.0562, 5.0574],
        [5.0574, 4.8770, 4.7239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.15150563418865204 
model_pd.l_d.mean(): -20.168813705444336 
model_pd.lagr.mean(): -20.01730728149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4871], device='cuda:0')), ('power', tensor([-21.0504], device='cuda:0'))])
epoch£º625	 i:0 	 global-step:12500	 l-p:0.15150563418865204
epoch£º625	 i:1 	 global-step:12501	 l-p:0.09771905839443207
epoch£º625	 i:2 	 global-step:12502	 l-p:0.08909404277801514
epoch£º625	 i:3 	 global-step:12503	 l-p:0.13578589260578156
epoch£º625	 i:4 	 global-step:12504	 l-p:0.14641648530960083
epoch£º625	 i:5 	 global-step:12505	 l-p:0.15370692312717438
epoch£º625	 i:6 	 global-step:12506	 l-p:0.10537274181842804
epoch£º625	 i:7 	 global-step:12507	 l-p:0.12769903242588043
epoch£º625	 i:8 	 global-step:12508	 l-p:0.12875917553901672
epoch£º625	 i:9 	 global-step:12509	 l-p:0.130777508020401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0925, 5.0508, 5.0811],
        [5.0925, 4.9151, 4.8616],
        [5.0925, 5.0925, 5.0925],
        [5.0925, 5.4850, 5.4481]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.04517533257603645 
model_pd.l_d.mean(): -20.66992950439453 
model_pd.lagr.mean(): -20.624753952026367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4117], device='cuda:0')), ('power', tensor([-21.4829], device='cuda:0'))])
epoch£º626	 i:0 	 global-step:12520	 l-p:0.04517533257603645
epoch£º626	 i:1 	 global-step:12521	 l-p:0.15259496867656708
epoch£º626	 i:2 	 global-step:12522	 l-p:0.121854767203331
epoch£º626	 i:3 	 global-step:12523	 l-p:0.1351296454668045
epoch£º626	 i:4 	 global-step:12524	 l-p:0.10522229969501495
epoch£º626	 i:5 	 global-step:12525	 l-p:0.11234109848737717
epoch£º626	 i:6 	 global-step:12526	 l-p:0.10037652403116226
epoch£º626	 i:7 	 global-step:12527	 l-p:0.15674854815006256
epoch£º626	 i:8 	 global-step:12528	 l-p:0.11881474405527115
epoch£º626	 i:9 	 global-step:12529	 l-p:0.1302531659603119
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1180, 5.0228, 5.0635],
        [5.1180, 4.9445, 4.7878],
        [5.1180, 5.0301, 5.0715],
        [5.1180, 5.0863, 5.1109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.1076880618929863 
model_pd.l_d.mean(): -20.58348274230957 
model_pd.lagr.mean(): -20.475793838500977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4073], device='cuda:0')), ('power', tensor([-21.3903], device='cuda:0'))])
epoch£º627	 i:0 	 global-step:12540	 l-p:0.1076880618929863
epoch£º627	 i:1 	 global-step:12541	 l-p:0.09769266098737717
epoch£º627	 i:2 	 global-step:12542	 l-p:0.1531345546245575
epoch£º627	 i:3 	 global-step:12543	 l-p:0.11526654660701752
epoch£º627	 i:4 	 global-step:12544	 l-p:0.12782977521419525
epoch£º627	 i:5 	 global-step:12545	 l-p:0.12557663023471832
epoch£º627	 i:6 	 global-step:12546	 l-p:0.1521659642457962
epoch£º627	 i:7 	 global-step:12547	 l-p:0.03685220703482628
epoch£º627	 i:8 	 global-step:12548	 l-p:0.15704438090324402
epoch£º627	 i:9 	 global-step:12549	 l-p:0.1506294459104538
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0694, 4.8829, 4.7880],
        [5.0694, 5.0691, 5.0694],
        [5.0694, 5.0685, 5.0694],
        [5.0694, 5.0041, 5.0435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.07524880766868591 
model_pd.l_d.mean(): -20.62016487121582 
model_pd.lagr.mean(): -20.5449161529541 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4253], device='cuda:0')), ('power', tensor([-21.4462], device='cuda:0'))])
epoch£º628	 i:0 	 global-step:12560	 l-p:0.07524880766868591
epoch£º628	 i:1 	 global-step:12561	 l-p:0.1465936303138733
epoch£º628	 i:2 	 global-step:12562	 l-p:0.17686571180820465
epoch£º628	 i:3 	 global-step:12563	 l-p:0.17087501287460327
epoch£º628	 i:4 	 global-step:12564	 l-p:0.10216941684484482
epoch£º628	 i:5 	 global-step:12565	 l-p:0.14127737283706665
epoch£º628	 i:6 	 global-step:12566	 l-p:0.1299903392791748
epoch£º628	 i:7 	 global-step:12567	 l-p:0.1232360228896141
epoch£º628	 i:8 	 global-step:12568	 l-p:0.15989908576011658
epoch£º628	 i:9 	 global-step:12569	 l-p:0.11021784693002701
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0423, 5.0414, 5.0423],
        [5.0423, 4.8608, 4.8123],
        [5.0423, 5.0378, 5.0421],
        [5.0423, 5.2367, 5.0782]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.1459413468837738 
model_pd.l_d.mean(): -20.886940002441406 
model_pd.lagr.mean(): -20.740999221801758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3952], device='cuda:0')), ('power', tensor([-21.6869], device='cuda:0'))])
epoch£º629	 i:0 	 global-step:12580	 l-p:0.1459413468837738
epoch£º629	 i:1 	 global-step:12581	 l-p:0.13368524610996246
epoch£º629	 i:2 	 global-step:12582	 l-p:0.13150401413440704
epoch£º629	 i:3 	 global-step:12583	 l-p:0.128194198012352
epoch£º629	 i:4 	 global-step:12584	 l-p:0.19644492864608765
epoch£º629	 i:5 	 global-step:12585	 l-p:0.09623409062623978
epoch£º629	 i:6 	 global-step:12586	 l-p:0.10955308377742767
epoch£º629	 i:7 	 global-step:12587	 l-p:0.12069906294345856
epoch£º629	 i:8 	 global-step:12588	 l-p:0.22489072382450104
epoch£º629	 i:9 	 global-step:12589	 l-p:0.15956035256385803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0282, 5.0282, 5.0282],
        [5.0282, 4.8453, 4.7972],
        [5.0282, 4.8361, 4.7232],
        [5.0282, 5.0282, 5.0282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.16368858516216278 
model_pd.l_d.mean(): -19.90825080871582 
model_pd.lagr.mean(): -19.74456214904785 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4981], device='cuda:0')), ('power', tensor([-20.7965], device='cuda:0'))])
epoch£º630	 i:0 	 global-step:12600	 l-p:0.16368858516216278
epoch£º630	 i:1 	 global-step:12601	 l-p:0.16397403180599213
epoch£º630	 i:2 	 global-step:12602	 l-p:0.2110854983329773
epoch£º630	 i:3 	 global-step:12603	 l-p:0.1071997582912445
epoch£º630	 i:4 	 global-step:12604	 l-p:0.1106274202466011
epoch£º630	 i:5 	 global-step:12605	 l-p:0.1389193832874298
epoch£º630	 i:6 	 global-step:12606	 l-p:0.10084155946969986
epoch£º630	 i:7 	 global-step:12607	 l-p:0.1210283413529396
epoch£º630	 i:8 	 global-step:12608	 l-p:0.13795916736125946
epoch£º630	 i:9 	 global-step:12609	 l-p:0.12083899974822998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0652, 5.0486, 5.0630],
        [5.0652, 4.9982, 4.7535],
        [5.0652, 5.0652, 5.0652],
        [5.0652, 4.9387, 4.9702]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.11708732694387436 
model_pd.l_d.mean(): -20.821657180786133 
model_pd.lagr.mean(): -20.704570770263672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3941], device='cuda:0')), ('power', tensor([-21.6192], device='cuda:0'))])
epoch£º631	 i:0 	 global-step:12620	 l-p:0.11708732694387436
epoch£º631	 i:1 	 global-step:12621	 l-p:0.10773389786481857
epoch£º631	 i:2 	 global-step:12622	 l-p:0.14428295195102692
epoch£º631	 i:3 	 global-step:12623	 l-p:0.15245233476161957
epoch£º631	 i:4 	 global-step:12624	 l-p:0.19867388904094696
epoch£º631	 i:5 	 global-step:12625	 l-p:0.17297233641147614
epoch£º631	 i:6 	 global-step:12626	 l-p:0.08977352827787399
epoch£º631	 i:7 	 global-step:12627	 l-p:0.1244664415717125
epoch£º631	 i:8 	 global-step:12628	 l-p:0.1288767158985138
epoch£º631	 i:9 	 global-step:12629	 l-p:0.12363601475954056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0640, 5.0096, 5.0458],
        [5.0640, 5.0472, 5.0617],
        [5.0640, 5.0588, 5.0637],
        [5.0640, 5.0599, 5.0638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): 0.11842315644025803 
model_pd.l_d.mean(): -19.348854064941406 
model_pd.lagr.mean(): -19.230430603027344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4982], device='cuda:0')), ('power', tensor([-20.2266], device='cuda:0'))])
epoch£º632	 i:0 	 global-step:12640	 l-p:0.11842315644025803
epoch£º632	 i:1 	 global-step:12641	 l-p:0.15100887417793274
epoch£º632	 i:2 	 global-step:12642	 l-p:0.13001134991645813
epoch£º632	 i:3 	 global-step:12643	 l-p:0.1461062729358673
epoch£º632	 i:4 	 global-step:12644	 l-p:0.10726138949394226
epoch£º632	 i:5 	 global-step:12645	 l-p:0.13371746242046356
epoch£º632	 i:6 	 global-step:12646	 l-p:0.12352539598941803
epoch£º632	 i:7 	 global-step:12647	 l-p:0.15136225521564484
epoch£º632	 i:8 	 global-step:12648	 l-p:0.10921672731637955
epoch£º632	 i:9 	 global-step:12649	 l-p:0.1832265853881836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0405, 5.0262, 5.0388],
        [5.0405, 4.8502, 4.7132],
        [5.0405, 4.9296, 4.9693],
        [5.0405, 5.5835, 5.6551]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.13428260385990143 
model_pd.l_d.mean(): -19.709381103515625 
model_pd.lagr.mean(): -19.575098037719727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5056], device='cuda:0')), ('power', tensor([-20.6016], device='cuda:0'))])
epoch£º633	 i:0 	 global-step:12660	 l-p:0.13428260385990143
epoch£º633	 i:1 	 global-step:12661	 l-p:0.148715078830719
epoch£º633	 i:2 	 global-step:12662	 l-p:0.11668920516967773
epoch£º633	 i:3 	 global-step:12663	 l-p:0.14782142639160156
epoch£º633	 i:4 	 global-step:12664	 l-p:0.12454353272914886
epoch£º633	 i:5 	 global-step:12665	 l-p:0.17549702525138855
epoch£º633	 i:6 	 global-step:12666	 l-p:0.14823533594608307
epoch£º633	 i:7 	 global-step:12667	 l-p:0.1765400618314743
epoch£º633	 i:8 	 global-step:12668	 l-p:0.13518469035625458
epoch£º633	 i:9 	 global-step:12669	 l-p:0.09227824211120605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0632, 5.0632, 5.0632],
        [5.0632, 5.0594, 5.0630],
        [5.0632, 4.9613, 5.0028],
        [5.0632, 4.9710, 5.0136]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.12373597919940948 
model_pd.l_d.mean(): -19.91504669189453 
model_pd.lagr.mean(): -19.791311264038086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4979], device='cuda:0')), ('power', tensor([-20.8031], device='cuda:0'))])
epoch£º634	 i:0 	 global-step:12680	 l-p:0.12373597919940948
epoch£º634	 i:1 	 global-step:12681	 l-p:0.18238605558872223
epoch£º634	 i:2 	 global-step:12682	 l-p:0.16460980474948883
epoch£º634	 i:3 	 global-step:12683	 l-p:0.16339725255966187
epoch£º634	 i:4 	 global-step:12684	 l-p:0.0401485450565815
epoch£º634	 i:5 	 global-step:12685	 l-p:0.14487619698047638
epoch£º634	 i:6 	 global-step:12686	 l-p:0.12306269258260727
epoch£º634	 i:7 	 global-step:12687	 l-p:0.06338363140821457
epoch£º634	 i:8 	 global-step:12688	 l-p:0.10925685614347458
epoch£º634	 i:9 	 global-step:12689	 l-p:0.139177605509758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1241, 4.9798, 4.9941],
        [5.1241, 4.9893, 5.0128],
        [5.1241, 5.1232, 5.1241],
        [5.1241, 4.9953, 4.7728]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.14141985774040222 
model_pd.l_d.mean(): -20.450542449951172 
model_pd.lagr.mean(): -20.30912208557129 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4452], device='cuda:0')), ('power', tensor([-21.2941], device='cuda:0'))])
epoch£º635	 i:0 	 global-step:12700	 l-p:0.14141985774040222
epoch£º635	 i:1 	 global-step:12701	 l-p:0.13169552385807037
epoch£º635	 i:2 	 global-step:12702	 l-p:0.11051232367753983
epoch£º635	 i:3 	 global-step:12703	 l-p:0.13940414786338806
epoch£º635	 i:4 	 global-step:12704	 l-p:0.04709947481751442
epoch£º635	 i:5 	 global-step:12705	 l-p:0.0973903238773346
epoch£º635	 i:6 	 global-step:12706	 l-p:0.14008131623268127
epoch£º635	 i:7 	 global-step:12707	 l-p:0.12324825674295425
epoch£º635	 i:8 	 global-step:12708	 l-p:0.1083257868885994
epoch£º635	 i:9 	 global-step:12709	 l-p:0.3713720142841339
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1556, 5.0964, 5.1339],
        [5.1556, 5.1382, 5.1532],
        [5.1556, 5.1047, 4.8641],
        [5.1556, 5.1556, 5.1556]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.11467950791120529 
model_pd.l_d.mean(): -20.332605361938477 
model_pd.lagr.mean(): -20.217926025390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4182], device='cuda:0')), ('power', tensor([-21.1459], device='cuda:0'))])
epoch£º636	 i:0 	 global-step:12720	 l-p:0.11467950791120529
epoch£º636	 i:1 	 global-step:12721	 l-p:0.1210295632481575
epoch£º636	 i:2 	 global-step:12722	 l-p:0.10178805887699127
epoch£º636	 i:3 	 global-step:12723	 l-p:0.12103450298309326
epoch£º636	 i:4 	 global-step:12724	 l-p:0.04541986063122749
epoch£º636	 i:5 	 global-step:12725	 l-p:0.0670320987701416
epoch£º636	 i:6 	 global-step:12726	 l-p:0.13266508281230927
epoch£º636	 i:7 	 global-step:12727	 l-p:-0.12589089572429657
epoch£º636	 i:8 	 global-step:12728	 l-p:0.17800858616828918
epoch£º636	 i:9 	 global-step:12729	 l-p:0.15819090604782104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1270, 4.9560, 4.9261],
        [5.1270, 5.6951, 5.7811],
        [5.1270, 5.0006, 5.0305],
        [5.1270, 5.1103, 5.1247]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.15450824797153473 
model_pd.l_d.mean(): -19.313535690307617 
model_pd.lagr.mean(): -19.159027099609375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4826], device='cuda:0')), ('power', tensor([-20.1745], device='cuda:0'))])
epoch£º637	 i:0 	 global-step:12740	 l-p:0.15450824797153473
epoch£º637	 i:1 	 global-step:12741	 l-p:0.12551334500312805
epoch£º637	 i:2 	 global-step:12742	 l-p:0.09704796224832535
epoch£º637	 i:3 	 global-step:12743	 l-p:0.14299368858337402
epoch£º637	 i:4 	 global-step:12744	 l-p:0.11738170683383942
epoch£º637	 i:5 	 global-step:12745	 l-p:0.12430822849273682
epoch£º637	 i:6 	 global-step:12746	 l-p:-0.02384873852133751
epoch£º637	 i:7 	 global-step:12747	 l-p:0.11994434148073196
epoch£º637	 i:8 	 global-step:12748	 l-p:0.16794821619987488
epoch£º637	 i:9 	 global-step:12749	 l-p:0.09247013181447983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1007, 5.1004, 5.1007],
        [5.1007, 5.1007, 5.1007],
        [5.1007, 5.3066, 5.1516],
        [5.1007, 5.5108, 5.4836]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.1393393725156784 
model_pd.l_d.mean(): -20.259122848510742 
model_pd.lagr.mean(): -20.119783401489258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5005], device='cuda:0')), ('power', tensor([-21.1564], device='cuda:0'))])
epoch£º638	 i:0 	 global-step:12760	 l-p:0.1393393725156784
epoch£º638	 i:1 	 global-step:12761	 l-p:0.08680684119462967
epoch£º638	 i:2 	 global-step:12762	 l-p:0.04220706224441528
epoch£º638	 i:3 	 global-step:12763	 l-p:0.12233725935220718
epoch£º638	 i:4 	 global-step:12764	 l-p:0.14132408797740936
epoch£º638	 i:5 	 global-step:12765	 l-p:0.10368306934833527
epoch£º638	 i:6 	 global-step:12766	 l-p:0.15540792047977448
epoch£º638	 i:7 	 global-step:12767	 l-p:0.14636273682117462
epoch£º638	 i:8 	 global-step:12768	 l-p:0.14135602116584778
epoch£º638	 i:9 	 global-step:12769	 l-p:0.13007374107837677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0958, 5.5393, 5.5353],
        [5.0958, 5.0920, 5.0956],
        [5.0958, 4.9121, 4.8502],
        [5.0958, 4.9192, 4.8822]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.10365844517946243 
model_pd.l_d.mean(): -18.202816009521484 
model_pd.lagr.mean(): -18.099157333374023 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6310], device='cuda:0')), ('power', tensor([-19.1968], device='cuda:0'))])
epoch£º639	 i:0 	 global-step:12780	 l-p:0.10365844517946243
epoch£º639	 i:1 	 global-step:12781	 l-p:0.18265819549560547
epoch£º639	 i:2 	 global-step:12782	 l-p:0.12655070424079895
epoch£º639	 i:3 	 global-step:12783	 l-p:0.08585953712463379
epoch£º639	 i:4 	 global-step:12784	 l-p:0.13919244706630707
epoch£º639	 i:5 	 global-step:12785	 l-p:0.06895331293344498
epoch£º639	 i:6 	 global-step:12786	 l-p:0.13985827565193176
epoch£º639	 i:7 	 global-step:12787	 l-p:0.14016662538051605
epoch£º639	 i:8 	 global-step:12788	 l-p:-1.8634414672851562
epoch£º639	 i:9 	 global-step:12789	 l-p:0.13612397015094757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1416, 4.9639, 4.9134],
        [5.1416, 4.9597, 4.8228],
        [5.1416, 5.1417, 5.1417],
        [5.1416, 5.1241, 5.1392]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 0.09776633977890015 
model_pd.l_d.mean(): -19.883808135986328 
model_pd.lagr.mean(): -19.786041259765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5198], device='cuda:0')), ('power', tensor([-20.7940], device='cuda:0'))])
epoch£º640	 i:0 	 global-step:12800	 l-p:0.09776633977890015
epoch£º640	 i:1 	 global-step:12801	 l-p:0.1407473385334015
epoch£º640	 i:2 	 global-step:12802	 l-p:-0.3589763045310974
epoch£º640	 i:3 	 global-step:12803	 l-p:0.09036411345005035
epoch£º640	 i:4 	 global-step:12804	 l-p:0.1672789603471756
epoch£º640	 i:5 	 global-step:12805	 l-p:0.11793136596679688
epoch£º640	 i:6 	 global-step:12806	 l-p:0.12609174847602844
epoch£º640	 i:7 	 global-step:12807	 l-p:0.1207486018538475
epoch£º640	 i:8 	 global-step:12808	 l-p:0.12726366519927979
epoch£º640	 i:9 	 global-step:12809	 l-p:0.1382361650466919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1346, 5.1332, 5.1346],
        [5.1346, 5.0965, 4.8533],
        [5.1346, 5.1346, 5.1346],
        [5.1346, 5.0747, 5.1126]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): 0.008987011387944221 
model_pd.l_d.mean(): -20.47248649597168 
model_pd.lagr.mean(): -20.463499069213867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4453], device='cuda:0')), ('power', tensor([-21.3165], device='cuda:0'))])
epoch£º641	 i:0 	 global-step:12820	 l-p:0.008987011387944221
epoch£º641	 i:1 	 global-step:12821	 l-p:0.0906042829155922
epoch£º641	 i:2 	 global-step:12822	 l-p:0.10606731474399567
epoch£º641	 i:3 	 global-step:12823	 l-p:0.10805986076593399
epoch£º641	 i:4 	 global-step:12824	 l-p:0.14940142631530762
epoch£º641	 i:5 	 global-step:12825	 l-p:0.17512404918670654
epoch£º641	 i:6 	 global-step:12826	 l-p:0.13345849514007568
epoch£º641	 i:7 	 global-step:12827	 l-p:-0.06902727484703064
epoch£º641	 i:8 	 global-step:12828	 l-p:0.1325812041759491
epoch£º641	 i:9 	 global-step:12829	 l-p:0.13539113104343414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1091, 5.1089, 5.1091],
        [5.1091, 4.9322, 4.8950],
        [5.1091, 5.1091, 5.1091],
        [5.1091, 5.5782, 5.5912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): -0.0037633967585861683 
model_pd.l_d.mean(): -19.043733596801758 
model_pd.lagr.mean(): -19.047496795654297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5653], device='cuda:0')), ('power', tensor([-19.9853], device='cuda:0'))])
epoch£º642	 i:0 	 global-step:12840	 l-p:-0.0037633967585861683
epoch£º642	 i:1 	 global-step:12841	 l-p:0.15736165642738342
epoch£º642	 i:2 	 global-step:12842	 l-p:0.18521475791931152
epoch£º642	 i:3 	 global-step:12843	 l-p:0.13040229678153992
epoch£º642	 i:4 	 global-step:12844	 l-p:0.0957963764667511
epoch£º642	 i:5 	 global-step:12845	 l-p:0.14588484168052673
epoch£º642	 i:6 	 global-step:12846	 l-p:0.12893792986869812
epoch£º642	 i:7 	 global-step:12847	 l-p:0.11404336988925934
epoch£º642	 i:8 	 global-step:12848	 l-p:0.0968610942363739
epoch£º642	 i:9 	 global-step:12849	 l-p:0.1834399402141571
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0463, 4.9792, 5.0199],
        [5.0463, 5.0463, 5.0463],
        [5.0463, 4.9136, 4.6787],
        [5.0463, 5.0463, 5.0463]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.10240806639194489 
model_pd.l_d.mean(): -18.891338348388672 
model_pd.lagr.mean(): -18.788930892944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5611], device='cuda:0')), ('power', tensor([-19.8257], device='cuda:0'))])
epoch£º643	 i:0 	 global-step:12860	 l-p:0.10240806639194489
epoch£º643	 i:1 	 global-step:12861	 l-p:0.12211804836988449
epoch£º643	 i:2 	 global-step:12862	 l-p:0.13227282464504242
epoch£º643	 i:3 	 global-step:12863	 l-p:0.10778236389160156
epoch£º643	 i:4 	 global-step:12864	 l-p:0.2315094918012619
epoch£º643	 i:5 	 global-step:12865	 l-p:0.10265126824378967
epoch£º643	 i:6 	 global-step:12866	 l-p:0.18227076530456543
epoch£º643	 i:7 	 global-step:12867	 l-p:0.12277742475271225
epoch£º643	 i:8 	 global-step:12868	 l-p:0.13645051419734955
epoch£º643	 i:9 	 global-step:12869	 l-p:0.1552174836397171
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4607,  0.3558,  1.0000,  0.2748,
          1.0000,  0.7724, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1271,  0.0639,  1.0000,  0.0321,
          1.0000,  0.5028, 31.6228]], device='cuda:0')
 pt:tensor([[5.0671, 4.8740, 4.7806],
        [5.0671, 4.9372, 4.7027],
        [5.0671, 4.8852, 4.8448],
        [5.0671, 4.9680, 5.0108]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): 0.1491173654794693 
model_pd.l_d.mean(): -20.120025634765625 
model_pd.lagr.mean(): -19.970909118652344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4987], device='cuda:0')), ('power', tensor([-21.0128], device='cuda:0'))])
epoch£º644	 i:0 	 global-step:12880	 l-p:0.1491173654794693
epoch£º644	 i:1 	 global-step:12881	 l-p:0.18032638728618622
epoch£º644	 i:2 	 global-step:12882	 l-p:0.14577393233776093
epoch£º644	 i:3 	 global-step:12883	 l-p:0.12775258719921112
epoch£º644	 i:4 	 global-step:12884	 l-p:0.159708634018898
epoch£º644	 i:5 	 global-step:12885	 l-p:0.14751997590065002
epoch£º644	 i:6 	 global-step:12886	 l-p:0.13313402235507965
epoch£º644	 i:7 	 global-step:12887	 l-p:0.04055595397949219
epoch£º644	 i:8 	 global-step:12888	 l-p:0.06919315457344055
epoch£º644	 i:9 	 global-step:12889	 l-p:0.06620199233293533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1191, 5.0612, 5.0987],
        [5.1191, 4.9519, 4.9381],
        [5.1191, 5.0922, 4.8475],
        [5.1191, 5.1158, 5.1189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.14517170190811157 
model_pd.l_d.mean(): -19.90359115600586 
model_pd.lagr.mean(): -19.758419036865234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4987], device='cuda:0')), ('power', tensor([-20.7922], device='cuda:0'))])
epoch£º645	 i:0 	 global-step:12900	 l-p:0.14517170190811157
epoch£º645	 i:1 	 global-step:12901	 l-p:-0.04920196533203125
epoch£º645	 i:2 	 global-step:12902	 l-p:0.117604561150074
epoch£º645	 i:3 	 global-step:12903	 l-p:0.11241592466831207
epoch£º645	 i:4 	 global-step:12904	 l-p:0.12227171659469604
epoch£º645	 i:5 	 global-step:12905	 l-p:0.026806162670254707
epoch£º645	 i:6 	 global-step:12906	 l-p:0.14234164357185364
epoch£º645	 i:7 	 global-step:12907	 l-p:0.08310221135616302
epoch£º645	 i:8 	 global-step:12908	 l-p:0.101207435131073
epoch£º645	 i:9 	 global-step:12909	 l-p:0.12574376165866852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9387,  0.9192,  1.0000,  0.9000,
          1.0000,  0.9792, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228]], device='cuda:0')
 pt:tensor([[5.1882, 5.1139, 4.8731],
        [5.1882, 5.6779, 5.7013],
        [5.1882, 5.7182, 5.7710],
        [5.1882, 5.0181, 4.9876]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): 0.10226208716630936 
model_pd.l_d.mean(): -19.242244720458984 
model_pd.lagr.mean(): -19.139982223510742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5247], device='cuda:0')), ('power', tensor([-20.1455], device='cuda:0'))])
epoch£º646	 i:0 	 global-step:12920	 l-p:0.10226208716630936
epoch£º646	 i:1 	 global-step:12921	 l-p:0.1588926911354065
epoch£º646	 i:2 	 global-step:12922	 l-p:0.12453124672174454
epoch£º646	 i:3 	 global-step:12923	 l-p:0.135483518242836
epoch£º646	 i:4 	 global-step:12924	 l-p:-1.4247335195541382
epoch£º646	 i:5 	 global-step:12925	 l-p:0.04656211659312248
epoch£º646	 i:6 	 global-step:12926	 l-p:0.12054229527711868
epoch£º646	 i:7 	 global-step:12927	 l-p:0.12387847900390625
epoch£º646	 i:8 	 global-step:12928	 l-p:0.13417848944664001
epoch£º646	 i:9 	 global-step:12929	 l-p:0.2152232527732849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1650, 5.1650, 5.1650],
        [5.1650, 5.1647, 5.1650],
        [5.1650, 5.0445, 4.8152],
        [5.1650, 5.1647, 5.1650]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): 0.1362815499305725 
model_pd.l_d.mean(): -19.909664154052734 
model_pd.lagr.mean(): -19.77338218688965 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5025], device='cuda:0')), ('power', tensor([-20.8024], device='cuda:0'))])
epoch£º647	 i:0 	 global-step:12940	 l-p:0.1362815499305725
epoch£º647	 i:1 	 global-step:12941	 l-p:0.1310279369354248
epoch£º647	 i:2 	 global-step:12942	 l-p:0.12391389906406403
epoch£º647	 i:3 	 global-step:12943	 l-p:0.12062524259090424
epoch£º647	 i:4 	 global-step:12944	 l-p:0.11042854189872742
epoch£º647	 i:5 	 global-step:12945	 l-p:0.12859520316123962
epoch£º647	 i:6 	 global-step:12946	 l-p:-0.00827336311340332
epoch£º647	 i:7 	 global-step:12947	 l-p:0.0994088426232338
epoch£º647	 i:8 	 global-step:12948	 l-p:0.14741383492946625
epoch£º647	 i:9 	 global-step:12949	 l-p:0.0921359583735466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0866, 5.0564, 4.8088],
        [5.0866, 5.0866, 5.0866],
        [5.0866, 5.0488, 5.0773],
        [5.0866, 5.0820, 5.0864]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): 0.11851339042186737 
model_pd.l_d.mean(): -20.621280670166016 
model_pd.lagr.mean(): -20.50276756286621 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4369], device='cuda:0')), ('power', tensor([-21.4594], device='cuda:0'))])
epoch£º648	 i:0 	 global-step:12960	 l-p:0.11851339042186737
epoch£º648	 i:1 	 global-step:12961	 l-p:0.12737281620502472
epoch£º648	 i:2 	 global-step:12962	 l-p:0.17319819331169128
epoch£º648	 i:3 	 global-step:12963	 l-p:0.1432151347398758
epoch£º648	 i:4 	 global-step:12964	 l-p:0.18064934015274048
epoch£º648	 i:5 	 global-step:12965	 l-p:0.1454220861196518
epoch£º648	 i:6 	 global-step:12966	 l-p:0.1318216323852539
epoch£º648	 i:7 	 global-step:12967	 l-p:0.099016472697258
epoch£º648	 i:8 	 global-step:12968	 l-p:0.12031920999288559
epoch£º648	 i:9 	 global-step:12969	 l-p:0.0956936925649643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0759, 5.0386, 5.0668],
        [5.0759, 5.0729, 5.0758],
        [5.0759, 5.0736, 5.0758],
        [5.0759, 4.8930, 4.8526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.12251635640859604 
model_pd.l_d.mean(): -19.928037643432617 
model_pd.lagr.mean(): -19.80552101135254 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4745], device='cuda:0')), ('power', tensor([-20.7921], device='cuda:0'))])
epoch£º649	 i:0 	 global-step:12980	 l-p:0.12251635640859604
epoch£º649	 i:1 	 global-step:12981	 l-p:0.1215333342552185
epoch£º649	 i:2 	 global-step:12982	 l-p:0.1294565051794052
epoch£º649	 i:3 	 global-step:12983	 l-p:0.15065611898899078
epoch£º649	 i:4 	 global-step:12984	 l-p:0.1827467530965805
epoch£º649	 i:5 	 global-step:12985	 l-p:0.1331617832183838
epoch£º649	 i:6 	 global-step:12986	 l-p:0.13245506584644318
epoch£º649	 i:7 	 global-step:12987	 l-p:0.15228663384914398
epoch£º649	 i:8 	 global-step:12988	 l-p:0.06047642603516579
epoch£º649	 i:9 	 global-step:12989	 l-p:0.21311046183109283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0450, 4.9502, 4.9943],
        [5.0450, 4.9966, 5.0306],
        [5.0450, 4.8779, 4.6656],
        [5.0450, 4.8940, 4.9113]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 0.10328113287687302 
model_pd.l_d.mean(): -20.564617156982422 
model_pd.lagr.mean(): -20.461336135864258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4514], device='cuda:0')), ('power', tensor([-21.4167], device='cuda:0'))])
epoch£º650	 i:0 	 global-step:13000	 l-p:0.10328113287687302
epoch£º650	 i:1 	 global-step:13001	 l-p:0.11177012324333191
epoch£º650	 i:2 	 global-step:13002	 l-p:0.16546845436096191
epoch£º650	 i:3 	 global-step:13003	 l-p:0.06604635715484619
epoch£º650	 i:4 	 global-step:13004	 l-p:0.13946735858917236
epoch£º650	 i:5 	 global-step:13005	 l-p:0.15355774760246277
epoch£º650	 i:6 	 global-step:13006	 l-p:0.1539568156003952
epoch£º650	 i:7 	 global-step:13007	 l-p:0.15458647906780243
epoch£º650	 i:8 	 global-step:13008	 l-p:0.14944219589233398
epoch£º650	 i:9 	 global-step:13009	 l-p:0.15693975985050201
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0683, 5.0683, 5.0683],
        [5.0683, 5.4981, 5.4834],
        [5.0683, 5.0683, 5.0683],
        [5.0683, 4.8965, 4.8838]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.16090430319309235 
model_pd.l_d.mean(): -20.144092559814453 
model_pd.lagr.mean(): -19.98318862915039 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4980], device='cuda:0')), ('power', tensor([-21.0366], device='cuda:0'))])
epoch£º651	 i:0 	 global-step:13020	 l-p:0.16090430319309235
epoch£º651	 i:1 	 global-step:13021	 l-p:0.12010323256254196
epoch£º651	 i:2 	 global-step:13022	 l-p:0.08749786019325256
epoch£º651	 i:3 	 global-step:13023	 l-p:0.11879857629537582
epoch£º651	 i:4 	 global-step:13024	 l-p:0.12880688905715942
epoch£º651	 i:5 	 global-step:13025	 l-p:0.13688039779663086
epoch£º651	 i:6 	 global-step:13026	 l-p:0.16151264309883118
epoch£º651	 i:7 	 global-step:13027	 l-p:0.18373864889144897
epoch£º651	 i:8 	 global-step:13028	 l-p:0.2394295483827591
epoch£º651	 i:9 	 global-step:13029	 l-p:0.09958217293024063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0404, 4.9274, 4.9687],
        [5.0404, 5.0286, 5.0392],
        [5.0404, 5.0404, 5.0404],
        [5.0404, 5.0391, 5.0404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.1059347614645958 
model_pd.l_d.mean(): -20.400827407836914 
model_pd.lagr.mean(): -20.294893264770508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4509], device='cuda:0')), ('power', tensor([-21.2493], device='cuda:0'))])
epoch£º652	 i:0 	 global-step:13040	 l-p:0.1059347614645958
epoch£º652	 i:1 	 global-step:13041	 l-p:0.15920355916023254
epoch£º652	 i:2 	 global-step:13042	 l-p:0.12616021931171417
epoch£º652	 i:3 	 global-step:13043	 l-p:0.1560748666524887
epoch£º652	 i:4 	 global-step:13044	 l-p:0.08170165121555328
epoch£º652	 i:5 	 global-step:13045	 l-p:0.17970210313796997
epoch£º652	 i:6 	 global-step:13046	 l-p:0.1520085632801056
epoch£º652	 i:7 	 global-step:13047	 l-p:0.15710769593715668
epoch£º652	 i:8 	 global-step:13048	 l-p:0.1255011409521103
epoch£º652	 i:9 	 global-step:13049	 l-p:0.10240446776151657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1232, 5.2476, 5.0479],
        [5.1232, 5.0226, 4.7785],
        [5.1232, 5.2554, 5.0593],
        [5.1232, 5.0564, 5.0968]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.16570386290550232 
model_pd.l_d.mean(): -20.02171516418457 
model_pd.lagr.mean(): -19.85601043701172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4761], device='cuda:0')), ('power', tensor([-20.8892], device='cuda:0'))])
epoch£º653	 i:0 	 global-step:13060	 l-p:0.16570386290550232
epoch£º653	 i:1 	 global-step:13061	 l-p:0.1192881390452385
epoch£º653	 i:2 	 global-step:13062	 l-p:0.14688636362552643
epoch£º653	 i:3 	 global-step:13063	 l-p:0.12505421042442322
epoch£º653	 i:4 	 global-step:13064	 l-p:0.1404106318950653
epoch£º653	 i:5 	 global-step:13065	 l-p:-0.009969181381165981
epoch£º653	 i:6 	 global-step:13066	 l-p:0.08380431681871414
epoch£º653	 i:7 	 global-step:13067	 l-p:0.12850533425807953
epoch£º653	 i:8 	 global-step:13068	 l-p:0.1157483160495758
epoch£º653	 i:9 	 global-step:13069	 l-p:0.06250818073749542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1441, 5.2521, 5.0449],
        [5.1441, 4.9868, 4.9908],
        [5.1441, 5.1441, 5.1441],
        [5.1441, 4.9693, 4.9415]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): 0.12262218445539474 
model_pd.l_d.mean(): -19.506471633911133 
model_pd.lagr.mean(): -19.38385009765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4880], device='cuda:0')), ('power', tensor([-20.3766], device='cuda:0'))])
epoch£º654	 i:0 	 global-step:13080	 l-p:0.12262218445539474
epoch£º654	 i:1 	 global-step:13081	 l-p:0.04085180163383484
epoch£º654	 i:2 	 global-step:13082	 l-p:0.14116238057613373
epoch£º654	 i:3 	 global-step:13083	 l-p:0.08478938788175583
epoch£º654	 i:4 	 global-step:13084	 l-p:0.13704745471477509
epoch£º654	 i:5 	 global-step:13085	 l-p:0.0660548210144043
epoch£º654	 i:6 	 global-step:13086	 l-p:0.1278679221868515
epoch£º654	 i:7 	 global-step:13087	 l-p:0.11056022346019745
epoch£º654	 i:8 	 global-step:13088	 l-p:0.25252845883369446
epoch£º654	 i:9 	 global-step:13089	 l-p:0.13548819720745087
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4043,  0.2990,  1.0000,  0.2211,
          1.0000,  0.7394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]], device='cuda:0')
 pt:tensor([[5.1715, 4.9859, 4.8454],
        [5.1715, 5.0138, 4.8098],
        [5.1715, 4.9934, 4.9523],
        [5.1715, 5.0441, 5.0754]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.11322519183158875 
model_pd.l_d.mean(): -20.379478454589844 
model_pd.lagr.mean(): -20.266252517700195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4292], device='cuda:0')), ('power', tensor([-21.2051], device='cuda:0'))])
epoch£º655	 i:0 	 global-step:13100	 l-p:0.11322519183158875
epoch£º655	 i:1 	 global-step:13101	 l-p:0.057395510375499725
epoch£º655	 i:2 	 global-step:13102	 l-p:0.14160236716270447
epoch£º655	 i:3 	 global-step:13103	 l-p:0.1737382709980011
epoch£º655	 i:4 	 global-step:13104	 l-p:0.02938268519937992
epoch£º655	 i:5 	 global-step:13105	 l-p:0.12885092198848724
epoch£º655	 i:6 	 global-step:13106	 l-p:0.1297564059495926
epoch£º655	 i:7 	 global-step:13107	 l-p:0.10490436106920242
epoch£º655	 i:8 	 global-step:13108	 l-p:-0.5387887358665466
epoch£º655	 i:9 	 global-step:13109	 l-p:0.12279430031776428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1324, 5.1118, 5.1292],
        [5.1324, 5.0997, 5.1252],
        [5.1324, 5.1176, 5.1306],
        [5.1324, 5.0216, 4.7799]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.11764602363109589 
model_pd.l_d.mean(): -20.43205451965332 
model_pd.lagr.mean(): -20.314409255981445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4300], device='cuda:0')), ('power', tensor([-21.2595], device='cuda:0'))])
epoch£º656	 i:0 	 global-step:13120	 l-p:0.11764602363109589
epoch£º656	 i:1 	 global-step:13121	 l-p:0.12970979511737823
epoch£º656	 i:2 	 global-step:13122	 l-p:0.03313840180635452
epoch£º656	 i:3 	 global-step:13123	 l-p:0.13458040356636047
epoch£º656	 i:4 	 global-step:13124	 l-p:0.13853417336940765
epoch£º656	 i:5 	 global-step:13125	 l-p:0.2156848907470703
epoch£º656	 i:6 	 global-step:13126	 l-p:0.14484333992004395
epoch£º656	 i:7 	 global-step:13127	 l-p:0.055618543177843094
epoch£º656	 i:8 	 global-step:13128	 l-p:0.13744458556175232
epoch£º656	 i:9 	 global-step:13129	 l-p:0.11969999223947525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3156,  0.2149,  1.0000,  0.1463,
          1.0000,  0.6809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[5.0939, 4.9142, 4.8856],
        [5.0939, 4.8976, 4.7738],
        [5.0939, 5.0758, 4.8274],
        [5.0939, 4.9927, 4.7442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.1407250016927719 
model_pd.l_d.mean(): -20.275081634521484 
model_pd.lagr.mean(): -20.134357452392578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4775], device='cuda:0')), ('power', tensor([-21.1487], device='cuda:0'))])
epoch£º657	 i:0 	 global-step:13140	 l-p:0.1407250016927719
epoch£º657	 i:1 	 global-step:13141	 l-p:0.08567004650831223
epoch£º657	 i:2 	 global-step:13142	 l-p:0.16963092982769012
epoch£º657	 i:3 	 global-step:13143	 l-p:0.14087195694446564
epoch£º657	 i:4 	 global-step:13144	 l-p:0.11625634878873825
epoch£º657	 i:5 	 global-step:13145	 l-p:0.10079354792833328
epoch£º657	 i:6 	 global-step:13146	 l-p:0.1362517923116684
epoch£º657	 i:7 	 global-step:13147	 l-p:0.12436863780021667
epoch£º657	 i:8 	 global-step:13148	 l-p:0.14289212226867676
epoch£º657	 i:9 	 global-step:13149	 l-p:-0.0035763883497565985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1247, 5.1246, 5.1247],
        [5.1247, 4.9345, 4.7850],
        [5.1247, 4.9376, 4.8811],
        [5.1247, 4.9336, 4.8579]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): 0.07605279982089996 
model_pd.l_d.mean(): -20.06346321105957 
model_pd.lagr.mean(): -19.987409591674805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4811], device='cuda:0')), ('power', tensor([-20.9369], device='cuda:0'))])
epoch£º658	 i:0 	 global-step:13160	 l-p:0.07605279982089996
epoch£º658	 i:1 	 global-step:13161	 l-p:0.149159774184227
epoch£º658	 i:2 	 global-step:13162	 l-p:0.16161656379699707
epoch£º658	 i:3 	 global-step:13163	 l-p:0.10251771658658981
epoch£º658	 i:4 	 global-step:13164	 l-p:0.13760937750339508
epoch£º658	 i:5 	 global-step:13165	 l-p:0.10808268189430237
epoch£º658	 i:6 	 global-step:13166	 l-p:0.0018573426641523838
epoch£º658	 i:7 	 global-step:13167	 l-p:0.12768451869487762
epoch£º658	 i:8 	 global-step:13168	 l-p:0.1491003930568695
epoch£º658	 i:9 	 global-step:13169	 l-p:0.1245236024260521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1219, 5.0925, 4.8432],
        [5.1219, 5.1216, 5.1219],
        [5.1219, 4.9273, 4.8109],
        [5.1219, 5.1210, 5.1219]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.12207488715648651 
model_pd.l_d.mean(): -20.319902420043945 
model_pd.lagr.mean(): -20.19782829284668 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4223], device='cuda:0')), ('power', tensor([-21.1373], device='cuda:0'))])
epoch£º659	 i:0 	 global-step:13180	 l-p:0.12207488715648651
epoch£º659	 i:1 	 global-step:13181	 l-p:0.11041364073753357
epoch£º659	 i:2 	 global-step:13182	 l-p:0.1628751903772354
epoch£º659	 i:3 	 global-step:13183	 l-p:0.13339859247207642
epoch£º659	 i:4 	 global-step:13184	 l-p:0.11832664906978607
epoch£º659	 i:5 	 global-step:13185	 l-p:0.13203014433383942
epoch£º659	 i:6 	 global-step:13186	 l-p:0.10980794578790665
epoch£º659	 i:7 	 global-step:13187	 l-p:0.13566012680530548
epoch£º659	 i:8 	 global-step:13188	 l-p:0.204941064119339
epoch£º659	 i:9 	 global-step:13189	 l-p:0.13279162347316742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0386, 5.2114, 5.0358],
        [5.0386, 4.9181, 4.9581],
        [5.0386, 5.0105, 4.7571],
        [5.0386, 5.0385, 5.0386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.14648652076721191 
model_pd.l_d.mean(): -19.529077529907227 
model_pd.lagr.mean(): -19.382591247558594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5675], device='cuda:0')), ('power', tensor([-20.4820], device='cuda:0'))])
epoch£º660	 i:0 	 global-step:13200	 l-p:0.14648652076721191
epoch£º660	 i:1 	 global-step:13201	 l-p:0.163326233625412
epoch£º660	 i:2 	 global-step:13202	 l-p:0.13815732300281525
epoch£º660	 i:3 	 global-step:13203	 l-p:0.12767508625984192
epoch£º660	 i:4 	 global-step:13204	 l-p:0.17831145226955414
epoch£º660	 i:5 	 global-step:13205	 l-p:0.13359297811985016
epoch£º660	 i:6 	 global-step:13206	 l-p:0.11789610236883163
epoch£º660	 i:7 	 global-step:13207	 l-p:0.12456924468278885
epoch£º660	 i:8 	 global-step:13208	 l-p:0.1250389814376831
epoch£º660	 i:9 	 global-step:13209	 l-p:0.2214975655078888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0448, 5.0439, 5.0448],
        [5.0448, 4.9424, 4.9867],
        [5.0448, 5.0177, 5.0397],
        [5.0448, 4.8964, 4.9194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.13836155831813812 
model_pd.l_d.mean(): -19.816436767578125 
model_pd.lagr.mean(): -19.678075790405273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4916], device='cuda:0')), ('power', tensor([-20.6961], device='cuda:0'))])
epoch£º661	 i:0 	 global-step:13220	 l-p:0.13836155831813812
epoch£º661	 i:1 	 global-step:13221	 l-p:0.13309918344020844
epoch£º661	 i:2 	 global-step:13222	 l-p:0.1577712744474411
epoch£º661	 i:3 	 global-step:13223	 l-p:0.19735029339790344
epoch£º661	 i:4 	 global-step:13224	 l-p:0.11533347517251968
epoch£º661	 i:5 	 global-step:13225	 l-p:0.11074132472276688
epoch£º661	 i:6 	 global-step:13226	 l-p:0.13530991971492767
epoch£º661	 i:7 	 global-step:13227	 l-p:0.17996148765087128
epoch£º661	 i:8 	 global-step:13228	 l-p:0.11025302857160568
epoch£º661	 i:9 	 global-step:13229	 l-p:0.18982379138469696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0517, 5.0517, 5.0517],
        [5.0517, 5.0517, 5.0517],
        [5.0517, 5.0295, 5.0481],
        [5.0517, 4.8755, 4.8635]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.13609661161899567 
model_pd.l_d.mean(): -20.741382598876953 
model_pd.lagr.mean(): -20.60528564453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4066], device='cuda:0')), ('power', tensor([-21.5504], device='cuda:0'))])
epoch£º662	 i:0 	 global-step:13240	 l-p:0.13609661161899567
epoch£º662	 i:1 	 global-step:13241	 l-p:0.10415685176849365
epoch£º662	 i:2 	 global-step:13242	 l-p:0.15003956854343414
epoch£º662	 i:3 	 global-step:13243	 l-p:0.09952154010534286
epoch£º662	 i:4 	 global-step:13244	 l-p:0.10721568763256073
epoch£º662	 i:5 	 global-step:13245	 l-p:0.12815645337104797
epoch£º662	 i:6 	 global-step:13246	 l-p:0.15568873286247253
epoch£º662	 i:7 	 global-step:13247	 l-p:0.15216697752475739
epoch£º662	 i:8 	 global-step:13248	 l-p:0.14386118948459625
epoch£º662	 i:9 	 global-step:13249	 l-p:0.14548657834529877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0973, 5.2161, 5.0120],
        [5.0973, 5.0366, 5.0757],
        [5.0973, 4.8986, 4.7737],
        [5.0973, 5.0843, 5.0958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.1278562694787979 
model_pd.l_d.mean(): -20.424644470214844 
model_pd.lagr.mean(): -20.29678726196289 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4276], device='cuda:0')), ('power', tensor([-21.2495], device='cuda:0'))])
epoch£º663	 i:0 	 global-step:13260	 l-p:0.1278562694787979
epoch£º663	 i:1 	 global-step:13261	 l-p:0.11544030904769897
epoch£º663	 i:2 	 global-step:13262	 l-p:0.07900305837392807
epoch£º663	 i:3 	 global-step:13263	 l-p:0.10635625571012497
epoch£º663	 i:4 	 global-step:13264	 l-p:0.13440106809139252
epoch£º663	 i:5 	 global-step:13265	 l-p:0.19538135826587677
epoch£º663	 i:6 	 global-step:13266	 l-p:0.16817304491996765
epoch£º663	 i:7 	 global-step:13267	 l-p:0.10044078528881073
epoch£º663	 i:8 	 global-step:13268	 l-p:0.10380905121564865
epoch£º663	 i:9 	 global-step:13269	 l-p:0.12206798046827316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1120, 5.1120, 5.1120],
        [5.1120, 4.9175, 4.7674],
        [5.1120, 5.1102, 5.1119],
        [5.1120, 4.9461, 4.9450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.12747259438037872 
model_pd.l_d.mean(): -20.217845916748047 
model_pd.lagr.mean(): -20.090373992919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4683], device='cuda:0')), ('power', tensor([-21.0810], device='cuda:0'))])
epoch£º664	 i:0 	 global-step:13280	 l-p:0.12747259438037872
epoch£º664	 i:1 	 global-step:13281	 l-p:0.15733879804611206
epoch£º664	 i:2 	 global-step:13282	 l-p:0.12483245879411697
epoch£º664	 i:3 	 global-step:13283	 l-p:0.10637478530406952
epoch£º664	 i:4 	 global-step:13284	 l-p:0.12831611931324005
epoch£º664	 i:5 	 global-step:13285	 l-p:0.12683646380901337
epoch£º664	 i:6 	 global-step:13286	 l-p:0.16178306937217712
epoch£º664	 i:7 	 global-step:13287	 l-p:0.12420635670423508
epoch£º664	 i:8 	 global-step:13288	 l-p:0.16587872803211212
epoch£º664	 i:9 	 global-step:13289	 l-p:0.051263611763715744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0695, 5.0672, 5.0694],
        [5.0695, 5.0671, 5.0694],
        [5.0695, 4.8683, 4.7269],
        [5.0695, 4.9949, 5.0380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.09493308514356613 
model_pd.l_d.mean(): -20.245376586914062 
model_pd.lagr.mean(): -20.15044403076172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4946], device='cuda:0')), ('power', tensor([-21.1362], device='cuda:0'))])
epoch£º665	 i:0 	 global-step:13300	 l-p:0.09493308514356613
epoch£º665	 i:1 	 global-step:13301	 l-p:0.13764813542366028
epoch£º665	 i:2 	 global-step:13302	 l-p:0.1788782775402069
epoch£º665	 i:3 	 global-step:13303	 l-p:0.12913385033607483
epoch£º665	 i:4 	 global-step:13304	 l-p:0.12486914545297623
epoch£º665	 i:5 	 global-step:13305	 l-p:0.1364397406578064
epoch£º665	 i:6 	 global-step:13306	 l-p:0.13692809641361237
epoch£º665	 i:7 	 global-step:13307	 l-p:0.17860908806324005
epoch£º665	 i:8 	 global-step:13308	 l-p:0.10143978148698807
epoch£º665	 i:9 	 global-step:13309	 l-p:0.14454691112041473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0584, 5.0584, 5.0584],
        [5.0584, 4.9679, 4.7105],
        [5.0584, 4.9842, 4.7258],
        [5.0584, 4.8952, 4.9037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.12456129491329193 
model_pd.l_d.mean(): -18.730112075805664 
model_pd.lagr.mean(): -18.60555076599121 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5544], device='cuda:0')), ('power', tensor([-19.6545], device='cuda:0'))])
epoch£º666	 i:0 	 global-step:13320	 l-p:0.12456129491329193
epoch£º666	 i:1 	 global-step:13321	 l-p:0.09395305067300797
epoch£º666	 i:2 	 global-step:13322	 l-p:0.1326809525489807
epoch£º666	 i:3 	 global-step:13323	 l-p:0.1556282937526703
epoch£º666	 i:4 	 global-step:13324	 l-p:0.10697881132364273
epoch£º666	 i:5 	 global-step:13325	 l-p:0.1612539291381836
epoch£º666	 i:6 	 global-step:13326	 l-p:0.16078773140907288
epoch£º666	 i:7 	 global-step:13327	 l-p:0.13805422186851501
epoch£º666	 i:8 	 global-step:13328	 l-p:0.14283044636249542
epoch£º666	 i:9 	 global-step:13329	 l-p:0.20456527173519135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2653,  0.1705,  1.0000,  0.1095,
          1.0000,  0.6426, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]], device='cuda:0')
 pt:tensor([[5.0551, 5.5319, 5.5478],
        [5.0551, 4.8562, 4.7902],
        [5.0551, 5.2935, 5.1526],
        [5.0551, 4.8537, 4.6948]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.12938405573368073 
model_pd.l_d.mean(): -20.112873077392578 
model_pd.lagr.mean(): -19.983488082885742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5119], device='cuda:0')), ('power', tensor([-21.0192], device='cuda:0'))])
epoch£º667	 i:0 	 global-step:13340	 l-p:0.12938405573368073
epoch£º667	 i:1 	 global-step:13341	 l-p:0.11500726640224457
epoch£º667	 i:2 	 global-step:13342	 l-p:0.14078854024410248
epoch£º667	 i:3 	 global-step:13343	 l-p:0.17572498321533203
epoch£º667	 i:4 	 global-step:13344	 l-p:0.16397373378276825
epoch£º667	 i:5 	 global-step:13345	 l-p:0.17780841886997223
epoch£º667	 i:6 	 global-step:13346	 l-p:0.11832397431135178
epoch£º667	 i:7 	 global-step:13347	 l-p:0.11106409132480621
epoch£º667	 i:8 	 global-step:13348	 l-p:0.1372455209493637
epoch£º667	 i:9 	 global-step:13349	 l-p:0.11436130106449127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0681, 5.0382, 5.0621],
        [5.0681, 5.0680, 5.0681],
        [5.0681, 4.8810, 4.8496],
        [5.0681, 4.9579, 4.7031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.11664677411317825 
model_pd.l_d.mean(): -19.811290740966797 
model_pd.lagr.mean(): -19.694643020629883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5177], device='cuda:0')), ('power', tensor([-20.7179], device='cuda:0'))])
epoch£º668	 i:0 	 global-step:13360	 l-p:0.11664677411317825
epoch£º668	 i:1 	 global-step:13361	 l-p:0.14087727665901184
epoch£º668	 i:2 	 global-step:13362	 l-p:0.12875406444072723
epoch£º668	 i:3 	 global-step:13363	 l-p:0.15731200575828552
epoch£º668	 i:4 	 global-step:13364	 l-p:0.12083278596401215
epoch£º668	 i:5 	 global-step:13365	 l-p:0.16054345667362213
epoch£º668	 i:6 	 global-step:13366	 l-p:0.11888160556554794
epoch£º668	 i:7 	 global-step:13367	 l-p:0.16431869566440582
epoch£º668	 i:8 	 global-step:13368	 l-p:0.11238143593072891
epoch£º668	 i:9 	 global-step:13369	 l-p:0.24237805604934692
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0413, 4.8838, 4.9015],
        [5.0413, 5.0412, 5.0413],
        [5.0413, 5.0413, 5.0413],
        [5.0413, 4.8403, 4.6676]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.13659518957138062 
model_pd.l_d.mean(): -19.945053100585938 
model_pd.lagr.mean(): -19.80845832824707 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4352], device='cuda:0')), ('power', tensor([-20.7687], device='cuda:0'))])
epoch£º669	 i:0 	 global-step:13380	 l-p:0.13659518957138062
epoch£º669	 i:1 	 global-step:13381	 l-p:0.16438601911067963
epoch£º669	 i:2 	 global-step:13382	 l-p:0.13242901861667633
epoch£º669	 i:3 	 global-step:13383	 l-p:0.1570928394794464
epoch£º669	 i:4 	 global-step:13384	 l-p:0.1690688133239746
epoch£º669	 i:5 	 global-step:13385	 l-p:0.10950671136379242
epoch£º669	 i:6 	 global-step:13386	 l-p:0.15301667153835297
epoch£º669	 i:7 	 global-step:13387	 l-p:0.19095061719417572
epoch£º669	 i:8 	 global-step:13388	 l-p:0.18173620104789734
epoch£º669	 i:9 	 global-step:13389	 l-p:0.11110104620456696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0494, 5.0494, 5.0494],
        [5.0494, 4.9781, 5.0209],
        [5.0494, 5.0479, 5.0494],
        [5.0494, 5.0494, 5.0494]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): 0.24511843919754028 
model_pd.l_d.mean(): -20.45633316040039 
model_pd.lagr.mean(): -20.211214065551758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4819], device='cuda:0')), ('power', tensor([-21.3379], device='cuda:0'))])
epoch£º670	 i:0 	 global-step:13400	 l-p:0.24511843919754028
epoch£º670	 i:1 	 global-step:13401	 l-p:0.11724762618541718
epoch£º670	 i:2 	 global-step:13402	 l-p:0.13853324949741364
epoch£º670	 i:3 	 global-step:13403	 l-p:0.13264065980911255
epoch£º670	 i:4 	 global-step:13404	 l-p:0.152282252907753
epoch£º670	 i:5 	 global-step:13405	 l-p:0.10005307197570801
epoch£º670	 i:6 	 global-step:13406	 l-p:0.15277422964572906
epoch£º670	 i:7 	 global-step:13407	 l-p:0.12536785006523132
epoch£º670	 i:8 	 global-step:13408	 l-p:0.14606904983520508
epoch£º670	 i:9 	 global-step:13409	 l-p:0.1559913456439972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0327, 5.0218, 5.0317],
        [5.0327, 5.0291, 5.0326],
        [5.0327, 4.8352, 4.7865],
        [5.0327, 5.0289, 5.0325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.14308606088161469 
model_pd.l_d.mean(): -20.580419540405273 
model_pd.lagr.mean(): -20.437334060668945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4366], device='cuda:0')), ('power', tensor([-21.4174], device='cuda:0'))])
epoch£º671	 i:0 	 global-step:13420	 l-p:0.14308606088161469
epoch£º671	 i:1 	 global-step:13421	 l-p:0.1768171489238739
epoch£º671	 i:2 	 global-step:13422	 l-p:0.11696622520685196
epoch£º671	 i:3 	 global-step:13423	 l-p:0.16927950084209442
epoch£º671	 i:4 	 global-step:13424	 l-p:0.33756309747695923
epoch£º671	 i:5 	 global-step:13425	 l-p:0.21237659454345703
epoch£º671	 i:6 	 global-step:13426	 l-p:0.09713739156723022
epoch£º671	 i:7 	 global-step:13427	 l-p:0.12409096211194992
epoch£º671	 i:8 	 global-step:13428	 l-p:0.18696101009845734
epoch£º671	 i:9 	 global-step:13429	 l-p:0.15075251460075378
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0208, 4.9993, 5.0175],
        [5.0208, 4.8724, 4.6243],
        [5.0208, 5.0208, 5.0208],
        [5.0208, 4.8309, 4.8040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.3017740845680237 
model_pd.l_d.mean(): -19.972715377807617 
model_pd.lagr.mean(): -19.670940399169922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5175], device='cuda:0')), ('power', tensor([-20.8822], device='cuda:0'))])
epoch£º672	 i:0 	 global-step:13440	 l-p:0.3017740845680237
epoch£º672	 i:1 	 global-step:13441	 l-p:0.13633213937282562
epoch£º672	 i:2 	 global-step:13442	 l-p:0.16136811673641205
epoch£º672	 i:3 	 global-step:13443	 l-p:0.1626535952091217
epoch£º672	 i:4 	 global-step:13444	 l-p:0.10787125676870346
epoch£º672	 i:5 	 global-step:13445	 l-p:0.11907657235860825
epoch£º672	 i:6 	 global-step:13446	 l-p:0.15245138108730316
epoch£º672	 i:7 	 global-step:13447	 l-p:0.1390703320503235
epoch£º672	 i:8 	 global-step:13448	 l-p:0.1203014925122261
epoch£º672	 i:9 	 global-step:13449	 l-p:0.25789305567741394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0186, 5.0183, 5.0186],
        [5.0186, 5.0186, 5.0186],
        [5.0186, 4.9975, 4.7401],
        [5.0186, 5.0079, 5.0176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.13481371104717255 
model_pd.l_d.mean(): -20.090747833251953 
model_pd.lagr.mean(): -19.955934524536133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4701], device='cuda:0')), ('power', tensor([-20.9533], device='cuda:0'))])
epoch£º673	 i:0 	 global-step:13460	 l-p:0.13481371104717255
epoch£º673	 i:1 	 global-step:13461	 l-p:0.16738082468509674
epoch£º673	 i:2 	 global-step:13462	 l-p:0.20222237706184387
epoch£º673	 i:3 	 global-step:13463	 l-p:0.38425979018211365
epoch£º673	 i:4 	 global-step:13464	 l-p:0.13324403762817383
epoch£º673	 i:5 	 global-step:13465	 l-p:0.15942968428134918
epoch£º673	 i:6 	 global-step:13466	 l-p:0.1871957778930664
epoch£º673	 i:7 	 global-step:13467	 l-p:0.09580214321613312
epoch£º673	 i:8 	 global-step:13468	 l-p:0.16444116830825806
epoch£º673	 i:9 	 global-step:13469	 l-p:0.09829734265804291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0314, 4.9182, 4.9625],
        [5.0314, 4.9328, 4.9789],
        [5.0314, 4.8814, 4.9076],
        [5.0314, 4.8206, 4.7019]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.17324937880039215 
model_pd.l_d.mean(): -19.406211853027344 
model_pd.lagr.mean(): -19.232961654663086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5603], device='cuda:0')), ('power', tensor([-20.3494], device='cuda:0'))])
epoch£º674	 i:0 	 global-step:13480	 l-p:0.17324937880039215
epoch£º674	 i:1 	 global-step:13481	 l-p:0.13276919722557068
epoch£º674	 i:2 	 global-step:13482	 l-p:0.10936746001243591
epoch£º674	 i:3 	 global-step:13483	 l-p:0.1348796784877777
epoch£º674	 i:4 	 global-step:13484	 l-p:0.07729017734527588
epoch£º674	 i:5 	 global-step:13485	 l-p:0.1382702738046646
epoch£º674	 i:6 	 global-step:13486	 l-p:0.22383438050746918
epoch£º674	 i:7 	 global-step:13487	 l-p:0.16763123869895935
epoch£º674	 i:8 	 global-step:13488	 l-p:0.1312425583600998
epoch£º674	 i:9 	 global-step:13489	 l-p:0.13945047557353973
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0987, 4.9311, 4.9346],
        [5.0987, 4.8963, 4.8096],
        [5.0987, 5.0036, 5.0490],
        [5.0987, 5.0260, 5.0689]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): 0.10034051537513733 
model_pd.l_d.mean(): -20.004030227661133 
model_pd.lagr.mean(): -19.903690338134766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4664], device='cuda:0')), ('power', tensor([-20.8612], device='cuda:0'))])
epoch£º675	 i:0 	 global-step:13500	 l-p:0.10034051537513733
epoch£º675	 i:1 	 global-step:13501	 l-p:0.1323249191045761
epoch£º675	 i:2 	 global-step:13502	 l-p:0.1332569718360901
epoch£º675	 i:3 	 global-step:13503	 l-p:0.12811177968978882
epoch£º675	 i:4 	 global-step:13504	 l-p:0.15847866237163544
epoch£º675	 i:5 	 global-step:13505	 l-p:0.14942778646945953
epoch£º675	 i:6 	 global-step:13506	 l-p:0.12080071866512299
epoch£º675	 i:7 	 global-step:13507	 l-p:0.12179332226514816
epoch£º675	 i:8 	 global-step:13508	 l-p:0.157301127910614
epoch£º675	 i:9 	 global-step:13509	 l-p:0.040086016058921814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1071, 5.1071, 5.1071],
        [5.1071, 4.9131, 4.7320],
        [5.1071, 5.0950, 5.1059],
        [5.1071, 5.1031, 5.1069]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): 0.10116496682167053 
model_pd.l_d.mean(): -20.29878044128418 
model_pd.lagr.mean(): -20.197614669799805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4463], device='cuda:0')), ('power', tensor([-21.1406], device='cuda:0'))])
epoch£º676	 i:0 	 global-step:13520	 l-p:0.10116496682167053
epoch£º676	 i:1 	 global-step:13521	 l-p:0.14882490038871765
epoch£º676	 i:2 	 global-step:13522	 l-p:0.18518856167793274
epoch£º676	 i:3 	 global-step:13523	 l-p:0.12569651007652283
epoch£º676	 i:4 	 global-step:13524	 l-p:0.1462738960981369
epoch£º676	 i:5 	 global-step:13525	 l-p:0.07698006182909012
epoch£º676	 i:6 	 global-step:13526	 l-p:0.19467806816101074
epoch£º676	 i:7 	 global-step:13527	 l-p:0.13879959285259247
epoch£º676	 i:8 	 global-step:13528	 l-p:0.12128344923257828
epoch£º676	 i:9 	 global-step:13529	 l-p:0.12640032172203064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0594, 5.0594, 5.0594],
        [5.0594, 4.9593, 5.0052],
        [5.0594, 4.8655, 4.8273],
        [5.0594, 5.0429, 5.0573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): 0.1294744908809662 
model_pd.l_d.mean(): -20.43065643310547 
model_pd.lagr.mean(): -20.30118179321289 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4581], device='cuda:0')), ('power', tensor([-21.2871], device='cuda:0'))])
epoch£º677	 i:0 	 global-step:13540	 l-p:0.1294744908809662
epoch£º677	 i:1 	 global-step:13541	 l-p:0.14836564660072327
epoch£º677	 i:2 	 global-step:13542	 l-p:0.11478407680988312
epoch£º677	 i:3 	 global-step:13543	 l-p:0.14359073340892792
epoch£º677	 i:4 	 global-step:13544	 l-p:0.185719832777977
epoch£º677	 i:5 	 global-step:13545	 l-p:0.13095973432064056
epoch£º677	 i:6 	 global-step:13546	 l-p:0.4553048610687256
epoch£º677	 i:7 	 global-step:13547	 l-p:-0.18913470208644867
epoch£º677	 i:8 	 global-step:13548	 l-p:0.1624792069196701
epoch£º677	 i:9 	 global-step:13549	 l-p:0.11232253909111023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9918, 4.9598, 4.9852],
        [4.9918, 4.9902, 4.9917],
        [4.9918, 4.9637, 4.9865],
        [4.9918, 4.9514, 4.9819]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): -0.05414457246661186 
model_pd.l_d.mean(): -19.76665687561035 
model_pd.lagr.mean(): -19.82080078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5689], device='cuda:0')), ('power', tensor([-20.7255], device='cuda:0'))])
epoch£º678	 i:0 	 global-step:13560	 l-p:-0.05414457246661186
epoch£º678	 i:1 	 global-step:13561	 l-p:0.18004028499126434
epoch£º678	 i:2 	 global-step:13562	 l-p:0.13015979528427124
epoch£º678	 i:3 	 global-step:13563	 l-p:0.23520015180110931
epoch£º678	 i:4 	 global-step:13564	 l-p:0.12947586178779602
epoch£º678	 i:5 	 global-step:13565	 l-p:0.13474011421203613
epoch£º678	 i:6 	 global-step:13566	 l-p:0.17118774354457855
epoch£º678	 i:7 	 global-step:13567	 l-p:0.14632421731948853
epoch£º678	 i:8 	 global-step:13568	 l-p:0.14430610835552216
epoch£º678	 i:9 	 global-step:13569	 l-p:0.2120024412870407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0030, 5.0029, 5.0030],
        [5.0030, 4.9538, 4.6886],
        [5.0030, 4.9759, 4.9981],
        [5.0030, 4.9523, 4.9881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.18063373863697052 
model_pd.l_d.mean(): -20.28116798400879 
model_pd.lagr.mean(): -20.100534439086914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4899], device='cuda:0')), ('power', tensor([-21.1679], device='cuda:0'))])
epoch£º679	 i:0 	 global-step:13580	 l-p:0.18063373863697052
epoch£º679	 i:1 	 global-step:13581	 l-p:0.16414138674736023
epoch£º679	 i:2 	 global-step:13582	 l-p:0.16899646818637848
epoch£º679	 i:3 	 global-step:13583	 l-p:0.6623791456222534
epoch£º679	 i:4 	 global-step:13584	 l-p:0.2136198729276657
epoch£º679	 i:5 	 global-step:13585	 l-p:0.11897637695074081
epoch£º679	 i:6 	 global-step:13586	 l-p:0.16066411137580872
epoch£º679	 i:7 	 global-step:13587	 l-p:0.11866166442632675
epoch£º679	 i:8 	 global-step:13588	 l-p:0.12194377928972244
epoch£º679	 i:9 	 global-step:13589	 l-p:0.11423204094171524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0423, 5.0354, 5.0418],
        [5.0423, 5.0398, 5.0422],
        [5.0423, 5.0044, 5.0334],
        [5.0423, 5.2270, 5.0536]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.20378582179546356 
model_pd.l_d.mean(): -20.781606674194336 
model_pd.lagr.mean(): -20.577821731567383 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4238], device='cuda:0')), ('power', tensor([-21.6091], device='cuda:0'))])
epoch£º680	 i:0 	 global-step:13600	 l-p:0.20378582179546356
epoch£º680	 i:1 	 global-step:13601	 l-p:0.14028099179267883
epoch£º680	 i:2 	 global-step:13602	 l-p:0.15090712904930115
epoch£º680	 i:3 	 global-step:13603	 l-p:0.16469718515872955
epoch£º680	 i:4 	 global-step:13604	 l-p:0.18512822687625885
epoch£º680	 i:5 	 global-step:13605	 l-p:0.15023916959762573
epoch£º680	 i:6 	 global-step:13606	 l-p:0.13509975373744965
epoch£º680	 i:7 	 global-step:13607	 l-p:0.11299142241477966
epoch£º680	 i:8 	 global-step:13608	 l-p:0.12289875000715256
epoch£º680	 i:9 	 global-step:13609	 l-p:0.11698652058839798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1056, 4.9262, 4.9133],
        [5.1056, 5.4916, 5.4398],
        [5.1056, 5.1047, 5.1056],
        [5.1056, 5.1047, 5.1056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.09998030215501785 
model_pd.l_d.mean(): -20.21065330505371 
model_pd.lagr.mean(): -20.110673904418945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4597], device='cuda:0')), ('power', tensor([-21.0647], device='cuda:0'))])
epoch£º681	 i:0 	 global-step:13620	 l-p:0.09998030215501785
epoch£º681	 i:1 	 global-step:13621	 l-p:0.13151496648788452
epoch£º681	 i:2 	 global-step:13622	 l-p:0.16560271382331848
epoch£º681	 i:3 	 global-step:13623	 l-p:0.1434008926153183
epoch£º681	 i:4 	 global-step:13624	 l-p:0.14361359179019928
epoch£º681	 i:5 	 global-step:13625	 l-p:0.07996822893619537
epoch£º681	 i:6 	 global-step:13626	 l-p:0.013912939466536045
epoch£º681	 i:7 	 global-step:13627	 l-p:0.1437574028968811
epoch£º681	 i:8 	 global-step:13628	 l-p:0.1533978432416916
epoch£º681	 i:9 	 global-step:13629	 l-p:0.1031462624669075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1190, 5.0236, 5.0691],
        [5.1190, 4.9960, 5.0360],
        [5.1190, 5.1166, 5.1190],
        [5.1190, 5.1040, 5.1172]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.13618142902851105 
model_pd.l_d.mean(): -20.367937088012695 
model_pd.lagr.mean(): -20.23175621032715 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4522], device='cuda:0')), ('power', tensor([-21.2172], device='cuda:0'))])
epoch£º682	 i:0 	 global-step:13640	 l-p:0.13618142902851105
epoch£º682	 i:1 	 global-step:13641	 l-p:0.12147046625614166
epoch£º682	 i:2 	 global-step:13642	 l-p:0.11979426443576813
epoch£º682	 i:3 	 global-step:13643	 l-p:0.15789684653282166
epoch£º682	 i:4 	 global-step:13644	 l-p:0.15769444406032562
epoch£º682	 i:5 	 global-step:13645	 l-p:0.12633495032787323
epoch£º682	 i:6 	 global-step:13646	 l-p:0.13416001200675964
epoch£º682	 i:7 	 global-step:13647	 l-p:0.1375550925731659
epoch£º682	 i:8 	 global-step:13648	 l-p:0.0936371311545372
epoch£º682	 i:9 	 global-step:13649	 l-p:0.2668103873729706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0217, 4.9018, 4.9455],
        [5.0217, 4.9853, 5.0135],
        [5.0217, 5.0217, 5.0217],
        [5.0217, 4.8683, 4.8944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.14292500913143158 
model_pd.l_d.mean(): -20.68648910522461 
model_pd.lagr.mean(): -20.543563842773438 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4500], device='cuda:0')), ('power', tensor([-21.5394], device='cuda:0'))])
epoch£º683	 i:0 	 global-step:13660	 l-p:0.14292500913143158
epoch£º683	 i:1 	 global-step:13661	 l-p:0.341611385345459
epoch£º683	 i:2 	 global-step:13662	 l-p:0.10908611863851547
epoch£º683	 i:3 	 global-step:13663	 l-p:0.12649711966514587
epoch£º683	 i:4 	 global-step:13664	 l-p:0.1807064414024353
epoch£º683	 i:5 	 global-step:13665	 l-p:0.122334785759449
epoch£º683	 i:6 	 global-step:13666	 l-p:-0.8558655977249146
epoch£º683	 i:7 	 global-step:13667	 l-p:0.1492924690246582
epoch£º683	 i:8 	 global-step:13668	 l-p:0.17320877313613892
epoch£º683	 i:9 	 global-step:13669	 l-p:0.1075388491153717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0207, 4.8604, 4.6126],
        [5.0207, 5.0207, 5.0207],
        [5.0207, 5.2496, 5.1012],
        [5.0207, 5.0185, 5.0206]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): 0.11753400415182114 
model_pd.l_d.mean(): -20.19034194946289 
model_pd.lagr.mean(): -20.07280731201172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4926], device='cuda:0')), ('power', tensor([-21.0781], device='cuda:0'))])
epoch£º684	 i:0 	 global-step:13680	 l-p:0.11753400415182114
epoch£º684	 i:1 	 global-step:13681	 l-p:0.11920333653688431
epoch£º684	 i:2 	 global-step:13682	 l-p:0.1286182701587677
epoch£º684	 i:3 	 global-step:13683	 l-p:0.18205386400222778
epoch£º684	 i:4 	 global-step:13684	 l-p:0.11449220031499863
epoch£º684	 i:5 	 global-step:13685	 l-p:0.1990395337343216
epoch£º684	 i:6 	 global-step:13686	 l-p:0.1898089498281479
epoch£º684	 i:7 	 global-step:13687	 l-p:0.2248431295156479
epoch£º684	 i:8 	 global-step:13688	 l-p:0.1281648576259613
epoch£º684	 i:9 	 global-step:13689	 l-p:0.24907392263412476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0464, 4.9670, 5.0121],
        [5.0464, 5.4062, 5.3386],
        [5.0464, 4.8328, 4.7063],
        [5.0464, 5.0463, 5.0464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.17175261676311493 
model_pd.l_d.mean(): -20.45477294921875 
model_pd.lagr.mean(): -20.28302001953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4722], device='cuda:0')), ('power', tensor([-21.3263], device='cuda:0'))])
epoch£º685	 i:0 	 global-step:13700	 l-p:0.17175261676311493
epoch£º685	 i:1 	 global-step:13701	 l-p:0.15848855674266815
epoch£º685	 i:2 	 global-step:13702	 l-p:0.1129130944609642
epoch£º685	 i:3 	 global-step:13703	 l-p:0.22303242981433868
epoch£º685	 i:4 	 global-step:13704	 l-p:0.1265667825937271
epoch£º685	 i:5 	 global-step:13705	 l-p:0.1138143539428711
epoch£º685	 i:6 	 global-step:13706	 l-p:0.06713663786649704
epoch£º685	 i:7 	 global-step:13707	 l-p:0.1619441956281662
epoch£º685	 i:8 	 global-step:13708	 l-p:0.10013670474290848
epoch£º685	 i:9 	 global-step:13709	 l-p:0.12637287378311157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1176, 5.1173, 5.1176],
        [5.1176, 5.1171, 5.1176],
        [5.1176, 5.0867, 5.1114],
        [5.1176, 5.3504, 5.2012]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.07028020173311234 
model_pd.l_d.mean(): -18.89109230041504 
model_pd.lagr.mean(): -18.820812225341797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5823], device='cuda:0')), ('power', tensor([-19.8474], device='cuda:0'))])
epoch£º686	 i:0 	 global-step:13720	 l-p:0.07028020173311234
epoch£º686	 i:1 	 global-step:13721	 l-p:0.13753461837768555
epoch£º686	 i:2 	 global-step:13722	 l-p:0.09716251492500305
epoch£º686	 i:3 	 global-step:13723	 l-p:0.1482447236776352
epoch£º686	 i:4 	 global-step:13724	 l-p:0.11132244020700455
epoch£º686	 i:5 	 global-step:13725	 l-p:0.11216196417808533
epoch£º686	 i:6 	 global-step:13726	 l-p:0.095835842192173
epoch£º686	 i:7 	 global-step:13727	 l-p:0.14060847461223602
epoch£º686	 i:8 	 global-step:13728	 l-p:0.13556024432182312
epoch£º686	 i:9 	 global-step:13729	 l-p:0.14046183228492737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4057,  0.3004,  1.0000,  0.2224,
          1.0000,  0.7403, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6689,  0.5850,  1.0000,  0.5116,
          1.0000,  0.8745, 31.6228]], device='cuda:0')
 pt:tensor([[5.1405, 5.3411, 5.1729],
        [5.1405, 4.9621, 4.7495],
        [5.1405, 4.9492, 4.9091],
        [5.1405, 5.2260, 5.0017]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): 0.07434231787919998 
model_pd.l_d.mean(): -19.59699821472168 
model_pd.lagr.mean(): -19.522655487060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5473], device='cuda:0')), ('power', tensor([-20.5303], device='cuda:0'))])
epoch£º687	 i:0 	 global-step:13740	 l-p:0.07434231787919998
epoch£º687	 i:1 	 global-step:13741	 l-p:0.17399361729621887
epoch£º687	 i:2 	 global-step:13742	 l-p:0.13769225776195526
epoch£º687	 i:3 	 global-step:13743	 l-p:0.13091856241226196
epoch£º687	 i:4 	 global-step:13744	 l-p:0.14981435239315033
epoch£º687	 i:5 	 global-step:13745	 l-p:0.10528048127889633
epoch£º687	 i:6 	 global-step:13746	 l-p:0.08860122412443161
epoch£º687	 i:7 	 global-step:13747	 l-p:0.1393774449825287
epoch£º687	 i:8 	 global-step:13748	 l-p:0.1407013237476349
epoch£º687	 i:9 	 global-step:13749	 l-p:0.01545076072216034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1249, 5.1249, 5.1249],
        [5.1249, 4.9253, 4.7503],
        [5.1249, 4.9673, 4.9851],
        [5.1249, 5.1249, 5.1249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): 0.043862275779247284 
model_pd.l_d.mean(): -19.318845748901367 
model_pd.lagr.mean(): -19.27498435974121 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5247], device='cuda:0')), ('power', tensor([-20.2235], device='cuda:0'))])
epoch£º688	 i:0 	 global-step:13760	 l-p:0.043862275779247284
epoch£º688	 i:1 	 global-step:13761	 l-p:0.1335793137550354
epoch£º688	 i:2 	 global-step:13762	 l-p:0.14930471777915955
epoch£º688	 i:3 	 global-step:13763	 l-p:0.17524009943008423
epoch£º688	 i:4 	 global-step:13764	 l-p:0.1497916728258133
epoch£º688	 i:5 	 global-step:13765	 l-p:0.11816629767417908
epoch£º688	 i:6 	 global-step:13766	 l-p:0.02570987492799759
epoch£º688	 i:7 	 global-step:13767	 l-p:0.13249154388904572
epoch£º688	 i:8 	 global-step:13768	 l-p:0.12167128920555115
epoch£º688	 i:9 	 global-step:13769	 l-p:0.1286522001028061
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1176, 5.0520, 5.0934],
        [5.1176, 5.1104, 5.1171],
        [5.1176, 5.1176, 5.1176],
        [5.1176, 5.1176, 5.1176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.17137911915779114 
model_pd.l_d.mean(): -20.503259658813477 
model_pd.lagr.mean(): -20.331880569458008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4394], device='cuda:0')), ('power', tensor([-21.3417], device='cuda:0'))])
epoch£º689	 i:0 	 global-step:13780	 l-p:0.17137911915779114
epoch£º689	 i:1 	 global-step:13781	 l-p:0.13443602621555328
epoch£º689	 i:2 	 global-step:13782	 l-p:0.12038419395685196
epoch£º689	 i:3 	 global-step:13783	 l-p:0.1245277002453804
epoch£º689	 i:4 	 global-step:13784	 l-p:0.11536737531423569
epoch£º689	 i:5 	 global-step:13785	 l-p:0.1191495805978775
epoch£º689	 i:6 	 global-step:13786	 l-p:0.17126032710075378
epoch£º689	 i:7 	 global-step:13787	 l-p:0.08651689440011978
epoch£º689	 i:8 	 global-step:13788	 l-p:0.20435021817684174
epoch£º689	 i:9 	 global-step:13789	 l-p:0.12546630203723907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0565, 5.0564, 5.0565],
        [5.0565, 5.4717, 5.4401],
        [5.0565, 5.0559, 5.0565],
        [5.0565, 5.0565, 5.0565]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.13738799095153809 
model_pd.l_d.mean(): -20.442564010620117 
model_pd.lagr.mean(): -20.30517578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4682], device='cuda:0')), ('power', tensor([-21.3098], device='cuda:0'))])
epoch£º690	 i:0 	 global-step:13800	 l-p:0.13738799095153809
epoch£º690	 i:1 	 global-step:13801	 l-p:0.07305533438920975
epoch£º690	 i:2 	 global-step:13802	 l-p:0.1235065832734108
epoch£º690	 i:3 	 global-step:13803	 l-p:0.24375636875629425
epoch£º690	 i:4 	 global-step:13804	 l-p:0.1302918642759323
epoch£º690	 i:5 	 global-step:13805	 l-p:0.1531267613172531
epoch£º690	 i:6 	 global-step:13806	 l-p:0.206802099943161
epoch£º690	 i:7 	 global-step:13807	 l-p:0.15488514304161072
epoch£º690	 i:8 	 global-step:13808	 l-p:0.1349058300256729
epoch£º690	 i:9 	 global-step:13809	 l-p:0.13189153373241425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0691, 5.0691, 5.0691],
        [5.0691, 4.9631, 5.0094],
        [5.0691, 4.9656, 4.6996],
        [5.0691, 5.0668, 5.0690]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.10227296501398087 
model_pd.l_d.mean(): -19.422183990478516 
model_pd.lagr.mean(): -19.31991195678711 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5134], device='cuda:0')), ('power', tensor([-20.3171], device='cuda:0'))])
epoch£º691	 i:0 	 global-step:13820	 l-p:0.10227296501398087
epoch£º691	 i:1 	 global-step:13821	 l-p:0.09475310891866684
epoch£º691	 i:2 	 global-step:13822	 l-p:0.16243262588977814
epoch£º691	 i:3 	 global-step:13823	 l-p:0.16155791282653809
epoch£º691	 i:4 	 global-step:13824	 l-p:0.18114954233169556
epoch£º691	 i:5 	 global-step:13825	 l-p:0.09585288912057877
epoch£º691	 i:6 	 global-step:13826	 l-p:0.19788245856761932
epoch£º691	 i:7 	 global-step:13827	 l-p:0.10930940508842468
epoch£º691	 i:8 	 global-step:13828	 l-p:0.11622542887926102
epoch£º691	 i:9 	 global-step:13829	 l-p:0.11818879842758179
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1047, 5.1044, 5.1047],
        [5.1047, 5.1041, 5.1047],
        [5.1047, 5.0411, 5.0820],
        [5.1047, 5.1047, 5.1047]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.07921828329563141 
model_pd.l_d.mean(): -20.67858123779297 
model_pd.lagr.mean(): -20.599363327026367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4050], device='cuda:0')), ('power', tensor([-21.4848], device='cuda:0'))])
epoch£º692	 i:0 	 global-step:13840	 l-p:0.07921828329563141
epoch£º692	 i:1 	 global-step:13841	 l-p:0.12512584030628204
epoch£º692	 i:2 	 global-step:13842	 l-p:0.15976962447166443
epoch£º692	 i:3 	 global-step:13843	 l-p:0.1429133415222168
epoch£º692	 i:4 	 global-step:13844	 l-p:0.10648305714130402
epoch£º692	 i:5 	 global-step:13845	 l-p:0.120236836373806
epoch£º692	 i:6 	 global-step:13846	 l-p:0.14536315202713013
epoch£º692	 i:7 	 global-step:13847	 l-p:0.12362559139728546
epoch£º692	 i:8 	 global-step:13848	 l-p:0.2049766629934311
epoch£º692	 i:9 	 global-step:13849	 l-p:0.11425011605024338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0598, 5.0561, 5.0597],
        [5.0598, 5.1633, 4.9458],
        [5.0598, 5.4183, 5.3477],
        [5.0598, 5.4588, 5.4153]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.09835334867238998 
model_pd.l_d.mean(): -19.405521392822266 
model_pd.lagr.mean(): -19.30716896057129 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5824], device='cuda:0')), ('power', tensor([-20.3716], device='cuda:0'))])
epoch£º693	 i:0 	 global-step:13860	 l-p:0.09835334867238998
epoch£º693	 i:1 	 global-step:13861	 l-p:0.07679872959852219
epoch£º693	 i:2 	 global-step:13862	 l-p:0.15907615423202515
epoch£º693	 i:3 	 global-step:13863	 l-p:0.16101761162281036
epoch£º693	 i:4 	 global-step:13864	 l-p:0.1820281744003296
epoch£º693	 i:5 	 global-step:13865	 l-p:0.13208957016468048
epoch£º693	 i:6 	 global-step:13866	 l-p:0.12373393028974533
epoch£º693	 i:7 	 global-step:13867	 l-p:0.21326610445976257
epoch£º693	 i:8 	 global-step:13868	 l-p:0.2266330122947693
epoch£º693	 i:9 	 global-step:13869	 l-p:0.11684305965900421
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0519, 4.9871, 5.0287],
        [5.0519, 4.8383, 4.6693],
        [5.0519, 5.0518, 5.0519],
        [5.0519, 5.0519, 5.0519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.10251720249652863 
model_pd.l_d.mean(): -19.047203063964844 
model_pd.lagr.mean(): -18.944684982299805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5579], device='cuda:0')), ('power', tensor([-19.9812], device='cuda:0'))])
epoch£º694	 i:0 	 global-step:13880	 l-p:0.10251720249652863
epoch£º694	 i:1 	 global-step:13881	 l-p:0.18300215899944305
epoch£º694	 i:2 	 global-step:13882	 l-p:0.13451692461967468
epoch£º694	 i:3 	 global-step:13883	 l-p:0.12868881225585938
epoch£º694	 i:4 	 global-step:13884	 l-p:0.11630607396364212
epoch£º694	 i:5 	 global-step:13885	 l-p:0.19463102519512177
epoch£º694	 i:6 	 global-step:13886	 l-p:0.14685125648975372
epoch£º694	 i:7 	 global-step:13887	 l-p:0.15376050770282745
epoch£º694	 i:8 	 global-step:13888	 l-p:0.1318514347076416
epoch£º694	 i:9 	 global-step:13889	 l-p:0.13761234283447266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0808, 4.8690, 4.7818],
        [5.0808, 4.9230, 4.6740],
        [5.0808, 4.9315, 4.6777],
        [5.0808, 5.0805, 5.0808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.12602470815181732 
model_pd.l_d.mean(): -19.3961238861084 
model_pd.lagr.mean(): -19.270099639892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4971], device='cuda:0')), ('power', tensor([-20.2737], device='cuda:0'))])
epoch£º695	 i:0 	 global-step:13900	 l-p:0.12602470815181732
epoch£º695	 i:1 	 global-step:13901	 l-p:0.1810988336801529
epoch£º695	 i:2 	 global-step:13902	 l-p:0.09777767956256866
epoch£º695	 i:3 	 global-step:13903	 l-p:0.13377948105335236
epoch£º695	 i:4 	 global-step:13904	 l-p:0.16970793902873993
epoch£º695	 i:5 	 global-step:13905	 l-p:0.12112899869680405
epoch£º695	 i:6 	 global-step:13906	 l-p:0.13731230795383453
epoch£º695	 i:7 	 global-step:13907	 l-p:0.09897597879171371
epoch£º695	 i:8 	 global-step:13908	 l-p:0.13160467147827148
epoch£º695	 i:9 	 global-step:13909	 l-p:0.1331208050251007
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0971, 5.0882, 5.0963],
        [5.0971, 4.9414, 4.6927],
        [5.0971, 5.0524, 5.0852],
        [5.0971, 5.4446, 5.3648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.19082023203372955 
model_pd.l_d.mean(): -20.6401424407959 
model_pd.lagr.mean(): -20.449321746826172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4267], device='cuda:0')), ('power', tensor([-21.4680], device='cuda:0'))])
epoch£º696	 i:0 	 global-step:13920	 l-p:0.19082023203372955
epoch£º696	 i:1 	 global-step:13921	 l-p:0.11858271062374115
epoch£º696	 i:2 	 global-step:13922	 l-p:0.16423889994621277
epoch£º696	 i:3 	 global-step:13923	 l-p:0.11917860805988312
epoch£º696	 i:4 	 global-step:13924	 l-p:0.14153920114040375
epoch£º696	 i:5 	 global-step:13925	 l-p:0.18530872464179993
epoch£º696	 i:6 	 global-step:13926	 l-p:0.12782688438892365
epoch£º696	 i:7 	 global-step:13927	 l-p:0.11717765778303146
epoch£º696	 i:8 	 global-step:13928	 l-p:0.13672448694705963
epoch£º696	 i:9 	 global-step:13929	 l-p:0.11139706522226334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0284, 5.0284, 5.0284],
        [5.0284, 5.0283, 5.0284],
        [5.0284, 5.1061, 4.8764],
        [5.0284, 5.0275, 5.0284]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.13213658332824707 
model_pd.l_d.mean(): -19.958051681518555 
model_pd.lagr.mean(): -19.82591438293457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4657], device='cuda:0')), ('power', tensor([-20.8135], device='cuda:0'))])
epoch£º697	 i:0 	 global-step:13940	 l-p:0.13213658332824707
epoch£º697	 i:1 	 global-step:13941	 l-p:0.16469725966453552
epoch£º697	 i:2 	 global-step:13942	 l-p:1.0583579540252686
epoch£º697	 i:3 	 global-step:13943	 l-p:0.12686459720134735
epoch£º697	 i:4 	 global-step:13944	 l-p:0.22626182436943054
epoch£º697	 i:5 	 global-step:13945	 l-p:0.14686207473278046
epoch£º697	 i:6 	 global-step:13946	 l-p:0.17787504196166992
epoch£º697	 i:7 	 global-step:13947	 l-p:0.15171460807323456
epoch£º697	 i:8 	 global-step:13948	 l-p:0.14022941887378693
epoch£º697	 i:9 	 global-step:13949	 l-p:0.167362779378891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0428, 5.0059, 5.0344],
        [5.0428, 5.0402, 5.0427],
        [5.0428, 4.8548, 4.8445],
        [5.0428, 4.8227, 4.7022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.1441577672958374 
model_pd.l_d.mean(): -20.419147491455078 
model_pd.lagr.mean(): -20.27499008178711 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4603], device='cuda:0')), ('power', tensor([-21.2777], device='cuda:0'))])
epoch£º698	 i:0 	 global-step:13960	 l-p:0.1441577672958374
epoch£º698	 i:1 	 global-step:13961	 l-p:0.11785753071308136
epoch£º698	 i:2 	 global-step:13962	 l-p:0.20298457145690918
epoch£º698	 i:3 	 global-step:13963	 l-p:0.14787407219409943
epoch£º698	 i:4 	 global-step:13964	 l-p:0.13135060667991638
epoch£º698	 i:5 	 global-step:13965	 l-p:0.16881978511810303
epoch£º698	 i:6 	 global-step:13966	 l-p:0.25244033336639404
epoch£º698	 i:7 	 global-step:13967	 l-p:0.16363303363323212
epoch£º698	 i:8 	 global-step:13968	 l-p:0.09405362606048584
epoch£º698	 i:9 	 global-step:13969	 l-p:0.11926215887069702
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0651, 5.0580, 5.0646],
        [5.0651, 5.0008, 5.0424],
        [5.0651, 4.8472, 4.7042],
        [5.0651, 5.0651, 5.0651]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): 0.13830433785915375 
model_pd.l_d.mean(): -20.2783260345459 
model_pd.lagr.mean(): -20.14002227783203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4811], device='cuda:0')), ('power', tensor([-21.1558], device='cuda:0'))])
epoch£º699	 i:0 	 global-step:13980	 l-p:0.13830433785915375
epoch£º699	 i:1 	 global-step:13981	 l-p:0.12327077239751816
epoch£º699	 i:2 	 global-step:13982	 l-p:0.14868023991584778
epoch£º699	 i:3 	 global-step:13983	 l-p:0.132522851228714
epoch£º699	 i:4 	 global-step:13984	 l-p:0.1774577796459198
epoch£º699	 i:5 	 global-step:13985	 l-p:0.06448650360107422
epoch£º699	 i:6 	 global-step:13986	 l-p:0.10188005864620209
epoch£º699	 i:7 	 global-step:13987	 l-p:0.17967408895492554
epoch£º699	 i:8 	 global-step:13988	 l-p:0.14728733897209167
epoch£º699	 i:9 	 global-step:13989	 l-p:0.1344929039478302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1244, 5.1193, 5.1241],
        [5.1244, 4.9162, 4.8393],
        [5.1244, 4.9319, 4.9036],
        [5.1244, 5.0589, 5.1005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): 0.11032779514789581 
model_pd.l_d.mean(): -20.275428771972656 
model_pd.lagr.mean(): -20.16510009765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4310], device='cuda:0')), ('power', tensor([-21.1010], device='cuda:0'))])
epoch£º700	 i:0 	 global-step:14000	 l-p:0.11032779514789581
epoch£º700	 i:1 	 global-step:14001	 l-p:0.12168317288160324
epoch£º700	 i:2 	 global-step:14002	 l-p:0.08219316601753235
epoch£º700	 i:3 	 global-step:14003	 l-p:0.12444537878036499
epoch£º700	 i:4 	 global-step:14004	 l-p:0.12324600666761398
epoch£º700	 i:5 	 global-step:14005	 l-p:0.1595001071691513
epoch£º700	 i:6 	 global-step:14006	 l-p:0.08944560587406158
epoch£º700	 i:7 	 global-step:14007	 l-p:0.14687982201576233
epoch£º700	 i:8 	 global-step:14008	 l-p:0.11777389049530029
epoch£º700	 i:9 	 global-step:14009	 l-p:0.15683990716934204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1275, 5.1202, 5.1269],
        [5.1275, 5.0475, 5.0929],
        [5.1275, 5.1275, 5.1275],
        [5.1275, 4.9606, 4.9730]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): 0.12445606291294098 
model_pd.l_d.mean(): -19.966249465942383 
model_pd.lagr.mean(): -19.841793060302734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4852], device='cuda:0')), ('power', tensor([-20.8421], device='cuda:0'))])
epoch£º701	 i:0 	 global-step:14020	 l-p:0.12445606291294098
epoch£º701	 i:1 	 global-step:14021	 l-p:0.1463531255722046
epoch£º701	 i:2 	 global-step:14022	 l-p:0.13543733954429626
epoch£º701	 i:3 	 global-step:14023	 l-p:0.16306209564208984
epoch£º701	 i:4 	 global-step:14024	 l-p:0.09079816937446594
epoch£º701	 i:5 	 global-step:14025	 l-p:0.09463551640510559
epoch£º701	 i:6 	 global-step:14026	 l-p:0.14340579509735107
epoch£º701	 i:7 	 global-step:14027	 l-p:0.08113371580839157
epoch£º701	 i:8 	 global-step:14028	 l-p:0.12921585142612457
epoch£º701	 i:9 	 global-step:14029	 l-p:0.16063350439071655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1104, 5.0464, 5.0877],
        [5.1104, 5.2756, 5.0859],
        [5.1104, 5.0483, 5.0890],
        [5.1104, 5.1104, 5.1104]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.12776818871498108 
model_pd.l_d.mean(): -20.137134552001953 
model_pd.lagr.mean(): -20.009366989135742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4795], device='cuda:0')), ('power', tensor([-21.0103], device='cuda:0'))])
epoch£º702	 i:0 	 global-step:14040	 l-p:0.12776818871498108
epoch£º702	 i:1 	 global-step:14041	 l-p:0.19118617475032806
epoch£º702	 i:2 	 global-step:14042	 l-p:0.04163486510515213
epoch£º702	 i:3 	 global-step:14043	 l-p:0.09810822457075119
epoch£º702	 i:4 	 global-step:14044	 l-p:0.13205881416797638
epoch£º702	 i:5 	 global-step:14045	 l-p:0.14008809626102448
epoch£º702	 i:6 	 global-step:14046	 l-p:0.10502775013446808
epoch£º702	 i:7 	 global-step:14047	 l-p:0.16451352834701538
epoch£º702	 i:8 	 global-step:14048	 l-p:0.1545243263244629
epoch£º702	 i:9 	 global-step:14049	 l-p:0.12608449161052704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0850, 5.0821, 5.0849],
        [5.0850, 5.3198, 5.1691],
        [5.0850, 5.0725, 5.0837],
        [5.0850, 5.3445, 5.2085]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): 0.14464637637138367 
model_pd.l_d.mean(): -20.299589157104492 
model_pd.lagr.mean(): -20.154943466186523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4887], device='cuda:0')), ('power', tensor([-21.1854], device='cuda:0'))])
epoch£º703	 i:0 	 global-step:14060	 l-p:0.14464637637138367
epoch£º703	 i:1 	 global-step:14061	 l-p:0.12968231737613678
epoch£º703	 i:2 	 global-step:14062	 l-p:0.08558610826730728
epoch£º703	 i:3 	 global-step:14063	 l-p:0.136599600315094
epoch£º703	 i:4 	 global-step:14064	 l-p:0.19892147183418274
epoch£º703	 i:5 	 global-step:14065	 l-p:0.18382789194583893
epoch£º703	 i:6 	 global-step:14066	 l-p:0.11771874874830246
epoch£º703	 i:7 	 global-step:14067	 l-p:0.1216883435845375
epoch£º703	 i:8 	 global-step:14068	 l-p:0.06430861353874207
epoch£º703	 i:9 	 global-step:14069	 l-p:0.15152938663959503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1131, 5.1131, 5.1131],
        [5.1131, 5.0681, 5.1012],
        [5.1131, 5.0317, 5.0777],
        [5.1131, 4.9364, 4.7003]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.13174867630004883 
model_pd.l_d.mean(): -20.894487380981445 
model_pd.lagr.mean(): -20.762739181518555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3705], device='cuda:0')), ('power', tensor([-21.6689], device='cuda:0'))])
epoch£º704	 i:0 	 global-step:14080	 l-p:0.13174867630004883
epoch£º704	 i:1 	 global-step:14081	 l-p:0.08920393139123917
epoch£º704	 i:2 	 global-step:14082	 l-p:0.16320005059242249
epoch£º704	 i:3 	 global-step:14083	 l-p:0.07901140302419662
epoch£º704	 i:4 	 global-step:14084	 l-p:0.14778079092502594
epoch£º704	 i:5 	 global-step:14085	 l-p:0.12269606441259384
epoch£º704	 i:6 	 global-step:14086	 l-p:0.1040397435426712
epoch£º704	 i:7 	 global-step:14087	 l-p:0.1400502473115921
epoch£º704	 i:8 	 global-step:14088	 l-p:0.10863018035888672
epoch£º704	 i:9 	 global-step:14089	 l-p:0.13265113532543182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1736, 5.1736, 5.1737],
        [5.1736, 5.1692, 5.1734],
        [5.1736, 5.0152, 4.7725],
        [5.1736, 5.2756, 5.0538]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): 96.08455657958984 
model_pd.l_d.mean(): -19.978113174438477 
model_pd.lagr.mean(): 76.1064453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4981], device='cuda:0')), ('power', tensor([-20.8676], device='cuda:0'))])
epoch£º705	 i:0 	 global-step:14100	 l-p:96.08455657958984
epoch£º705	 i:1 	 global-step:14101	 l-p:0.1485014110803604
epoch£º705	 i:2 	 global-step:14102	 l-p:0.13172870874404907
epoch£º705	 i:3 	 global-step:14103	 l-p:0.1450839638710022
epoch£º705	 i:4 	 global-step:14104	 l-p:0.11053559929132462
epoch£º705	 i:5 	 global-step:14105	 l-p:0.11268448829650879
epoch£º705	 i:6 	 global-step:14106	 l-p:0.12815144658088684
epoch£º705	 i:7 	 global-step:14107	 l-p:0.13838624954223633
epoch£º705	 i:8 	 global-step:14108	 l-p:0.07225273549556732
epoch£º705	 i:9 	 global-step:14109	 l-p:0.12186000496149063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1313, 5.0558, 5.1005],
        [5.1313, 5.3137, 5.1316],
        [5.1313, 5.0946, 5.1230],
        [5.1313, 4.9157, 4.7839]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.15319572389125824 
model_pd.l_d.mean(): -20.346755981445312 
model_pd.lagr.mean(): -20.193559646606445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4623], device='cuda:0')), ('power', tensor([-21.2061], device='cuda:0'))])
epoch£º706	 i:0 	 global-step:14120	 l-p:0.15319572389125824
epoch£º706	 i:1 	 global-step:14121	 l-p:0.12472300976514816
epoch£º706	 i:2 	 global-step:14122	 l-p:0.14523351192474365
epoch£º706	 i:3 	 global-step:14123	 l-p:0.10665063560009003
epoch£º706	 i:4 	 global-step:14124	 l-p:0.10164084285497665
epoch£º706	 i:5 	 global-step:14125	 l-p:0.20518659055233002
epoch£º706	 i:6 	 global-step:14126	 l-p:0.12307482212781906
epoch£º706	 i:7 	 global-step:14127	 l-p:0.28484341502189636
epoch£º706	 i:8 	 global-step:14128	 l-p:0.21341578662395477
epoch£º706	 i:9 	 global-step:14129	 l-p:0.13715016841888428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0242, 5.0239, 5.0242],
        [5.0242, 5.0241, 5.0242],
        [5.0242, 5.0241, 5.0242],
        [5.0242, 4.9140, 4.9625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 0.8372770547866821 
model_pd.l_d.mean(): -20.7523193359375 
model_pd.lagr.mean(): -19.915042877197266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4367], device='cuda:0')), ('power', tensor([-21.5927], device='cuda:0'))])
epoch£º707	 i:0 	 global-step:14140	 l-p:0.8372770547866821
epoch£º707	 i:1 	 global-step:14141	 l-p:0.11250592023134232
epoch£º707	 i:2 	 global-step:14142	 l-p:0.2200649529695511
epoch£º707	 i:3 	 global-step:14143	 l-p:0.13568115234375
epoch£º707	 i:4 	 global-step:14144	 l-p:0.12783785164356232
epoch£º707	 i:5 	 global-step:14145	 l-p:0.26816245913505554
epoch£º707	 i:6 	 global-step:14146	 l-p:0.18981944024562836
epoch£º707	 i:7 	 global-step:14147	 l-p:0.14558100700378418
epoch£º707	 i:8 	 global-step:14148	 l-p:0.12605218589305878
epoch£º707	 i:9 	 global-step:14149	 l-p:0.11393879354000092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0443, 4.9693, 5.0146],
        [5.0443, 4.8711, 4.8856],
        [5.0443, 4.9865, 4.7119],
        [5.0443, 4.9685, 5.0140]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.1353936344385147 
model_pd.l_d.mean(): -20.216630935668945 
model_pd.lagr.mean(): -20.08123779296875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4963], device='cuda:0')), ('power', tensor([-21.1087], device='cuda:0'))])
epoch£º708	 i:0 	 global-step:14160	 l-p:0.1353936344385147
epoch£º708	 i:1 	 global-step:14161	 l-p:0.11370332539081573
epoch£º708	 i:2 	 global-step:14162	 l-p:0.1658652126789093
epoch£º708	 i:3 	 global-step:14163	 l-p:0.17373435199260712
epoch£º708	 i:4 	 global-step:14164	 l-p:0.13867522776126862
epoch£º708	 i:5 	 global-step:14165	 l-p:0.06613999605178833
epoch£º708	 i:6 	 global-step:14166	 l-p:0.31037279963493347
epoch£º708	 i:7 	 global-step:14167	 l-p:0.19004619121551514
epoch£º708	 i:8 	 global-step:14168	 l-p:0.14550325274467468
epoch£º708	 i:9 	 global-step:14169	 l-p:0.17106498777866364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0903, 4.8798, 4.8212],
        [5.0903, 5.0522, 5.0815],
        [5.0903, 5.5504, 5.5444],
        [5.0903, 4.9112, 4.6683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): 0.10472258180379868 
model_pd.l_d.mean(): -20.026416778564453 
model_pd.lagr.mean(): -19.921693801879883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4764], device='cuda:0')), ('power', tensor([-20.8943], device='cuda:0'))])
epoch£º709	 i:0 	 global-step:14180	 l-p:0.10472258180379868
epoch£º709	 i:1 	 global-step:14181	 l-p:0.1498544067144394
epoch£º709	 i:2 	 global-step:14182	 l-p:0.06621021777391434
epoch£º709	 i:3 	 global-step:14183	 l-p:0.0958385095000267
epoch£º709	 i:4 	 global-step:14184	 l-p:0.15483511984348297
epoch£º709	 i:5 	 global-step:14185	 l-p:0.11483517289161682
epoch£º709	 i:6 	 global-step:14186	 l-p:0.1366812288761139
epoch£º709	 i:7 	 global-step:14187	 l-p:0.1449909508228302
epoch£º709	 i:8 	 global-step:14188	 l-p:0.12392057478427887
epoch£º709	 i:9 	 global-step:14189	 l-p:0.15709911286830902
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1396, 5.1223, 5.1373],
        [5.1396, 4.9340, 4.8798],
        [5.1396, 4.9658, 4.9732],
        [5.1396, 5.1120, 5.1346]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.12123769521713257 
model_pd.l_d.mean(): -20.339109420776367 
model_pd.lagr.mean(): -20.217872619628906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4281], device='cuda:0')), ('power', tensor([-21.1628], device='cuda:0'))])
epoch£º710	 i:0 	 global-step:14200	 l-p:0.12123769521713257
epoch£º710	 i:1 	 global-step:14201	 l-p:0.12559382617473602
epoch£º710	 i:2 	 global-step:14202	 l-p:0.12552669644355774
epoch£º710	 i:3 	 global-step:14203	 l-p:0.06435438990592957
epoch£º710	 i:4 	 global-step:14204	 l-p:0.15404602885246277
epoch£º710	 i:5 	 global-step:14205	 l-p:0.09552877396345139
epoch£º710	 i:6 	 global-step:14206	 l-p:0.21022430062294006
epoch£º710	 i:7 	 global-step:14207	 l-p:0.1443151831626892
epoch£º710	 i:8 	 global-step:14208	 l-p:0.1544756442308426
epoch£º710	 i:9 	 global-step:14209	 l-p:0.13457423448562622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0895, 5.0785, 5.0885],
        [5.0895, 5.1496, 4.9086],
        [5.0895, 5.0756, 4.8100],
        [5.0895, 5.0895, 5.0895]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.14896804094314575 
model_pd.l_d.mean(): -19.594247817993164 
model_pd.lagr.mean(): -19.445280075073242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5408], device='cuda:0')), ('power', tensor([-20.5208], device='cuda:0'))])
epoch£º711	 i:0 	 global-step:14220	 l-p:0.14896804094314575
epoch£º711	 i:1 	 global-step:14221	 l-p:0.18652626872062683
epoch£º711	 i:2 	 global-step:14222	 l-p:0.09996814280748367
epoch£º711	 i:3 	 global-step:14223	 l-p:0.1573888659477234
epoch£º711	 i:4 	 global-step:14224	 l-p:0.14688147604465485
epoch£º711	 i:5 	 global-step:14225	 l-p:0.1529257893562317
epoch£º711	 i:6 	 global-step:14226	 l-p:0.024398330599069595
epoch£º711	 i:7 	 global-step:14227	 l-p:0.12174367904663086
epoch£º711	 i:8 	 global-step:14228	 l-p:0.08455704152584076
epoch£º711	 i:9 	 global-step:14229	 l-p:0.11839181929826736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1739, 5.1197, 4.8507],
        [5.1739, 5.2705, 5.0445],
        [5.1739, 5.1005, 5.1447],
        [5.1739, 5.4039, 5.2461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): -0.7353121042251587 
model_pd.l_d.mean(): -20.05014991760254 
model_pd.lagr.mean(): -20.78546142578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5078], device='cuda:0')), ('power', tensor([-20.9510], device='cuda:0'))])
epoch£º712	 i:0 	 global-step:14240	 l-p:-0.7353121042251587
epoch£º712	 i:1 	 global-step:14241	 l-p:0.11656976491212845
epoch£º712	 i:2 	 global-step:14242	 l-p:0.13230466842651367
epoch£º712	 i:3 	 global-step:14243	 l-p:0.13847370445728302
epoch£º712	 i:4 	 global-step:14244	 l-p:0.11623170226812363
epoch£º712	 i:5 	 global-step:14245	 l-p:0.12337657809257507
epoch£º712	 i:6 	 global-step:14246	 l-p:0.13794922828674316
epoch£º712	 i:7 	 global-step:14247	 l-p:0.08349833637475967
epoch£º712	 i:8 	 global-step:14248	 l-p:0.07044888287782669
epoch£º712	 i:9 	 global-step:14249	 l-p:0.12384095042943954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1622, 5.1551, 5.1617],
        [5.1622, 4.9472, 4.8001],
        [5.1622, 5.1622, 5.1622],
        [5.1622, 4.9814, 4.7496]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): 0.08602555841207504 
model_pd.l_d.mean(): -20.718984603881836 
model_pd.lagr.mean(): -20.632959365844727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3835], device='cuda:0')), ('power', tensor([-21.5036], device='cuda:0'))])
epoch£º713	 i:0 	 global-step:14260	 l-p:0.08602555841207504
epoch£º713	 i:1 	 global-step:14261	 l-p:0.03809531033039093
epoch£º713	 i:2 	 global-step:14262	 l-p:0.13541261851787567
epoch£º713	 i:3 	 global-step:14263	 l-p:0.13201816380023956
epoch£º713	 i:4 	 global-step:14264	 l-p:0.15625503659248352
epoch£º713	 i:5 	 global-step:14265	 l-p:0.14123749732971191
epoch£º713	 i:6 	 global-step:14266	 l-p:0.12034358084201813
epoch£º713	 i:7 	 global-step:14267	 l-p:0.12348123639822006
epoch£º713	 i:8 	 global-step:14268	 l-p:0.06749832630157471
epoch£º713	 i:9 	 global-step:14269	 l-p:0.18347902595996857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1063, 5.1063, 5.1063],
        [5.1063, 5.0367, 4.7620],
        [5.1063, 4.9314, 4.6837],
        [5.1063, 5.1020, 5.1060]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.15019649267196655 
model_pd.l_d.mean(): -20.439958572387695 
model_pd.lagr.mean(): -20.289762496948242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4491], device='cuda:0')), ('power', tensor([-21.2873], device='cuda:0'))])
epoch£º714	 i:0 	 global-step:14280	 l-p:0.15019649267196655
epoch£º714	 i:1 	 global-step:14281	 l-p:0.16993524134159088
epoch£º714	 i:2 	 global-step:14282	 l-p:0.13801495730876923
epoch£º714	 i:3 	 global-step:14283	 l-p:0.14210468530654907
epoch£º714	 i:4 	 global-step:14284	 l-p:0.19210274517536163
epoch£º714	 i:5 	 global-step:14285	 l-p:0.19405497610569
epoch£º714	 i:6 	 global-step:14286	 l-p:0.10832566022872925
epoch£º714	 i:7 	 global-step:14287	 l-p:0.09863176196813583
epoch£º714	 i:8 	 global-step:14288	 l-p:0.1250196397304535
epoch£º714	 i:9 	 global-step:14289	 l-p:0.0967881977558136
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0581, 5.0459, 5.0569],
        [5.0581, 5.4204, 5.3477],
        [5.0581, 4.8309, 4.6880],
        [5.0581, 5.0581, 5.0581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): 0.13513915240764618 
model_pd.l_d.mean(): -20.228193283081055 
model_pd.lagr.mean(): -20.093053817749023 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4832], device='cuda:0')), ('power', tensor([-21.1069], device='cuda:0'))])
epoch£º715	 i:0 	 global-step:14300	 l-p:0.13513915240764618
epoch£º715	 i:1 	 global-step:14301	 l-p:0.1943858116865158
epoch£º715	 i:2 	 global-step:14302	 l-p:0.11953217536211014
epoch£º715	 i:3 	 global-step:14303	 l-p:0.15752145648002625
epoch£º715	 i:4 	 global-step:14304	 l-p:0.1826489269733429
epoch£º715	 i:5 	 global-step:14305	 l-p:0.1787152886390686
epoch£º715	 i:6 	 global-step:14306	 l-p:0.13006313145160675
epoch£º715	 i:7 	 global-step:14307	 l-p:0.23987670242786407
epoch£º715	 i:8 	 global-step:14308	 l-p:0.12758494913578033
epoch£º715	 i:9 	 global-step:14309	 l-p:0.11239311844110489
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0653, 4.9437, 4.6674],
        [5.0653, 5.0628, 5.0652],
        [5.0653, 5.0653, 5.0653],
        [5.0653, 5.4103, 5.3258]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 0.17571353912353516 
model_pd.l_d.mean(): -20.191566467285156 
model_pd.lagr.mean(): -20.015853881835938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4988], device='cuda:0')), ('power', tensor([-21.0858], device='cuda:0'))])
epoch£º716	 i:0 	 global-step:14320	 l-p:0.17571353912353516
epoch£º716	 i:1 	 global-step:14321	 l-p:0.136821910738945
epoch£º716	 i:2 	 global-step:14322	 l-p:0.14124925434589386
epoch£º716	 i:3 	 global-step:14323	 l-p:0.1297832578420639
epoch£º716	 i:4 	 global-step:14324	 l-p:0.20526950061321259
epoch£º716	 i:5 	 global-step:14325	 l-p:0.11025941371917725
epoch£º716	 i:6 	 global-step:14326	 l-p:0.13057303428649902
epoch£º716	 i:7 	 global-step:14327	 l-p:0.16251468658447266
epoch£º716	 i:8 	 global-step:14328	 l-p:0.07747171819210052
epoch£º716	 i:9 	 global-step:14329	 l-p:0.13000589609146118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0927, 4.9907, 5.0394],
        [5.0927, 4.9629, 5.0066],
        [5.0927, 5.0874, 5.0924],
        [5.0927, 5.2074, 4.9893]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.14749236404895782 
model_pd.l_d.mean(): -20.695302963256836 
model_pd.lagr.mean(): -20.54781150817871 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4152], device='cuda:0')), ('power', tensor([-21.5123], device='cuda:0'))])
epoch£º717	 i:0 	 global-step:14340	 l-p:0.14749236404895782
epoch£º717	 i:1 	 global-step:14341	 l-p:0.1086837574839592
epoch£º717	 i:2 	 global-step:14342	 l-p:0.20106874406337738
epoch£º717	 i:3 	 global-step:14343	 l-p:0.1044195145368576
epoch£º717	 i:4 	 global-step:14344	 l-p:0.12939435243606567
epoch£º717	 i:5 	 global-step:14345	 l-p:0.07230006158351898
epoch£º717	 i:6 	 global-step:14346	 l-p:0.16161993145942688
epoch£º717	 i:7 	 global-step:14347	 l-p:0.2001103162765503
epoch£º717	 i:8 	 global-step:14348	 l-p:0.2705563008785248
epoch£º717	 i:9 	 global-step:14349	 l-p:0.12140309810638428
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228]], device='cuda:0')
 pt:tensor([[5.0616, 4.8570, 4.8268],
        [5.0616, 4.8930, 4.9141],
        [5.0616, 5.4596, 5.4101],
        [5.0616, 4.9212, 4.9617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.13106684386730194 
model_pd.l_d.mean(): -20.710338592529297 
model_pd.lagr.mean(): -20.57927131652832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4166], device='cuda:0')), ('power', tensor([-21.5291], device='cuda:0'))])
epoch£º718	 i:0 	 global-step:14360	 l-p:0.13106684386730194
epoch£º718	 i:1 	 global-step:14361	 l-p:0.10494815558195114
epoch£º718	 i:2 	 global-step:14362	 l-p:0.24843072891235352
epoch£º718	 i:3 	 global-step:14363	 l-p:0.1570337563753128
epoch£º718	 i:4 	 global-step:14364	 l-p:0.17998431622982025
epoch£º718	 i:5 	 global-step:14365	 l-p:0.10717014223337173
epoch£º718	 i:6 	 global-step:14366	 l-p:0.12374912947416306
epoch£º718	 i:7 	 global-step:14367	 l-p:0.10939563810825348
epoch£º718	 i:8 	 global-step:14368	 l-p:0.2565024793148041
epoch£º718	 i:9 	 global-step:14369	 l-p:0.11917455494403839
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0689, 4.9180, 4.9527],
        [5.0689, 4.9702, 4.6911],
        [5.0689, 5.1421, 4.9051],
        [5.0689, 5.0462, 5.0654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.1274770200252533 
model_pd.l_d.mean(): -18.517091751098633 
model_pd.lagr.mean(): -18.38961410522461 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6020], device='cuda:0')), ('power', tensor([-19.4868], device='cuda:0'))])
epoch£º719	 i:0 	 global-step:14380	 l-p:0.1274770200252533
epoch£º719	 i:1 	 global-step:14381	 l-p:0.18643693625926971
epoch£º719	 i:2 	 global-step:14382	 l-p:0.13045619428157806
epoch£º719	 i:3 	 global-step:14383	 l-p:0.10344839841127396
epoch£º719	 i:4 	 global-step:14384	 l-p:0.13981537520885468
epoch£º719	 i:5 	 global-step:14385	 l-p:0.17015689611434937
epoch£º719	 i:6 	 global-step:14386	 l-p:0.13567137718200684
epoch£º719	 i:7 	 global-step:14387	 l-p:0.10050550103187561
epoch£º719	 i:8 	 global-step:14388	 l-p:0.1536744236946106
epoch£º719	 i:9 	 global-step:14389	 l-p:0.14223574101924896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1227, 5.0526, 5.0964],
        [5.1227, 4.9216, 4.8899],
        [5.1227, 4.9433, 4.6988],
        [5.1227, 4.9024, 4.8024]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.109885573387146 
model_pd.l_d.mean(): -20.192005157470703 
model_pd.lagr.mean(): -20.08211898803711 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4595], device='cuda:0')), ('power', tensor([-21.0455], device='cuda:0'))])
epoch£º720	 i:0 	 global-step:14400	 l-p:0.109885573387146
epoch£º720	 i:1 	 global-step:14401	 l-p:0.12207870185375214
epoch£º720	 i:2 	 global-step:14402	 l-p:0.039970483630895615
epoch£º720	 i:3 	 global-step:14403	 l-p:0.13333538174629211
epoch£º720	 i:4 	 global-step:14404	 l-p:0.12217871099710464
epoch£º720	 i:5 	 global-step:14405	 l-p:0.13706043362617493
epoch£º720	 i:6 	 global-step:14406	 l-p:0.1409517228603363
epoch£º720	 i:7 	 global-step:14407	 l-p:0.18062454462051392
epoch£º720	 i:8 	 global-step:14408	 l-p:0.1436261683702469
epoch£º720	 i:9 	 global-step:14409	 l-p:0.114204041659832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228]], device='cuda:0')
 pt:tensor([[5.1271, 4.9875, 4.7197],
        [5.1271, 5.5456, 5.5074],
        [5.1271, 4.9050, 4.7847],
        [5.1271, 4.9089, 4.7361]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.11617906391620636 
model_pd.l_d.mean(): -19.45161247253418 
model_pd.lagr.mean(): -19.335433959960938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5051], device='cuda:0')), ('power', tensor([-20.3385], device='cuda:0'))])
epoch£º721	 i:0 	 global-step:14420	 l-p:0.11617906391620636
epoch£º721	 i:1 	 global-step:14421	 l-p:0.08073634654283524
epoch£º721	 i:2 	 global-step:14422	 l-p:0.14219844341278076
epoch£º721	 i:3 	 global-step:14423	 l-p:0.1479705423116684
epoch£º721	 i:4 	 global-step:14424	 l-p:0.10745836794376373
epoch£º721	 i:5 	 global-step:14425	 l-p:0.13135071098804474
epoch£º721	 i:6 	 global-step:14426	 l-p:0.15857556462287903
epoch£º721	 i:7 	 global-step:14427	 l-p:0.12326916307210922
epoch£º721	 i:8 	 global-step:14428	 l-p:0.13711728155612946
epoch£º721	 i:9 	 global-step:14429	 l-p:0.18439996242523193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0766, 5.0766, 5.0766],
        [5.0766, 5.0741, 5.0765],
        [5.0766, 5.0766, 5.0766],
        [5.0766, 5.0290, 5.0638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.1020977795124054 
model_pd.l_d.mean(): -20.068593978881836 
model_pd.lagr.mean(): -19.966495513916016 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5019], device='cuda:0')), ('power', tensor([-20.9637], device='cuda:0'))])
epoch£º722	 i:0 	 global-step:14440	 l-p:0.1020977795124054
epoch£º722	 i:1 	 global-step:14441	 l-p:0.14072655141353607
epoch£º722	 i:2 	 global-step:14442	 l-p:0.1657731533050537
epoch£º722	 i:3 	 global-step:14443	 l-p:0.1317215859889984
epoch£º722	 i:4 	 global-step:14444	 l-p:0.13751773536205292
epoch£º722	 i:5 	 global-step:14445	 l-p:0.27647578716278076
epoch£º722	 i:6 	 global-step:14446	 l-p:0.12144683301448822
epoch£º722	 i:7 	 global-step:14447	 l-p:0.1334606558084488
epoch£º722	 i:8 	 global-step:14448	 l-p:0.13561630249023438
epoch£º722	 i:9 	 global-step:14449	 l-p:1.3118505477905273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0235, 4.9576, 5.0006],
        [5.0235, 4.9199, 4.6361],
        [5.0235, 5.0233, 5.0235],
        [5.0235, 5.0235, 5.0235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.11728473752737045 
model_pd.l_d.mean(): -20.37208366394043 
model_pd.lagr.mean(): -20.254798889160156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4881], device='cuda:0')), ('power', tensor([-21.2585], device='cuda:0'))])
epoch£º723	 i:0 	 global-step:14460	 l-p:0.11728473752737045
epoch£º723	 i:1 	 global-step:14461	 l-p:0.19766521453857422
epoch£º723	 i:2 	 global-step:14462	 l-p:0.22052176296710968
epoch£º723	 i:3 	 global-step:14463	 l-p:0.1296492964029312
epoch£º723	 i:4 	 global-step:14464	 l-p:0.41194403171539307
epoch£º723	 i:5 	 global-step:14465	 l-p:0.19830168783664703
epoch£º723	 i:6 	 global-step:14466	 l-p:0.10525823384523392
epoch£º723	 i:7 	 global-step:14467	 l-p:0.12713883817195892
epoch£º723	 i:8 	 global-step:14468	 l-p:0.49733787775039673
epoch£º723	 i:9 	 global-step:14469	 l-p:0.13098862767219543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0488, 5.0460, 5.0487],
        [5.0488, 4.9829, 4.7025],
        [5.0488, 5.0368, 5.0476],
        [5.0488, 4.8303, 4.7722]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.11911491304636002 
model_pd.l_d.mean(): -19.667116165161133 
model_pd.lagr.mean(): -19.54800033569336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5118], device='cuda:0')), ('power', tensor([-20.5650], device='cuda:0'))])
epoch£º724	 i:0 	 global-step:14480	 l-p:0.11911491304636002
epoch£º724	 i:1 	 global-step:14481	 l-p:0.1086549162864685
epoch£º724	 i:2 	 global-step:14482	 l-p:0.2097465842962265
epoch£º724	 i:3 	 global-step:14483	 l-p:0.14720231294631958
epoch£º724	 i:4 	 global-step:14484	 l-p:0.12107397615909576
epoch£º724	 i:5 	 global-step:14485	 l-p:0.1458081156015396
epoch£º724	 i:6 	 global-step:14486	 l-p:0.13113602995872498
epoch£º724	 i:7 	 global-step:14487	 l-p:0.14189401268959045
epoch£º724	 i:8 	 global-step:14488	 l-p:0.21352261304855347
epoch£º724	 i:9 	 global-step:14489	 l-p:0.10368096828460693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0974, 5.0931, 5.0972],
        [5.0974, 4.9505, 4.6804],
        [5.0974, 4.9131, 4.6654],
        [5.0974, 4.9663, 5.0104]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.13072605431079865 
model_pd.l_d.mean(): -20.40581703186035 
model_pd.lagr.mean(): -20.27509117126465 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4286], device='cuda:0')), ('power', tensor([-21.2313], device='cuda:0'))])
epoch£º725	 i:0 	 global-step:14500	 l-p:0.13072605431079865
epoch£º725	 i:1 	 global-step:14501	 l-p:0.12484832853078842
epoch£º725	 i:2 	 global-step:14502	 l-p:0.12990859150886536
epoch£º725	 i:3 	 global-step:14503	 l-p:0.1570533812046051
epoch£º725	 i:4 	 global-step:14504	 l-p:0.12958213686943054
epoch£º725	 i:5 	 global-step:14505	 l-p:0.26016682386398315
epoch£º725	 i:6 	 global-step:14506	 l-p:0.1903131753206253
epoch£º725	 i:7 	 global-step:14507	 l-p:0.10257108509540558
epoch£º725	 i:8 	 global-step:14508	 l-p:0.11229623854160309
epoch£º725	 i:9 	 global-step:14509	 l-p:0.12558400630950928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]], device='cuda:0')
 pt:tensor([[5.0515, 4.9258, 4.9728],
        [5.0515, 4.8195, 4.6864],
        [5.0515, 5.0034, 4.7252],
        [5.0515, 5.2500, 5.0750]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): 0.16801905632019043 
model_pd.l_d.mean(): -19.07168197631836 
model_pd.lagr.mean(): -18.903663635253906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5693], device='cuda:0')), ('power', tensor([-20.0179], device='cuda:0'))])
epoch£º726	 i:0 	 global-step:14520	 l-p:0.16801905632019043
epoch£º726	 i:1 	 global-step:14521	 l-p:0.1796473264694214
epoch£º726	 i:2 	 global-step:14522	 l-p:0.18933506309986115
epoch£º726	 i:3 	 global-step:14523	 l-p:0.18059316277503967
epoch£º726	 i:4 	 global-step:14524	 l-p:0.14540928602218628
epoch£º726	 i:5 	 global-step:14525	 l-p:0.13033562898635864
epoch£º726	 i:6 	 global-step:14526	 l-p:0.20360122621059418
epoch£º726	 i:7 	 global-step:14527	 l-p:0.06377578526735306
epoch£º726	 i:8 	 global-step:14528	 l-p:0.15040209889411926
epoch£º726	 i:9 	 global-step:14529	 l-p:0.12201892584562302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1002, 5.1002, 5.1002],
        [5.1002, 5.2464, 5.0421],
        [5.1002, 5.5104, 5.4665],
        [5.1002, 5.1002, 5.1002]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.15730907022953033 
model_pd.l_d.mean(): -20.36383628845215 
model_pd.lagr.mean(): -20.206527709960938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4697], device='cuda:0')), ('power', tensor([-21.2311], device='cuda:0'))])
epoch£º727	 i:0 	 global-step:14540	 l-p:0.15730907022953033
epoch£º727	 i:1 	 global-step:14541	 l-p:0.12307818979024887
epoch£º727	 i:2 	 global-step:14542	 l-p:0.13655899465084076
epoch£º727	 i:3 	 global-step:14543	 l-p:0.11388333886861801
epoch£º727	 i:4 	 global-step:14544	 l-p:0.1338479071855545
epoch£º727	 i:5 	 global-step:14545	 l-p:0.2009837031364441
epoch£º727	 i:6 	 global-step:14546	 l-p:0.12508991360664368
epoch£º727	 i:7 	 global-step:14547	 l-p:0.15929388999938965
epoch£º727	 i:8 	 global-step:14548	 l-p:0.1497027575969696
epoch£º727	 i:9 	 global-step:14549	 l-p:0.07071475684642792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1210, 5.1075, 5.1196],
        [5.1210, 5.1202, 5.1210],
        [5.1210, 4.9374, 4.6910],
        [5.1210, 5.5686, 5.5496]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.12950889766216278 
model_pd.l_d.mean(): -20.305503845214844 
model_pd.lagr.mean(): -20.175994873046875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4596], device='cuda:0')), ('power', tensor([-21.1612], device='cuda:0'))])
epoch£º728	 i:0 	 global-step:14560	 l-p:0.12950889766216278
epoch£º728	 i:1 	 global-step:14561	 l-p:0.1015753522515297
epoch£º728	 i:2 	 global-step:14562	 l-p:0.12595677375793457
epoch£º728	 i:3 	 global-step:14563	 l-p:0.1457580029964447
epoch£º728	 i:4 	 global-step:14564	 l-p:0.1385495662689209
epoch£º728	 i:5 	 global-step:14565	 l-p:0.13259246945381165
epoch£º728	 i:6 	 global-step:14566	 l-p:-0.3323976397514343
epoch£º728	 i:7 	 global-step:14567	 l-p:0.13735027611255646
epoch£º728	 i:8 	 global-step:14568	 l-p:0.1140148714184761
epoch£º728	 i:9 	 global-step:14569	 l-p:0.10293003171682358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1820, 5.0455, 5.0853],
        [5.1820, 5.1086, 5.1532],
        [5.1820, 5.0173, 5.0374],
        [5.1820, 5.1820, 5.1820]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): -1.0435922145843506 
model_pd.l_d.mean(): -19.156147003173828 
model_pd.lagr.mean(): -20.199739456176758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5671], device='cuda:0')), ('power', tensor([-20.1017], device='cuda:0'))])
epoch£º729	 i:0 	 global-step:14580	 l-p:-1.0435922145843506
epoch£º729	 i:1 	 global-step:14581	 l-p:0.11393239349126816
epoch£º729	 i:2 	 global-step:14582	 l-p:0.1615232229232788
epoch£º729	 i:3 	 global-step:14583	 l-p:0.12768422067165375
epoch£º729	 i:4 	 global-step:14584	 l-p:0.14169147610664368
epoch£º729	 i:5 	 global-step:14585	 l-p:0.11515843868255615
epoch£º729	 i:6 	 global-step:14586	 l-p:0.12520189583301544
epoch£º729	 i:7 	 global-step:14587	 l-p:0.039998892694711685
epoch£º729	 i:8 	 global-step:14588	 l-p:0.13600097596645355
epoch£º729	 i:9 	 global-step:14589	 l-p:0.121391162276268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5584,  0.4599,  1.0000,  0.3787,
          1.0000,  0.8235, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4602,  0.3553,  1.0000,  0.2743,
          1.0000,  0.7721, 31.6228]], device='cuda:0')
 pt:tensor([[5.1138, 5.0311, 4.7513],
        [5.1138, 5.3162, 5.1413],
        [5.1138, 4.9327, 4.6816],
        [5.1138, 4.9417, 4.6839]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.05621219426393509 
model_pd.l_d.mean(): -19.145767211914062 
model_pd.lagr.mean(): -19.089555740356445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5555], device='cuda:0')), ('power', tensor([-20.0791], device='cuda:0'))])
epoch£º730	 i:0 	 global-step:14600	 l-p:0.05621219426393509
epoch£º730	 i:1 	 global-step:14601	 l-p:0.14073514938354492
epoch£º730	 i:2 	 global-step:14602	 l-p:0.18394704163074493
epoch£º730	 i:3 	 global-step:14603	 l-p:0.12813010811805725
epoch£º730	 i:4 	 global-step:14604	 l-p:0.1484452188014984
epoch£º730	 i:5 	 global-step:14605	 l-p:0.13545890152454376
epoch£º730	 i:6 	 global-step:14606	 l-p:0.13948234915733337
epoch£º730	 i:7 	 global-step:14607	 l-p:0.140530526638031
epoch£º730	 i:8 	 global-step:14608	 l-p:0.4419037401676178
epoch£º730	 i:9 	 global-step:14609	 l-p:0.09405026584863663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0242, 5.3219, 5.2062],
        [5.0242, 4.8384, 4.8476],
        [5.0242, 4.7875, 4.6434],
        [5.0242, 5.0131, 5.0231]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.12039335817098618 
model_pd.l_d.mean(): -20.118297576904297 
model_pd.lagr.mean(): -19.99790382385254 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5215], device='cuda:0')), ('power', tensor([-21.0346], device='cuda:0'))])
epoch£º731	 i:0 	 global-step:14620	 l-p:0.12039335817098618
epoch£º731	 i:1 	 global-step:14621	 l-p:0.13114827871322632
epoch£º731	 i:2 	 global-step:14622	 l-p:0.22335399687290192
epoch£º731	 i:3 	 global-step:14623	 l-p:0.4209522008895874
epoch£º731	 i:4 	 global-step:14624	 l-p:0.1393594592809677
epoch£º731	 i:5 	 global-step:14625	 l-p:0.38647937774658203
epoch£º731	 i:6 	 global-step:14626	 l-p:0.11872317641973495
epoch£º731	 i:7 	 global-step:14627	 l-p:-0.052527278661727905
epoch£º731	 i:8 	 global-step:14628	 l-p:0.061610348522663116
epoch£º731	 i:9 	 global-step:14629	 l-p:0.12191245704889297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9775, 4.8913, 4.6028],
        [4.9775, 4.9751, 4.9774],
        [4.9775, 4.9775, 4.9775],
        [4.9775, 4.9737, 4.9773]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): -0.2623441517353058 
model_pd.l_d.mean(): -19.382347106933594 
model_pd.lagr.mean(): -19.644691467285156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5605], device='cuda:0')), ('power', tensor([-20.3253], device='cuda:0'))])
epoch£º732	 i:0 	 global-step:14640	 l-p:-0.2623441517353058
epoch£º732	 i:1 	 global-step:14641	 l-p:0.5503923892974854
epoch£º732	 i:2 	 global-step:14642	 l-p:0.1266714334487915
epoch£º732	 i:3 	 global-step:14643	 l-p:0.1391345113515854
epoch£º732	 i:4 	 global-step:14644	 l-p:-14.861590385437012
epoch£º732	 i:5 	 global-step:14645	 l-p:0.12221640348434448
epoch£º732	 i:6 	 global-step:14646	 l-p:0.2025720477104187
epoch£º732	 i:7 	 global-step:14647	 l-p:0.09752070903778076
epoch£º732	 i:8 	 global-step:14648	 l-p:0.13878995180130005
epoch£º732	 i:9 	 global-step:14649	 l-p:0.29868072271347046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0570, 5.0570, 5.0570],
        [5.0570, 5.0449, 5.0558],
        [5.0570, 4.9779, 5.0250],
        [5.0570, 5.0239, 5.0503]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.13332504034042358 
model_pd.l_d.mean(): -19.69782257080078 
model_pd.lagr.mean(): -19.564496994018555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5171], device='cuda:0')), ('power', tensor([-20.6017], device='cuda:0'))])
epoch£º733	 i:0 	 global-step:14660	 l-p:0.13332504034042358
epoch£º733	 i:1 	 global-step:14661	 l-p:0.10919788479804993
epoch£º733	 i:2 	 global-step:14662	 l-p:0.2306421995162964
epoch£º733	 i:3 	 global-step:14663	 l-p:0.17713269591331482
epoch£º733	 i:4 	 global-step:14664	 l-p:0.07213842868804932
epoch£º733	 i:5 	 global-step:14665	 l-p:0.15085485577583313
epoch£º733	 i:6 	 global-step:14666	 l-p:0.1748676300048828
epoch£º733	 i:7 	 global-step:14667	 l-p:0.1076195016503334
epoch£º733	 i:8 	 global-step:14668	 l-p:0.1681150645017624
epoch£º733	 i:9 	 global-step:14669	 l-p:0.10744777321815491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:734
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1158, 5.1146, 5.1158],
        [5.1158, 5.2285, 5.0066],
        [5.1158, 5.4464, 5.3484],
        [5.1158, 4.9096, 4.6867]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:734, step:0 
model_pd.l_p.mean(): 0.15865683555603027 
model_pd.l_d.mean(): -20.27988624572754 
model_pd.lagr.mean(): -20.12122917175293 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4597], device='cuda:0')), ('power', tensor([-21.1353], device='cuda:0'))])
epoch£º734	 i:0 	 global-step:14680	 l-p:0.15865683555603027
epoch£º734	 i:1 	 global-step:14681	 l-p:0.11304929852485657
epoch£º734	 i:2 	 global-step:14682	 l-p:0.10960034281015396
epoch£º734	 i:3 	 global-step:14683	 l-p:0.1684322953224182
epoch£º734	 i:4 	 global-step:14684	 l-p:0.12073976546525955
epoch£º734	 i:5 	 global-step:14685	 l-p:0.14510753750801086
epoch£º734	 i:6 	 global-step:14686	 l-p:0.08648010343313217
epoch£º734	 i:7 	 global-step:14687	 l-p:0.10732793807983398
epoch£º734	 i:8 	 global-step:14688	 l-p:0.13185067474842072
epoch£º734	 i:9 	 global-step:14689	 l-p:0.24706238508224487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:735
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0903, 4.8620, 4.7617],
        [5.0903, 5.0062, 5.0542],
        [5.0903, 4.9947, 4.7119],
        [5.0903, 4.9644, 4.6841]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:735, step:0 
model_pd.l_p.mean(): 0.14631693065166473 
model_pd.l_d.mean(): -19.865034103393555 
model_pd.lagr.mean(): -19.718717575073242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4784], device='cuda:0')), ('power', tensor([-20.7320], device='cuda:0'))])
epoch£º735	 i:0 	 global-step:14700	 l-p:0.14631693065166473
epoch£º735	 i:1 	 global-step:14701	 l-p:0.09784696251153946
epoch£º735	 i:2 	 global-step:14702	 l-p:0.10525601357221603
epoch£º735	 i:3 	 global-step:14703	 l-p:0.1309046596288681
epoch£º735	 i:4 	 global-step:14704	 l-p:0.23178695142269135
epoch£º735	 i:5 	 global-step:14705	 l-p:0.13269154727458954
epoch£º735	 i:6 	 global-step:14706	 l-p:0.11656671017408371
epoch£º735	 i:7 	 global-step:14707	 l-p:0.15688112378120422
epoch£º735	 i:8 	 global-step:14708	 l-p:0.14671267569065094
epoch£º735	 i:9 	 global-step:14709	 l-p:0.2355959266424179
====================================================================================================
====================================================================================================
====================================================================================================

epoch:736
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0782, 5.4047, 5.3047],
        [5.0782, 5.0110, 5.0545],
        [5.0782, 4.8456, 4.6920],
        [5.0782, 4.8452, 4.7006]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:736, step:0 
model_pd.l_p.mean(): 0.16441796720027924 
model_pd.l_d.mean(): -20.284399032592773 
model_pd.lagr.mean(): -20.11998176574707 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4895], device='cuda:0')), ('power', tensor([-21.1707], device='cuda:0'))])
epoch£º736	 i:0 	 global-step:14720	 l-p:0.16441796720027924
epoch£º736	 i:1 	 global-step:14721	 l-p:0.10910177230834961
epoch£º736	 i:2 	 global-step:14722	 l-p:0.19186462461948395
epoch£º736	 i:3 	 global-step:14723	 l-p:0.13954974710941315
epoch£º736	 i:4 	 global-step:14724	 l-p:0.09686708450317383
epoch£º736	 i:5 	 global-step:14725	 l-p:0.13038580119609833
epoch£º736	 i:6 	 global-step:14726	 l-p:0.13845446705818176
epoch£º736	 i:7 	 global-step:14727	 l-p:0.1325414478778839
epoch£º736	 i:8 	 global-step:14728	 l-p:0.09464570879936218
epoch£º736	 i:9 	 global-step:14729	 l-p:0.17920462787151337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:737
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1041, 5.0705, 5.0973],
        [5.1041, 5.5351, 5.5036],
        [5.1041, 4.9942, 4.7118],
        [5.1041, 5.4673, 5.3900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:737, step:0 
model_pd.l_p.mean(): 0.12333881855010986 
model_pd.l_d.mean(): -20.525104522705078 
model_pd.lagr.mean(): -20.401765823364258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4203], device='cuda:0')), ('power', tensor([-21.3442], device='cuda:0'))])
epoch£º737	 i:0 	 global-step:14740	 l-p:0.12333881855010986
epoch£º737	 i:1 	 global-step:14741	 l-p:0.1993718296289444
epoch£º737	 i:2 	 global-step:14742	 l-p:0.08424066007137299
epoch£º737	 i:3 	 global-step:14743	 l-p:0.10994630306959152
epoch£º737	 i:4 	 global-step:14744	 l-p:0.1793254017829895
epoch£º737	 i:5 	 global-step:14745	 l-p:0.12768492102622986
epoch£º737	 i:6 	 global-step:14746	 l-p:0.14184236526489258
epoch£º737	 i:7 	 global-step:14747	 l-p:0.13300339877605438
epoch£º737	 i:8 	 global-step:14748	 l-p:0.14308591187000275
epoch£º737	 i:9 	 global-step:14749	 l-p:0.12128652632236481
====================================================================================================
====================================================================================================
====================================================================================================

epoch:738
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1155, 5.1031, 5.1142],
        [5.1155, 5.1129, 5.1154],
        [5.1155, 5.1147, 5.1155],
        [5.1155, 5.1155, 5.1155]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:738, step:0 
model_pd.l_p.mean(): 0.1032339483499527 
model_pd.l_d.mean(): -20.110828399658203 
model_pd.lagr.mean(): -20.00759506225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4858], device='cuda:0')), ('power', tensor([-20.9900], device='cuda:0'))])
epoch£º738	 i:0 	 global-step:14760	 l-p:0.1032339483499527
epoch£º738	 i:1 	 global-step:14761	 l-p:0.13584846258163452
epoch£º738	 i:2 	 global-step:14762	 l-p:0.08929679542779922
epoch£º738	 i:3 	 global-step:14763	 l-p:0.1435643583536148
epoch£º738	 i:4 	 global-step:14764	 l-p:0.11827278137207031
epoch£º738	 i:5 	 global-step:14765	 l-p:0.14487095177173615
epoch£º738	 i:6 	 global-step:14766	 l-p:0.12889932096004486
epoch£º738	 i:7 	 global-step:14767	 l-p:0.15130965411663055
epoch£º738	 i:8 	 global-step:14768	 l-p:0.14556318521499634
epoch£º738	 i:9 	 global-step:14769	 l-p:0.16353900730609894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:739
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1045, 5.1045, 5.1045],
        [5.1045, 5.1045, 5.1045],
        [5.1045, 5.0367, 4.7541],
        [5.1045, 4.9703, 5.0151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:739, step:0 
model_pd.l_p.mean(): 0.15044492483139038 
model_pd.l_d.mean(): -19.563838958740234 
model_pd.lagr.mean(): -19.413394927978516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5275], device='cuda:0')), ('power', tensor([-20.4760], device='cuda:0'))])
epoch£º739	 i:0 	 global-step:14780	 l-p:0.15044492483139038
epoch£º739	 i:1 	 global-step:14781	 l-p:0.13015572726726532
epoch£º739	 i:2 	 global-step:14782	 l-p:0.1788855344057083
epoch£º739	 i:3 	 global-step:14783	 l-p:0.15807317197322845
epoch£º739	 i:4 	 global-step:14784	 l-p:0.12619371712207794
epoch£º739	 i:5 	 global-step:14785	 l-p:0.18118616938591003
epoch£º739	 i:6 	 global-step:14786	 l-p:0.0929480791091919
epoch£º739	 i:7 	 global-step:14787	 l-p:0.1321941316127777
epoch£º739	 i:8 	 global-step:14788	 l-p:0.0832885354757309
epoch£º739	 i:9 	 global-step:14789	 l-p:0.09528085589408875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:740
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1311, 5.1311, 5.1311],
        [5.1311, 5.1197, 5.1300],
        [5.1311, 4.9165, 4.8665],
        [5.1311, 4.9221, 4.6989]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:740, step:0 
model_pd.l_p.mean(): 0.1313655972480774 
model_pd.l_d.mean(): -19.069231033325195 
model_pd.lagr.mean(): -18.9378662109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5301], device='cuda:0')), ('power', tensor([-19.9749], device='cuda:0'))])
epoch£º740	 i:0 	 global-step:14800	 l-p:0.1313655972480774
epoch£º740	 i:1 	 global-step:14801	 l-p:0.08304854482412338
epoch£º740	 i:2 	 global-step:14802	 l-p:0.10172724723815918
epoch£º740	 i:3 	 global-step:14803	 l-p:0.07575364410877228
epoch£º740	 i:4 	 global-step:14804	 l-p:0.13923053443431854
epoch£º740	 i:5 	 global-step:14805	 l-p:0.135755255818367
epoch£º740	 i:6 	 global-step:14806	 l-p:0.14811432361602783
epoch£º740	 i:7 	 global-step:14807	 l-p:0.15891186892986298
epoch£º740	 i:8 	 global-step:14808	 l-p:0.13666999340057373
epoch£º740	 i:9 	 global-step:14809	 l-p:0.1530008763074875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:741
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1321, 4.9628, 4.6983],
        [5.1321, 5.5412, 5.4927],
        [5.1321, 4.9708, 4.7020],
        [5.1321, 5.1321, 5.1321]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:741, step:0 
model_pd.l_p.mean(): 0.11682048439979553 
model_pd.l_d.mean(): -20.176389694213867 
model_pd.lagr.mean(): -20.059568405151367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4828], device='cuda:0')), ('power', tensor([-21.0537], device='cuda:0'))])
epoch£º741	 i:0 	 global-step:14820	 l-p:0.11682048439979553
epoch£º741	 i:1 	 global-step:14821	 l-p:0.1105094850063324
epoch£º741	 i:2 	 global-step:14822	 l-p:0.1282324343919754
epoch£º741	 i:3 	 global-step:14823	 l-p:0.1575879156589508
epoch£º741	 i:4 	 global-step:14824	 l-p:0.07245980203151703
epoch£º741	 i:5 	 global-step:14825	 l-p:0.11476530134677887
epoch£º741	 i:6 	 global-step:14826	 l-p:0.14659461379051208
epoch£º741	 i:7 	 global-step:14827	 l-p:0.146754190325737
epoch£º741	 i:8 	 global-step:14828	 l-p:0.15990394353866577
epoch£º741	 i:9 	 global-step:14829	 l-p:0.14786212146282196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:742
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0967, 5.4964, 5.4425],
        [5.0967, 4.9686, 5.0158],
        [5.0967, 5.0926, 5.0965],
        [5.0967, 4.8673, 4.7769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:742, step:0 
model_pd.l_p.mean(): 0.134202778339386 
model_pd.l_d.mean(): -19.687267303466797 
model_pd.lagr.mean(): -19.553064346313477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5490], device='cuda:0')), ('power', tensor([-20.6240], device='cuda:0'))])
epoch£º742	 i:0 	 global-step:14840	 l-p:0.134202778339386
epoch£º742	 i:1 	 global-step:14841	 l-p:0.1138879582285881
epoch£º742	 i:2 	 global-step:14842	 l-p:0.11410067975521088
epoch£º742	 i:3 	 global-step:14843	 l-p:0.13144344091415405
epoch£º742	 i:4 	 global-step:14844	 l-p:0.22851994633674622
epoch£º742	 i:5 	 global-step:14845	 l-p:0.1442583054304123
epoch£º742	 i:6 	 global-step:14846	 l-p:0.1954830437898636
epoch£º742	 i:7 	 global-step:14847	 l-p:0.12116103619337082
epoch£º742	 i:8 	 global-step:14848	 l-p:0.06668242812156677
epoch£º742	 i:9 	 global-step:14849	 l-p:0.12475767731666565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:743
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1229, 5.1229, 5.1229],
        [5.1229, 5.0577, 5.1005],
        [5.1229, 5.0803, 4.8006],
        [5.1229, 4.9445, 4.6830]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:743, step:0 
model_pd.l_p.mean(): 0.050970062613487244 
model_pd.l_d.mean(): -19.72411346435547 
model_pd.lagr.mean(): -19.67314338684082 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5178], device='cuda:0')), ('power', tensor([-20.6292], device='cuda:0'))])
epoch£º743	 i:0 	 global-step:14860	 l-p:0.050970062613487244
epoch£º743	 i:1 	 global-step:14861	 l-p:0.12856119871139526
epoch£º743	 i:2 	 global-step:14862	 l-p:0.13225150108337402
epoch£º743	 i:3 	 global-step:14863	 l-p:0.13195233047008514
epoch£º743	 i:4 	 global-step:14864	 l-p:0.12221957743167877
epoch£º743	 i:5 	 global-step:14865	 l-p:0.1258537769317627
epoch£º743	 i:6 	 global-step:14866	 l-p:0.16184353828430176
epoch£º743	 i:7 	 global-step:14867	 l-p:0.14100989699363708
epoch£º743	 i:8 	 global-step:14868	 l-p:0.12354936450719833
epoch£º743	 i:9 	 global-step:14869	 l-p:0.15957990288734436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:744
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1118, 5.0260, 5.0746],
        [5.1118, 4.9751, 5.0196],
        [5.1118, 5.0274, 5.0758],
        [5.1118, 5.0730, 5.1030]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:744, step:0 
model_pd.l_p.mean(): 0.1325814425945282 
model_pd.l_d.mean(): -20.750699996948242 
model_pd.lagr.mean(): -20.618118286132812 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3910], device='cuda:0')), ('power', tensor([-21.5437], device='cuda:0'))])
epoch£º744	 i:0 	 global-step:14880	 l-p:0.1325814425945282
epoch£º744	 i:1 	 global-step:14881	 l-p:0.13039056956768036
epoch£º744	 i:2 	 global-step:14882	 l-p:0.10962838679552078
epoch£º744	 i:3 	 global-step:14883	 l-p:0.11113549023866653
epoch£º744	 i:4 	 global-step:14884	 l-p:0.20945312082767487
epoch£º744	 i:5 	 global-step:14885	 l-p:0.09383349865674973
epoch£º744	 i:6 	 global-step:14886	 l-p:0.15095996856689453
epoch£º744	 i:7 	 global-step:14887	 l-p:0.1332688182592392
epoch£º744	 i:8 	 global-step:14888	 l-p:-0.4946097731590271
epoch£º744	 i:9 	 global-step:14889	 l-p:0.2810198962688446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:745
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0284, 4.9945, 5.0216],
        [5.0284, 5.0284, 5.0284],
        [5.0284, 5.4265, 5.3731],
        [5.0284, 4.8649, 4.5819]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:745, step:0 
model_pd.l_p.mean(): 0.3070313036441803 
model_pd.l_d.mean(): -19.53883171081543 
model_pd.lagr.mean(): -19.231800079345703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5528], device='cuda:0')), ('power', tensor([-20.4767], device='cuda:0'))])
epoch£º745	 i:0 	 global-step:14900	 l-p:0.3070313036441803
epoch£º745	 i:1 	 global-step:14901	 l-p:0.1391928493976593
epoch£º745	 i:2 	 global-step:14902	 l-p:0.2671421766281128
epoch£º745	 i:3 	 global-step:14903	 l-p:0.10966812074184418
epoch£º745	 i:4 	 global-step:14904	 l-p:0.1335725039243698
epoch£º745	 i:5 	 global-step:14905	 l-p:0.12086498737335205
epoch£º745	 i:6 	 global-step:14906	 l-p:0.2980932295322418
epoch£º745	 i:7 	 global-step:14907	 l-p:0.1541108638048172
epoch£º745	 i:8 	 global-step:14908	 l-p:0.18012136220932007
epoch£º745	 i:9 	 global-step:14909	 l-p:0.11797704547643661
====================================================================================================
====================================================================================================
====================================================================================================

epoch:746
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0865, 5.0798, 5.0861],
        [5.0865, 5.0767, 5.0857],
        [5.0865, 5.1247, 4.8673],
        [5.0865, 5.0865, 5.0865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:746, step:0 
model_pd.l_p.mean(): 0.1454465091228485 
model_pd.l_d.mean(): -19.108678817749023 
model_pd.lagr.mean(): -18.963232040405273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5521], device='cuda:0')), ('power', tensor([-20.0379], device='cuda:0'))])
epoch£º746	 i:0 	 global-step:14920	 l-p:0.1454465091228485
epoch£º746	 i:1 	 global-step:14921	 l-p:0.19820652902126312
epoch£º746	 i:2 	 global-step:14922	 l-p:0.12705107033252716
epoch£º746	 i:3 	 global-step:14923	 l-p:0.13240523636341095
epoch£º746	 i:4 	 global-step:14924	 l-p:0.12268901616334915
epoch£º746	 i:5 	 global-step:14925	 l-p:0.12804992496967316
epoch£º746	 i:6 	 global-step:14926	 l-p:0.13303403556346893
epoch£º746	 i:7 	 global-step:14927	 l-p:0.1072595939040184
epoch£º746	 i:8 	 global-step:14928	 l-p:0.10992363095283508
epoch£º746	 i:9 	 global-step:14929	 l-p:0.1210542842745781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:747
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1441, 5.0903, 5.1283],
        [5.1441, 5.1055, 5.1354],
        [5.1441, 5.1463, 4.8770],
        [5.1441, 5.1671, 4.9044]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:747, step:0 
model_pd.l_p.mean(): 0.18038871884346008 
model_pd.l_d.mean(): -19.777780532836914 
model_pd.lagr.mean(): -19.59739112854004 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4993], device='cuda:0')), ('power', tensor([-20.6647], device='cuda:0'))])
epoch£º747	 i:0 	 global-step:14940	 l-p:0.18038871884346008
epoch£º747	 i:1 	 global-step:14941	 l-p:0.16287961602210999
epoch£º747	 i:2 	 global-step:14942	 l-p:0.13519582152366638
epoch£º747	 i:3 	 global-step:14943	 l-p:0.10458217561244965
epoch£º747	 i:4 	 global-step:14944	 l-p:0.1324380785226822
epoch£º747	 i:5 	 global-step:14945	 l-p:0.0999479815363884
epoch£º747	 i:6 	 global-step:14946	 l-p:0.15317729115486145
epoch£º747	 i:7 	 global-step:14947	 l-p:0.0816182792186737
epoch£º747	 i:8 	 global-step:14948	 l-p:0.14692460000514984
epoch£º747	 i:9 	 global-step:14949	 l-p:0.1427682489156723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:748
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0844, 4.8881, 4.8860],
        [5.0844, 5.4757, 5.4153],
        [5.0844, 4.9726, 5.0234],
        [5.0844, 4.8733, 4.8463]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:748, step:0 
model_pd.l_p.mean(): 0.16097840666770935 
model_pd.l_d.mean(): -20.571603775024414 
model_pd.lagr.mean(): -20.410625457763672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4385], device='cuda:0')), ('power', tensor([-21.4104], device='cuda:0'))])
epoch£º748	 i:0 	 global-step:14960	 l-p:0.16097840666770935
epoch£º748	 i:1 	 global-step:14961	 l-p:0.15093402564525604
epoch£º748	 i:2 	 global-step:14962	 l-p:0.1786803901195526
epoch£º748	 i:3 	 global-step:14963	 l-p:0.13022075593471527
epoch£º748	 i:4 	 global-step:14964	 l-p:0.09150559455156326
epoch£º748	 i:5 	 global-step:14965	 l-p:0.24889913201332092
epoch£º748	 i:6 	 global-step:14966	 l-p:0.10203039646148682
epoch£º748	 i:7 	 global-step:14967	 l-p:0.1666068434715271
epoch£º748	 i:8 	 global-step:14968	 l-p:0.12512773275375366
epoch£º748	 i:9 	 global-step:14969	 l-p:0.12455924600362778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:749
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0843, 5.4352, 5.3480],
        [5.0843, 5.0609, 5.0806],
        [5.0843, 5.0566, 5.0795],
        [5.0843, 4.8549, 4.6493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:749, step:0 
model_pd.l_p.mean(): 0.13651713728904724 
model_pd.l_d.mean(): -19.932641983032227 
model_pd.lagr.mean(): -19.796125411987305 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5251], device='cuda:0')), ('power', tensor([-20.8493], device='cuda:0'))])
epoch£º749	 i:0 	 global-step:14980	 l-p:0.13651713728904724
epoch£º749	 i:1 	 global-step:14981	 l-p:0.12964463233947754
epoch£º749	 i:2 	 global-step:14982	 l-p:0.13048695027828217
epoch£º749	 i:3 	 global-step:14983	 l-p:0.13410772383213043
epoch£º749	 i:4 	 global-step:14984	 l-p:0.10882677137851715
epoch£º749	 i:5 	 global-step:14985	 l-p:0.15073314309120178
epoch£º749	 i:6 	 global-step:14986	 l-p:0.2631012499332428
epoch£º749	 i:7 	 global-step:14987	 l-p:0.19449305534362793
epoch£º749	 i:8 	 global-step:14988	 l-p:0.14257420599460602
epoch£º749	 i:9 	 global-step:14989	 l-p:0.12336009740829468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:750
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0908, 4.9909, 4.7013],
        [5.0908, 5.4015, 5.2880],
        [5.0908, 5.0864, 5.0906],
        [5.0908, 5.0853, 5.0905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:750, step:0 
model_pd.l_p.mean(): 0.11943304538726807 
model_pd.l_d.mean(): -20.249866485595703 
model_pd.lagr.mean(): -20.130434036254883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4748], device='cuda:0')), ('power', tensor([-21.1203], device='cuda:0'))])
epoch£º750	 i:0 	 global-step:15000	 l-p:0.11943304538726807
epoch£º750	 i:1 	 global-step:15001	 l-p:0.19255183637142181
epoch£º750	 i:2 	 global-step:15002	 l-p:0.19381649792194366
epoch£º750	 i:3 	 global-step:15003	 l-p:0.18669995665550232
epoch£º750	 i:4 	 global-step:15004	 l-p:0.03543315827846527
epoch£º750	 i:5 	 global-step:15005	 l-p:0.13571852445602417
epoch£º750	 i:6 	 global-step:15006	 l-p:0.12395082414150238
epoch£º750	 i:7 	 global-step:15007	 l-p:0.1217803955078125
epoch£º750	 i:8 	 global-step:15008	 l-p:0.1227908506989479
epoch£º750	 i:9 	 global-step:15009	 l-p:0.1323613077402115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:751
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1292, 5.0151, 5.0652],
        [5.1292, 5.0592, 4.7733],
        [5.1292, 5.1292, 5.1292],
        [5.1292, 4.9559, 4.9778]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:751, step:0 
model_pd.l_p.mean(): 0.14644373953342438 
model_pd.l_d.mean(): -20.50921630859375 
model_pd.lagr.mean(): -20.36277198791504 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4532], device='cuda:0')), ('power', tensor([-21.3621], device='cuda:0'))])
epoch£º751	 i:0 	 global-step:15020	 l-p:0.14644373953342438
epoch£º751	 i:1 	 global-step:15021	 l-p:0.1263570785522461
epoch£º751	 i:2 	 global-step:15022	 l-p:0.15507446229457855
epoch£º751	 i:3 	 global-step:15023	 l-p:0.16933844983577728
epoch£º751	 i:4 	 global-step:15024	 l-p:0.0771513283252716
epoch£º751	 i:5 	 global-step:15025	 l-p:0.11721764504909515
epoch£º751	 i:6 	 global-step:15026	 l-p:0.1444491595029831
epoch£º751	 i:7 	 global-step:15027	 l-p:0.13086122274398804
epoch£º751	 i:8 	 global-step:15028	 l-p:0.14904049038887024
epoch£º751	 i:9 	 global-step:15029	 l-p:0.09817318618297577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:752
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1103, 5.1104, 5.1104],
        [5.1103, 5.0690, 5.1006],
        [5.1103, 4.9809, 5.0287],
        [5.1103, 5.1060, 5.1101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:752, step:0 
model_pd.l_p.mean(): 0.10758624970912933 
model_pd.l_d.mean(): -19.308889389038086 
model_pd.lagr.mean(): -19.201303482055664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5422], device='cuda:0')), ('power', tensor([-20.2316], device='cuda:0'))])
epoch£º752	 i:0 	 global-step:15040	 l-p:0.10758624970912933
epoch£º752	 i:1 	 global-step:15041	 l-p:0.12392543256282806
epoch£º752	 i:2 	 global-step:15042	 l-p:0.09371241182088852
epoch£º752	 i:3 	 global-step:15043	 l-p:0.1321561187505722
epoch£º752	 i:4 	 global-step:15044	 l-p:0.17376194894313812
epoch£º752	 i:5 	 global-step:15045	 l-p:0.18033486604690552
epoch£º752	 i:6 	 global-step:15046	 l-p:0.1312924176454544
epoch£º752	 i:7 	 global-step:15047	 l-p:0.13688598573207855
epoch£º752	 i:8 	 global-step:15048	 l-p:0.1293708235025406
epoch£º752	 i:9 	 global-step:15049	 l-p:0.12162426859140396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:753
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1313, 5.1293, 5.1312],
        [5.1313, 5.1312, 5.1313],
        [5.1313, 4.9964, 5.0422],
        [5.1313, 4.9105, 4.8565]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:753, step:0 
model_pd.l_p.mean(): 0.1354880928993225 
model_pd.l_d.mean(): -20.426387786865234 
model_pd.lagr.mean(): -20.2908992767334 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4597], device='cuda:0')), ('power', tensor([-21.2845], device='cuda:0'))])
epoch£º753	 i:0 	 global-step:15060	 l-p:0.1354880928993225
epoch£º753	 i:1 	 global-step:15061	 l-p:0.14260618388652802
epoch£º753	 i:2 	 global-step:15062	 l-p:0.1317884624004364
epoch£º753	 i:3 	 global-step:15063	 l-p:0.16256222128868103
epoch£º753	 i:4 	 global-step:15064	 l-p:0.15714222192764282
epoch£º753	 i:5 	 global-step:15065	 l-p:0.06907957047224045
epoch£º753	 i:6 	 global-step:15066	 l-p:0.194084033370018
epoch£º753	 i:7 	 global-step:15067	 l-p:0.1503324657678604
epoch£º753	 i:8 	 global-step:15068	 l-p:0.13611169159412384
epoch£º753	 i:9 	 global-step:15069	 l-p:0.12609030306339264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:754
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1103, 5.1048, 5.1100],
        [5.1103, 5.1061, 5.1101],
        [5.1103, 5.2025, 4.9660],
        [5.1103, 5.1101, 5.1103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:754, step:0 
model_pd.l_p.mean(): 0.11638788878917694 
model_pd.l_d.mean(): -20.347808837890625 
model_pd.lagr.mean(): -20.231420516967773 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4514], device='cuda:0')), ('power', tensor([-21.1958], device='cuda:0'))])
epoch£º754	 i:0 	 global-step:15080	 l-p:0.11638788878917694
epoch£º754	 i:1 	 global-step:15081	 l-p:0.10015144944190979
epoch£º754	 i:2 	 global-step:15082	 l-p:0.1349867433309555
epoch£º754	 i:3 	 global-step:15083	 l-p:0.17621614038944244
epoch£º754	 i:4 	 global-step:15084	 l-p:0.17977125942707062
epoch£º754	 i:5 	 global-step:15085	 l-p:0.12168433517217636
epoch£º754	 i:6 	 global-step:15086	 l-p:0.1264558732509613
epoch£º754	 i:7 	 global-step:15087	 l-p:0.14406196773052216
epoch£º754	 i:8 	 global-step:15088	 l-p:0.13836389780044556
epoch£º754	 i:9 	 global-step:15089	 l-p:0.10874231159687042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:755
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1138, 4.9483, 4.9788],
        [5.1138, 5.0713, 5.1036],
        [5.1138, 5.0095, 4.7195],
        [5.1138, 4.8835, 4.8056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:755, step:0 
model_pd.l_p.mean(): 0.10511752218008041 
model_pd.l_d.mean(): -19.950389862060547 
model_pd.lagr.mean(): -19.845272064208984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4922], device='cuda:0')), ('power', tensor([-20.8332], device='cuda:0'))])
epoch£º755	 i:0 	 global-step:15100	 l-p:0.10511752218008041
epoch£º755	 i:1 	 global-step:15101	 l-p:0.12772679328918457
epoch£º755	 i:2 	 global-step:15102	 l-p:0.14793522655963898
epoch£º755	 i:3 	 global-step:15103	 l-p:0.15100842714309692
epoch£º755	 i:4 	 global-step:15104	 l-p:0.14532892405986786
epoch£º755	 i:5 	 global-step:15105	 l-p:0.21633557975292206
epoch£º755	 i:6 	 global-step:15106	 l-p:0.11593557894229889
epoch£º755	 i:7 	 global-step:15107	 l-p:0.13668204843997955
epoch£º755	 i:8 	 global-step:15108	 l-p:0.2693585157394409
epoch£º755	 i:9 	 global-step:15109	 l-p:0.21138562262058258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:756
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0413, 4.8960, 4.6038],
        [5.0413, 5.0413, 5.0413],
        [5.0413, 4.7977, 4.6118],
        [5.0413, 4.8016, 4.7126]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:756, step:0 
model_pd.l_p.mean(): 0.1192108690738678 
model_pd.l_d.mean(): -20.480361938476562 
model_pd.lagr.mean(): -20.36115074157715 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4553], device='cuda:0')), ('power', tensor([-21.3349], device='cuda:0'))])
epoch£º756	 i:0 	 global-step:15120	 l-p:0.1192108690738678
epoch£º756	 i:1 	 global-step:15121	 l-p:0.30394071340560913
epoch£º756	 i:2 	 global-step:15122	 l-p:-0.27308037877082825
epoch£º756	 i:3 	 global-step:15123	 l-p:0.1627938449382782
epoch£º756	 i:4 	 global-step:15124	 l-p:0.21460655331611633
epoch£º756	 i:5 	 global-step:15125	 l-p:0.1604493260383606
epoch£º756	 i:6 	 global-step:15126	 l-p:0.10743837058544159
epoch£º756	 i:7 	 global-step:15127	 l-p:0.13781249523162842
epoch£º756	 i:8 	 global-step:15128	 l-p:0.07383402436971664
epoch£º756	 i:9 	 global-step:15129	 l-p:0.15676265954971313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:757
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0822, 4.8765, 4.6215],
        [5.0822, 4.9706, 4.6779],
        [5.0822, 4.9452, 4.9921],
        [5.0822, 5.0822, 5.0822]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:757, step:0 
model_pd.l_p.mean(): 0.1457280069589615 
model_pd.l_d.mean(): -18.583683013916016 
model_pd.lagr.mean(): -18.437955856323242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5617], device='cuda:0')), ('power', tensor([-19.5130], device='cuda:0'))])
epoch£º757	 i:0 	 global-step:15140	 l-p:0.1457280069589615
epoch£º757	 i:1 	 global-step:15141	 l-p:0.15687741339206696
epoch£º757	 i:2 	 global-step:15142	 l-p:0.1492813527584076
epoch£º757	 i:3 	 global-step:15143	 l-p:0.1207723468542099
epoch£º757	 i:4 	 global-step:15144	 l-p:0.16630226373672485
epoch£º757	 i:5 	 global-step:15145	 l-p:0.13210470974445343
epoch£º757	 i:6 	 global-step:15146	 l-p:0.12052884697914124
epoch£º757	 i:7 	 global-step:15147	 l-p:0.0853361040353775
epoch£º757	 i:8 	 global-step:15148	 l-p:0.12237691879272461
epoch£º757	 i:9 	 global-step:15149	 l-p:0.17359836399555206
====================================================================================================
====================================================================================================
====================================================================================================

epoch:758
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1394, 4.9475, 4.9503],
        [5.1394, 4.9744, 5.0046],
        [5.1394, 5.0673, 5.1128],
        [5.1394, 4.9055, 4.8035]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:758, step:0 
model_pd.l_p.mean(): 0.07282619178295135 
model_pd.l_d.mean(): -20.603370666503906 
model_pd.lagr.mean(): -20.53054428100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4138], device='cuda:0')), ('power', tensor([-21.4172], device='cuda:0'))])
epoch£º758	 i:0 	 global-step:15160	 l-p:0.07282619178295135
epoch£º758	 i:1 	 global-step:15161	 l-p:0.13567957282066345
epoch£º758	 i:2 	 global-step:15162	 l-p:0.15729039907455444
epoch£º758	 i:3 	 global-step:15163	 l-p:0.10559213161468506
epoch£º758	 i:4 	 global-step:15164	 l-p:0.17524337768554688
epoch£º758	 i:5 	 global-step:15165	 l-p:0.11014758795499802
epoch£º758	 i:6 	 global-step:15166	 l-p:0.11227435618638992
epoch£º758	 i:7 	 global-step:15167	 l-p:0.09093479067087173
epoch£º758	 i:8 	 global-step:15168	 l-p:0.07196169346570969
epoch£º758	 i:9 	 global-step:15169	 l-p:0.1341339647769928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:759
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1795, 5.1794, 5.1795],
        [5.1795, 5.1795, 5.1795],
        [5.1795, 5.0706, 4.7842],
        [5.1795, 5.1795, 5.1795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:759, step:0 
model_pd.l_p.mean(): 0.06052178144454956 
model_pd.l_d.mean(): -19.690616607666016 
model_pd.lagr.mean(): -19.630094528198242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4541], device='cuda:0')), ('power', tensor([-20.5291], device='cuda:0'))])
epoch£º759	 i:0 	 global-step:15180	 l-p:0.06052178144454956
epoch£º759	 i:1 	 global-step:15181	 l-p:0.15110139548778534
epoch£º759	 i:2 	 global-step:15182	 l-p:0.12720812857151031
epoch£º759	 i:3 	 global-step:15183	 l-p:0.14591728150844574
epoch£º759	 i:4 	 global-step:15184	 l-p:0.11958228051662445
epoch£º759	 i:5 	 global-step:15185	 l-p:0.15444236993789673
epoch£º759	 i:6 	 global-step:15186	 l-p:0.08106596022844315
epoch£º759	 i:7 	 global-step:15187	 l-p:0.12818555533885956
epoch£º759	 i:8 	 global-step:15188	 l-p:-0.03702922165393829
epoch£º759	 i:9 	 global-step:15189	 l-p:0.11571186035871506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:760
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1719, 5.1390, 5.1654],
        [5.1719, 4.9815, 4.9843],
        [5.1719, 4.9409, 4.7609],
        [5.1719, 5.1018, 5.1466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:760, step:0 
model_pd.l_p.mean(): 0.02768736332654953 
model_pd.l_d.mean(): -19.121484756469727 
model_pd.lagr.mean(): -19.09379768371582 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5454], device='cuda:0')), ('power', tensor([-20.0439], device='cuda:0'))])
epoch£º760	 i:0 	 global-step:15200	 l-p:0.02768736332654953
epoch£º760	 i:1 	 global-step:15201	 l-p:0.12557588517665863
epoch£º760	 i:2 	 global-step:15202	 l-p:0.06641367822885513
epoch£º760	 i:3 	 global-step:15203	 l-p:0.1466878354549408
epoch£º760	 i:4 	 global-step:15204	 l-p:0.1348213404417038
epoch£º760	 i:5 	 global-step:15205	 l-p:0.12221557646989822
epoch£º760	 i:6 	 global-step:15206	 l-p:0.09128805994987488
epoch£º760	 i:7 	 global-step:15207	 l-p:0.1396457701921463
epoch£º760	 i:8 	 global-step:15208	 l-p:0.14444895088672638
epoch£º760	 i:9 	 global-step:15209	 l-p:0.11841383576393127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:761
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1261, 5.1092, 5.1240],
        [5.1261, 4.9262, 4.6697],
        [5.1261, 4.9119, 4.8813],
        [5.1261, 5.0735, 4.7863]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:761, step:0 
model_pd.l_p.mean(): 0.13567820191383362 
model_pd.l_d.mean(): -18.289167404174805 
model_pd.lagr.mean(): -18.15349006652832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6078], device='cuda:0')), ('power', tensor([-19.2607], device='cuda:0'))])
epoch£º761	 i:0 	 global-step:15220	 l-p:0.13567820191383362
epoch£º761	 i:1 	 global-step:15221	 l-p:0.09189385920763016
epoch£º761	 i:2 	 global-step:15222	 l-p:0.1252930462360382
epoch£º761	 i:3 	 global-step:15223	 l-p:0.13798724114894867
epoch£º761	 i:4 	 global-step:15224	 l-p:0.1341664046049118
epoch£º761	 i:5 	 global-step:15225	 l-p:1.4018687009811401
epoch£º761	 i:6 	 global-step:15226	 l-p:0.1882316917181015
epoch£º761	 i:7 	 global-step:15227	 l-p:0.15849849581718445
epoch£º761	 i:8 	 global-step:15228	 l-p:0.11122459173202515
epoch£º761	 i:9 	 global-step:15229	 l-p:0.11979978531599045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:762
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0092, 4.8013, 4.7964],
        [5.0092, 4.7887, 4.7634],
        [5.0092, 4.7696, 4.5434],
        [5.0092, 5.0092, 5.0092]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:762, step:0 
model_pd.l_p.mean(): 0.13657651841640472 
model_pd.l_d.mean(): -20.563282012939453 
model_pd.lagr.mean(): -20.426706314086914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4699], device='cuda:0')), ('power', tensor([-21.4345], device='cuda:0'))])
epoch£º762	 i:0 	 global-step:15240	 l-p:0.13657651841640472
epoch£º762	 i:1 	 global-step:15241	 l-p:0.4284907579421997
epoch£º762	 i:2 	 global-step:15242	 l-p:0.13602299988269806
epoch£º762	 i:3 	 global-step:15243	 l-p:0.13236534595489502
epoch£º762	 i:4 	 global-step:15244	 l-p:0.13473349809646606
epoch£º762	 i:5 	 global-step:15245	 l-p:0.13094548881053925
epoch£º762	 i:6 	 global-step:15246	 l-p:0.3223974406719208
epoch£º762	 i:7 	 global-step:15247	 l-p:-0.017639050260186195
epoch£º762	 i:8 	 global-step:15248	 l-p:0.30503129959106445
epoch£º762	 i:9 	 global-step:15249	 l-p:0.0645090639591217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:763
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9999, 4.7464, 4.6104],
        [4.9999, 4.9944, 4.9996],
        [4.9999, 4.9572, 4.9899],
        [4.9999, 5.1688, 4.9713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:763, step:0 
model_pd.l_p.mean(): 0.1542132943868637 
model_pd.l_d.mean(): -19.314062118530273 
model_pd.lagr.mean(): -19.159849166870117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5479], device='cuda:0')), ('power', tensor([-20.2427], device='cuda:0'))])
epoch£º763	 i:0 	 global-step:15260	 l-p:0.1542132943868637
epoch£º763	 i:1 	 global-step:15261	 l-p:-0.2265222817659378
epoch£º763	 i:2 	 global-step:15262	 l-p:0.10881029814481735
epoch£º763	 i:3 	 global-step:15263	 l-p:0.12271282076835632
epoch£º763	 i:4 	 global-step:15264	 l-p:0.14738088846206665
epoch£º763	 i:5 	 global-step:15265	 l-p:0.1275985687971115
epoch£º763	 i:6 	 global-step:15266	 l-p:0.5297166109085083
epoch£º763	 i:7 	 global-step:15267	 l-p:0.1574125736951828
epoch£º763	 i:8 	 global-step:15268	 l-p:0.14622753858566284
epoch£º763	 i:9 	 global-step:15269	 l-p:0.14480143785476685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:764
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0886, 5.0109, 5.0586],
        [5.0886, 5.0848, 5.0884],
        [5.0886, 4.8610, 4.6320],
        [5.0886, 4.8712, 4.8415]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:764, step:0 
model_pd.l_p.mean(): 0.17684897780418396 
model_pd.l_d.mean(): -20.638669967651367 
model_pd.lagr.mean(): -20.461820602416992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4400], device='cuda:0')), ('power', tensor([-21.4803], device='cuda:0'))])
epoch£º764	 i:0 	 global-step:15280	 l-p:0.17684897780418396
epoch£º764	 i:1 	 global-step:15281	 l-p:0.10113634169101715
epoch£º764	 i:2 	 global-step:15282	 l-p:0.14681091904640198
epoch£º764	 i:3 	 global-step:15283	 l-p:0.17096996307373047
epoch£º764	 i:4 	 global-step:15284	 l-p:0.12830406427383423
epoch£º764	 i:5 	 global-step:15285	 l-p:0.12754786014556885
epoch£º764	 i:6 	 global-step:15286	 l-p:0.13071630895137787
epoch£º764	 i:7 	 global-step:15287	 l-p:0.17245878279209137
epoch£º764	 i:8 	 global-step:15288	 l-p:0.06194646656513214
epoch£º764	 i:9 	 global-step:15289	 l-p:0.07882197201251984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:765
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1547, 5.0486, 5.0998],
        [5.1547, 5.1487, 5.1544],
        [5.1547, 5.1546, 5.1547],
        [5.1547, 5.0345, 5.0847]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:765, step:0 
model_pd.l_p.mean(): 0.11784511059522629 
model_pd.l_d.mean(): -18.4334774017334 
model_pd.lagr.mean(): -18.315631866455078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5718], device='cuda:0')), ('power', tensor([-19.3704], device='cuda:0'))])
epoch£º765	 i:0 	 global-step:15300	 l-p:0.11784511059522629
epoch£º765	 i:1 	 global-step:15301	 l-p:0.17769482731819153
epoch£º765	 i:2 	 global-step:15302	 l-p:0.09444043040275574
epoch£º765	 i:3 	 global-step:15303	 l-p:0.11949128657579422
epoch£º765	 i:4 	 global-step:15304	 l-p:0.16065894067287445
epoch£º765	 i:5 	 global-step:15305	 l-p:0.1314387172460556
epoch£º765	 i:6 	 global-step:15306	 l-p:2.598102331161499
epoch£º765	 i:7 	 global-step:15307	 l-p:0.13009683787822723
epoch£º765	 i:8 	 global-step:15308	 l-p:0.08114482462406158
epoch£º765	 i:9 	 global-step:15309	 l-p:0.08986280858516693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:766
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2149, 5.5782, 5.4917],
        [5.2149, 4.9822, 4.8552],
        [5.2149, 5.1671, 5.2022],
        [5.2149, 5.3325, 5.1046]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:766, step:0 
model_pd.l_p.mean(): 0.12363339215517044 
model_pd.l_d.mean(): -20.358057022094727 
model_pd.lagr.mean(): -20.23442268371582 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4316], device='cuda:0')), ('power', tensor([-21.1858], device='cuda:0'))])
epoch£º766	 i:0 	 global-step:15320	 l-p:0.12363339215517044
epoch£º766	 i:1 	 global-step:15321	 l-p:0.12679681181907654
epoch£º766	 i:2 	 global-step:15322	 l-p:0.14321038126945496
epoch£º766	 i:3 	 global-step:15323	 l-p:-1.2353814840316772
epoch£º766	 i:4 	 global-step:15324	 l-p:0.14444759488105774
epoch£º766	 i:5 	 global-step:15325	 l-p:0.12660332024097443
epoch£º766	 i:6 	 global-step:15326	 l-p:0.10515329241752625
epoch£º766	 i:7 	 global-step:15327	 l-p:0.11896086484193802
epoch£º766	 i:8 	 global-step:15328	 l-p:0.09198299050331116
epoch£º766	 i:9 	 global-step:15329	 l-p:0.1307361125946045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:767
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1911, 5.1276, 5.1700],
        [5.1911, 5.1910, 5.1911],
        [5.1911, 5.1911, 5.1911],
        [5.1911, 5.4424, 5.2866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:767, step:0 
model_pd.l_p.mean(): 0.11944469064474106 
model_pd.l_d.mean(): -20.633420944213867 
model_pd.lagr.mean(): -20.51397705078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3958], device='cuda:0')), ('power', tensor([-21.4292], device='cuda:0'))])
epoch£º767	 i:0 	 global-step:15340	 l-p:0.11944469064474106
epoch£º767	 i:1 	 global-step:15341	 l-p:0.07424230873584747
epoch£º767	 i:2 	 global-step:15342	 l-p:0.11590046435594559
epoch£º767	 i:3 	 global-step:15343	 l-p:0.1148233488202095
epoch£º767	 i:4 	 global-step:15344	 l-p:0.1342516988515854
epoch£º767	 i:5 	 global-step:15345	 l-p:0.14103823900222778
epoch£º767	 i:6 	 global-step:15346	 l-p:0.1895163506269455
epoch£º767	 i:7 	 global-step:15347	 l-p:0.17579986155033112
epoch£º767	 i:8 	 global-step:15348	 l-p:0.14366759359836578
epoch£º767	 i:9 	 global-step:15349	 l-p:0.06706305593252182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:768
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1287, 4.8862, 4.7520],
        [5.1287, 5.6129, 5.6113],
        [5.1287, 5.0801, 5.1159],
        [5.1287, 5.0479, 4.7548]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:768, step:0 
model_pd.l_p.mean(): 0.16960391402244568 
model_pd.l_d.mean(): -19.346010208129883 
model_pd.lagr.mean(): -19.176406860351562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5026], device='cuda:0')), ('power', tensor([-20.2284], device='cuda:0'))])
epoch£º768	 i:0 	 global-step:15360	 l-p:0.16960391402244568
epoch£º768	 i:1 	 global-step:15361	 l-p:0.12539933621883392
epoch£º768	 i:2 	 global-step:15362	 l-p:0.15515251457691193
epoch£º768	 i:3 	 global-step:15363	 l-p:0.09775612503290176
epoch£º768	 i:4 	 global-step:15364	 l-p:0.11402406543493271
epoch£º768	 i:5 	 global-step:15365	 l-p:0.03447829559445381
epoch£º768	 i:6 	 global-step:15366	 l-p:0.13554953038692474
epoch£º768	 i:7 	 global-step:15367	 l-p:0.18190984427928925
epoch£º768	 i:8 	 global-step:15368	 l-p:0.13771848380565643
epoch£º768	 i:9 	 global-step:15369	 l-p:0.11125031858682632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:769
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1435, 4.9079, 4.7131],
        [5.1435, 5.1135, 4.8294],
        [5.1435, 5.1051, 4.8191],
        [5.1435, 5.1293, 5.1419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:769, step:0 
model_pd.l_p.mean(): 0.12609414756298065 
model_pd.l_d.mean(): -20.09950828552246 
model_pd.lagr.mean(): -19.973413467407227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4741], device='cuda:0')), ('power', tensor([-20.9664], device='cuda:0'))])
epoch£º769	 i:0 	 global-step:15380	 l-p:0.12609414756298065
epoch£º769	 i:1 	 global-step:15381	 l-p:0.1556817889213562
epoch£º769	 i:2 	 global-step:15382	 l-p:0.09808242321014404
epoch£º769	 i:3 	 global-step:15383	 l-p:0.22063040733337402
epoch£º769	 i:4 	 global-step:15384	 l-p:0.10858220607042313
epoch£º769	 i:5 	 global-step:15385	 l-p:0.12331521511077881
epoch£º769	 i:6 	 global-step:15386	 l-p:0.15360599756240845
epoch£º769	 i:7 	 global-step:15387	 l-p:0.11834736913442612
epoch£º769	 i:8 	 global-step:15388	 l-p:0.17758913338184357
epoch£º769	 i:9 	 global-step:15389	 l-p:0.1319953054189682
====================================================================================================
====================================================================================================
====================================================================================================

epoch:770
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0858, 5.0857, 5.0858],
        [5.0858, 5.0185, 4.7245],
        [5.0858, 4.8494, 4.6316],
        [5.0858, 4.9683, 5.0204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:770, step:0 
model_pd.l_p.mean(): 0.2214917689561844 
model_pd.l_d.mean(): -18.196714401245117 
model_pd.lagr.mean(): -17.975223541259766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6187], device='cuda:0')), ('power', tensor([-19.1778], device='cuda:0'))])
epoch£º770	 i:0 	 global-step:15400	 l-p:0.2214917689561844
epoch£º770	 i:1 	 global-step:15401	 l-p:0.14712950587272644
epoch£º770	 i:2 	 global-step:15402	 l-p:0.11412395536899567
epoch£º770	 i:3 	 global-step:15403	 l-p:0.20217138528823853
epoch£º770	 i:4 	 global-step:15404	 l-p:0.14651761949062347
epoch£º770	 i:5 	 global-step:15405	 l-p:0.12580692768096924
epoch£º770	 i:6 	 global-step:15406	 l-p:0.12627647817134857
epoch£º770	 i:7 	 global-step:15407	 l-p:0.1483280211687088
epoch£º770	 i:8 	 global-step:15408	 l-p:0.10700999945402145
epoch£º770	 i:9 	 global-step:15409	 l-p:0.19760237634181976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:771
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0550, 5.0550, 5.0550],
        [5.0550, 5.0113, 5.0446],
        [5.0550, 5.0503, 5.0548],
        [5.0550, 4.8052, 4.6256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:771, step:0 
model_pd.l_p.mean(): 0.12610118091106415 
model_pd.l_d.mean(): -20.633758544921875 
model_pd.lagr.mean(): -20.507658004760742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4241], device='cuda:0')), ('power', tensor([-21.4589], device='cuda:0'))])
epoch£º771	 i:0 	 global-step:15420	 l-p:0.12610118091106415
epoch£º771	 i:1 	 global-step:15421	 l-p:0.11365842819213867
epoch£º771	 i:2 	 global-step:15422	 l-p:0.3715302050113678
epoch£º771	 i:3 	 global-step:15423	 l-p:0.1364060491323471
epoch£º771	 i:4 	 global-step:15424	 l-p:0.3525026738643646
epoch£º771	 i:5 	 global-step:15425	 l-p:0.14871720969676971
epoch£º771	 i:6 	 global-step:15426	 l-p:0.13667696714401245
epoch£º771	 i:7 	 global-step:15427	 l-p:0.17215275764465332
epoch£º771	 i:8 	 global-step:15428	 l-p:0.21709677577018738
epoch£º771	 i:9 	 global-step:15429	 l-p:0.17395871877670288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:772
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0477, 5.0053, 5.0379],
        [5.0477, 4.9731, 5.0203],
        [5.0477, 5.3272, 5.1914],
        [5.0477, 5.0039, 5.0372]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:772, step:0 
model_pd.l_p.mean(): -1.4845162630081177 
model_pd.l_d.mean(): -20.65106773376465 
model_pd.lagr.mean(): -22.135583877563477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4423], device='cuda:0')), ('power', tensor([-21.4953], device='cuda:0'))])
epoch£º772	 i:0 	 global-step:15440	 l-p:-1.4845162630081177
epoch£º772	 i:1 	 global-step:15441	 l-p:0.14673484861850739
epoch£º772	 i:2 	 global-step:15442	 l-p:0.09024398773908615
epoch£º772	 i:3 	 global-step:15443	 l-p:0.143683061003685
epoch£º772	 i:4 	 global-step:15444	 l-p:0.18548890948295593
epoch£º772	 i:5 	 global-step:15445	 l-p:0.1139860525727272
epoch£º772	 i:6 	 global-step:15446	 l-p:0.11507972329854965
epoch£º772	 i:7 	 global-step:15447	 l-p:0.17485632002353668
epoch£º772	 i:8 	 global-step:15448	 l-p:0.07481126487255096
epoch£º772	 i:9 	 global-step:15449	 l-p:0.16228453814983368
====================================================================================================
====================================================================================================
====================================================================================================

epoch:773
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1401, 5.1401, 5.1401],
        [5.1401, 5.1022, 5.1319],
        [5.1401, 4.9056, 4.8267],
        [5.1401, 4.9084, 4.8398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:773, step:0 
model_pd.l_p.mean(): 0.13464035093784332 
model_pd.l_d.mean(): -20.848777770996094 
model_pd.lagr.mean(): -20.71413803100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3810], device='cuda:0')), ('power', tensor([-21.6333], device='cuda:0'))])
epoch£º773	 i:0 	 global-step:15460	 l-p:0.13464035093784332
epoch£º773	 i:1 	 global-step:15461	 l-p:0.13174472749233246
epoch£º773	 i:2 	 global-step:15462	 l-p:0.08967021107673645
epoch£º773	 i:3 	 global-step:15463	 l-p:0.07620877772569656
epoch£º773	 i:4 	 global-step:15464	 l-p:0.1411876380443573
epoch£º773	 i:5 	 global-step:15465	 l-p:0.15928520262241364
epoch£º773	 i:6 	 global-step:15466	 l-p:-71.51789093017578
epoch£º773	 i:7 	 global-step:15467	 l-p:0.13207855820655823
epoch£º773	 i:8 	 global-step:15468	 l-p:0.11666892468929291
epoch£º773	 i:9 	 global-step:15469	 l-p:0.10923554003238678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:774
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2305, 5.1317, 5.1823],
        [5.2305, 5.1904, 5.2213],
        [5.2305, 4.9962, 4.8687],
        [5.2305, 5.2112, 5.2279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:774, step:0 
model_pd.l_p.mean(): 0.08519286662340164 
model_pd.l_d.mean(): -20.556493759155273 
model_pd.lagr.mean(): -20.47130012512207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4045], device='cuda:0')), ('power', tensor([-21.3598], device='cuda:0'))])
epoch£º774	 i:0 	 global-step:15480	 l-p:0.08519286662340164
epoch£º774	 i:1 	 global-step:15481	 l-p:0.11842425912618637
epoch£º774	 i:2 	 global-step:15482	 l-p:0.12790004909038544
epoch£º774	 i:3 	 global-step:15483	 l-p:0.21470104157924652
epoch£º774	 i:4 	 global-step:15484	 l-p:0.13584695756435394
epoch£º774	 i:5 	 global-step:15485	 l-p:0.06745738536119461
epoch£º774	 i:6 	 global-step:15486	 l-p:0.13684222102165222
epoch£º774	 i:7 	 global-step:15487	 l-p:0.7161652445793152
epoch£º774	 i:8 	 global-step:15488	 l-p:0.1250547170639038
epoch£º774	 i:9 	 global-step:15489	 l-p:0.1170448288321495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:775
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2719, 5.0443, 4.9475],
        [5.2719, 5.0595, 5.0185],
        [5.2719, 5.2719, 5.2719],
        [5.2719, 5.3996, 5.1744]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:775, step:0 
model_pd.l_p.mean(): 0.12550893425941467 
model_pd.l_d.mean(): -18.662796020507812 
model_pd.lagr.mean(): -18.53728675842285 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5967], device='cuda:0')), ('power', tensor([-19.6298], device='cuda:0'))])
epoch£º775	 i:0 	 global-step:15500	 l-p:0.12550893425941467
epoch£º775	 i:1 	 global-step:15501	 l-p:0.14185301959514618
epoch£º775	 i:2 	 global-step:15502	 l-p:0.0952163115143776
epoch£º775	 i:3 	 global-step:15503	 l-p:0.36713728308677673
epoch£º775	 i:4 	 global-step:15504	 l-p:0.09125295281410217
epoch£º775	 i:5 	 global-step:15505	 l-p:0.11293128877878189
epoch£º775	 i:6 	 global-step:15506	 l-p:0.1686495691537857
epoch£º775	 i:7 	 global-step:15507	 l-p:0.13850198686122894
epoch£º775	 i:8 	 global-step:15508	 l-p:0.13826805353164673
epoch£º775	 i:9 	 global-step:15509	 l-p:0.047662392258644104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:776
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]], device='cuda:0')
 pt:tensor([[5.2384, 5.0647, 5.0867],
        [5.2384, 5.0177, 4.9621],
        [5.2384, 5.0234, 4.9828],
        [5.2384, 5.6049, 5.5180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:776, step:0 
model_pd.l_p.mean(): 0.20399437844753265 
model_pd.l_d.mean(): -20.44610595703125 
model_pd.lagr.mean(): -20.242111206054688 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4242], device='cuda:0')), ('power', tensor([-21.2678], device='cuda:0'))])
epoch£º776	 i:0 	 global-step:15520	 l-p:0.20399437844753265
epoch£º776	 i:1 	 global-step:15521	 l-p:0.1196732297539711
epoch£º776	 i:2 	 global-step:15522	 l-p:0.13724756240844727
epoch£º776	 i:3 	 global-step:15523	 l-p:0.1271214783191681
epoch£º776	 i:4 	 global-step:15524	 l-p:0.12485913932323456
epoch£º776	 i:5 	 global-step:15525	 l-p:0.11173674464225769
epoch£º776	 i:6 	 global-step:15526	 l-p:0.13035745918750763
epoch£º776	 i:7 	 global-step:15527	 l-p:0.08948463201522827
epoch£º776	 i:8 	 global-step:15528	 l-p:0.15155068039894104
epoch£º776	 i:9 	 global-step:15529	 l-p:0.1335413157939911
====================================================================================================
====================================================================================================
====================================================================================================

epoch:777
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1467, 4.9321, 4.9077],
        [5.1467, 5.0615, 5.1112],
        [5.1467, 4.9208, 4.8717],
        [5.1467, 4.9023, 4.7499]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:777, step:0 
model_pd.l_p.mean(): 0.118779256939888 
model_pd.l_d.mean(): -20.449329376220703 
model_pd.lagr.mean(): -20.330549240112305 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4378], device='cuda:0')), ('power', tensor([-21.2852], device='cuda:0'))])
epoch£º777	 i:0 	 global-step:15540	 l-p:0.118779256939888
epoch£º777	 i:1 	 global-step:15541	 l-p:0.1431039273738861
epoch£º777	 i:2 	 global-step:15542	 l-p:0.1527080088853836
epoch£º777	 i:3 	 global-step:15543	 l-p:0.16290026903152466
epoch£º777	 i:4 	 global-step:15544	 l-p:0.16256900131702423
epoch£º777	 i:5 	 global-step:15545	 l-p:0.09140234440565109
epoch£º777	 i:6 	 global-step:15546	 l-p:0.14172405004501343
epoch£º777	 i:7 	 global-step:15547	 l-p:0.23969408869743347
epoch£º777	 i:8 	 global-step:15548	 l-p:0.05245771259069443
epoch£º777	 i:9 	 global-step:15549	 l-p:0.16861492395401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:778
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0880, 5.0871, 5.0880],
        [5.0880, 5.0081, 5.0569],
        [5.0880, 5.0707, 5.0859],
        [5.0880, 5.0811, 5.0876]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:778, step:0 
model_pd.l_p.mean(): 0.18575453758239746 
model_pd.l_d.mean(): -18.899837493896484 
model_pd.lagr.mean(): -18.714082717895508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6133], device='cuda:0')), ('power', tensor([-19.8885], device='cuda:0'))])
epoch£º778	 i:0 	 global-step:15560	 l-p:0.18575453758239746
epoch£º778	 i:1 	 global-step:15561	 l-p:0.15049467980861664
epoch£º778	 i:2 	 global-step:15562	 l-p:0.08971518278121948
epoch£º778	 i:3 	 global-step:15563	 l-p:0.19873403012752533
epoch£º778	 i:4 	 global-step:15564	 l-p:0.09702540934085846
epoch£º778	 i:5 	 global-step:15565	 l-p:0.07863982766866684
epoch£º778	 i:6 	 global-step:15566	 l-p:0.17381751537322998
epoch£º778	 i:7 	 global-step:15567	 l-p:0.14455489814281464
epoch£º778	 i:8 	 global-step:15568	 l-p:0.13874608278274536
epoch£º778	 i:9 	 global-step:15569	 l-p:0.14895999431610107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:779
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1400, 4.9795, 5.0171],
        [5.1400, 4.8974, 4.7112],
        [5.1400, 5.1399, 5.1400],
        [5.1400, 5.0608, 5.1091]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:779, step:0 
model_pd.l_p.mean(): 0.11296586692333221 
model_pd.l_d.mean(): -20.334299087524414 
model_pd.lagr.mean(): -20.221332550048828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4499], device='cuda:0')), ('power', tensor([-21.1805], device='cuda:0'))])
epoch£º779	 i:0 	 global-step:15580	 l-p:0.11296586692333221
epoch£º779	 i:1 	 global-step:15581	 l-p:0.16870790719985962
epoch£º779	 i:2 	 global-step:15582	 l-p:0.13232070207595825
epoch£º779	 i:3 	 global-step:15583	 l-p:0.12241344153881073
epoch£º779	 i:4 	 global-step:15584	 l-p:0.153666153550148
epoch£º779	 i:5 	 global-step:15585	 l-p:0.05818270519375801
epoch£º779	 i:6 	 global-step:15586	 l-p:0.13184109330177307
epoch£º779	 i:7 	 global-step:15587	 l-p:0.13790635764598846
epoch£º779	 i:8 	 global-step:15588	 l-p:0.1527855396270752
epoch£º779	 i:9 	 global-step:15589	 l-p:0.16347983479499817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:780
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1227, 5.1173, 5.1224],
        [5.1227, 5.1039, 5.1202],
        [5.1227, 5.2635, 5.0456],
        [5.1227, 5.2568, 5.0354]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:780, step:0 
model_pd.l_p.mean(): 0.17480742931365967 
model_pd.l_d.mean(): -18.50448989868164 
model_pd.lagr.mean(): -18.329683303833008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5707], device='cuda:0')), ('power', tensor([-19.4416], device='cuda:0'))])
epoch£º780	 i:0 	 global-step:15600	 l-p:0.17480742931365967
epoch£º780	 i:1 	 global-step:15601	 l-p:0.06430885195732117
epoch£º780	 i:2 	 global-step:15602	 l-p:0.14045138657093048
epoch£º780	 i:3 	 global-step:15603	 l-p:0.1111157089471817
epoch£º780	 i:4 	 global-step:15604	 l-p:0.15715061128139496
epoch£º780	 i:5 	 global-step:15605	 l-p:0.11292137950658798
epoch£º780	 i:6 	 global-step:15606	 l-p:0.1733764261007309
epoch£º780	 i:7 	 global-step:15607	 l-p:0.11435779184103012
epoch£º780	 i:8 	 global-step:15608	 l-p:0.07245005667209625
epoch£º780	 i:9 	 global-step:15609	 l-p:0.12966525554656982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:781
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1945, 5.3531, 5.1431],
        [5.1945, 5.1945, 5.1945],
        [5.1945, 5.1884, 5.1941],
        [5.1945, 5.1917, 5.1944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:781, step:0 
model_pd.l_p.mean(): 0.16417942941188812 
model_pd.l_d.mean(): -20.33951759338379 
model_pd.lagr.mean(): -20.175338745117188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4485], device='cuda:0')), ('power', tensor([-21.1844], device='cuda:0'))])
epoch£º781	 i:0 	 global-step:15620	 l-p:0.16417942941188812
epoch£º781	 i:1 	 global-step:15621	 l-p:0.12138208746910095
epoch£º781	 i:2 	 global-step:15622	 l-p:0.11683174222707748
epoch£º781	 i:3 	 global-step:15623	 l-p:0.09513470530509949
epoch£º781	 i:4 	 global-step:15624	 l-p:0.12571486830711365
epoch£º781	 i:5 	 global-step:15625	 l-p:0.04938044026494026
epoch£º781	 i:6 	 global-step:15626	 l-p:0.10990022122859955
epoch£º781	 i:7 	 global-step:15627	 l-p:-0.4606122374534607
epoch£º781	 i:8 	 global-step:15628	 l-p:0.1366853266954422
epoch£º781	 i:9 	 global-step:15629	 l-p:0.11905059218406677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:782
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2035, 5.1853, 5.2012],
        [5.2035, 5.2614, 5.0042],
        [5.2035, 5.1392, 5.1822],
        [5.2035, 5.1353, 5.1798]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:782, step:0 
model_pd.l_p.mean(): 0.019634949043393135 
model_pd.l_d.mean(): -19.964065551757812 
model_pd.lagr.mean(): -19.94443130493164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4194], device='cuda:0')), ('power', tensor([-20.7717], device='cuda:0'))])
epoch£º782	 i:0 	 global-step:15640	 l-p:0.019634949043393135
epoch£º782	 i:1 	 global-step:15641	 l-p:0.14320968091487885
epoch£º782	 i:2 	 global-step:15642	 l-p:0.12287382036447525
epoch£º782	 i:3 	 global-step:15643	 l-p:0.13083204627037048
epoch£º782	 i:4 	 global-step:15644	 l-p:0.13186189532279968
epoch£º782	 i:5 	 global-step:15645	 l-p:0.11483054608106613
epoch£º782	 i:6 	 global-step:15646	 l-p:0.07642707973718643
epoch£º782	 i:7 	 global-step:15647	 l-p:0.1278752088546753
epoch£º782	 i:8 	 global-step:15648	 l-p:0.15588805079460144
epoch£º782	 i:9 	 global-step:15649	 l-p:0.17330917716026306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:783
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1252, 5.0661, 4.7714],
        [5.1252, 4.8930, 4.8361],
        [5.1252, 5.0324, 4.7336],
        [5.1252, 5.0801, 5.1141]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:783, step:0 
model_pd.l_p.mean(): 0.16235679388046265 
model_pd.l_d.mean(): -20.511816024780273 
model_pd.lagr.mean(): -20.349458694458008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4211], device='cuda:0')), ('power', tensor([-21.3315], device='cuda:0'))])
epoch£º783	 i:0 	 global-step:15660	 l-p:0.16235679388046265
epoch£º783	 i:1 	 global-step:15661	 l-p:0.16432306170463562
epoch£º783	 i:2 	 global-step:15662	 l-p:0.1273726373910904
epoch£º783	 i:3 	 global-step:15663	 l-p:0.12083166092634201
epoch£º783	 i:4 	 global-step:15664	 l-p:0.12995165586471558
epoch£º783	 i:5 	 global-step:15665	 l-p:0.10285534709692001
epoch£º783	 i:6 	 global-step:15666	 l-p:0.1699906885623932
epoch£º783	 i:7 	 global-step:15667	 l-p:0.15110178291797638
epoch£º783	 i:8 	 global-step:15668	 l-p:0.09463243186473846
epoch£º783	 i:9 	 global-step:15669	 l-p:0.18953755497932434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:784
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0929, 5.0236, 4.7255],
        [5.0929, 5.0929, 5.0929],
        [5.0929, 4.9887, 5.0418],
        [5.0929, 4.9806, 5.0339]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:784, step:0 
model_pd.l_p.mean(): 0.13380563259124756 
model_pd.l_d.mean(): -20.07330322265625 
model_pd.lagr.mean(): -19.939496994018555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5184], device='cuda:0')), ('power', tensor([-20.9856], device='cuda:0'))])
epoch£º784	 i:0 	 global-step:15680	 l-p:0.13380563259124756
epoch£º784	 i:1 	 global-step:15681	 l-p:0.13929972052574158
epoch£º784	 i:2 	 global-step:15682	 l-p:0.10898356139659882
epoch£º784	 i:3 	 global-step:15683	 l-p:0.19158945977687836
epoch£º784	 i:4 	 global-step:15684	 l-p:0.10231097787618637
epoch£º784	 i:5 	 global-step:15685	 l-p:0.13697855174541473
epoch£º784	 i:6 	 global-step:15686	 l-p:0.18123608827590942
epoch£º784	 i:7 	 global-step:15687	 l-p:0.16165803372859955
epoch£º784	 i:8 	 global-step:15688	 l-p:0.10881608724594116
epoch£º784	 i:9 	 global-step:15689	 l-p:0.1325070708990097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:785
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1278, 5.1278, 5.1278],
        [5.1278, 5.1232, 5.1275],
        [5.1278, 4.9080, 4.6560],
        [5.1278, 4.9553, 4.9865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:785, step:0 
model_pd.l_p.mean(): 0.0980621725320816 
model_pd.l_d.mean(): -19.787059783935547 
model_pd.lagr.mean(): -19.688997268676758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4555], device='cuda:0')), ('power', tensor([-20.6288], device='cuda:0'))])
epoch£º785	 i:0 	 global-step:15700	 l-p:0.0980621725320816
epoch£º785	 i:1 	 global-step:15701	 l-p:0.1537265032529831
epoch£º785	 i:2 	 global-step:15702	 l-p:0.08809024840593338
epoch£º785	 i:3 	 global-step:15703	 l-p:0.11096230894327164
epoch£º785	 i:4 	 global-step:15704	 l-p:0.1346307098865509
epoch£º785	 i:5 	 global-step:15705	 l-p:0.22044837474822998
epoch£º785	 i:6 	 global-step:15706	 l-p:0.16194145381450653
epoch£º785	 i:7 	 global-step:15707	 l-p:0.18770068883895874
epoch£º785	 i:8 	 global-step:15708	 l-p:0.12235035747289658
epoch£º785	 i:9 	 global-step:15709	 l-p:0.1713254451751709
====================================================================================================
====================================================================================================
====================================================================================================

epoch:786
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1083, 5.1083, 5.1083],
        [5.1083, 5.4824, 5.4028],
        [5.1083, 5.0781, 4.7886],
        [5.1083, 5.0425, 5.0866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:786, step:0 
model_pd.l_p.mean(): 0.1300428807735443 
model_pd.l_d.mean(): -20.56696319580078 
model_pd.lagr.mean(): -20.436920166015625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4278], device='cuda:0')), ('power', tensor([-21.3946], device='cuda:0'))])
epoch£º786	 i:0 	 global-step:15720	 l-p:0.1300428807735443
epoch£º786	 i:1 	 global-step:15721	 l-p:0.1391662061214447
epoch£º786	 i:2 	 global-step:15722	 l-p:0.15247134864330292
epoch£º786	 i:3 	 global-step:15723	 l-p:0.19307947158813477
epoch£º786	 i:4 	 global-step:15724	 l-p:0.11892194300889969
epoch£º786	 i:5 	 global-step:15725	 l-p:0.1031731590628624
epoch£º786	 i:6 	 global-step:15726	 l-p:0.126789391040802
epoch£º786	 i:7 	 global-step:15727	 l-p:0.15264290571212769
epoch£º786	 i:8 	 global-step:15728	 l-p:0.114842489361763
epoch£º786	 i:9 	 global-step:15729	 l-p:0.15199309587478638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:787
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1418, 5.0760, 4.7799],
        [5.1418, 5.0967, 5.1308],
        [5.1418, 5.1233, 5.1394],
        [5.1418, 5.0611, 5.1100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:787, step:0 
model_pd.l_p.mean(): 0.1171194538474083 
model_pd.l_d.mean(): -19.26640510559082 
model_pd.lagr.mean(): -19.1492862701416 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5619], device='cuda:0')), ('power', tensor([-20.2087], device='cuda:0'))])
epoch£º787	 i:0 	 global-step:15740	 l-p:0.1171194538474083
epoch£º787	 i:1 	 global-step:15741	 l-p:0.11346711218357086
epoch£º787	 i:2 	 global-step:15742	 l-p:0.14135323464870453
epoch£º787	 i:3 	 global-step:15743	 l-p:0.10151681303977966
epoch£º787	 i:4 	 global-step:15744	 l-p:0.10913543403148651
epoch£º787	 i:5 	 global-step:15745	 l-p:0.14283022284507751
epoch£º787	 i:6 	 global-step:15746	 l-p:0.11855946481227875
epoch£º787	 i:7 	 global-step:15747	 l-p:0.15995950996875763
epoch£º787	 i:8 	 global-step:15748	 l-p:0.16474959254264832
epoch£º787	 i:9 	 global-step:15749	 l-p:0.13923054933547974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:788
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1151, 4.8705, 4.7796],
        [5.1151, 5.1151, 5.1151],
        [5.1151, 4.8648, 4.7401],
        [5.1151, 5.1874, 4.9353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:788, step:0 
model_pd.l_p.mean(): 0.13903824985027313 
model_pd.l_d.mean(): -20.549179077148438 
model_pd.lagr.mean(): -20.410140991210938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4225], device='cuda:0')), ('power', tensor([-21.3710], device='cuda:0'))])
epoch£º788	 i:0 	 global-step:15760	 l-p:0.13903824985027313
epoch£º788	 i:1 	 global-step:15761	 l-p:0.15235161781311035
epoch£º788	 i:2 	 global-step:15762	 l-p:0.11955402046442032
epoch£º788	 i:3 	 global-step:15763	 l-p:0.11374931037425995
epoch£º788	 i:4 	 global-step:15764	 l-p:0.19840168952941895
epoch£º788	 i:5 	 global-step:15765	 l-p:0.15293961763381958
epoch£º788	 i:6 	 global-step:15766	 l-p:0.1320871263742447
epoch£º788	 i:7 	 global-step:15767	 l-p:0.12470700591802597
epoch£º788	 i:8 	 global-step:15768	 l-p:0.14868837594985962
epoch£º788	 i:9 	 global-step:15769	 l-p:0.13613435626029968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:789
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1185, 5.0718, 5.1068],
        [5.1185, 4.8815, 4.8178],
        [5.1185, 5.1185, 5.1185],
        [5.1185, 5.1185, 5.1185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:789, step:0 
model_pd.l_p.mean(): 0.11673141270875931 
model_pd.l_d.mean(): -20.258291244506836 
model_pd.lagr.mean(): -20.141559600830078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4618], device='cuda:0')), ('power', tensor([-21.1154], device='cuda:0'))])
epoch£º789	 i:0 	 global-step:15780	 l-p:0.11673141270875931
epoch£º789	 i:1 	 global-step:15781	 l-p:0.14361286163330078
epoch£º789	 i:2 	 global-step:15782	 l-p:0.17958568036556244
epoch£º789	 i:3 	 global-step:15783	 l-p:0.1445368379354477
epoch£º789	 i:4 	 global-step:15784	 l-p:0.17126834392547607
epoch£º789	 i:5 	 global-step:15785	 l-p:0.13266287744045258
epoch£º789	 i:6 	 global-step:15786	 l-p:0.1150941252708435
epoch£º789	 i:7 	 global-step:15787	 l-p:0.11744588613510132
epoch£º789	 i:8 	 global-step:15788	 l-p:0.14766263961791992
epoch£º789	 i:9 	 global-step:15789	 l-p:0.11418186873197556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:790
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1391, 5.0649, 5.1120],
        [5.1391, 5.1391, 5.1391],
        [5.1391, 5.0885, 4.7941],
        [5.1391, 4.9034, 4.8394]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:790, step:0 
model_pd.l_p.mean(): 0.11339978873729706 
model_pd.l_d.mean(): -20.75836181640625 
model_pd.lagr.mean(): -20.644962310791016 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3996], device='cuda:0')), ('power', tensor([-21.5604], device='cuda:0'))])
epoch£º790	 i:0 	 global-step:15800	 l-p:0.11339978873729706
epoch£º790	 i:1 	 global-step:15801	 l-p:0.18617475032806396
epoch£º790	 i:2 	 global-step:15802	 l-p:0.14166377484798431
epoch£º790	 i:3 	 global-step:15803	 l-p:0.12620286643505096
epoch£º790	 i:4 	 global-step:15804	 l-p:0.12222772091627121
epoch£º790	 i:5 	 global-step:15805	 l-p:0.12750203907489777
epoch£º790	 i:6 	 global-step:15806	 l-p:0.16138935089111328
epoch£º790	 i:7 	 global-step:15807	 l-p:0.12519031763076782
epoch£º790	 i:8 	 global-step:15808	 l-p:0.1411675661802292
epoch£º790	 i:9 	 global-step:15809	 l-p:0.08598614484071732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:791
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1373, 4.9112, 4.8725],
        [5.1373, 5.5675, 5.5240],
        [5.1373, 5.0277, 5.0809],
        [5.1373, 4.9410, 4.9501]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:791, step:0 
model_pd.l_p.mean(): 0.1607813984155655 
model_pd.l_d.mean(): -20.489181518554688 
model_pd.lagr.mean(): -20.328399658203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4115], device='cuda:0')), ('power', tensor([-21.2985], device='cuda:0'))])
epoch£º791	 i:0 	 global-step:15820	 l-p:0.1607813984155655
epoch£º791	 i:1 	 global-step:15821	 l-p:0.1522556096315384
epoch£º791	 i:2 	 global-step:15822	 l-p:0.07156748324632645
epoch£º791	 i:3 	 global-step:15823	 l-p:0.12260482460260391
epoch£º791	 i:4 	 global-step:15824	 l-p:0.10273286700248718
epoch£º791	 i:5 	 global-step:15825	 l-p:0.135675311088562
epoch£º791	 i:6 	 global-step:15826	 l-p:0.09887955337762833
epoch£º791	 i:7 	 global-step:15827	 l-p:0.15423132479190826
epoch£º791	 i:8 	 global-step:15828	 l-p:0.12786690890789032
epoch£º791	 i:9 	 global-step:15829	 l-p:0.15461711585521698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:792
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1521, 4.9080, 4.7102],
        [5.1521, 5.0201, 4.7211],
        [5.1521, 5.0825, 5.1280],
        [5.1521, 5.0115, 5.0597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:792, step:0 
model_pd.l_p.mean(): 0.13274990022182465 
model_pd.l_d.mean(): -20.627952575683594 
model_pd.lagr.mean(): -20.495203018188477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4075], device='cuda:0')), ('power', tensor([-21.4358], device='cuda:0'))])
epoch£º792	 i:0 	 global-step:15840	 l-p:0.13274990022182465
epoch£º792	 i:1 	 global-step:15841	 l-p:0.06563454121351242
epoch£º792	 i:2 	 global-step:15842	 l-p:0.1416787952184677
epoch£º792	 i:3 	 global-step:15843	 l-p:0.14193060994148254
epoch£º792	 i:4 	 global-step:15844	 l-p:0.11841540783643723
epoch£º792	 i:5 	 global-step:15845	 l-p:0.12548518180847168
epoch£º792	 i:6 	 global-step:15846	 l-p:0.09037427604198456
epoch£º792	 i:7 	 global-step:15847	 l-p:0.1930239200592041
epoch£º792	 i:8 	 global-step:15848	 l-p:0.1530998945236206
epoch£º792	 i:9 	 global-step:15849	 l-p:0.3734499216079712
====================================================================================================
====================================================================================================
====================================================================================================

epoch:793
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0837, 5.0837, 5.0837],
        [5.0837, 5.0810, 5.0836],
        [5.0837, 5.0837, 5.0837],
        [5.0837, 5.0359, 5.0717]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:793, step:0 
model_pd.l_p.mean(): 0.13142997026443481 
model_pd.l_d.mean(): -20.621482849121094 
model_pd.lagr.mean(): -20.490053176879883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4104], device='cuda:0')), ('power', tensor([-21.4321], device='cuda:0'))])
epoch£º793	 i:0 	 global-step:15860	 l-p:0.13142997026443481
epoch£º793	 i:1 	 global-step:15861	 l-p:0.14754346013069153
epoch£º793	 i:2 	 global-step:15862	 l-p:0.1101962998509407
epoch£º793	 i:3 	 global-step:15863	 l-p:0.11580249667167664
epoch£º793	 i:4 	 global-step:15864	 l-p:0.11705964803695679
epoch£º793	 i:5 	 global-step:15865	 l-p:0.42320701479911804
epoch£º793	 i:6 	 global-step:15866	 l-p:0.13390016555786133
epoch£º793	 i:7 	 global-step:15867	 l-p:0.18876643478870392
epoch£º793	 i:8 	 global-step:15868	 l-p:0.15519028902053833
epoch£º793	 i:9 	 global-step:15869	 l-p:0.2941719591617584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:794
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0665, 4.8073, 4.6570],
        [5.0665, 5.0444, 5.0634],
        [5.0665, 4.8091, 4.6832],
        [5.0665, 5.0665, 5.0665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:794, step:0 
model_pd.l_p.mean(): 0.16587144136428833 
model_pd.l_d.mean(): -20.102062225341797 
model_pd.lagr.mean(): -19.93619155883789 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5079], device='cuda:0')), ('power', tensor([-21.0040], device='cuda:0'))])
epoch£º794	 i:0 	 global-step:15880	 l-p:0.16587144136428833
epoch£º794	 i:1 	 global-step:15881	 l-p:0.11320941895246506
epoch£º794	 i:2 	 global-step:15882	 l-p:0.19249770045280457
epoch£º794	 i:3 	 global-step:15883	 l-p:0.12983886897563934
epoch£º794	 i:4 	 global-step:15884	 l-p:0.15987084805965424
epoch£º794	 i:5 	 global-step:15885	 l-p:0.1787509173154831
epoch£º794	 i:6 	 global-step:15886	 l-p:0.10855359584093094
epoch£º794	 i:7 	 global-step:15887	 l-p:0.18435287475585938
epoch£º794	 i:8 	 global-step:15888	 l-p:0.1168089434504509
epoch£º794	 i:9 	 global-step:15889	 l-p:0.16125187277793884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:795
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1289, 5.1289, 5.1289],
        [5.1289, 4.9206, 4.6494],
        [5.1289, 5.0861, 5.1189],
        [5.1289, 5.0881, 5.1197]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:795, step:0 
model_pd.l_p.mean(): 0.10574620962142944 
model_pd.l_d.mean(): -20.21965980529785 
model_pd.lagr.mean(): -20.113914489746094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4590], device='cuda:0')), ('power', tensor([-21.0732], device='cuda:0'))])
epoch£º795	 i:0 	 global-step:15900	 l-p:0.10574620962142944
epoch£º795	 i:1 	 global-step:15901	 l-p:0.14741457998752594
epoch£º795	 i:2 	 global-step:15902	 l-p:0.1584843248128891
epoch£º795	 i:3 	 global-step:15903	 l-p:0.13399873673915863
epoch£º795	 i:4 	 global-step:15904	 l-p:0.15064723789691925
epoch£º795	 i:5 	 global-step:15905	 l-p:0.09765612334012985
epoch£º795	 i:6 	 global-step:15906	 l-p:0.18848228454589844
epoch£º795	 i:7 	 global-step:15907	 l-p:0.12203238904476166
epoch£º795	 i:8 	 global-step:15908	 l-p:0.11512145400047302
epoch£º795	 i:9 	 global-step:15909	 l-p:0.12417750060558319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:796
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1267, 5.0752, 5.1129],
        [5.1267, 5.0700, 4.7725],
        [5.1267, 4.9936, 5.0448],
        [5.1267, 5.1267, 5.1267]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:796, step:0 
model_pd.l_p.mean(): 0.14529936015605927 
model_pd.l_d.mean(): -19.815486907958984 
model_pd.lagr.mean(): -19.67018699645996 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4911], device='cuda:0')), ('power', tensor([-20.6946], device='cuda:0'))])
epoch£º796	 i:0 	 global-step:15920	 l-p:0.14529936015605927
epoch£º796	 i:1 	 global-step:15921	 l-p:0.12414364516735077
epoch£º796	 i:2 	 global-step:15922	 l-p:0.1013626828789711
epoch£º796	 i:3 	 global-step:15923	 l-p:0.18108253180980682
epoch£º796	 i:4 	 global-step:15924	 l-p:0.1643037497997284
epoch£º796	 i:5 	 global-step:15925	 l-p:0.12154645472764969
epoch£º796	 i:6 	 global-step:15926	 l-p:0.11499293148517609
epoch£º796	 i:7 	 global-step:15927	 l-p:0.1570267230272293
epoch£º796	 i:8 	 global-step:15928	 l-p:0.13771653175354004
epoch£º796	 i:9 	 global-step:15929	 l-p:0.17113612592220306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:797
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1120, 5.0213, 5.0731],
        [5.1120, 5.0954, 5.1101],
        [5.1120, 5.1114, 5.1120],
        [5.1120, 5.1120, 5.1120]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:797, step:0 
model_pd.l_p.mean(): 0.12155473232269287 
model_pd.l_d.mean(): -20.369003295898438 
model_pd.lagr.mean(): -20.247447967529297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4608], device='cuda:0')), ('power', tensor([-21.2272], device='cuda:0'))])
epoch£º797	 i:0 	 global-step:15940	 l-p:0.12155473232269287
epoch£º797	 i:1 	 global-step:15941	 l-p:0.11949016153812408
epoch£º797	 i:2 	 global-step:15942	 l-p:0.14851662516593933
epoch£º797	 i:3 	 global-step:15943	 l-p:0.18920810520648956
epoch£º797	 i:4 	 global-step:15944	 l-p:0.2853657901287079
epoch£º797	 i:5 	 global-step:15945	 l-p:0.12131743878126144
epoch£º797	 i:6 	 global-step:15946	 l-p:0.17793676257133484
epoch£º797	 i:7 	 global-step:15947	 l-p:0.11619874089956284
epoch£º797	 i:8 	 global-step:15948	 l-p:0.17910093069076538
epoch£º797	 i:9 	 global-step:15949	 l-p:0.22509312629699707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:798
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0632, 5.0810, 4.8043],
        [5.0632, 5.0585, 5.0630],
        [5.0632, 4.9264, 4.9784],
        [5.0632, 5.3177, 5.1618]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:798, step:0 
model_pd.l_p.mean(): 0.1539108008146286 
model_pd.l_d.mean(): -19.754539489746094 
model_pd.lagr.mean(): -19.600627899169922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4839], device='cuda:0')), ('power', tensor([-20.6252], device='cuda:0'))])
epoch£º798	 i:0 	 global-step:15960	 l-p:0.1539108008146286
epoch£º798	 i:1 	 global-step:15961	 l-p:0.1147511675953865
epoch£º798	 i:2 	 global-step:15962	 l-p:0.28977614641189575
epoch£º798	 i:3 	 global-step:15963	 l-p:0.1440025120973587
epoch£º798	 i:4 	 global-step:15964	 l-p:-0.16416750848293304
epoch£º798	 i:5 	 global-step:15965	 l-p:0.19062800705432892
epoch£º798	 i:6 	 global-step:15966	 l-p:0.11504320055246353
epoch£º798	 i:7 	 global-step:15967	 l-p:0.2134888470172882
epoch£º798	 i:8 	 global-step:15968	 l-p:0.20787669718265533
epoch£º798	 i:9 	 global-step:15969	 l-p:0.18877533078193665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:799
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0682, 5.0248, 5.0581],
        [5.0682, 5.0669, 5.0682],
        [5.0682, 5.0682, 5.0682],
        [5.0682, 5.0655, 5.0681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:799, step:0 
model_pd.l_p.mean(): 0.11227202415466309 
model_pd.l_d.mean(): -20.847597122192383 
model_pd.lagr.mean(): -20.73532485961914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3983], device='cuda:0')), ('power', tensor([-21.6499], device='cuda:0'))])
epoch£º799	 i:0 	 global-step:15980	 l-p:0.11227202415466309
epoch£º799	 i:1 	 global-step:15981	 l-p:0.47381827235221863
epoch£º799	 i:2 	 global-step:15982	 l-p:0.16057056188583374
epoch£º799	 i:3 	 global-step:15983	 l-p:0.11705324053764343
epoch£º799	 i:4 	 global-step:15984	 l-p:0.12671184539794922
epoch£º799	 i:5 	 global-step:15985	 l-p:0.10553839802742004
epoch£º799	 i:6 	 global-step:15986	 l-p:0.11831444501876831
epoch£º799	 i:7 	 global-step:15987	 l-p:0.11851239204406738
epoch£º799	 i:8 	 global-step:15988	 l-p:0.2720795273780823
epoch£º799	 i:9 	 global-step:15989	 l-p:0.128496915102005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:800
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0933, 4.8694, 4.8463],
        [5.0933, 5.2543, 5.0442],
        [5.0933, 5.0854, 5.0928],
        [5.0933, 4.8348, 4.6805]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:800, step:0 
model_pd.l_p.mean(): 0.09707150608301163 
model_pd.l_d.mean(): -18.3944034576416 
model_pd.lagr.mean(): -18.297332763671875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5926], device='cuda:0')), ('power', tensor([-19.3522], device='cuda:0'))])
epoch£º800	 i:0 	 global-step:16000	 l-p:0.09707150608301163
epoch£º800	 i:1 	 global-step:16001	 l-p:0.10860739648342133
epoch£º800	 i:2 	 global-step:16002	 l-p:0.1424746960401535
epoch£º800	 i:3 	 global-step:16003	 l-p:0.20896121859550476
epoch£º800	 i:4 	 global-step:16004	 l-p:0.14931488037109375
epoch£º800	 i:5 	 global-step:16005	 l-p:0.12145876884460449
epoch£º800	 i:6 	 global-step:16006	 l-p:0.21028780937194824
epoch£º800	 i:7 	 global-step:16007	 l-p:0.13739724457263947
epoch£º800	 i:8 	 global-step:16008	 l-p:0.18276813626289368
epoch£º800	 i:9 	 global-step:16009	 l-p:0.12983953952789307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:801
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7815,  0.7198,  1.0000,  0.6630,
          1.0000,  0.9211, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7501,  0.6816,  1.0000,  0.6193,
          1.0000,  0.9086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228]], device='cuda:0')
 pt:tensor([[5.1073, 5.2722, 5.0637],
        [5.1073, 5.2266, 4.9945],
        [5.1073, 4.8913, 4.6196],
        [5.1073, 4.8808, 4.8515]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:801, step:0 
model_pd.l_p.mean(): 0.12226900458335876 
model_pd.l_d.mean(): -19.57391929626465 
model_pd.lagr.mean(): -19.451650619506836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4960], device='cuda:0')), ('power', tensor([-20.4537], device='cuda:0'))])
epoch£º801	 i:0 	 global-step:16020	 l-p:0.12226900458335876
epoch£º801	 i:1 	 global-step:16021	 l-p:0.12846146523952484
epoch£º801	 i:2 	 global-step:16022	 l-p:0.09908513724803925
epoch£º801	 i:3 	 global-step:16023	 l-p:0.1490584760904312
epoch£º801	 i:4 	 global-step:16024	 l-p:0.15016399323940277
epoch£º801	 i:5 	 global-step:16025	 l-p:0.16306863725185394
epoch£º801	 i:6 	 global-step:16026	 l-p:0.11804864555597305
epoch£º801	 i:7 	 global-step:16027	 l-p:0.1264757663011551
epoch£º801	 i:8 	 global-step:16028	 l-p:0.20208804309368134
epoch£º801	 i:9 	 global-step:16029	 l-p:0.17353121936321259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:802
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1119, 5.1084, 5.1118],
        [5.1119, 4.9906, 5.0444],
        [5.1119, 5.1119, 5.1119],
        [5.1119, 4.8579, 4.7421]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:802, step:0 
model_pd.l_p.mean(): 0.1849367469549179 
model_pd.l_d.mean(): -18.951292037963867 
model_pd.lagr.mean(): -18.766355514526367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5240], device='cuda:0')), ('power', tensor([-19.8484], device='cuda:0'))])
epoch£º802	 i:0 	 global-step:16040	 l-p:0.1849367469549179
epoch£º802	 i:1 	 global-step:16041	 l-p:0.15388144552707672
epoch£º802	 i:2 	 global-step:16042	 l-p:0.18058764934539795
epoch£º802	 i:3 	 global-step:16043	 l-p:0.12488734722137451
epoch£º802	 i:4 	 global-step:16044	 l-p:0.08496297895908356
epoch£º802	 i:5 	 global-step:16045	 l-p:0.13179735839366913
epoch£º802	 i:6 	 global-step:16046	 l-p:0.1445576250553131
epoch£º802	 i:7 	 global-step:16047	 l-p:0.12047684192657471
epoch£º802	 i:8 	 global-step:16048	 l-p:0.12851151823997498
epoch£º802	 i:9 	 global-step:16049	 l-p:0.13497911393642426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:803
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4241,  0.3187,  1.0000,  0.2394,
          1.0000,  0.7513, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3293,  0.2274,  1.0000,  0.1570,
          1.0000,  0.6906, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2741,  0.1781,  1.0000,  0.1157,
          1.0000,  0.6496, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[5.1314, 4.9023, 4.6506],
        [5.1314, 4.8763, 4.7230],
        [5.1314, 4.8865, 4.8058],
        [5.1314, 4.8765, 4.7309]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:803, step:0 
model_pd.l_p.mean(): 0.14026358723640442 
model_pd.l_d.mean(): -20.56034278869629 
model_pd.lagr.mean(): -20.42007827758789 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4351], device='cuda:0')), ('power', tensor([-21.3954], device='cuda:0'))])
epoch£º803	 i:0 	 global-step:16060	 l-p:0.14026358723640442
epoch£º803	 i:1 	 global-step:16061	 l-p:0.12181378155946732
epoch£º803	 i:2 	 global-step:16062	 l-p:0.09929833561182022
epoch£º803	 i:3 	 global-step:16063	 l-p:0.1435394138097763
epoch£º803	 i:4 	 global-step:16064	 l-p:0.16779190301895142
epoch£º803	 i:5 	 global-step:16065	 l-p:0.16237391531467438
epoch£º803	 i:6 	 global-step:16066	 l-p:0.10536137223243713
epoch£º803	 i:7 	 global-step:16067	 l-p:0.15367630124092102
epoch£º803	 i:8 	 global-step:16068	 l-p:0.16398493945598602
epoch£º803	 i:9 	 global-step:16069	 l-p:0.11241239309310913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:804
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1370, 5.1343, 5.1369],
        [5.1370, 5.1017, 5.1299],
        [5.1370, 4.8887, 4.7926],
        [5.1370, 5.0032, 4.6995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:804, step:0 
model_pd.l_p.mean(): 0.13359422981739044 
model_pd.l_d.mean(): -19.676536560058594 
model_pd.lagr.mean(): -19.54294204711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5385], device='cuda:0')), ('power', tensor([-20.6022], device='cuda:0'))])
epoch£º804	 i:0 	 global-step:16080	 l-p:0.13359422981739044
epoch£º804	 i:1 	 global-step:16081	 l-p:0.1585758477449417
epoch£º804	 i:2 	 global-step:16082	 l-p:0.10190971195697784
epoch£º804	 i:3 	 global-step:16083	 l-p:0.13585403561592102
epoch£º804	 i:4 	 global-step:16084	 l-p:0.1204417422413826
epoch£º804	 i:5 	 global-step:16085	 l-p:0.11419375985860825
epoch£º804	 i:6 	 global-step:16086	 l-p:0.15761610865592957
epoch£º804	 i:7 	 global-step:16087	 l-p:0.1286807358264923
epoch£º804	 i:8 	 global-step:16088	 l-p:0.08991194516420364
epoch£º804	 i:9 	 global-step:16089	 l-p:0.19416862726211548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:805
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1285, 5.1285, 5.1285],
        [5.1285, 5.1285, 5.1285],
        [5.1285, 5.4220, 5.2876],
        [5.1285, 5.0110, 5.0648]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:805, step:0 
model_pd.l_p.mean(): 0.12520572543144226 
model_pd.l_d.mean(): -19.31279945373535 
model_pd.lagr.mean(): -19.187593460083008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5293], device='cuda:0')), ('power', tensor([-20.2222], device='cuda:0'))])
epoch£º805	 i:0 	 global-step:16100	 l-p:0.12520572543144226
epoch£º805	 i:1 	 global-step:16101	 l-p:0.12780004739761353
epoch£º805	 i:2 	 global-step:16102	 l-p:0.17238502204418182
epoch£º805	 i:3 	 global-step:16103	 l-p:0.10444726794958115
epoch£º805	 i:4 	 global-step:16104	 l-p:0.15183059871196747
epoch£º805	 i:5 	 global-step:16105	 l-p:0.13901473581790924
epoch£º805	 i:6 	 global-step:16106	 l-p:0.1706485152244568
epoch£º805	 i:7 	 global-step:16107	 l-p:0.13203361630439758
epoch£º805	 i:8 	 global-step:16108	 l-p:0.12371470779180527
epoch£º805	 i:9 	 global-step:16109	 l-p:0.13317613303661346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:806
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1289, 5.1289, 5.1289],
        [5.1289, 5.1283, 5.1289],
        [5.1289, 5.1289, 5.1289],
        [5.1289, 5.1264, 5.1288]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:806, step:0 
model_pd.l_p.mean(): 0.07526151835918427 
model_pd.l_d.mean(): -20.204748153686523 
model_pd.lagr.mean(): -20.129486083984375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4704], device='cuda:0')), ('power', tensor([-21.0698], device='cuda:0'))])
epoch£º806	 i:0 	 global-step:16120	 l-p:0.07526151835918427
epoch£º806	 i:1 	 global-step:16121	 l-p:0.1759972721338272
epoch£º806	 i:2 	 global-step:16122	 l-p:0.10449215024709702
epoch£º806	 i:3 	 global-step:16123	 l-p:0.16313347220420837
epoch£º806	 i:4 	 global-step:16124	 l-p:0.1421213299036026
epoch£º806	 i:5 	 global-step:16125	 l-p:0.13106510043144226
epoch£º806	 i:6 	 global-step:16126	 l-p:0.1263035237789154
epoch£º806	 i:7 	 global-step:16127	 l-p:0.12498390674591064
epoch£º806	 i:8 	 global-step:16128	 l-p:0.18026202917099
epoch£º806	 i:9 	 global-step:16129	 l-p:0.12907741963863373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:807
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1432, 5.1425, 5.1432],
        [5.1432, 5.1427, 5.1432],
        [5.1432, 5.1419, 5.1431],
        [5.1432, 5.1432, 5.1432]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:807, step:0 
model_pd.l_p.mean(): 0.0933079794049263 
model_pd.l_d.mean(): -20.74839210510254 
model_pd.lagr.mean(): -20.65508460998535 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3949], device='cuda:0')), ('power', tensor([-21.5454], device='cuda:0'))])
epoch£º807	 i:0 	 global-step:16140	 l-p:0.0933079794049263
epoch£º807	 i:1 	 global-step:16141	 l-p:0.1525641232728958
epoch£º807	 i:2 	 global-step:16142	 l-p:0.13903039693832397
epoch£º807	 i:3 	 global-step:16143	 l-p:0.14858397841453552
epoch£º807	 i:4 	 global-step:16144	 l-p:0.1262723058462143
epoch£º807	 i:5 	 global-step:16145	 l-p:0.07303325831890106
epoch£º807	 i:6 	 global-step:16146	 l-p:0.11849377304315567
epoch£º807	 i:7 	 global-step:16147	 l-p:0.14734280109405518
epoch£º807	 i:8 	 global-step:16148	 l-p:0.12072061002254486
epoch£º807	 i:9 	 global-step:16149	 l-p:0.1821824163198471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:808
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1499, 5.1445, 5.1496],
        [5.1499, 5.1367, 5.1486],
        [5.1499, 5.1499, 5.1499],
        [5.1499, 5.1457, 5.1497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:808, step:0 
model_pd.l_p.mean(): 0.15188537538051605 
model_pd.l_d.mean(): -19.888927459716797 
model_pd.lagr.mean(): -19.737041473388672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4498], device='cuda:0')), ('power', tensor([-20.7267], device='cuda:0'))])
epoch£º808	 i:0 	 global-step:16160	 l-p:0.15188537538051605
epoch£º808	 i:1 	 global-step:16161	 l-p:0.12402733415365219
epoch£º808	 i:2 	 global-step:16162	 l-p:0.1497040092945099
epoch£º808	 i:3 	 global-step:16163	 l-p:0.1432781219482422
epoch£º808	 i:4 	 global-step:16164	 l-p:0.05434982106089592
epoch£º808	 i:5 	 global-step:16165	 l-p:0.16227641701698303
epoch£º808	 i:6 	 global-step:16166	 l-p:0.14376449584960938
epoch£º808	 i:7 	 global-step:16167	 l-p:0.12231402844190598
epoch£º808	 i:8 	 global-step:16168	 l-p:0.1373579204082489
epoch£º808	 i:9 	 global-step:16169	 l-p:0.10830968618392944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:809
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1520, 5.1520, 5.1520],
        [5.1520, 5.1515, 5.1520],
        [5.1520, 5.0913, 5.1334],
        [5.1520, 5.1519, 5.1520]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:809, step:0 
model_pd.l_p.mean(): 0.11583784967660904 
model_pd.l_d.mean(): -19.139368057250977 
model_pd.lagr.mean(): -19.023530960083008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5420], device='cuda:0')), ('power', tensor([-20.0586], device='cuda:0'))])
epoch£º809	 i:0 	 global-step:16180	 l-p:0.11583784967660904
epoch£º809	 i:1 	 global-step:16181	 l-p:0.10642427206039429
epoch£º809	 i:2 	 global-step:16182	 l-p:0.12707896530628204
epoch£º809	 i:3 	 global-step:16183	 l-p:0.10397448390722275
epoch£º809	 i:4 	 global-step:16184	 l-p:0.12221986800432205
epoch£º809	 i:5 	 global-step:16185	 l-p:0.12820100784301758
epoch£º809	 i:6 	 global-step:16186	 l-p:0.1393362134695053
epoch£º809	 i:7 	 global-step:16187	 l-p:0.160258486866951
epoch£º809	 i:8 	 global-step:16188	 l-p:0.12410092353820801
epoch£º809	 i:9 	 global-step:16189	 l-p:0.13291257619857788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:810
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1695, 4.9792, 4.9967],
        [5.1695, 4.9235, 4.7187],
        [5.1695, 5.6629, 5.6600],
        [5.1695, 5.1675, 5.1695]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:810, step:0 
model_pd.l_p.mean(): 0.1745675653219223 
model_pd.l_d.mean(): -19.715696334838867 
model_pd.lagr.mean(): -19.541128158569336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4553], device='cuda:0')), ('power', tensor([-20.5559], device='cuda:0'))])
epoch£º810	 i:0 	 global-step:16200	 l-p:0.1745675653219223
epoch£º810	 i:1 	 global-step:16201	 l-p:0.15846200287342072
epoch£º810	 i:2 	 global-step:16202	 l-p:0.08955059200525284
epoch£º810	 i:3 	 global-step:16203	 l-p:0.1041761115193367
epoch£º810	 i:4 	 global-step:16204	 l-p:0.12332990020513535
epoch£º810	 i:5 	 global-step:16205	 l-p:0.0745752677321434
epoch£º810	 i:6 	 global-step:16206	 l-p:0.13549622893333435
epoch£º810	 i:7 	 global-step:16207	 l-p:0.11452509462833405
epoch£º810	 i:8 	 global-step:16208	 l-p:0.12299348413944244
epoch£º810	 i:9 	 global-step:16209	 l-p:0.13578097522258759
====================================================================================================
====================================================================================================
====================================================================================================

epoch:811
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1658, 4.9128, 4.7668],
        [5.1658, 5.0147, 4.7147],
        [5.1658, 5.0758, 5.1273],
        [5.1658, 5.0253, 5.0746]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:811, step:0 
model_pd.l_p.mean(): 0.05492760241031647 
model_pd.l_d.mean(): -19.39948844909668 
model_pd.lagr.mean(): -19.344560623168945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5380], device='cuda:0')), ('power', tensor([-20.3194], device='cuda:0'))])
epoch£º811	 i:0 	 global-step:16220	 l-p:0.05492760241031647
epoch£º811	 i:1 	 global-step:16221	 l-p:0.1601996123790741
epoch£º811	 i:2 	 global-step:16222	 l-p:0.10607186704874039
epoch£º811	 i:3 	 global-step:16223	 l-p:0.12333995848894119
epoch£º811	 i:4 	 global-step:16224	 l-p:0.16083915531635284
epoch£º811	 i:5 	 global-step:16225	 l-p:0.15802663564682007
epoch£º811	 i:6 	 global-step:16226	 l-p:0.12676024436950684
epoch£º811	 i:7 	 global-step:16227	 l-p:0.108871228992939
epoch£º811	 i:8 	 global-step:16228	 l-p:0.0914437472820282
epoch£º811	 i:9 	 global-step:16229	 l-p:0.13352404534816742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:812
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1754, 5.1691, 5.1750],
        [5.1754, 5.1534, 5.1723],
        [5.1754, 5.3638, 5.1659],
        [5.1754, 5.1496, 5.1713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:812, step:0 
model_pd.l_p.mean(): 0.02807578444480896 
model_pd.l_d.mean(): -20.53824234008789 
model_pd.lagr.mean(): -20.51016616821289 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4307], device='cuda:0')), ('power', tensor([-21.3683], device='cuda:0'))])
epoch£º812	 i:0 	 global-step:16240	 l-p:0.02807578444480896
epoch£º812	 i:1 	 global-step:16241	 l-p:0.11715512722730637
epoch£º812	 i:2 	 global-step:16242	 l-p:0.10465562343597412
epoch£º812	 i:3 	 global-step:16243	 l-p:0.13673965632915497
epoch£º812	 i:4 	 global-step:16244	 l-p:0.15434114634990692
epoch£º812	 i:5 	 global-step:16245	 l-p:0.14678272604942322
epoch£º812	 i:6 	 global-step:16246	 l-p:0.11691506952047348
epoch£º812	 i:7 	 global-step:16247	 l-p:0.12880276143550873
epoch£º812	 i:8 	 global-step:16248	 l-p:0.13348683714866638
epoch£º812	 i:9 	 global-step:16249	 l-p:0.12359318137168884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:813
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1665, 5.0592, 5.1128],
        [5.1665, 4.9314, 4.8767],
        [5.1665, 4.9419, 4.9113],
        [5.1665, 5.1665, 5.1665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:813, step:0 
model_pd.l_p.mean(): 0.14073306322097778 
model_pd.l_d.mean(): -19.654844284057617 
model_pd.lagr.mean(): -19.514110565185547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4681], device='cuda:0')), ('power', tensor([-20.5072], device='cuda:0'))])
epoch£º813	 i:0 	 global-step:16260	 l-p:0.14073306322097778
epoch£º813	 i:1 	 global-step:16261	 l-p:0.097848080098629
epoch£º813	 i:2 	 global-step:16262	 l-p:0.12571057677268982
epoch£º813	 i:3 	 global-step:16263	 l-p:0.12328250706195831
epoch£º813	 i:4 	 global-step:16264	 l-p:0.1871533989906311
epoch£º813	 i:5 	 global-step:16265	 l-p:0.10077568143606186
epoch£º813	 i:6 	 global-step:16266	 l-p:0.10646619647741318
epoch£º813	 i:7 	 global-step:16267	 l-p:0.14041756093502045
epoch£º813	 i:8 	 global-step:16268	 l-p:0.17113013565540314
epoch£º813	 i:9 	 global-step:16269	 l-p:0.11435040086507797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:814
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1237, 5.1237, 5.1237],
        [5.1237, 5.1237, 5.1237],
        [5.1237, 5.0789, 5.1130],
        [5.1237, 5.1237, 5.1237]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:814, step:0 
model_pd.l_p.mean(): 0.13491442799568176 
model_pd.l_d.mean(): -20.55422592163086 
model_pd.lagr.mean(): -20.4193115234375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4077], device='cuda:0')), ('power', tensor([-21.3609], device='cuda:0'))])
epoch£º814	 i:0 	 global-step:16280	 l-p:0.13491442799568176
epoch£º814	 i:1 	 global-step:16281	 l-p:0.2006504386663437
epoch£º814	 i:2 	 global-step:16282	 l-p:0.12208659946918488
epoch£º814	 i:3 	 global-step:16283	 l-p:0.10808078199625015
epoch£º814	 i:4 	 global-step:16284	 l-p:0.14736762642860413
epoch£º814	 i:5 	 global-step:16285	 l-p:0.18573027849197388
epoch£º814	 i:6 	 global-step:16286	 l-p:0.13799546658992767
epoch£º814	 i:7 	 global-step:16287	 l-p:0.09630325436592102
epoch£º814	 i:8 	 global-step:16288	 l-p:0.1310267448425293
epoch£º814	 i:9 	 global-step:16289	 l-p:0.1713251769542694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:815
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1009, 5.1009, 5.1009],
        [5.1009, 5.2661, 5.0568],
        [5.1009, 5.0153, 5.0664],
        [5.1009, 4.9958, 5.0502]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:815, step:0 
model_pd.l_p.mean(): 0.11055555939674377 
model_pd.l_d.mean(): -18.897682189941406 
model_pd.lagr.mean(): -18.787126541137695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5734], device='cuda:0')), ('power', tensor([-19.8450], device='cuda:0'))])
epoch£º815	 i:0 	 global-step:16300	 l-p:0.11055555939674377
epoch£º815	 i:1 	 global-step:16301	 l-p:0.09434860944747925
epoch£º815	 i:2 	 global-step:16302	 l-p:0.13995343446731567
epoch£º815	 i:3 	 global-step:16303	 l-p:0.21637122333049774
epoch£º815	 i:4 	 global-step:16304	 l-p:0.12133409082889557
epoch£º815	 i:5 	 global-step:16305	 l-p:0.1288948506116867
epoch£º815	 i:6 	 global-step:16306	 l-p:0.18477225303649902
epoch£º815	 i:7 	 global-step:16307	 l-p:0.13653600215911865
epoch£º815	 i:8 	 global-step:16308	 l-p:0.16909301280975342
epoch£º815	 i:9 	 global-step:16309	 l-p:0.22920051217079163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:816
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0950, 5.0118, 5.0623],
        [5.0950, 4.9489, 4.9987],
        [5.0950, 5.0867, 5.0944],
        [5.0950, 5.0499, 5.0842]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:816, step:0 
model_pd.l_p.mean(): 0.15383374691009521 
model_pd.l_d.mean(): -20.70911979675293 
model_pd.lagr.mean(): -20.555286407470703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3995], device='cuda:0')), ('power', tensor([-21.5101], device='cuda:0'))])
epoch£º816	 i:0 	 global-step:16320	 l-p:0.15383374691009521
epoch£º816	 i:1 	 global-step:16321	 l-p:0.13205374777317047
epoch£º816	 i:2 	 global-step:16322	 l-p:0.13101257383823395
epoch£º816	 i:3 	 global-step:16323	 l-p:0.07438433170318604
epoch£º816	 i:4 	 global-step:16324	 l-p:0.2835802733898163
epoch£º816	 i:5 	 global-step:16325	 l-p:0.12158851325511932
epoch£º816	 i:6 	 global-step:16326	 l-p:0.21214734017848969
epoch£º816	 i:7 	 global-step:16327	 l-p:0.2661527991294861
epoch£º816	 i:8 	 global-step:16328	 l-p:0.1308950036764145
epoch£º816	 i:9 	 global-step:16329	 l-p:0.13029035925865173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:817
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0824, 5.2543, 5.0488],
        [5.0824, 5.0822, 5.0824],
        [5.0824, 5.0813, 5.0823],
        [5.0824, 4.8230, 4.6214]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:817, step:0 
model_pd.l_p.mean(): 0.09618186950683594 
model_pd.l_d.mean(): -18.48574447631836 
model_pd.lagr.mean(): -18.389562606811523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6037], device='cuda:0')), ('power', tensor([-19.4567], device='cuda:0'))])
epoch£º817	 i:0 	 global-step:16340	 l-p:0.09618186950683594
epoch£º817	 i:1 	 global-step:16341	 l-p:0.1562798023223877
epoch£º817	 i:2 	 global-step:16342	 l-p:0.1147717833518982
epoch£º817	 i:3 	 global-step:16343	 l-p:0.173455610871315
epoch£º817	 i:4 	 global-step:16344	 l-p:0.1173204779624939
epoch£º817	 i:5 	 global-step:16345	 l-p:0.23557929694652557
epoch£º817	 i:6 	 global-step:16346	 l-p:0.1321539729833603
epoch£º817	 i:7 	 global-step:16347	 l-p:0.2502715289592743
epoch£º817	 i:8 	 global-step:16348	 l-p:0.17086580395698547
epoch£º817	 i:9 	 global-step:16349	 l-p:0.11908561736345291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:818
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1046, 5.3662, 5.2117],
        [5.1046, 5.2575, 5.0412],
        [5.1046, 5.1046, 5.1046],
        [5.1046, 5.0314, 5.0788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:818, step:0 
model_pd.l_p.mean(): 0.10639936476945877 
model_pd.l_d.mean(): -19.108964920043945 
model_pd.lagr.mean(): -19.002565383911133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5821], device='cuda:0')), ('power', tensor([-20.0692], device='cuda:0'))])
epoch£º818	 i:0 	 global-step:16360	 l-p:0.10639936476945877
epoch£º818	 i:1 	 global-step:16361	 l-p:0.14853286743164062
epoch£º818	 i:2 	 global-step:16362	 l-p:0.24218621850013733
epoch£º818	 i:3 	 global-step:16363	 l-p:0.13339562714099884
epoch£º818	 i:4 	 global-step:16364	 l-p:0.1961708962917328
epoch£º818	 i:5 	 global-step:16365	 l-p:0.10112106800079346
epoch£º818	 i:6 	 global-step:16366	 l-p:0.08410073071718216
epoch£º818	 i:7 	 global-step:16367	 l-p:0.12787622213363647
epoch£º818	 i:8 	 global-step:16368	 l-p:0.1255403310060501
epoch£º818	 i:9 	 global-step:16369	 l-p:0.16175003349781036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:819
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1320, 5.0136, 5.0679],
        [5.1320, 4.8863, 4.6583],
        [5.1320, 5.1272, 5.1318],
        [5.1320, 5.1838, 4.9186]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:819, step:0 
model_pd.l_p.mean(): 0.123973049223423 
model_pd.l_d.mean(): -20.704917907714844 
model_pd.lagr.mean(): -20.580944061279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4209], device='cuda:0')), ('power', tensor([-21.5280], device='cuda:0'))])
epoch£º819	 i:0 	 global-step:16380	 l-p:0.123973049223423
epoch£º819	 i:1 	 global-step:16381	 l-p:0.20474524796009064
epoch£º819	 i:2 	 global-step:16382	 l-p:0.10708672553300858
epoch£º819	 i:3 	 global-step:16383	 l-p:0.11589469760656357
epoch£º819	 i:4 	 global-step:16384	 l-p:0.1899764984846115
epoch£º819	 i:5 	 global-step:16385	 l-p:0.1250656247138977
epoch£º819	 i:6 	 global-step:16386	 l-p:0.10914374887943268
epoch£º819	 i:7 	 global-step:16387	 l-p:0.17221283912658691
epoch£º819	 i:8 	 global-step:16388	 l-p:0.16624581813812256
epoch£º819	 i:9 	 global-step:16389	 l-p:0.08178441971540451
====================================================================================================
====================================================================================================
====================================================================================================

epoch:820
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1142, 4.8812, 4.6237],
        [5.1142, 5.1129, 5.1142],
        [5.1142, 4.8555, 4.6713],
        [5.1142, 5.1142, 5.1142]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:820, step:0 
model_pd.l_p.mean(): 0.19506682455539703 
model_pd.l_d.mean(): -19.399354934692383 
model_pd.lagr.mean(): -19.204288482666016 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4988], device='cuda:0')), ('power', tensor([-20.2787], device='cuda:0'))])
epoch£º820	 i:0 	 global-step:16400	 l-p:0.19506682455539703
epoch£º820	 i:1 	 global-step:16401	 l-p:0.11246878653764725
epoch£º820	 i:2 	 global-step:16402	 l-p:0.1530093401670456
epoch£º820	 i:3 	 global-step:16403	 l-p:0.12882675230503082
epoch£º820	 i:4 	 global-step:16404	 l-p:0.13340234756469727
epoch£º820	 i:5 	 global-step:16405	 l-p:0.09986715018749237
epoch£º820	 i:6 	 global-step:16406	 l-p:0.17684774100780487
epoch£º820	 i:7 	 global-step:16407	 l-p:0.13076576590538025
epoch£º820	 i:8 	 global-step:16408	 l-p:0.16836017370224
epoch£º820	 i:9 	 global-step:16409	 l-p:0.1003839299082756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:821
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1374, 4.9830, 5.0286],
        [5.1374, 5.0523, 5.1031],
        [5.1374, 4.9713, 5.0108],
        [5.1374, 4.8811, 4.7517]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:821, step:0 
model_pd.l_p.mean(): 0.11229194700717926 
model_pd.l_d.mean(): -20.80121421813965 
model_pd.lagr.mean(): -20.688922882080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3904], device='cuda:0')), ('power', tensor([-21.5945], device='cuda:0'))])
epoch£º821	 i:0 	 global-step:16420	 l-p:0.11229194700717926
epoch£º821	 i:1 	 global-step:16421	 l-p:0.16063249111175537
epoch£º821	 i:2 	 global-step:16422	 l-p:0.11096839606761932
epoch£º821	 i:3 	 global-step:16423	 l-p:0.14295732975006104
epoch£º821	 i:4 	 global-step:16424	 l-p:0.159318745136261
epoch£º821	 i:5 	 global-step:16425	 l-p:0.09622714668512344
epoch£º821	 i:6 	 global-step:16426	 l-p:0.17577151954174042
epoch£º821	 i:7 	 global-step:16427	 l-p:0.12287086248397827
epoch£º821	 i:8 	 global-step:16428	 l-p:0.12057997286319733
epoch£º821	 i:9 	 global-step:16429	 l-p:0.09528365731239319
====================================================================================================
====================================================================================================
====================================================================================================

epoch:822
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1690, 5.1615, 5.1685],
        [5.1690, 5.1057, 4.8046],
        [5.1690, 5.0539, 4.7485],
        [5.1690, 5.1687, 5.1690]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:822, step:0 
model_pd.l_p.mean(): 0.13205330073833466 
model_pd.l_d.mean(): -19.898433685302734 
model_pd.lagr.mean(): -19.766380310058594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4643], device='cuda:0')), ('power', tensor([-20.7514], device='cuda:0'))])
epoch£º822	 i:0 	 global-step:16440	 l-p:0.13205330073833466
epoch£º822	 i:1 	 global-step:16441	 l-p:0.0766325518488884
epoch£º822	 i:2 	 global-step:16442	 l-p:0.14687353372573853
epoch£º822	 i:3 	 global-step:16443	 l-p:0.08243940025568008
epoch£º822	 i:4 	 global-step:16444	 l-p:0.11879277974367142
epoch£º822	 i:5 	 global-step:16445	 l-p:0.1274319738149643
epoch£º822	 i:6 	 global-step:16446	 l-p:0.11351443082094193
epoch£º822	 i:7 	 global-step:16447	 l-p:0.09770025312900543
epoch£º822	 i:8 	 global-step:16448	 l-p:0.13438569009304047
epoch£º822	 i:9 	 global-step:16449	 l-p:0.18375900387763977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:823
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1752, 5.0891, 5.1399],
        [5.1752, 5.1752, 5.1752],
        [5.1752, 4.9501, 4.9196],
        [5.1752, 5.4897, 5.3656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:823, step:0 
model_pd.l_p.mean(): 0.20190872251987457 
model_pd.l_d.mean(): -19.986543655395508 
model_pd.lagr.mean(): -19.784635543823242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5170], device='cuda:0')), ('power', tensor([-20.8958], device='cuda:0'))])
epoch£º823	 i:0 	 global-step:16460	 l-p:0.20190872251987457
epoch£º823	 i:1 	 global-step:16461	 l-p:0.14413613080978394
epoch£º823	 i:2 	 global-step:16462	 l-p:0.1174817755818367
epoch£º823	 i:3 	 global-step:16463	 l-p:0.16106192767620087
epoch£º823	 i:4 	 global-step:16464	 l-p:0.12832164764404297
epoch£º823	 i:5 	 global-step:16465	 l-p:0.09542663395404816
epoch£º823	 i:6 	 global-step:16466	 l-p:0.04014204069972038
epoch£º823	 i:7 	 global-step:16467	 l-p:0.13803236186504364
epoch£º823	 i:8 	 global-step:16468	 l-p:0.08547115325927734
epoch£º823	 i:9 	 global-step:16469	 l-p:0.11672311276197433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:824
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1692, 4.9142, 4.7622],
        [5.1692, 5.1433, 5.1651],
        [5.1692, 5.0641, 5.1179],
        [5.1692, 5.1692, 5.1692]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:824, step:0 
model_pd.l_p.mean(): 0.10641486942768097 
model_pd.l_d.mean(): -20.158456802368164 
model_pd.lagr.mean(): -20.05204200744629 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4730], device='cuda:0')), ('power', tensor([-21.0253], device='cuda:0'))])
epoch£º824	 i:0 	 global-step:16480	 l-p:0.10641486942768097
epoch£º824	 i:1 	 global-step:16481	 l-p:0.10908210277557373
epoch£º824	 i:2 	 global-step:16482	 l-p:0.15745781362056732
epoch£º824	 i:3 	 global-step:16483	 l-p:0.1221548467874527
epoch£º824	 i:4 	 global-step:16484	 l-p:0.13913318514823914
epoch£º824	 i:5 	 global-step:16485	 l-p:0.04293922334909439
epoch£º824	 i:6 	 global-step:16486	 l-p:0.13313531875610352
epoch£º824	 i:7 	 global-step:16487	 l-p:0.14379487931728363
epoch£º824	 i:8 	 global-step:16488	 l-p:0.13809265196323395
epoch£º824	 i:9 	 global-step:16489	 l-p:0.14597749710083008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:825
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1574, 5.5385, 5.4577],
        [5.1574, 5.1570, 5.1574],
        [5.1574, 5.1574, 5.1574],
        [5.1574, 5.1163, 5.1482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:825, step:0 
model_pd.l_p.mean(): 0.08199311792850494 
model_pd.l_d.mean(): -19.65547752380371 
model_pd.lagr.mean(): -19.573484420776367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5050], device='cuda:0')), ('power', tensor([-20.5460], device='cuda:0'))])
epoch£º825	 i:0 	 global-step:16500	 l-p:0.08199311792850494
epoch£º825	 i:1 	 global-step:16501	 l-p:0.11777808517217636
epoch£º825	 i:2 	 global-step:16502	 l-p:0.1330350786447525
epoch£º825	 i:3 	 global-step:16503	 l-p:0.1259165108203888
epoch£º825	 i:4 	 global-step:16504	 l-p:0.15821179747581482
epoch£º825	 i:5 	 global-step:16505	 l-p:0.12285147607326508
epoch£º825	 i:6 	 global-step:16506	 l-p:0.1247723177075386
epoch£º825	 i:7 	 global-step:16507	 l-p:0.1509372442960739
epoch£º825	 i:8 	 global-step:16508	 l-p:0.15385323762893677
epoch£º825	 i:9 	 global-step:16509	 l-p:0.13894441723823547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:826
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1425, 5.1388, 5.1423],
        [5.1425, 5.1325, 5.1417],
        [5.1425, 5.1398, 5.1424],
        [5.1425, 4.8921, 4.8001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:826, step:0 
model_pd.l_p.mean(): 0.12592802941799164 
model_pd.l_d.mean(): -20.519107818603516 
model_pd.lagr.mean(): -20.393178939819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4205], device='cuda:0')), ('power', tensor([-21.3383], device='cuda:0'))])
epoch£º826	 i:0 	 global-step:16520	 l-p:0.12592802941799164
epoch£º826	 i:1 	 global-step:16521	 l-p:0.17069782316684723
epoch£º826	 i:2 	 global-step:16522	 l-p:0.13699664175510406
epoch£º826	 i:3 	 global-step:16523	 l-p:0.07981651276350021
epoch£º826	 i:4 	 global-step:16524	 l-p:0.10015641897916794
epoch£º826	 i:5 	 global-step:16525	 l-p:0.12484715133905411
epoch£º826	 i:6 	 global-step:16526	 l-p:0.1375121921300888
epoch£º826	 i:7 	 global-step:16527	 l-p:0.14589284360408783
epoch£º826	 i:8 	 global-step:16528	 l-p:0.14197596907615662
epoch£º826	 i:9 	 global-step:16529	 l-p:0.14137978851795197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:827
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1572, 5.1564, 5.1572],
        [5.1572, 5.1159, 5.1479],
        [5.1572, 5.1572, 5.1572],
        [5.1572, 5.0048, 5.0510]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:827, step:0 
model_pd.l_p.mean(): 0.12468193471431732 
model_pd.l_d.mean(): -20.20981216430664 
model_pd.lagr.mean(): -20.08513069152832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4467], device='cuda:0')), ('power', tensor([-21.0504], device='cuda:0'))])
epoch£º827	 i:0 	 global-step:16540	 l-p:0.12468193471431732
epoch£º827	 i:1 	 global-step:16541	 l-p:0.13672815263271332
epoch£º827	 i:2 	 global-step:16542	 l-p:0.11356563121080399
epoch£º827	 i:3 	 global-step:16543	 l-p:0.09199842065572739
epoch£º827	 i:4 	 global-step:16544	 l-p:0.16391576826572418
epoch£º827	 i:5 	 global-step:16545	 l-p:0.1798662543296814
epoch£º827	 i:6 	 global-step:16546	 l-p:0.08074361830949783
epoch£º827	 i:7 	 global-step:16547	 l-p:0.13524878025054932
epoch£º827	 i:8 	 global-step:16548	 l-p:0.12806075811386108
epoch£º827	 i:9 	 global-step:16549	 l-p:0.11543761938810349
====================================================================================================
====================================================================================================
====================================================================================================

epoch:828
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1697, 5.0557, 5.1098],
        [5.1697, 4.9744, 4.9890],
        [5.1697, 5.1241, 5.1586],
        [5.1697, 5.1303, 5.1611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:828, step:0 
model_pd.l_p.mean(): 0.1181710809469223 
model_pd.l_d.mean(): -20.298351287841797 
model_pd.lagr.mean(): -20.180179595947266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4346], device='cuda:0')), ('power', tensor([-21.1280], device='cuda:0'))])
epoch£º828	 i:0 	 global-step:16560	 l-p:0.1181710809469223
epoch£º828	 i:1 	 global-step:16561	 l-p:0.0640060156583786
epoch£º828	 i:2 	 global-step:16562	 l-p:0.0658465102314949
epoch£º828	 i:3 	 global-step:16563	 l-p:0.12717483937740326
epoch£º828	 i:4 	 global-step:16564	 l-p:0.15064378082752228
epoch£º828	 i:5 	 global-step:16565	 l-p:0.17003920674324036
epoch£º828	 i:6 	 global-step:16566	 l-p:0.14379863440990448
epoch£º828	 i:7 	 global-step:16567	 l-p:0.13759668171405792
epoch£º828	 i:8 	 global-step:16568	 l-p:0.12409614026546478
epoch£º828	 i:9 	 global-step:16569	 l-p:0.12052714824676514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:829
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1777, 5.1017, 5.1498],
        [5.1777, 5.0893, 4.7846],
        [5.1777, 5.1775, 5.1777],
        [5.1777, 5.1771, 5.1777]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:829, step:0 
model_pd.l_p.mean(): 0.07411651313304901 
model_pd.l_d.mean(): -19.447734832763672 
model_pd.lagr.mean(): -19.373619079589844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4879], device='cuda:0')), ('power', tensor([-20.3167], device='cuda:0'))])
epoch£º829	 i:0 	 global-step:16580	 l-p:0.07411651313304901
epoch£º829	 i:1 	 global-step:16581	 l-p:0.10768965631723404
epoch£º829	 i:2 	 global-step:16582	 l-p:0.1329936683177948
epoch£º829	 i:3 	 global-step:16583	 l-p:0.05720231309533119
epoch£º829	 i:4 	 global-step:16584	 l-p:0.14759501814842224
epoch£º829	 i:5 	 global-step:16585	 l-p:0.10889789462089539
epoch£º829	 i:6 	 global-step:16586	 l-p:0.1944221705198288
epoch£º829	 i:7 	 global-step:16587	 l-p:0.16057559847831726
epoch£º829	 i:8 	 global-step:16588	 l-p:0.13000236451625824
epoch£º829	 i:9 	 global-step:16589	 l-p:0.08987066149711609
====================================================================================================
====================================================================================================
====================================================================================================

epoch:830
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1712, 5.1706, 5.1712],
        [5.1712, 4.9468, 4.9197],
        [5.1712, 4.9161, 4.7791],
        [5.1712, 5.1712, 5.1712]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:830, step:0 
model_pd.l_p.mean(): 0.1435825675725937 
model_pd.l_d.mean(): -20.45797348022461 
model_pd.lagr.mean(): -20.314390182495117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4282], device='cuda:0')), ('power', tensor([-21.2840], device='cuda:0'))])
epoch£º830	 i:0 	 global-step:16600	 l-p:0.1435825675725937
epoch£º830	 i:1 	 global-step:16601	 l-p:0.10603000223636627
epoch£º830	 i:2 	 global-step:16602	 l-p:0.13578777015209198
epoch£º830	 i:3 	 global-step:16603	 l-p:0.08364914357662201
epoch£º830	 i:4 	 global-step:16604	 l-p:0.12883669137954712
epoch£º830	 i:5 	 global-step:16605	 l-p:0.14738327264785767
epoch£º830	 i:6 	 global-step:16606	 l-p:0.16709576547145844
epoch£º830	 i:7 	 global-step:16607	 l-p:0.0788789764046669
epoch£º830	 i:8 	 global-step:16608	 l-p:0.20408064126968384
epoch£º830	 i:9 	 global-step:16609	 l-p:0.1168171688914299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:831
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1339, 5.1338, 5.1339],
        [5.1339, 5.1311, 5.1338],
        [5.1339, 5.1191, 5.1323],
        [5.1339, 5.1339, 5.1339]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:831, step:0 
model_pd.l_p.mean(): 0.1345980316400528 
model_pd.l_d.mean(): -19.88587760925293 
model_pd.lagr.mean(): -19.751279830932617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4697], device='cuda:0')), ('power', tensor([-20.7442], device='cuda:0'))])
epoch£º831	 i:0 	 global-step:16620	 l-p:0.1345980316400528
epoch£º831	 i:1 	 global-step:16621	 l-p:0.09821971505880356
epoch£º831	 i:2 	 global-step:16622	 l-p:0.13742713630199432
epoch£º831	 i:3 	 global-step:16623	 l-p:0.16615578532218933
epoch£º831	 i:4 	 global-step:16624	 l-p:0.08790230751037598
epoch£º831	 i:5 	 global-step:16625	 l-p:0.15290257334709167
epoch£º831	 i:6 	 global-step:16626	 l-p:0.1621105670928955
epoch£º831	 i:7 	 global-step:16627	 l-p:0.15030428767204285
epoch£º831	 i:8 	 global-step:16628	 l-p:0.14344605803489685
epoch£º831	 i:9 	 global-step:16629	 l-p:0.1544915735721588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:832
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1218, 5.1176, 5.1216],
        [5.1218, 4.8922, 4.8629],
        [5.1218, 4.9597, 4.6539],
        [5.1218, 4.8921, 4.8626]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:832, step:0 
model_pd.l_p.mean(): 0.1011311486363411 
model_pd.l_d.mean(): -20.099302291870117 
model_pd.lagr.mean(): -19.998170852661133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4715], device='cuda:0')), ('power', tensor([-20.9635], device='cuda:0'))])
epoch£º832	 i:0 	 global-step:16640	 l-p:0.1011311486363411
epoch£º832	 i:1 	 global-step:16641	 l-p:0.14695145189762115
epoch£º832	 i:2 	 global-step:16642	 l-p:0.09363947063684464
epoch£º832	 i:3 	 global-step:16643	 l-p:0.08472108840942383
epoch£º832	 i:4 	 global-step:16644	 l-p:0.13090142607688904
epoch£º832	 i:5 	 global-step:16645	 l-p:0.17111679911613464
epoch£º832	 i:6 	 global-step:16646	 l-p:0.2725699245929718
epoch£º832	 i:7 	 global-step:16647	 l-p:0.1593688428401947
epoch£º832	 i:8 	 global-step:16648	 l-p:0.14616039395332336
epoch£º832	 i:9 	 global-step:16649	 l-p:0.18091163039207458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:833
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1078, 5.1078, 5.1078],
        [5.1078, 5.1007, 5.1074],
        [5.1078, 4.9948, 5.0499],
        [5.1078, 5.1040, 5.1077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:833, step:0 
model_pd.l_p.mean(): 0.1340014934539795 
model_pd.l_d.mean(): -19.640703201293945 
model_pd.lagr.mean(): -19.506702423095703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5322], device='cuda:0')), ('power', tensor([-20.5591], device='cuda:0'))])
epoch£º833	 i:0 	 global-step:16660	 l-p:0.1340014934539795
epoch£º833	 i:1 	 global-step:16661	 l-p:0.14806604385375977
epoch£º833	 i:2 	 global-step:16662	 l-p:0.13515795767307281
epoch£º833	 i:3 	 global-step:16663	 l-p:0.13816781342029572
epoch£º833	 i:4 	 global-step:16664	 l-p:0.10161296278238297
epoch£º833	 i:5 	 global-step:16665	 l-p:0.1185278370976448
epoch£º833	 i:6 	 global-step:16666	 l-p:0.11108226329088211
epoch£º833	 i:7 	 global-step:16667	 l-p:0.078262560069561
epoch£º833	 i:8 	 global-step:16668	 l-p:0.2274867594242096
epoch£º833	 i:9 	 global-step:16669	 l-p:0.3390825092792511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:834
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0982, 5.0982, 5.0982],
        [5.0982, 5.4922, 5.4210],
        [5.0982, 5.0982, 5.0982],
        [5.0982, 4.9415, 4.9882]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:834, step:0 
model_pd.l_p.mean(): 0.1573270857334137 
model_pd.l_d.mean(): -19.586339950561523 
model_pd.lagr.mean(): -19.429012298583984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5803], device='cuda:0')), ('power', tensor([-20.5536], device='cuda:0'))])
epoch£º834	 i:0 	 global-step:16680	 l-p:0.1573270857334137
epoch£º834	 i:1 	 global-step:16681	 l-p:0.12464883178472519
epoch£º834	 i:2 	 global-step:16682	 l-p:0.1188255101442337
epoch£º834	 i:3 	 global-step:16683	 l-p:0.12869220972061157
epoch£º834	 i:4 	 global-step:16684	 l-p:0.30202537775039673
epoch£º834	 i:5 	 global-step:16685	 l-p:0.10473182797431946
epoch£º834	 i:6 	 global-step:16686	 l-p:0.2506363093852997
epoch£º834	 i:7 	 global-step:16687	 l-p:0.1106056347489357
epoch£º834	 i:8 	 global-step:16688	 l-p:0.09885475039482117
epoch£º834	 i:9 	 global-step:16689	 l-p:0.1443193107843399
====================================================================================================
====================================================================================================
====================================================================================================

epoch:835
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1009, 5.1008, 5.1009],
        [5.1009, 4.9777, 5.0327],
        [5.1009, 4.8391, 4.6457],
        [5.1009, 4.9017, 4.6069]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:835, step:0 
model_pd.l_p.mean(): 0.07991216331720352 
model_pd.l_d.mean(): -19.71820068359375 
model_pd.lagr.mean(): -19.638288497924805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4993], device='cuda:0')), ('power', tensor([-20.6040], device='cuda:0'))])
epoch£º835	 i:0 	 global-step:16700	 l-p:0.07991216331720352
epoch£º835	 i:1 	 global-step:16701	 l-p:0.14929930865764618
epoch£º835	 i:2 	 global-step:16702	 l-p:0.11932040750980377
epoch£º835	 i:3 	 global-step:16703	 l-p:0.21896745264530182
epoch£º835	 i:4 	 global-step:16704	 l-p:0.23206114768981934
epoch£º835	 i:5 	 global-step:16705	 l-p:0.13294485211372375
epoch£º835	 i:6 	 global-step:16706	 l-p:0.10038121789693832
epoch£º835	 i:7 	 global-step:16707	 l-p:0.1790342479944229
epoch£º835	 i:8 	 global-step:16708	 l-p:0.1390383243560791
epoch£º835	 i:9 	 global-step:16709	 l-p:0.13224133849143982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:836
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1141, 5.1010, 5.1128],
        [5.1141, 5.1197, 4.8348],
        [5.1141, 5.1131, 5.1141],
        [5.1141, 5.0073, 5.0621]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:836, step:0 
model_pd.l_p.mean(): 0.1486939936876297 
model_pd.l_d.mean(): -20.41234588623047 
model_pd.lagr.mean(): -20.263652801513672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4648], device='cuda:0')), ('power', tensor([-21.2754], device='cuda:0'))])
epoch£º836	 i:0 	 global-step:16720	 l-p:0.1486939936876297
epoch£º836	 i:1 	 global-step:16721	 l-p:0.13678327202796936
epoch£º836	 i:2 	 global-step:16722	 l-p:0.11870047450065613
epoch£º836	 i:3 	 global-step:16723	 l-p:0.10316009074449539
epoch£º836	 i:4 	 global-step:16724	 l-p:0.16961105167865753
epoch£º836	 i:5 	 global-step:16725	 l-p:0.1324090212583542
epoch£º836	 i:6 	 global-step:16726	 l-p:0.12647126615047455
epoch£º836	 i:7 	 global-step:16727	 l-p:0.14344803988933563
epoch£º836	 i:8 	 global-step:16728	 l-p:0.13392655551433563
epoch£º836	 i:9 	 global-step:16729	 l-p:0.18591657280921936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:837
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1292, 5.0576, 5.1045],
        [5.1292, 5.1290, 5.1292],
        [5.1292, 5.0268, 4.7168],
        [5.1292, 4.9084, 4.8945]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:837, step:0 
model_pd.l_p.mean(): 0.18706603348255157 
model_pd.l_d.mean(): -20.73151206970215 
model_pd.lagr.mean(): -20.54444694519043 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4216], device='cuda:0')), ('power', tensor([-21.5558], device='cuda:0'))])
epoch£º837	 i:0 	 global-step:16740	 l-p:0.18706603348255157
epoch£º837	 i:1 	 global-step:16741	 l-p:0.13204126060009003
epoch£º837	 i:2 	 global-step:16742	 l-p:0.12433870881795883
epoch£º837	 i:3 	 global-step:16743	 l-p:0.12674836814403534
epoch£º837	 i:4 	 global-step:16744	 l-p:0.11924516409635544
epoch£º837	 i:5 	 global-step:16745	 l-p:0.13181541860103607
epoch£º837	 i:6 	 global-step:16746	 l-p:0.18463118374347687
epoch£º837	 i:7 	 global-step:16747	 l-p:0.08358854800462723
epoch£º837	 i:8 	 global-step:16748	 l-p:0.15120764076709747
epoch£º837	 i:9 	 global-step:16749	 l-p:0.1256023347377777
====================================================================================================
====================================================================================================
====================================================================================================

epoch:838
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1369, 5.1369, 5.1369],
        [5.1369, 5.5035, 5.4129],
        [5.1369, 4.9835, 5.0306],
        [5.1369, 5.0142, 5.0686]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:838, step:0 
model_pd.l_p.mean(): 0.10609038919210434 
model_pd.l_d.mean(): -19.613445281982422 
model_pd.lagr.mean(): -19.507354736328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5851], device='cuda:0')), ('power', tensor([-20.5862], device='cuda:0'))])
epoch£º838	 i:0 	 global-step:16760	 l-p:0.10609038919210434
epoch£º838	 i:1 	 global-step:16761	 l-p:0.152674600481987
epoch£º838	 i:2 	 global-step:16762	 l-p:0.12286487221717834
epoch£º838	 i:3 	 global-step:16763	 l-p:0.12668469548225403
epoch£º838	 i:4 	 global-step:16764	 l-p:0.11509732156991959
epoch£º838	 i:5 	 global-step:16765	 l-p:0.15049660205841064
epoch£º838	 i:6 	 global-step:16766	 l-p:0.11966031789779663
epoch£º838	 i:7 	 global-step:16767	 l-p:0.18355907499790192
epoch£º838	 i:8 	 global-step:16768	 l-p:0.1303004026412964
epoch£º838	 i:9 	 global-step:16769	 l-p:0.11801142245531082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:839
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1562, 5.1560, 5.1562],
        [5.1562, 5.1560, 5.1562],
        [5.1562, 5.1562, 5.1562],
        [5.1562, 5.1535, 5.1561]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:839, step:0 
model_pd.l_p.mean(): 0.12208518385887146 
model_pd.l_d.mean(): -20.165945053100586 
model_pd.lagr.mean(): -20.043859481811523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4796], device='cuda:0')), ('power', tensor([-21.0397], device='cuda:0'))])
epoch£º839	 i:0 	 global-step:16780	 l-p:0.12208518385887146
epoch£º839	 i:1 	 global-step:16781	 l-p:0.1707320213317871
epoch£º839	 i:2 	 global-step:16782	 l-p:0.13334211707115173
epoch£º839	 i:3 	 global-step:16783	 l-p:0.18304279446601868
epoch£º839	 i:4 	 global-step:16784	 l-p:0.1679164320230484
epoch£º839	 i:5 	 global-step:16785	 l-p:0.10018592327833176
epoch£º839	 i:6 	 global-step:16786	 l-p:0.07747133076190948
epoch£º839	 i:7 	 global-step:16787	 l-p:0.11743166297674179
epoch£º839	 i:8 	 global-step:16788	 l-p:0.1195436641573906
epoch£º839	 i:9 	 global-step:16789	 l-p:0.11651498824357986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:840
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1504, 5.1504, 5.1504],
        [5.1504, 5.0200, 4.7108],
        [5.1504, 5.0248, 5.0788],
        [5.1504, 5.1504, 5.1504]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:840, step:0 
model_pd.l_p.mean(): 0.10856878757476807 
model_pd.l_d.mean(): -19.14838981628418 
model_pd.lagr.mean(): -19.03982162475586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5482], device='cuda:0')), ('power', tensor([-20.0742], device='cuda:0'))])
epoch£º840	 i:0 	 global-step:16800	 l-p:0.10856878757476807
epoch£º840	 i:1 	 global-step:16801	 l-p:0.1014568880200386
epoch£º840	 i:2 	 global-step:16802	 l-p:0.09038052707910538
epoch£º840	 i:3 	 global-step:16803	 l-p:0.11955031752586365
epoch£º840	 i:4 	 global-step:16804	 l-p:0.17723454535007477
epoch£º840	 i:5 	 global-step:16805	 l-p:0.1615394502878189
epoch£º840	 i:6 	 global-step:16806	 l-p:0.12796376645565033
epoch£º840	 i:7 	 global-step:16807	 l-p:0.1581428498029709
epoch£º840	 i:8 	 global-step:16808	 l-p:0.09879706799983978
epoch£º840	 i:9 	 global-step:16809	 l-p:0.13995292782783508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:841
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1581, 4.9137, 4.6799],
        [5.1581, 5.1523, 5.1578],
        [5.1581, 5.1560, 5.1581],
        [5.1581, 5.0322, 4.7233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:841, step:0 
model_pd.l_p.mean(): 0.07914020121097565 
model_pd.l_d.mean(): -20.349706649780273 
model_pd.lagr.mean(): -20.270566940307617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4635], device='cuda:0')), ('power', tensor([-21.2102], device='cuda:0'))])
epoch£º841	 i:0 	 global-step:16820	 l-p:0.07914020121097565
epoch£º841	 i:1 	 global-step:16821	 l-p:0.09766889363527298
epoch£º841	 i:2 	 global-step:16822	 l-p:0.12296122312545776
epoch£º841	 i:3 	 global-step:16823	 l-p:0.18624097108840942
epoch£º841	 i:4 	 global-step:16824	 l-p:0.1267174929380417
epoch£º841	 i:5 	 global-step:16825	 l-p:0.11559291929006577
epoch£º841	 i:6 	 global-step:16826	 l-p:0.15646964311599731
epoch£º841	 i:7 	 global-step:16827	 l-p:0.10670215636491776
epoch£º841	 i:8 	 global-step:16828	 l-p:0.13016566634178162
epoch£º841	 i:9 	 global-step:16829	 l-p:0.16332362592220306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:842
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1492, 5.0646, 5.1155],
        [5.1492, 4.9428, 4.6581],
        [5.1492, 5.3226, 5.1148],
        [5.1492, 5.1492, 5.1492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:842, step:0 
model_pd.l_p.mean(): 0.13130943477153778 
model_pd.l_d.mean(): -20.73753547668457 
model_pd.lagr.mean(): -20.606225967407227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3993], device='cuda:0')), ('power', tensor([-21.5389], device='cuda:0'))])
epoch£º842	 i:0 	 global-step:16840	 l-p:0.13130943477153778
epoch£º842	 i:1 	 global-step:16841	 l-p:0.06909772753715515
epoch£º842	 i:2 	 global-step:16842	 l-p:0.1734015792608261
epoch£º842	 i:3 	 global-step:16843	 l-p:0.12315677106380463
epoch£º842	 i:4 	 global-step:16844	 l-p:0.1363748162984848
epoch£º842	 i:5 	 global-step:16845	 l-p:0.11209902167320251
epoch£º842	 i:6 	 global-step:16846	 l-p:0.12798377871513367
epoch£º842	 i:7 	 global-step:16847	 l-p:0.10745716094970703
epoch£º842	 i:8 	 global-step:16848	 l-p:0.22542110085487366
epoch£º842	 i:9 	 global-step:16849	 l-p:0.11913368105888367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:843
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1435, 5.0019, 4.6928],
        [5.1435, 5.1435, 5.1435],
        [5.1435, 5.1281, 5.1418],
        [5.1435, 5.3087, 5.0965]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:843, step:0 
model_pd.l_p.mean(): 0.10744480043649673 
model_pd.l_d.mean(): -19.8092041015625 
model_pd.lagr.mean(): -19.701759338378906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4878], device='cuda:0')), ('power', tensor([-20.6848], device='cuda:0'))])
epoch£º843	 i:0 	 global-step:16860	 l-p:0.10744480043649673
epoch£º843	 i:1 	 global-step:16861	 l-p:0.11761214584112167
epoch£º843	 i:2 	 global-step:16862	 l-p:0.10146060585975647
epoch£º843	 i:3 	 global-step:16863	 l-p:0.1428195685148239
epoch£º843	 i:4 	 global-step:16864	 l-p:0.19611161947250366
epoch£º843	 i:5 	 global-step:16865	 l-p:0.08526039868593216
epoch£º843	 i:6 	 global-step:16866	 l-p:0.14882826805114746
epoch£º843	 i:7 	 global-step:16867	 l-p:0.10972095280885696
epoch£º843	 i:8 	 global-step:16868	 l-p:0.14298155903816223
epoch£º843	 i:9 	 global-step:16869	 l-p:0.17518183588981628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:844
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1503, 4.9907, 4.6850],
        [5.1503, 5.1503, 5.1503],
        [5.1503, 4.9121, 4.8631],
        [5.1503, 5.0128, 5.0649]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:844, step:0 
model_pd.l_p.mean(): 0.12122832983732224 
model_pd.l_d.mean(): -20.506698608398438 
model_pd.lagr.mean(): -20.385469436645508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4302], device='cuda:0')), ('power', tensor([-21.3357], device='cuda:0'))])
epoch£º844	 i:0 	 global-step:16880	 l-p:0.12122832983732224
epoch£º844	 i:1 	 global-step:16881	 l-p:0.12903130054473877
epoch£º844	 i:2 	 global-step:16882	 l-p:0.20286838710308075
epoch£º844	 i:3 	 global-step:16883	 l-p:0.05108881741762161
epoch£º844	 i:4 	 global-step:16884	 l-p:0.14235210418701172
epoch£º844	 i:5 	 global-step:16885	 l-p:0.13832007348537445
epoch£º844	 i:6 	 global-step:16886	 l-p:0.1396196186542511
epoch£º844	 i:7 	 global-step:16887	 l-p:0.15091697871685028
epoch£º844	 i:8 	 global-step:16888	 l-p:0.11997433751821518
epoch£º844	 i:9 	 global-step:16889	 l-p:0.13006141781806946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:845
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1445, 5.1445, 5.1445],
        [5.1445, 5.1267, 5.1423],
        [5.1445, 4.9642, 4.9960],
        [5.1445, 4.8888, 4.7840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:845, step:0 
model_pd.l_p.mean(): 0.13554514944553375 
model_pd.l_d.mean(): -20.280221939086914 
model_pd.lagr.mean(): -20.144676208496094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4634], device='cuda:0')), ('power', tensor([-21.1394], device='cuda:0'))])
epoch£º845	 i:0 	 global-step:16900	 l-p:0.13554514944553375
epoch£º845	 i:1 	 global-step:16901	 l-p:0.1248195692896843
epoch£º845	 i:2 	 global-step:16902	 l-p:0.12773561477661133
epoch£º845	 i:3 	 global-step:16903	 l-p:0.17381317913532257
epoch£º845	 i:4 	 global-step:16904	 l-p:0.13230644166469574
epoch£º845	 i:5 	 global-step:16905	 l-p:0.12073434889316559
epoch£º845	 i:6 	 global-step:16906	 l-p:0.10955066233873367
epoch£º845	 i:7 	 global-step:16907	 l-p:0.13414186239242554
epoch£º845	 i:8 	 global-step:16908	 l-p:0.12362991273403168
epoch£º845	 i:9 	 global-step:16909	 l-p:0.15106788277626038
====================================================================================================
====================================================================================================
====================================================================================================

epoch:846
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1417, 5.0345, 5.0893],
        [5.1417, 5.1018, 5.1331],
        [5.1417, 5.0666, 5.1148],
        [5.1417, 5.1417, 5.1417]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:846, step:0 
model_pd.l_p.mean(): 0.13436533510684967 
model_pd.l_d.mean(): -19.37799835205078 
model_pd.lagr.mean(): -19.243633270263672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5169], device='cuda:0')), ('power', tensor([-20.2757], device='cuda:0'))])
epoch£º846	 i:0 	 global-step:16920	 l-p:0.13436533510684967
epoch£º846	 i:1 	 global-step:16921	 l-p:0.1689271628856659
epoch£º846	 i:2 	 global-step:16922	 l-p:0.19554972648620605
epoch£º846	 i:3 	 global-step:16923	 l-p:0.09138190746307373
epoch£º846	 i:4 	 global-step:16924	 l-p:0.12500731647014618
epoch£º846	 i:5 	 global-step:16925	 l-p:0.0677793100476265
epoch£º846	 i:6 	 global-step:16926	 l-p:0.11787694692611694
epoch£º846	 i:7 	 global-step:16927	 l-p:0.1389518827199936
epoch£º846	 i:8 	 global-step:16928	 l-p:0.14362481236457825
epoch£º846	 i:9 	 global-step:16929	 l-p:0.14308308064937592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:847
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1564, 5.1076, 5.1440],
        [5.1564, 5.1564, 5.1564],
        [5.1564, 5.1042, 5.1424],
        [5.1564, 5.0472, 5.1019]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:847, step:0 
model_pd.l_p.mean(): 0.12268348783254623 
model_pd.l_d.mean(): -20.57763671875 
model_pd.lagr.mean(): -20.454954147338867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4098], device='cuda:0')), ('power', tensor([-21.3869], device='cuda:0'))])
epoch£º847	 i:0 	 global-step:16940	 l-p:0.12268348783254623
epoch£º847	 i:1 	 global-step:16941	 l-p:0.152004674077034
epoch£º847	 i:2 	 global-step:16942	 l-p:0.14469124376773834
epoch£º847	 i:3 	 global-step:16943	 l-p:0.08955428749322891
epoch£º847	 i:4 	 global-step:16944	 l-p:0.14796477556228638
epoch£º847	 i:5 	 global-step:16945	 l-p:0.1722002774477005
epoch£º847	 i:6 	 global-step:16946	 l-p:0.0617852583527565
epoch£º847	 i:7 	 global-step:16947	 l-p:0.14913158118724823
epoch£º847	 i:8 	 global-step:16948	 l-p:0.1648891270160675
epoch£º847	 i:9 	 global-step:16949	 l-p:0.11814731359481812
====================================================================================================
====================================================================================================
====================================================================================================

epoch:848
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1515, 5.0779, 5.1255],
        [5.1515, 4.9455, 4.9523],
        [5.1515, 5.1510, 5.1514],
        [5.1515, 4.9321, 4.6566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:848, step:0 
model_pd.l_p.mean(): 0.17112651467323303 
model_pd.l_d.mean(): -20.639846801757812 
model_pd.lagr.mean(): -20.468719482421875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4056], device='cuda:0')), ('power', tensor([-21.4458], device='cuda:0'))])
epoch£º848	 i:0 	 global-step:16960	 l-p:0.17112651467323303
epoch£º848	 i:1 	 global-step:16961	 l-p:0.05914713814854622
epoch£º848	 i:2 	 global-step:16962	 l-p:0.13653619587421417
epoch£º848	 i:3 	 global-step:16963	 l-p:0.15060819685459137
epoch£º848	 i:4 	 global-step:16964	 l-p:0.15848933160305023
epoch£º848	 i:5 	 global-step:16965	 l-p:0.13731110095977783
epoch£º848	 i:6 	 global-step:16966	 l-p:0.1309843510389328
epoch£º848	 i:7 	 global-step:16967	 l-p:0.15431369841098785
epoch£º848	 i:8 	 global-step:16968	 l-p:0.07893097400665283
epoch£º848	 i:9 	 global-step:16969	 l-p:0.09437371790409088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:849
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1733, 4.9358, 4.8860],
        [5.1733, 5.1402, 4.8427],
        [5.1733, 4.9481, 4.9242],
        [5.1733, 5.1249, 5.1611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:849, step:0 
model_pd.l_p.mean(): 0.1237405389547348 
model_pd.l_d.mean(): -18.806516647338867 
model_pd.lagr.mean(): -18.682775497436523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5495], device='cuda:0')), ('power', tensor([-19.7274], device='cuda:0'))])
epoch£º849	 i:0 	 global-step:16980	 l-p:0.1237405389547348
epoch£º849	 i:1 	 global-step:16981	 l-p:0.13640451431274414
epoch£º849	 i:2 	 global-step:16982	 l-p:0.12903107702732086
epoch£º849	 i:3 	 global-step:16983	 l-p:0.11667640507221222
epoch£º849	 i:4 	 global-step:16984	 l-p:0.08195801824331284
epoch£º849	 i:5 	 global-step:16985	 l-p:0.1621171087026596
epoch£º849	 i:6 	 global-step:16986	 l-p:0.13801349699497223
epoch£º849	 i:7 	 global-step:16987	 l-p:0.07202491909265518
epoch£º849	 i:8 	 global-step:16988	 l-p:0.12448207288980484
epoch£º849	 i:9 	 global-step:16989	 l-p:0.15904411673545837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:850
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8903,  0.8564,  1.0000,  0.8239,
          1.0000,  0.9620, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228]], device='cuda:0')
 pt:tensor([[5.1684, 5.5062, 5.3949],
        [5.1684, 4.9102, 4.7246],
        [5.1684, 5.0127, 5.0587],
        [5.1684, 5.5229, 5.4224]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:850, step:0 
model_pd.l_p.mean(): 0.11150140315294266 
model_pd.l_d.mean(): -20.162179946899414 
model_pd.lagr.mean(): -20.050678253173828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4733], device='cuda:0')), ('power', tensor([-21.0294], device='cuda:0'))])
epoch£º850	 i:0 	 global-step:17000	 l-p:0.11150140315294266
epoch£º850	 i:1 	 global-step:17001	 l-p:0.14114584028720856
epoch£º850	 i:2 	 global-step:17002	 l-p:0.11514157801866531
epoch£º850	 i:3 	 global-step:17003	 l-p:0.13949398696422577
epoch£º850	 i:4 	 global-step:17004	 l-p:0.09966527670621872
epoch£º850	 i:5 	 global-step:17005	 l-p:0.1755659431219101
epoch£º850	 i:6 	 global-step:17006	 l-p:0.11351476609706879
epoch£º850	 i:7 	 global-step:17007	 l-p:0.2012435495853424
epoch£º850	 i:8 	 global-step:17008	 l-p:0.10322973877191544
epoch£º850	 i:9 	 global-step:17009	 l-p:0.12023677676916122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:851
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1454, 5.1454, 5.1454],
        [5.1454, 5.1452, 5.1454],
        [5.1454, 5.0948, 5.1322],
        [5.1454, 4.9524, 4.6559]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:851, step:0 
model_pd.l_p.mean(): 0.1604456603527069 
model_pd.l_d.mean(): -19.521270751953125 
model_pd.lagr.mean(): -19.360824584960938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5579], device='cuda:0')), ('power', tensor([-20.4641], device='cuda:0'))])
epoch£º851	 i:0 	 global-step:17020	 l-p:0.1604456603527069
epoch£º851	 i:1 	 global-step:17021	 l-p:0.10946055501699448
epoch£º851	 i:2 	 global-step:17022	 l-p:0.18820121884346008
epoch£º851	 i:3 	 global-step:17023	 l-p:0.11551383882761002
epoch£º851	 i:4 	 global-step:17024	 l-p:0.11701416969299316
epoch£º851	 i:5 	 global-step:17025	 l-p:0.09976369887590408
epoch£º851	 i:6 	 global-step:17026	 l-p:0.18529388308525085
epoch£º851	 i:7 	 global-step:17027	 l-p:0.08170383423566818
epoch£º851	 i:8 	 global-step:17028	 l-p:0.13303148746490479
epoch£º851	 i:9 	 global-step:17029	 l-p:0.11865919083356857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:852
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1522, 5.6216, 5.5987],
        [5.1522, 4.8910, 4.7527],
        [5.1522, 4.8981, 4.8042],
        [5.1522, 5.1521, 5.1522]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:852, step:0 
model_pd.l_p.mean(): 0.09964337944984436 
model_pd.l_d.mean(): -19.569087982177734 
model_pd.lagr.mean(): -19.469444274902344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5104], device='cuda:0')), ('power', tensor([-20.4636], device='cuda:0'))])
epoch£º852	 i:0 	 global-step:17040	 l-p:0.09964337944984436
epoch£º852	 i:1 	 global-step:17041	 l-p:0.09612516313791275
epoch£º852	 i:2 	 global-step:17042	 l-p:0.14929968118667603
epoch£º852	 i:3 	 global-step:17043	 l-p:0.14557312428951263
epoch£º852	 i:4 	 global-step:17044	 l-p:0.11292937397956848
epoch£º852	 i:5 	 global-step:17045	 l-p:0.123055100440979
epoch£º852	 i:6 	 global-step:17046	 l-p:0.162710040807724
epoch£º852	 i:7 	 global-step:17047	 l-p:0.13872306048870087
epoch£º852	 i:8 	 global-step:17048	 l-p:0.15380272269248962
epoch£º852	 i:9 	 global-step:17049	 l-p:0.13811467587947845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:853
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1346, 5.1513, 4.8688],
        [5.1346, 5.1231, 5.1336],
        [5.1346, 4.9706, 5.0140],
        [5.1346, 4.9947, 5.0471]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:853, step:0 
model_pd.l_p.mean(): 0.1751789003610611 
model_pd.l_d.mean(): -19.39939308166504 
model_pd.lagr.mean(): -19.224214553833008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5194], device='cuda:0')), ('power', tensor([-20.3001], device='cuda:0'))])
epoch£º853	 i:0 	 global-step:17060	 l-p:0.1751789003610611
epoch£º853	 i:1 	 global-step:17061	 l-p:0.12435968220233917
epoch£º853	 i:2 	 global-step:17062	 l-p:0.12502196431159973
epoch£º853	 i:3 	 global-step:17063	 l-p:0.15085464715957642
epoch£º853	 i:4 	 global-step:17064	 l-p:0.167221337556839
epoch£º853	 i:5 	 global-step:17065	 l-p:0.13835544884204865
epoch£º853	 i:6 	 global-step:17066	 l-p:0.17534604668617249
epoch£º853	 i:7 	 global-step:17067	 l-p:0.08351977169513702
epoch£º853	 i:8 	 global-step:17068	 l-p:0.15765270590782166
epoch£º853	 i:9 	 global-step:17069	 l-p:0.11208028346300125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:854
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1260, 4.9141, 4.6253],
        [5.1260, 5.1259, 5.1260],
        [5.1260, 5.4452, 5.3227],
        [5.1260, 5.1260, 5.1260]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:854, step:0 
model_pd.l_p.mean(): 0.13701055943965912 
model_pd.l_d.mean(): -19.93399429321289 
model_pd.lagr.mean(): -19.79698371887207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5323], device='cuda:0')), ('power', tensor([-20.8581], device='cuda:0'))])
epoch£º854	 i:0 	 global-step:17080	 l-p:0.13701055943965912
epoch£º854	 i:1 	 global-step:17081	 l-p:0.24347907304763794
epoch£º854	 i:2 	 global-step:17082	 l-p:0.11064866930246353
epoch£º854	 i:3 	 global-step:17083	 l-p:0.17044982314109802
epoch£º854	 i:4 	 global-step:17084	 l-p:0.12620298564434052
epoch£º854	 i:5 	 global-step:17085	 l-p:0.09590375423431396
epoch£º854	 i:6 	 global-step:17086	 l-p:0.1179962009191513
epoch£º854	 i:7 	 global-step:17087	 l-p:0.14169347286224365
epoch£º854	 i:8 	 global-step:17088	 l-p:0.1269814521074295
epoch£º854	 i:9 	 global-step:17089	 l-p:0.13433054089546204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:855
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1248, 5.0161, 4.7023],
        [5.1248, 5.2390, 4.9993],
        [5.1248, 5.1191, 5.1245],
        [5.1248, 4.8827, 4.8341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:855, step:0 
model_pd.l_p.mean(): 0.15838851034641266 
model_pd.l_d.mean(): -20.482685089111328 
model_pd.lagr.mean(): -20.324296951293945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4486], device='cuda:0')), ('power', tensor([-21.3304], device='cuda:0'))])
epoch£º855	 i:0 	 global-step:17100	 l-p:0.15838851034641266
epoch£º855	 i:1 	 global-step:17101	 l-p:0.1325855255126953
epoch£º855	 i:2 	 global-step:17102	 l-p:0.12014001607894897
epoch£º855	 i:3 	 global-step:17103	 l-p:0.11723733693361282
epoch£º855	 i:4 	 global-step:17104	 l-p:0.1666736602783203
epoch£º855	 i:5 	 global-step:17105	 l-p:0.16098730266094208
epoch£º855	 i:6 	 global-step:17106	 l-p:0.1423194855451584
epoch£º855	 i:7 	 global-step:17107	 l-p:0.2690141499042511
epoch£º855	 i:8 	 global-step:17108	 l-p:0.15696746110916138
epoch£º855	 i:9 	 global-step:17109	 l-p:0.12585550546646118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:856
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0972, 4.8911, 4.9037],
        [5.0972, 5.0907, 5.0968],
        [5.0972, 4.8853, 4.5912],
        [5.0972, 5.0972, 5.0972]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:856, step:0 
model_pd.l_p.mean(): 0.18761561810970306 
model_pd.l_d.mean(): -19.67259407043457 
model_pd.lagr.mean(): -19.48497772216797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5273], device='cuda:0')), ('power', tensor([-20.5866], device='cuda:0'))])
epoch£º856	 i:0 	 global-step:17120	 l-p:0.18761561810970306
epoch£º856	 i:1 	 global-step:17121	 l-p:0.13413479924201965
epoch£º856	 i:2 	 global-step:17122	 l-p:0.16821888089179993
epoch£º856	 i:3 	 global-step:17123	 l-p:0.11578570306301117
epoch£º856	 i:4 	 global-step:17124	 l-p:0.28509119153022766
epoch£º856	 i:5 	 global-step:17125	 l-p:0.20515377819538116
epoch£º856	 i:6 	 global-step:17126	 l-p:0.15216663479804993
epoch£º856	 i:7 	 global-step:17127	 l-p:0.12021814286708832
epoch£º856	 i:8 	 global-step:17128	 l-p:0.11348019540309906
epoch£º856	 i:9 	 global-step:17129	 l-p:0.1098700687289238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:857
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1110, 4.9987, 4.6835],
        [5.1110, 4.9008, 4.9077],
        [5.1110, 5.0916, 5.1085],
        [5.1110, 5.0360, 4.7255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:857, step:0 
model_pd.l_p.mean(): 0.1120513305068016 
model_pd.l_d.mean(): -19.137929916381836 
model_pd.lagr.mean(): -19.02587890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6024], device='cuda:0')), ('power', tensor([-20.1197], device='cuda:0'))])
epoch£º857	 i:0 	 global-step:17140	 l-p:0.1120513305068016
epoch£º857	 i:1 	 global-step:17141	 l-p:0.15934337675571442
epoch£º857	 i:2 	 global-step:17142	 l-p:0.1770639419555664
epoch£º857	 i:3 	 global-step:17143	 l-p:0.25745439529418945
epoch£º857	 i:4 	 global-step:17144	 l-p:0.09868305176496506
epoch£º857	 i:5 	 global-step:17145	 l-p:0.12038825452327728
epoch£º857	 i:6 	 global-step:17146	 l-p:0.1266738921403885
epoch£º857	 i:7 	 global-step:17147	 l-p:0.16466818749904633
epoch£º857	 i:8 	 global-step:17148	 l-p:0.13022558391094208
epoch£º857	 i:9 	 global-step:17149	 l-p:0.10606912523508072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:858
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1410, 5.1405, 5.1410],
        [5.1410, 4.9716, 5.0122],
        [5.1410, 5.1409, 5.1410],
        [5.1410, 5.1333, 5.1404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:858, step:0 
model_pd.l_p.mean(): 0.1735237091779709 
model_pd.l_d.mean(): -20.498960494995117 
model_pd.lagr.mean(): -20.325437545776367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4231], device='cuda:0')), ('power', tensor([-21.3204], device='cuda:0'))])
epoch£º858	 i:0 	 global-step:17160	 l-p:0.1735237091779709
epoch£º858	 i:1 	 global-step:17161	 l-p:0.14952662587165833
epoch£º858	 i:2 	 global-step:17162	 l-p:0.12922027707099915
epoch£º858	 i:3 	 global-step:17163	 l-p:0.13934463262557983
epoch£º858	 i:4 	 global-step:17164	 l-p:0.07138101011514664
epoch£º858	 i:5 	 global-step:17165	 l-p:0.15773159265518188
epoch£º858	 i:6 	 global-step:17166	 l-p:0.12100442498922348
epoch£º858	 i:7 	 global-step:17167	 l-p:0.10682102292776108
epoch£º858	 i:8 	 global-step:17168	 l-p:0.1690491884946823
epoch£º858	 i:9 	 global-step:17169	 l-p:0.1030077263712883
====================================================================================================
====================================================================================================
====================================================================================================

epoch:859
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1559, 4.8946, 4.7061],
        [5.1559, 5.1558, 5.1559],
        [5.1559, 5.5587, 5.4896],
        [5.1559, 5.1045, 5.1424]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:859, step:0 
model_pd.l_p.mean(): 0.12027198821306229 
model_pd.l_d.mean(): -20.392030715942383 
model_pd.lagr.mean(): -20.271759033203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4332], device='cuda:0')), ('power', tensor([-21.2220], device='cuda:0'))])
epoch£º859	 i:0 	 global-step:17180	 l-p:0.12027198821306229
epoch£º859	 i:1 	 global-step:17181	 l-p:0.0991922989487648
epoch£º859	 i:2 	 global-step:17182	 l-p:0.1472117006778717
epoch£º859	 i:3 	 global-step:17183	 l-p:0.11392734199762344
epoch£º859	 i:4 	 global-step:17184	 l-p:0.10199397057294846
epoch£º859	 i:5 	 global-step:17185	 l-p:0.15527111291885376
epoch£º859	 i:6 	 global-step:17186	 l-p:0.13813240826129913
epoch£º859	 i:7 	 global-step:17187	 l-p:0.20590047538280487
epoch£º859	 i:8 	 global-step:17188	 l-p:0.1427881270647049
epoch£º859	 i:9 	 global-step:17189	 l-p:0.15802475810050964
====================================================================================================
====================================================================================================
====================================================================================================

epoch:860
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1212, 5.0741, 5.1097],
        [5.1212, 5.1128, 5.1206],
        [5.1212, 4.9630, 5.0101],
        [5.1212, 5.1062, 5.1196]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:860, step:0 
model_pd.l_p.mean(): 0.11038729548454285 
model_pd.l_d.mean(): -20.342634201049805 
model_pd.lagr.mean(): -20.23224639892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4573], device='cuda:0')), ('power', tensor([-21.1967], device='cuda:0'))])
epoch£º860	 i:0 	 global-step:17200	 l-p:0.11038729548454285
epoch£º860	 i:1 	 global-step:17201	 l-p:0.15483590960502625
epoch£º860	 i:2 	 global-step:17202	 l-p:0.149333193898201
epoch£º860	 i:3 	 global-step:17203	 l-p:0.2127876877784729
epoch£º860	 i:4 	 global-step:17204	 l-p:0.14581669867038727
epoch£º860	 i:5 	 global-step:17205	 l-p:0.09830198436975479
epoch£º860	 i:6 	 global-step:17206	 l-p:0.1303343027830124
epoch£º860	 i:7 	 global-step:17207	 l-p:0.13170641660690308
epoch£º860	 i:8 	 global-step:17208	 l-p:0.17576661705970764
epoch£º860	 i:9 	 global-step:17209	 l-p:0.16267427802085876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:861
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1152, 4.8537, 4.7489],
        [5.1152, 5.2209, 4.9767],
        [5.1152, 5.0248, 5.0777],
        [5.1152, 5.1027, 5.1140]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:861, step:0 
model_pd.l_p.mean(): 0.17029020190238953 
model_pd.l_d.mean(): -20.474857330322266 
model_pd.lagr.mean(): -20.304567337036133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4488], device='cuda:0')), ('power', tensor([-21.3225], device='cuda:0'))])
epoch£º861	 i:0 	 global-step:17220	 l-p:0.17029020190238953
epoch£º861	 i:1 	 global-step:17221	 l-p:0.13215278089046478
epoch£º861	 i:2 	 global-step:17222	 l-p:0.1227017343044281
epoch£º861	 i:3 	 global-step:17223	 l-p:0.1732349395751953
epoch£º861	 i:4 	 global-step:17224	 l-p:0.13529536128044128
epoch£º861	 i:5 	 global-step:17225	 l-p:0.1534106284379959
epoch£º861	 i:6 	 global-step:17226	 l-p:0.12335246801376343
epoch£º861	 i:7 	 global-step:17227	 l-p:0.1120714321732521
epoch£º861	 i:8 	 global-step:17228	 l-p:0.12687832117080688
epoch£º861	 i:9 	 global-step:17229	 l-p:0.18359608948230743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:862
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1275, 4.8819, 4.6292],
        [5.1275, 4.9271, 4.9441],
        [5.1275, 5.1194, 5.1269],
        [5.1275, 5.1275, 5.1275]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:862, step:0 
model_pd.l_p.mean(): 0.13135789334774017 
model_pd.l_d.mean(): -20.249313354492188 
model_pd.lagr.mean(): -20.117956161499023 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4522], device='cuda:0')), ('power', tensor([-21.0963], device='cuda:0'))])
epoch£º862	 i:0 	 global-step:17240	 l-p:0.13135789334774017
epoch£º862	 i:1 	 global-step:17241	 l-p:0.13840636610984802
epoch£º862	 i:2 	 global-step:17242	 l-p:0.12859836220741272
epoch£º862	 i:3 	 global-step:17243	 l-p:0.11459849774837494
epoch£º862	 i:4 	 global-step:17244	 l-p:0.18470798432826996
epoch£º862	 i:5 	 global-step:17245	 l-p:0.18963249027729034
epoch£º862	 i:6 	 global-step:17246	 l-p:0.13852645456790924
epoch£º862	 i:7 	 global-step:17247	 l-p:0.13753823935985565
epoch£º862	 i:8 	 global-step:17248	 l-p:0.1645033359527588
epoch£º862	 i:9 	 global-step:17249	 l-p:0.12585189938545227
====================================================================================================
====================================================================================================
====================================================================================================

epoch:863
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1244, 5.0839, 5.1156],
        [5.1244, 5.1034, 5.1215],
        [5.1244, 5.0411, 5.0922],
        [5.1244, 4.8834, 4.6219]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:863, step:0 
model_pd.l_p.mean(): 0.16127178072929382 
model_pd.l_d.mean(): -19.429059982299805 
model_pd.lagr.mean(): -19.26778793334961 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5019], device='cuda:0')), ('power', tensor([-20.3122], device='cuda:0'))])
epoch£º863	 i:0 	 global-step:17260	 l-p:0.16127178072929382
epoch£º863	 i:1 	 global-step:17261	 l-p:0.17070716619491577
epoch£º863	 i:2 	 global-step:17262	 l-p:0.13076749444007874
epoch£º863	 i:3 	 global-step:17263	 l-p:0.12076548486948013
epoch£º863	 i:4 	 global-step:17264	 l-p:0.0914374440908432
epoch£º863	 i:5 	 global-step:17265	 l-p:0.12250219285488129
epoch£º863	 i:6 	 global-step:17266	 l-p:0.1149812564253807
epoch£º863	 i:7 	 global-step:17267	 l-p:0.19967514276504517
epoch£º863	 i:8 	 global-step:17268	 l-p:0.15492258965969086
epoch£º863	 i:9 	 global-step:17269	 l-p:0.11584775894880295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:864
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1408, 5.0353, 4.7216],
        [5.1408, 5.1407, 5.1408],
        [5.1408, 5.1408, 5.1408],
        [5.1408, 5.5316, 5.4544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:864, step:0 
model_pd.l_p.mean(): 0.16567355394363403 
model_pd.l_d.mean(): -19.348876953125 
model_pd.lagr.mean(): -19.183202743530273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4851], device='cuda:0')), ('power', tensor([-20.2131], device='cuda:0'))])
epoch£º864	 i:0 	 global-step:17280	 l-p:0.16567355394363403
epoch£º864	 i:1 	 global-step:17281	 l-p:0.12366069853305817
epoch£º864	 i:2 	 global-step:17282	 l-p:0.10863174498081207
epoch£º864	 i:3 	 global-step:17283	 l-p:0.18187958002090454
epoch£º864	 i:4 	 global-step:17284	 l-p:0.13782265782356262
epoch£º864	 i:5 	 global-step:17285	 l-p:0.06538057327270508
epoch£º864	 i:6 	 global-step:17286	 l-p:0.12558862566947937
epoch£º864	 i:7 	 global-step:17287	 l-p:0.12352319806814194
epoch£º864	 i:8 	 global-step:17288	 l-p:0.1431128829717636
epoch£º864	 i:9 	 global-step:17289	 l-p:0.14356331527233124
====================================================================================================
====================================================================================================
====================================================================================================

epoch:865
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1560, 5.1559, 5.1560],
        [5.1560, 5.1550, 5.1560],
        [5.1560, 5.1560, 5.1560],
        [5.1560, 5.1560, 5.1560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:865, step:0 
model_pd.l_p.mean(): 0.12764355540275574 
model_pd.l_d.mean(): -20.226608276367188 
model_pd.lagr.mean(): -20.09896469116211 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4380], device='cuda:0')), ('power', tensor([-21.0585], device='cuda:0'))])
epoch£º865	 i:0 	 global-step:17300	 l-p:0.12764355540275574
epoch£º865	 i:1 	 global-step:17301	 l-p:0.18622823059558868
epoch£º865	 i:2 	 global-step:17302	 l-p:0.15319308638572693
epoch£º865	 i:3 	 global-step:17303	 l-p:0.07626700401306152
epoch£º865	 i:4 	 global-step:17304	 l-p:0.1733340173959732
epoch£º865	 i:5 	 global-step:17305	 l-p:0.09289834648370743
epoch£º865	 i:6 	 global-step:17306	 l-p:0.11261064559221268
epoch£º865	 i:7 	 global-step:17307	 l-p:0.09502734988927841
epoch£º865	 i:8 	 global-step:17308	 l-p:0.13049128651618958
epoch£º865	 i:9 	 global-step:17309	 l-p:0.14149369299411774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:866
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1681, 5.3278, 5.1101],
        [5.1681, 5.6401, 5.6175],
        [5.1681, 5.1060, 5.1491],
        [5.1681, 5.1654, 5.1680]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:866, step:0 
model_pd.l_p.mean(): 0.07843723148107529 
model_pd.l_d.mean(): -19.512447357177734 
model_pd.lagr.mean(): -19.434009552001953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5002], device='cuda:0')), ('power', tensor([-20.3954], device='cuda:0'))])
epoch£º866	 i:0 	 global-step:17320	 l-p:0.07843723148107529
epoch£º866	 i:1 	 global-step:17321	 l-p:0.1643305867910385
epoch£º866	 i:2 	 global-step:17322	 l-p:0.1230766624212265
epoch£º866	 i:3 	 global-step:17323	 l-p:0.14334140717983246
epoch£º866	 i:4 	 global-step:17324	 l-p:0.1098456159234047
epoch£º866	 i:5 	 global-step:17325	 l-p:0.16467586159706116
epoch£º866	 i:6 	 global-step:17326	 l-p:0.10993288457393646
epoch£º866	 i:7 	 global-step:17327	 l-p:0.12369698286056519
epoch£º866	 i:8 	 global-step:17328	 l-p:0.12078391015529633
epoch£º866	 i:9 	 global-step:17329	 l-p:0.11356284469366074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:867
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1682, 5.1682, 5.1682],
        [5.1682, 4.9925, 5.0286],
        [5.1682, 5.2729, 5.0272],
        [5.1682, 5.1647, 5.1680]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:867, step:0 
model_pd.l_p.mean(): 0.1334507018327713 
model_pd.l_d.mean(): -19.335371017456055 
model_pd.lagr.mean(): -19.201919555664062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5299], device='cuda:0')), ('power', tensor([-20.2457], device='cuda:0'))])
epoch£º867	 i:0 	 global-step:17340	 l-p:0.1334507018327713
epoch£º867	 i:1 	 global-step:17341	 l-p:0.10895610600709915
epoch£º867	 i:2 	 global-step:17342	 l-p:0.21153894066810608
epoch£º867	 i:3 	 global-step:17343	 l-p:0.12710294127464294
epoch£º867	 i:4 	 global-step:17344	 l-p:0.10995455831289291
epoch£º867	 i:5 	 global-step:17345	 l-p:0.10938475281000137
epoch£º867	 i:6 	 global-step:17346	 l-p:0.12235810607671738
epoch£º867	 i:7 	 global-step:17347	 l-p:0.1613815724849701
epoch£º867	 i:8 	 global-step:17348	 l-p:0.0995752140879631
epoch£º867	 i:9 	 global-step:17349	 l-p:0.13859298825263977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:868
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7108,  0.6343,  1.0000,  0.5661,
          1.0000,  0.8924, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2047,  0.1207,  1.0000,  0.0711,
          1.0000,  0.5894, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228]], device='cuda:0')
 pt:tensor([[5.1255, 4.8577, 4.6993],
        [5.1255, 5.1767, 4.9067],
        [5.1255, 4.9150, 4.9218],
        [5.1255, 4.8614, 4.6567]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:868, step:0 
model_pd.l_p.mean(): 0.1679169237613678 
model_pd.l_d.mean(): -20.441625595092773 
model_pd.lagr.mean(): -20.27370834350586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4618], device='cuda:0')), ('power', tensor([-21.3022], device='cuda:0'))])
epoch£º868	 i:0 	 global-step:17360	 l-p:0.1679169237613678
epoch£º868	 i:1 	 global-step:17361	 l-p:0.11911927908658981
epoch£º868	 i:2 	 global-step:17362	 l-p:0.2002783864736557
epoch£º868	 i:3 	 global-step:17363	 l-p:0.16423721611499786
epoch£º868	 i:4 	 global-step:17364	 l-p:0.20079439878463745
epoch£º868	 i:5 	 global-step:17365	 l-p:0.10734203457832336
epoch£º868	 i:6 	 global-step:17366	 l-p:0.1073659285902977
epoch£º868	 i:7 	 global-step:17367	 l-p:0.17600052058696747
epoch£º868	 i:8 	 global-step:17368	 l-p:0.129783034324646
epoch£º868	 i:9 	 global-step:17369	 l-p:0.09786541759967804
====================================================================================================
====================================================================================================
====================================================================================================

epoch:869
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1043, 5.4865, 5.4042],
        [5.1043, 5.1005, 5.1041],
        [5.1043, 4.9093, 4.6040],
        [5.1043, 4.9467, 4.9951]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:869, step:0 
model_pd.l_p.mean(): 0.1368667334318161 
model_pd.l_d.mean(): -19.38532829284668 
model_pd.lagr.mean(): -19.24846076965332 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5721], device='cuda:0')), ('power', tensor([-20.3403], device='cuda:0'))])
epoch£º869	 i:0 	 global-step:17380	 l-p:0.1368667334318161
epoch£º869	 i:1 	 global-step:17381	 l-p:0.09145993739366531
epoch£º869	 i:2 	 global-step:17382	 l-p:0.08723930269479752
epoch£º869	 i:3 	 global-step:17383	 l-p:0.18052490055561066
epoch£º869	 i:4 	 global-step:17384	 l-p:0.1264265477657318
epoch£º869	 i:5 	 global-step:17385	 l-p:0.2750474810600281
epoch£º869	 i:6 	 global-step:17386	 l-p:0.19046567380428314
epoch£º869	 i:7 	 global-step:17387	 l-p:0.5335794687271118
epoch£º869	 i:8 	 global-step:17388	 l-p:0.13584831357002258
epoch£º869	 i:9 	 global-step:17389	 l-p:0.162953183054924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:870
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2005,  0.1173,  1.0000,  0.0687,
          1.0000,  0.5853, 31.6228]], device='cuda:0')
 pt:tensor([[5.0868, 4.8736, 4.8807],
        [5.0868, 4.8155, 4.6737],
        [5.0868, 4.8276, 4.7466],
        [5.0868, 4.8779, 4.8900]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:870, step:0 
model_pd.l_p.mean(): 0.1284768134355545 
model_pd.l_d.mean(): -20.825641632080078 
model_pd.lagr.mean(): -20.69716453552246 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4033], device='cuda:0')), ('power', tensor([-21.6328], device='cuda:0'))])
epoch£º870	 i:0 	 global-step:17400	 l-p:0.1284768134355545
epoch£º870	 i:1 	 global-step:17401	 l-p:0.2081400603055954
epoch£º870	 i:2 	 global-step:17402	 l-p:0.10834214836359024
epoch£º870	 i:3 	 global-step:17403	 l-p:0.38912615180015564
epoch£º870	 i:4 	 global-step:17404	 l-p:0.12070776522159576
epoch£º870	 i:5 	 global-step:17405	 l-p:0.17455808818340302
epoch£º870	 i:6 	 global-step:17406	 l-p:0.14787732064723969
epoch£º870	 i:7 	 global-step:17407	 l-p:0.1840507537126541
epoch£º870	 i:8 	 global-step:17408	 l-p:0.12097329646348953
epoch£º870	 i:9 	 global-step:17409	 l-p:0.20515598356723785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:871
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0923, 4.8278, 4.7257],
        [5.0923, 5.0744, 5.0901],
        [5.0923, 4.9419, 4.6237],
        [5.0923, 4.8553, 4.8269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:871, step:0 
model_pd.l_p.mean(): 0.1716614067554474 
model_pd.l_d.mean(): -20.1038875579834 
model_pd.lagr.mean(): -19.932226181030273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5158], device='cuda:0')), ('power', tensor([-21.0141], device='cuda:0'))])
epoch£º871	 i:0 	 global-step:17420	 l-p:0.1716614067554474
epoch£º871	 i:1 	 global-step:17421	 l-p:0.1680171936750412
epoch£º871	 i:2 	 global-step:17422	 l-p:0.1628957837820053
epoch£º871	 i:3 	 global-step:17423	 l-p:0.1339864730834961
epoch£º871	 i:4 	 global-step:17424	 l-p:0.21285074949264526
epoch£º871	 i:5 	 global-step:17425	 l-p:0.12668167054653168
epoch£º871	 i:6 	 global-step:17426	 l-p:0.1253914088010788
epoch£º871	 i:7 	 global-step:17427	 l-p:0.17617477476596832
epoch£º871	 i:8 	 global-step:17428	 l-p:0.10531306266784668
epoch£º871	 i:9 	 global-step:17429	 l-p:0.14204607903957367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:872
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1253, 5.0829, 5.1158],
        [5.1253, 5.0502, 4.7385],
        [5.1253, 5.1253, 5.1253],
        [5.1253, 5.0926, 5.1193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:872, step:0 
model_pd.l_p.mean(): 0.20746874809265137 
model_pd.l_d.mean(): -20.28012466430664 
model_pd.lagr.mean(): -20.072656631469727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4806], device='cuda:0')), ('power', tensor([-21.1572], device='cuda:0'))])
epoch£º872	 i:0 	 global-step:17440	 l-p:0.20746874809265137
epoch£º872	 i:1 	 global-step:17441	 l-p:0.1314486563205719
epoch£º872	 i:2 	 global-step:17442	 l-p:0.11403724551200867
epoch£º872	 i:3 	 global-step:17443	 l-p:0.10435596108436584
epoch£º872	 i:4 	 global-step:17444	 l-p:0.14421971142292023
epoch£º872	 i:5 	 global-step:17445	 l-p:0.13380445539951324
epoch£º872	 i:6 	 global-step:17446	 l-p:0.1712026298046112
epoch£º872	 i:7 	 global-step:17447	 l-p:0.08880861103534698
epoch£º872	 i:8 	 global-step:17448	 l-p:0.1708650141954422
epoch£º872	 i:9 	 global-step:17449	 l-p:0.12298471480607986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:873
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1435, 5.0137, 5.0686],
        [5.1435, 5.1358, 5.1430],
        [5.1435, 4.8854, 4.7913],
        [5.1435, 4.9096, 4.8798]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:873, step:0 
model_pd.l_p.mean(): 0.10983239859342575 
model_pd.l_d.mean(): -19.272214889526367 
model_pd.lagr.mean(): -19.162382125854492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5350], device='cuda:0')), ('power', tensor([-20.1867], device='cuda:0'))])
epoch£º873	 i:0 	 global-step:17460	 l-p:0.10983239859342575
epoch£º873	 i:1 	 global-step:17461	 l-p:0.14205116033554077
epoch£º873	 i:2 	 global-step:17462	 l-p:0.16470298171043396
epoch£º873	 i:3 	 global-step:17463	 l-p:0.09700658917427063
epoch£º873	 i:4 	 global-step:17464	 l-p:0.14428362250328064
epoch£º873	 i:5 	 global-step:17465	 l-p:0.17007586359977722
epoch£º873	 i:6 	 global-step:17466	 l-p:0.15633371472358704
epoch£º873	 i:7 	 global-step:17467	 l-p:0.1682594120502472
epoch£º873	 i:8 	 global-step:17468	 l-p:0.15046754479408264
epoch£º873	 i:9 	 global-step:17469	 l-p:0.1197987049818039
====================================================================================================
====================================================================================================
====================================================================================================

epoch:874
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1188, 5.1188, 5.1188],
        [5.1188, 5.1185, 5.1188],
        [5.1188, 5.1188, 5.1188],
        [5.1188, 5.2841, 5.0696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:874, step:0 
model_pd.l_p.mean(): 0.11862071603536606 
model_pd.l_d.mean(): -20.522865295410156 
model_pd.lagr.mean(): -20.404245376586914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4232], device='cuda:0')), ('power', tensor([-21.3449], device='cuda:0'))])
epoch£º874	 i:0 	 global-step:17480	 l-p:0.11862071603536606
epoch£º874	 i:1 	 global-step:17481	 l-p:0.14912791550159454
epoch£º874	 i:2 	 global-step:17482	 l-p:0.13757094740867615
epoch£º874	 i:3 	 global-step:17483	 l-p:0.16597414016723633
epoch£º874	 i:4 	 global-step:17484	 l-p:0.17574745416641235
epoch£º874	 i:5 	 global-step:17485	 l-p:0.12467842549085617
epoch£º874	 i:6 	 global-step:17486	 l-p:0.19548220932483673
epoch£º874	 i:7 	 global-step:17487	 l-p:0.13808868825435638
epoch£º874	 i:8 	 global-step:17488	 l-p:0.11296681314706802
epoch£º874	 i:9 	 global-step:17489	 l-p:0.13579389452934265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:875
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1288, 5.0843, 5.1185],
        [5.1288, 4.9647, 5.0097],
        [5.1288, 5.1288, 5.1288],
        [5.1288, 4.9338, 4.6304]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:875, step:0 
model_pd.l_p.mean(): 0.10091875493526459 
model_pd.l_d.mean(): -20.53376579284668 
model_pd.lagr.mean(): -20.43284797668457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4152], device='cuda:0')), ('power', tensor([-21.3478], device='cuda:0'))])
epoch£º875	 i:0 	 global-step:17500	 l-p:0.10091875493526459
epoch£º875	 i:1 	 global-step:17501	 l-p:0.1559041440486908
epoch£º875	 i:2 	 global-step:17502	 l-p:0.19844786822795868
epoch£º875	 i:3 	 global-step:17503	 l-p:0.1964644342660904
epoch£º875	 i:4 	 global-step:17504	 l-p:0.11342288553714752
epoch£º875	 i:5 	 global-step:17505	 l-p:0.11136243492364883
epoch£º875	 i:6 	 global-step:17506	 l-p:0.1375638097524643
epoch£º875	 i:7 	 global-step:17507	 l-p:0.11006614565849304
epoch£º875	 i:8 	 global-step:17508	 l-p:0.08957533538341522
epoch£º875	 i:9 	 global-step:17509	 l-p:0.19961769878864288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:876
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1370, 4.9796, 4.6656],
        [5.1370, 5.5888, 5.5522],
        [5.1370, 5.5424, 5.4740],
        [5.1370, 5.1370, 5.1370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:876, step:0 
model_pd.l_p.mean(): 0.19868747889995575 
model_pd.l_d.mean(): -19.342998504638672 
model_pd.lagr.mean(): -19.144311904907227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5347], device='cuda:0')), ('power', tensor([-20.2585], device='cuda:0'))])
epoch£º876	 i:0 	 global-step:17520	 l-p:0.19868747889995575
epoch£º876	 i:1 	 global-step:17521	 l-p:0.1024119183421135
epoch£º876	 i:2 	 global-step:17522	 l-p:0.10748517513275146
epoch£º876	 i:3 	 global-step:17523	 l-p:0.13138112425804138
epoch£º876	 i:4 	 global-step:17524	 l-p:0.12343952059745789
epoch£º876	 i:5 	 global-step:17525	 l-p:0.10703763365745544
epoch£º876	 i:6 	 global-step:17526	 l-p:0.13131551444530487
epoch£º876	 i:7 	 global-step:17527	 l-p:0.15923555195331573
epoch£º876	 i:8 	 global-step:17528	 l-p:0.12381350249052048
epoch£º876	 i:9 	 global-step:17529	 l-p:0.17376208305358887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:877
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1374, 5.1359, 5.1374],
        [5.1374, 5.1374, 5.1374],
        [5.1374, 4.8884, 4.6376],
        [5.1374, 5.5889, 5.5519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:877, step:0 
model_pd.l_p.mean(): 0.1103077083826065 
model_pd.l_d.mean(): -19.03321647644043 
model_pd.lagr.mean(): -18.922908782958984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5105], device='cuda:0')), ('power', tensor([-19.9178], device='cuda:0'))])
epoch£º877	 i:0 	 global-step:17540	 l-p:0.1103077083826065
epoch£º877	 i:1 	 global-step:17541	 l-p:0.14857777953147888
epoch£º877	 i:2 	 global-step:17542	 l-p:0.15382859110832214
epoch£º877	 i:3 	 global-step:17543	 l-p:0.11280764639377594
epoch£º877	 i:4 	 global-step:17544	 l-p:0.11587855964899063
epoch£º877	 i:5 	 global-step:17545	 l-p:0.1400642693042755
epoch£º877	 i:6 	 global-step:17546	 l-p:0.16318079829216003
epoch£º877	 i:7 	 global-step:17547	 l-p:0.13038270175457
epoch£º877	 i:8 	 global-step:17548	 l-p:0.17719602584838867
epoch£º877	 i:9 	 global-step:17549	 l-p:0.15276454389095306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:878
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1361, 5.1360, 5.1361],
        [5.1361, 5.4535, 5.3277],
        [5.1361, 5.4611, 5.3401],
        [5.1361, 5.1361, 5.1361]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:878, step:0 
model_pd.l_p.mean(): 0.17126800119876862 
model_pd.l_d.mean(): -20.500036239624023 
model_pd.lagr.mean(): -20.328767776489258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4279], device='cuda:0')), ('power', tensor([-21.3266], device='cuda:0'))])
epoch£º878	 i:0 	 global-step:17560	 l-p:0.17126800119876862
epoch£º878	 i:1 	 global-step:17561	 l-p:0.1986800730228424
epoch£º878	 i:2 	 global-step:17562	 l-p:0.09741037338972092
epoch£º878	 i:3 	 global-step:17563	 l-p:0.14320673048496246
epoch£º878	 i:4 	 global-step:17564	 l-p:0.11897768080234528
epoch£º878	 i:5 	 global-step:17565	 l-p:0.13461066782474518
epoch£º878	 i:6 	 global-step:17566	 l-p:0.12983420491218567
epoch£º878	 i:7 	 global-step:17567	 l-p:0.10150808840990067
epoch£º878	 i:8 	 global-step:17568	 l-p:0.13961651921272278
epoch£º878	 i:9 	 global-step:17569	 l-p:0.12290410697460175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:879
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1449, 5.1447, 5.1449],
        [5.1449, 5.0587, 5.1107],
        [5.1449, 4.9305, 4.6395],
        [5.1449, 5.1118, 5.1387]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:879, step:0 
model_pd.l_p.mean(): 0.14703145623207092 
model_pd.l_d.mean(): -20.43326759338379 
model_pd.lagr.mean(): -20.286235809326172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4375], device='cuda:0')), ('power', tensor([-21.2685], device='cuda:0'))])
epoch£º879	 i:0 	 global-step:17580	 l-p:0.14703145623207092
epoch£º879	 i:1 	 global-step:17581	 l-p:0.09949678182601929
epoch£º879	 i:2 	 global-step:17582	 l-p:0.13028788566589355
epoch£º879	 i:3 	 global-step:17583	 l-p:0.19567543268203735
epoch£º879	 i:4 	 global-step:17584	 l-p:0.0633058100938797
epoch£º879	 i:5 	 global-step:17585	 l-p:0.13988782465457916
epoch£º879	 i:6 	 global-step:17586	 l-p:0.11089425534009933
epoch£º879	 i:7 	 global-step:17587	 l-p:0.13274185359477997
epoch£º879	 i:8 	 global-step:17588	 l-p:0.21226400136947632
epoch£º879	 i:9 	 global-step:17589	 l-p:0.13924755156040192
====================================================================================================
====================================================================================================
====================================================================================================

epoch:880
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1325, 4.8649, 4.6713],
        [5.1325, 5.0395, 5.0931],
        [5.1325, 4.8814, 4.8174],
        [5.1325, 5.1323, 5.1325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:880, step:0 
model_pd.l_p.mean(): 0.1661609411239624 
model_pd.l_d.mean(): -20.02011489868164 
model_pd.lagr.mean(): -19.853954315185547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5241], device='cuda:0')), ('power', tensor([-20.9373], device='cuda:0'))])
epoch£º880	 i:0 	 global-step:17600	 l-p:0.1661609411239624
epoch£º880	 i:1 	 global-step:17601	 l-p:0.12470098584890366
epoch£º880	 i:2 	 global-step:17602	 l-p:0.1501956284046173
epoch£º880	 i:3 	 global-step:17603	 l-p:0.11673583835363388
epoch£º880	 i:4 	 global-step:17604	 l-p:0.17420154809951782
epoch£º880	 i:5 	 global-step:17605	 l-p:0.21063914895057678
epoch£º880	 i:6 	 global-step:17606	 l-p:0.13277408480644226
epoch£º880	 i:7 	 global-step:17607	 l-p:0.1356499046087265
epoch£º880	 i:8 	 global-step:17608	 l-p:0.10083482414484024
epoch£º880	 i:9 	 global-step:17609	 l-p:0.1440022885799408
====================================================================================================
====================================================================================================
====================================================================================================

epoch:881
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1201, 5.1201, 5.1201],
        [5.1201, 5.0111, 5.0671],
        [5.1201, 4.8680, 4.8041],
        [5.1201, 5.4126, 5.2714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:881, step:0 
model_pd.l_p.mean(): 0.16939771175384521 
model_pd.l_d.mean(): -19.18574333190918 
model_pd.lagr.mean(): -19.016345977783203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5052], device='cuda:0')), ('power', tensor([-20.0678], device='cuda:0'))])
epoch£º881	 i:0 	 global-step:17620	 l-p:0.16939771175384521
epoch£º881	 i:1 	 global-step:17621	 l-p:0.24094222486019135
epoch£º881	 i:2 	 global-step:17622	 l-p:0.13799265027046204
epoch£º881	 i:3 	 global-step:17623	 l-p:0.1264747679233551
epoch£º881	 i:4 	 global-step:17624	 l-p:0.11091283708810806
epoch£º881	 i:5 	 global-step:17625	 l-p:0.1477425992488861
epoch£º881	 i:6 	 global-step:17626	 l-p:0.12581712007522583
epoch£º881	 i:7 	 global-step:17627	 l-p:0.12952452898025513
epoch£º881	 i:8 	 global-step:17628	 l-p:0.1311301440000534
epoch£º881	 i:9 	 global-step:17629	 l-p:0.10233880579471588
====================================================================================================
====================================================================================================
====================================================================================================

epoch:882
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1297, 4.8842, 4.8356],
        [5.1297, 4.8689, 4.6403],
        [5.1297, 5.1297, 5.1297],
        [5.1297, 5.1256, 5.1295]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:882, step:0 
model_pd.l_p.mean(): 0.09100589901208878 
model_pd.l_d.mean(): -20.26325798034668 
model_pd.lagr.mean(): -20.172252655029297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4798], device='cuda:0')), ('power', tensor([-21.1391], device='cuda:0'))])
epoch£º882	 i:0 	 global-step:17640	 l-p:0.09100589901208878
epoch£º882	 i:1 	 global-step:17641	 l-p:0.1502077877521515
epoch£º882	 i:2 	 global-step:17642	 l-p:0.18378368020057678
epoch£º882	 i:3 	 global-step:17643	 l-p:0.08972631394863129
epoch£º882	 i:4 	 global-step:17644	 l-p:0.2623443603515625
epoch£º882	 i:5 	 global-step:17645	 l-p:0.14946939051151276
epoch£º882	 i:6 	 global-step:17646	 l-p:0.14616145193576813
epoch£º882	 i:7 	 global-step:17647	 l-p:0.14192582666873932
epoch£º882	 i:8 	 global-step:17648	 l-p:0.12038595229387283
epoch£º882	 i:9 	 global-step:17649	 l-p:0.12452532351016998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:883
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1096, 5.1068, 5.1095],
        [5.1096, 5.1091, 5.1096],
        [5.1096, 5.0967, 5.1084],
        [5.1096, 4.9440, 4.9897]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:883, step:0 
model_pd.l_p.mean(): 0.1302216649055481 
model_pd.l_d.mean(): -20.509214401245117 
model_pd.lagr.mean(): -20.378992080688477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4460], device='cuda:0')), ('power', tensor([-21.3547], device='cuda:0'))])
epoch£º883	 i:0 	 global-step:17660	 l-p:0.1302216649055481
epoch£º883	 i:1 	 global-step:17661	 l-p:0.14177295565605164
epoch£º883	 i:2 	 global-step:17662	 l-p:0.13728581368923187
epoch£º883	 i:3 	 global-step:17663	 l-p:0.2658073604106903
epoch£º883	 i:4 	 global-step:17664	 l-p:0.15869589149951935
epoch£º883	 i:5 	 global-step:17665	 l-p:0.16143091022968292
epoch£º883	 i:6 	 global-step:17666	 l-p:0.19165311753749847
epoch£º883	 i:7 	 global-step:17667	 l-p:0.1029568538069725
epoch£º883	 i:8 	 global-step:17668	 l-p:0.11162381619215012
epoch£º883	 i:9 	 global-step:17669	 l-p:0.13708090782165527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:884
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1249, 5.1249, 5.1249],
        [5.1249, 4.8572, 4.6526],
        [5.1249, 5.1249, 5.1249],
        [5.1249, 4.9179, 4.6176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:884, step:0 
model_pd.l_p.mean(): 0.11012527346611023 
model_pd.l_d.mean(): -19.80208969116211 
model_pd.lagr.mean(): -19.691965103149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4827], device='cuda:0')), ('power', tensor([-20.6723], device='cuda:0'))])
epoch£º884	 i:0 	 global-step:17680	 l-p:0.11012527346611023
epoch£º884	 i:1 	 global-step:17681	 l-p:0.09463202208280563
epoch£º884	 i:2 	 global-step:17682	 l-p:0.15530988574028015
epoch£º884	 i:3 	 global-step:17683	 l-p:0.12828467786312103
epoch£º884	 i:4 	 global-step:17684	 l-p:0.14560465514659882
epoch£º884	 i:5 	 global-step:17685	 l-p:0.08588596433401108
epoch£º884	 i:6 	 global-step:17686	 l-p:0.15228597819805145
epoch£º884	 i:7 	 global-step:17687	 l-p:0.15597200393676758
epoch£º884	 i:8 	 global-step:17688	 l-p:0.20273669064044952
epoch£º884	 i:9 	 global-step:17689	 l-p:0.134353369474411
====================================================================================================
====================================================================================================
====================================================================================================

epoch:885
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1574, 4.9890, 4.6772],
        [5.1574, 5.1574, 5.1574],
        [5.1574, 4.9022, 4.8219],
        [5.1574, 5.1761, 4.8913]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:885, step:0 
model_pd.l_p.mean(): 0.13811491429805756 
model_pd.l_d.mean(): -20.215845108032227 
model_pd.lagr.mean(): -20.077730178833008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4754], device='cuda:0')), ('power', tensor([-21.0862], device='cuda:0'))])
epoch£º885	 i:0 	 global-step:17700	 l-p:0.13811491429805756
epoch£º885	 i:1 	 global-step:17701	 l-p:0.19519811868667603
epoch£º885	 i:2 	 global-step:17702	 l-p:0.07882653921842575
epoch£º885	 i:3 	 global-step:17703	 l-p:0.13702242076396942
epoch£º885	 i:4 	 global-step:17704	 l-p:0.13568001985549927
epoch£º885	 i:5 	 global-step:17705	 l-p:0.12012568861246109
epoch£º885	 i:6 	 global-step:17706	 l-p:0.12030494213104248
epoch£º885	 i:7 	 global-step:17707	 l-p:0.11913429200649261
epoch£º885	 i:8 	 global-step:17708	 l-p:0.12350781261920929
epoch£º885	 i:9 	 global-step:17709	 l-p:0.13978759944438934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:886
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1496, 5.1479, 5.1496],
        [5.1496, 4.9922, 5.0404],
        [5.1496, 5.1419, 5.1491],
        [5.1496, 5.0551, 5.1090]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:886, step:0 
model_pd.l_p.mean(): 0.14743633568286896 
model_pd.l_d.mean(): -19.228473663330078 
model_pd.lagr.mean(): -19.081037521362305 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5704], device='cuda:0')), ('power', tensor([-20.1788], device='cuda:0'))])
epoch£º886	 i:0 	 global-step:17720	 l-p:0.14743633568286896
epoch£º886	 i:1 	 global-step:17721	 l-p:0.10039072483778
epoch£º886	 i:2 	 global-step:17722	 l-p:0.17668652534484863
epoch£º886	 i:3 	 global-step:17723	 l-p:0.14726625382900238
epoch£º886	 i:4 	 global-step:17724	 l-p:0.11760912835597992
epoch£º886	 i:5 	 global-step:17725	 l-p:0.23479445278644562
epoch£º886	 i:6 	 global-step:17726	 l-p:0.15890172123908997
epoch£º886	 i:7 	 global-step:17727	 l-p:0.09754303842782974
epoch£º886	 i:8 	 global-step:17728	 l-p:0.14395727217197418
epoch£º886	 i:9 	 global-step:17729	 l-p:0.08319907635450363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:887
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1227, 5.0998, 5.1195],
        [5.1227, 4.8536, 4.7255],
        [5.1227, 5.0042, 5.0608],
        [5.1227, 5.0280, 5.0822]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:887, step:0 
model_pd.l_p.mean(): 0.12056843191385269 
model_pd.l_d.mean(): -20.821096420288086 
model_pd.lagr.mean(): -20.70052719116211 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3883], device='cuda:0')), ('power', tensor([-21.6127], device='cuda:0'))])
epoch£º887	 i:0 	 global-step:17740	 l-p:0.12056843191385269
epoch£º887	 i:1 	 global-step:17741	 l-p:0.11641594022512436
epoch£º887	 i:2 	 global-step:17742	 l-p:0.14997991919517517
epoch£º887	 i:3 	 global-step:17743	 l-p:0.12035602331161499
epoch£º887	 i:4 	 global-step:17744	 l-p:0.22505244612693787
epoch£º887	 i:5 	 global-step:17745	 l-p:0.12156057357788086
epoch£º887	 i:6 	 global-step:17746	 l-p:0.2140243500471115
epoch£º887	 i:7 	 global-step:17747	 l-p:0.12102087587118149
epoch£º887	 i:8 	 global-step:17748	 l-p:0.21240317821502686
epoch£º887	 i:9 	 global-step:17749	 l-p:0.08525130152702332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:888
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[5.1205, 5.4123, 5.2699],
        [5.1205, 4.8711, 4.8173],
        [5.1205, 5.3895, 5.2332],
        [5.1205, 4.8732, 4.8244]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:888, step:0 
model_pd.l_p.mean(): 0.07597124576568604 
model_pd.l_d.mean(): -19.292579650878906 
model_pd.lagr.mean(): -19.21660804748535 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5853], device='cuda:0')), ('power', tensor([-20.2596], device='cuda:0'))])
epoch£º888	 i:0 	 global-step:17760	 l-p:0.07597124576568604
epoch£º888	 i:1 	 global-step:17761	 l-p:0.2533745765686035
epoch£º888	 i:2 	 global-step:17762	 l-p:0.13446441292762756
epoch£º888	 i:3 	 global-step:17763	 l-p:0.11619982123374939
epoch£º888	 i:4 	 global-step:17764	 l-p:0.17473573982715607
epoch£º888	 i:5 	 global-step:17765	 l-p:0.12313397228717804
epoch£º888	 i:6 	 global-step:17766	 l-p:0.10621857643127441
epoch£º888	 i:7 	 global-step:17767	 l-p:0.1149633377790451
epoch£º888	 i:8 	 global-step:17768	 l-p:0.18757666647434235
epoch£º888	 i:9 	 global-step:17769	 l-p:0.1932307332754135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:889
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1163, 5.1163, 5.1163],
        [5.1163, 5.1163, 5.1163],
        [5.1163, 4.9437, 4.9860],
        [5.1163, 5.0528, 5.0969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:889, step:0 
model_pd.l_p.mean(): 0.16336709260940552 
model_pd.l_d.mean(): -20.735448837280273 
model_pd.lagr.mean(): -20.57208251953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4233], device='cuda:0')), ('power', tensor([-21.5616], device='cuda:0'))])
epoch£º889	 i:0 	 global-step:17780	 l-p:0.16336709260940552
epoch£º889	 i:1 	 global-step:17781	 l-p:0.17528636753559113
epoch£º889	 i:2 	 global-step:17782	 l-p:0.12966275215148926
epoch£º889	 i:3 	 global-step:17783	 l-p:0.144383043050766
epoch£º889	 i:4 	 global-step:17784	 l-p:0.11836975067853928
epoch£º889	 i:5 	 global-step:17785	 l-p:0.1484849452972412
epoch£º889	 i:6 	 global-step:17786	 l-p:0.12482716888189316
epoch£º889	 i:7 	 global-step:17787	 l-p:0.16240260004997253
epoch£º889	 i:8 	 global-step:17788	 l-p:0.10578225553035736
epoch£º889	 i:9 	 global-step:17789	 l-p:0.16230082511901855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:890
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1444, 5.1444, 5.1444],
        [5.1444, 5.1444, 5.1444],
        [5.1444, 4.9318, 4.9382],
        [5.1444, 4.9235, 4.9190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:890, step:0 
model_pd.l_p.mean(): 0.11425439268350601 
model_pd.l_d.mean(): -20.29867172241211 
model_pd.lagr.mean(): -20.184417724609375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4654], device='cuda:0')), ('power', tensor([-21.1603], device='cuda:0'))])
epoch£º890	 i:0 	 global-step:17800	 l-p:0.11425439268350601
epoch£º890	 i:1 	 global-step:17801	 l-p:0.127073273062706
epoch£º890	 i:2 	 global-step:17802	 l-p:0.09306329488754272
epoch£º890	 i:3 	 global-step:17803	 l-p:0.12813128530979156
epoch£º890	 i:4 	 global-step:17804	 l-p:0.1917455494403839
epoch£º890	 i:5 	 global-step:17805	 l-p:0.1378285139799118
epoch£º890	 i:6 	 global-step:17806	 l-p:0.13846802711486816
epoch£º890	 i:7 	 global-step:17807	 l-p:0.14372709393501282
epoch£º890	 i:8 	 global-step:17808	 l-p:0.10729074478149414
epoch£º890	 i:9 	 global-step:17809	 l-p:0.12755580246448517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:891
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1732, 5.1521, 5.1703],
        [5.1732, 5.2724, 5.0217],
        [5.1732, 5.1137, 5.1558],
        [5.1732, 5.1708, 5.1731]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:891, step:0 
model_pd.l_p.mean(): 0.12647025287151337 
model_pd.l_d.mean(): -18.968942642211914 
model_pd.lagr.mean(): -18.842472076416016 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5492], device='cuda:0')), ('power', tensor([-19.8925], device='cuda:0'))])
epoch£º891	 i:0 	 global-step:17820	 l-p:0.12647025287151337
epoch£º891	 i:1 	 global-step:17821	 l-p:0.10560785233974457
epoch£º891	 i:2 	 global-step:17822	 l-p:0.13559234142303467
epoch£º891	 i:3 	 global-step:17823	 l-p:0.15686868131160736
epoch£º891	 i:4 	 global-step:17824	 l-p:0.10514252632856369
epoch£º891	 i:5 	 global-step:17825	 l-p:0.1190020814538002
epoch£º891	 i:6 	 global-step:17826	 l-p:0.10286444425582886
epoch£º891	 i:7 	 global-step:17827	 l-p:0.143743634223938
epoch£º891	 i:8 	 global-step:17828	 l-p:0.15861739218235016
epoch£º891	 i:9 	 global-step:17829	 l-p:0.10487471520900726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:892
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1771, 4.9377, 4.8981],
        [5.1771, 5.1771, 5.1771],
        [5.1771, 5.0448, 4.7283],
        [5.1771, 5.1726, 5.1768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:892, step:0 
model_pd.l_p.mean(): 0.13420847058296204 
model_pd.l_d.mean(): -19.84290885925293 
model_pd.lagr.mean(): -19.70870018005371 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5247], device='cuda:0')), ('power', tensor([-20.7574], device='cuda:0'))])
epoch£º892	 i:0 	 global-step:17840	 l-p:0.13420847058296204
epoch£º892	 i:1 	 global-step:17841	 l-p:0.1372307389974594
epoch£º892	 i:2 	 global-step:17842	 l-p:0.178872212767601
epoch£º892	 i:3 	 global-step:17843	 l-p:0.04205389320850372
epoch£º892	 i:4 	 global-step:17844	 l-p:0.1626719981431961
epoch£º892	 i:5 	 global-step:17845	 l-p:0.09388280659914017
epoch£º892	 i:6 	 global-step:17846	 l-p:0.09913349151611328
epoch£º892	 i:7 	 global-step:17847	 l-p:0.11639606207609177
epoch£º892	 i:8 	 global-step:17848	 l-p:0.14547041058540344
epoch£º892	 i:9 	 global-step:17849	 l-p:0.1411873698234558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:893
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1638, 5.2272, 4.9599],
        [5.1638, 5.5547, 5.4743],
        [5.1638, 5.1503, 5.1625],
        [5.1638, 4.9570, 4.9692]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:893, step:0 
model_pd.l_p.mean(): 0.07220396399497986 
model_pd.l_d.mean(): -19.241954803466797 
model_pd.lagr.mean(): -19.169750213623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5869], device='cuda:0')), ('power', tensor([-20.2096], device='cuda:0'))])
epoch£º893	 i:0 	 global-step:17860	 l-p:0.07220396399497986
epoch£º893	 i:1 	 global-step:17861	 l-p:0.17065416276454926
epoch£º893	 i:2 	 global-step:17862	 l-p:0.13644957542419434
epoch£º893	 i:3 	 global-step:17863	 l-p:0.12424241751432419
epoch£º893	 i:4 	 global-step:17864	 l-p:0.20863276720046997
epoch£º893	 i:5 	 global-step:17865	 l-p:0.12003465741872787
epoch£º893	 i:6 	 global-step:17866	 l-p:0.11657803505659103
epoch£º893	 i:7 	 global-step:17867	 l-p:0.15827453136444092
epoch£º893	 i:8 	 global-step:17868	 l-p:0.11118614673614502
epoch£º893	 i:9 	 global-step:17869	 l-p:0.1354931741952896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:894
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1289, 5.1284, 5.1289],
        [5.1289, 5.1229, 5.1286],
        [5.1289, 4.9079, 4.6141],
        [5.1289, 4.8572, 4.7142]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:894, step:0 
model_pd.l_p.mean(): 0.1251082420349121 
model_pd.l_d.mean(): -19.895750045776367 
model_pd.lagr.mean(): -19.770641326904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4812], device='cuda:0')), ('power', tensor([-20.7661], device='cuda:0'))])
epoch£º894	 i:0 	 global-step:17880	 l-p:0.1251082420349121
epoch£º894	 i:1 	 global-step:17881	 l-p:0.07595303654670715
epoch£º894	 i:2 	 global-step:17882	 l-p:0.13261115550994873
epoch£º894	 i:3 	 global-step:17883	 l-p:0.12437021732330322
epoch£º894	 i:4 	 global-step:17884	 l-p:0.09556402266025543
epoch£º894	 i:5 	 global-step:17885	 l-p:0.12445326894521713
epoch£º894	 i:6 	 global-step:17886	 l-p:0.11042695492506027
epoch£º894	 i:7 	 global-step:17887	 l-p:0.24881598353385925
epoch£º894	 i:8 	 global-step:17888	 l-p:0.4373263418674469
epoch£º894	 i:9 	 global-step:17889	 l-p:0.24772116541862488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:895
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0909, 5.0909, 5.0909],
        [5.0909, 5.0779, 5.0897],
        [5.0909, 5.3706, 5.2207],
        [5.0909, 4.8151, 4.6723]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:895, step:0 
model_pd.l_p.mean(): 0.13225723803043365 
model_pd.l_d.mean(): -18.79053497314453 
model_pd.lagr.mean(): -18.65827751159668 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6035], device='cuda:0')), ('power', tensor([-19.7669], device='cuda:0'))])
epoch£º895	 i:0 	 global-step:17900	 l-p:0.13225723803043365
epoch£º895	 i:1 	 global-step:17901	 l-p:0.18730752170085907
epoch£º895	 i:2 	 global-step:17902	 l-p:0.16570815443992615
epoch£º895	 i:3 	 global-step:17903	 l-p:0.16785629093647003
epoch£º895	 i:4 	 global-step:17904	 l-p:0.14026550948619843
epoch£º895	 i:5 	 global-step:17905	 l-p:0.11949219554662704
epoch£º895	 i:6 	 global-step:17906	 l-p:0.1371465027332306
epoch£º895	 i:7 	 global-step:17907	 l-p:0.1949615776538849
epoch£º895	 i:8 	 global-step:17908	 l-p:0.2757940888404846
epoch£º895	 i:9 	 global-step:17909	 l-p:0.09651245176792145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:896
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1121, 5.4895, 5.4013],
        [5.1121, 5.0617, 5.0994],
        [5.1121, 5.1121, 5.1121],
        [5.1121, 4.8400, 4.7104]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:896, step:0 
model_pd.l_p.mean(): 0.1354224681854248 
model_pd.l_d.mean(): -20.110097885131836 
model_pd.lagr.mean(): -19.97467613220215 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4893], device='cuda:0')), ('power', tensor([-20.9930], device='cuda:0'))])
epoch£º896	 i:0 	 global-step:17920	 l-p:0.1354224681854248
epoch£º896	 i:1 	 global-step:17921	 l-p:0.2005704641342163
epoch£º896	 i:2 	 global-step:17922	 l-p:0.1174631342291832
epoch£º896	 i:3 	 global-step:17923	 l-p:0.19512376189231873
epoch£º896	 i:4 	 global-step:17924	 l-p:0.17714953422546387
epoch£º896	 i:5 	 global-step:17925	 l-p:0.13088485598564148
epoch£º896	 i:6 	 global-step:17926	 l-p:0.1210777536034584
epoch£º896	 i:7 	 global-step:17927	 l-p:0.15366467833518982
epoch£º896	 i:8 	 global-step:17928	 l-p:0.11371862888336182
epoch£º896	 i:9 	 global-step:17929	 l-p:0.13239386677742004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:897
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1430, 5.0981, 5.1325],
        [5.1430, 5.1385, 5.1428],
        [5.1430, 5.0966, 5.1319],
        [5.1430, 4.8938, 4.8395]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:897, step:0 
model_pd.l_p.mean(): 0.1411626935005188 
model_pd.l_d.mean(): -20.392045974731445 
model_pd.lagr.mean(): -20.250883102416992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4274], device='cuda:0')), ('power', tensor([-21.2160], device='cuda:0'))])
epoch£º897	 i:0 	 global-step:17940	 l-p:0.1411626935005188
epoch£º897	 i:1 	 global-step:17941	 l-p:0.07878810912370682
epoch£º897	 i:2 	 global-step:17942	 l-p:0.11889861524105072
epoch£º897	 i:3 	 global-step:17943	 l-p:0.1396801471710205
epoch£º897	 i:4 	 global-step:17944	 l-p:0.16119170188903809
epoch£º897	 i:5 	 global-step:17945	 l-p:0.14096668362617493
epoch£º897	 i:6 	 global-step:17946	 l-p:0.1486358791589737
epoch£º897	 i:7 	 global-step:17947	 l-p:0.07539123296737671
epoch£º897	 i:8 	 global-step:17948	 l-p:0.2238282710313797
epoch£º897	 i:9 	 global-step:17949	 l-p:0.14461146295070648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:898
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1482, 4.9016, 4.8523],
        [5.1482, 5.1476, 5.1482],
        [5.1482, 5.1034, 5.1378],
        [5.1482, 5.6043, 5.5682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:898, step:0 
model_pd.l_p.mean(): 0.15781371295452118 
model_pd.l_d.mean(): -19.505779266357422 
model_pd.lagr.mean(): -19.347965240478516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5146], device='cuda:0')), ('power', tensor([-20.4035], device='cuda:0'))])
epoch£º898	 i:0 	 global-step:17960	 l-p:0.15781371295452118
epoch£º898	 i:1 	 global-step:17961	 l-p:0.163251593708992
epoch£º898	 i:2 	 global-step:17962	 l-p:0.1566547006368637
epoch£º898	 i:3 	 global-step:17963	 l-p:0.14238737523555756
epoch£º898	 i:4 	 global-step:17964	 l-p:0.06868866831064224
epoch£º898	 i:5 	 global-step:17965	 l-p:0.0875089168548584
epoch£º898	 i:6 	 global-step:17966	 l-p:0.10651290416717529
epoch£º898	 i:7 	 global-step:17967	 l-p:0.14467091858386993
epoch£º898	 i:8 	 global-step:17968	 l-p:0.1452779769897461
epoch£º898	 i:9 	 global-step:17969	 l-p:0.10821937024593353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:899
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1942, 5.0914, 4.7757],
        [5.1942, 4.9401, 4.7066],
        [5.1942, 4.9509, 4.9012],
        [5.1942, 5.1932, 5.1942]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:899, step:0 
model_pd.l_p.mean(): 0.15412688255310059 
model_pd.l_d.mean(): -19.942792892456055 
model_pd.lagr.mean(): -19.788665771484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4466], device='cuda:0')), ('power', tensor([-20.7783], device='cuda:0'))])
epoch£º899	 i:0 	 global-step:17980	 l-p:0.15412688255310059
epoch£º899	 i:1 	 global-step:17981	 l-p:0.07329244166612625
epoch£º899	 i:2 	 global-step:17982	 l-p:0.09355641901493073
epoch£º899	 i:3 	 global-step:17983	 l-p:0.11956502497196198
epoch£º899	 i:4 	 global-step:17984	 l-p:0.13004939258098602
epoch£º899	 i:5 	 global-step:17985	 l-p:0.18298961222171783
epoch£º899	 i:6 	 global-step:17986	 l-p:0.14912021160125732
epoch£º899	 i:7 	 global-step:17987	 l-p:0.018688714131712914
epoch£º899	 i:8 	 global-step:17988	 l-p:0.12441769987344742
epoch£º899	 i:9 	 global-step:17989	 l-p:0.12277963012456894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:900
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1983, 4.9340, 4.8018],
        [5.1983, 5.0894, 5.1450],
        [5.1983, 5.1983, 5.1983],
        [5.1983, 5.0963, 5.1512]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:900, step:0 
model_pd.l_p.mean(): 0.14567707479000092 
model_pd.l_d.mean(): -20.48005485534668 
model_pd.lagr.mean(): -20.33437728881836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4159], device='cuda:0')), ('power', tensor([-21.2938], device='cuda:0'))])
epoch£º900	 i:0 	 global-step:18000	 l-p:0.14567707479000092
epoch£º900	 i:1 	 global-step:18001	 l-p:0.04302145913243294
epoch£º900	 i:2 	 global-step:18002	 l-p:0.1503782868385315
epoch£º900	 i:3 	 global-step:18003	 l-p:0.15517541766166687
epoch£º900	 i:4 	 global-step:18004	 l-p:0.12580670416355133
epoch£º900	 i:5 	 global-step:18005	 l-p:0.13145898282527924
epoch£º900	 i:6 	 global-step:18006	 l-p:0.09962743520736694
epoch£º900	 i:7 	 global-step:18007	 l-p:0.11970053613185883
epoch£º900	 i:8 	 global-step:18008	 l-p:0.1250677853822708
epoch£º900	 i:9 	 global-step:18009	 l-p:0.11647822707891464
====================================================================================================
====================================================================================================
====================================================================================================

epoch:901
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1612, 5.1185, 5.1517],
        [5.1612, 5.1569, 5.1610],
        [5.1612, 5.2743, 5.0297],
        [5.1612, 4.9284, 4.9052]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:901, step:0 
model_pd.l_p.mean(): 0.18044696748256683 
model_pd.l_d.mean(): -20.294950485229492 
model_pd.lagr.mean(): -20.114503860473633 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4731], device='cuda:0')), ('power', tensor([-21.1645], device='cuda:0'))])
epoch£º901	 i:0 	 global-step:18020	 l-p:0.18044696748256683
epoch£º901	 i:1 	 global-step:18021	 l-p:0.14120154082775116
epoch£º901	 i:2 	 global-step:18022	 l-p:0.12273704260587692
epoch£º901	 i:3 	 global-step:18023	 l-p:0.09428346157073975
epoch£º901	 i:4 	 global-step:18024	 l-p:0.12604878842830658
epoch£º901	 i:5 	 global-step:18025	 l-p:0.14892373979091644
epoch£º901	 i:6 	 global-step:18026	 l-p:0.1822904795408249
epoch£º901	 i:7 	 global-step:18027	 l-p:0.10176767408847809
epoch£º901	 i:8 	 global-step:18028	 l-p:0.2146587073802948
epoch£º901	 i:9 	 global-step:18029	 l-p:0.11841993033885956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:902
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1212, 5.1181, 5.1211],
        [5.1212, 4.8821, 4.8535],
        [5.1212, 4.9762, 4.6546],
        [5.1212, 5.0473, 5.0958]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:902, step:0 
model_pd.l_p.mean(): 0.1328127086162567 
model_pd.l_d.mean(): -19.427452087402344 
model_pd.lagr.mean(): -19.294639587402344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4897], device='cuda:0')), ('power', tensor([-20.2979], device='cuda:0'))])
epoch£º902	 i:0 	 global-step:18040	 l-p:0.1328127086162567
epoch£º902	 i:1 	 global-step:18041	 l-p:0.1630273163318634
epoch£º902	 i:2 	 global-step:18042	 l-p:0.1964176893234253
epoch£º902	 i:3 	 global-step:18043	 l-p:0.1933894157409668
epoch£º902	 i:4 	 global-step:18044	 l-p:0.11155307292938232
epoch£º902	 i:5 	 global-step:18045	 l-p:0.1744050532579422
epoch£º902	 i:6 	 global-step:18046	 l-p:0.11604852229356766
epoch£º902	 i:7 	 global-step:18047	 l-p:0.10815839469432831
epoch£º902	 i:8 	 global-step:18048	 l-p:0.1377497762441635
epoch£º902	 i:9 	 global-step:18049	 l-p:0.11031488329172134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:903
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1414, 5.0935, 5.1297],
        [5.1414, 5.4368, 5.2951],
        [5.1414, 4.9553, 4.9888],
        [5.1414, 4.9936, 5.0462]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:903, step:0 
model_pd.l_p.mean(): 0.1441018283367157 
model_pd.l_d.mean(): -20.109573364257812 
model_pd.lagr.mean(): -19.965471267700195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5120], device='cuda:0')), ('power', tensor([-21.0159], device='cuda:0'))])
epoch£º903	 i:0 	 global-step:18060	 l-p:0.1441018283367157
epoch£º903	 i:1 	 global-step:18061	 l-p:0.13855859637260437
epoch£º903	 i:2 	 global-step:18062	 l-p:0.14967723190784454
epoch£º903	 i:3 	 global-step:18063	 l-p:0.13419948518276215
epoch£º903	 i:4 	 global-step:18064	 l-p:0.0994395837187767
epoch£º903	 i:5 	 global-step:18065	 l-p:0.15556062757968903
epoch£º903	 i:6 	 global-step:18066	 l-p:0.13241244852542877
epoch£º903	 i:7 	 global-step:18067	 l-p:0.12727391719818115
epoch£º903	 i:8 	 global-step:18068	 l-p:0.12647736072540283
epoch£º903	 i:9 	 global-step:18069	 l-p:0.1438685953617096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:904
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1515, 5.1259, 5.1476],
        [5.1515, 4.9688, 5.0043],
        [5.1515, 5.1515, 5.1515],
        [5.1515, 5.1472, 5.1513]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:904, step:0 
model_pd.l_p.mean(): 0.116798035800457 
model_pd.l_d.mean(): -19.207786560058594 
model_pd.lagr.mean(): -19.090988159179688 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5351], device='cuda:0')), ('power', tensor([-20.1212], device='cuda:0'))])
epoch£º904	 i:0 	 global-step:18080	 l-p:0.116798035800457
epoch£º904	 i:1 	 global-step:18081	 l-p:0.1525588482618332
epoch£º904	 i:2 	 global-step:18082	 l-p:0.15210852026939392
epoch£º904	 i:3 	 global-step:18083	 l-p:0.14696964621543884
epoch£º904	 i:4 	 global-step:18084	 l-p:0.12694691121578217
epoch£º904	 i:5 	 global-step:18085	 l-p:0.19172559678554535
epoch£º904	 i:6 	 global-step:18086	 l-p:0.12260459363460541
epoch£º904	 i:7 	 global-step:18087	 l-p:0.19284692406654358
epoch£º904	 i:8 	 global-step:18088	 l-p:0.08024516701698303
epoch£º904	 i:9 	 global-step:18089	 l-p:0.1280200034379959
====================================================================================================
====================================================================================================
====================================================================================================

epoch:905
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1226, 5.1217, 5.1225],
        [5.1226, 5.0118, 5.0686],
        [5.1226, 4.8506, 4.6447],
        [5.1226, 4.8670, 4.8027]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:905, step:0 
model_pd.l_p.mean(): 0.138408824801445 
model_pd.l_d.mean(): -20.41529083251953 
model_pd.lagr.mean(): -20.27688217163086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4211], device='cuda:0')), ('power', tensor([-21.2332], device='cuda:0'))])
epoch£º905	 i:0 	 global-step:18100	 l-p:0.138408824801445
epoch£º905	 i:1 	 global-step:18101	 l-p:0.12438295781612396
epoch£º905	 i:2 	 global-step:18102	 l-p:0.1321416050195694
epoch£º905	 i:3 	 global-step:18103	 l-p:0.12415193021297455
epoch£º905	 i:4 	 global-step:18104	 l-p:0.09189959615468979
epoch£º905	 i:5 	 global-step:18105	 l-p:0.20139411091804504
epoch£º905	 i:6 	 global-step:18106	 l-p:0.2000846117734909
epoch£º905	 i:7 	 global-step:18107	 l-p:0.2487127184867859
epoch£º905	 i:8 	 global-step:18108	 l-p:0.13672885298728943
epoch£º905	 i:9 	 global-step:18109	 l-p:0.17802096903324127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:906
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1151, 4.8767, 4.5935],
        [5.1151, 4.8879, 4.5941],
        [5.1151, 5.1950, 4.9346],
        [5.1151, 5.1147, 5.1151]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:906, step:0 
model_pd.l_p.mean(): 0.12626145780086517 
model_pd.l_d.mean(): -20.0786190032959 
model_pd.lagr.mean(): -19.95235824584961 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4776], device='cuda:0')), ('power', tensor([-20.9487], device='cuda:0'))])
epoch£º906	 i:0 	 global-step:18120	 l-p:0.12626145780086517
epoch£º906	 i:1 	 global-step:18121	 l-p:0.09990761429071426
epoch£º906	 i:2 	 global-step:18122	 l-p:0.13252194225788116
epoch£º906	 i:3 	 global-step:18123	 l-p:0.23986278474330902
epoch£º906	 i:4 	 global-step:18124	 l-p:0.14783735573291779
epoch£º906	 i:5 	 global-step:18125	 l-p:0.18889980018138885
epoch£º906	 i:6 	 global-step:18126	 l-p:0.15051749348640442
epoch£º906	 i:7 	 global-step:18127	 l-p:0.12163754552602768
epoch£º906	 i:8 	 global-step:18128	 l-p:0.11666900664567947
epoch£º906	 i:9 	 global-step:18129	 l-p:0.15300194919109344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:907
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1409, 5.1133, 5.1365],
        [5.1409, 5.0143, 5.0709],
        [5.1409, 5.0616, 5.1119],
        [5.1409, 4.9011, 4.6249]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:907, step:0 
model_pd.l_p.mean(): 0.2200455367565155 
model_pd.l_d.mean(): -19.47014045715332 
model_pd.lagr.mean(): -19.25009536743164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4779], device='cuda:0')), ('power', tensor([-20.3292], device='cuda:0'))])
epoch£º907	 i:0 	 global-step:18140	 l-p:0.2200455367565155
epoch£º907	 i:1 	 global-step:18141	 l-p:0.12845435738563538
epoch£º907	 i:2 	 global-step:18142	 l-p:0.05896933376789093
epoch£º907	 i:3 	 global-step:18143	 l-p:0.159254252910614
epoch£º907	 i:4 	 global-step:18144	 l-p:0.13413220643997192
epoch£º907	 i:5 	 global-step:18145	 l-p:0.1130533367395401
epoch£º907	 i:6 	 global-step:18146	 l-p:0.13392746448516846
epoch£º907	 i:7 	 global-step:18147	 l-p:0.11806617677211761
epoch£º907	 i:8 	 global-step:18148	 l-p:0.17816363275051117
epoch£º907	 i:9 	 global-step:18149	 l-p:0.12510289251804352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:908
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1810,  0.1024,  1.0000,  0.0579,
          1.0000,  0.5657, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4788,  0.3746,  1.0000,  0.2931,
          1.0000,  0.7823, 31.6228]], device='cuda:0')
 pt:tensor([[5.1518, 5.4268, 5.2720],
        [5.1518, 4.9616, 4.9917],
        [5.1518, 4.9471, 4.6437],
        [5.1518, 4.9381, 4.6401]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:908, step:0 
model_pd.l_p.mean(): 0.11698003858327866 
model_pd.l_d.mean(): -19.188892364501953 
model_pd.lagr.mean(): -19.07191276550293 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5868], device='cuda:0')), ('power', tensor([-20.1555], device='cuda:0'))])
epoch£º908	 i:0 	 global-step:18160	 l-p:0.11698003858327866
epoch£º908	 i:1 	 global-step:18161	 l-p:0.13542446494102478
epoch£º908	 i:2 	 global-step:18162	 l-p:0.11817064136266708
epoch£º908	 i:3 	 global-step:18163	 l-p:0.1288023144006729
epoch£º908	 i:4 	 global-step:18164	 l-p:0.09386076778173447
epoch£º908	 i:5 	 global-step:18165	 l-p:0.13506555557250977
epoch£º908	 i:6 	 global-step:18166	 l-p:0.1845209151506424
epoch£º908	 i:7 	 global-step:18167	 l-p:0.1299261599779129
epoch£º908	 i:8 	 global-step:18168	 l-p:0.13555346429347992
epoch£º908	 i:9 	 global-step:18169	 l-p:0.15090712904930115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:909
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1618, 5.1478, 5.1604],
        [5.1618, 5.1618, 5.1618],
        [5.1618, 5.0487, 4.7292],
        [5.1618, 5.1618, 5.1618]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:909, step:0 
model_pd.l_p.mean(): 0.13747094571590424 
model_pd.l_d.mean(): -20.24597930908203 
model_pd.lagr.mean(): -20.108509063720703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4459], device='cuda:0')), ('power', tensor([-21.0864], device='cuda:0'))])
epoch£º909	 i:0 	 global-step:18180	 l-p:0.13747094571590424
epoch£º909	 i:1 	 global-step:18181	 l-p:0.13294634222984314
epoch£º909	 i:2 	 global-step:18182	 l-p:0.11804067343473434
epoch£º909	 i:3 	 global-step:18183	 l-p:0.1074400320649147
epoch£º909	 i:4 	 global-step:18184	 l-p:0.1318007856607437
epoch£º909	 i:5 	 global-step:18185	 l-p:0.12249045819044113
epoch£º909	 i:6 	 global-step:18186	 l-p:0.21713604032993317
epoch£º909	 i:7 	 global-step:18187	 l-p:0.09379962086677551
epoch£º909	 i:8 	 global-step:18188	 l-p:0.1005922257900238
epoch£º909	 i:9 	 global-step:18189	 l-p:0.1500045210123062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:910
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1516, 5.1516, 5.1516],
        [5.1516, 5.1512, 5.1516],
        [5.1516, 5.1516, 5.1516],
        [5.1516, 4.9200, 4.6363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:910, step:0 
model_pd.l_p.mean(): 0.1792406439781189 
model_pd.l_d.mean(): -19.168672561645508 
model_pd.lagr.mean(): -18.989431381225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5365], device='cuda:0')), ('power', tensor([-20.0828], device='cuda:0'))])
epoch£º910	 i:0 	 global-step:18200	 l-p:0.1792406439781189
epoch£º910	 i:1 	 global-step:18201	 l-p:0.15340851247310638
epoch£º910	 i:2 	 global-step:18202	 l-p:0.06082819774746895
epoch£º910	 i:3 	 global-step:18203	 l-p:0.15023164451122284
epoch£º910	 i:4 	 global-step:18204	 l-p:0.16813784837722778
epoch£º910	 i:5 	 global-step:18205	 l-p:0.12454813718795776
epoch£º910	 i:6 	 global-step:18206	 l-p:0.11369787901639938
epoch£º910	 i:7 	 global-step:18207	 l-p:0.18771307170391083
epoch£º910	 i:8 	 global-step:18208	 l-p:0.1048610731959343
epoch£º910	 i:9 	 global-step:18209	 l-p:0.14325131475925446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:911
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1268, 4.9240, 4.9447],
        [5.1268, 5.1268, 5.1268],
        [5.1268, 4.9801, 4.6579],
        [5.1268, 4.8967, 4.8838]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:911, step:0 
model_pd.l_p.mean(): 0.15887445211410522 
model_pd.l_d.mean(): -20.57587432861328 
model_pd.lagr.mean(): -20.41699981689453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4355], device='cuda:0')), ('power', tensor([-21.4117], device='cuda:0'))])
epoch£º911	 i:0 	 global-step:18220	 l-p:0.15887445211410522
epoch£º911	 i:1 	 global-step:18221	 l-p:0.26644057035446167
epoch£º911	 i:2 	 global-step:18222	 l-p:0.13355128467082977
epoch£º911	 i:3 	 global-step:18223	 l-p:0.1835556924343109
epoch£º911	 i:4 	 global-step:18224	 l-p:0.13288535177707672
epoch£º911	 i:5 	 global-step:18225	 l-p:0.1514028012752533
epoch£º911	 i:6 	 global-step:18226	 l-p:0.09099657088518143
epoch£º911	 i:7 	 global-step:18227	 l-p:0.13116145133972168
epoch£º911	 i:8 	 global-step:18228	 l-p:0.11309799551963806
epoch£º911	 i:9 	 global-step:18229	 l-p:0.0947115495800972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:912
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1238, 5.2263, 4.9761],
        [5.1238, 5.0477, 5.0970],
        [5.1238, 4.8598, 4.6196],
        [5.1238, 5.1185, 5.1235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:912, step:0 
model_pd.l_p.mean(): 0.1557065099477768 
model_pd.l_d.mean(): -19.265775680541992 
model_pd.lagr.mean(): -19.110069274902344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5859], device='cuda:0')), ('power', tensor([-20.2329], device='cuda:0'))])
epoch£º912	 i:0 	 global-step:18240	 l-p:0.1557065099477768
epoch£º912	 i:1 	 global-step:18241	 l-p:0.11269906163215637
epoch£º912	 i:2 	 global-step:18242	 l-p:0.17098230123519897
epoch£º912	 i:3 	 global-step:18243	 l-p:0.17317984998226166
epoch£º912	 i:4 	 global-step:18244	 l-p:0.10762210935354233
epoch£º912	 i:5 	 global-step:18245	 l-p:0.1660722941160202
epoch£º912	 i:6 	 global-step:18246	 l-p:0.13943548500537872
epoch£º912	 i:7 	 global-step:18247	 l-p:0.13835197687149048
epoch£º912	 i:8 	 global-step:18248	 l-p:0.16237732768058777
epoch£º912	 i:9 	 global-step:18249	 l-p:0.12762439250946045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:913
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1279, 5.1279, 5.1279],
        [5.1279, 4.8644, 4.6242],
        [5.1279, 5.1274, 5.1279],
        [5.1279, 4.9091, 4.6096]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:913, step:0 
model_pd.l_p.mean(): 0.11001510173082352 
model_pd.l_d.mean(): -19.98954200744629 
model_pd.lagr.mean(): -19.879526138305664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5122], device='cuda:0')), ('power', tensor([-20.8938], device='cuda:0'))])
epoch£º913	 i:0 	 global-step:18260	 l-p:0.11001510173082352
epoch£º913	 i:1 	 global-step:18261	 l-p:0.11507934331893921
epoch£º913	 i:2 	 global-step:18262	 l-p:0.12669433653354645
epoch£º913	 i:3 	 global-step:18263	 l-p:0.141257181763649
epoch£º913	 i:4 	 global-step:18264	 l-p:0.1408216953277588
epoch£º913	 i:5 	 global-step:18265	 l-p:0.18143855035305023
epoch£º913	 i:6 	 global-step:18266	 l-p:0.17723669111728668
epoch£º913	 i:7 	 global-step:18267	 l-p:0.18467378616333008
epoch£º913	 i:8 	 global-step:18268	 l-p:0.2315479815006256
epoch£º913	 i:9 	 global-step:18269	 l-p:0.12257450073957443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:914
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1227, 4.8567, 4.7623],
        [5.1227, 5.1227, 5.1227],
        [5.1227, 5.0722, 5.1099],
        [5.1227, 4.8469, 4.6866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:914, step:0 
model_pd.l_p.mean(): 0.12818126380443573 
model_pd.l_d.mean(): -19.797626495361328 
model_pd.lagr.mean(): -19.669445037841797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4981], device='cuda:0')), ('power', tensor([-20.6838], device='cuda:0'))])
epoch£º914	 i:0 	 global-step:18280	 l-p:0.12818126380443573
epoch£º914	 i:1 	 global-step:18281	 l-p:0.10403463244438171
epoch£º914	 i:2 	 global-step:18282	 l-p:0.09267869591712952
epoch£º914	 i:3 	 global-step:18283	 l-p:0.16252505779266357
epoch£º914	 i:4 	 global-step:18284	 l-p:0.1400410681962967
epoch£º914	 i:5 	 global-step:18285	 l-p:0.16237962245941162
epoch£º914	 i:6 	 global-step:18286	 l-p:0.19041606783866882
epoch£º914	 i:7 	 global-step:18287	 l-p:0.1239454597234726
epoch£º914	 i:8 	 global-step:18288	 l-p:0.10364125669002533
epoch£º914	 i:9 	 global-step:18289	 l-p:0.16363565623760223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:915
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1696, 5.0946, 5.1434],
        [5.1696, 5.1674, 5.1695],
        [5.1696, 5.1696, 5.1696],
        [5.1696, 5.1651, 5.1694]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:915, step:0 
model_pd.l_p.mean(): 0.11137016117572784 
model_pd.l_d.mean(): -20.49152183532715 
model_pd.lagr.mean(): -20.380151748657227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4447], device='cuda:0')), ('power', tensor([-21.3352], device='cuda:0'))])
epoch£º915	 i:0 	 global-step:18300	 l-p:0.11137016117572784
epoch£º915	 i:1 	 global-step:18301	 l-p:0.12330945581197739
epoch£º915	 i:2 	 global-step:18302	 l-p:0.09075922518968582
epoch£º915	 i:3 	 global-step:18303	 l-p:0.13394451141357422
epoch£º915	 i:4 	 global-step:18304	 l-p:0.1869705319404602
epoch£º915	 i:5 	 global-step:18305	 l-p:0.10886146128177643
epoch£º915	 i:6 	 global-step:18306	 l-p:0.18513129651546478
epoch£º915	 i:7 	 global-step:18307	 l-p:0.10524757206439972
epoch£º915	 i:8 	 global-step:18308	 l-p:0.1157616600394249
epoch£º915	 i:9 	 global-step:18309	 l-p:0.12233626842498779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:916
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1804, 5.6520, 5.6247],
        [5.1804, 5.1804, 5.1804],
        [5.1804, 5.1548, 5.1764],
        [5.1804, 4.9479, 4.6689]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:916, step:0 
model_pd.l_p.mean(): 0.13418376445770264 
model_pd.l_d.mean(): -19.778366088867188 
model_pd.lagr.mean(): -19.644182205200195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4437], device='cuda:0')), ('power', tensor([-20.6078], device='cuda:0'))])
epoch£º916	 i:0 	 global-step:18320	 l-p:0.13418376445770264
epoch£º916	 i:1 	 global-step:18321	 l-p:0.10009439289569855
epoch£º916	 i:2 	 global-step:18322	 l-p:0.19208557903766632
epoch£º916	 i:3 	 global-step:18323	 l-p:0.10167520493268967
epoch£º916	 i:4 	 global-step:18324	 l-p:0.12250219285488129
epoch£º916	 i:5 	 global-step:18325	 l-p:0.10275260359048843
epoch£º916	 i:6 	 global-step:18326	 l-p:0.12142852693796158
epoch£º916	 i:7 	 global-step:18327	 l-p:0.15050144493579865
epoch£º916	 i:8 	 global-step:18328	 l-p:0.12776535749435425
epoch£º916	 i:9 	 global-step:18329	 l-p:0.12405393272638321
====================================================================================================
====================================================================================================
====================================================================================================

epoch:917
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1636, 5.1206, 5.1539],
        [5.1636, 5.1636, 5.1636],
        [5.1636, 5.4365, 5.2796],
        [5.1636, 5.0939, 5.1407]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:917, step:0 
model_pd.l_p.mean(): 0.12812720239162445 
model_pd.l_d.mean(): -20.748640060424805 
model_pd.lagr.mean(): -20.620512008666992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3837], device='cuda:0')), ('power', tensor([-21.5340], device='cuda:0'))])
epoch£º917	 i:0 	 global-step:18340	 l-p:0.12812720239162445
epoch£º917	 i:1 	 global-step:18341	 l-p:0.12882208824157715
epoch£º917	 i:2 	 global-step:18342	 l-p:0.12279445677995682
epoch£º917	 i:3 	 global-step:18343	 l-p:0.1452963948249817
epoch£º917	 i:4 	 global-step:18344	 l-p:0.15541638433933258
epoch£º917	 i:5 	 global-step:18345	 l-p:0.12147189676761627
epoch£º917	 i:6 	 global-step:18346	 l-p:0.16317620873451233
epoch£º917	 i:7 	 global-step:18347	 l-p:0.1890631765127182
epoch£º917	 i:8 	 global-step:18348	 l-p:0.14364320039749146
epoch£º917	 i:9 	 global-step:18349	 l-p:0.08793593943119049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:918
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1357, 5.1357, 5.1357],
        [5.1357, 5.1219, 5.1343],
        [5.1357, 5.1357, 5.1357],
        [5.1357, 4.9834, 4.6611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:918, step:0 
model_pd.l_p.mean(): 0.15806132555007935 
model_pd.l_d.mean(): -18.22947120666504 
model_pd.lagr.mean(): -18.071409225463867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6126], device='cuda:0')), ('power', tensor([-19.2048], device='cuda:0'))])
epoch£º918	 i:0 	 global-step:18360	 l-p:0.15806132555007935
epoch£º918	 i:1 	 global-step:18361	 l-p:0.15325620770454407
epoch£º918	 i:2 	 global-step:18362	 l-p:0.0754525437951088
epoch£º918	 i:3 	 global-step:18363	 l-p:0.154875710606575
epoch£º918	 i:4 	 global-step:18364	 l-p:0.16406913101673126
epoch£º918	 i:5 	 global-step:18365	 l-p:0.13228636980056763
epoch£º918	 i:6 	 global-step:18366	 l-p:0.13186685740947723
epoch£º918	 i:7 	 global-step:18367	 l-p:0.19332273304462433
epoch£º918	 i:8 	 global-step:18368	 l-p:0.108278788626194
epoch£º918	 i:9 	 global-step:18369	 l-p:0.12095251679420471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:919
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228]], device='cuda:0')
 pt:tensor([[5.1434, 5.5219, 5.4321],
        [5.1434, 5.0857, 4.7729],
        [5.1434, 5.1960, 4.9221],
        [5.1434, 4.9380, 4.9560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:919, step:0 
model_pd.l_p.mean(): 0.1402742862701416 
model_pd.l_d.mean(): -19.30909538269043 
model_pd.lagr.mean(): -19.168821334838867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5129], device='cuda:0')), ('power', tensor([-20.2014], device='cuda:0'))])
epoch£º919	 i:0 	 global-step:18380	 l-p:0.1402742862701416
epoch£º919	 i:1 	 global-step:18381	 l-p:0.13940684497356415
epoch£º919	 i:2 	 global-step:18382	 l-p:0.16319234669208527
epoch£º919	 i:3 	 global-step:18383	 l-p:0.11345352977514267
epoch£º919	 i:4 	 global-step:18384	 l-p:0.17623141407966614
epoch£º919	 i:5 	 global-step:18385	 l-p:0.14811383187770844
epoch£º919	 i:6 	 global-step:18386	 l-p:0.12988464534282684
epoch£º919	 i:7 	 global-step:18387	 l-p:0.13764503598213196
epoch£º919	 i:8 	 global-step:18388	 l-p:0.1190953403711319
epoch£º919	 i:9 	 global-step:18389	 l-p:0.13158972561359406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:920
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1391, 4.9207, 4.6209],
        [5.1391, 5.1391, 5.1391],
        [5.1391, 4.9287, 4.9418],
        [5.1391, 4.8893, 4.8402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:920, step:0 
model_pd.l_p.mean(): 0.15065163373947144 
model_pd.l_d.mean(): -19.720651626586914 
model_pd.lagr.mean(): -19.56999969482422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5093], device='cuda:0')), ('power', tensor([-20.6169], device='cuda:0'))])
epoch£º920	 i:0 	 global-step:18400	 l-p:0.15065163373947144
epoch£º920	 i:1 	 global-step:18401	 l-p:0.11229124665260315
epoch£º920	 i:2 	 global-step:18402	 l-p:0.14589174091815948
epoch£º920	 i:3 	 global-step:18403	 l-p:0.11557602882385254
epoch£º920	 i:4 	 global-step:18404	 l-p:0.11826733499765396
epoch£º920	 i:5 	 global-step:18405	 l-p:0.1711391806602478
epoch£º920	 i:6 	 global-step:18406	 l-p:0.14566335082054138
epoch£º920	 i:7 	 global-step:18407	 l-p:0.1093481034040451
epoch£º920	 i:8 	 global-step:18408	 l-p:0.2103991061449051
epoch£º920	 i:9 	 global-step:18409	 l-p:0.15655694901943207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:921
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1207, 5.1207, 5.1207],
        [5.1207, 4.9923, 4.6676],
        [5.1207, 5.0440, 5.0936],
        [5.1207, 5.1207, 5.1207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:921, step:0 
model_pd.l_p.mean(): 0.16309162974357605 
model_pd.l_d.mean(): -19.144126892089844 
model_pd.lagr.mean(): -18.981035232543945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5756], device='cuda:0')), ('power', tensor([-20.0982], device='cuda:0'))])
epoch£º921	 i:0 	 global-step:18420	 l-p:0.16309162974357605
epoch£º921	 i:1 	 global-step:18421	 l-p:0.1375616192817688
epoch£º921	 i:2 	 global-step:18422	 l-p:0.15458592772483826
epoch£º921	 i:3 	 global-step:18423	 l-p:0.1646580696105957
epoch£º921	 i:4 	 global-step:18424	 l-p:0.108863465487957
epoch£º921	 i:5 	 global-step:18425	 l-p:0.10965283960103989
epoch£º921	 i:6 	 global-step:18426	 l-p:0.12739484012126923
epoch£º921	 i:7 	 global-step:18427	 l-p:0.14557071030139923
epoch£º921	 i:8 	 global-step:18428	 l-p:0.16672660410404205
epoch£º921	 i:9 	 global-step:18429	 l-p:0.15451975166797638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:922
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1498, 5.1499, 5.1499],
        [5.1498, 4.9583, 4.9888],
        [5.1498, 5.4440, 5.3000],
        [5.1498, 5.2507, 4.9987]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:922, step:0 
model_pd.l_p.mean(): 0.17519310116767883 
model_pd.l_d.mean(): -20.366785049438477 
model_pd.lagr.mean(): -20.191591262817383 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4598], device='cuda:0')), ('power', tensor([-21.2239], device='cuda:0'))])
epoch£º922	 i:0 	 global-step:18440	 l-p:0.17519310116767883
epoch£º922	 i:1 	 global-step:18441	 l-p:0.1158481165766716
epoch£º922	 i:2 	 global-step:18442	 l-p:0.13625416159629822
epoch£º922	 i:3 	 global-step:18443	 l-p:0.12174364179372787
epoch£º922	 i:4 	 global-step:18444	 l-p:0.1164981797337532
epoch£º922	 i:5 	 global-step:18445	 l-p:0.10108134895563126
epoch£º922	 i:6 	 global-step:18446	 l-p:0.097662553191185
epoch£º922	 i:7 	 global-step:18447	 l-p:0.17496661841869354
epoch£º922	 i:8 	 global-step:18448	 l-p:0.20907722413539886
epoch£º922	 i:9 	 global-step:18449	 l-p:0.10172276943922043
====================================================================================================
====================================================================================================
====================================================================================================

epoch:923
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1562, 4.9267, 4.6377],
        [5.1562, 4.9204, 4.8974],
        [5.1562, 5.0054, 5.0576],
        [5.1562, 5.1537, 5.1561]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:923, step:0 
model_pd.l_p.mean(): 0.12335985153913498 
model_pd.l_d.mean(): -20.324359893798828 
model_pd.lagr.mean(): -20.201000213623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4439], device='cuda:0')), ('power', tensor([-21.1642], device='cuda:0'))])
epoch£º923	 i:0 	 global-step:18460	 l-p:0.12335985153913498
epoch£º923	 i:1 	 global-step:18461	 l-p:0.19086559116840363
epoch£º923	 i:2 	 global-step:18462	 l-p:0.13770532608032227
epoch£º923	 i:3 	 global-step:18463	 l-p:0.11597566306591034
epoch£º923	 i:4 	 global-step:18464	 l-p:0.13131563365459442
epoch£º923	 i:5 	 global-step:18465	 l-p:0.1267477422952652
epoch£º923	 i:6 	 global-step:18466	 l-p:0.10783682763576508
epoch£º923	 i:7 	 global-step:18467	 l-p:0.16239693760871887
epoch£º923	 i:8 	 global-step:18468	 l-p:0.1459110677242279
epoch£º923	 i:9 	 global-step:18469	 l-p:0.1027488112449646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:924
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1406, 4.9337, 4.9510],
        [5.1406, 4.9159, 4.6197],
        [5.1406, 5.0802, 5.1230],
        [5.1406, 4.9883, 5.0405]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:924, step:0 
model_pd.l_p.mean(): 0.12172765284776688 
model_pd.l_d.mean(): -20.0970458984375 
model_pd.lagr.mean(): -19.975318908691406 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4972], device='cuda:0')), ('power', tensor([-20.9878], device='cuda:0'))])
epoch£º924	 i:0 	 global-step:18480	 l-p:0.12172765284776688
epoch£º924	 i:1 	 global-step:18481	 l-p:0.19821697473526
epoch£º924	 i:2 	 global-step:18482	 l-p:0.1463507115840912
epoch£º924	 i:3 	 global-step:18483	 l-p:0.13668428361415863
epoch£º924	 i:4 	 global-step:18484	 l-p:0.12625378370285034
epoch£º924	 i:5 	 global-step:18485	 l-p:0.12635211646556854
epoch£º924	 i:6 	 global-step:18486	 l-p:0.24427440762519836
epoch£º924	 i:7 	 global-step:18487	 l-p:0.11403878778219223
epoch£º924	 i:8 	 global-step:18488	 l-p:0.09930232912302017
epoch£º924	 i:9 	 global-step:18489	 l-p:0.11787324398756027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:925
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1050, 5.1049, 5.1050],
        [5.1050, 5.0607, 4.7497],
        [5.1050, 4.9298, 4.6069],
        [5.1050, 4.8666, 4.5756]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:925, step:0 
model_pd.l_p.mean(): 0.1653701364994049 
model_pd.l_d.mean(): -20.304067611694336 
model_pd.lagr.mean(): -20.138696670532227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4640], device='cuda:0')), ('power', tensor([-21.1643], device='cuda:0'))])
epoch£º925	 i:0 	 global-step:18500	 l-p:0.1653701364994049
epoch£º925	 i:1 	 global-step:18501	 l-p:0.10444264858961105
epoch£º925	 i:2 	 global-step:18502	 l-p:0.12738987803459167
epoch£º925	 i:3 	 global-step:18503	 l-p:0.2818368375301361
epoch£º925	 i:4 	 global-step:18504	 l-p:0.11680112779140472
epoch£º925	 i:5 	 global-step:18505	 l-p:0.24116918444633484
epoch£º925	 i:6 	 global-step:18506	 l-p:0.20539310574531555
epoch£º925	 i:7 	 global-step:18507	 l-p:0.1992102563381195
epoch£º925	 i:8 	 global-step:18508	 l-p:0.1334323137998581
epoch£º925	 i:9 	 global-step:18509	 l-p:-0.013733197934925556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:926
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0644, 5.0644, 5.0644],
        [5.0644, 4.7834, 4.6535],
        [5.0644, 4.7814, 4.6370],
        [5.0644, 4.9496, 4.6219]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:926, step:0 
model_pd.l_p.mean(): 0.13612224161624908 
model_pd.l_d.mean(): -19.796823501586914 
model_pd.lagr.mean(): -19.660701751708984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4740], device='cuda:0')), ('power', tensor([-20.6579], device='cuda:0'))])
epoch£º926	 i:0 	 global-step:18520	 l-p:0.13612224161624908
epoch£º926	 i:1 	 global-step:18521	 l-p:-0.009609060361981392
epoch£º926	 i:2 	 global-step:18522	 l-p:0.1223052591085434
epoch£º926	 i:3 	 global-step:18523	 l-p:0.13963691890239716
epoch£º926	 i:4 	 global-step:18524	 l-p:0.11760564893484116
epoch£º926	 i:5 	 global-step:18525	 l-p:-0.7512542605400085
epoch£º926	 i:6 	 global-step:18526	 l-p:0.3684076964855194
epoch£º926	 i:7 	 global-step:18527	 l-p:0.11548513174057007
epoch£º926	 i:8 	 global-step:18528	 l-p:0.39129015803337097
epoch£º926	 i:9 	 global-step:18529	 l-p:0.14469577372074127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:927
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0584, 5.0577, 5.0584],
        [5.0584, 4.9815, 5.0316],
        [5.0584, 5.0542, 5.0582],
        [5.0584, 4.9690, 5.0231]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:927, step:0 
model_pd.l_p.mean(): 0.14484210312366486 
model_pd.l_d.mean(): -18.480310440063477 
model_pd.lagr.mean(): -18.335468292236328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5663], device='cuda:0')), ('power', tensor([-19.4124], device='cuda:0'))])
epoch£º927	 i:0 	 global-step:18540	 l-p:0.14484210312366486
epoch£º927	 i:1 	 global-step:18541	 l-p:0.3557088077068329
epoch£º927	 i:2 	 global-step:18542	 l-p:0.13255710899829865
epoch£º927	 i:3 	 global-step:18543	 l-p:0.14026089012622833
epoch£º927	 i:4 	 global-step:18544	 l-p:2.1425485610961914
epoch£º927	 i:5 	 global-step:18545	 l-p:0.12332018464803696
epoch£º927	 i:6 	 global-step:18546	 l-p:0.13715970516204834
epoch£º927	 i:7 	 global-step:18547	 l-p:0.0637989416718483
epoch£º927	 i:8 	 global-step:18548	 l-p:0.1281529814004898
epoch£º927	 i:9 	 global-step:18549	 l-p:0.21943576633930206
====================================================================================================
====================================================================================================
====================================================================================================

epoch:928
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0682, 5.0682, 5.0682],
        [5.0682, 4.9569, 5.0148],
        [5.0682, 5.0681, 5.0682],
        [5.0682, 5.0664, 5.0681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:928, step:0 
model_pd.l_p.mean(): 0.11491774022579193 
model_pd.l_d.mean(): -20.275390625 
model_pd.lagr.mean(): -20.160472869873047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4615], device='cuda:0')), ('power', tensor([-21.1325], device='cuda:0'))])
epoch£º928	 i:0 	 global-step:18560	 l-p:0.11491774022579193
epoch£º928	 i:1 	 global-step:18561	 l-p:0.13064758479595184
epoch£º928	 i:2 	 global-step:18562	 l-p:0.16618382930755615
epoch£º928	 i:3 	 global-step:18563	 l-p:0.14720752835273743
epoch£º928	 i:4 	 global-step:18564	 l-p:0.267813116312027
epoch£º928	 i:5 	 global-step:18565	 l-p:0.13445404171943665
epoch£º928	 i:6 	 global-step:18566	 l-p:0.3554556965827942
epoch£º928	 i:7 	 global-step:18567	 l-p:0.12281312048435211
epoch£º928	 i:8 	 global-step:18568	 l-p:0.19555339217185974
epoch£º928	 i:9 	 global-step:18569	 l-p:-0.23377180099487305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:929
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0821, 5.0821, 5.0821],
        [5.0821, 5.0324, 5.0699],
        [5.0821, 4.7989, 4.6257],
        [5.0821, 5.0821, 5.0821]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:929, step:0 
model_pd.l_p.mean(): 0.0987871065735817 
model_pd.l_d.mean(): -20.486644744873047 
model_pd.lagr.mean(): -20.38785743713379 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4507], device='cuda:0')), ('power', tensor([-21.3365], device='cuda:0'))])
epoch£º929	 i:0 	 global-step:18580	 l-p:0.0987871065735817
epoch£º929	 i:1 	 global-step:18581	 l-p:-0.9436857104301453
epoch£º929	 i:2 	 global-step:18582	 l-p:0.23207435011863708
epoch£º929	 i:3 	 global-step:18583	 l-p:0.13706746697425842
epoch£º929	 i:4 	 global-step:18584	 l-p:0.09899343550205231
epoch£º929	 i:5 	 global-step:18585	 l-p:0.13911239802837372
epoch£º929	 i:6 	 global-step:18586	 l-p:0.15932686626911163
epoch£º929	 i:7 	 global-step:18587	 l-p:0.13464100658893585
epoch£º929	 i:8 	 global-step:18588	 l-p:0.1927807778120041
epoch£º929	 i:9 	 global-step:18589	 l-p:0.13604481518268585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:930
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1005, 5.0922, 5.0999],
        [5.1005, 4.9439, 4.6171],
        [5.1005, 5.0802, 5.0979],
        [5.1005, 5.0993, 5.1005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:930, step:0 
model_pd.l_p.mean(): 0.10692470520734787 
model_pd.l_d.mean(): -20.399919509887695 
model_pd.lagr.mean(): -20.29299545288086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-21.2619], device='cuda:0'))])
epoch£º930	 i:0 	 global-step:18600	 l-p:0.10692470520734787
epoch£º930	 i:1 	 global-step:18601	 l-p:0.13929541409015656
epoch£º930	 i:2 	 global-step:18602	 l-p:0.1765713095664978
epoch£º930	 i:3 	 global-step:18603	 l-p:0.44319114089012146
epoch£º930	 i:4 	 global-step:18604	 l-p:0.21031932532787323
epoch£º930	 i:5 	 global-step:18605	 l-p:0.13705569505691528
epoch£º930	 i:6 	 global-step:18606	 l-p:0.17761443555355072
epoch£º930	 i:7 	 global-step:18607	 l-p:0.23620575666427612
epoch£º930	 i:8 	 global-step:18608	 l-p:0.09415831416845322
epoch£º930	 i:9 	 global-step:18609	 l-p:0.08347422629594803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:931
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1165, 5.1143, 5.1164],
        [5.1165, 4.8385, 4.6985],
        [5.1165, 5.1165, 5.1165],
        [5.1165, 5.0044, 5.0619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:931, step:0 
model_pd.l_p.mean(): 0.15800465643405914 
model_pd.l_d.mean(): -20.13135528564453 
model_pd.lagr.mean(): -19.973350524902344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4732], device='cuda:0')), ('power', tensor([-20.9979], device='cuda:0'))])
epoch£º931	 i:0 	 global-step:18620	 l-p:0.15800465643405914
epoch£º931	 i:1 	 global-step:18621	 l-p:0.1329231858253479
epoch£º931	 i:2 	 global-step:18622	 l-p:0.15421243011951447
epoch£º931	 i:3 	 global-step:18623	 l-p:0.1885634958744049
epoch£º931	 i:4 	 global-step:18624	 l-p:0.11993968486785889
epoch£º931	 i:5 	 global-step:18625	 l-p:0.13104848563671112
epoch£º931	 i:6 	 global-step:18626	 l-p:0.10752921551465988
epoch£º931	 i:7 	 global-step:18627	 l-p:0.14200390875339508
epoch£º931	 i:8 	 global-step:18628	 l-p:0.19334731996059418
epoch£º931	 i:9 	 global-step:18629	 l-p:0.11735200881958008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:932
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1453, 4.9852, 4.6623],
        [5.1453, 5.0598, 5.1123],
        [5.1453, 4.9184, 4.9122],
        [5.1453, 5.1313, 5.1439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:932, step:0 
model_pd.l_p.mean(): 0.1801857203245163 
model_pd.l_d.mean(): -18.891023635864258 
model_pd.lagr.mean(): -18.710838317871094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5664], device='cuda:0')), ('power', tensor([-19.8309], device='cuda:0'))])
epoch£º932	 i:0 	 global-step:18640	 l-p:0.1801857203245163
epoch£º932	 i:1 	 global-step:18641	 l-p:0.1564481258392334
epoch£º932	 i:2 	 global-step:18642	 l-p:0.12073121964931488
epoch£º932	 i:3 	 global-step:18643	 l-p:0.19250017404556274
epoch£º932	 i:4 	 global-step:18644	 l-p:0.08796781301498413
epoch£º932	 i:5 	 global-step:18645	 l-p:0.1032252386212349
epoch£º932	 i:6 	 global-step:18646	 l-p:0.12280955910682678
epoch£º932	 i:7 	 global-step:18647	 l-p:0.07919275015592575
epoch£º932	 i:8 	 global-step:18648	 l-p:0.1306297779083252
epoch£º932	 i:9 	 global-step:18649	 l-p:0.19134016335010529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:933
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1546, 5.0092, 4.6855],
        [5.1546, 4.9039, 4.8538],
        [5.1546, 4.8793, 4.6926],
        [5.1546, 5.1546, 5.1546]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:933, step:0 
model_pd.l_p.mean(): 0.1267937570810318 
model_pd.l_d.mean(): -20.442977905273438 
model_pd.lagr.mean(): -20.316184997558594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4508], device='cuda:0')), ('power', tensor([-21.2921], device='cuda:0'))])
epoch£º933	 i:0 	 global-step:18660	 l-p:0.1267937570810318
epoch£º933	 i:1 	 global-step:18661	 l-p:0.08564265817403793
epoch£º933	 i:2 	 global-step:18662	 l-p:0.1498057246208191
epoch£º933	 i:3 	 global-step:18663	 l-p:0.12478867918252945
epoch£º933	 i:4 	 global-step:18664	 l-p:0.15022076666355133
epoch£º933	 i:5 	 global-step:18665	 l-p:0.12063222378492355
epoch£º933	 i:6 	 global-step:18666	 l-p:0.19162894785404205
epoch£º933	 i:7 	 global-step:18667	 l-p:0.1715390980243683
epoch£º933	 i:8 	 global-step:18668	 l-p:0.10355232656002045
epoch£º933	 i:9 	 global-step:18669	 l-p:0.15369243919849396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:934
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1444, 5.1444, 5.1444],
        [5.1444, 5.1444, 5.1444],
        [5.1444, 4.9877, 5.0389],
        [5.1444, 5.0268, 5.0843]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:934, step:0 
model_pd.l_p.mean(): 0.18810594081878662 
model_pd.l_d.mean(): -19.89423370361328 
model_pd.lagr.mean(): -19.706127166748047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4905], device='cuda:0')), ('power', tensor([-20.7743], device='cuda:0'))])
epoch£º934	 i:0 	 global-step:18680	 l-p:0.18810594081878662
epoch£º934	 i:1 	 global-step:18681	 l-p:0.13108547031879425
epoch£º934	 i:2 	 global-step:18682	 l-p:0.09536555409431458
epoch£º934	 i:3 	 global-step:18683	 l-p:0.12036502361297607
epoch£º934	 i:4 	 global-step:18684	 l-p:0.1609780341386795
epoch£º934	 i:5 	 global-step:18685	 l-p:0.16184315085411072
epoch£º934	 i:6 	 global-step:18686	 l-p:0.1460547000169754
epoch£º934	 i:7 	 global-step:18687	 l-p:0.11816949397325516
epoch£º934	 i:8 	 global-step:18688	 l-p:0.13124807178974152
epoch£º934	 i:9 	 global-step:18689	 l-p:0.13494014739990234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:935
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1369, 4.9092, 4.6116],
        [5.1369, 5.1222, 5.1354],
        [5.1369, 5.1290, 5.1363],
        [5.1369, 4.9837, 5.0363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:935, step:0 
model_pd.l_p.mean(): 0.17443455755710602 
model_pd.l_d.mean(): -20.32205581665039 
model_pd.lagr.mean(): -20.147621154785156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4732], device='cuda:0')), ('power', tensor([-21.1922], device='cuda:0'))])
epoch£º935	 i:0 	 global-step:18700	 l-p:0.17443455755710602
epoch£º935	 i:1 	 global-step:18701	 l-p:0.12665720283985138
epoch£º935	 i:2 	 global-step:18702	 l-p:0.1462666541337967
epoch£º935	 i:3 	 global-step:18703	 l-p:0.15406692028045654
epoch£º935	 i:4 	 global-step:18704	 l-p:0.11191743612289429
epoch£º935	 i:5 	 global-step:18705	 l-p:0.13018451631069183
epoch£º935	 i:6 	 global-step:18706	 l-p:0.16005584597587585
epoch£º935	 i:7 	 global-step:18707	 l-p:0.13721902668476105
epoch£º935	 i:8 	 global-step:18708	 l-p:0.17477615177631378
epoch£º935	 i:9 	 global-step:18709	 l-p:0.13190360367298126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:936
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1342, 5.0222, 5.0796],
        [5.1342, 5.1007, 5.1280],
        [5.1342, 4.8634, 4.7597],
        [5.1342, 5.2664, 5.0292]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:936, step:0 
model_pd.l_p.mean(): 0.1656862199306488 
model_pd.l_d.mean(): -19.604631423950195 
model_pd.lagr.mean(): -19.438945770263672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4872], device='cuda:0')), ('power', tensor([-20.4758], device='cuda:0'))])
epoch£º936	 i:0 	 global-step:18720	 l-p:0.1656862199306488
epoch£º936	 i:1 	 global-step:18721	 l-p:0.134474977850914
epoch£º936	 i:2 	 global-step:18722	 l-p:0.19537311792373657
epoch£º936	 i:3 	 global-step:18723	 l-p:0.13687704503536224
epoch£º936	 i:4 	 global-step:18724	 l-p:0.13495999574661255
epoch£º936	 i:5 	 global-step:18725	 l-p:0.1347752958536148
epoch£º936	 i:6 	 global-step:18726	 l-p:0.1171003207564354
epoch£º936	 i:7 	 global-step:18727	 l-p:0.1219947561621666
epoch£º936	 i:8 	 global-step:18728	 l-p:0.12126187980175018
epoch£º936	 i:9 	 global-step:18729	 l-p:0.11375606060028076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:937
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1621, 4.9070, 4.8465],
        [5.1621, 5.1776, 4.8868],
        [5.1621, 4.9214, 4.8921],
        [5.1621, 5.1617, 5.1621]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:937, step:0 
model_pd.l_p.mean(): 0.08378320187330246 
model_pd.l_d.mean(): -20.572599411010742 
model_pd.lagr.mean(): -20.488815307617188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4239], device='cuda:0')), ('power', tensor([-21.3964], device='cuda:0'))])
epoch£º937	 i:0 	 global-step:18740	 l-p:0.08378320187330246
epoch£º937	 i:1 	 global-step:18741	 l-p:0.11860902607440948
epoch£º937	 i:2 	 global-step:18742	 l-p:0.13499535620212555
epoch£º937	 i:3 	 global-step:18743	 l-p:0.1320882886648178
epoch£º937	 i:4 	 global-step:18744	 l-p:0.11802227050065994
epoch£º937	 i:5 	 global-step:18745	 l-p:0.12446453422307968
epoch£º937	 i:6 	 global-step:18746	 l-p:0.15185923874378204
epoch£º937	 i:7 	 global-step:18747	 l-p:0.20631450414657593
epoch£º937	 i:8 	 global-step:18748	 l-p:0.11977370083332062
epoch£º937	 i:9 	 global-step:18749	 l-p:0.1515621393918991
====================================================================================================
====================================================================================================
====================================================================================================

epoch:938
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1610, 5.1610, 5.1610],
        [5.1610, 5.1304, 5.1557],
        [5.1610, 4.8850, 4.7260],
        [5.1610, 5.0297, 5.0865]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:938, step:0 
model_pd.l_p.mean(): 0.13015468418598175 
model_pd.l_d.mean(): -19.280532836914062 
model_pd.lagr.mean(): -19.15037727355957 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4791], device='cuda:0')), ('power', tensor([-20.1373], device='cuda:0'))])
epoch£º938	 i:0 	 global-step:18760	 l-p:0.13015468418598175
epoch£º938	 i:1 	 global-step:18761	 l-p:0.1597619205713272
epoch£º938	 i:2 	 global-step:18762	 l-p:0.16265596449375153
epoch£º938	 i:3 	 global-step:18763	 l-p:0.09468265622854233
epoch£º938	 i:4 	 global-step:18764	 l-p:0.15107685327529907
epoch£º938	 i:5 	 global-step:18765	 l-p:0.15927723050117493
epoch£º938	 i:6 	 global-step:18766	 l-p:0.11418561637401581
epoch£º938	 i:7 	 global-step:18767	 l-p:0.03907230123877525
epoch£º938	 i:8 	 global-step:18768	 l-p:0.117024265229702
epoch£º938	 i:9 	 global-step:18769	 l-p:0.17325901985168457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:939
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6033,  0.5098,  1.0000,  0.4308,
          1.0000,  0.8450, 31.6228]], device='cuda:0')
 pt:tensor([[5.1735, 5.4276, 5.2574],
        [5.1735, 4.9676, 4.6605],
        [5.1735, 4.9007, 4.7676],
        [5.1735, 5.0801, 4.7590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:939, step:0 
model_pd.l_p.mean(): 0.07568898051977158 
model_pd.l_d.mean(): -19.94768524169922 
model_pd.lagr.mean(): -19.87199592590332 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4838], device='cuda:0')), ('power', tensor([-20.8218], device='cuda:0'))])
epoch£º939	 i:0 	 global-step:18780	 l-p:0.07568898051977158
epoch£º939	 i:1 	 global-step:18781	 l-p:0.11073610186576843
epoch£º939	 i:2 	 global-step:18782	 l-p:0.12203461676836014
epoch£º939	 i:3 	 global-step:18783	 l-p:0.15805819630622864
epoch£º939	 i:4 	 global-step:18784	 l-p:0.07926664501428604
epoch£º939	 i:5 	 global-step:18785	 l-p:0.17858965694904327
epoch£º939	 i:6 	 global-step:18786	 l-p:0.1125132367014885
epoch£º939	 i:7 	 global-step:18787	 l-p:0.1583176553249359
epoch£º939	 i:8 	 global-step:18788	 l-p:0.16897670924663544
epoch£º939	 i:9 	 global-step:18789	 l-p:0.1270686686038971
====================================================================================================
====================================================================================================
====================================================================================================

epoch:940
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1673, 5.1662, 5.1673],
        [5.1673, 4.9601, 4.6526],
        [5.1673, 5.1526, 5.1658],
        [5.1673, 5.0909, 5.1404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:940, step:0 
model_pd.l_p.mean(): 0.13230937719345093 
model_pd.l_d.mean(): -19.48872947692871 
model_pd.lagr.mean(): -19.356420516967773 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5359], device='cuda:0')), ('power', tensor([-20.4082], device='cuda:0'))])
epoch£º940	 i:0 	 global-step:18800	 l-p:0.13230937719345093
epoch£º940	 i:1 	 global-step:18801	 l-p:0.08147915452718735
epoch£º940	 i:2 	 global-step:18802	 l-p:0.1464342623949051
epoch£º940	 i:3 	 global-step:18803	 l-p:0.1112048402428627
epoch£º940	 i:4 	 global-step:18804	 l-p:0.16810396313667297
epoch£º940	 i:5 	 global-step:18805	 l-p:0.10850396752357483
epoch£º940	 i:6 	 global-step:18806	 l-p:0.16089005768299103
epoch£º940	 i:7 	 global-step:18807	 l-p:0.16801200807094574
epoch£º940	 i:8 	 global-step:18808	 l-p:0.08271520584821701
epoch£º940	 i:9 	 global-step:18809	 l-p:0.16355769336223602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:941
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1717, 5.0278, 4.7041],
        [5.1717, 5.1715, 5.1717],
        [5.1717, 4.9097, 4.8287],
        [5.1717, 4.9217, 4.6541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:941, step:0 
model_pd.l_p.mean(): 0.14245139062404633 
model_pd.l_d.mean(): -19.389760971069336 
model_pd.lagr.mean(): -19.2473087310791 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4954], device='cuda:0')), ('power', tensor([-20.2655], device='cuda:0'))])
epoch£º941	 i:0 	 global-step:18820	 l-p:0.14245139062404633
epoch£º941	 i:1 	 global-step:18821	 l-p:0.1295333057641983
epoch£º941	 i:2 	 global-step:18822	 l-p:0.14890705049037933
epoch£º941	 i:3 	 global-step:18823	 l-p:0.09421920776367188
epoch£º941	 i:4 	 global-step:18824	 l-p:0.15041494369506836
epoch£º941	 i:5 	 global-step:18825	 l-p:0.16271530091762543
epoch£º941	 i:6 	 global-step:18826	 l-p:0.10943052172660828
epoch£º941	 i:7 	 global-step:18827	 l-p:0.1280832439661026
epoch£º941	 i:8 	 global-step:18828	 l-p:0.08086209744215012
epoch£º941	 i:9 	 global-step:18829	 l-p:0.11918184161186218
====================================================================================================
====================================================================================================
====================================================================================================

epoch:942
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1747, 4.9780, 5.0048],
        [5.1747, 5.5788, 5.5033],
        [5.1747, 4.9520, 4.6550],
        [5.1747, 5.1694, 5.1744]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:942, step:0 
model_pd.l_p.mean(): 0.12104389071464539 
model_pd.l_d.mean(): -20.16405487060547 
model_pd.lagr.mean(): -20.043010711669922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4392], device='cuda:0')), ('power', tensor([-20.9960], device='cuda:0'))])
epoch£º942	 i:0 	 global-step:18840	 l-p:0.12104389071464539
epoch£º942	 i:1 	 global-step:18841	 l-p:0.1541951596736908
epoch£º942	 i:2 	 global-step:18842	 l-p:0.12355620414018631
epoch£º942	 i:3 	 global-step:18843	 l-p:0.1711946278810501
epoch£º942	 i:4 	 global-step:18844	 l-p:0.1308612823486328
epoch£º942	 i:5 	 global-step:18845	 l-p:0.11586347222328186
epoch£º942	 i:6 	 global-step:18846	 l-p:0.12571559846401215
epoch£º942	 i:7 	 global-step:18847	 l-p:0.12375794351100922
epoch£º942	 i:8 	 global-step:18848	 l-p:0.1781253069639206
epoch£º942	 i:9 	 global-step:18849	 l-p:0.05646848678588867
====================================================================================================
====================================================================================================
====================================================================================================

epoch:943
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1757, 5.1757, 5.1757],
        [5.1757, 4.9136, 4.8323],
        [5.1757, 5.1757, 5.1757],
        [5.1757, 5.1757, 5.1757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:943, step:0 
model_pd.l_p.mean(): 0.12102383375167847 
model_pd.l_d.mean(): -20.22064971923828 
model_pd.lagr.mean(): -20.099626541137695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4168], device='cuda:0')), ('power', tensor([-21.0304], device='cuda:0'))])
epoch£º943	 i:0 	 global-step:18860	 l-p:0.12102383375167847
epoch£º943	 i:1 	 global-step:18861	 l-p:0.13680630922317505
epoch£º943	 i:2 	 global-step:18862	 l-p:0.13341327011585236
epoch£º943	 i:3 	 global-step:18863	 l-p:0.09400518238544464
epoch£º943	 i:4 	 global-step:18864	 l-p:0.14180928468704224
epoch£º943	 i:5 	 global-step:18865	 l-p:0.0867776945233345
epoch£º943	 i:6 	 global-step:18866	 l-p:0.15496771037578583
epoch£º943	 i:7 	 global-step:18867	 l-p:0.12977208197116852
epoch£º943	 i:8 	 global-step:18868	 l-p:0.11662529408931732
epoch£º943	 i:9 	 global-step:18869	 l-p:0.1608704775571823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:944
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6128,  0.5205,  1.0000,  0.4421,
          1.0000,  0.8494, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1755,  0.0983,  1.0000,  0.0550,
          1.0000,  0.5599, 31.6228]], device='cuda:0')
 pt:tensor([[5.1869, 4.9314, 4.6758],
        [5.1869, 5.1066, 4.7875],
        [5.1869, 4.9419, 4.9023],
        [5.1869, 5.0008, 5.0357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:944, step:0 
model_pd.l_p.mean(): 0.11346232146024704 
model_pd.l_d.mean(): -20.814376831054688 
model_pd.lagr.mean(): -20.70091438293457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3745], device='cuda:0')), ('power', tensor([-21.5915], device='cuda:0'))])
epoch£º944	 i:0 	 global-step:18880	 l-p:0.11346232146024704
epoch£º944	 i:1 	 global-step:18881	 l-p:0.10407409071922302
epoch£º944	 i:2 	 global-step:18882	 l-p:0.1194123849272728
epoch£º944	 i:3 	 global-step:18883	 l-p:0.1417585164308548
epoch£º944	 i:4 	 global-step:18884	 l-p:0.11509447544813156
epoch£º944	 i:5 	 global-step:18885	 l-p:0.11132171750068665
epoch£º944	 i:6 	 global-step:18886	 l-p:0.1428155153989792
epoch£º944	 i:7 	 global-step:18887	 l-p:0.15815326571464539
epoch£º944	 i:8 	 global-step:18888	 l-p:0.11717583984136581
epoch£º944	 i:9 	 global-step:18889	 l-p:0.1300702691078186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:945
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1833, 5.0465, 4.7226],
        [5.1833, 5.1791, 5.1831],
        [5.1833, 5.0869, 5.1419],
        [5.1833, 5.1833, 5.1833]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:945, step:0 
model_pd.l_p.mean(): 0.09810840338468552 
model_pd.l_d.mean(): -19.480361938476562 
model_pd.lagr.mean(): -19.382253646850586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4490], device='cuda:0')), ('power', tensor([-20.3097], device='cuda:0'))])
epoch£º945	 i:0 	 global-step:18900	 l-p:0.09810840338468552
epoch£º945	 i:1 	 global-step:18901	 l-p:0.12335097789764404
epoch£º945	 i:2 	 global-step:18902	 l-p:0.1257830411195755
epoch£º945	 i:3 	 global-step:18903	 l-p:0.06321850419044495
epoch£º945	 i:4 	 global-step:18904	 l-p:0.11272281408309937
epoch£º945	 i:5 	 global-step:18905	 l-p:0.15035845339298248
epoch£º945	 i:6 	 global-step:18906	 l-p:0.10301455110311508
epoch£º945	 i:7 	 global-step:18907	 l-p:0.1385716050863266
epoch£º945	 i:8 	 global-step:18908	 l-p:0.15431304275989532
epoch£º945	 i:9 	 global-step:18909	 l-p:0.26838046312332153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:946
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1532, 5.0096, 4.6832],
        [5.1532, 5.1354, 4.8313],
        [5.1532, 5.1532, 5.1532],
        [5.1532, 5.5940, 5.5433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:946, step:0 
model_pd.l_p.mean(): 0.20352788269519806 
model_pd.l_d.mean(): -19.534164428710938 
model_pd.lagr.mean(): -19.330636978149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5290], device='cuda:0')), ('power', tensor([-20.4474], device='cuda:0'))])
epoch£º946	 i:0 	 global-step:18920	 l-p:0.20352788269519806
epoch£º946	 i:1 	 global-step:18921	 l-p:0.12218502908945084
epoch£º946	 i:2 	 global-step:18922	 l-p:0.12365397810935974
epoch£º946	 i:3 	 global-step:18923	 l-p:0.09845728427171707
epoch£º946	 i:4 	 global-step:18924	 l-p:0.12777869403362274
epoch£º946	 i:5 	 global-step:18925	 l-p:0.1292421817779541
epoch£º946	 i:6 	 global-step:18926	 l-p:0.191059872508049
epoch£º946	 i:7 	 global-step:18927	 l-p:0.09630272537469864
epoch£º946	 i:8 	 global-step:18928	 l-p:0.1393313705921173
epoch£º946	 i:9 	 global-step:18929	 l-p:0.11969663947820663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:947
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1595, 5.1459, 5.1582],
        [5.1595, 5.1574, 5.1595],
        [5.1595, 5.1595, 5.1595],
        [5.1595, 5.1596, 5.1595]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:947, step:0 
model_pd.l_p.mean(): 0.1907755434513092 
model_pd.l_d.mean(): -20.67382049560547 
model_pd.lagr.mean(): -20.48304557800293 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4156], device='cuda:0')), ('power', tensor([-21.4909], device='cuda:0'))])
epoch£º947	 i:0 	 global-step:18940	 l-p:0.1907755434513092
epoch£º947	 i:1 	 global-step:18941	 l-p:0.15286710858345032
epoch£º947	 i:2 	 global-step:18942	 l-p:0.1267913579940796
epoch£º947	 i:3 	 global-step:18943	 l-p:0.11990734189748764
epoch£º947	 i:4 	 global-step:18944	 l-p:0.09654457867145538
epoch£º947	 i:5 	 global-step:18945	 l-p:0.14648854732513428
epoch£º947	 i:6 	 global-step:18946	 l-p:0.13812482357025146
epoch£º947	 i:7 	 global-step:18947	 l-p:0.09445855766534805
epoch£º947	 i:8 	 global-step:18948	 l-p:0.14087353646755219
epoch£º947	 i:9 	 global-step:18949	 l-p:0.16404758393764496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:948
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1498, 5.1738, 4.8849],
        [5.1498, 5.0747, 5.1240],
        [5.1498, 5.1497, 5.1498],
        [5.1498, 5.0138, 5.0707]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:948, step:0 
model_pd.l_p.mean(): 0.11621946096420288 
model_pd.l_d.mean(): -19.1920166015625 
model_pd.lagr.mean(): -19.07579803466797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5781], device='cuda:0')), ('power', tensor([-20.1496], device='cuda:0'))])
epoch£º948	 i:0 	 global-step:18960	 l-p:0.11621946096420288
epoch£º948	 i:1 	 global-step:18961	 l-p:0.11291196942329407
epoch£º948	 i:2 	 global-step:18962	 l-p:0.11732854694128036
epoch£º948	 i:3 	 global-step:18963	 l-p:0.14602446556091309
epoch£º948	 i:4 	 global-step:18964	 l-p:0.14481288194656372
epoch£º948	 i:5 	 global-step:18965	 l-p:0.16811706125736237
epoch£º948	 i:6 	 global-step:18966	 l-p:0.15846896171569824
epoch£º948	 i:7 	 global-step:18967	 l-p:0.06739537417888641
epoch£º948	 i:8 	 global-step:18968	 l-p:0.14122384786605835
epoch£º948	 i:9 	 global-step:18969	 l-p:0.1931431144475937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:949
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1490, 5.1479, 5.1490],
        [5.1490, 5.0296, 5.0876],
        [5.1490, 5.0717, 5.1218],
        [5.1490, 5.0942, 5.1344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:949, step:0 
model_pd.l_p.mean(): 0.1254780888557434 
model_pd.l_d.mean(): -20.618732452392578 
model_pd.lagr.mean(): -20.493253707885742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4046], device='cuda:0')), ('power', tensor([-21.4234], device='cuda:0'))])
epoch£º949	 i:0 	 global-step:18980	 l-p:0.1254780888557434
epoch£º949	 i:1 	 global-step:18981	 l-p:0.1002298891544342
epoch£º949	 i:2 	 global-step:18982	 l-p:0.15259850025177002
epoch£º949	 i:3 	 global-step:18983	 l-p:0.14817175269126892
epoch£º949	 i:4 	 global-step:18984	 l-p:0.1993836611509323
epoch£º949	 i:5 	 global-step:18985	 l-p:0.17792002856731415
epoch£º949	 i:6 	 global-step:18986	 l-p:0.12723875045776367
epoch£º949	 i:7 	 global-step:18987	 l-p:0.08350234478712082
epoch£º949	 i:8 	 global-step:18988	 l-p:0.1502407044172287
epoch£º949	 i:9 	 global-step:18989	 l-p:0.12316837161779404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:950
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1580, 5.1500, 5.1575],
        [5.1580, 4.8791, 4.7195],
        [5.1580, 4.8888, 4.6509],
        [5.1580, 5.1254, 5.1521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:950, step:0 
model_pd.l_p.mean(): 0.11594180762767792 
model_pd.l_d.mean(): -19.573991775512695 
model_pd.lagr.mean(): -19.458049774169922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4992], device='cuda:0')), ('power', tensor([-20.4570], device='cuda:0'))])
epoch£º950	 i:0 	 global-step:19000	 l-p:0.11594180762767792
epoch£º950	 i:1 	 global-step:19001	 l-p:0.19336624443531036
epoch£º950	 i:2 	 global-step:19002	 l-p:0.15466001629829407
epoch£º950	 i:3 	 global-step:19003	 l-p:0.13614527881145477
epoch£º950	 i:4 	 global-step:19004	 l-p:0.11271234601736069
epoch£º950	 i:5 	 global-step:19005	 l-p:0.1491381675004959
epoch£º950	 i:6 	 global-step:19006	 l-p:0.16901078820228577
epoch£º950	 i:7 	 global-step:19007	 l-p:0.11316848546266556
epoch£º950	 i:8 	 global-step:19008	 l-p:0.13507302105426788
epoch£º950	 i:9 	 global-step:19009	 l-p:0.09208469837903976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:951
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1444, 4.9673, 5.0105],
        [5.1444, 5.1444, 5.1444],
        [5.1444, 5.1444, 5.1444],
        [5.1444, 4.8638, 4.7043]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:951, step:0 
model_pd.l_p.mean(): 0.2164859026670456 
model_pd.l_d.mean(): -18.9123477935791 
model_pd.lagr.mean(): -18.69586181640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5323], device='cuda:0')), ('power', tensor([-19.8173], device='cuda:0'))])
epoch£º951	 i:0 	 global-step:19020	 l-p:0.2164859026670456
epoch£º951	 i:1 	 global-step:19021	 l-p:0.13854151964187622
epoch£º951	 i:2 	 global-step:19022	 l-p:0.13168996572494507
epoch£º951	 i:3 	 global-step:19023	 l-p:0.08858796954154968
epoch£º951	 i:4 	 global-step:19024	 l-p:0.1282111555337906
epoch£º951	 i:5 	 global-step:19025	 l-p:0.15922139585018158
epoch£º951	 i:6 	 global-step:19026	 l-p:0.16267572343349457
epoch£º951	 i:7 	 global-step:19027	 l-p:0.13865871727466583
epoch£º951	 i:8 	 global-step:19028	 l-p:0.11223266273736954
epoch£º951	 i:9 	 global-step:19029	 l-p:0.12579703330993652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:952
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1419, 5.1358, 5.1416],
        [5.1419, 4.9879, 4.6600],
        [5.1419, 5.1000, 5.1329],
        [5.1419, 5.1185, 5.1386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:952, step:0 
model_pd.l_p.mean(): 0.11865470558404922 
model_pd.l_d.mean(): -20.186540603637695 
model_pd.lagr.mean(): -20.067886352539062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4616], device='cuda:0')), ('power', tensor([-21.0421], device='cuda:0'))])
epoch£º952	 i:0 	 global-step:19040	 l-p:0.11865470558404922
epoch£º952	 i:1 	 global-step:19041	 l-p:0.14978666603565216
epoch£º952	 i:2 	 global-step:19042	 l-p:0.11418662965297699
epoch£º952	 i:3 	 global-step:19043	 l-p:0.16092723608016968
epoch£º952	 i:4 	 global-step:19044	 l-p:0.18656402826309204
epoch£º952	 i:5 	 global-step:19045	 l-p:0.13915513455867767
epoch£º952	 i:6 	 global-step:19046	 l-p:0.11769271641969681
epoch£º952	 i:7 	 global-step:19047	 l-p:0.12708386778831482
epoch£º952	 i:8 	 global-step:19048	 l-p:0.13262826204299927
epoch£º952	 i:9 	 global-step:19049	 l-p:0.1912815123796463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:953
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1264, 4.8437, 4.6521],
        [5.1264, 5.1263, 5.1264],
        [5.1264, 5.1263, 5.1264],
        [5.1264, 5.1225, 5.1263]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:953, step:0 
model_pd.l_p.mean(): 0.101494699716568 
model_pd.l_d.mean(): -20.28809356689453 
model_pd.lagr.mean(): -20.186599731445312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4785], device='cuda:0')), ('power', tensor([-21.1631], device='cuda:0'))])
epoch£º953	 i:0 	 global-step:19060	 l-p:0.101494699716568
epoch£º953	 i:1 	 global-step:19061	 l-p:0.22499710321426392
epoch£º953	 i:2 	 global-step:19062	 l-p:0.17144404351711273
epoch£º953	 i:3 	 global-step:19063	 l-p:0.17940214276313782
epoch£º953	 i:4 	 global-step:19064	 l-p:0.15844666957855225
epoch£º953	 i:5 	 global-step:19065	 l-p:0.13374119997024536
epoch£º953	 i:6 	 global-step:19066	 l-p:0.09868238866329193
epoch£º953	 i:7 	 global-step:19067	 l-p:0.10899034142494202
epoch£º953	 i:8 	 global-step:19068	 l-p:0.20866888761520386
epoch£º953	 i:9 	 global-step:19069	 l-p:0.16702042520046234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:954
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1104, 5.0420, 5.0888],
        [5.1104, 5.4786, 5.3797],
        [5.1104, 5.1074, 5.1103],
        [5.1104, 4.9200, 4.9566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:954, step:0 
model_pd.l_p.mean(): 0.14557328820228577 
model_pd.l_d.mean(): -19.68390655517578 
model_pd.lagr.mean(): -19.538333892822266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5619], device='cuda:0')), ('power', tensor([-20.6339], device='cuda:0'))])
epoch£º954	 i:0 	 global-step:19080	 l-p:0.14557328820228577
epoch£º954	 i:1 	 global-step:19081	 l-p:0.11042112112045288
epoch£º954	 i:2 	 global-step:19082	 l-p:0.300068199634552
epoch£º954	 i:3 	 global-step:19083	 l-p:0.21312369406223297
epoch£º954	 i:4 	 global-step:19084	 l-p:0.09756670892238617
epoch£º954	 i:5 	 global-step:19085	 l-p:0.12320714443922043
epoch£º954	 i:6 	 global-step:19086	 l-p:0.24768076837062836
epoch£º954	 i:7 	 global-step:19087	 l-p:0.1051843985915184
epoch£º954	 i:8 	 global-step:19088	 l-p:0.16223883628845215
epoch£º954	 i:9 	 global-step:19089	 l-p:0.1295325607061386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:955
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1101, 5.1101, 5.1101],
        [5.1101, 5.4779, 5.3787],
        [5.1101, 5.0306, 5.0817],
        [5.1101, 5.0681, 5.1010]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:955, step:0 
model_pd.l_p.mean(): 0.09594765305519104 
model_pd.l_d.mean(): -20.03670883178711 
model_pd.lagr.mean(): -19.94076156616211 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5155], device='cuda:0')), ('power', tensor([-20.9453], device='cuda:0'))])
epoch£º955	 i:0 	 global-step:19100	 l-p:0.09594765305519104
epoch£º955	 i:1 	 global-step:19101	 l-p:0.116619773209095
epoch£º955	 i:2 	 global-step:19102	 l-p:0.09413924813270569
epoch£º955	 i:3 	 global-step:19103	 l-p:2.7479400634765625
epoch£º955	 i:4 	 global-step:19104	 l-p:0.1999698281288147
epoch£º955	 i:5 	 global-step:19105	 l-p:0.13225753605365753
epoch£º955	 i:6 	 global-step:19106	 l-p:0.18441836535930634
epoch£º955	 i:7 	 global-step:19107	 l-p:0.19527751207351685
epoch£º955	 i:8 	 global-step:19108	 l-p:0.15106727182865143
epoch£º955	 i:9 	 global-step:19109	 l-p:0.4550999402999878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:956
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0799, 5.0408, 4.7274],
        [5.0799, 5.0792, 5.0799],
        [5.0799, 5.0788, 5.0799],
        [5.0799, 4.7916, 4.6321]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:956, step:0 
model_pd.l_p.mean(): 0.1483478993177414 
model_pd.l_d.mean(): -20.829490661621094 
model_pd.lagr.mean(): -20.681142807006836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4171], device='cuda:0')), ('power', tensor([-21.6509], device='cuda:0'))])
epoch£º956	 i:0 	 global-step:19120	 l-p:0.1483478993177414
epoch£º956	 i:1 	 global-step:19121	 l-p:0.1598944216966629
epoch£º956	 i:2 	 global-step:19122	 l-p:0.12283840030431747
epoch£º956	 i:3 	 global-step:19123	 l-p:0.18080569803714752
epoch£º956	 i:4 	 global-step:19124	 l-p:0.12548421323299408
epoch£º956	 i:5 	 global-step:19125	 l-p:0.15948918461799622
epoch£º956	 i:6 	 global-step:19126	 l-p:0.4341069757938385
epoch£º956	 i:7 	 global-step:19127	 l-p:0.1260099560022354
epoch£º956	 i:8 	 global-step:19128	 l-p:-0.17077793180942535
epoch£º956	 i:9 	 global-step:19129	 l-p:0.21761618554592133
====================================================================================================
====================================================================================================
====================================================================================================

epoch:957
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0935, 5.0932, 5.0935],
        [5.0935, 4.9140, 4.9583],
        [5.0935, 5.0716, 5.0906],
        [5.0935, 5.0699, 5.0902]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:957, step:0 
model_pd.l_p.mean(): 0.16217727959156036 
model_pd.l_d.mean(): -20.562519073486328 
model_pd.lagr.mean(): -20.400341033935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4536], device='cuda:0')), ('power', tensor([-21.4168], device='cuda:0'))])
epoch£º957	 i:0 	 global-step:19140	 l-p:0.16217727959156036
epoch£º957	 i:1 	 global-step:19141	 l-p:0.1347418874502182
epoch£º957	 i:2 	 global-step:19142	 l-p:0.20341968536376953
epoch£º957	 i:3 	 global-step:19143	 l-p:0.11148852854967117
epoch£º957	 i:4 	 global-step:19144	 l-p:0.0959736630320549
epoch£º957	 i:5 	 global-step:19145	 l-p:0.11927054822444916
epoch£º957	 i:6 	 global-step:19146	 l-p:0.15317745506763458
epoch£º957	 i:7 	 global-step:19147	 l-p:0.20830032229423523
epoch£º957	 i:8 	 global-step:19148	 l-p:0.2951935827732086
epoch£º957	 i:9 	 global-step:19149	 l-p:0.19428488612174988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:958
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1170, 5.0934, 5.1136],
        [5.1170, 5.0454, 5.0935],
        [5.1170, 5.0376, 5.0886],
        [5.1170, 5.0080, 4.6789]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:958, step:0 
model_pd.l_p.mean(): 0.1388522833585739 
model_pd.l_d.mean(): -20.679834365844727 
model_pd.lagr.mean(): -20.54098129272461 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4164], device='cuda:0')), ('power', tensor([-21.4978], device='cuda:0'))])
epoch£º958	 i:0 	 global-step:19160	 l-p:0.1388522833585739
epoch£º958	 i:1 	 global-step:19161	 l-p:0.23923629522323608
epoch£º958	 i:2 	 global-step:19162	 l-p:0.12275730073451996
epoch£º958	 i:3 	 global-step:19163	 l-p:0.17142662405967712
epoch£º958	 i:4 	 global-step:19164	 l-p:0.1581590175628662
epoch£º958	 i:5 	 global-step:19165	 l-p:0.10203471779823303
epoch£º958	 i:6 	 global-step:19166	 l-p:0.10913394391536713
epoch£º958	 i:7 	 global-step:19167	 l-p:0.1648259311914444
epoch£º958	 i:8 	 global-step:19168	 l-p:0.1271648108959198
epoch£º958	 i:9 	 global-step:19169	 l-p:0.1383042335510254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:959
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1745, 4.9922, 4.6711],
        [5.1745, 5.1745, 5.1745],
        [5.1745, 4.9355, 4.6470],
        [5.1745, 5.1716, 5.1744]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:959, step:0 
model_pd.l_p.mean(): 0.05553574487566948 
model_pd.l_d.mean(): -20.409597396850586 
model_pd.lagr.mean(): -20.354061126708984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4368], device='cuda:0')), ('power', tensor([-21.2436], device='cuda:0'))])
epoch£º959	 i:0 	 global-step:19180	 l-p:0.05553574487566948
epoch£º959	 i:1 	 global-step:19181	 l-p:0.15036556124687195
epoch£º959	 i:2 	 global-step:19182	 l-p:0.16565023362636566
epoch£º959	 i:3 	 global-step:19183	 l-p:0.1367613822221756
epoch£º959	 i:4 	 global-step:19184	 l-p:0.12199150025844574
epoch£º959	 i:5 	 global-step:19185	 l-p:0.12575557827949524
epoch£º959	 i:6 	 global-step:19186	 l-p:0.13829952478408813
epoch£º959	 i:7 	 global-step:19187	 l-p:0.13868075609207153
epoch£º959	 i:8 	 global-step:19188	 l-p:0.11562971770763397
epoch£º959	 i:9 	 global-step:19189	 l-p:0.09955652058124542
====================================================================================================
====================================================================================================
====================================================================================================

epoch:960
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2038, 5.2037, 5.2038],
        [5.2038, 5.2002, 5.2037],
        [5.2038, 5.5047, 5.3604],
        [5.2038, 5.4933, 5.3420]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:960, step:0 
model_pd.l_p.mean(): 0.10635185241699219 
model_pd.l_d.mean(): -19.42418670654297 
model_pd.lagr.mean(): -19.317834854125977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5008], device='cuda:0')), ('power', tensor([-20.3061], device='cuda:0'))])
epoch£º960	 i:0 	 global-step:19200	 l-p:0.10635185241699219
epoch£º960	 i:1 	 global-step:19201	 l-p:0.16397738456726074
epoch£º960	 i:2 	 global-step:19202	 l-p:0.12954667210578918
epoch£º960	 i:3 	 global-step:19203	 l-p:0.11961352825164795
epoch£º960	 i:4 	 global-step:19204	 l-p:0.1365012675523758
epoch£º960	 i:5 	 global-step:19205	 l-p:0.10638142377138138
epoch£º960	 i:6 	 global-step:19206	 l-p:0.11732528358697891
epoch£º960	 i:7 	 global-step:19207	 l-p:0.10493861883878708
epoch£º960	 i:8 	 global-step:19208	 l-p:0.12411851435899734
epoch£º960	 i:9 	 global-step:19209	 l-p:0.11643951386213303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:961
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1856, 5.0386, 4.7123],
        [5.1856, 5.1116, 5.1604],
        [5.1856, 5.0174, 4.6937],
        [5.1856, 5.1807, 5.1854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:961, step:0 
model_pd.l_p.mean(): 0.1273898035287857 
model_pd.l_d.mean(): -19.42271614074707 
model_pd.lagr.mean(): -19.295326232910156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5743], device='cuda:0')), ('power', tensor([-20.3807], device='cuda:0'))])
epoch£º961	 i:0 	 global-step:19220	 l-p:0.1273898035287857
epoch£º961	 i:1 	 global-step:19221	 l-p:0.12141212075948715
epoch£º961	 i:2 	 global-step:19222	 l-p:0.16766571998596191
epoch£º961	 i:3 	 global-step:19223	 l-p:0.13272108137607574
epoch£º961	 i:4 	 global-step:19224	 l-p:0.1660802662372589
epoch£º961	 i:5 	 global-step:19225	 l-p:0.1217956617474556
epoch£º961	 i:6 	 global-step:19226	 l-p:0.10527828335762024
epoch£º961	 i:7 	 global-step:19227	 l-p:0.13519731163978577
epoch£º961	 i:8 	 global-step:19228	 l-p:0.11894431710243225
epoch£º961	 i:9 	 global-step:19229	 l-p:0.12041876465082169
====================================================================================================
====================================================================================================
====================================================================================================

epoch:962
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1628, 5.1138, 5.1508],
        [5.1628, 5.1628, 5.1628],
        [5.1628, 5.1613, 5.1627],
        [5.1628, 5.0614, 5.1179]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:962, step:0 
model_pd.l_p.mean(): 0.11272242665290833 
model_pd.l_d.mean(): -20.670927047729492 
model_pd.lagr.mean(): -20.558204650878906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4002], device='cuda:0')), ('power', tensor([-21.4719], device='cuda:0'))])
epoch£º962	 i:0 	 global-step:19240	 l-p:0.11272242665290833
epoch£º962	 i:1 	 global-step:19241	 l-p:0.09545072168111801
epoch£º962	 i:2 	 global-step:19242	 l-p:0.12779857218265533
epoch£º962	 i:3 	 global-step:19243	 l-p:0.14436131715774536
epoch£º962	 i:4 	 global-step:19244	 l-p:0.10633187741041183
epoch£º962	 i:5 	 global-step:19245	 l-p:0.2286371886730194
epoch£º962	 i:6 	 global-step:19246	 l-p:0.10911515355110168
epoch£º962	 i:7 	 global-step:19247	 l-p:0.26432734727859497
epoch£º962	 i:8 	 global-step:19248	 l-p:0.15501883625984192
epoch£º962	 i:9 	 global-step:19249	 l-p:0.16292329132556915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:963
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1178, 5.1153, 5.1177],
        [5.1178, 5.1162, 5.1178],
        [5.1178, 5.0621, 5.1029],
        [5.1178, 5.0736, 5.1079]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:963, step:0 
model_pd.l_p.mean(): 0.12101726233959198 
model_pd.l_d.mean(): -19.910856246948242 
model_pd.lagr.mean(): -19.789838790893555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4943], device='cuda:0')), ('power', tensor([-20.7951], device='cuda:0'))])
epoch£º963	 i:0 	 global-step:19260	 l-p:0.12101726233959198
epoch£º963	 i:1 	 global-step:19261	 l-p:0.13864164054393768
epoch£º963	 i:2 	 global-step:19262	 l-p:0.1297234296798706
epoch£º963	 i:3 	 global-step:19263	 l-p:0.08543472737073898
epoch£º963	 i:4 	 global-step:19264	 l-p:0.18077659606933594
epoch£º963	 i:5 	 global-step:19265	 l-p:0.23805993795394897
epoch£º963	 i:6 	 global-step:19266	 l-p:0.5374857783317566
epoch£º963	 i:7 	 global-step:19267	 l-p:0.15477019548416138
epoch£º963	 i:8 	 global-step:19268	 l-p:0.223129004240036
epoch£º963	 i:9 	 global-step:19269	 l-p:0.11714320629835129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:964
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1038, 5.1021, 5.1037],
        [5.1038, 5.1034, 5.1038],
        [5.1038, 5.5276, 5.4650],
        [5.1038, 4.8864, 4.5683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:964, step:0 
model_pd.l_p.mean(): 0.19714629650115967 
model_pd.l_d.mean(): -18.46218490600586 
model_pd.lagr.mean(): -18.265039443969727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5968], device='cuda:0')), ('power', tensor([-19.4256], device='cuda:0'))])
epoch£º964	 i:0 	 global-step:19280	 l-p:0.19714629650115967
epoch£º964	 i:1 	 global-step:19281	 l-p:0.18912966549396515
epoch£º964	 i:2 	 global-step:19282	 l-p:0.15441066026687622
epoch£º964	 i:3 	 global-step:19283	 l-p:0.22039754688739777
epoch£º964	 i:4 	 global-step:19284	 l-p:0.1451486051082611
epoch£º964	 i:5 	 global-step:19285	 l-p:0.32508087158203125
epoch£º964	 i:6 	 global-step:19286	 l-p:0.13306398689746857
epoch£º964	 i:7 	 global-step:19287	 l-p:0.15053804218769073
epoch£º964	 i:8 	 global-step:19288	 l-p:0.10450200736522675
epoch£º964	 i:9 	 global-step:19289	 l-p:0.06933743506669998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:965
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[5.1171, 4.8769, 4.5762],
        [5.1171, 4.8421, 4.7469],
        [5.1171, 4.8788, 4.5765],
        [5.1171, 4.8591, 4.8103]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:965, step:0 
model_pd.l_p.mean(): 0.1785321980714798 
model_pd.l_d.mean(): -20.364559173583984 
model_pd.lagr.mean(): -20.18602752685547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4520], device='cuda:0')), ('power', tensor([-21.2135], device='cuda:0'))])
epoch£º965	 i:0 	 global-step:19300	 l-p:0.1785321980714798
epoch£º965	 i:1 	 global-step:19301	 l-p:0.13860784471035004
epoch£º965	 i:2 	 global-step:19302	 l-p:0.13943234086036682
epoch£º965	 i:3 	 global-step:19303	 l-p:0.09481727331876755
epoch£º965	 i:4 	 global-step:19304	 l-p:0.13062892854213715
epoch£º965	 i:5 	 global-step:19305	 l-p:0.22220422327518463
epoch£º965	 i:6 	 global-step:19306	 l-p:0.21543659269809723
epoch£º965	 i:7 	 global-step:19307	 l-p:0.16544002294540405
epoch£º965	 i:8 	 global-step:19308	 l-p:0.12977568805217743
epoch£º965	 i:9 	 global-step:19309	 l-p:0.07655744254589081
====================================================================================================
====================================================================================================
====================================================================================================

epoch:966
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1456, 5.2481, 4.9930],
        [5.1456, 5.1451, 5.1456],
        [5.1456, 5.2182, 4.9487],
        [5.1456, 4.8945, 4.8558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:966, step:0 
model_pd.l_p.mean(): 0.21653741598129272 
model_pd.l_d.mean(): -19.359817504882812 
model_pd.lagr.mean(): -19.143280029296875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5457], device='cuda:0')), ('power', tensor([-20.2870], device='cuda:0'))])
epoch£º966	 i:0 	 global-step:19320	 l-p:0.21653741598129272
epoch£º966	 i:1 	 global-step:19321	 l-p:0.11296845972537994
epoch£º966	 i:2 	 global-step:19322	 l-p:0.07442182302474976
epoch£º966	 i:3 	 global-step:19323	 l-p:0.12393592298030853
epoch£º966	 i:4 	 global-step:19324	 l-p:0.13286815583705902
epoch£º966	 i:5 	 global-step:19325	 l-p:0.14622622728347778
epoch£º966	 i:6 	 global-step:19326	 l-p:0.14063099026679993
epoch£º966	 i:7 	 global-step:19327	 l-p:0.12022319436073303
epoch£º966	 i:8 	 global-step:19328	 l-p:0.14093676209449768
epoch£º966	 i:9 	 global-step:19329	 l-p:0.1815861016511917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:967
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1484, 5.1483, 5.1484],
        [5.1484, 5.1484, 5.1484],
        [5.1484, 5.1484, 5.1484],
        [5.1484, 4.8745, 4.6352]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:967, step:0 
model_pd.l_p.mean(): 0.16350263357162476 
model_pd.l_d.mean(): -20.67485237121582 
model_pd.lagr.mean(): -20.511350631713867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4358], device='cuda:0')), ('power', tensor([-21.5128], device='cuda:0'))])
epoch£º967	 i:0 	 global-step:19340	 l-p:0.16350263357162476
epoch£º967	 i:1 	 global-step:19341	 l-p:0.14197911322116852
epoch£º967	 i:2 	 global-step:19342	 l-p:0.16019290685653687
epoch£º967	 i:3 	 global-step:19343	 l-p:0.12299807369709015
epoch£º967	 i:4 	 global-step:19344	 l-p:0.11297770589590073
epoch£º967	 i:5 	 global-step:19345	 l-p:0.13158829510211945
epoch£º967	 i:6 	 global-step:19346	 l-p:0.17087969183921814
epoch£º967	 i:7 	 global-step:19347	 l-p:0.16633450984954834
epoch£º967	 i:8 	 global-step:19348	 l-p:0.1657407283782959
epoch£º967	 i:9 	 global-step:19349	 l-p:0.12748390436172485
====================================================================================================
====================================================================================================
====================================================================================================

epoch:968
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1381, 5.0665, 4.7440],
        [5.1381, 4.9794, 4.6487],
        [5.1381, 5.0510, 5.1045],
        [5.1381, 4.8642, 4.6195]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:968, step:0 
model_pd.l_p.mean(): 0.0904616117477417 
model_pd.l_d.mean(): -20.126605987548828 
model_pd.lagr.mean(): -20.036144256591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5088], device='cuda:0')), ('power', tensor([-21.0300], device='cuda:0'))])
epoch£º968	 i:0 	 global-step:19360	 l-p:0.0904616117477417
epoch£º968	 i:1 	 global-step:19361	 l-p:0.1897239238023758
epoch£º968	 i:2 	 global-step:19362	 l-p:0.11224828660488129
epoch£º968	 i:3 	 global-step:19363	 l-p:0.15237900614738464
epoch£º968	 i:4 	 global-step:19364	 l-p:0.0951145738363266
epoch£º968	 i:5 	 global-step:19365	 l-p:0.16451287269592285
epoch£º968	 i:6 	 global-step:19366	 l-p:0.10590177029371262
epoch£º968	 i:7 	 global-step:19367	 l-p:0.14955352246761322
epoch£º968	 i:8 	 global-step:19368	 l-p:0.20073479413986206
epoch£º968	 i:9 	 global-step:19369	 l-p:0.14144405722618103
====================================================================================================
====================================================================================================
====================================================================================================

epoch:969
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1527, 4.8931, 4.6220],
        [5.1527, 5.2155, 4.9411],
        [5.1527, 4.9940, 4.6644],
        [5.1527, 5.0283, 5.0868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:969, step:0 
model_pd.l_p.mean(): 0.15466374158859253 
model_pd.l_d.mean(): -20.78241729736328 
model_pd.lagr.mean(): -20.62775421142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3900], device='cuda:0')), ('power', tensor([-21.5749], device='cuda:0'))])
epoch£º969	 i:0 	 global-step:19380	 l-p:0.15466374158859253
epoch£º969	 i:1 	 global-step:19381	 l-p:0.13202503323554993
epoch£º969	 i:2 	 global-step:19382	 l-p:0.1138327568769455
epoch£º969	 i:3 	 global-step:19383	 l-p:0.1195731833577156
epoch£º969	 i:4 	 global-step:19384	 l-p:0.14224134385585785
epoch£º969	 i:5 	 global-step:19385	 l-p:0.20023605227470398
epoch£º969	 i:6 	 global-step:19386	 l-p:0.1238134354352951
epoch£º969	 i:7 	 global-step:19387	 l-p:0.13366729021072388
epoch£º969	 i:8 	 global-step:19388	 l-p:0.1946718692779541
epoch£º969	 i:9 	 global-step:19389	 l-p:0.09794576466083527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:970
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1513, 5.1512, 5.1513],
        [5.1513, 4.9191, 4.9126],
        [5.1513, 4.9966, 5.0505],
        [5.1513, 4.8753, 4.7680]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:970, step:0 
model_pd.l_p.mean(): 0.11957299709320068 
model_pd.l_d.mean(): -20.4929256439209 
model_pd.lagr.mean(): -20.37335205078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4390], device='cuda:0')), ('power', tensor([-21.3308], device='cuda:0'))])
epoch£º970	 i:0 	 global-step:19400	 l-p:0.11957299709320068
epoch£º970	 i:1 	 global-step:19401	 l-p:0.11500224471092224
epoch£º970	 i:2 	 global-step:19402	 l-p:0.18491917848587036
epoch£º970	 i:3 	 global-step:19403	 l-p:0.18820305168628693
epoch£º970	 i:4 	 global-step:19404	 l-p:0.13579393923282623
epoch£º970	 i:5 	 global-step:19405	 l-p:0.10845185071229935
epoch£º970	 i:6 	 global-step:19406	 l-p:0.11996614187955856
epoch£º970	 i:7 	 global-step:19407	 l-p:0.12625743448734283
epoch£º970	 i:8 	 global-step:19408	 l-p:0.13597074151039124
epoch£º970	 i:9 	 global-step:19409	 l-p:0.11112026125192642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:971
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1712, 5.0772, 5.1323],
        [5.1712, 5.0817, 5.1357],
        [5.1712, 5.1704, 5.1712],
        [5.1712, 4.8923, 4.6813]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:971, step:0 
model_pd.l_p.mean(): 0.10230065882205963 
model_pd.l_d.mean(): -20.591327667236328 
model_pd.lagr.mean(): -20.48902702331543 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4134], device='cuda:0')), ('power', tensor([-21.4046], device='cuda:0'))])
epoch£º971	 i:0 	 global-step:19420	 l-p:0.10230065882205963
epoch£º971	 i:1 	 global-step:19421	 l-p:0.1371953934431076
epoch£º971	 i:2 	 global-step:19422	 l-p:0.09049620479345322
epoch£º971	 i:3 	 global-step:19423	 l-p:0.14060962200164795
epoch£º971	 i:4 	 global-step:19424	 l-p:0.14601679146289825
epoch£º971	 i:5 	 global-step:19425	 l-p:0.16015571355819702
epoch£º971	 i:6 	 global-step:19426	 l-p:0.18225757777690887
epoch£º971	 i:7 	 global-step:19427	 l-p:0.1083349958062172
epoch£º971	 i:8 	 global-step:19428	 l-p:0.15048393607139587
epoch£º971	 i:9 	 global-step:19429	 l-p:0.13981471955776215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:972
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1582, 5.0898, 5.1366],
        [5.1582, 5.1566, 5.1582],
        [5.1582, 4.9019, 4.8526],
        [5.1582, 5.1553, 5.1581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:972, step:0 
model_pd.l_p.mean(): 0.15646576881408691 
model_pd.l_d.mean(): -19.806161880493164 
model_pd.lagr.mean(): -19.649696350097656 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5246], device='cuda:0')), ('power', tensor([-20.7199], device='cuda:0'))])
epoch£º972	 i:0 	 global-step:19440	 l-p:0.15646576881408691
epoch£º972	 i:1 	 global-step:19441	 l-p:0.11799721419811249
epoch£º972	 i:2 	 global-step:19442	 l-p:0.13999396562576294
epoch£º972	 i:3 	 global-step:19443	 l-p:0.14481356739997864
epoch£º972	 i:4 	 global-step:19444	 l-p:0.16063258051872253
epoch£º972	 i:5 	 global-step:19445	 l-p:0.14558526873588562
epoch£º972	 i:6 	 global-step:19446	 l-p:0.19122520089149475
epoch£º972	 i:7 	 global-step:19447	 l-p:0.0825008824467659
epoch£º972	 i:8 	 global-step:19448	 l-p:0.10864324122667313
epoch£º972	 i:9 	 global-step:19449	 l-p:0.10745729506015778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:973
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1646, 5.1508, 5.1632],
        [5.1646, 4.9484, 4.9612],
        [5.1646, 5.0517, 5.1099],
        [5.1646, 5.1219, 4.8070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:973, step:0 
model_pd.l_p.mean(): 0.17998391389846802 
model_pd.l_d.mean(): -19.8316593170166 
model_pd.lagr.mean(): -19.651676177978516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4446], device='cuda:0')), ('power', tensor([-20.6630], device='cuda:0'))])
epoch£º973	 i:0 	 global-step:19460	 l-p:0.17998391389846802
epoch£º973	 i:1 	 global-step:19461	 l-p:0.12901581823825836
epoch£º973	 i:2 	 global-step:19462	 l-p:0.06995570659637451
epoch£º973	 i:3 	 global-step:19463	 l-p:0.13401657342910767
epoch£º973	 i:4 	 global-step:19464	 l-p:0.16186614334583282
epoch£º973	 i:5 	 global-step:19465	 l-p:0.12268343567848206
epoch£º973	 i:6 	 global-step:19466	 l-p:0.15146256983280182
epoch£º973	 i:7 	 global-step:19467	 l-p:0.16832882165908813
epoch£º973	 i:8 	 global-step:19468	 l-p:0.11371196806430817
epoch£º973	 i:9 	 global-step:19469	 l-p:0.11953076720237732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:974
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1489, 4.8788, 4.7958],
        [5.1489, 5.0948, 5.1348],
        [5.1489, 4.9272, 4.9352],
        [5.1489, 5.1809, 4.8926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:974, step:0 
model_pd.l_p.mean(): 0.1412959098815918 
model_pd.l_d.mean(): -19.089366912841797 
model_pd.lagr.mean(): -18.948070526123047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5072], device='cuda:0')), ('power', tensor([-19.9716], device='cuda:0'))])
epoch£º974	 i:0 	 global-step:19480	 l-p:0.1412959098815918
epoch£º974	 i:1 	 global-step:19481	 l-p:0.12123797088861465
epoch£º974	 i:2 	 global-step:19482	 l-p:0.13354043662548065
epoch£º974	 i:3 	 global-step:19483	 l-p:0.14072591066360474
epoch£º974	 i:4 	 global-step:19484	 l-p:0.2285415381193161
epoch£º974	 i:5 	 global-step:19485	 l-p:0.13568134605884552
epoch£º974	 i:6 	 global-step:19486	 l-p:0.1317588835954666
epoch£º974	 i:7 	 global-step:19487	 l-p:0.11300718039274216
epoch£º974	 i:8 	 global-step:19488	 l-p:0.13715392351150513
epoch£º974	 i:9 	 global-step:19489	 l-p:0.17606107890605927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:975
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1354, 5.1354, 5.1354],
        [5.1354, 5.1354, 5.1354],
        [5.1354, 4.8494, 4.6541],
        [5.1354, 5.0009, 5.0594]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:975, step:0 
model_pd.l_p.mean(): 0.1090138852596283 
model_pd.l_d.mean(): -19.38750457763672 
model_pd.lagr.mean(): -19.27849006652832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4829], device='cuda:0')), ('power', tensor([-20.2502], device='cuda:0'))])
epoch£º975	 i:0 	 global-step:19500	 l-p:0.1090138852596283
epoch£º975	 i:1 	 global-step:19501	 l-p:0.19718928635120392
epoch£º975	 i:2 	 global-step:19502	 l-p:0.16193361580371857
epoch£º975	 i:3 	 global-step:19503	 l-p:0.1332578808069229
epoch£º975	 i:4 	 global-step:19504	 l-p:0.10235729068517685
epoch£º975	 i:5 	 global-step:19505	 l-p:0.1497126966714859
epoch£º975	 i:6 	 global-step:19506	 l-p:0.10709501057863235
epoch£º975	 i:7 	 global-step:19507	 l-p:0.31531062722206116
epoch£º975	 i:8 	 global-step:19508	 l-p:0.1114945262670517
epoch£º975	 i:9 	 global-step:19509	 l-p:0.147303506731987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:976
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1142, 4.8452, 4.7756],
        [5.1142, 5.0876, 5.1101],
        [5.1142, 5.1434, 4.8537],
        [5.1142, 5.1142, 5.1142]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:976, step:0 
model_pd.l_p.mean(): 0.2449653148651123 
model_pd.l_d.mean(): -20.38001823425293 
model_pd.lagr.mean(): -20.135053634643555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4740], device='cuda:0')), ('power', tensor([-21.2521], device='cuda:0'))])
epoch£º976	 i:0 	 global-step:19520	 l-p:0.2449653148651123
epoch£º976	 i:1 	 global-step:19521	 l-p:0.34922972321510315
epoch£º976	 i:2 	 global-step:19522	 l-p:0.15339817106723785
epoch£º976	 i:3 	 global-step:19523	 l-p:0.15578170120716095
epoch£º976	 i:4 	 global-step:19524	 l-p:0.1618528664112091
epoch£º976	 i:5 	 global-step:19525	 l-p:0.12732648849487305
epoch£º976	 i:6 	 global-step:19526	 l-p:0.11057759821414948
epoch£º976	 i:7 	 global-step:19527	 l-p:0.12776611745357513
epoch£º976	 i:8 	 global-step:19528	 l-p:0.12213386595249176
epoch£º976	 i:9 	 global-step:19529	 l-p:0.14437274634838104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:977
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1227, 5.1227, 5.1227],
        [5.1227, 5.1227, 5.1227],
        [5.1227, 5.0253, 5.0817],
        [5.1227, 4.8632, 4.8147]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:977, step:0 
model_pd.l_p.mean(): 0.1699211597442627 
model_pd.l_d.mean(): -20.50518035888672 
model_pd.lagr.mean(): -20.33525848388672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4599], device='cuda:0')), ('power', tensor([-21.3650], device='cuda:0'))])
epoch£º977	 i:0 	 global-step:19540	 l-p:0.1699211597442627
epoch£º977	 i:1 	 global-step:19541	 l-p:0.1787787228822708
epoch£º977	 i:2 	 global-step:19542	 l-p:0.11430590599775314
epoch£º977	 i:3 	 global-step:19543	 l-p:0.12376558035612106
epoch£º977	 i:4 	 global-step:19544	 l-p:0.16650229692459106
epoch£º977	 i:5 	 global-step:19545	 l-p:0.08884557336568832
epoch£º977	 i:6 	 global-step:19546	 l-p:0.11267026513814926
epoch£º977	 i:7 	 global-step:19547	 l-p:0.21094578504562378
epoch£º977	 i:8 	 global-step:19548	 l-p:0.36393076181411743
epoch£º977	 i:9 	 global-step:19549	 l-p:0.19471393525600433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:978
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1173, 5.1166, 5.1172],
        [5.1173, 5.0346, 5.0870],
        [5.1173, 5.1173, 5.1173],
        [5.1173, 4.8279, 4.6533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:978, step:0 
model_pd.l_p.mean(): 0.13291439414024353 
model_pd.l_d.mean(): -20.153682708740234 
model_pd.lagr.mean(): -20.020769119262695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4884], device='cuda:0')), ('power', tensor([-21.0364], device='cuda:0'))])
epoch£º978	 i:0 	 global-step:19560	 l-p:0.13291439414024353
epoch£º978	 i:1 	 global-step:19561	 l-p:0.16591434180736542
epoch£º978	 i:2 	 global-step:19562	 l-p:0.24410828948020935
epoch£º978	 i:3 	 global-step:19563	 l-p:0.11837305873632431
epoch£º978	 i:4 	 global-step:19564	 l-p:0.14777320623397827
epoch£º978	 i:5 	 global-step:19565	 l-p:0.13222390413284302
epoch£º978	 i:6 	 global-step:19566	 l-p:0.18788497149944305
epoch£º978	 i:7 	 global-step:19567	 l-p:0.23780563473701477
epoch£º978	 i:8 	 global-step:19568	 l-p:0.08503005653619766
epoch£º978	 i:9 	 global-step:19569	 l-p:0.12064704298973083
====================================================================================================
====================================================================================================
====================================================================================================

epoch:979
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228]], device='cuda:0')
 pt:tensor([[5.1324, 5.5347, 5.4556],
        [5.1324, 4.8494, 4.6231],
        [5.1324, 4.8747, 4.5904],
        [5.1324, 4.8537, 4.7492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:979, step:0 
model_pd.l_p.mean(): 0.16680380702018738 
model_pd.l_d.mean(): -19.674283981323242 
model_pd.lagr.mean(): -19.50748062133789 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5429], device='cuda:0')), ('power', tensor([-20.6044], device='cuda:0'))])
epoch£º979	 i:0 	 global-step:19580	 l-p:0.16680380702018738
epoch£º979	 i:1 	 global-step:19581	 l-p:0.11448575556278229
epoch£º979	 i:2 	 global-step:19582	 l-p:0.17900612950325012
epoch£º979	 i:3 	 global-step:19583	 l-p:0.14407210052013397
epoch£º979	 i:4 	 global-step:19584	 l-p:0.11443943530321121
epoch£º979	 i:5 	 global-step:19585	 l-p:0.12001542747020721
epoch£º979	 i:6 	 global-step:19586	 l-p:0.12056225538253784
epoch£º979	 i:7 	 global-step:19587	 l-p:0.1645428091287613
epoch£º979	 i:8 	 global-step:19588	 l-p:0.15312069654464722
epoch£º979	 i:9 	 global-step:19589	 l-p:0.1226835772395134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:980
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1678, 5.1678, 5.1678],
        [5.1678, 5.1648, 5.1677],
        [5.1678, 5.1215, 5.1571],
        [5.1678, 5.1674, 5.1678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:980, step:0 
model_pd.l_p.mean(): 0.1838930994272232 
model_pd.l_d.mean(): -20.50796127319336 
model_pd.lagr.mean(): -20.324068069458008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4284], device='cuda:0')), ('power', tensor([-21.3351], device='cuda:0'))])
epoch£º980	 i:0 	 global-step:19600	 l-p:0.1838930994272232
epoch£º980	 i:1 	 global-step:19601	 l-p:0.1453389972448349
epoch£º980	 i:2 	 global-step:19602	 l-p:0.13794542849063873
epoch£º980	 i:3 	 global-step:19603	 l-p:0.06975317746400833
epoch£º980	 i:4 	 global-step:19604	 l-p:0.1021198257803917
epoch£º980	 i:5 	 global-step:19605	 l-p:0.18281112611293793
epoch£º980	 i:6 	 global-step:19606	 l-p:0.1155291274189949
epoch£º980	 i:7 	 global-step:19607	 l-p:0.14256718754768372
epoch£º980	 i:8 	 global-step:19608	 l-p:0.12506170570850372
epoch£º980	 i:9 	 global-step:19609	 l-p:0.10743744671344757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:981
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1855, 5.0611, 5.1196],
        [5.1855, 5.1855, 5.1855],
        [5.1855, 5.1833, 5.1854],
        [5.1855, 4.9345, 4.6534]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:981, step:0 
model_pd.l_p.mean(): 0.13785535097122192 
model_pd.l_d.mean(): -19.594663619995117 
model_pd.lagr.mean(): -19.45680809020996 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4797], device='cuda:0')), ('power', tensor([-20.4579], device='cuda:0'))])
epoch£º981	 i:0 	 global-step:19620	 l-p:0.13785535097122192
epoch£º981	 i:1 	 global-step:19621	 l-p:0.13055041432380676
epoch£º981	 i:2 	 global-step:19622	 l-p:0.1314728856086731
epoch£º981	 i:3 	 global-step:19623	 l-p:0.1368534415960312
epoch£º981	 i:4 	 global-step:19624	 l-p:0.1270560473203659
epoch£º981	 i:5 	 global-step:19625	 l-p:0.1540614366531372
epoch£º981	 i:6 	 global-step:19626	 l-p:0.15224124491214752
epoch£º981	 i:7 	 global-step:19627	 l-p:0.16197173297405243
epoch£º981	 i:8 	 global-step:19628	 l-p:0.08766254037618637
epoch£º981	 i:9 	 global-step:19629	 l-p:0.1712329089641571
====================================================================================================
====================================================================================================
====================================================================================================

epoch:982
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1405, 4.8866, 4.8480],
        [5.1405, 5.2876, 5.0537],
        [5.1405, 5.1405, 5.1405],
        [5.1405, 5.0525, 5.1065]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:982, step:0 
model_pd.l_p.mean(): 0.1469225287437439 
model_pd.l_d.mean(): -20.350292205810547 
model_pd.lagr.mean(): -20.203369140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4684], device='cuda:0')), ('power', tensor([-21.2160], device='cuda:0'))])
epoch£º982	 i:0 	 global-step:19640	 l-p:0.1469225287437439
epoch£º982	 i:1 	 global-step:19641	 l-p:0.16079649329185486
epoch£º982	 i:2 	 global-step:19642	 l-p:0.17241531610488892
epoch£º982	 i:3 	 global-step:19643	 l-p:0.15014702081680298
epoch£º982	 i:4 	 global-step:19644	 l-p:0.15238994359970093
epoch£º982	 i:5 	 global-step:19645	 l-p:0.06983029097318649
epoch£º982	 i:6 	 global-step:19646	 l-p:0.14692802727222443
epoch£º982	 i:7 	 global-step:19647	 l-p:0.12383660674095154
epoch£º982	 i:8 	 global-step:19648	 l-p:0.1890752911567688
epoch£º982	 i:9 	 global-step:19649	 l-p:0.16924241185188293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:983
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1358, 4.9629, 5.0111],
        [5.1358, 5.0008, 5.0595],
        [5.1358, 5.0477, 5.1018],
        [5.1358, 5.1328, 5.1357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:983, step:0 
model_pd.l_p.mean(): 0.07468155771493912 
model_pd.l_d.mean(): -20.52350425720215 
model_pd.lagr.mean(): -20.448822021484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4534], device='cuda:0')), ('power', tensor([-21.3768], device='cuda:0'))])
epoch£º983	 i:0 	 global-step:19660	 l-p:0.07468155771493912
epoch£º983	 i:1 	 global-step:19661	 l-p:0.11890146136283875
epoch£º983	 i:2 	 global-step:19662	 l-p:0.1471504420042038
epoch£º983	 i:3 	 global-step:19663	 l-p:0.13448196649551392
epoch£º983	 i:4 	 global-step:19664	 l-p:0.13051436841487885
epoch£º983	 i:5 	 global-step:19665	 l-p:0.11813706904649734
epoch£º983	 i:6 	 global-step:19666	 l-p:0.14998742938041687
epoch£º983	 i:7 	 global-step:19667	 l-p:0.1452414095401764
epoch£º983	 i:8 	 global-step:19668	 l-p:0.24659191071987152
epoch£º983	 i:9 	 global-step:19669	 l-p:0.1480604112148285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:984
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1546, 4.9617, 4.6354],
        [5.1546, 5.1483, 5.1542],
        [5.1546, 5.1545, 5.1546],
        [5.1546, 5.1494, 5.1543]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:984, step:0 
model_pd.l_p.mean(): 0.07935449481010437 
model_pd.l_d.mean(): -20.04007911682129 
model_pd.lagr.mean(): -19.960723876953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4979], device='cuda:0')), ('power', tensor([-20.9305], device='cuda:0'))])
epoch£º984	 i:0 	 global-step:19680	 l-p:0.07935449481010437
epoch£º984	 i:1 	 global-step:19681	 l-p:0.18469826877117157
epoch£º984	 i:2 	 global-step:19682	 l-p:0.11864104866981506
epoch£º984	 i:3 	 global-step:19683	 l-p:0.1382705122232437
epoch£º984	 i:4 	 global-step:19684	 l-p:0.12083875387907028
epoch£º984	 i:5 	 global-step:19685	 l-p:0.11535512655973434
epoch£º984	 i:6 	 global-step:19686	 l-p:0.11453946679830551
epoch£º984	 i:7 	 global-step:19687	 l-p:0.18070083856582642
epoch£º984	 i:8 	 global-step:19688	 l-p:0.20590011775493622
epoch£º984	 i:9 	 global-step:19689	 l-p:0.1432158648967743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:985
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1438, 5.5247, 5.4305],
        [5.1438, 4.9710, 5.0191],
        [5.1438, 5.1757, 4.8862],
        [5.1438, 5.1438, 5.1438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:985, step:0 
model_pd.l_p.mean(): 0.18823905289173126 
model_pd.l_d.mean(): -20.286109924316406 
model_pd.lagr.mean(): -20.097871780395508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4658], device='cuda:0')), ('power', tensor([-21.1479], device='cuda:0'))])
epoch£º985	 i:0 	 global-step:19700	 l-p:0.18823905289173126
epoch£º985	 i:1 	 global-step:19701	 l-p:0.12822560966014862
epoch£º985	 i:2 	 global-step:19702	 l-p:0.1463049203157425
epoch£º985	 i:3 	 global-step:19703	 l-p:0.15972170233726501
epoch£º985	 i:4 	 global-step:19704	 l-p:0.16140000522136688
epoch£º985	 i:5 	 global-step:19705	 l-p:0.1137031689286232
epoch£º985	 i:6 	 global-step:19706	 l-p:0.13368797302246094
epoch£º985	 i:7 	 global-step:19707	 l-p:0.12915001809597015
epoch£º985	 i:8 	 global-step:19708	 l-p:0.18661649525165558
epoch£º985	 i:9 	 global-step:19709	 l-p:0.08823744207620621
====================================================================================================
====================================================================================================
====================================================================================================

epoch:986
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1524, 5.1524, 5.1524],
        [5.1524, 5.0590, 5.1144],
        [5.1524, 5.1479, 5.1521],
        [5.1524, 5.0208, 5.0797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:986, step:0 
model_pd.l_p.mean(): 0.0967840775847435 
model_pd.l_d.mean(): -20.354103088378906 
model_pd.lagr.mean(): -20.2573184967041 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4593], device='cuda:0')), ('power', tensor([-21.2104], device='cuda:0'))])
epoch£º986	 i:0 	 global-step:19720	 l-p:0.0967840775847435
epoch£º986	 i:1 	 global-step:19721	 l-p:0.16252021491527557
epoch£º986	 i:2 	 global-step:19722	 l-p:0.11810193955898285
epoch£º986	 i:3 	 global-step:19723	 l-p:0.09157655388116837
epoch£º986	 i:4 	 global-step:19724	 l-p:0.15667952597141266
epoch£º986	 i:5 	 global-step:19725	 l-p:0.20872780680656433
epoch£º986	 i:6 	 global-step:19726	 l-p:0.17798906564712524
epoch£º986	 i:7 	 global-step:19727	 l-p:0.13584868609905243
epoch£º986	 i:8 	 global-step:19728	 l-p:0.10272824764251709
epoch£º986	 i:9 	 global-step:19729	 l-p:0.10699819028377533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:987
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1805, 5.1805, 5.1805],
        [5.1805, 5.1805, 5.1805],
        [5.1805, 5.1311, 5.1685],
        [5.1805, 5.0928, 5.1466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:987, step:0 
model_pd.l_p.mean(): 0.11517562717199326 
model_pd.l_d.mean(): -19.22459602355957 
model_pd.lagr.mean(): -19.109420776367188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4832], device='cuda:0')), ('power', tensor([-20.0845], device='cuda:0'))])
epoch£º987	 i:0 	 global-step:19740	 l-p:0.11517562717199326
epoch£º987	 i:1 	 global-step:19741	 l-p:0.15324945747852325
epoch£º987	 i:2 	 global-step:19742	 l-p:0.037126924842596054
epoch£º987	 i:3 	 global-step:19743	 l-p:0.09300878643989563
epoch£º987	 i:4 	 global-step:19744	 l-p:0.1515529453754425
epoch£º987	 i:5 	 global-step:19745	 l-p:0.1426016241312027
epoch£º987	 i:6 	 global-step:19746	 l-p:0.13207952678203583
epoch£º987	 i:7 	 global-step:19747	 l-p:0.17085139453411102
epoch£º987	 i:8 	 global-step:19748	 l-p:0.15951091051101685
epoch£º987	 i:9 	 global-step:19749	 l-p:0.14413723349571228
====================================================================================================
====================================================================================================
====================================================================================================

epoch:988
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1779, 4.8967, 4.6798],
        [5.1779, 4.8960, 4.7613],
        [5.1779, 5.1775, 5.1779],
        [5.1779, 5.1779, 5.1779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:988, step:0 
model_pd.l_p.mean(): 0.16495037078857422 
model_pd.l_d.mean(): -20.583728790283203 
model_pd.lagr.mean(): -20.418777465820312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4093], device='cuda:0')), ('power', tensor([-21.3925], device='cuda:0'))])
epoch£º988	 i:0 	 global-step:19760	 l-p:0.16495037078857422
epoch£º988	 i:1 	 global-step:19761	 l-p:0.10939598083496094
epoch£º988	 i:2 	 global-step:19762	 l-p:0.13021574914455414
epoch£º988	 i:3 	 global-step:19763	 l-p:0.14671774208545685
epoch£º988	 i:4 	 global-step:19764	 l-p:0.1689794957637787
epoch£º988	 i:5 	 global-step:19765	 l-p:0.11608901619911194
epoch£º988	 i:6 	 global-step:19766	 l-p:0.09414911270141602
epoch£º988	 i:7 	 global-step:19767	 l-p:0.17880527675151825
epoch£º988	 i:8 	 global-step:19768	 l-p:0.1560264229774475
epoch£º988	 i:9 	 global-step:19769	 l-p:0.10963716357946396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:989
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1510, 4.9371, 4.9559],
        [5.1510, 5.1478, 5.1509],
        [5.1510, 5.0192, 5.0782],
        [5.1510, 5.1510, 5.1510]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:989, step:0 
model_pd.l_p.mean(): 0.13156893849372864 
model_pd.l_d.mean(): -20.608245849609375 
model_pd.lagr.mean(): -20.47667694091797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4106], device='cuda:0')), ('power', tensor([-21.4189], device='cuda:0'))])
epoch£º989	 i:0 	 global-step:19780	 l-p:0.13156893849372864
epoch£º989	 i:1 	 global-step:19781	 l-p:0.19778190553188324
epoch£º989	 i:2 	 global-step:19782	 l-p:0.14429524540901184
epoch£º989	 i:3 	 global-step:19783	 l-p:0.0755242258310318
epoch£º989	 i:4 	 global-step:19784	 l-p:0.17300431430339813
epoch£º989	 i:5 	 global-step:19785	 l-p:0.17237912118434906
epoch£º989	 i:6 	 global-step:19786	 l-p:0.13023324310779572
epoch£º989	 i:7 	 global-step:19787	 l-p:0.16212098300457
epoch£º989	 i:8 	 global-step:19788	 l-p:0.07707034051418304
epoch£º989	 i:9 	 global-step:19789	 l-p:0.13524582982063293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:990
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228]], device='cuda:0')
 pt:tensor([[5.1716, 5.4544, 5.2968],
        [5.1716, 4.8968, 4.8005],
        [5.1716, 5.0013, 4.6702],
        [5.1716, 5.4800, 5.3382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:990, step:0 
model_pd.l_p.mean(): 0.10160753130912781 
model_pd.l_d.mean(): -19.02248191833496 
model_pd.lagr.mean(): -18.920873641967773 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5896], device='cuda:0')), ('power', tensor([-19.9889], device='cuda:0'))])
epoch£º990	 i:0 	 global-step:19800	 l-p:0.10160753130912781
epoch£º990	 i:1 	 global-step:19801	 l-p:0.13812337815761566
epoch£º990	 i:2 	 global-step:19802	 l-p:0.12083207070827484
epoch£º990	 i:3 	 global-step:19803	 l-p:0.13184666633605957
epoch£º990	 i:4 	 global-step:19804	 l-p:0.11106530576944351
epoch£º990	 i:5 	 global-step:19805	 l-p:0.15172675251960754
epoch£º990	 i:6 	 global-step:19806	 l-p:0.14453212916851044
epoch£º990	 i:7 	 global-step:19807	 l-p:0.1409468948841095
epoch£º990	 i:8 	 global-step:19808	 l-p:0.14336234331130981
epoch£º990	 i:9 	 global-step:19809	 l-p:0.0723671019077301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:991
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2021, 5.1513, 5.1894],
        [5.2021, 4.9207, 4.7739],
        [5.2021, 5.1210, 5.1726],
        [5.2021, 5.1979, 5.2019]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:991, step:0 
model_pd.l_p.mean(): 0.13203592598438263 
model_pd.l_d.mean(): -20.73823356628418 
model_pd.lagr.mean(): -20.606197357177734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3812], device='cuda:0')), ('power', tensor([-21.5209], device='cuda:0'))])
epoch£º991	 i:0 	 global-step:19820	 l-p:0.13203592598438263
epoch£º991	 i:1 	 global-step:19821	 l-p:0.052507027983665466
epoch£º991	 i:2 	 global-step:19822	 l-p:0.1332523375749588
epoch£º991	 i:3 	 global-step:19823	 l-p:0.15594050288200378
epoch£º991	 i:4 	 global-step:19824	 l-p:0.12739014625549316
epoch£º991	 i:5 	 global-step:19825	 l-p:0.11881973594427109
epoch£º991	 i:6 	 global-step:19826	 l-p:0.14114993810653687
epoch£º991	 i:7 	 global-step:19827	 l-p:0.16629041731357574
epoch£º991	 i:8 	 global-step:19828	 l-p:0.10698969662189484
epoch£º991	 i:9 	 global-step:19829	 l-p:0.10897362232208252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:992
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1835, 5.1834, 5.1835],
        [5.1835, 5.1685, 5.1820],
        [5.1835, 5.2270, 4.9416],
        [5.1835, 5.0273, 4.6952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:992, step:0 
model_pd.l_p.mean(): 0.11589805036783218 
model_pd.l_d.mean(): -20.504215240478516 
model_pd.lagr.mean(): -20.388317108154297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4180], device='cuda:0')), ('power', tensor([-21.3206], device='cuda:0'))])
epoch£º992	 i:0 	 global-step:19840	 l-p:0.11589805036783218
epoch£º992	 i:1 	 global-step:19841	 l-p:0.1539621502161026
epoch£º992	 i:2 	 global-step:19842	 l-p:0.13303838670253754
epoch£º992	 i:3 	 global-step:19843	 l-p:0.10742484778165817
epoch£º992	 i:4 	 global-step:19844	 l-p:0.1643924117088318
epoch£º992	 i:5 	 global-step:19845	 l-p:0.10421691834926605
epoch£º992	 i:6 	 global-step:19846	 l-p:0.16160480678081512
epoch£º992	 i:7 	 global-step:19847	 l-p:0.1294824779033661
epoch£º992	 i:8 	 global-step:19848	 l-p:0.10961385816335678
epoch£º992	 i:9 	 global-step:19849	 l-p:0.14048342406749725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:993
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1650, 5.2650, 5.0055],
        [5.1650, 4.8783, 4.6844],
        [5.1650, 5.1650, 5.1650],
        [5.1650, 5.1539, 5.1641]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:993, step:0 
model_pd.l_p.mean(): 0.2521772086620331 
model_pd.l_d.mean(): -20.338748931884766 
model_pd.lagr.mean(): -20.086572647094727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4683], device='cuda:0')), ('power', tensor([-21.2041], device='cuda:0'))])
epoch£º993	 i:0 	 global-step:19860	 l-p:0.2521772086620331
epoch£º993	 i:1 	 global-step:19861	 l-p:0.15054523944854736
epoch£º993	 i:2 	 global-step:19862	 l-p:0.1218964159488678
epoch£º993	 i:3 	 global-step:19863	 l-p:0.151605486869812
epoch£º993	 i:4 	 global-step:19864	 l-p:0.09567830711603165
epoch£º993	 i:5 	 global-step:19865	 l-p:0.09799892455339432
epoch£º993	 i:6 	 global-step:19866	 l-p:0.12184027582406998
epoch£º993	 i:7 	 global-step:19867	 l-p:0.13083240389823914
epoch£º993	 i:8 	 global-step:19868	 l-p:0.1374787986278534
epoch£º993	 i:9 	 global-step:19869	 l-p:0.13001756370067596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:994
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1352, 5.1302, 5.1349],
        [5.1352, 5.1559, 4.8606],
        [5.1352, 4.8904, 4.5864],
        [5.1352, 5.1350, 5.1352]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:994, step:0 
model_pd.l_p.mean(): 0.13274464011192322 
model_pd.l_d.mean(): -20.2080135345459 
model_pd.lagr.mean(): -20.07526969909668 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4743], device='cuda:0')), ('power', tensor([-21.0771], device='cuda:0'))])
epoch£º994	 i:0 	 global-step:19880	 l-p:0.13274464011192322
epoch£º994	 i:1 	 global-step:19881	 l-p:0.11206924915313721
epoch£º994	 i:2 	 global-step:19882	 l-p:0.11658880859613419
epoch£º994	 i:3 	 global-step:19883	 l-p:0.1459478884935379
epoch£º994	 i:4 	 global-step:19884	 l-p:0.12410962581634521
epoch£º994	 i:5 	 global-step:19885	 l-p:0.13156795501708984
epoch£º994	 i:6 	 global-step:19886	 l-p:0.5757701992988586
epoch£º994	 i:7 	 global-step:19887	 l-p:0.1992543339729309
epoch£º994	 i:8 	 global-step:19888	 l-p:0.18681220710277557
epoch£º994	 i:9 	 global-step:19889	 l-p:0.17401282489299774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:995
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1131, 4.9910, 5.0510],
        [5.1131, 4.9071, 4.5772],
        [5.1131, 4.9122, 4.9455],
        [5.1131, 5.1109, 5.1130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:995, step:0 
model_pd.l_p.mean(): 0.17501609027385712 
model_pd.l_d.mean(): -20.562305450439453 
model_pd.lagr.mean(): -20.38728904724121 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4492], device='cuda:0')), ('power', tensor([-21.4120], device='cuda:0'))])
epoch£º995	 i:0 	 global-step:19900	 l-p:0.17501609027385712
epoch£º995	 i:1 	 global-step:19901	 l-p:0.11263392865657806
epoch£º995	 i:2 	 global-step:19902	 l-p:0.12751780450344086
epoch£º995	 i:3 	 global-step:19903	 l-p:0.2664312422275543
epoch£º995	 i:4 	 global-step:19904	 l-p:0.156203955411911
epoch£º995	 i:5 	 global-step:19905	 l-p:0.2907171845436096
epoch£º995	 i:6 	 global-step:19906	 l-p:0.11934909224510193
epoch£º995	 i:7 	 global-step:19907	 l-p:0.0861014723777771
epoch£º995	 i:8 	 global-step:19908	 l-p:0.15224596858024597
epoch£º995	 i:9 	 global-step:19909	 l-p:0.12286155670881271
====================================================================================================
====================================================================================================
====================================================================================================

epoch:996
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1317, 4.9067, 4.9161],
        [5.1317, 5.4400, 5.2985],
        [5.1317, 4.8404, 4.6498],
        [5.1317, 5.1310, 5.1316]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:996, step:0 
model_pd.l_p.mean(): 0.11201845109462738 
model_pd.l_d.mean(): -18.52076530456543 
model_pd.lagr.mean(): -18.40874671936035 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5969], device='cuda:0')), ('power', tensor([-19.4854], device='cuda:0'))])
epoch£º996	 i:0 	 global-step:19920	 l-p:0.11201845109462738
epoch£º996	 i:1 	 global-step:19921	 l-p:0.24687117338180542
epoch£º996	 i:2 	 global-step:19922	 l-p:0.13822296261787415
epoch£º996	 i:3 	 global-step:19923	 l-p:0.1493823230266571
epoch£º996	 i:4 	 global-step:19924	 l-p:0.17369821667671204
epoch£º996	 i:5 	 global-step:19925	 l-p:0.14276455342769623
epoch£º996	 i:6 	 global-step:19926	 l-p:0.1286027878522873
epoch£º996	 i:7 	 global-step:19927	 l-p:0.13950906693935394
epoch£º996	 i:8 	 global-step:19928	 l-p:0.12777607142925262
epoch£º996	 i:9 	 global-step:19929	 l-p:0.11877017468214035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:997
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1550, 5.0759, 5.1272],
        [5.1550, 5.1550, 5.1550],
        [5.1550, 4.8692, 4.7344],
        [5.1550, 5.2210, 4.9451]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:997, step:0 
model_pd.l_p.mean(): 0.1513618528842926 
model_pd.l_d.mean(): -19.779956817626953 
model_pd.lagr.mean(): -19.62859535217285 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5392], device='cuda:0')), ('power', tensor([-20.7083], device='cuda:0'))])
epoch£º997	 i:0 	 global-step:19940	 l-p:0.1513618528842926
epoch£º997	 i:1 	 global-step:19941	 l-p:0.13941697776317596
epoch£º997	 i:2 	 global-step:19942	 l-p:0.11494828760623932
epoch£º997	 i:3 	 global-step:19943	 l-p:0.10994598269462585
epoch£º997	 i:4 	 global-step:19944	 l-p:0.1689111739397049
epoch£º997	 i:5 	 global-step:19945	 l-p:0.18790271878242493
epoch£º997	 i:6 	 global-step:19946	 l-p:0.14136752486228943
epoch£º997	 i:7 	 global-step:19947	 l-p:0.130772203207016
epoch£º997	 i:8 	 global-step:19948	 l-p:0.10924606025218964
epoch£º997	 i:9 	 global-step:19949	 l-p:0.19491685926914215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:998
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1435, 4.9583, 4.6254],
        [5.1435, 5.1419, 5.1435],
        [5.1435, 5.1435, 5.1435],
        [5.1435, 5.1050, 5.1358]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:998, step:0 
model_pd.l_p.mean(): 0.1354738175868988 
model_pd.l_d.mean(): -20.863130569458008 
model_pd.lagr.mean(): -20.727657318115234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3780], device='cuda:0')), ('power', tensor([-21.6447], device='cuda:0'))])
epoch£º998	 i:0 	 global-step:19960	 l-p:0.1354738175868988
epoch£º998	 i:1 	 global-step:19961	 l-p:0.08166713267564774
epoch£º998	 i:2 	 global-step:19962	 l-p:0.15637768805027008
epoch£º998	 i:3 	 global-step:19963	 l-p:0.20942793786525726
epoch£º998	 i:4 	 global-step:19964	 l-p:0.1274840235710144
epoch£º998	 i:5 	 global-step:19965	 l-p:0.1465805619955063
epoch£º998	 i:6 	 global-step:19966	 l-p:0.16128118336200714
epoch£º998	 i:7 	 global-step:19967	 l-p:0.1037500724196434
epoch£º998	 i:8 	 global-step:19968	 l-p:0.1649535894393921
epoch£º998	 i:9 	 global-step:19969	 l-p:0.14797919988632202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:999
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1570, 5.1567, 5.1570],
        [5.1570, 4.9546, 4.9847],
        [5.1570, 5.1540, 5.1569],
        [5.1570, 5.1524, 5.1568]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:999, step:0 
model_pd.l_p.mean(): 0.1508220136165619 
model_pd.l_d.mean(): -17.911954879760742 
model_pd.lagr.mean(): -17.761133193969727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6037], device='cuda:0')), ('power', tensor([-18.8722], device='cuda:0'))])
epoch£º999	 i:0 	 global-step:19980	 l-p:0.1508220136165619
epoch£º999	 i:1 	 global-step:19981	 l-p:0.09942073374986649
epoch£º999	 i:2 	 global-step:19982	 l-p:0.17735682427883148
epoch£º999	 i:3 	 global-step:19983	 l-p:0.1256626546382904
epoch£º999	 i:4 	 global-step:19984	 l-p:0.12095340341329575
epoch£º999	 i:5 	 global-step:19985	 l-p:0.1463133841753006
epoch£º999	 i:6 	 global-step:19986	 l-p:0.12936583161354065
epoch£º999	 i:7 	 global-step:19987	 l-p:0.12641936540603638
epoch£º999	 i:8 	 global-step:19988	 l-p:0.17700591683387756
epoch£º999	 i:9 	 global-step:19989	 l-p:0.11440148949623108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1000
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1667, 5.0783, 5.1326],
        [5.1667, 4.9332, 4.6237],
        [5.1667, 5.0730, 5.1286],
        [5.1667, 5.1961, 4.9039]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1000, step:0 
model_pd.l_p.mean(): 0.12286156415939331 
model_pd.l_d.mean(): -20.357688903808594 
model_pd.lagr.mean(): -20.234827041625977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4395], device='cuda:0')), ('power', tensor([-21.1935], device='cuda:0'))])
epoch£º1000	 i:0 	 global-step:20000	 l-p:0.12286156415939331
epoch£º1000	 i:1 	 global-step:20001	 l-p:0.12397735565900803
epoch£º1000	 i:2 	 global-step:20002	 l-p:0.1350497156381607
epoch£º1000	 i:3 	 global-step:20003	 l-p:0.1623927503824234
epoch£º1000	 i:4 	 global-step:20004	 l-p:0.1353565901517868
epoch£º1000	 i:5 	 global-step:20005	 l-p:0.13076987862586975
epoch£º1000	 i:6 	 global-step:20006	 l-p:0.13841165602207184
epoch£º1000	 i:7 	 global-step:20007	 l-p:0.0930267944931984
epoch£º1000	 i:8 	 global-step:20008	 l-p:0.2132827341556549
epoch£º1000	 i:9 	 global-step:20009	 l-p:0.13918475806713104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1001
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1528, 5.1516, 5.1528],
        [5.1528, 4.8973, 4.8596],
        [5.1528, 4.9869, 5.0388],
        [5.1528, 5.1528, 5.1528]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1001, step:0 
model_pd.l_p.mean(): 0.10504563897848129 
model_pd.l_d.mean(): -19.312942504882812 
model_pd.lagr.mean(): -19.207897186279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4803], device='cuda:0')), ('power', tensor([-20.1716], device='cuda:0'))])
epoch£º1001	 i:0 	 global-step:20020	 l-p:0.10504563897848129
epoch£º1001	 i:1 	 global-step:20021	 l-p:0.1913146674633026
epoch£º1001	 i:2 	 global-step:20022	 l-p:0.14136777818202972
epoch£º1001	 i:3 	 global-step:20023	 l-p:0.21276794373989105
epoch£º1001	 i:4 	 global-step:20024	 l-p:0.08788222074508667
epoch£º1001	 i:5 	 global-step:20025	 l-p:0.08446072041988373
epoch£º1001	 i:6 	 global-step:20026	 l-p:0.13045603036880493
epoch£º1001	 i:7 	 global-step:20027	 l-p:0.17165108025074005
epoch£º1001	 i:8 	 global-step:20028	 l-p:0.1890779286623001
epoch£º1001	 i:9 	 global-step:20029	 l-p:0.10714065283536911
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1002
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1478, 5.1259, 5.1449],
        [5.1478, 4.9227, 4.9313],
        [5.1478, 5.5116, 5.4046],
        [5.1478, 5.0119, 5.0710]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1002, step:0 
model_pd.l_p.mean(): 0.10471601039171219 
model_pd.l_d.mean(): -20.34626007080078 
model_pd.lagr.mean(): -20.241544723510742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4541], device='cuda:0')), ('power', tensor([-21.1971], device='cuda:0'))])
epoch£º1002	 i:0 	 global-step:20040	 l-p:0.10471601039171219
epoch£º1002	 i:1 	 global-step:20041	 l-p:0.21758460998535156
epoch£º1002	 i:2 	 global-step:20042	 l-p:0.13070358335971832
epoch£º1002	 i:3 	 global-step:20043	 l-p:0.11166662722826004
epoch£º1002	 i:4 	 global-step:20044	 l-p:0.10650249570608139
epoch£º1002	 i:5 	 global-step:20045	 l-p:0.20182010531425476
epoch£º1002	 i:6 	 global-step:20046	 l-p:0.1475740522146225
epoch£º1002	 i:7 	 global-step:20047	 l-p:0.13044440746307373
epoch£º1002	 i:8 	 global-step:20048	 l-p:0.14001016318798065
epoch£º1002	 i:9 	 global-step:20049	 l-p:0.12956665456295013
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1003
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1549, 4.9788, 4.6447],
        [5.1549, 5.1549, 5.1549],
        [5.1549, 5.1549, 5.1549],
        [5.1549, 5.1121, 5.1456]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1003, step:0 
model_pd.l_p.mean(): 0.12312395125627518 
model_pd.l_d.mean(): -19.590984344482422 
model_pd.lagr.mean(): -19.46786117553711 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5502], device='cuda:0')), ('power', tensor([-20.5272], device='cuda:0'))])
epoch£º1003	 i:0 	 global-step:20060	 l-p:0.12312395125627518
epoch£º1003	 i:1 	 global-step:20061	 l-p:0.14466339349746704
epoch£º1003	 i:2 	 global-step:20062	 l-p:0.15831683576107025
epoch£º1003	 i:3 	 global-step:20063	 l-p:0.2018653005361557
epoch£º1003	 i:4 	 global-step:20064	 l-p:0.1279008984565735
epoch£º1003	 i:5 	 global-step:20065	 l-p:0.14261361956596375
epoch£º1003	 i:6 	 global-step:20066	 l-p:0.11506929248571396
epoch£º1003	 i:7 	 global-step:20067	 l-p:0.10849252343177795
epoch£º1003	 i:8 	 global-step:20068	 l-p:0.13940966129302979
epoch£º1003	 i:9 	 global-step:20069	 l-p:0.1508731096982956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1004
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1440, 4.8538, 4.6502],
        [5.1440, 5.1288, 4.8190],
        [5.1440, 4.8947, 4.8698],
        [5.1440, 5.1437, 5.1440]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1004, step:0 
model_pd.l_p.mean(): 0.1748306304216385 
model_pd.l_d.mean(): -20.146133422851562 
model_pd.lagr.mean(): -19.971302032470703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5031], device='cuda:0')), ('power', tensor([-21.0439], device='cuda:0'))])
epoch£º1004	 i:0 	 global-step:20080	 l-p:0.1748306304216385
epoch£º1004	 i:1 	 global-step:20081	 l-p:0.1719425916671753
epoch£º1004	 i:2 	 global-step:20082	 l-p:0.13511550426483154
epoch£º1004	 i:3 	 global-step:20083	 l-p:0.13611769676208496
epoch£º1004	 i:4 	 global-step:20084	 l-p:0.13049733638763428
epoch£º1004	 i:5 	 global-step:20085	 l-p:0.12112057209014893
epoch£º1004	 i:6 	 global-step:20086	 l-p:0.10554590076208115
epoch£º1004	 i:7 	 global-step:20087	 l-p:0.1371588259935379
epoch£º1004	 i:8 	 global-step:20088	 l-p:0.16219721734523773
epoch£º1004	 i:9 	 global-step:20089	 l-p:0.2234164923429489
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1005
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1352, 5.0201, 5.0796],
        [5.1352, 4.8834, 4.8554],
        [5.1352, 5.1338, 5.1352],
        [5.1352, 4.9605, 5.0093]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1005, step:0 
model_pd.l_p.mean(): 0.1285775750875473 
model_pd.l_d.mean(): -20.358156204223633 
model_pd.lagr.mean(): -20.229578018188477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4351], device='cuda:0')), ('power', tensor([-21.1895], device='cuda:0'))])
epoch£º1005	 i:0 	 global-step:20100	 l-p:0.1285775750875473
epoch£º1005	 i:1 	 global-step:20101	 l-p:0.11495986580848694
epoch£º1005	 i:2 	 global-step:20102	 l-p:0.1440672129392624
epoch£º1005	 i:3 	 global-step:20103	 l-p:0.1836252510547638
epoch£º1005	 i:4 	 global-step:20104	 l-p:0.09840696305036545
epoch£º1005	 i:5 	 global-step:20105	 l-p:0.20044738054275513
epoch£º1005	 i:6 	 global-step:20106	 l-p:0.18448978662490845
epoch£º1005	 i:7 	 global-step:20107	 l-p:0.14964300394058228
epoch£º1005	 i:8 	 global-step:20108	 l-p:0.10871360450983047
epoch£º1005	 i:9 	 global-step:20109	 l-p:0.21376949548721313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1006
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]], device='cuda:0')
 pt:tensor([[5.1340, 5.4186, 5.2618],
        [5.1340, 4.9535, 4.6179],
        [5.1340, 5.2701, 5.0284],
        [5.1340, 5.2126, 4.9422]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1006, step:0 
model_pd.l_p.mean(): 0.1609352082014084 
model_pd.l_d.mean(): -18.98245620727539 
model_pd.lagr.mean(): -18.821521759033203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6072], device='cuda:0')), ('power', tensor([-19.9663], device='cuda:0'))])
epoch£º1006	 i:0 	 global-step:20120	 l-p:0.1609352082014084
epoch£º1006	 i:1 	 global-step:20121	 l-p:0.10595325380563736
epoch£º1006	 i:2 	 global-step:20122	 l-p:0.165313720703125
epoch£º1006	 i:3 	 global-step:20123	 l-p:0.10608629137277603
epoch£º1006	 i:4 	 global-step:20124	 l-p:0.16539569199085236
epoch£º1006	 i:5 	 global-step:20125	 l-p:0.17129987478256226
epoch£º1006	 i:6 	 global-step:20126	 l-p:0.1196712777018547
epoch£º1006	 i:7 	 global-step:20127	 l-p:0.14717182517051697
epoch£º1006	 i:8 	 global-step:20128	 l-p:0.1483902931213379
epoch£º1006	 i:9 	 global-step:20129	 l-p:0.18762969970703125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1007
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1454, 5.0892, 5.1305],
        [5.1454, 5.1447, 5.1454],
        [5.1454, 5.1454, 5.1454],
        [5.1454, 5.1232, 5.1424]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1007, step:0 
model_pd.l_p.mean(): 0.14434491097927094 
model_pd.l_d.mean(): -19.870834350585938 
model_pd.lagr.mean(): -19.726490020751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5332], device='cuda:0')), ('power', tensor([-20.7947], device='cuda:0'))])
epoch£º1007	 i:0 	 global-step:20140	 l-p:0.14434491097927094
epoch£º1007	 i:1 	 global-step:20141	 l-p:0.22450599074363708
epoch£º1007	 i:2 	 global-step:20142	 l-p:0.12965478003025055
epoch£º1007	 i:3 	 global-step:20143	 l-p:0.11754538118839264
epoch£º1007	 i:4 	 global-step:20144	 l-p:0.1430969089269638
epoch£º1007	 i:5 	 global-step:20145	 l-p:0.11572618037462234
epoch£º1007	 i:6 	 global-step:20146	 l-p:0.16230729222297668
epoch£º1007	 i:7 	 global-step:20147	 l-p:0.11636210232973099
epoch£º1007	 i:8 	 global-step:20148	 l-p:0.13267332315444946
epoch£º1007	 i:9 	 global-step:20149	 l-p:0.14891499280929565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1008
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1506, 4.9507, 4.9834],
        [5.1506, 5.1161, 5.1443],
        [5.1506, 4.9520, 4.6224],
        [5.1506, 4.8953, 4.6021]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1008, step:0 
model_pd.l_p.mean(): 0.09026086330413818 
model_pd.l_d.mean(): -20.508689880371094 
model_pd.lagr.mean(): -20.418428421020508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4571], device='cuda:0')), ('power', tensor([-21.3656], device='cuda:0'))])
epoch£º1008	 i:0 	 global-step:20160	 l-p:0.09026086330413818
epoch£º1008	 i:1 	 global-step:20161	 l-p:0.17504319548606873
epoch£º1008	 i:2 	 global-step:20162	 l-p:0.10564280301332474
epoch£º1008	 i:3 	 global-step:20163	 l-p:0.17829570174217224
epoch£º1008	 i:4 	 global-step:20164	 l-p:0.15120531618595123
epoch£º1008	 i:5 	 global-step:20165	 l-p:0.17526626586914062
epoch£º1008	 i:6 	 global-step:20166	 l-p:0.10439954698085785
epoch£º1008	 i:7 	 global-step:20167	 l-p:0.13998912274837494
epoch£º1008	 i:8 	 global-step:20168	 l-p:0.13930541276931763
epoch£º1008	 i:9 	 global-step:20169	 l-p:0.1418846696615219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1009
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1658, 5.1629, 5.1657],
        [5.1658, 5.1658, 5.1658],
        [5.1658, 4.9742, 5.0123],
        [5.1658, 5.0436, 5.1031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1009, step:0 
model_pd.l_p.mean(): 0.17625965178012848 
model_pd.l_d.mean(): -20.596887588500977 
model_pd.lagr.mean(): -20.42062759399414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4294], device='cuda:0')), ('power', tensor([-21.4268], device='cuda:0'))])
epoch£º1009	 i:0 	 global-step:20180	 l-p:0.17625965178012848
epoch£º1009	 i:1 	 global-step:20181	 l-p:0.11584776639938354
epoch£º1009	 i:2 	 global-step:20182	 l-p:0.1515093743801117
epoch£º1009	 i:3 	 global-step:20183	 l-p:0.10573188215494156
epoch£º1009	 i:4 	 global-step:20184	 l-p:0.17872470617294312
epoch£º1009	 i:5 	 global-step:20185	 l-p:0.1119437888264656
epoch£º1009	 i:6 	 global-step:20186	 l-p:0.13745331764221191
epoch£º1009	 i:7 	 global-step:20187	 l-p:0.12276016920804977
epoch£º1009	 i:8 	 global-step:20188	 l-p:0.0861474946141243
epoch£º1009	 i:9 	 global-step:20189	 l-p:0.16092227399349213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1010
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1719, 5.0111, 4.6763],
        [5.1719, 5.1718, 5.1719],
        [5.1719, 5.1719, 5.1719],
        [5.1719, 5.1497, 5.1689]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1010, step:0 
model_pd.l_p.mean(): 0.12456510961055756 
model_pd.l_d.mean(): -19.974218368530273 
model_pd.lagr.mean(): -19.849653244018555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4856], device='cuda:0')), ('power', tensor([-20.8506], device='cuda:0'))])
epoch£º1010	 i:0 	 global-step:20200	 l-p:0.12456510961055756
epoch£º1010	 i:1 	 global-step:20201	 l-p:0.151763454079628
epoch£º1010	 i:2 	 global-step:20202	 l-p:0.06831112504005432
epoch£º1010	 i:3 	 global-step:20203	 l-p:0.18378375470638275
epoch£º1010	 i:4 	 global-step:20204	 l-p:0.18924205005168915
epoch£º1010	 i:5 	 global-step:20205	 l-p:0.12458203732967377
epoch£º1010	 i:6 	 global-step:20206	 l-p:0.11366597563028336
epoch£º1010	 i:7 	 global-step:20207	 l-p:0.11361242085695267
epoch£º1010	 i:8 	 global-step:20208	 l-p:0.0972982794046402
epoch£º1010	 i:9 	 global-step:20209	 l-p:0.18540580570697784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1011
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1644, 5.2312, 4.9551],
        [5.1644, 5.2613, 4.9994],
        [5.1644, 5.2512, 4.9845],
        [5.1644, 5.1639, 5.1644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1011, step:0 
model_pd.l_p.mean(): 0.2185489386320114 
model_pd.l_d.mean(): -18.276397705078125 
model_pd.lagr.mean(): -18.05784797668457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6144], device='cuda:0')), ('power', tensor([-19.2545], device='cuda:0'))])
epoch£º1011	 i:0 	 global-step:20220	 l-p:0.2185489386320114
epoch£º1011	 i:1 	 global-step:20221	 l-p:0.09217783063650131
epoch£º1011	 i:2 	 global-step:20222	 l-p:0.14568747580051422
epoch£º1011	 i:3 	 global-step:20223	 l-p:0.11707393079996109
epoch£º1011	 i:4 	 global-step:20224	 l-p:0.10401123017072678
epoch£º1011	 i:5 	 global-step:20225	 l-p:0.14730599522590637
epoch£º1011	 i:6 	 global-step:20226	 l-p:0.13352565467357635
epoch£º1011	 i:7 	 global-step:20227	 l-p:0.1410244107246399
epoch£º1011	 i:8 	 global-step:20228	 l-p:0.12428735196590424
epoch£º1011	 i:9 	 global-step:20229	 l-p:0.13973478972911835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1012
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1646, 5.0076, 5.0625],
        [5.1646, 5.1599, 5.1644],
        [5.1646, 4.8784, 4.6602],
        [5.1646, 5.0469, 5.1063]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1012, step:0 
model_pd.l_p.mean(): 0.131212055683136 
model_pd.l_d.mean(): -20.59311294555664 
model_pd.lagr.mean(): -20.46190071105957 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4211], device='cuda:0')), ('power', tensor([-21.4143], device='cuda:0'))])
epoch£º1012	 i:0 	 global-step:20240	 l-p:0.131212055683136
epoch£º1012	 i:1 	 global-step:20241	 l-p:0.1433628499507904
epoch£º1012	 i:2 	 global-step:20242	 l-p:0.18487529456615448
epoch£º1012	 i:3 	 global-step:20243	 l-p:0.1875821202993393
epoch£º1012	 i:4 	 global-step:20244	 l-p:0.12450500577688217
epoch£º1012	 i:5 	 global-step:20245	 l-p:0.11339414119720459
epoch£º1012	 i:6 	 global-step:20246	 l-p:0.13686850666999817
epoch£º1012	 i:7 	 global-step:20247	 l-p:0.14725151658058167
epoch£º1012	 i:8 	 global-step:20248	 l-p:0.13184495270252228
epoch£º1012	 i:9 	 global-step:20249	 l-p:0.08523573726415634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1013
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1555, 5.0366, 4.7010],
        [5.1555, 5.1555, 5.1555],
        [5.1555, 4.8656, 4.7036],
        [5.1555, 4.8659, 4.6649]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1013, step:0 
model_pd.l_p.mean(): 0.1483665406703949 
model_pd.l_d.mean(): -20.45396614074707 
model_pd.lagr.mean(): -20.305599212646484 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4563], device='cuda:0')), ('power', tensor([-21.3090], device='cuda:0'))])
epoch£º1013	 i:0 	 global-step:20260	 l-p:0.1483665406703949
epoch£º1013	 i:1 	 global-step:20261	 l-p:0.133071169257164
epoch£º1013	 i:2 	 global-step:20262	 l-p:0.12534819543361664
epoch£º1013	 i:3 	 global-step:20263	 l-p:0.2281700223684311
epoch£º1013	 i:4 	 global-step:20264	 l-p:0.1082974523305893
epoch£º1013	 i:5 	 global-step:20265	 l-p:0.11798366159200668
epoch£º1013	 i:6 	 global-step:20266	 l-p:0.10352059453725815
epoch£º1013	 i:7 	 global-step:20267	 l-p:0.15323537588119507
epoch£º1013	 i:8 	 global-step:20268	 l-p:0.1680334061384201
epoch£º1013	 i:9 	 global-step:20269	 l-p:0.11674092710018158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1014
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1503, 5.5802, 5.5168],
        [5.1503, 5.1499, 5.1503],
        [5.1503, 5.1502, 5.1503],
        [5.1503, 4.8595, 4.6650]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1014, step:0 
model_pd.l_p.mean(): 0.14264516532421112 
model_pd.l_d.mean(): -19.747037887573242 
model_pd.lagr.mean(): -19.604393005371094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4845], device='cuda:0')), ('power', tensor([-20.6181], device='cuda:0'))])
epoch£º1014	 i:0 	 global-step:20280	 l-p:0.14264516532421112
epoch£º1014	 i:1 	 global-step:20281	 l-p:0.20238569378852844
epoch£º1014	 i:2 	 global-step:20282	 l-p:0.11718239635229111
epoch£º1014	 i:3 	 global-step:20283	 l-p:0.09850753843784332
epoch£º1014	 i:4 	 global-step:20284	 l-p:0.12704388797283173
epoch£º1014	 i:5 	 global-step:20285	 l-p:0.20013417303562164
epoch£º1014	 i:6 	 global-step:20286	 l-p:0.12347753345966339
epoch£º1014	 i:7 	 global-step:20287	 l-p:0.20022174715995789
epoch£º1014	 i:8 	 global-step:20288	 l-p:0.09798458218574524
epoch£º1014	 i:9 	 global-step:20289	 l-p:0.1584281623363495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1015
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1339, 4.9704, 4.6323],
        [5.1339, 5.1278, 5.1336],
        [5.1339, 5.1320, 5.1339],
        [5.1339, 5.4967, 5.3889]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1015, step:0 
model_pd.l_p.mean(): 0.15446099638938904 
model_pd.l_d.mean(): -20.03788185119629 
model_pd.lagr.mean(): -19.883420944213867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5128], device='cuda:0')), ('power', tensor([-20.9437], device='cuda:0'))])
epoch£º1015	 i:0 	 global-step:20300	 l-p:0.15446099638938904
epoch£º1015	 i:1 	 global-step:20301	 l-p:0.13902254402637482
epoch£º1015	 i:2 	 global-step:20302	 l-p:0.12158466130495071
epoch£º1015	 i:3 	 global-step:20303	 l-p:0.15665371716022491
epoch£º1015	 i:4 	 global-step:20304	 l-p:0.10291910916566849
epoch£º1015	 i:5 	 global-step:20305	 l-p:0.23332102596759796
epoch£º1015	 i:6 	 global-step:20306	 l-p:0.16231416165828705
epoch£º1015	 i:7 	 global-step:20307	 l-p:0.1522006243467331
epoch£º1015	 i:8 	 global-step:20308	 l-p:0.2984411120414734
epoch£º1015	 i:9 	 global-step:20309	 l-p:0.12302279472351074
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1016
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1209, 5.1205, 5.1209],
        [5.1209, 5.1209, 5.1209],
        [5.1209, 5.0640, 5.1057],
        [5.1209, 5.1193, 5.1208]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1016, step:0 
model_pd.l_p.mean(): 0.1449289619922638 
model_pd.l_d.mean(): -18.9382381439209 
model_pd.lagr.mean(): -18.79330825805664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4947], device='cuda:0')), ('power', tensor([-19.8047], device='cuda:0'))])
epoch£º1016	 i:0 	 global-step:20320	 l-p:0.1449289619922638
epoch£º1016	 i:1 	 global-step:20321	 l-p:0.2515623867511749
epoch£º1016	 i:2 	 global-step:20322	 l-p:0.1086927279829979
epoch£º1016	 i:3 	 global-step:20323	 l-p:0.14452779293060303
epoch£º1016	 i:4 	 global-step:20324	 l-p:0.10425940155982971
epoch£º1016	 i:5 	 global-step:20325	 l-p:0.10611911863088608
epoch£º1016	 i:6 	 global-step:20326	 l-p:0.2933439314365387
epoch£º1016	 i:7 	 global-step:20327	 l-p:0.15688854455947876
epoch£º1016	 i:8 	 global-step:20328	 l-p:0.21012520790100098
epoch£º1016	 i:9 	 global-step:20329	 l-p:0.10456494987010956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1017
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1253, 5.1252, 5.1253],
        [5.1253, 5.0322, 5.0881],
        [5.1253, 4.8874, 4.8829],
        [5.1253, 5.1253, 5.1253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1017, step:0 
model_pd.l_p.mean(): 0.11612667888402939 
model_pd.l_d.mean(): -19.98764419555664 
model_pd.lagr.mean(): -19.871517181396484 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5035], device='cuda:0')), ('power', tensor([-20.8829], device='cuda:0'))])
epoch£º1017	 i:0 	 global-step:20340	 l-p:0.11612667888402939
epoch£º1017	 i:1 	 global-step:20341	 l-p:0.13006164133548737
epoch£º1017	 i:2 	 global-step:20342	 l-p:0.1298213005065918
epoch£º1017	 i:3 	 global-step:20343	 l-p:0.33403027057647705
epoch£º1017	 i:4 	 global-step:20344	 l-p:0.14706198871135712
epoch£º1017	 i:5 	 global-step:20345	 l-p:0.11743935942649841
epoch£º1017	 i:6 	 global-step:20346	 l-p:0.13007020950317383
epoch£º1017	 i:7 	 global-step:20347	 l-p:0.1744614541530609
epoch£º1017	 i:8 	 global-step:20348	 l-p:0.20610716938972473
epoch£º1017	 i:9 	 global-step:20349	 l-p:0.16022907197475433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1018
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1199, 5.1151, 5.1196],
        [5.1199, 5.0365, 5.0895],
        [5.1199, 5.0822, 4.7638],
        [5.1199, 4.8382, 4.7438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1018, step:0 
model_pd.l_p.mean(): 0.1212841123342514 
model_pd.l_d.mean(): -19.499832153320312 
model_pd.lagr.mean(): -19.37854766845703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5093], device='cuda:0')), ('power', tensor([-20.3920], device='cuda:0'))])
epoch£º1018	 i:0 	 global-step:20360	 l-p:0.1212841123342514
epoch£º1018	 i:1 	 global-step:20361	 l-p:0.41983646154403687
epoch£º1018	 i:2 	 global-step:20362	 l-p:0.144076868891716
epoch£º1018	 i:3 	 global-step:20363	 l-p:0.1693144142627716
epoch£º1018	 i:4 	 global-step:20364	 l-p:0.13387571275234222
epoch£º1018	 i:5 	 global-step:20365	 l-p:0.09673518687486649
epoch£º1018	 i:6 	 global-step:20366	 l-p:0.19165749847888947
epoch£º1018	 i:7 	 global-step:20367	 l-p:0.1377391368150711
epoch£º1018	 i:8 	 global-step:20368	 l-p:0.12404435873031616
epoch£º1018	 i:9 	 global-step:20369	 l-p:0.12816360592842102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1019
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1265, 5.1156, 5.1256],
        [5.1265, 4.8558, 4.7914],
        [5.1265, 5.1416, 4.8428],
        [5.1265, 5.5193, 5.4314]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1019, step:0 
model_pd.l_p.mean(): 0.13953951001167297 
model_pd.l_d.mean(): -20.3834285736084 
model_pd.lagr.mean(): -20.24388885498047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4350], device='cuda:0')), ('power', tensor([-21.2151], device='cuda:0'))])
epoch£º1019	 i:0 	 global-step:20380	 l-p:0.13953951001167297
epoch£º1019	 i:1 	 global-step:20381	 l-p:0.19783945381641388
epoch£º1019	 i:2 	 global-step:20382	 l-p:0.19794154167175293
epoch£º1019	 i:3 	 global-step:20383	 l-p:0.13981486856937408
epoch£º1019	 i:4 	 global-step:20384	 l-p:0.13233987987041473
epoch£º1019	 i:5 	 global-step:20385	 l-p:0.24761494994163513
epoch£º1019	 i:6 	 global-step:20386	 l-p:0.12180237472057343
epoch£º1019	 i:7 	 global-step:20387	 l-p:0.17540378868579865
epoch£º1019	 i:8 	 global-step:20388	 l-p:0.08971988409757614
epoch£º1019	 i:9 	 global-step:20389	 l-p:0.11748788505792618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1020
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1349, 4.8634, 4.5877],
        [5.1349, 5.0433, 5.0987],
        [5.1349, 4.8420, 4.6472],
        [5.1349, 4.9903, 4.6514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1020, step:0 
model_pd.l_p.mean(): 0.13580888509750366 
model_pd.l_d.mean(): -20.318878173828125 
model_pd.lagr.mean(): -20.183069229125977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4657], device='cuda:0')), ('power', tensor([-21.1812], device='cuda:0'))])
epoch£º1020	 i:0 	 global-step:20400	 l-p:0.13580888509750366
epoch£º1020	 i:1 	 global-step:20401	 l-p:0.2197878658771515
epoch£º1020	 i:2 	 global-step:20402	 l-p:0.1669187694787979
epoch£º1020	 i:3 	 global-step:20403	 l-p:0.2479446679353714
epoch£º1020	 i:4 	 global-step:20404	 l-p:0.10002509504556656
epoch£º1020	 i:5 	 global-step:20405	 l-p:0.07192175835371017
epoch£º1020	 i:6 	 global-step:20406	 l-p:0.12553702294826508
epoch£º1020	 i:7 	 global-step:20407	 l-p:0.16667896509170532
epoch£º1020	 i:8 	 global-step:20408	 l-p:0.12629061937332153
epoch£º1020	 i:9 	 global-step:20409	 l-p:0.13054601848125458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1021
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1455, 4.8552, 4.7072],
        [5.1455, 5.1455, 5.1455],
        [5.1455, 4.8654, 4.6114],
        [5.1455, 4.8828, 4.8337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1021, step:0 
model_pd.l_p.mean(): 0.12877157330513 
model_pd.l_d.mean(): -20.578428268432617 
model_pd.lagr.mean(): -20.449657440185547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4362], device='cuda:0')), ('power', tensor([-21.4150], device='cuda:0'))])
epoch£º1021	 i:0 	 global-step:20420	 l-p:0.12877157330513
epoch£º1021	 i:1 	 global-step:20421	 l-p:0.1231866404414177
epoch£º1021	 i:2 	 global-step:20422	 l-p:0.10850498080253601
epoch£º1021	 i:3 	 global-step:20423	 l-p:0.19810687005519867
epoch£º1021	 i:4 	 global-step:20424	 l-p:0.12808464467525482
epoch£º1021	 i:5 	 global-step:20425	 l-p:0.13689155876636505
epoch£º1021	 i:6 	 global-step:20426	 l-p:0.2049531638622284
epoch£º1021	 i:7 	 global-step:20427	 l-p:0.21709458529949188
epoch£º1021	 i:8 	 global-step:20428	 l-p:0.10309822112321854
epoch£º1021	 i:9 	 global-step:20429	 l-p:0.14317703247070312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1022
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1360, 5.1359, 5.1360],
        [5.1360, 4.8644, 4.5886],
        [5.1360, 4.9309, 4.9610],
        [5.1360, 5.1360, 5.1360]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1022, step:0 
model_pd.l_p.mean(): 0.14054171741008759 
model_pd.l_d.mean(): -20.526718139648438 
model_pd.lagr.mean(): -20.38617706298828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4334], device='cuda:0')), ('power', tensor([-21.3595], device='cuda:0'))])
epoch£º1022	 i:0 	 global-step:20440	 l-p:0.14054171741008759
epoch£º1022	 i:1 	 global-step:20441	 l-p:0.15480385720729828
epoch£º1022	 i:2 	 global-step:20442	 l-p:0.16507305204868317
epoch£º1022	 i:3 	 global-step:20443	 l-p:0.1332574486732483
epoch£º1022	 i:4 	 global-step:20444	 l-p:0.1123676672577858
epoch£º1022	 i:5 	 global-step:20445	 l-p:0.10859598219394684
epoch£º1022	 i:6 	 global-step:20446	 l-p:0.18926241993904114
epoch£º1022	 i:7 	 global-step:20447	 l-p:0.14264187216758728
epoch£º1022	 i:8 	 global-step:20448	 l-p:0.26599210500717163
epoch£º1022	 i:9 	 global-step:20449	 l-p:0.11628798395395279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1023
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1348, 5.1341, 5.1348],
        [5.1348, 4.8494, 4.6061],
        [5.1348, 5.1348, 5.1348],
        [5.1348, 4.9651, 5.0166]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1023, step:0 
model_pd.l_p.mean(): 0.1442195326089859 
model_pd.l_d.mean(): -20.895427703857422 
model_pd.lagr.mean(): -20.75120735168457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3710], device='cuda:0')), ('power', tensor([-21.6705], device='cuda:0'))])
epoch£º1023	 i:0 	 global-step:20460	 l-p:0.1442195326089859
epoch£º1023	 i:1 	 global-step:20461	 l-p:0.23579464852809906
epoch£º1023	 i:2 	 global-step:20462	 l-p:0.1184309720993042
epoch£º1023	 i:3 	 global-step:20463	 l-p:0.253749281167984
epoch£º1023	 i:4 	 global-step:20464	 l-p:0.17366309463977814
epoch£º1023	 i:5 	 global-step:20465	 l-p:0.13137459754943848
epoch£º1023	 i:6 	 global-step:20466	 l-p:0.12168660759925842
epoch£º1023	 i:7 	 global-step:20467	 l-p:0.0855245366692543
epoch£º1023	 i:8 	 global-step:20468	 l-p:0.12175525724887848
epoch£º1023	 i:9 	 global-step:20469	 l-p:0.13269676268100739
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1024
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1396, 5.1396, 5.1396],
        [5.1396, 5.1063, 4.7894],
        [5.1396, 5.0810, 4.7563],
        [5.1396, 5.1396, 5.1396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1024, step:0 
model_pd.l_p.mean(): 0.19740986824035645 
model_pd.l_d.mean(): -19.561731338500977 
model_pd.lagr.mean(): -19.364320755004883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5399], device='cuda:0')), ('power', tensor([-20.4867], device='cuda:0'))])
epoch£º1024	 i:0 	 global-step:20480	 l-p:0.19740986824035645
epoch£º1024	 i:1 	 global-step:20481	 l-p:0.1334025114774704
epoch£º1024	 i:2 	 global-step:20482	 l-p:0.11550144851207733
epoch£º1024	 i:3 	 global-step:20483	 l-p:0.22547347843647003
epoch£º1024	 i:4 	 global-step:20484	 l-p:0.12845119833946228
epoch£º1024	 i:5 	 global-step:20485	 l-p:0.09286681562662125
epoch£º1024	 i:6 	 global-step:20486	 l-p:0.17422282695770264
epoch£º1024	 i:7 	 global-step:20487	 l-p:0.11837729811668396
epoch£º1024	 i:8 	 global-step:20488	 l-p:0.12038630992174149
epoch£º1024	 i:9 	 global-step:20489	 l-p:0.17001567780971527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1025
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1443, 5.1429, 5.1443],
        [5.1443, 5.1389, 5.1441],
        [5.1443, 5.1443, 5.1443],
        [5.1443, 5.1435, 5.1443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1025, step:0 
model_pd.l_p.mean(): 0.18380358815193176 
model_pd.l_d.mean(): -20.574920654296875 
model_pd.lagr.mean(): -20.391117095947266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4137], device='cuda:0')), ('power', tensor([-21.3881], device='cuda:0'))])
epoch£º1025	 i:0 	 global-step:20500	 l-p:0.18380358815193176
epoch£º1025	 i:1 	 global-step:20501	 l-p:0.1496851146221161
epoch£º1025	 i:2 	 global-step:20502	 l-p:0.13360024988651276
epoch£º1025	 i:3 	 global-step:20503	 l-p:0.12834830582141876
epoch£º1025	 i:4 	 global-step:20504	 l-p:0.16621236503124237
epoch£º1025	 i:5 	 global-step:20505	 l-p:0.12185283750295639
epoch£º1025	 i:6 	 global-step:20506	 l-p:0.12529505789279938
epoch£º1025	 i:7 	 global-step:20507	 l-p:0.11963058263063431
epoch£º1025	 i:8 	 global-step:20508	 l-p:0.16762453317642212
epoch£º1025	 i:9 	 global-step:20509	 l-p:0.1562497913837433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1026
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7151,  0.6395,  1.0000,  0.5719,
          1.0000,  0.8943, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4406,  0.3353,  1.0000,  0.2551,
          1.0000,  0.7609, 31.6228]], device='cuda:0')
 pt:tensor([[5.1480, 5.5479, 5.4638],
        [5.1480, 4.8579, 4.6439],
        [5.1480, 5.1743, 4.8798],
        [5.1480, 4.8842, 4.5986]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1026, step:0 
model_pd.l_p.mean(): 0.0771377757191658 
model_pd.l_d.mean(): -19.783584594726562 
model_pd.lagr.mean(): -19.70644760131836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5357], device='cuda:0')), ('power', tensor([-20.7083], device='cuda:0'))])
epoch£º1026	 i:0 	 global-step:20520	 l-p:0.0771377757191658
epoch£º1026	 i:1 	 global-step:20521	 l-p:0.1295214295387268
epoch£º1026	 i:2 	 global-step:20522	 l-p:0.16177627444267273
epoch£º1026	 i:3 	 global-step:20523	 l-p:0.19473697245121002
epoch£º1026	 i:4 	 global-step:20524	 l-p:0.1564481109380722
epoch£º1026	 i:5 	 global-step:20525	 l-p:0.13999414443969727
epoch£º1026	 i:6 	 global-step:20526	 l-p:0.1385868638753891
epoch£º1026	 i:7 	 global-step:20527	 l-p:0.1489974409341812
epoch£º1026	 i:8 	 global-step:20528	 l-p:0.13902844488620758
epoch£º1026	 i:9 	 global-step:20529	 l-p:0.15200768411159515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1027
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1507, 4.8586, 4.6819],
        [5.1507, 4.9838, 4.6465],
        [5.1507, 5.0601, 5.1152],
        [5.1507, 4.8877, 4.8379]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1027, step:0 
model_pd.l_p.mean(): 0.1355300098657608 
model_pd.l_d.mean(): -19.820117950439453 
model_pd.lagr.mean(): -19.684587478637695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5430], device='cuda:0')), ('power', tensor([-20.7532], device='cuda:0'))])
epoch£º1027	 i:0 	 global-step:20540	 l-p:0.1355300098657608
epoch£º1027	 i:1 	 global-step:20541	 l-p:0.13488121330738068
epoch£º1027	 i:2 	 global-step:20542	 l-p:0.1510762721300125
epoch£º1027	 i:3 	 global-step:20543	 l-p:0.13835129141807556
epoch£º1027	 i:4 	 global-step:20544	 l-p:0.09723351895809174
epoch£º1027	 i:5 	 global-step:20545	 l-p:0.23151175677776337
epoch£º1027	 i:6 	 global-step:20546	 l-p:0.15836773812770844
epoch£º1027	 i:7 	 global-step:20547	 l-p:0.12457992136478424
epoch£º1027	 i:8 	 global-step:20548	 l-p:0.1316693127155304
epoch£º1027	 i:9 	 global-step:20549	 l-p:0.1433226615190506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1028
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1452, 5.1445, 5.1452],
        [5.1452, 5.1452, 5.1452],
        [5.1452, 5.1406, 5.1450],
        [5.1452, 5.1232, 5.1423]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1028, step:0 
model_pd.l_p.mean(): 0.18044352531433105 
model_pd.l_d.mean(): -20.813133239746094 
model_pd.lagr.mean(): -20.6326904296875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3774], device='cuda:0')), ('power', tensor([-21.5932], device='cuda:0'))])
epoch£º1028	 i:0 	 global-step:20560	 l-p:0.18044352531433105
epoch£º1028	 i:1 	 global-step:20561	 l-p:0.18836629390716553
epoch£º1028	 i:2 	 global-step:20562	 l-p:0.1124485582113266
epoch£º1028	 i:3 	 global-step:20563	 l-p:0.14940626919269562
epoch£º1028	 i:4 	 global-step:20564	 l-p:0.2028147131204605
epoch£º1028	 i:5 	 global-step:20565	 l-p:0.15727435052394867
epoch£º1028	 i:6 	 global-step:20566	 l-p:0.10600423812866211
epoch£º1028	 i:7 	 global-step:20567	 l-p:0.12910719215869904
epoch£º1028	 i:8 	 global-step:20568	 l-p:0.10706385225057602
epoch£º1028	 i:9 	 global-step:20569	 l-p:0.11609486490488052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1029
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1526, 4.8852, 4.6057],
        [5.1526, 5.1526, 5.1526],
        [5.1526, 5.1526, 5.1526],
        [5.1526, 4.9400, 4.9628]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1029, step:0 
model_pd.l_p.mean(): 0.1299547553062439 
model_pd.l_d.mean(): -20.671695709228516 
model_pd.lagr.mean(): -20.54174041748047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4017], device='cuda:0')), ('power', tensor([-21.4742], device='cuda:0'))])
epoch£º1029	 i:0 	 global-step:20580	 l-p:0.1299547553062439
epoch£º1029	 i:1 	 global-step:20581	 l-p:0.12835858762264252
epoch£º1029	 i:2 	 global-step:20582	 l-p:0.08747996389865875
epoch£º1029	 i:3 	 global-step:20583	 l-p:0.1658659130334854
epoch£º1029	 i:4 	 global-step:20584	 l-p:0.2535100281238556
epoch£º1029	 i:5 	 global-step:20585	 l-p:0.10790982097387314
epoch£º1029	 i:6 	 global-step:20586	 l-p:0.12429708242416382
epoch£º1029	 i:7 	 global-step:20587	 l-p:0.13210776448249817
epoch£º1029	 i:8 	 global-step:20588	 l-p:0.1061614602804184
epoch£º1029	 i:9 	 global-step:20589	 l-p:0.21473953127861023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1030
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1443, 5.1313, 5.1431],
        [5.1443, 5.1064, 5.1369],
        [5.1443, 5.2205, 4.9481],
        [5.1443, 4.9127, 4.9154]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1030, step:0 
model_pd.l_p.mean(): 0.15708790719509125 
model_pd.l_d.mean(): -20.094472885131836 
model_pd.lagr.mean(): -19.93738555908203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4894], device='cuda:0')), ('power', tensor([-20.9771], device='cuda:0'))])
epoch£º1030	 i:0 	 global-step:20600	 l-p:0.15708790719509125
epoch£º1030	 i:1 	 global-step:20601	 l-p:0.13040263950824738
epoch£º1030	 i:2 	 global-step:20602	 l-p:0.17008700966835022
epoch£º1030	 i:3 	 global-step:20603	 l-p:0.10023975372314453
epoch£º1030	 i:4 	 global-step:20604	 l-p:0.11054077744483948
epoch£º1030	 i:5 	 global-step:20605	 l-p:0.16790179908275604
epoch£º1030	 i:6 	 global-step:20606	 l-p:0.12745258212089539
epoch£º1030	 i:7 	 global-step:20607	 l-p:0.25969234108924866
epoch£º1030	 i:8 	 global-step:20608	 l-p:0.08508573472499847
epoch£º1030	 i:9 	 global-step:20609	 l-p:0.16873694956302643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1031
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1421, 5.3968, 5.2210],
        [5.1421, 4.9939, 4.6548],
        [5.1421, 4.8657, 4.7843],
        [5.1421, 5.1197, 5.1391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1031, step:0 
model_pd.l_p.mean(): 0.13092096149921417 
model_pd.l_d.mean(): -19.77048683166504 
model_pd.lagr.mean(): -19.63956642150879 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4981], device='cuda:0')), ('power', tensor([-20.6561], device='cuda:0'))])
epoch£º1031	 i:0 	 global-step:20620	 l-p:0.13092096149921417
epoch£º1031	 i:1 	 global-step:20621	 l-p:0.1665889322757721
epoch£º1031	 i:2 	 global-step:20622	 l-p:0.14488962292671204
epoch£º1031	 i:3 	 global-step:20623	 l-p:0.1798267662525177
epoch£º1031	 i:4 	 global-step:20624	 l-p:0.12516695261001587
epoch£º1031	 i:5 	 global-step:20625	 l-p:0.13184097409248352
epoch£º1031	 i:6 	 global-step:20626	 l-p:0.13273905217647552
epoch£º1031	 i:7 	 global-step:20627	 l-p:0.08913224935531616
epoch£º1031	 i:8 	 global-step:20628	 l-p:0.10454988479614258
epoch£º1031	 i:9 	 global-step:20629	 l-p:0.2512708902359009
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1032
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1478, 4.9056, 4.8939],
        [5.1478, 5.0704, 5.1212],
        [5.1478, 5.1473, 5.1478],
        [5.1478, 5.0036, 5.0622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1032, step:0 
model_pd.l_p.mean(): 0.18579615652561188 
model_pd.l_d.mean(): -20.083572387695312 
model_pd.lagr.mean(): -19.897775650024414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4746], device='cuda:0')), ('power', tensor([-20.9507], device='cuda:0'))])
epoch£º1032	 i:0 	 global-step:20640	 l-p:0.18579615652561188
epoch£º1032	 i:1 	 global-step:20641	 l-p:0.13177712261676788
epoch£º1032	 i:2 	 global-step:20642	 l-p:0.17923656105995178
epoch£º1032	 i:3 	 global-step:20643	 l-p:0.1498173624277115
epoch£º1032	 i:4 	 global-step:20644	 l-p:0.12916968762874603
epoch£º1032	 i:5 	 global-step:20645	 l-p:0.14674362540245056
epoch£º1032	 i:6 	 global-step:20646	 l-p:0.10539034008979797
epoch£º1032	 i:7 	 global-step:20647	 l-p:0.17214542627334595
epoch£º1032	 i:8 	 global-step:20648	 l-p:0.08066374808549881
epoch£º1032	 i:9 	 global-step:20649	 l-p:0.1352512091398239
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1033
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1611, 5.1611, 5.1611],
        [5.1611, 4.8704, 4.7081],
        [5.1611, 4.9410, 4.9559],
        [5.1611, 5.1599, 5.1611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1033, step:0 
model_pd.l_p.mean(): 0.07863038033246994 
model_pd.l_d.mean(): -20.39240074157715 
model_pd.lagr.mean(): -20.313770294189453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4604], device='cuda:0')), ('power', tensor([-21.2506], device='cuda:0'))])
epoch£º1033	 i:0 	 global-step:20660	 l-p:0.07863038033246994
epoch£º1033	 i:1 	 global-step:20661	 l-p:0.14031709730625153
epoch£º1033	 i:2 	 global-step:20662	 l-p:0.20731742680072784
epoch£º1033	 i:3 	 global-step:20663	 l-p:0.16213737428188324
epoch£º1033	 i:4 	 global-step:20664	 l-p:0.13152527809143066
epoch£º1033	 i:5 	 global-step:20665	 l-p:0.15304306149482727
epoch£º1033	 i:6 	 global-step:20666	 l-p:0.12979303300380707
epoch£º1033	 i:7 	 global-step:20667	 l-p:0.12375446408987045
epoch£º1033	 i:8 	 global-step:20668	 l-p:0.15569795668125153
epoch£º1033	 i:9 	 global-step:20669	 l-p:0.10207639634609222
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1034
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1615, 4.9111, 4.6113],
        [5.1615, 5.1503, 5.1606],
        [5.1615, 5.1052, 5.1465],
        [5.1615, 5.1585, 5.1614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1034, step:0 
model_pd.l_p.mean(): 0.2299901843070984 
model_pd.l_d.mean(): -20.74686050415039 
model_pd.lagr.mean(): -20.516870498657227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3866], device='cuda:0')), ('power', tensor([-21.5353], device='cuda:0'))])
epoch£º1034	 i:0 	 global-step:20680	 l-p:0.2299901843070984
epoch£º1034	 i:1 	 global-step:20681	 l-p:0.1375351846218109
epoch£º1034	 i:2 	 global-step:20682	 l-p:0.11543384194374084
epoch£º1034	 i:3 	 global-step:20683	 l-p:0.1397070735692978
epoch£º1034	 i:4 	 global-step:20684	 l-p:0.20274871587753296
epoch£º1034	 i:5 	 global-step:20685	 l-p:0.18050317466259003
epoch£º1034	 i:6 	 global-step:20686	 l-p:0.06280794739723206
epoch£º1034	 i:7 	 global-step:20687	 l-p:0.10485279560089111
epoch£º1034	 i:8 	 global-step:20688	 l-p:0.08459165692329407
epoch£º1034	 i:9 	 global-step:20689	 l-p:0.11482785642147064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1035
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1699, 5.1567, 5.1687],
        [5.1699, 5.1610, 5.1693],
        [5.1699, 4.9024, 4.6269],
        [5.1699, 5.1511, 5.1677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1035, step:0 
model_pd.l_p.mean(): 0.08683214336633682 
model_pd.l_d.mean(): -19.53781509399414 
model_pd.lagr.mean(): -19.45098304748535 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4843], device='cuda:0')), ('power', tensor([-20.4048], device='cuda:0'))])
epoch£º1035	 i:0 	 global-step:20700	 l-p:0.08683214336633682
epoch£º1035	 i:1 	 global-step:20701	 l-p:0.15069074928760529
epoch£º1035	 i:2 	 global-step:20702	 l-p:0.17408816516399384
epoch£º1035	 i:3 	 global-step:20703	 l-p:0.11648883670568466
epoch£º1035	 i:4 	 global-step:20704	 l-p:0.10010484606027603
epoch£º1035	 i:5 	 global-step:20705	 l-p:0.13958507776260376
epoch£º1035	 i:6 	 global-step:20706	 l-p:0.1398443877696991
epoch£º1035	 i:7 	 global-step:20707	 l-p:0.1257656216621399
epoch£º1035	 i:8 	 global-step:20708	 l-p:0.19762688875198364
epoch£º1035	 i:9 	 global-step:20709	 l-p:0.11648428440093994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1036
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1677, 5.1437, 5.1643],
        [5.1677, 5.6020, 5.5402],
        [5.1677, 4.9737, 5.0111],
        [5.1677, 4.9913, 5.0390]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1036, step:0 
model_pd.l_p.mean(): 0.14367495477199554 
model_pd.l_d.mean(): -19.27381134033203 
model_pd.lagr.mean(): -19.130136489868164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5483], device='cuda:0')), ('power', tensor([-20.2022], device='cuda:0'))])
epoch£º1036	 i:0 	 global-step:20720	 l-p:0.14367495477199554
epoch£º1036	 i:1 	 global-step:20721	 l-p:0.15382468700408936
epoch£º1036	 i:2 	 global-step:20722	 l-p:0.16949781775474548
epoch£º1036	 i:3 	 global-step:20723	 l-p:0.1010458767414093
epoch£º1036	 i:4 	 global-step:20724	 l-p:0.11489742249250412
epoch£º1036	 i:5 	 global-step:20725	 l-p:0.08857665956020355
epoch£º1036	 i:6 	 global-step:20726	 l-p:0.16421069204807281
epoch£º1036	 i:7 	 global-step:20727	 l-p:0.15321917831897736
epoch£º1036	 i:8 	 global-step:20728	 l-p:0.1307457834482193
epoch£º1036	 i:9 	 global-step:20729	 l-p:0.1557798981666565
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1037
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1639, 5.1431, 5.1613],
        [5.1639, 4.9822, 5.0274],
        [5.1639, 4.8733, 4.7134],
        [5.1639, 5.1597, 5.1638]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1037, step:0 
model_pd.l_p.mean(): 0.12467623502016068 
model_pd.l_d.mean(): -18.829626083374023 
model_pd.lagr.mean(): -18.7049503326416 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5490], device='cuda:0')), ('power', tensor([-19.7504], device='cuda:0'))])
epoch£º1037	 i:0 	 global-step:20740	 l-p:0.12467623502016068
epoch£º1037	 i:1 	 global-step:20741	 l-p:0.16530346870422363
epoch£º1037	 i:2 	 global-step:20742	 l-p:0.15606822073459625
epoch£º1037	 i:3 	 global-step:20743	 l-p:0.07117006927728653
epoch£º1037	 i:4 	 global-step:20744	 l-p:0.1487695872783661
epoch£º1037	 i:5 	 global-step:20745	 l-p:0.11900348216295242
epoch£º1037	 i:6 	 global-step:20746	 l-p:0.13882747292518616
epoch£º1037	 i:7 	 global-step:20747	 l-p:0.1738293468952179
epoch£º1037	 i:8 	 global-step:20748	 l-p:0.12762120366096497
epoch£º1037	 i:9 	 global-step:20749	 l-p:0.13183413445949554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1038
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1730, 4.9190, 4.8848],
        [5.1730, 5.0613, 4.7261],
        [5.1730, 4.9239, 4.8986],
        [5.1730, 5.1730, 5.1730]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1038, step:0 
model_pd.l_p.mean(): 0.1391790807247162 
model_pd.l_d.mean(): -19.65472984313965 
model_pd.lagr.mean(): -19.51555061340332 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4389], device='cuda:0')), ('power', tensor([-20.4768], device='cuda:0'))])
epoch£º1038	 i:0 	 global-step:20760	 l-p:0.1391790807247162
epoch£º1038	 i:1 	 global-step:20761	 l-p:0.09118638187646866
epoch£º1038	 i:2 	 global-step:20762	 l-p:0.12346319109201431
epoch£º1038	 i:3 	 global-step:20763	 l-p:0.1975030153989792
epoch£º1038	 i:4 	 global-step:20764	 l-p:0.08111335337162018
epoch£º1038	 i:5 	 global-step:20765	 l-p:0.17452560365200043
epoch£º1038	 i:6 	 global-step:20766	 l-p:0.15631607174873352
epoch£º1038	 i:7 	 global-step:20767	 l-p:0.10930293053388596
epoch£º1038	 i:8 	 global-step:20768	 l-p:0.16750235855579376
epoch£º1038	 i:9 	 global-step:20769	 l-p:0.11977837979793549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1039
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1677, 5.0763, 5.1316],
        [5.1677, 5.0001, 5.0518],
        [5.1677, 5.1128, 5.1534],
        [5.1677, 4.9713, 4.6404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1039, step:0 
model_pd.l_p.mean(): 0.14636580646038055 
model_pd.l_d.mean(): -20.43244743347168 
model_pd.lagr.mean(): -20.286081314086914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4626], device='cuda:0')), ('power', tensor([-21.2937], device='cuda:0'))])
epoch£º1039	 i:0 	 global-step:20780	 l-p:0.14636580646038055
epoch£º1039	 i:1 	 global-step:20781	 l-p:0.12040012329816818
epoch£º1039	 i:2 	 global-step:20782	 l-p:0.12026065587997437
epoch£º1039	 i:3 	 global-step:20783	 l-p:0.11546928435564041
epoch£º1039	 i:4 	 global-step:20784	 l-p:0.1298447698354721
epoch£º1039	 i:5 	 global-step:20785	 l-p:0.1266324818134308
epoch£º1039	 i:6 	 global-step:20786	 l-p:0.15489332377910614
epoch£º1039	 i:7 	 global-step:20787	 l-p:0.12285967916250229
epoch£º1039	 i:8 	 global-step:20788	 l-p:0.28223916888237
epoch£º1039	 i:9 	 global-step:20789	 l-p:0.09825574606657028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1040
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1489, 5.1489, 5.1489],
        [5.1489, 5.1137, 5.1424],
        [5.1489, 4.8677, 4.6131],
        [5.1489, 5.1458, 5.1488]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1040, step:0 
model_pd.l_p.mean(): 0.09219533950090408 
model_pd.l_d.mean(): -19.953275680541992 
model_pd.lagr.mean(): -19.861080169677734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5277], device='cuda:0')), ('power', tensor([-20.8729], device='cuda:0'))])
epoch£º1040	 i:0 	 global-step:20800	 l-p:0.09219533950090408
epoch£º1040	 i:1 	 global-step:20801	 l-p:0.13273470103740692
epoch£º1040	 i:2 	 global-step:20802	 l-p:0.1524602323770523
epoch£º1040	 i:3 	 global-step:20803	 l-p:0.1431913673877716
epoch£º1040	 i:4 	 global-step:20804	 l-p:0.11284024268388748
epoch£º1040	 i:5 	 global-step:20805	 l-p:0.18232305347919464
epoch£º1040	 i:6 	 global-step:20806	 l-p:0.1731337308883667
epoch£º1040	 i:7 	 global-step:20807	 l-p:0.18315787613391876
epoch£º1040	 i:8 	 global-step:20808	 l-p:0.13769276440143585
epoch£º1040	 i:9 	 global-step:20809	 l-p:0.14891216158866882
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1041
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1487, 5.1486, 5.1487],
        [5.1487, 4.8904, 4.8518],
        [5.1487, 5.1443, 5.1485],
        [5.1487, 5.1246, 5.1453]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1041, step:0 
model_pd.l_p.mean(): 0.11850953847169876 
model_pd.l_d.mean(): -18.747034072875977 
model_pd.lagr.mean(): -18.628524780273438 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5555], device='cuda:0')), ('power', tensor([-19.6729], device='cuda:0'))])
epoch£º1041	 i:0 	 global-step:20820	 l-p:0.11850953847169876
epoch£º1041	 i:1 	 global-step:20821	 l-p:0.1339271068572998
epoch£º1041	 i:2 	 global-step:20822	 l-p:0.11237817257642746
epoch£º1041	 i:3 	 global-step:20823	 l-p:0.11938468366861343
epoch£º1041	 i:4 	 global-step:20824	 l-p:0.15614716708660126
epoch£º1041	 i:5 	 global-step:20825	 l-p:0.20826169848442078
epoch£º1041	 i:6 	 global-step:20826	 l-p:0.13114510476589203
epoch£º1041	 i:7 	 global-step:20827	 l-p:0.20550067722797394
epoch£º1041	 i:8 	 global-step:20828	 l-p:0.16185890138149261
epoch£º1041	 i:9 	 global-step:20829	 l-p:0.06857022643089294
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1042
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1603, 5.1603, 5.1603],
        [5.1603, 4.9853, 5.0342],
        [5.1603, 5.1601, 5.1603],
        [5.1603, 4.9583, 4.6281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1042, step:0 
model_pd.l_p.mean(): 0.1501496434211731 
model_pd.l_d.mean(): -20.26362419128418 
model_pd.lagr.mean(): -20.113473892211914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4567], device='cuda:0')), ('power', tensor([-21.1155], device='cuda:0'))])
epoch£º1042	 i:0 	 global-step:20840	 l-p:0.1501496434211731
epoch£º1042	 i:1 	 global-step:20841	 l-p:0.12072193622589111
epoch£º1042	 i:2 	 global-step:20842	 l-p:0.13343402743339539
epoch£º1042	 i:3 	 global-step:20843	 l-p:0.11752279847860336
epoch£º1042	 i:4 	 global-step:20844	 l-p:0.1514987200498581
epoch£º1042	 i:5 	 global-step:20845	 l-p:0.09730874747037888
epoch£º1042	 i:6 	 global-step:20846	 l-p:0.1141352429986
epoch£º1042	 i:7 	 global-step:20847	 l-p:0.15747657418251038
epoch£º1042	 i:8 	 global-step:20848	 l-p:0.1783798336982727
epoch£º1042	 i:9 	 global-step:20849	 l-p:0.13454262912273407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1043
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1781, 5.1781, 5.1781],
        [5.1781, 5.0903, 5.1445],
        [5.1781, 5.5129, 5.3853],
        [5.1781, 4.9439, 4.9409]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1043, step:0 
model_pd.l_p.mean(): 0.16885022819042206 
model_pd.l_d.mean(): -20.432567596435547 
model_pd.lagr.mean(): -20.263717651367188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4468], device='cuda:0')), ('power', tensor([-21.2774], device='cuda:0'))])
epoch£º1043	 i:0 	 global-step:20860	 l-p:0.16885022819042206
epoch£º1043	 i:1 	 global-step:20861	 l-p:0.11295618116855621
epoch£º1043	 i:2 	 global-step:20862	 l-p:0.16366100311279297
epoch£º1043	 i:3 	 global-step:20863	 l-p:0.1346285492181778
epoch£º1043	 i:4 	 global-step:20864	 l-p:0.13463068008422852
epoch£º1043	 i:5 	 global-step:20865	 l-p:0.11184236407279968
epoch£º1043	 i:6 	 global-step:20866	 l-p:0.06587420403957367
epoch£º1043	 i:7 	 global-step:20867	 l-p:0.15938682854175568
epoch£º1043	 i:8 	 global-step:20868	 l-p:0.11404140293598175
epoch£º1043	 i:9 	 global-step:20869	 l-p:0.146158829331398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1044
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1821, 5.1810, 5.1821],
        [5.1821, 5.1819, 5.1821],
        [5.1821, 5.1255, 5.1669],
        [5.1821, 5.0892, 4.7570]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1044, step:0 
model_pd.l_p.mean(): 0.19596633315086365 
model_pd.l_d.mean(): -20.14911651611328 
model_pd.lagr.mean(): -19.953149795532227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4576], device='cuda:0')), ('power', tensor([-20.9998], device='cuda:0'))])
epoch£º1044	 i:0 	 global-step:20880	 l-p:0.19596633315086365
epoch£º1044	 i:1 	 global-step:20881	 l-p:0.14547725021839142
epoch£º1044	 i:2 	 global-step:20882	 l-p:0.03902854397892952
epoch£º1044	 i:3 	 global-step:20883	 l-p:0.1339779943227768
epoch£º1044	 i:4 	 global-step:20884	 l-p:0.12374574691057205
epoch£º1044	 i:5 	 global-step:20885	 l-p:0.1329001933336258
epoch£º1044	 i:6 	 global-step:20886	 l-p:0.17275428771972656
epoch£º1044	 i:7 	 global-step:20887	 l-p:0.11272609233856201
epoch£º1044	 i:8 	 global-step:20888	 l-p:0.10613254457712173
epoch£º1044	 i:9 	 global-step:20889	 l-p:0.14793749153614044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1045
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1782, 5.4762, 5.3255],
        [5.1782, 5.0860, 5.1414],
        [5.1782, 5.1721, 5.1778],
        [5.1782, 4.8930, 4.6642]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1045, step:0 
model_pd.l_p.mean(): 0.1506260484457016 
model_pd.l_d.mean(): -18.87770652770996 
model_pd.lagr.mean(): -18.727081298828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5315], device='cuda:0')), ('power', tensor([-19.7812], device='cuda:0'))])
epoch£º1045	 i:0 	 global-step:20900	 l-p:0.1506260484457016
epoch£º1045	 i:1 	 global-step:20901	 l-p:0.16965697705745697
epoch£º1045	 i:2 	 global-step:20902	 l-p:0.11930134147405624
epoch£º1045	 i:3 	 global-step:20903	 l-p:0.198587566614151
epoch£º1045	 i:4 	 global-step:20904	 l-p:0.11322906613349915
epoch£º1045	 i:5 	 global-step:20905	 l-p:0.14443668723106384
epoch£º1045	 i:6 	 global-step:20906	 l-p:0.08352577686309814
epoch£º1045	 i:7 	 global-step:20907	 l-p:0.1379714161157608
epoch£º1045	 i:8 	 global-step:20908	 l-p:0.09085738658905029
epoch£º1045	 i:9 	 global-step:20909	 l-p:0.11236495524644852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1046
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1801, 4.8984, 4.7897],
        [5.1801, 5.1794, 5.1801],
        [5.1801, 5.0045, 4.6697],
        [5.1801, 5.1801, 5.1801]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1046, step:0 
model_pd.l_p.mean(): 0.10052049160003662 
model_pd.l_d.mean(): -20.180198669433594 
model_pd.lagr.mean(): -20.07967758178711 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4640], device='cuda:0')), ('power', tensor([-21.0381], device='cuda:0'))])
epoch£º1046	 i:0 	 global-step:20920	 l-p:0.10052049160003662
epoch£º1046	 i:1 	 global-step:20921	 l-p:0.11504419893026352
epoch£º1046	 i:2 	 global-step:20922	 l-p:0.09012212604284286
epoch£º1046	 i:3 	 global-step:20923	 l-p:0.15921731293201447
epoch£º1046	 i:4 	 global-step:20924	 l-p:0.12386590987443924
epoch£º1046	 i:5 	 global-step:20925	 l-p:0.11511367559432983
epoch£º1046	 i:6 	 global-step:20926	 l-p:0.13970349729061127
epoch£º1046	 i:7 	 global-step:20927	 l-p:0.1536809802055359
epoch£º1046	 i:8 	 global-step:20928	 l-p:0.20878276228904724
epoch£º1046	 i:9 	 global-step:20929	 l-p:0.12908294796943665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1047
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1702, 5.0209, 5.0783],
        [5.1702, 5.1434, 5.1661],
        [5.1702, 4.8796, 4.7195],
        [5.1702, 5.1005, 5.1482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1047, step:0 
model_pd.l_p.mean(): 0.108197420835495 
model_pd.l_d.mean(): -19.817174911499023 
model_pd.lagr.mean(): -19.70897674560547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-20.6682], device='cuda:0'))])
epoch£º1047	 i:0 	 global-step:20940	 l-p:0.108197420835495
epoch£º1047	 i:1 	 global-step:20941	 l-p:0.19990859925746918
epoch£º1047	 i:2 	 global-step:20942	 l-p:0.12976235151290894
epoch£º1047	 i:3 	 global-step:20943	 l-p:0.14156244695186615
epoch£º1047	 i:4 	 global-step:20944	 l-p:0.1125163733959198
epoch£º1047	 i:5 	 global-step:20945	 l-p:0.20211556553840637
epoch£º1047	 i:6 	 global-step:20946	 l-p:0.11642397940158844
epoch£º1047	 i:7 	 global-step:20947	 l-p:0.14453639090061188
epoch£º1047	 i:8 	 global-step:20948	 l-p:0.09858673065900803
epoch£º1047	 i:9 	 global-step:20949	 l-p:0.11244110763072968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1048
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1656, 5.3103, 5.0714],
        [5.1656, 4.9831, 5.0281],
        [5.1656, 5.1656, 5.1656],
        [5.1656, 4.8913, 4.6246]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1048, step:0 
model_pd.l_p.mean(): 0.08542797714471817 
model_pd.l_d.mean(): -19.725624084472656 
model_pd.lagr.mean(): -19.640195846557617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5201], device='cuda:0')), ('power', tensor([-20.6332], device='cuda:0'))])
epoch£º1048	 i:0 	 global-step:20960	 l-p:0.08542797714471817
epoch£º1048	 i:1 	 global-step:20961	 l-p:0.08227193355560303
epoch£º1048	 i:2 	 global-step:20962	 l-p:0.14659897983074188
epoch£º1048	 i:3 	 global-step:20963	 l-p:0.13122621178627014
epoch£º1048	 i:4 	 global-step:20964	 l-p:0.12007846683263779
epoch£º1048	 i:5 	 global-step:20965	 l-p:0.23064880073070526
epoch£º1048	 i:6 	 global-step:20966	 l-p:0.1133328527212143
epoch£º1048	 i:7 	 global-step:20967	 l-p:0.19826020300388336
epoch£º1048	 i:8 	 global-step:20968	 l-p:0.1253889501094818
epoch£º1048	 i:9 	 global-step:20969	 l-p:0.15766821801662445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1049
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1578, 5.0813, 5.1318],
        [5.1578, 4.9748, 4.6388],
        [5.1578, 5.5339, 5.4331],
        [5.1578, 4.9629, 5.0005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1049, step:0 
model_pd.l_p.mean(): 0.12835277616977692 
model_pd.l_d.mean(): -19.32256317138672 
model_pd.lagr.mean(): -19.194210052490234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5199], device='cuda:0')), ('power', tensor([-20.2223], device='cuda:0'))])
epoch£º1049	 i:0 	 global-step:20980	 l-p:0.12835277616977692
epoch£º1049	 i:1 	 global-step:20981	 l-p:0.11722258478403091
epoch£º1049	 i:2 	 global-step:20982	 l-p:0.12833577394485474
epoch£º1049	 i:3 	 global-step:20983	 l-p:0.15729081630706787
epoch£º1049	 i:4 	 global-step:20984	 l-p:0.18332713842391968
epoch£º1049	 i:5 	 global-step:20985	 l-p:0.10770832747220993
epoch£º1049	 i:6 	 global-step:20986	 l-p:0.15019652247428894
epoch£º1049	 i:7 	 global-step:20987	 l-p:0.15517380833625793
epoch£º1049	 i:8 	 global-step:20988	 l-p:0.13287709653377533
epoch£º1049	 i:9 	 global-step:20989	 l-p:0.14141184091567993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1050
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1592, 4.8830, 4.8011],
        [5.1592, 5.1132, 5.1488],
        [5.1592, 5.1578, 5.1592],
        [5.1592, 5.1213, 5.1518]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1050, step:0 
model_pd.l_p.mean(): 0.13226918876171112 
model_pd.l_d.mean(): -20.71170997619629 
model_pd.lagr.mean(): -20.57944107055664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4137], device='cuda:0')), ('power', tensor([-21.5275], device='cuda:0'))])
epoch£º1050	 i:0 	 global-step:21000	 l-p:0.13226918876171112
epoch£º1050	 i:1 	 global-step:21001	 l-p:0.11063539236783981
epoch£º1050	 i:2 	 global-step:21002	 l-p:0.11312778294086456
epoch£º1050	 i:3 	 global-step:21003	 l-p:0.1471107453107834
epoch£º1050	 i:4 	 global-step:21004	 l-p:0.14876875281333923
epoch£º1050	 i:5 	 global-step:21005	 l-p:0.11526013165712357
epoch£º1050	 i:6 	 global-step:21006	 l-p:0.2252427190542221
epoch£º1050	 i:7 	 global-step:21007	 l-p:0.1568211168050766
epoch£º1050	 i:8 	 global-step:21008	 l-p:0.12059739977121353
epoch£º1050	 i:9 	 global-step:21009	 l-p:0.17643563449382782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1051
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1468, 4.8525, 4.6758],
        [5.1468, 5.1457, 5.1467],
        [5.1468, 4.8526, 4.6608],
        [5.1468, 4.8565, 4.7231]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1051, step:0 
model_pd.l_p.mean(): 0.15631578862667084 
model_pd.l_d.mean(): -18.9103946685791 
model_pd.lagr.mean(): -18.754079818725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5558], device='cuda:0')), ('power', tensor([-19.8397], device='cuda:0'))])
epoch£º1051	 i:0 	 global-step:21020	 l-p:0.15631578862667084
epoch£º1051	 i:1 	 global-step:21021	 l-p:0.15670205652713776
epoch£º1051	 i:2 	 global-step:21022	 l-p:0.20658132433891296
epoch£º1051	 i:3 	 global-step:21023	 l-p:0.10658050328493118
epoch£º1051	 i:4 	 global-step:21024	 l-p:0.1478809118270874
epoch£º1051	 i:5 	 global-step:21025	 l-p:0.09068681299686432
epoch£º1051	 i:6 	 global-step:21026	 l-p:0.1262703686952591
epoch£º1051	 i:7 	 global-step:21027	 l-p:0.12385798990726471
epoch£º1051	 i:8 	 global-step:21028	 l-p:0.1441819667816162
epoch£º1051	 i:9 	 global-step:21029	 l-p:0.17795775830745697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1052
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1574, 5.1574, 5.1574],
        [5.1574, 5.1574, 5.1574],
        [5.1574, 5.0166, 5.0758],
        [5.1574, 4.9065, 4.8816]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1052, step:0 
model_pd.l_p.mean(): 0.09853795915842056 
model_pd.l_d.mean(): -20.51188087463379 
model_pd.lagr.mean(): -20.41334342956543 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4290], device='cuda:0')), ('power', tensor([-21.3397], device='cuda:0'))])
epoch£º1052	 i:0 	 global-step:21040	 l-p:0.09853795915842056
epoch£º1052	 i:1 	 global-step:21041	 l-p:0.13990426063537598
epoch£º1052	 i:2 	 global-step:21042	 l-p:0.1391073763370514
epoch£º1052	 i:3 	 global-step:21043	 l-p:0.13366903364658356
epoch£º1052	 i:4 	 global-step:21044	 l-p:0.19059395790100098
epoch£º1052	 i:5 	 global-step:21045	 l-p:0.0984141156077385
epoch£º1052	 i:6 	 global-step:21046	 l-p:0.08256450295448303
epoch£º1052	 i:7 	 global-step:21047	 l-p:0.22345714271068573
epoch£º1052	 i:8 	 global-step:21048	 l-p:0.12994424998760223
epoch£º1052	 i:9 	 global-step:21049	 l-p:0.13268019258975983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1053
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228]], device='cuda:0')
 pt:tensor([[5.1744, 4.9462, 4.6286],
        [5.1744, 5.4534, 5.2908],
        [5.1744, 4.9117, 4.8615],
        [5.1744, 4.9698, 4.9995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1053, step:0 
model_pd.l_p.mean(): 0.15587930381298065 
model_pd.l_d.mean(): -20.372331619262695 
model_pd.lagr.mean(): -20.21645164489746 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4576], device='cuda:0')), ('power', tensor([-21.2272], device='cuda:0'))])
epoch£º1053	 i:0 	 global-step:21060	 l-p:0.15587930381298065
epoch£º1053	 i:1 	 global-step:21061	 l-p:0.1162576824426651
epoch£º1053	 i:2 	 global-step:21062	 l-p:0.16788658499717712
epoch£º1053	 i:3 	 global-step:21063	 l-p:0.13383813202381134
epoch£º1053	 i:4 	 global-step:21064	 l-p:0.10504540801048279
epoch£º1053	 i:5 	 global-step:21065	 l-p:0.1096208393573761
epoch£º1053	 i:6 	 global-step:21066	 l-p:0.11371643096208572
epoch£º1053	 i:7 	 global-step:21067	 l-p:0.14468467235565186
epoch£º1053	 i:8 	 global-step:21068	 l-p:0.16388054192066193
epoch£º1053	 i:9 	 global-step:21069	 l-p:0.11906495690345764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1054
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1809, 5.1778, 5.1808],
        [5.1809, 5.1809, 5.1809],
        [5.1809, 5.1806, 5.1809],
        [5.1809, 5.1766, 5.1807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1054, step:0 
model_pd.l_p.mean(): 0.11920937895774841 
model_pd.l_d.mean(): -17.559011459350586 
model_pd.lagr.mean(): -17.439802169799805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6144], device='cuda:0')), ('power', tensor([-18.5238], device='cuda:0'))])
epoch£º1054	 i:0 	 global-step:21080	 l-p:0.11920937895774841
epoch£º1054	 i:1 	 global-step:21081	 l-p:0.11973321437835693
epoch£º1054	 i:2 	 global-step:21082	 l-p:0.14037711918354034
epoch£º1054	 i:3 	 global-step:21083	 l-p:0.11862055212259293
epoch£º1054	 i:4 	 global-step:21084	 l-p:0.18633824586868286
epoch£º1054	 i:5 	 global-step:21085	 l-p:0.08953278511762619
epoch£º1054	 i:6 	 global-step:21086	 l-p:0.1390778124332428
epoch£º1054	 i:7 	 global-step:21087	 l-p:0.1390828937292099
epoch£º1054	 i:8 	 global-step:21088	 l-p:0.1666778326034546
epoch£º1054	 i:9 	 global-step:21089	 l-p:0.09042979776859283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1055
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1851, 5.1791, 5.1847],
        [5.1851, 5.1376, 5.1740],
        [5.1851, 4.8998, 4.6707],
        [5.1851, 5.0160, 5.0671]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1055, step:0 
model_pd.l_p.mean(): 0.16941982507705688 
model_pd.l_d.mean(): -19.32543182373047 
model_pd.lagr.mean(): -19.1560115814209 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5797], device='cuda:0')), ('power', tensor([-20.2872], device='cuda:0'))])
epoch£º1055	 i:0 	 global-step:21100	 l-p:0.16941982507705688
epoch£º1055	 i:1 	 global-step:21101	 l-p:0.05824793130159378
epoch£º1055	 i:2 	 global-step:21102	 l-p:0.13828635215759277
epoch£º1055	 i:3 	 global-step:21103	 l-p:0.16001354157924652
epoch£º1055	 i:4 	 global-step:21104	 l-p:0.14414726197719574
epoch£º1055	 i:5 	 global-step:21105	 l-p:0.13574010133743286
epoch£º1055	 i:6 	 global-step:21106	 l-p:0.11127255856990814
epoch£º1055	 i:7 	 global-step:21107	 l-p:0.07995433360338211
epoch£º1055	 i:8 	 global-step:21108	 l-p:0.15749675035476685
epoch£º1055	 i:9 	 global-step:21109	 l-p:0.13159281015396118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1056
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1973, 5.1973, 5.1973],
        [5.1973, 5.1967, 5.1973],
        [5.1973, 5.1896, 5.1968],
        [5.1973, 5.1972, 5.1973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1056, step:0 
model_pd.l_p.mean(): 0.1774796098470688 
model_pd.l_d.mean(): -20.83120346069336 
model_pd.lagr.mean(): -20.653724670410156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3646], device='cuda:0')), ('power', tensor([-21.5984], device='cuda:0'))])
epoch£º1056	 i:0 	 global-step:21120	 l-p:0.1774796098470688
epoch£º1056	 i:1 	 global-step:21121	 l-p:0.12252955883741379
epoch£º1056	 i:2 	 global-step:21122	 l-p:0.16080348193645477
epoch£º1056	 i:3 	 global-step:21123	 l-p:0.09568735957145691
epoch£º1056	 i:4 	 global-step:21124	 l-p:0.1212509348988533
epoch£º1056	 i:5 	 global-step:21125	 l-p:0.0771232396364212
epoch£º1056	 i:6 	 global-step:21126	 l-p:0.10624739527702332
epoch£º1056	 i:7 	 global-step:21127	 l-p:0.15937675535678864
epoch£º1056	 i:8 	 global-step:21128	 l-p:0.09379035979509354
epoch£º1056	 i:9 	 global-step:21129	 l-p:0.15618190169334412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1057
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1915, 5.1288, 5.1733],
        [5.1915, 5.2617, 4.9853],
        [5.1915, 5.1915, 5.1915],
        [5.1915, 5.1001, 5.1553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1057, step:0 
model_pd.l_p.mean(): 0.16438518464565277 
model_pd.l_d.mean(): -20.345565795898438 
model_pd.lagr.mean(): -20.181180953979492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4427], device='cuda:0')), ('power', tensor([-21.1846], device='cuda:0'))])
epoch£º1057	 i:0 	 global-step:21140	 l-p:0.16438518464565277
epoch£º1057	 i:1 	 global-step:21141	 l-p:0.09232483804225922
epoch£º1057	 i:2 	 global-step:21142	 l-p:0.11377605050802231
epoch£º1057	 i:3 	 global-step:21143	 l-p:0.0992223396897316
epoch£º1057	 i:4 	 global-step:21144	 l-p:0.129745215177536
epoch£º1057	 i:5 	 global-step:21145	 l-p:0.1275579184293747
epoch£º1057	 i:6 	 global-step:21146	 l-p:0.11319959163665771
epoch£º1057	 i:7 	 global-step:21147	 l-p:0.16094711422920227
epoch£º1057	 i:8 	 global-step:21148	 l-p:0.1507570743560791
epoch£º1057	 i:9 	 global-step:21149	 l-p:0.12882409989833832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1058
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1894, 5.1705, 5.1871],
        [5.1894, 4.9549, 4.9519],
        [5.1894, 4.9240, 4.8658],
        [5.1894, 5.1327, 5.1742]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1058, step:0 
model_pd.l_p.mean(): 0.13389036059379578 
model_pd.l_d.mean(): -20.602142333984375 
model_pd.lagr.mean(): -20.468252182006836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4054], device='cuda:0')), ('power', tensor([-21.4073], device='cuda:0'))])
epoch£º1058	 i:0 	 global-step:21160	 l-p:0.13389036059379578
epoch£º1058	 i:1 	 global-step:21161	 l-p:0.10842893272638321
epoch£º1058	 i:2 	 global-step:21162	 l-p:0.11149229109287262
epoch£º1058	 i:3 	 global-step:21163	 l-p:0.1271834671497345
epoch£º1058	 i:4 	 global-step:21164	 l-p:0.16439275443553925
epoch£º1058	 i:5 	 global-step:21165	 l-p:0.176136776804924
epoch£º1058	 i:6 	 global-step:21166	 l-p:0.15046082437038422
epoch£º1058	 i:7 	 global-step:21167	 l-p:0.0678553357720375
epoch£º1058	 i:8 	 global-step:21168	 l-p:0.12553991377353668
epoch£º1058	 i:9 	 global-step:21169	 l-p:0.18888793885707855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1059
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1618, 5.1581, 5.1617],
        [5.1618, 4.9680, 5.0068],
        [5.1618, 5.1429, 5.1595],
        [5.1618, 5.1394, 5.1588]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1059, step:0 
model_pd.l_p.mean(): 0.12376928329467773 
model_pd.l_d.mean(): -20.424301147460938 
model_pd.lagr.mean(): -20.3005313873291 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4419], device='cuda:0')), ('power', tensor([-21.2639], device='cuda:0'))])
epoch£º1059	 i:0 	 global-step:21180	 l-p:0.12376928329467773
epoch£º1059	 i:1 	 global-step:21181	 l-p:0.11026028543710709
epoch£º1059	 i:2 	 global-step:21182	 l-p:0.09680122137069702
epoch£º1059	 i:3 	 global-step:21183	 l-p:0.18072906136512756
epoch£º1059	 i:4 	 global-step:21184	 l-p:0.2674138844013214
epoch£º1059	 i:5 	 global-step:21185	 l-p:0.1422978788614273
epoch£º1059	 i:6 	 global-step:21186	 l-p:0.14451360702514648
epoch£º1059	 i:7 	 global-step:21187	 l-p:0.1164214238524437
epoch£º1059	 i:8 	 global-step:21188	 l-p:0.1307169646024704
epoch£º1059	 i:9 	 global-step:21189	 l-p:0.1271066814661026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1060
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1430, 5.1399, 5.1429],
        [5.1430, 4.8590, 4.6034],
        [5.1430, 4.9373, 4.6042],
        [5.1430, 5.1424, 5.1430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1060, step:0 
model_pd.l_p.mean(): 0.1333315223455429 
model_pd.l_d.mean(): -19.770587921142578 
model_pd.lagr.mean(): -19.637256622314453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4520], device='cuda:0')), ('power', tensor([-20.6084], device='cuda:0'))])
epoch£º1060	 i:0 	 global-step:21200	 l-p:0.1333315223455429
epoch£º1060	 i:1 	 global-step:21201	 l-p:0.17691448330879211
epoch£º1060	 i:2 	 global-step:21202	 l-p:0.1282922327518463
epoch£º1060	 i:3 	 global-step:21203	 l-p:0.16456668078899384
epoch£º1060	 i:4 	 global-step:21204	 l-p:0.11425942927598953
epoch£º1060	 i:5 	 global-step:21205	 l-p:0.13681526482105255
epoch£º1060	 i:6 	 global-step:21206	 l-p:0.14164747297763824
epoch£º1060	 i:7 	 global-step:21207	 l-p:0.19068099558353424
epoch£º1060	 i:8 	 global-step:21208	 l-p:0.10832752287387848
epoch£º1060	 i:9 	 global-step:21209	 l-p:0.24710097908973694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1061
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1341, 5.1341, 5.1341],
        [5.1341, 4.9376, 4.9759],
        [5.1341, 5.4920, 5.3795],
        [5.1341, 4.8551, 4.7736]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1061, step:0 
model_pd.l_p.mean(): 0.12762144207954407 
model_pd.l_d.mean(): -19.556066513061523 
model_pd.lagr.mean(): -19.42844581604004 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5279], device='cuda:0')), ('power', tensor([-20.4685], device='cuda:0'))])
epoch£º1061	 i:0 	 global-step:21220	 l-p:0.12762144207954407
epoch£º1061	 i:1 	 global-step:21221	 l-p:0.15808643400669098
epoch£º1061	 i:2 	 global-step:21222	 l-p:0.20825843513011932
epoch£º1061	 i:3 	 global-step:21223	 l-p:0.2041478157043457
epoch£º1061	 i:4 	 global-step:21224	 l-p:0.09433341026306152
epoch£º1061	 i:5 	 global-step:21225	 l-p:0.13769648969173431
epoch£º1061	 i:6 	 global-step:21226	 l-p:0.1338183581829071
epoch£º1061	 i:7 	 global-step:21227	 l-p:0.12510643899440765
epoch£º1061	 i:8 	 global-step:21228	 l-p:0.1806025207042694
epoch£º1061	 i:9 	 global-step:21229	 l-p:0.1683439016342163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1062
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1418, 5.1413, 5.1418],
        [5.1418, 5.0955, 5.1313],
        [5.1418, 4.9634, 5.0119],
        [5.1418, 4.9966, 5.0557]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1062, step:0 
model_pd.l_p.mean(): 0.0986911728978157 
model_pd.l_d.mean(): -20.06606101989746 
model_pd.lagr.mean(): -19.967369079589844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4915], device='cuda:0')), ('power', tensor([-20.9503], device='cuda:0'))])
epoch£º1062	 i:0 	 global-step:21240	 l-p:0.0986911728978157
epoch£º1062	 i:1 	 global-step:21241	 l-p:0.13569585978984833
epoch£º1062	 i:2 	 global-step:21242	 l-p:0.13219760358333588
epoch£º1062	 i:3 	 global-step:21243	 l-p:0.20956021547317505
epoch£º1062	 i:4 	 global-step:21244	 l-p:0.11364676803350449
epoch£º1062	 i:5 	 global-step:21245	 l-p:0.11565703898668289
epoch£º1062	 i:6 	 global-step:21246	 l-p:0.13923904299736023
epoch£º1062	 i:7 	 global-step:21247	 l-p:0.12298839539289474
epoch£º1062	 i:8 	 global-step:21248	 l-p:0.1211932823061943
epoch£º1062	 i:9 	 global-step:21249	 l-p:0.3038053512573242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1063
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1415, 5.1412, 5.1415],
        [5.1415, 4.8813, 4.8429],
        [5.1415, 5.1415, 5.1415],
        [5.1415, 4.8587, 4.7637]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1063, step:0 
model_pd.l_p.mean(): 0.14052678644657135 
model_pd.l_d.mean(): -20.488040924072266 
model_pd.lagr.mean(): -20.34751319885254 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4290], device='cuda:0')), ('power', tensor([-21.3154], device='cuda:0'))])
epoch£º1063	 i:0 	 global-step:21260	 l-p:0.14052678644657135
epoch£º1063	 i:1 	 global-step:21261	 l-p:0.061325594782829285
epoch£º1063	 i:2 	 global-step:21262	 l-p:0.16656175255775452
epoch£º1063	 i:3 	 global-step:21263	 l-p:0.14542584121227264
epoch£º1063	 i:4 	 global-step:21264	 l-p:0.13003024458885193
epoch£º1063	 i:5 	 global-step:21265	 l-p:0.20484010875225067
epoch£º1063	 i:6 	 global-step:21266	 l-p:0.16474588215351105
epoch£º1063	 i:7 	 global-step:21267	 l-p:0.23424983024597168
epoch£º1063	 i:8 	 global-step:21268	 l-p:0.11794769018888474
epoch£º1063	 i:9 	 global-step:21269	 l-p:0.1649017632007599
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1064
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1372, 5.1370, 5.1372],
        [5.1372, 4.8412, 4.6759],
        [5.1372, 4.9134, 4.9279],
        [5.1372, 5.0592, 5.1105]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1064, step:0 
model_pd.l_p.mean(): 0.2979905605316162 
model_pd.l_d.mean(): -20.444875717163086 
model_pd.lagr.mean(): -20.14688491821289 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4318], device='cuda:0')), ('power', tensor([-21.2744], device='cuda:0'))])
epoch£º1064	 i:0 	 global-step:21280	 l-p:0.2979905605316162
epoch£º1064	 i:1 	 global-step:21281	 l-p:0.15987235307693481
epoch£º1064	 i:2 	 global-step:21282	 l-p:0.16553020477294922
epoch£º1064	 i:3 	 global-step:21283	 l-p:0.13011354207992554
epoch£º1064	 i:4 	 global-step:21284	 l-p:0.1337398737668991
epoch£º1064	 i:5 	 global-step:21285	 l-p:0.12288080155849457
epoch£º1064	 i:6 	 global-step:21286	 l-p:0.09376653283834457
epoch£º1064	 i:7 	 global-step:21287	 l-p:0.11765866726636887
epoch£º1064	 i:8 	 global-step:21288	 l-p:0.1617804616689682
epoch£º1064	 i:9 	 global-step:21289	 l-p:0.1304956078529358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1065
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1437, 5.1437, 5.1437],
        [5.1437, 5.1437, 5.1437],
        [5.1437, 5.0161, 4.6758],
        [5.1437, 5.1430, 5.1437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1065, step:0 
model_pd.l_p.mean(): 0.13539265096187592 
model_pd.l_d.mean(): -19.711713790893555 
model_pd.lagr.mean(): -19.57632064819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4968], device='cuda:0')), ('power', tensor([-20.5949], device='cuda:0'))])
epoch£º1065	 i:0 	 global-step:21300	 l-p:0.13539265096187592
epoch£º1065	 i:1 	 global-step:21301	 l-p:0.24772673845291138
epoch£º1065	 i:2 	 global-step:21302	 l-p:0.14680950343608856
epoch£º1065	 i:3 	 global-step:21303	 l-p:0.14134155213832855
epoch£º1065	 i:4 	 global-step:21304	 l-p:0.23035353422164917
epoch£º1065	 i:5 	 global-step:21305	 l-p:0.11642485111951828
epoch£º1065	 i:6 	 global-step:21306	 l-p:0.12563864886760712
epoch£º1065	 i:7 	 global-step:21307	 l-p:0.061636924743652344
epoch£º1065	 i:8 	 global-step:21308	 l-p:0.12273826450109482
epoch£º1065	 i:9 	 global-step:21309	 l-p:0.15795442461967468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1066
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1426, 5.5032, 5.3920],
        [5.1426, 4.9049, 4.5868],
        [5.1426, 5.0275, 5.0875],
        [5.1426, 4.8916, 4.5841]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1066, step:0 
model_pd.l_p.mean(): 0.21805357933044434 
model_pd.l_d.mean(): -20.364519119262695 
model_pd.lagr.mean(): -20.146465301513672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4463], device='cuda:0')), ('power', tensor([-21.2076], device='cuda:0'))])
epoch£º1066	 i:0 	 global-step:21320	 l-p:0.21805357933044434
epoch£º1066	 i:1 	 global-step:21321	 l-p:0.11576966196298599
epoch£º1066	 i:2 	 global-step:21322	 l-p:0.12596198916435242
epoch£º1066	 i:3 	 global-step:21323	 l-p:0.19802367687225342
epoch£º1066	 i:4 	 global-step:21324	 l-p:0.1714029312133789
epoch£º1066	 i:5 	 global-step:21325	 l-p:0.13424818217754364
epoch£º1066	 i:6 	 global-step:21326	 l-p:0.09636827558279037
epoch£º1066	 i:7 	 global-step:21327	 l-p:0.1282336711883545
epoch£º1066	 i:8 	 global-step:21328	 l-p:0.16078336536884308
epoch£º1066	 i:9 	 global-step:21329	 l-p:0.13574780523777008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1067
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1407, 5.1407, 5.1407],
        [5.1407, 5.1395, 5.1407],
        [5.1407, 5.1354, 5.1404],
        [5.1407, 5.4599, 5.3223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1067, step:0 
model_pd.l_p.mean(): 0.17915385961532593 
model_pd.l_d.mean(): -20.323406219482422 
model_pd.lagr.mean(): -20.14425277709961 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4674], device='cuda:0')), ('power', tensor([-21.1876], device='cuda:0'))])
epoch£º1067	 i:0 	 global-step:21340	 l-p:0.17915385961532593
epoch£º1067	 i:1 	 global-step:21341	 l-p:0.1349007934331894
epoch£º1067	 i:2 	 global-step:21342	 l-p:0.1710910052061081
epoch£º1067	 i:3 	 global-step:21343	 l-p:0.08531772345304489
epoch£º1067	 i:4 	 global-step:21344	 l-p:0.14871563017368317
epoch£º1067	 i:5 	 global-step:21345	 l-p:0.1677618771791458
epoch£º1067	 i:6 	 global-step:21346	 l-p:0.16158072650432587
epoch£º1067	 i:7 	 global-step:21347	 l-p:0.16365933418273926
epoch£º1067	 i:8 	 global-step:21348	 l-p:0.11158712208271027
epoch£º1067	 i:9 	 global-step:21349	 l-p:0.16708454489707947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1068
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1474, 5.1474, 5.1474],
        [5.1474, 5.1468, 5.1474],
        [5.1474, 5.1473, 5.1474],
        [5.1474, 5.1473, 5.1474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1068, step:0 
model_pd.l_p.mean(): 0.21140959858894348 
model_pd.l_d.mean(): -19.37377166748047 
model_pd.lagr.mean(): -19.16236114501953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4928], device='cuda:0')), ('power', tensor([-20.2465], device='cuda:0'))])
epoch£º1068	 i:0 	 global-step:21360	 l-p:0.21140959858894348
epoch£º1068	 i:1 	 global-step:21361	 l-p:0.17259632050991058
epoch£º1068	 i:2 	 global-step:21362	 l-p:0.12693502008914948
epoch£º1068	 i:3 	 global-step:21363	 l-p:0.1394418627023697
epoch£º1068	 i:4 	 global-step:21364	 l-p:0.07200159132480621
epoch£º1068	 i:5 	 global-step:21365	 l-p:0.16827978193759918
epoch£º1068	 i:6 	 global-step:21366	 l-p:0.11568373441696167
epoch£º1068	 i:7 	 global-step:21367	 l-p:0.14320331811904907
epoch£º1068	 i:8 	 global-step:21368	 l-p:0.17534732818603516
epoch£º1068	 i:9 	 global-step:21369	 l-p:0.10461027175188065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1069
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1655, 4.9147, 4.8904],
        [5.1655, 5.1519, 5.1642],
        [5.1655, 5.1445, 5.1628],
        [5.1655, 5.0263, 5.0858]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1069, step:0 
model_pd.l_p.mean(): 0.11155852675437927 
model_pd.l_d.mean(): -20.5733642578125 
model_pd.lagr.mean(): -20.46180534362793 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4069], device='cuda:0')), ('power', tensor([-21.3795], device='cuda:0'))])
epoch£º1069	 i:0 	 global-step:21380	 l-p:0.11155852675437927
epoch£º1069	 i:1 	 global-step:21381	 l-p:0.0920925885438919
epoch£º1069	 i:2 	 global-step:21382	 l-p:0.16247640550136566
epoch£º1069	 i:3 	 global-step:21383	 l-p:0.14324913918972015
epoch£º1069	 i:4 	 global-step:21384	 l-p:0.12904956936836243
epoch£º1069	 i:5 	 global-step:21385	 l-p:0.14902788400650024
epoch£º1069	 i:6 	 global-step:21386	 l-p:0.11856669932603836
epoch£º1069	 i:7 	 global-step:21387	 l-p:0.1288338452577591
epoch£º1069	 i:8 	 global-step:21388	 l-p:0.1705554872751236
epoch£º1069	 i:9 	 global-step:21389	 l-p:0.1426989585161209
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1070
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1873, 5.3232, 5.0786],
        [5.1873, 5.1859, 5.1873],
        [5.1873, 5.1494, 5.1799],
        [5.1873, 5.2543, 4.9760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1070, step:0 
model_pd.l_p.mean(): 0.09670902043581009 
model_pd.l_d.mean(): -19.335424423217773 
model_pd.lagr.mean(): -19.23871612548828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5498], device='cuda:0')), ('power', tensor([-20.2665], device='cuda:0'))])
epoch£º1070	 i:0 	 global-step:21400	 l-p:0.09670902043581009
epoch£º1070	 i:1 	 global-step:21401	 l-p:0.14199578762054443
epoch£º1070	 i:2 	 global-step:21402	 l-p:0.13953948020935059
epoch£º1070	 i:3 	 global-step:21403	 l-p:0.1360599547624588
epoch£º1070	 i:4 	 global-step:21404	 l-p:0.13522644340991974
epoch£º1070	 i:5 	 global-step:21405	 l-p:0.127723827958107
epoch£º1070	 i:6 	 global-step:21406	 l-p:0.07528430968523026
epoch£º1070	 i:7 	 global-step:21407	 l-p:0.15769366919994354
epoch£º1070	 i:8 	 global-step:21408	 l-p:0.15549249947071075
epoch£º1070	 i:9 	 global-step:21409	 l-p:0.11153695732355118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1071
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2006, 5.2006, 5.2006],
        [5.2006, 5.1824, 5.1985],
        [5.2006, 5.0453, 5.1010],
        [5.2006, 4.9534, 4.9310]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1071, step:0 
model_pd.l_p.mean(): 0.10808086395263672 
model_pd.l_d.mean(): -19.869279861450195 
model_pd.lagr.mean(): -19.761199951171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4755], device='cuda:0')), ('power', tensor([-20.7333], device='cuda:0'))])
epoch£º1071	 i:0 	 global-step:21420	 l-p:0.10808086395263672
epoch£º1071	 i:1 	 global-step:21421	 l-p:0.11075883358716965
epoch£º1071	 i:2 	 global-step:21422	 l-p:0.13929013907909393
epoch£º1071	 i:3 	 global-step:21423	 l-p:0.14528338611125946
epoch£º1071	 i:4 	 global-step:21424	 l-p:0.14887751638889313
epoch£º1071	 i:5 	 global-step:21425	 l-p:0.08432997763156891
epoch£º1071	 i:6 	 global-step:21426	 l-p:0.09457121044397354
epoch£º1071	 i:7 	 global-step:21427	 l-p:0.12229663878679276
epoch£º1071	 i:8 	 global-step:21428	 l-p:0.16942916810512543
epoch£º1071	 i:9 	 global-step:21429	 l-p:0.13856272399425507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1072
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1992, 5.1993, 5.1992],
        [5.1992, 5.5878, 5.4933],
        [5.1992, 5.1984, 5.1992],
        [5.1992, 4.9105, 4.7055]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1072, step:0 
model_pd.l_p.mean(): 0.1465037763118744 
model_pd.l_d.mean(): -19.378828048706055 
model_pd.lagr.mean(): -19.232324600219727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5454], device='cuda:0')), ('power', tensor([-20.3061], device='cuda:0'))])
epoch£º1072	 i:0 	 global-step:21440	 l-p:0.1465037763118744
epoch£º1072	 i:1 	 global-step:21441	 l-p:0.09519924223423004
epoch£º1072	 i:2 	 global-step:21442	 l-p:0.09940139204263687
epoch£º1072	 i:3 	 global-step:21443	 l-p:0.10394182801246643
epoch£º1072	 i:4 	 global-step:21444	 l-p:0.11904043704271317
epoch£º1072	 i:5 	 global-step:21445	 l-p:0.15768499672412872
epoch£º1072	 i:6 	 global-step:21446	 l-p:0.14705541729927063
epoch£º1072	 i:7 	 global-step:21447	 l-p:0.12109344452619553
epoch£º1072	 i:8 	 global-step:21448	 l-p:0.12075050175189972
epoch£º1072	 i:9 	 global-step:21449	 l-p:0.14386308193206787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1073
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1991, 5.1771, 5.1962],
        [5.1991, 5.1923, 5.1987],
        [5.1991, 5.1244, 4.7949],
        [5.1991, 5.1831, 5.1974]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1073, step:0 
model_pd.l_p.mean(): 0.09328090399503708 
model_pd.l_d.mean(): -19.521455764770508 
model_pd.lagr.mean(): -19.42817497253418 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5315], device='cuda:0')), ('power', tensor([-20.4370], device='cuda:0'))])
epoch£º1073	 i:0 	 global-step:21460	 l-p:0.09328090399503708
epoch£º1073	 i:1 	 global-step:21461	 l-p:0.1047515869140625
epoch£º1073	 i:2 	 global-step:21462	 l-p:0.1225397065281868
epoch£º1073	 i:3 	 global-step:21463	 l-p:0.13064371049404144
epoch£º1073	 i:4 	 global-step:21464	 l-p:0.11071160435676575
epoch£º1073	 i:5 	 global-step:21465	 l-p:0.12059257924556732
epoch£º1073	 i:6 	 global-step:21466	 l-p:0.1720317006111145
epoch£º1073	 i:7 	 global-step:21467	 l-p:0.1444697082042694
epoch£º1073	 i:8 	 global-step:21468	 l-p:0.1343715339899063
epoch£º1073	 i:9 	 global-step:21469	 l-p:0.15124227106571198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1074
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1825, 5.1825, 5.1825],
        [5.1825, 4.8943, 4.7615],
        [5.1825, 5.0894, 5.1452],
        [5.1825, 4.8937, 4.7576]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1074, step:0 
model_pd.l_p.mean(): 0.09615255147218704 
model_pd.l_d.mean(): -19.673686981201172 
model_pd.lagr.mean(): -19.577533721923828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4937], device='cuda:0')), ('power', tensor([-20.5529], device='cuda:0'))])
epoch£º1074	 i:0 	 global-step:21480	 l-p:0.09615255147218704
epoch£º1074	 i:1 	 global-step:21481	 l-p:0.12308011949062347
epoch£º1074	 i:2 	 global-step:21482	 l-p:0.13361796736717224
epoch£º1074	 i:3 	 global-step:21483	 l-p:0.15103329718112946
epoch£º1074	 i:4 	 global-step:21484	 l-p:0.16385513544082642
epoch£º1074	 i:5 	 global-step:21485	 l-p:0.16243384778499603
epoch£º1074	 i:6 	 global-step:21486	 l-p:0.12139832973480225
epoch£º1074	 i:7 	 global-step:21487	 l-p:0.14957334101200104
epoch£º1074	 i:8 	 global-step:21488	 l-p:0.1653221696615219
epoch£º1074	 i:9 	 global-step:21489	 l-p:0.09377051889896393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1075
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1656, 5.0666, 5.1239],
        [5.1656, 5.1618, 5.1655],
        [5.1656, 5.1653, 5.1656],
        [5.1656, 4.9290, 4.9265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1075, step:0 
model_pd.l_p.mean(): 0.09185581654310226 
model_pd.l_d.mean(): -20.641868591308594 
model_pd.lagr.mean(): -20.550012588500977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4024], device='cuda:0')), ('power', tensor([-21.4447], device='cuda:0'))])
epoch£º1075	 i:0 	 global-step:21500	 l-p:0.09185581654310226
epoch£º1075	 i:1 	 global-step:21501	 l-p:0.15873490273952484
epoch£º1075	 i:2 	 global-step:21502	 l-p:0.13323484361171722
epoch£º1075	 i:3 	 global-step:21503	 l-p:0.1312410533428192
epoch£º1075	 i:4 	 global-step:21504	 l-p:0.15418919920921326
epoch£º1075	 i:5 	 global-step:21505	 l-p:0.12805421650409698
epoch£º1075	 i:6 	 global-step:21506	 l-p:0.12973639369010925
epoch£º1075	 i:7 	 global-step:21507	 l-p:0.13817676901817322
epoch£º1075	 i:8 	 global-step:21508	 l-p:0.1832890510559082
epoch£º1075	 i:9 	 global-step:21509	 l-p:0.12562283873558044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1076
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1648, 5.1455, 5.1624],
        [5.1648, 5.4079, 5.2230],
        [5.1648, 5.4380, 5.2712],
        [5.1648, 5.1294, 5.1582]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1076, step:0 
model_pd.l_p.mean(): 0.10368214547634125 
model_pd.l_d.mean(): -20.205842971801758 
model_pd.lagr.mean(): -20.102161407470703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4866], device='cuda:0')), ('power', tensor([-21.0876], device='cuda:0'))])
epoch£º1076	 i:0 	 global-step:21520	 l-p:0.10368214547634125
epoch£º1076	 i:1 	 global-step:21521	 l-p:0.15346455574035645
epoch£º1076	 i:2 	 global-step:21522	 l-p:0.16991014778614044
epoch£º1076	 i:3 	 global-step:21523	 l-p:0.13212484121322632
epoch£º1076	 i:4 	 global-step:21524	 l-p:0.1308777630329132
epoch£º1076	 i:5 	 global-step:21525	 l-p:0.10588603466749191
epoch£º1076	 i:6 	 global-step:21526	 l-p:0.1786118596792221
epoch£º1076	 i:7 	 global-step:21527	 l-p:0.07916057854890823
epoch£º1076	 i:8 	 global-step:21528	 l-p:0.1982240080833435
epoch£º1076	 i:9 	 global-step:21529	 l-p:0.13071461021900177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1077
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1646, 4.8815, 4.7811],
        [5.1646, 4.8977, 4.8432],
        [5.1646, 5.1595, 5.1644],
        [5.1646, 5.1639, 5.1646]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1077, step:0 
model_pd.l_p.mean(): 0.16301067173480988 
model_pd.l_d.mean(): -19.89826202392578 
model_pd.lagr.mean(): -19.73525047302246 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4272], device='cuda:0')), ('power', tensor([-20.7129], device='cuda:0'))])
epoch£º1077	 i:0 	 global-step:21540	 l-p:0.16301067173480988
epoch£º1077	 i:1 	 global-step:21541	 l-p:0.10371137410402298
epoch£º1077	 i:2 	 global-step:21542	 l-p:0.1258738487958908
epoch£º1077	 i:3 	 global-step:21543	 l-p:0.18666832149028778
epoch£º1077	 i:4 	 global-step:21544	 l-p:0.17836304008960724
epoch£º1077	 i:5 	 global-step:21545	 l-p:0.11753396689891815
epoch£º1077	 i:6 	 global-step:21546	 l-p:0.1259835660457611
epoch£º1077	 i:7 	 global-step:21547	 l-p:0.15308208763599396
epoch£º1077	 i:8 	 global-step:21548	 l-p:0.1004662811756134
epoch£º1077	 i:9 	 global-step:21549	 l-p:0.12173254042863846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1078
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1682, 4.8780, 4.7440],
        [5.1682, 4.9771, 5.0181],
        [5.1682, 4.9145, 4.8859],
        [5.1682, 5.1682, 5.1682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1078, step:0 
model_pd.l_p.mean(): 0.15486857295036316 
model_pd.l_d.mean(): -19.20708465576172 
model_pd.lagr.mean(): -19.052215576171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5363], device='cuda:0')), ('power', tensor([-20.1217], device='cuda:0'))])
epoch£º1078	 i:0 	 global-step:21560	 l-p:0.15486857295036316
epoch£º1078	 i:1 	 global-step:21561	 l-p:0.13615410029888153
epoch£º1078	 i:2 	 global-step:21562	 l-p:0.12549905478954315
epoch£º1078	 i:3 	 global-step:21563	 l-p:0.14453978836536407
epoch£º1078	 i:4 	 global-step:21564	 l-p:0.12358905375003815
epoch£º1078	 i:5 	 global-step:21565	 l-p:0.09618627279996872
epoch£º1078	 i:6 	 global-step:21566	 l-p:0.1176217645406723
epoch£º1078	 i:7 	 global-step:21567	 l-p:0.14436812698841095
epoch£º1078	 i:8 	 global-step:21568	 l-p:0.19770433008670807
epoch£º1078	 i:9 	 global-step:21569	 l-p:0.14007288217544556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1079
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1672, 5.1275, 5.1591],
        [5.1672, 5.0413, 4.7016],
        [5.1672, 5.1624, 5.1669],
        [5.1672, 4.8733, 4.7075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1079, step:0 
model_pd.l_p.mean(): 0.24379943311214447 
model_pd.l_d.mean(): -20.007686614990234 
model_pd.lagr.mean(): -19.763887405395508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5011], device='cuda:0')), ('power', tensor([-20.9009], device='cuda:0'))])
epoch£º1079	 i:0 	 global-step:21580	 l-p:0.24379943311214447
epoch£º1079	 i:1 	 global-step:21581	 l-p:0.11079522222280502
epoch£º1079	 i:2 	 global-step:21582	 l-p:0.05968127027153969
epoch£º1079	 i:3 	 global-step:21583	 l-p:0.13372622430324554
epoch£º1079	 i:4 	 global-step:21584	 l-p:0.11661592870950699
epoch£º1079	 i:5 	 global-step:21585	 l-p:0.13791590929031372
epoch£º1079	 i:6 	 global-step:21586	 l-p:0.1312207132577896
epoch£º1079	 i:7 	 global-step:21587	 l-p:0.13279426097869873
epoch£º1079	 i:8 	 global-step:21588	 l-p:0.162077397108078
epoch£º1079	 i:9 	 global-step:21589	 l-p:0.13599002361297607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1080
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1731, 5.1447, 5.1686],
        [5.1731, 5.0724, 5.1301],
        [5.1731, 5.4328, 5.2575],
        [5.1731, 4.9656, 4.6350]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1080, step:0 
model_pd.l_p.mean(): 0.11303152143955231 
model_pd.l_d.mean(): -20.085275650024414 
model_pd.lagr.mean(): -19.972244262695312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4849], device='cuda:0')), ('power', tensor([-20.9631], device='cuda:0'))])
epoch£º1080	 i:0 	 global-step:21600	 l-p:0.11303152143955231
epoch£º1080	 i:1 	 global-step:21601	 l-p:0.1138882040977478
epoch£º1080	 i:2 	 global-step:21602	 l-p:0.11135667562484741
epoch£º1080	 i:3 	 global-step:21603	 l-p:0.12958654761314392
epoch£º1080	 i:4 	 global-step:21604	 l-p:0.1952536553144455
epoch£º1080	 i:5 	 global-step:21605	 l-p:0.0941149890422821
epoch£º1080	 i:6 	 global-step:21606	 l-p:0.13305330276489258
epoch£º1080	 i:7 	 global-step:21607	 l-p:0.17199954390525818
epoch£º1080	 i:8 	 global-step:21608	 l-p:0.15708684921264648
epoch£º1080	 i:9 	 global-step:21609	 l-p:0.1523994356393814
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1081
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1674, 5.1673, 5.1674],
        [5.1674, 5.1674, 5.1674],
        [5.1674, 5.1660, 5.1674],
        [5.1674, 5.1588, 5.1668]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1081, step:0 
model_pd.l_p.mean(): 0.1124330461025238 
model_pd.l_d.mean(): -19.1885929107666 
model_pd.lagr.mean(): -19.076160430908203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5249], device='cuda:0')), ('power', tensor([-20.0911], device='cuda:0'))])
epoch£º1081	 i:0 	 global-step:21620	 l-p:0.1124330461025238
epoch£º1081	 i:1 	 global-step:21621	 l-p:0.20455898344516754
epoch£º1081	 i:2 	 global-step:21622	 l-p:0.17360122501850128
epoch£º1081	 i:3 	 global-step:21623	 l-p:0.12636791169643402
epoch£º1081	 i:4 	 global-step:21624	 l-p:0.2004273533821106
epoch£º1081	 i:5 	 global-step:21625	 l-p:0.12093793600797653
epoch£º1081	 i:6 	 global-step:21626	 l-p:0.10147061944007874
epoch£º1081	 i:7 	 global-step:21627	 l-p:0.08060098439455032
epoch£º1081	 i:8 	 global-step:21628	 l-p:0.10635070502758026
epoch£º1081	 i:9 	 global-step:21629	 l-p:0.14504067599773407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1082
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1689, 5.1689, 5.1689],
        [5.1689, 5.0450, 5.1054],
        [5.1689, 4.9096, 4.6135],
        [5.1689, 5.0007, 4.6609]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1082, step:0 
model_pd.l_p.mean(): 0.11066526174545288 
model_pd.l_d.mean(): -19.21515655517578 
model_pd.lagr.mean(): -19.1044921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5414], device='cuda:0')), ('power', tensor([-20.1352], device='cuda:0'))])
epoch£º1082	 i:0 	 global-step:21640	 l-p:0.11066526174545288
epoch£º1082	 i:1 	 global-step:21641	 l-p:0.09899566322565079
epoch£º1082	 i:2 	 global-step:21642	 l-p:0.13341106474399567
epoch£º1082	 i:3 	 global-step:21643	 l-p:0.13634982705116272
epoch£º1082	 i:4 	 global-step:21644	 l-p:0.1137692779302597
epoch£º1082	 i:5 	 global-step:21645	 l-p:0.1790090948343277
epoch£º1082	 i:6 	 global-step:21646	 l-p:0.23266296088695526
epoch£º1082	 i:7 	 global-step:21647	 l-p:0.13933336734771729
epoch£º1082	 i:8 	 global-step:21648	 l-p:0.14493060111999512
epoch£º1082	 i:9 	 global-step:21649	 l-p:0.1064641922712326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1083
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1556, 5.1526, 5.1554],
        [5.1556, 4.9007, 4.8724],
        [5.1556, 5.4394, 5.2789],
        [5.1556, 5.0086, 4.6666]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1083, step:0 
model_pd.l_p.mean(): 0.11526578664779663 
model_pd.l_d.mean(): -20.017393112182617 
model_pd.lagr.mean(): -19.902128219604492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5146], device='cuda:0')), ('power', tensor([-20.9246], device='cuda:0'))])
epoch£º1083	 i:0 	 global-step:21660	 l-p:0.11526578664779663
epoch£º1083	 i:1 	 global-step:21661	 l-p:0.19465740025043488
epoch£º1083	 i:2 	 global-step:21662	 l-p:0.14495690166950226
epoch£º1083	 i:3 	 global-step:21663	 l-p:0.12751269340515137
epoch£º1083	 i:4 	 global-step:21664	 l-p:0.09498107433319092
epoch£º1083	 i:5 	 global-step:21665	 l-p:0.11393758654594421
epoch£º1083	 i:6 	 global-step:21666	 l-p:0.22198033332824707
epoch£º1083	 i:7 	 global-step:21667	 l-p:0.15561123192310333
epoch£º1083	 i:8 	 global-step:21668	 l-p:0.09779523313045502
epoch£º1083	 i:9 	 global-step:21669	 l-p:0.189340740442276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1084
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1441, 5.4428, 5.2917],
        [5.1441, 5.1298, 5.1427],
        [5.1441, 4.8485, 4.6328],
        [5.1441, 4.9945, 4.6516]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1084, step:0 
model_pd.l_p.mean(): 0.12696479260921478 
model_pd.l_d.mean(): -20.06289291381836 
model_pd.lagr.mean(): -19.935928344726562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5147], device='cuda:0')), ('power', tensor([-20.9711], device='cuda:0'))])
epoch£º1084	 i:0 	 global-step:21680	 l-p:0.12696479260921478
epoch£º1084	 i:1 	 global-step:21681	 l-p:0.16846372187137604
epoch£º1084	 i:2 	 global-step:21682	 l-p:0.1251041740179062
epoch£º1084	 i:3 	 global-step:21683	 l-p:0.1507563591003418
epoch£º1084	 i:4 	 global-step:21684	 l-p:0.1008998453617096
epoch£º1084	 i:5 	 global-step:21685	 l-p:0.1207377016544342
epoch£º1084	 i:6 	 global-step:21686	 l-p:0.1289318948984146
epoch£º1084	 i:7 	 global-step:21687	 l-p:0.10875609517097473
epoch£º1084	 i:8 	 global-step:21688	 l-p:0.224693164229393
epoch£º1084	 i:9 	 global-step:21689	 l-p:0.3352002203464508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1085
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1294, 5.0093, 5.0701],
        [5.1294, 4.8487, 4.5742],
        [5.1294, 4.8882, 4.8841],
        [5.1294, 5.1266, 5.1293]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1085, step:0 
model_pd.l_p.mean(): 0.2944900393486023 
model_pd.l_d.mean(): -19.513595581054688 
model_pd.lagr.mean(): -19.219104766845703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5001], device='cuda:0')), ('power', tensor([-20.3964], device='cuda:0'))])
epoch£º1085	 i:0 	 global-step:21700	 l-p:0.2944900393486023
epoch£º1085	 i:1 	 global-step:21701	 l-p:0.1365353912115097
epoch£º1085	 i:2 	 global-step:21702	 l-p:0.14686237275600433
epoch£º1085	 i:3 	 global-step:21703	 l-p:0.13189730048179626
epoch£º1085	 i:4 	 global-step:21704	 l-p:0.08574101328849792
epoch£º1085	 i:5 	 global-step:21705	 l-p:0.19366897642612457
epoch£º1085	 i:6 	 global-step:21706	 l-p:0.13362275063991547
epoch£º1085	 i:7 	 global-step:21707	 l-p:0.11138231307268143
epoch£º1085	 i:8 	 global-step:21708	 l-p:0.10764098912477493
epoch£º1085	 i:9 	 global-step:21709	 l-p:0.3154425621032715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1086
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1219, 5.0298, 5.0858],
        [5.1219, 5.1940, 4.9176],
        [5.1219, 5.1219, 5.1219],
        [5.1219, 5.1218, 5.1219]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1086, step:0 
model_pd.l_p.mean(): 0.21197684109210968 
model_pd.l_d.mean(): -19.71788787841797 
model_pd.lagr.mean(): -19.505910873413086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4320], device='cuda:0')), ('power', tensor([-20.5340], device='cuda:0'))])
epoch£º1086	 i:0 	 global-step:21720	 l-p:0.21197684109210968
epoch£º1086	 i:1 	 global-step:21721	 l-p:0.1338883638381958
epoch£º1086	 i:2 	 global-step:21722	 l-p:0.07060474902391434
epoch£º1086	 i:3 	 global-step:21723	 l-p:0.13555659353733063
epoch£º1086	 i:4 	 global-step:21724	 l-p:0.112574003636837
epoch£º1086	 i:5 	 global-step:21725	 l-p:0.17407608032226562
epoch£º1086	 i:6 	 global-step:21726	 l-p:0.2555548846721649
epoch£º1086	 i:7 	 global-step:21727	 l-p:0.11195500940084457
epoch£º1086	 i:8 	 global-step:21728	 l-p:0.1442781239748001
epoch£º1086	 i:9 	 global-step:21729	 l-p:0.3300432562828064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1087
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1282, 5.1281, 5.1282],
        [5.1282, 5.0944, 5.1222],
        [5.1282, 5.1088, 5.1259],
        [5.1282, 4.9470, 4.6044]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1087, step:0 
model_pd.l_p.mean(): 0.18210291862487793 
model_pd.l_d.mean(): -19.408573150634766 
model_pd.lagr.mean(): -19.226470947265625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5335], device='cuda:0')), ('power', tensor([-20.3241], device='cuda:0'))])
epoch£º1087	 i:0 	 global-step:21740	 l-p:0.18210291862487793
epoch£º1087	 i:1 	 global-step:21741	 l-p:0.13213689625263214
epoch£º1087	 i:2 	 global-step:21742	 l-p:0.17074967920780182
epoch£º1087	 i:3 	 global-step:21743	 l-p:0.14829979836940765
epoch£º1087	 i:4 	 global-step:21744	 l-p:0.12003754079341888
epoch£º1087	 i:5 	 global-step:21745	 l-p:0.29290804266929626
epoch£º1087	 i:6 	 global-step:21746	 l-p:0.11918868869543076
epoch£º1087	 i:7 	 global-step:21747	 l-p:0.13371337950229645
epoch£º1087	 i:8 	 global-step:21748	 l-p:0.10363133251667023
epoch£º1087	 i:9 	 global-step:21749	 l-p:0.17623260617256165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1088
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1372, 5.1262, 5.1363],
        [5.1372, 5.1113, 4.7940],
        [5.1372, 5.3952, 5.2192],
        [5.1372, 5.0672, 4.7360]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1088, step:0 
model_pd.l_p.mean(): 0.1422049105167389 
model_pd.l_d.mean(): -19.97683334350586 
model_pd.lagr.mean(): -19.83462905883789 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5080], device='cuda:0')), ('power', tensor([-20.8765], device='cuda:0'))])
epoch£º1088	 i:0 	 global-step:21760	 l-p:0.1422049105167389
epoch£º1088	 i:1 	 global-step:21761	 l-p:0.1331275850534439
epoch£º1088	 i:2 	 global-step:21762	 l-p:0.17866675555706024
epoch£º1088	 i:3 	 global-step:21763	 l-p:0.1445212960243225
epoch£º1088	 i:4 	 global-step:21764	 l-p:0.07250967621803284
epoch£º1088	 i:5 	 global-step:21765	 l-p:0.1941765993833542
epoch£º1088	 i:6 	 global-step:21766	 l-p:0.10827288031578064
epoch£º1088	 i:7 	 global-step:21767	 l-p:0.27379679679870605
epoch£º1088	 i:8 	 global-step:21768	 l-p:0.13200721144676208
epoch£º1088	 i:9 	 global-step:21769	 l-p:0.14735068380832672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1089
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1464, 5.0538, 5.1098],
        [5.1464, 5.0490, 5.1063],
        [5.1464, 4.8679, 4.5936],
        [5.1464, 5.1464, 5.1464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1089, step:0 
model_pd.l_p.mean(): 0.1503974050283432 
model_pd.l_d.mean(): -20.542694091796875 
model_pd.lagr.mean(): -20.392295837402344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4413], device='cuda:0')), ('power', tensor([-21.3838], device='cuda:0'))])
epoch£º1089	 i:0 	 global-step:21780	 l-p:0.1503974050283432
epoch£º1089	 i:1 	 global-step:21781	 l-p:0.1955602765083313
epoch£º1089	 i:2 	 global-step:21782	 l-p:0.15674887597560883
epoch£º1089	 i:3 	 global-step:21783	 l-p:0.14226894080638885
epoch£º1089	 i:4 	 global-step:21784	 l-p:0.11758136749267578
epoch£º1089	 i:5 	 global-step:21785	 l-p:0.11809704452753067
epoch£º1089	 i:6 	 global-step:21786	 l-p:0.12457194924354553
epoch£º1089	 i:7 	 global-step:21787	 l-p:0.10187500715255737
epoch£º1089	 i:8 	 global-step:21788	 l-p:0.15750008821487427
epoch£º1089	 i:9 	 global-step:21789	 l-p:0.17911556363105774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1090
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1610, 4.8676, 4.7227],
        [5.1610, 4.8763, 4.6214],
        [5.1610, 5.2183, 4.9347],
        [5.1610, 5.1596, 5.1610]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1090, step:0 
model_pd.l_p.mean(): 0.19601237773895264 
model_pd.l_d.mean(): -20.516742706298828 
model_pd.lagr.mean(): -20.320730209350586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4294], device='cuda:0')), ('power', tensor([-21.3451], device='cuda:0'))])
epoch£º1090	 i:0 	 global-step:21800	 l-p:0.19601237773895264
epoch£º1090	 i:1 	 global-step:21801	 l-p:0.14877550303936005
epoch£º1090	 i:2 	 global-step:21802	 l-p:0.1060151681303978
epoch£º1090	 i:3 	 global-step:21803	 l-p:0.11919822543859482
epoch£º1090	 i:4 	 global-step:21804	 l-p:0.1417236477136612
epoch£º1090	 i:5 	 global-step:21805	 l-p:0.15318162739276886
epoch£º1090	 i:6 	 global-step:21806	 l-p:0.16467928886413574
epoch£º1090	 i:7 	 global-step:21807	 l-p:0.12739518284797668
epoch£º1090	 i:8 	 global-step:21808	 l-p:0.11957550048828125
epoch£º1090	 i:9 	 global-step:21809	 l-p:0.11880103498697281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1091
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1664, 5.0498, 4.7102],
        [5.1664, 4.8807, 4.6303],
        [5.1664, 4.9837, 4.6447],
        [5.1664, 5.1662, 5.1664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1091, step:0 
model_pd.l_p.mean(): 0.15248724818229675 
model_pd.l_d.mean(): -20.14952850341797 
model_pd.lagr.mean(): -19.997041702270508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4335], device='cuda:0')), ('power', tensor([-20.9753], device='cuda:0'))])
epoch£º1091	 i:0 	 global-step:21820	 l-p:0.15248724818229675
epoch£º1091	 i:1 	 global-step:21821	 l-p:0.13981278240680695
epoch£º1091	 i:2 	 global-step:21822	 l-p:0.1793498545885086
epoch£º1091	 i:3 	 global-step:21823	 l-p:0.14208664000034332
epoch£º1091	 i:4 	 global-step:21824	 l-p:0.15597973763942719
epoch£º1091	 i:5 	 global-step:21825	 l-p:0.15515203773975372
epoch£º1091	 i:6 	 global-step:21826	 l-p:0.12398860603570938
epoch£º1091	 i:7 	 global-step:21827	 l-p:0.08957534283399582
epoch£º1091	 i:8 	 global-step:21828	 l-p:0.13788463175296783
epoch£º1091	 i:9 	 global-step:21829	 l-p:0.12341384589672089
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1092
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1621, 5.0129, 5.0714],
        [5.1621, 5.1431, 5.1598],
        [5.1621, 5.0977, 4.7686],
        [5.1621, 4.9072, 4.8790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1092, step:0 
model_pd.l_p.mean(): 0.1296304166316986 
model_pd.l_d.mean(): -20.34893035888672 
model_pd.lagr.mean(): -20.21929931640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4713], device='cuda:0')), ('power', tensor([-21.2176], device='cuda:0'))])
epoch£º1092	 i:0 	 global-step:21840	 l-p:0.1296304166316986
epoch£º1092	 i:1 	 global-step:21841	 l-p:0.11968780308961868
epoch£º1092	 i:2 	 global-step:21842	 l-p:0.1387535184621811
epoch£º1092	 i:3 	 global-step:21843	 l-p:0.1314665824174881
epoch£º1092	 i:4 	 global-step:21844	 l-p:0.24906779825687408
epoch£º1092	 i:5 	 global-step:21845	 l-p:0.14076204597949982
epoch£º1092	 i:6 	 global-step:21846	 l-p:0.07892493158578873
epoch£º1092	 i:7 	 global-step:21847	 l-p:0.17088176310062408
epoch£º1092	 i:8 	 global-step:21848	 l-p:0.12700317800045013
epoch£º1092	 i:9 	 global-step:21849	 l-p:0.12564383447170258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1093
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1565, 5.0906, 4.7608],
        [5.1565, 5.1563, 5.1565],
        [5.1565, 5.1565, 5.1565],
        [5.1565, 5.1564, 5.1565]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1093, step:0 
model_pd.l_p.mean(): 0.17445437610149384 
model_pd.l_d.mean(): -20.326921463012695 
model_pd.lagr.mean(): -20.152467727661133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4594], device='cuda:0')), ('power', tensor([-21.1828], device='cuda:0'))])
epoch£º1093	 i:0 	 global-step:21860	 l-p:0.17445437610149384
epoch£º1093	 i:1 	 global-step:21861	 l-p:0.11711456626653671
epoch£º1093	 i:2 	 global-step:21862	 l-p:0.11026410758495331
epoch£º1093	 i:3 	 global-step:21863	 l-p:0.11197187006473541
epoch£º1093	 i:4 	 global-step:21864	 l-p:0.14621490240097046
epoch£º1093	 i:5 	 global-step:21865	 l-p:0.10485438257455826
epoch£º1093	 i:6 	 global-step:21866	 l-p:0.1602993756532669
epoch£º1093	 i:7 	 global-step:21867	 l-p:0.10408368706703186
epoch£º1093	 i:8 	 global-step:21868	 l-p:0.20120418071746826
epoch£º1093	 i:9 	 global-step:21869	 l-p:0.2030804604291916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1094
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1554, 5.0734, 5.1262],
        [5.1554, 5.0501, 5.1089],
        [5.1554, 5.1554, 5.1554],
        [5.1554, 4.9036, 4.8819]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1094, step:0 
model_pd.l_p.mean(): 0.15721015632152557 
model_pd.l_d.mean(): -20.678977966308594 
model_pd.lagr.mean(): -20.52176856994629 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4065], device='cuda:0')), ('power', tensor([-21.4867], device='cuda:0'))])
epoch£º1094	 i:0 	 global-step:21880	 l-p:0.15721015632152557
epoch£º1094	 i:1 	 global-step:21881	 l-p:0.12555350363254547
epoch£º1094	 i:2 	 global-step:21882	 l-p:0.1087905764579773
epoch£º1094	 i:3 	 global-step:21883	 l-p:0.13018658757209778
epoch£º1094	 i:4 	 global-step:21884	 l-p:0.1683233231306076
epoch£º1094	 i:5 	 global-step:21885	 l-p:0.12623608112335205
epoch£º1094	 i:6 	 global-step:21886	 l-p:0.10348530858755112
epoch£º1094	 i:7 	 global-step:21887	 l-p:0.17260654270648956
epoch£º1094	 i:8 	 global-step:21888	 l-p:0.19776493310928345
epoch£º1094	 i:9 	 global-step:21889	 l-p:0.15949195623397827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1095
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1526, 5.1503, 5.1525],
        [5.1526, 4.9930, 5.0492],
        [5.1526, 4.8968, 4.8686],
        [5.1526, 5.1526, 5.1526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1095, step:0 
model_pd.l_p.mean(): 0.1390659064054489 
model_pd.l_d.mean(): -20.213390350341797 
model_pd.lagr.mean(): -20.074323654174805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4684], device='cuda:0')), ('power', tensor([-21.0765], device='cuda:0'))])
epoch£º1095	 i:0 	 global-step:21900	 l-p:0.1390659064054489
epoch£º1095	 i:1 	 global-step:21901	 l-p:0.09815708547830582
epoch£º1095	 i:2 	 global-step:21902	 l-p:0.19606825709342957
epoch£º1095	 i:3 	 global-step:21903	 l-p:0.12606893479824066
epoch£º1095	 i:4 	 global-step:21904	 l-p:0.15120024979114532
epoch£º1095	 i:5 	 global-step:21905	 l-p:0.09909687936306
epoch£º1095	 i:6 	 global-step:21906	 l-p:0.10799609124660492
epoch£º1095	 i:7 	 global-step:21907	 l-p:0.21220263838768005
epoch£º1095	 i:8 	 global-step:21908	 l-p:0.18375736474990845
epoch£º1095	 i:9 	 global-step:21909	 l-p:0.13391360640525818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1096
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1504, 4.8982, 4.8765],
        [5.1504, 4.8942, 4.8657],
        [5.1504, 5.1502, 5.1504],
        [5.1504, 4.9317, 4.9525]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1096, step:0 
model_pd.l_p.mean(): 0.115846648812294 
model_pd.l_d.mean(): -17.665061950683594 
model_pd.lagr.mean(): -17.54921531677246 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6414], device='cuda:0')), ('power', tensor([-18.6597], device='cuda:0'))])
epoch£º1096	 i:0 	 global-step:21920	 l-p:0.115846648812294
epoch£º1096	 i:1 	 global-step:21921	 l-p:0.13330449163913727
epoch£º1096	 i:2 	 global-step:21922	 l-p:0.21445629000663757
epoch£º1096	 i:3 	 global-step:21923	 l-p:0.09464183449745178
epoch£º1096	 i:4 	 global-step:21924	 l-p:0.17702846229076385
epoch£º1096	 i:5 	 global-step:21925	 l-p:0.12424241751432419
epoch£º1096	 i:6 	 global-step:21926	 l-p:0.12463699281215668
epoch£º1096	 i:7 	 global-step:21927	 l-p:0.14700984954833984
epoch£º1096	 i:8 	 global-step:21928	 l-p:0.16474734246730804
epoch£º1096	 i:9 	 global-step:21929	 l-p:0.15062366425991058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1097
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1610, 5.1610, 5.1610],
        [5.1610, 5.1593, 5.1609],
        [5.1610, 5.1610, 5.1610],
        [5.1610, 5.2792, 5.0247]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1097, step:0 
model_pd.l_p.mean(): 0.10223313421010971 
model_pd.l_d.mean(): -19.435230255126953 
model_pd.lagr.mean(): -19.332996368408203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4806], device='cuda:0')), ('power', tensor([-20.2964], device='cuda:0'))])
epoch£º1097	 i:0 	 global-step:21940	 l-p:0.10223313421010971
epoch£º1097	 i:1 	 global-step:21941	 l-p:0.12133374810218811
epoch£º1097	 i:2 	 global-step:21942	 l-p:0.13365542888641357
epoch£º1097	 i:3 	 global-step:21943	 l-p:0.12625615298748016
epoch£º1097	 i:4 	 global-step:21944	 l-p:0.13163340091705322
epoch£º1097	 i:5 	 global-step:21945	 l-p:0.1311105191707611
epoch£º1097	 i:6 	 global-step:21946	 l-p:0.18889926373958588
epoch£º1097	 i:7 	 global-step:21947	 l-p:0.13282254338264465
epoch£º1097	 i:8 	 global-step:21948	 l-p:0.156870037317276
epoch£º1097	 i:9 	 global-step:21949	 l-p:0.17811362445354462
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1098
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1622, 5.1575, 5.1620],
        [5.1622, 5.1493, 5.1610],
        [5.1622, 4.8662, 4.7001],
        [5.1622, 4.9406, 4.9577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1098, step:0 
model_pd.l_p.mean(): 0.1807824820280075 
model_pd.l_d.mean(): -20.676729202270508 
model_pd.lagr.mean(): -20.495946884155273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4031], device='cuda:0')), ('power', tensor([-21.4809], device='cuda:0'))])
epoch£º1098	 i:0 	 global-step:21960	 l-p:0.1807824820280075
epoch£º1098	 i:1 	 global-step:21961	 l-p:0.19035136699676514
epoch£º1098	 i:2 	 global-step:21962	 l-p:0.17186839878559113
epoch£º1098	 i:3 	 global-step:21963	 l-p:0.16335542500019073
epoch£º1098	 i:4 	 global-step:21964	 l-p:0.16078881919384003
epoch£º1098	 i:5 	 global-step:21965	 l-p:0.11509501934051514
epoch£º1098	 i:6 	 global-step:21966	 l-p:0.0713636726140976
epoch£º1098	 i:7 	 global-step:21967	 l-p:0.10975907742977142
epoch£º1098	 i:8 	 global-step:21968	 l-p:0.11651039123535156
epoch£º1098	 i:9 	 global-step:21969	 l-p:0.09834064543247223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1099
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1766, 5.1598, 4.8456],
        [5.1766, 5.1765, 5.1766],
        [5.1766, 5.0986, 5.1499],
        [5.1766, 5.1766, 5.1766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1099, step:0 
model_pd.l_p.mean(): 0.1304977536201477 
model_pd.l_d.mean(): -20.40999412536621 
model_pd.lagr.mean(): -20.279497146606445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4494], device='cuda:0')), ('power', tensor([-21.2570], device='cuda:0'))])
epoch£º1099	 i:0 	 global-step:21980	 l-p:0.1304977536201477
epoch£º1099	 i:1 	 global-step:21981	 l-p:0.20497317612171173
epoch£º1099	 i:2 	 global-step:21982	 l-p:0.11596450209617615
epoch£º1099	 i:3 	 global-step:21983	 l-p:0.10817229747772217
epoch£º1099	 i:4 	 global-step:21984	 l-p:0.09214616566896439
epoch£º1099	 i:5 	 global-step:21985	 l-p:0.19707824289798737
epoch£º1099	 i:6 	 global-step:21986	 l-p:0.08330580592155457
epoch£º1099	 i:7 	 global-step:21987	 l-p:0.1786504089832306
epoch£º1099	 i:8 	 global-step:21988	 l-p:0.11697316914796829
epoch£º1099	 i:9 	 global-step:21989	 l-p:0.11434312909841537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1823, 5.1823, 5.1823],
        [5.1823, 5.5167, 5.3865],
        [5.1823, 5.2646, 4.9921],
        [5.1823, 4.9253, 4.8911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1100, step:0 
model_pd.l_p.mean(): 0.10920041799545288 
model_pd.l_d.mean(): -20.244897842407227 
model_pd.lagr.mean(): -20.135698318481445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4704], device='cuda:0')), ('power', tensor([-21.1106], device='cuda:0'))])
epoch£º1100	 i:0 	 global-step:22000	 l-p:0.10920041799545288
epoch£º1100	 i:1 	 global-step:22001	 l-p:0.17109741270542145
epoch£º1100	 i:2 	 global-step:22002	 l-p:0.10527598112821579
epoch£º1100	 i:3 	 global-step:22003	 l-p:0.151565819978714
epoch£º1100	 i:4 	 global-step:22004	 l-p:0.1285841017961502
epoch£º1100	 i:5 	 global-step:22005	 l-p:0.10132825374603271
epoch£º1100	 i:6 	 global-step:22006	 l-p:0.10017209500074387
epoch£º1100	 i:7 	 global-step:22007	 l-p:0.1738945096731186
epoch£º1100	 i:8 	 global-step:22008	 l-p:0.13934288918972015
epoch£º1100	 i:9 	 global-step:22009	 l-p:0.13419003784656525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1863, 4.9333, 4.6309],
        [5.1863, 5.1593, 5.1822],
        [5.1863, 5.1812, 5.1861],
        [5.1863, 4.9552, 4.6353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1101, step:0 
model_pd.l_p.mean(): 0.11631883680820465 
model_pd.l_d.mean(): -20.185667037963867 
model_pd.lagr.mean(): -20.069347381591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4769], device='cuda:0')), ('power', tensor([-21.0571], device='cuda:0'))])
epoch£º1101	 i:0 	 global-step:22020	 l-p:0.11631883680820465
epoch£º1101	 i:1 	 global-step:22021	 l-p:0.12253052741289139
epoch£º1101	 i:2 	 global-step:22022	 l-p:0.1593049168586731
epoch£º1101	 i:3 	 global-step:22023	 l-p:0.07874248921871185
epoch£º1101	 i:4 	 global-step:22024	 l-p:0.1384766399860382
epoch£º1101	 i:5 	 global-step:22025	 l-p:0.12230823189020157
epoch£º1101	 i:6 	 global-step:22026	 l-p:0.1157807782292366
epoch£º1101	 i:7 	 global-step:22027	 l-p:0.11239174008369446
epoch£º1101	 i:8 	 global-step:22028	 l-p:0.11574242264032364
epoch£º1101	 i:9 	 global-step:22029	 l-p:0.2200155258178711
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1900, 4.9256, 4.8758],
        [5.1900, 5.0077, 4.6700],
        [5.1900, 5.1814, 5.1894],
        [5.1900, 4.8967, 4.7317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1102, step:0 
model_pd.l_p.mean(): 0.12913097441196442 
model_pd.l_d.mean(): -19.514095306396484 
model_pd.lagr.mean(): -19.384963989257812 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4670], device='cuda:0')), ('power', tensor([-20.3627], device='cuda:0'))])
epoch£º1102	 i:0 	 global-step:22040	 l-p:0.12913097441196442
epoch£º1102	 i:1 	 global-step:22041	 l-p:0.15683463215827942
epoch£º1102	 i:2 	 global-step:22042	 l-p:0.14995700120925903
epoch£º1102	 i:3 	 global-step:22043	 l-p:0.15410315990447998
epoch£º1102	 i:4 	 global-step:22044	 l-p:0.14236170053482056
epoch£º1102	 i:5 	 global-step:22045	 l-p:0.0999588817358017
epoch£º1102	 i:6 	 global-step:22046	 l-p:0.1297733038663864
epoch£º1102	 i:7 	 global-step:22047	 l-p:0.11372151970863342
epoch£º1102	 i:8 	 global-step:22048	 l-p:0.16791889071464539
epoch£º1102	 i:9 	 global-step:22049	 l-p:0.07163702696561813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1785, 5.1563, 5.1755],
        [5.1785, 5.0140, 5.0682],
        [5.1785, 5.1695, 4.8580],
        [5.1785, 5.1785, 5.1785]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1103, step:0 
model_pd.l_p.mean(): 0.13979437947273254 
model_pd.l_d.mean(): -20.317123413085938 
model_pd.lagr.mean(): -20.17732810974121 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4602], device='cuda:0')), ('power', tensor([-21.1737], device='cuda:0'))])
epoch£º1103	 i:0 	 global-step:22060	 l-p:0.13979437947273254
epoch£º1103	 i:1 	 global-step:22061	 l-p:0.14696383476257324
epoch£º1103	 i:2 	 global-step:22062	 l-p:0.1967567354440689
epoch£º1103	 i:3 	 global-step:22063	 l-p:0.1365264505147934
epoch£º1103	 i:4 	 global-step:22064	 l-p:0.14576201140880585
epoch£º1103	 i:5 	 global-step:22065	 l-p:0.05695780739188194
epoch£º1103	 i:6 	 global-step:22066	 l-p:0.11494050920009613
epoch£º1103	 i:7 	 global-step:22067	 l-p:0.13109846413135529
epoch£º1103	 i:8 	 global-step:22068	 l-p:0.10878243297338486
epoch£º1103	 i:9 	 global-step:22069	 l-p:0.1667068898677826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1781, 5.4673, 5.3087],
        [5.1781, 5.0871, 5.1426],
        [5.1781, 5.1781, 5.1781],
        [5.1781, 5.1738, 5.1779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1104, step:0 
model_pd.l_p.mean(): 0.11672350019216537 
model_pd.l_d.mean(): -20.36684226989746 
model_pd.lagr.mean(): -20.250118255615234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4578], device='cuda:0')), ('power', tensor([-21.2218], device='cuda:0'))])
epoch£º1104	 i:0 	 global-step:22080	 l-p:0.11672350019216537
epoch£º1104	 i:1 	 global-step:22081	 l-p:0.1484372913837433
epoch£º1104	 i:2 	 global-step:22082	 l-p:0.07061842828989029
epoch£º1104	 i:3 	 global-step:22083	 l-p:0.13213051855564117
epoch£º1104	 i:4 	 global-step:22084	 l-p:0.12157678604125977
epoch£º1104	 i:5 	 global-step:22085	 l-p:0.1455228328704834
epoch£º1104	 i:6 	 global-step:22086	 l-p:0.16382554173469543
epoch£º1104	 i:7 	 global-step:22087	 l-p:0.11488564312458038
epoch£º1104	 i:8 	 global-step:22088	 l-p:0.20650069415569305
epoch£º1104	 i:9 	 global-step:22089	 l-p:0.12673161923885345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1748, 4.9725, 4.6377],
        [5.1748, 5.1670, 5.1743],
        [5.1748, 5.0732, 4.7355],
        [5.1748, 4.9980, 5.0474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1105, step:0 
model_pd.l_p.mean(): 0.1610776036977768 
model_pd.l_d.mean(): -20.25641632080078 
model_pd.lagr.mean(): -20.095338821411133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4672], device='cuda:0')), ('power', tensor([-21.1190], device='cuda:0'))])
epoch£º1105	 i:0 	 global-step:22100	 l-p:0.1610776036977768
epoch£º1105	 i:1 	 global-step:22101	 l-p:0.1300564408302307
epoch£º1105	 i:2 	 global-step:22102	 l-p:0.132769376039505
epoch£º1105	 i:3 	 global-step:22103	 l-p:0.17249038815498352
epoch£º1105	 i:4 	 global-step:22104	 l-p:0.1527901291847229
epoch£º1105	 i:5 	 global-step:22105	 l-p:0.1614970564842224
epoch£º1105	 i:6 	 global-step:22106	 l-p:0.09330637007951736
epoch£º1105	 i:7 	 global-step:22107	 l-p:0.11707364767789841
epoch£º1105	 i:8 	 global-step:22108	 l-p:0.09335771948099136
epoch£º1105	 i:9 	 global-step:22109	 l-p:0.15022069215774536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1655, 5.2446, 4.9704],
        [5.1655, 5.1624, 5.1654],
        [5.1655, 5.1411, 5.1620],
        [5.1655, 5.1316, 5.1594]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1106, step:0 
model_pd.l_p.mean(): 0.13512778282165527 
model_pd.l_d.mean(): -20.139503479003906 
model_pd.lagr.mean(): -20.004375457763672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4605], device='cuda:0')), ('power', tensor([-20.9930], device='cuda:0'))])
epoch£º1106	 i:0 	 global-step:22120	 l-p:0.13512778282165527
epoch£º1106	 i:1 	 global-step:22121	 l-p:0.10562150925397873
epoch£º1106	 i:2 	 global-step:22122	 l-p:0.12520188093185425
epoch£º1106	 i:3 	 global-step:22123	 l-p:0.14470934867858887
epoch£º1106	 i:4 	 global-step:22124	 l-p:0.112484872341156
epoch£º1106	 i:5 	 global-step:22125	 l-p:0.21690315008163452
epoch£º1106	 i:6 	 global-step:22126	 l-p:0.13411149382591248
epoch£º1106	 i:7 	 global-step:22127	 l-p:0.16710498929023743
epoch£º1106	 i:8 	 global-step:22128	 l-p:0.12295342981815338
epoch£º1106	 i:9 	 global-step:22129	 l-p:0.14786098897457123
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1575, 5.2898, 5.0421],
        [5.1575, 5.1575, 5.1575],
        [5.1575, 5.0570, 5.1150],
        [5.1575, 5.1544, 5.1574]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1107, step:0 
model_pd.l_p.mean(): 0.10833528637886047 
model_pd.l_d.mean(): -20.34041404724121 
model_pd.lagr.mean(): -20.232078552246094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4706], device='cuda:0')), ('power', tensor([-21.2082], device='cuda:0'))])
epoch£º1107	 i:0 	 global-step:22140	 l-p:0.10833528637886047
epoch£º1107	 i:1 	 global-step:22141	 l-p:0.13474498689174652
epoch£º1107	 i:2 	 global-step:22142	 l-p:0.16818469762802124
epoch£º1107	 i:3 	 global-step:22143	 l-p:0.1198381781578064
epoch£º1107	 i:4 	 global-step:22144	 l-p:0.18995729088783264
epoch£º1107	 i:5 	 global-step:22145	 l-p:0.1610974669456482
epoch£º1107	 i:6 	 global-step:22146	 l-p:0.12642455101013184
epoch£º1107	 i:7 	 global-step:22147	 l-p:0.13046211004257202
epoch£º1107	 i:8 	 global-step:22148	 l-p:0.11421938985586166
epoch£º1107	 i:9 	 global-step:22149	 l-p:0.17107585072517395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1596, 4.8684, 4.7460],
        [5.1596, 5.1596, 5.1596],
        [5.1596, 4.9190, 4.9145],
        [5.1596, 4.8686, 4.6275]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1108, step:0 
model_pd.l_p.mean(): 0.13870345056056976 
model_pd.l_d.mean(): -20.077299118041992 
model_pd.lagr.mean(): -19.938594818115234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4819], device='cuda:0')), ('power', tensor([-20.9518], device='cuda:0'))])
epoch£º1108	 i:0 	 global-step:22160	 l-p:0.13870345056056976
epoch£º1108	 i:1 	 global-step:22161	 l-p:0.19085444509983063
epoch£º1108	 i:2 	 global-step:22162	 l-p:0.17448921501636505
epoch£º1108	 i:3 	 global-step:22163	 l-p:0.11947507411241531
epoch£º1108	 i:4 	 global-step:22164	 l-p:0.17479358613491058
epoch£º1108	 i:5 	 global-step:22165	 l-p:0.12183139473199844
epoch£º1108	 i:6 	 global-step:22166	 l-p:0.09488076716661453
epoch£º1108	 i:7 	 global-step:22167	 l-p:0.09941409528255463
epoch£º1108	 i:8 	 global-step:22168	 l-p:0.16411550343036652
epoch£º1108	 i:9 	 global-step:22169	 l-p:0.13179147243499756
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1655, 5.2477, 4.9748],
        [5.1655, 5.1654, 5.1655],
        [5.1655, 4.9684, 5.0067],
        [5.1655, 4.9407, 4.9548]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1109, step:0 
model_pd.l_p.mean(): 0.15839505195617676 
model_pd.l_d.mean(): -20.17009162902832 
model_pd.lagr.mean(): -20.011695861816406 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4705], device='cuda:0')), ('power', tensor([-21.0345], device='cuda:0'))])
epoch£º1109	 i:0 	 global-step:22180	 l-p:0.15839505195617676
epoch£º1109	 i:1 	 global-step:22181	 l-p:0.1902685910463333
epoch£º1109	 i:2 	 global-step:22182	 l-p:0.13611410558223724
epoch£º1109	 i:3 	 global-step:22183	 l-p:0.1454842984676361
epoch£º1109	 i:4 	 global-step:22184	 l-p:0.1564178764820099
epoch£º1109	 i:5 	 global-step:22185	 l-p:0.12554027140140533
epoch£º1109	 i:6 	 global-step:22186	 l-p:0.10365371406078339
epoch£º1109	 i:7 	 global-step:22187	 l-p:0.08643773943185806
epoch£º1109	 i:8 	 global-step:22188	 l-p:0.13977281749248505
epoch£º1109	 i:9 	 global-step:22189	 l-p:0.1282459944486618
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1110
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228]], device='cuda:0')
 pt:tensor([[5.1805, 4.9966, 5.0425],
        [5.1805, 5.0110, 4.6700],
        [5.1805, 5.2092, 4.9123],
        [5.1805, 5.2580, 4.9829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1110, step:0 
model_pd.l_p.mean(): 0.17113834619522095 
model_pd.l_d.mean(): -20.053794860839844 
model_pd.lagr.mean(): -19.88265609741211 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5040], device='cuda:0')), ('power', tensor([-20.9508], device='cuda:0'))])
epoch£º1110	 i:0 	 global-step:22200	 l-p:0.17113834619522095
epoch£º1110	 i:1 	 global-step:22201	 l-p:0.12011684477329254
epoch£º1110	 i:2 	 global-step:22202	 l-p:0.10685750097036362
epoch£º1110	 i:3 	 global-step:22203	 l-p:0.17338037490844727
epoch£º1110	 i:4 	 global-step:22204	 l-p:0.15616419911384583
epoch£º1110	 i:5 	 global-step:22205	 l-p:0.13443779945373535
epoch£º1110	 i:6 	 global-step:22206	 l-p:0.12659743428230286
epoch£º1110	 i:7 	 global-step:22207	 l-p:0.08690109103918076
epoch£º1110	 i:8 	 global-step:22208	 l-p:0.1172003373503685
epoch£º1110	 i:9 	 global-step:22209	 l-p:0.13224804401397705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1885, 5.1580, 4.8388],
        [5.1885, 5.1495, 5.1807],
        [5.1885, 5.1885, 5.1885],
        [5.1885, 4.8943, 4.7279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1111, step:0 
model_pd.l_p.mean(): 0.13223184645175934 
model_pd.l_d.mean(): -18.716064453125 
model_pd.lagr.mean(): -18.583831787109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5355], device='cuda:0')), ('power', tensor([-19.6207], device='cuda:0'))])
epoch£º1111	 i:0 	 global-step:22220	 l-p:0.13223184645175934
epoch£º1111	 i:1 	 global-step:22221	 l-p:0.09337078779935837
epoch£º1111	 i:2 	 global-step:22222	 l-p:0.1478114128112793
epoch£º1111	 i:3 	 global-step:22223	 l-p:0.1573779135942459
epoch£º1111	 i:4 	 global-step:22224	 l-p:0.13268786668777466
epoch£º1111	 i:5 	 global-step:22225	 l-p:0.18505702912807465
epoch£º1111	 i:6 	 global-step:22226	 l-p:0.1286838948726654
epoch£º1111	 i:7 	 global-step:22227	 l-p:0.10779333859682083
epoch£º1111	 i:8 	 global-step:22228	 l-p:0.06047612056136131
epoch£º1111	 i:9 	 global-step:22229	 l-p:0.20095649361610413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1112
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[5.1708, 4.9216, 4.6110],
        [5.1708, 4.9451, 4.6186],
        [5.1708, 4.9041, 4.8539],
        [5.1708, 4.9218, 4.6111]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1112, step:0 
model_pd.l_p.mean(): 0.13180187344551086 
model_pd.l_d.mean(): -19.688404083251953 
model_pd.lagr.mean(): -19.556602478027344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4915], device='cuda:0')), ('power', tensor([-20.5656], device='cuda:0'))])
epoch£º1112	 i:0 	 global-step:22240	 l-p:0.13180187344551086
epoch£º1112	 i:1 	 global-step:22241	 l-p:0.1954999715089798
epoch£º1112	 i:2 	 global-step:22242	 l-p:0.07565449178218842
epoch£º1112	 i:3 	 global-step:22243	 l-p:0.18666259944438934
epoch£º1112	 i:4 	 global-step:22244	 l-p:0.16604192554950714
epoch£º1112	 i:5 	 global-step:22245	 l-p:0.11698729544878006
epoch£º1112	 i:6 	 global-step:22246	 l-p:0.09482547640800476
epoch£º1112	 i:7 	 global-step:22247	 l-p:0.134286567568779
epoch£º1112	 i:8 	 global-step:22248	 l-p:0.13193003833293915
epoch£º1112	 i:9 	 global-step:22249	 l-p:0.13107016682624817
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1757, 5.1744, 5.1757],
        [5.1757, 5.1751, 5.1757],
        [5.1757, 5.1757, 5.1757],
        [5.1757, 4.9231, 4.8991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1113, step:0 
model_pd.l_p.mean(): 0.16455592215061188 
model_pd.l_d.mean(): -19.266490936279297 
model_pd.lagr.mean(): -19.1019344329834 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5287], device='cuda:0')), ('power', tensor([-20.1744], device='cuda:0'))])
epoch£º1113	 i:0 	 global-step:22260	 l-p:0.16455592215061188
epoch£º1113	 i:1 	 global-step:22261	 l-p:0.16462384164333344
epoch£º1113	 i:2 	 global-step:22262	 l-p:0.1259547472000122
epoch£º1113	 i:3 	 global-step:22263	 l-p:0.1318712830543518
epoch£º1113	 i:4 	 global-step:22264	 l-p:0.10774613916873932
epoch£º1113	 i:5 	 global-step:22265	 l-p:0.15636110305786133
epoch£º1113	 i:6 	 global-step:22266	 l-p:0.1276756376028061
epoch£º1113	 i:7 	 global-step:22267	 l-p:0.11142338812351227
epoch£º1113	 i:8 	 global-step:22268	 l-p:0.163051575422287
epoch£º1113	 i:9 	 global-step:22269	 l-p:0.1089869886636734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1697, 5.0861, 5.1394],
        [5.1697, 4.8728, 4.7009],
        [5.1697, 4.8895, 4.6201],
        [5.1697, 5.1667, 5.1695]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1114, step:0 
model_pd.l_p.mean(): 0.13073459267616272 
model_pd.l_d.mean(): -18.3607234954834 
model_pd.lagr.mean(): -18.22998809814453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5399], device='cuda:0')), ('power', tensor([-19.2632], device='cuda:0'))])
epoch£º1114	 i:0 	 global-step:22280	 l-p:0.13073459267616272
epoch£º1114	 i:1 	 global-step:22281	 l-p:0.15327443182468414
epoch£º1114	 i:2 	 global-step:22282	 l-p:0.15474916994571686
epoch£º1114	 i:3 	 global-step:22283	 l-p:0.14983326196670532
epoch£º1114	 i:4 	 global-step:22284	 l-p:0.12427211552858353
epoch£º1114	 i:5 	 global-step:22285	 l-p:0.15745681524276733
epoch£º1114	 i:6 	 global-step:22286	 l-p:0.17009864747524261
epoch£º1114	 i:7 	 global-step:22287	 l-p:0.13861964643001556
epoch£º1114	 i:8 	 global-step:22288	 l-p:0.10370467603206635
epoch£º1114	 i:9 	 global-step:22289	 l-p:0.09418603777885437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1686, 4.9806, 4.6407],
        [5.1686, 5.1617, 5.1682],
        [5.1686, 5.2422, 4.9650],
        [5.1686, 5.1540, 5.1672]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1115, step:0 
model_pd.l_p.mean(): 0.12129400670528412 
model_pd.l_d.mean(): -19.98457908630371 
model_pd.lagr.mean(): -19.863285064697266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4927], device='cuda:0')), ('power', tensor([-20.8686], device='cuda:0'))])
epoch£º1115	 i:0 	 global-step:22300	 l-p:0.12129400670528412
epoch£º1115	 i:1 	 global-step:22301	 l-p:0.13880600035190582
epoch£º1115	 i:2 	 global-step:22302	 l-p:0.1296684741973877
epoch£º1115	 i:3 	 global-step:22303	 l-p:0.12532734870910645
epoch£º1115	 i:4 	 global-step:22304	 l-p:0.21942466497421265
epoch£º1115	 i:5 	 global-step:22305	 l-p:0.15280692279338837
epoch£º1115	 i:6 	 global-step:22306	 l-p:0.1417524665594101
epoch£º1115	 i:7 	 global-step:22307	 l-p:0.11965534090995789
epoch£º1115	 i:8 	 global-step:22308	 l-p:0.1246846467256546
epoch£º1115	 i:9 	 global-step:22309	 l-p:0.10791318863630295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1671, 4.9375, 4.9468],
        [5.1671, 4.9149, 4.8931],
        [5.1671, 4.9094, 4.6056],
        [5.1671, 5.1671, 5.1671]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1116, step:0 
model_pd.l_p.mean(): 0.13906031847000122 
model_pd.l_d.mean(): -20.3488826751709 
model_pd.lagr.mean(): -20.209821701049805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4427], device='cuda:0')), ('power', tensor([-21.1879], device='cuda:0'))])
epoch£º1116	 i:0 	 global-step:22320	 l-p:0.13906031847000122
epoch£º1116	 i:1 	 global-step:22321	 l-p:0.17973406612873077
epoch£º1116	 i:2 	 global-step:22322	 l-p:0.12312930077314377
epoch£º1116	 i:3 	 global-step:22323	 l-p:0.17958670854568481
epoch£º1116	 i:4 	 global-step:22324	 l-p:0.1513136923313141
epoch£º1116	 i:5 	 global-step:22325	 l-p:0.1278887391090393
epoch£º1116	 i:6 	 global-step:22326	 l-p:0.11048141121864319
epoch£º1116	 i:7 	 global-step:22327	 l-p:0.10603363066911697
epoch£º1116	 i:8 	 global-step:22328	 l-p:0.13940054178237915
epoch£º1116	 i:9 	 global-step:22329	 l-p:0.14018946886062622
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1647, 5.1011, 5.1463],
        [5.1647, 5.1564, 5.1642],
        [5.1647, 4.9034, 4.8657],
        [5.1647, 5.1646, 5.1647]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1117, step:0 
model_pd.l_p.mean(): 0.13457147777080536 
model_pd.l_d.mean(): -20.550004959106445 
model_pd.lagr.mean(): -20.415433883666992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4177], device='cuda:0')), ('power', tensor([-21.3669], device='cuda:0'))])
epoch£º1117	 i:0 	 global-step:22340	 l-p:0.13457147777080536
epoch£º1117	 i:1 	 global-step:22341	 l-p:0.12533430755138397
epoch£º1117	 i:2 	 global-step:22342	 l-p:0.21589648723602295
epoch£º1117	 i:3 	 global-step:22343	 l-p:0.052830904722213745
epoch£º1117	 i:4 	 global-step:22344	 l-p:0.17643208801746368
epoch£º1117	 i:5 	 global-step:22345	 l-p:0.17115704715251923
epoch£º1117	 i:6 	 global-step:22346	 l-p:0.10804213583469391
epoch£º1117	 i:7 	 global-step:22347	 l-p:0.1045699417591095
epoch£º1117	 i:8 	 global-step:22348	 l-p:0.18150801956653595
epoch£º1117	 i:9 	 global-step:22349	 l-p:0.12966929376125336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1655, 5.1600, 5.1652],
        [5.1655, 4.9880, 5.0377],
        [5.1655, 5.0741, 5.1299],
        [5.1655, 5.1627, 5.1654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1118, step:0 
model_pd.l_p.mean(): 0.11984612047672272 
model_pd.l_d.mean(): -19.110754013061523 
model_pd.lagr.mean(): -18.990907669067383 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5050], device='cuda:0')), ('power', tensor([-19.9911], device='cuda:0'))])
epoch£º1118	 i:0 	 global-step:22360	 l-p:0.11984612047672272
epoch£º1118	 i:1 	 global-step:22361	 l-p:0.12805448472499847
epoch£º1118	 i:2 	 global-step:22362	 l-p:0.10074249655008316
epoch£º1118	 i:3 	 global-step:22363	 l-p:0.12561580538749695
epoch£º1118	 i:4 	 global-step:22364	 l-p:0.14611268043518066
epoch£º1118	 i:5 	 global-step:22365	 l-p:0.17309893667697906
epoch£º1118	 i:6 	 global-step:22366	 l-p:0.20189552009105682
epoch£º1118	 i:7 	 global-step:22367	 l-p:0.18541403114795685
epoch£º1118	 i:8 	 global-step:22368	 l-p:0.12356670200824738
epoch£º1118	 i:9 	 global-step:22369	 l-p:0.11127819120883942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1607, 4.9146, 4.9031],
        [5.1607, 5.2828, 5.0295],
        [5.1607, 5.1554, 5.1604],
        [5.1607, 5.5167, 5.4001]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1119, step:0 
model_pd.l_p.mean(): 0.10822023451328278 
model_pd.l_d.mean(): -19.122909545898438 
model_pd.lagr.mean(): -19.01468849182129 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5664], device='cuda:0')), ('power', tensor([-20.0671], device='cuda:0'))])
epoch£º1119	 i:0 	 global-step:22380	 l-p:0.10822023451328278
epoch£º1119	 i:1 	 global-step:22381	 l-p:0.12501749396324158
epoch£º1119	 i:2 	 global-step:22382	 l-p:0.16037048399448395
epoch£º1119	 i:3 	 global-step:22383	 l-p:0.16382472217082977
epoch£º1119	 i:4 	 global-step:22384	 l-p:0.1541483998298645
epoch£º1119	 i:5 	 global-step:22385	 l-p:0.11118971556425095
epoch£º1119	 i:6 	 global-step:22386	 l-p:0.11821387708187103
epoch£º1119	 i:7 	 global-step:22387	 l-p:0.14617440104484558
epoch£º1119	 i:8 	 global-step:22388	 l-p:0.149911567568779
epoch£º1119	 i:9 	 global-step:22389	 l-p:0.1584005504846573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1730, 4.8862, 4.6332],
        [5.1730, 5.0463, 5.1072],
        [5.1730, 5.1226, 5.1609],
        [5.1730, 5.2605, 4.9898]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1120, step:0 
model_pd.l_p.mean(): 0.14538365602493286 
model_pd.l_d.mean(): -19.28496742248535 
model_pd.lagr.mean(): -19.139583587646484 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5465], device='cuda:0')), ('power', tensor([-20.2116], device='cuda:0'))])
epoch£º1120	 i:0 	 global-step:22400	 l-p:0.14538365602493286
epoch£º1120	 i:1 	 global-step:22401	 l-p:0.18150851130485535
epoch£º1120	 i:2 	 global-step:22402	 l-p:0.07596301287412643
epoch£º1120	 i:3 	 global-step:22403	 l-p:0.12119945138692856
epoch£º1120	 i:4 	 global-step:22404	 l-p:0.14933910965919495
epoch£º1120	 i:5 	 global-step:22405	 l-p:0.12604345381259918
epoch£º1120	 i:6 	 global-step:22406	 l-p:0.141626238822937
epoch£º1120	 i:7 	 global-step:22407	 l-p:0.14009787142276764
epoch£º1120	 i:8 	 global-step:22408	 l-p:0.1321478933095932
epoch£º1120	 i:9 	 global-step:22409	 l-p:0.13258177042007446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1885, 5.0648, 4.7237],
        [5.1885, 5.2108, 4.9108],
        [5.1885, 5.1776, 5.1876],
        [5.1885, 5.1882, 5.1885]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1121, step:0 
model_pd.l_p.mean(): 0.12156467884778976 
model_pd.l_d.mean(): -20.742647171020508 
model_pd.lagr.mean(): -20.621082305908203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3946], device='cuda:0')), ('power', tensor([-21.5392], device='cuda:0'))])
epoch£º1121	 i:0 	 global-step:22420	 l-p:0.12156467884778976
epoch£º1121	 i:1 	 global-step:22421	 l-p:0.14941884577274323
epoch£º1121	 i:2 	 global-step:22422	 l-p:0.11278438568115234
epoch£º1121	 i:3 	 global-step:22423	 l-p:0.137195885181427
epoch£º1121	 i:4 	 global-step:22424	 l-p:0.13001666963100433
epoch£º1121	 i:5 	 global-step:22425	 l-p:0.09916360676288605
epoch£º1121	 i:6 	 global-step:22426	 l-p:0.1695217788219452
epoch£º1121	 i:7 	 global-step:22427	 l-p:0.10335353016853333
epoch£º1121	 i:8 	 global-step:22428	 l-p:0.1254720240831375
epoch£º1121	 i:9 	 global-step:22429	 l-p:0.17920684814453125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1794, 5.1359, 5.1700],
        [5.1794, 5.1456, 5.1734],
        [5.1794, 5.0896, 5.1449],
        [5.1794, 5.4365, 5.2579]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1122, step:0 
model_pd.l_p.mean(): 0.15970763564109802 
model_pd.l_d.mean(): -19.858144760131836 
model_pd.lagr.mean(): -19.698436737060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5261], device='cuda:0')), ('power', tensor([-20.7744], device='cuda:0'))])
epoch£º1122	 i:0 	 global-step:22440	 l-p:0.15970763564109802
epoch£º1122	 i:1 	 global-step:22441	 l-p:0.16167178750038147
epoch£º1122	 i:2 	 global-step:22442	 l-p:0.10618428885936737
epoch£º1122	 i:3 	 global-step:22443	 l-p:0.10703390091657639
epoch£º1122	 i:4 	 global-step:22444	 l-p:0.13025334477424622
epoch£º1122	 i:5 	 global-step:22445	 l-p:0.14005115628242493
epoch£º1122	 i:6 	 global-step:22446	 l-p:0.18536590039730072
epoch£º1122	 i:7 	 global-step:22447	 l-p:0.1328762173652649
epoch£º1122	 i:8 	 global-step:22448	 l-p:0.11534333974123001
epoch£º1122	 i:9 	 global-step:22449	 l-p:0.09545322507619858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1831, 4.8968, 4.6455],
        [5.1831, 4.9770, 4.6427],
        [5.1831, 5.0933, 5.1485],
        [5.1831, 5.1775, 5.1828]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1123, step:0 
model_pd.l_p.mean(): 0.11730940639972687 
model_pd.l_d.mean(): -20.212282180786133 
model_pd.lagr.mean(): -20.094972610473633 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4798], device='cuda:0')), ('power', tensor([-21.0872], device='cuda:0'))])
epoch£º1123	 i:0 	 global-step:22460	 l-p:0.11730940639972687
epoch£º1123	 i:1 	 global-step:22461	 l-p:0.10996770858764648
epoch£º1123	 i:2 	 global-step:22462	 l-p:0.16373588144779205
epoch£º1123	 i:3 	 global-step:22463	 l-p:0.1107562705874443
epoch£º1123	 i:4 	 global-step:22464	 l-p:0.11907958984375
epoch£º1123	 i:5 	 global-step:22465	 l-p:0.15544496476650238
epoch£º1123	 i:6 	 global-step:22466	 l-p:0.10078536719083786
epoch£º1123	 i:7 	 global-step:22467	 l-p:0.14872996509075165
epoch£º1123	 i:8 	 global-step:22468	 l-p:0.17665863037109375
epoch£º1123	 i:9 	 global-step:22469	 l-p:0.12771061062812805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1805, 5.0031, 4.6619],
        [5.1805, 5.0606, 5.1212],
        [5.1805, 4.9090, 4.8479],
        [5.1805, 5.0751, 5.1340]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1124, step:0 
model_pd.l_p.mean(): 0.16071383655071259 
model_pd.l_d.mean(): -20.515228271484375 
model_pd.lagr.mean(): -20.354515075683594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4213], device='cuda:0')), ('power', tensor([-21.3352], device='cuda:0'))])
epoch£º1124	 i:0 	 global-step:22480	 l-p:0.16071383655071259
epoch£º1124	 i:1 	 global-step:22481	 l-p:0.10739482939243317
epoch£º1124	 i:2 	 global-step:22482	 l-p:0.13015682995319366
epoch£º1124	 i:3 	 global-step:22483	 l-p:0.11456933617591858
epoch£º1124	 i:4 	 global-step:22484	 l-p:0.13690219819545746
epoch£º1124	 i:5 	 global-step:22485	 l-p:0.08859509229660034
epoch£º1124	 i:6 	 global-step:22486	 l-p:0.14892041683197021
epoch£º1124	 i:7 	 global-step:22487	 l-p:0.12701348960399628
epoch£º1124	 i:8 	 global-step:22488	 l-p:0.1714162826538086
epoch£º1124	 i:9 	 global-step:22489	 l-p:0.18369457125663757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1698, 5.0755, 5.1321],
        [5.1698, 4.8721, 4.6749],
        [5.1698, 4.8725, 4.7072],
        [5.1698, 5.1607, 5.1691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1125, step:0 
model_pd.l_p.mean(): 0.15792840719223022 
model_pd.l_d.mean(): -20.726037979125977 
model_pd.lagr.mean(): -20.5681095123291 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4148], device='cuda:0')), ('power', tensor([-21.5432], device='cuda:0'))])
epoch£º1125	 i:0 	 global-step:22500	 l-p:0.15792840719223022
epoch£º1125	 i:1 	 global-step:22501	 l-p:0.15382997691631317
epoch£º1125	 i:2 	 global-step:22502	 l-p:0.12775635719299316
epoch£º1125	 i:3 	 global-step:22503	 l-p:0.11045299470424652
epoch£º1125	 i:4 	 global-step:22504	 l-p:0.13927388191223145
epoch£º1125	 i:5 	 global-step:22505	 l-p:0.14114589989185333
epoch£º1125	 i:6 	 global-step:22506	 l-p:0.15453903377056122
epoch£º1125	 i:7 	 global-step:22507	 l-p:0.08609215915203094
epoch£º1125	 i:8 	 global-step:22508	 l-p:0.14157390594482422
epoch£º1125	 i:9 	 global-step:22509	 l-p:0.1589798629283905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1749, 5.1749, 5.1749],
        [5.1749, 5.0846, 5.1400],
        [5.1749, 5.0056, 5.0588],
        [5.1749, 4.8839, 4.7611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1126, step:0 
model_pd.l_p.mean(): 0.1057262048125267 
model_pd.l_d.mean(): -20.57756805419922 
model_pd.lagr.mean(): -20.47184181213379 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4324], device='cuda:0')), ('power', tensor([-21.4102], device='cuda:0'))])
epoch£º1126	 i:0 	 global-step:22520	 l-p:0.1057262048125267
epoch£º1126	 i:1 	 global-step:22521	 l-p:0.2077159285545349
epoch£º1126	 i:2 	 global-step:22522	 l-p:0.17241433262825012
epoch£º1126	 i:3 	 global-step:22523	 l-p:0.10913770645856857
epoch£º1126	 i:4 	 global-step:22524	 l-p:0.1179596334695816
epoch£º1126	 i:5 	 global-step:22525	 l-p:0.12924742698669434
epoch£º1126	 i:6 	 global-step:22526	 l-p:0.1597357839345932
epoch£º1126	 i:7 	 global-step:22527	 l-p:0.11739087104797363
epoch£º1126	 i:8 	 global-step:22528	 l-p:0.12933100759983063
epoch£º1126	 i:9 	 global-step:22529	 l-p:0.09108045697212219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1854, 4.8910, 4.7412],
        [5.1854, 5.1849, 5.1854],
        [5.1854, 5.1852, 5.1854],
        [5.1854, 4.9996, 4.6600]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1127, step:0 
model_pd.l_p.mean(): 0.17618457973003387 
model_pd.l_d.mean(): -20.53583526611328 
model_pd.lagr.mean(): -20.359651565551758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4510], device='cuda:0')), ('power', tensor([-21.3869], device='cuda:0'))])
epoch£º1127	 i:0 	 global-step:22540	 l-p:0.17618457973003387
epoch£º1127	 i:1 	 global-step:22541	 l-p:0.07791277021169662
epoch£º1127	 i:2 	 global-step:22542	 l-p:0.1331205517053604
epoch£º1127	 i:3 	 global-step:22543	 l-p:0.11176569759845734
epoch£º1127	 i:4 	 global-step:22544	 l-p:0.11230704188346863
epoch£º1127	 i:5 	 global-step:22545	 l-p:0.1528998464345932
epoch£º1127	 i:6 	 global-step:22546	 l-p:0.12892623245716095
epoch£º1127	 i:7 	 global-step:22547	 l-p:0.14720161259174347
epoch£º1127	 i:8 	 global-step:22548	 l-p:0.11045241355895996
epoch£º1127	 i:9 	 global-step:22549	 l-p:0.17928069829940796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1796, 5.1796, 5.1796],
        [5.1796, 5.1796, 5.1796],
        [5.1796, 5.1238, 5.1650],
        [5.1796, 4.8826, 4.7038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1128, step:0 
model_pd.l_p.mean(): 0.1841808408498764 
model_pd.l_d.mean(): -18.656105041503906 
model_pd.lagr.mean(): -18.471923828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5532], device='cuda:0')), ('power', tensor([-19.5779], device='cuda:0'))])
epoch£º1128	 i:0 	 global-step:22560	 l-p:0.1841808408498764
epoch£º1128	 i:1 	 global-step:22561	 l-p:0.1478343904018402
epoch£º1128	 i:2 	 global-step:22562	 l-p:0.11832290142774582
epoch£º1128	 i:3 	 global-step:22563	 l-p:0.10968223214149475
epoch£º1128	 i:4 	 global-step:22564	 l-p:0.12281625717878342
epoch£º1128	 i:5 	 global-step:22565	 l-p:0.15954627096652985
epoch£º1128	 i:6 	 global-step:22566	 l-p:0.10022461414337158
epoch£º1128	 i:7 	 global-step:22567	 l-p:0.1194930300116539
epoch£º1128	 i:8 	 global-step:22568	 l-p:0.15984919667243958
epoch£º1128	 i:9 	 global-step:22569	 l-p:0.12911471724510193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1721, 5.1721, 5.1722],
        [5.1721, 5.0792, 5.1354],
        [5.1721, 5.1597, 5.1710],
        [5.1721, 5.1559, 5.1704]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1129, step:0 
model_pd.l_p.mean(): 0.11487657576799393 
model_pd.l_d.mean(): -19.55912971496582 
model_pd.lagr.mean(): -19.44425392150879 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4595], device='cuda:0')), ('power', tensor([-20.4008], device='cuda:0'))])
epoch£º1129	 i:0 	 global-step:22580	 l-p:0.11487657576799393
epoch£º1129	 i:1 	 global-step:22581	 l-p:0.18429970741271973
epoch£º1129	 i:2 	 global-step:22582	 l-p:0.11321033537387848
epoch£º1129	 i:3 	 global-step:22583	 l-p:0.12394829839468002
epoch£º1129	 i:4 	 global-step:22584	 l-p:0.1375008076429367
epoch£º1129	 i:5 	 global-step:22585	 l-p:0.15075497329235077
epoch£º1129	 i:6 	 global-step:22586	 l-p:0.1845928281545639
epoch£º1129	 i:7 	 global-step:22587	 l-p:0.19641005992889404
epoch£º1129	 i:8 	 global-step:22588	 l-p:0.0865020751953125
epoch£º1129	 i:9 	 global-step:22589	 l-p:0.0896311104297638
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1682, 5.4063, 5.2164],
        [5.1682, 5.1682, 5.1682],
        [5.1682, 5.4365, 5.2646],
        [5.1682, 4.9155, 4.8939]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1130, step:0 
model_pd.l_p.mean(): 0.17251259088516235 
model_pd.l_d.mean(): -19.067590713500977 
model_pd.lagr.mean(): -18.895078659057617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5599], device='cuda:0')), ('power', tensor([-20.0040], device='cuda:0'))])
epoch£º1130	 i:0 	 global-step:22600	 l-p:0.17251259088516235
epoch£º1130	 i:1 	 global-step:22601	 l-p:0.1416569948196411
epoch£º1130	 i:2 	 global-step:22602	 l-p:0.11345592141151428
epoch£º1130	 i:3 	 global-step:22603	 l-p:0.11545362323522568
epoch£º1130	 i:4 	 global-step:22604	 l-p:0.16650623083114624
epoch£º1130	 i:5 	 global-step:22605	 l-p:0.17988824844360352
epoch£º1130	 i:6 	 global-step:22606	 l-p:0.1177140325307846
epoch£º1130	 i:7 	 global-step:22607	 l-p:0.09531816095113754
epoch£º1130	 i:8 	 global-step:22608	 l-p:0.12405012547969818
epoch£º1130	 i:9 	 global-step:22609	 l-p:0.1467197835445404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1736, 5.1736, 5.1736],
        [5.1736, 5.1300, 5.1642],
        [5.1736, 5.2307, 4.9453],
        [5.1736, 5.0777, 5.1347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1131, step:0 
model_pd.l_p.mean(): 0.16157054901123047 
model_pd.l_d.mean(): -19.818687438964844 
model_pd.lagr.mean(): -19.657115936279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4925], device='cuda:0')), ('power', tensor([-20.6994], device='cuda:0'))])
epoch£º1131	 i:0 	 global-step:22620	 l-p:0.16157054901123047
epoch£º1131	 i:1 	 global-step:22621	 l-p:0.14242562651634216
epoch£º1131	 i:2 	 global-step:22622	 l-p:0.13254152238368988
epoch£º1131	 i:3 	 global-step:22623	 l-p:0.1075480654835701
epoch£º1131	 i:4 	 global-step:22624	 l-p:0.12911425530910492
epoch£º1131	 i:5 	 global-step:22625	 l-p:0.18945035338401794
epoch£º1131	 i:6 	 global-step:22626	 l-p:0.10024881362915039
epoch£º1131	 i:7 	 global-step:22627	 l-p:0.09495929628610611
epoch£º1131	 i:8 	 global-step:22628	 l-p:0.18799257278442383
epoch£º1131	 i:9 	 global-step:22629	 l-p:0.12299241870641708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1132
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228]], device='cuda:0')
 pt:tensor([[5.1744, 5.4287, 5.2481],
        [5.1744, 4.9180, 4.8895],
        [5.1744, 4.9111, 4.6128],
        [5.1744, 4.8933, 4.8091]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1132, step:0 
model_pd.l_p.mean(): 0.12917329370975494 
model_pd.l_d.mean(): -20.423734664916992 
model_pd.lagr.mean(): -20.2945613861084 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4421], device='cuda:0')), ('power', tensor([-21.2635], device='cuda:0'))])
epoch£º1132	 i:0 	 global-step:22640	 l-p:0.12917329370975494
epoch£º1132	 i:1 	 global-step:22641	 l-p:0.17402680218219757
epoch£º1132	 i:2 	 global-step:22642	 l-p:0.159239262342453
epoch£º1132	 i:3 	 global-step:22643	 l-p:0.166829913854599
epoch£º1132	 i:4 	 global-step:22644	 l-p:0.06929071247577667
epoch£º1132	 i:5 	 global-step:22645	 l-p:0.11453533172607422
epoch£º1132	 i:6 	 global-step:22646	 l-p:0.13054914772510529
epoch£º1132	 i:7 	 global-step:22647	 l-p:0.13687030971050262
epoch£º1132	 i:8 	 global-step:22648	 l-p:0.11401659995317459
epoch£º1132	 i:9 	 global-step:22649	 l-p:0.15681153535842896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1829, 5.1829, 5.1829],
        [5.1829, 4.9572, 4.6300],
        [5.1829, 4.9642, 4.9848],
        [5.1829, 5.1335, 5.1711]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1133, step:0 
model_pd.l_p.mean(): 0.0826062560081482 
model_pd.l_d.mean(): -19.86454963684082 
model_pd.lagr.mean(): -19.781944274902344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4930], device='cuda:0')), ('power', tensor([-20.7466], device='cuda:0'))])
epoch£º1133	 i:0 	 global-step:22660	 l-p:0.0826062560081482
epoch£º1133	 i:1 	 global-step:22661	 l-p:0.13058577477931976
epoch£º1133	 i:2 	 global-step:22662	 l-p:0.1440003216266632
epoch£º1133	 i:3 	 global-step:22663	 l-p:0.12435843795537949
epoch£º1133	 i:4 	 global-step:22664	 l-p:0.15137384831905365
epoch£º1133	 i:5 	 global-step:22665	 l-p:0.12565642595291138
epoch£º1133	 i:6 	 global-step:22666	 l-p:0.11198443919420242
epoch£º1133	 i:7 	 global-step:22667	 l-p:0.16799667477607727
epoch£º1133	 i:8 	 global-step:22668	 l-p:0.11583816260099411
epoch£º1133	 i:9 	 global-step:22669	 l-p:0.1646040678024292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1909, 4.9392, 4.9170],
        [5.1909, 4.9622, 4.9717],
        [5.1909, 5.0996, 5.1553],
        [5.1909, 5.0117, 5.0603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1134, step:0 
model_pd.l_p.mean(): 0.15987879037857056 
model_pd.l_d.mean(): -20.145591735839844 
model_pd.lagr.mean(): -19.9857120513916 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4852], device='cuda:0')), ('power', tensor([-21.0248], device='cuda:0'))])
epoch£º1134	 i:0 	 global-step:22680	 l-p:0.15987879037857056
epoch£º1134	 i:1 	 global-step:22681	 l-p:0.1515330970287323
epoch£º1134	 i:2 	 global-step:22682	 l-p:0.13652700185775757
epoch£º1134	 i:3 	 global-step:22683	 l-p:0.1061897799372673
epoch£º1134	 i:4 	 global-step:22684	 l-p:0.17016898095607758
epoch£º1134	 i:5 	 global-step:22685	 l-p:0.1433010846376419
epoch£º1134	 i:6 	 global-step:22686	 l-p:0.10708175599575043
epoch£º1134	 i:7 	 global-step:22687	 l-p:0.12019261717796326
epoch£º1134	 i:8 	 global-step:22688	 l-p:0.12212669849395752
epoch£º1134	 i:9 	 global-step:22689	 l-p:0.09004630893468857
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1907, 5.4658, 5.2974],
        [5.1907, 4.9100, 4.6436],
        [5.1907, 5.1389, 5.1779],
        [5.1907, 4.8947, 4.7278]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1135, step:0 
model_pd.l_p.mean(): 0.13853412866592407 
model_pd.l_d.mean(): -20.078428268432617 
model_pd.lagr.mean(): -19.93989372253418 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5009], device='cuda:0')), ('power', tensor([-20.9727], device='cuda:0'))])
epoch£º1135	 i:0 	 global-step:22700	 l-p:0.13853412866592407
epoch£º1135	 i:1 	 global-step:22701	 l-p:0.1304781436920166
epoch£º1135	 i:2 	 global-step:22702	 l-p:0.12653093039989471
epoch£º1135	 i:3 	 global-step:22703	 l-p:0.12091919779777527
epoch£º1135	 i:4 	 global-step:22704	 l-p:0.14291761815547943
epoch£º1135	 i:5 	 global-step:22705	 l-p:0.06411826610565186
epoch£º1135	 i:6 	 global-step:22706	 l-p:0.13104166090488434
epoch£º1135	 i:7 	 global-step:22707	 l-p:0.112569160759449
epoch£º1135	 i:8 	 global-step:22708	 l-p:0.17802192270755768
epoch£º1135	 i:9 	 global-step:22709	 l-p:0.14636996388435364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1991, 5.1705, 5.1946],
        [5.1991, 5.1971, 5.1991],
        [5.1991, 5.0277, 5.0796],
        [5.1991, 4.9041, 4.7398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1136, step:0 
model_pd.l_p.mean(): 0.1070585623383522 
model_pd.l_d.mean(): -20.653545379638672 
model_pd.lagr.mean(): -20.546485900878906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3830], device='cuda:0')), ('power', tensor([-21.4364], device='cuda:0'))])
epoch£º1136	 i:0 	 global-step:22720	 l-p:0.1070585623383522
epoch£º1136	 i:1 	 global-step:22721	 l-p:0.0965876653790474
epoch£º1136	 i:2 	 global-step:22722	 l-p:0.1493818759918213
epoch£º1136	 i:3 	 global-step:22723	 l-p:0.13499665260314941
epoch£º1136	 i:4 	 global-step:22724	 l-p:0.12763556838035583
epoch£º1136	 i:5 	 global-step:22725	 l-p:0.09964302182197571
epoch£º1136	 i:6 	 global-step:22726	 l-p:0.1418229639530182
epoch£º1136	 i:7 	 global-step:22727	 l-p:0.13156235218048096
epoch£º1136	 i:8 	 global-step:22728	 l-p:0.10936447232961655
epoch£º1136	 i:9 	 global-step:22729	 l-p:0.16886773705482483
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2044, 5.2040, 5.2044],
        [5.2044, 5.2044, 5.2044],
        [5.2044, 5.1992, 5.2041],
        [5.2044, 4.9104, 4.7037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1137, step:0 
model_pd.l_p.mean(): 0.09273453801870346 
model_pd.l_d.mean(): -20.145814895629883 
model_pd.lagr.mean(): -20.05307960510254 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4660], device='cuda:0')), ('power', tensor([-21.0052], device='cuda:0'))])
epoch£º1137	 i:0 	 global-step:22740	 l-p:0.09273453801870346
epoch£º1137	 i:1 	 global-step:22741	 l-p:0.10415530949831009
epoch£º1137	 i:2 	 global-step:22742	 l-p:0.08676236122846603
epoch£º1137	 i:3 	 global-step:22743	 l-p:0.13659144937992096
epoch£º1137	 i:4 	 global-step:22744	 l-p:0.1544661819934845
epoch£º1137	 i:5 	 global-step:22745	 l-p:0.1722792237997055
epoch£º1137	 i:6 	 global-step:22746	 l-p:0.1504707634449005
epoch£º1137	 i:7 	 global-step:22747	 l-p:0.11615892499685287
epoch£º1137	 i:8 	 global-step:22748	 l-p:0.1679101586341858
epoch£º1137	 i:9 	 global-step:22749	 l-p:0.08780046552419662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1935, 5.1892, 5.1933],
        [5.1935, 5.1935, 5.1935],
        [5.1935, 5.1384, 5.1792],
        [5.1935, 5.1354, 4.8064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1138, step:0 
model_pd.l_p.mean(): 0.11293715238571167 
model_pd.l_d.mean(): -19.31290626525879 
model_pd.lagr.mean(): -19.199968338012695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4871], device='cuda:0')), ('power', tensor([-20.1786], device='cuda:0'))])
epoch£º1138	 i:0 	 global-step:22760	 l-p:0.11293715238571167
epoch£º1138	 i:1 	 global-step:22761	 l-p:0.1494939625263214
epoch£º1138	 i:2 	 global-step:22762	 l-p:0.14927753806114197
epoch£º1138	 i:3 	 global-step:22763	 l-p:0.12899698317050934
epoch£º1138	 i:4 	 global-step:22764	 l-p:0.18143326044082642
epoch£º1138	 i:5 	 global-step:22765	 l-p:0.08491818606853485
epoch£º1138	 i:6 	 global-step:22766	 l-p:0.11398274451494217
epoch£º1138	 i:7 	 global-step:22767	 l-p:0.1731484830379486
epoch£º1138	 i:8 	 global-step:22768	 l-p:0.1360931247472763
epoch£º1138	 i:9 	 global-step:22769	 l-p:0.10228362679481506
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1786, 5.1786, 5.1786],
        [5.1786, 5.1648, 5.1773],
        [5.1786, 4.9165, 4.6162],
        [5.1786, 5.1623, 5.1768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1139, step:0 
model_pd.l_p.mean(): 0.13369518518447876 
model_pd.l_d.mean(): -20.01795768737793 
model_pd.lagr.mean(): -19.884262084960938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4873], device='cuda:0')), ('power', tensor([-20.8969], device='cuda:0'))])
epoch£º1139	 i:0 	 global-step:22780	 l-p:0.13369518518447876
epoch£º1139	 i:1 	 global-step:22781	 l-p:0.12030424177646637
epoch£º1139	 i:2 	 global-step:22782	 l-p:0.14521756768226624
epoch£º1139	 i:3 	 global-step:22783	 l-p:0.13422627747058868
epoch£º1139	 i:4 	 global-step:22784	 l-p:0.15169470012187958
epoch£º1139	 i:5 	 global-step:22785	 l-p:0.16383208334445953
epoch£º1139	 i:6 	 global-step:22786	 l-p:0.26522478461265564
epoch£º1139	 i:7 	 global-step:22787	 l-p:0.09015819430351257
epoch£º1139	 i:8 	 global-step:22788	 l-p:0.11330015957355499
epoch£º1139	 i:9 	 global-step:22789	 l-p:0.08128976821899414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1586, 5.1586, 5.1586],
        [5.1586, 5.1074, 5.1462],
        [5.1586, 5.1575, 5.1586],
        [5.1586, 5.1586, 5.1586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1140, step:0 
model_pd.l_p.mean(): 0.170139878988266 
model_pd.l_d.mean(): -20.78488540649414 
model_pd.lagr.mean(): -20.61474609375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3870], device='cuda:0')), ('power', tensor([-21.5743], device='cuda:0'))])
epoch£º1140	 i:0 	 global-step:22800	 l-p:0.170139878988266
epoch£º1140	 i:1 	 global-step:22801	 l-p:0.14267513155937195
epoch£º1140	 i:2 	 global-step:22802	 l-p:0.06536703556776047
epoch£º1140	 i:3 	 global-step:22803	 l-p:0.22854934632778168
epoch£º1140	 i:4 	 global-step:22804	 l-p:0.1307351142168045
epoch£º1140	 i:5 	 global-step:22805	 l-p:0.14239466190338135
epoch£º1140	 i:6 	 global-step:22806	 l-p:0.14249995350837708
epoch£º1140	 i:7 	 global-step:22807	 l-p:0.1305193156003952
epoch£º1140	 i:8 	 global-step:22808	 l-p:0.18753695487976074
epoch£º1140	 i:9 	 global-step:22809	 l-p:0.12847238779067993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1500, 5.0175, 5.0789],
        [5.1500, 5.1493, 5.1500],
        [5.1500, 5.0305, 4.6866],
        [5.1500, 4.9182, 4.9280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1141, step:0 
model_pd.l_p.mean(): 0.12315802276134491 
model_pd.l_d.mean(): -18.935855865478516 
model_pd.lagr.mean(): -18.812698364257812 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6087], device='cuda:0')), ('power', tensor([-19.9204], device='cuda:0'))])
epoch£º1141	 i:0 	 global-step:22820	 l-p:0.12315802276134491
epoch£º1141	 i:1 	 global-step:22821	 l-p:0.21551159024238586
epoch£º1141	 i:2 	 global-step:22822	 l-p:0.14739452302455902
epoch£º1141	 i:3 	 global-step:22823	 l-p:0.11120449751615524
epoch£º1141	 i:4 	 global-step:22824	 l-p:0.12628495693206787
epoch£º1141	 i:5 	 global-step:22825	 l-p:0.18773311376571655
epoch£º1141	 i:6 	 global-step:22826	 l-p:0.20428602397441864
epoch£º1141	 i:7 	 global-step:22827	 l-p:0.09303877502679825
epoch£º1141	 i:8 	 global-step:22828	 l-p:0.14235974848270416
epoch£º1141	 i:9 	 global-step:22829	 l-p:0.11538995802402496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1596, 5.1591, 5.1596],
        [5.1596, 5.5269, 5.4167],
        [5.1596, 5.1595, 5.1596],
        [5.1596, 5.0891, 4.7556]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1142, step:0 
model_pd.l_p.mean(): 0.12640567123889923 
model_pd.l_d.mean(): -19.637142181396484 
model_pd.lagr.mean(): -19.5107364654541 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4525], device='cuda:0')), ('power', tensor([-20.4731], device='cuda:0'))])
epoch£º1142	 i:0 	 global-step:22840	 l-p:0.12640567123889923
epoch£º1142	 i:1 	 global-step:22841	 l-p:0.2523011863231659
epoch£º1142	 i:2 	 global-step:22842	 l-p:0.1365322768688202
epoch£º1142	 i:3 	 global-step:22843	 l-p:0.13362379372119904
epoch£º1142	 i:4 	 global-step:22844	 l-p:0.14918699860572815
epoch£º1142	 i:5 	 global-step:22845	 l-p:0.13062001764774323
epoch£º1142	 i:6 	 global-step:22846	 l-p:0.13274644315242767
epoch£º1142	 i:7 	 global-step:22847	 l-p:0.13256478309631348
epoch£º1142	 i:8 	 global-step:22848	 l-p:0.11346784234046936
epoch£º1142	 i:9 	 global-step:22849	 l-p:0.11252821981906891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1659, 5.4288, 5.2530],
        [5.1659, 5.1318, 5.1598],
        [5.1659, 4.9674, 5.0061],
        [5.1659, 5.1647, 5.1659]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1143, step:0 
model_pd.l_p.mean(): 0.10569974780082703 
model_pd.l_d.mean(): -19.974782943725586 
model_pd.lagr.mean(): -19.869083404541016 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4311], device='cuda:0')), ('power', tensor([-20.7947], device='cuda:0'))])
epoch£º1143	 i:0 	 global-step:22860	 l-p:0.10569974780082703
epoch£º1143	 i:1 	 global-step:22861	 l-p:0.13163886964321136
epoch£º1143	 i:2 	 global-step:22862	 l-p:0.17872901260852814
epoch£º1143	 i:3 	 global-step:22863	 l-p:0.14262248575687408
epoch£º1143	 i:4 	 global-step:22864	 l-p:0.1242612898349762
epoch£º1143	 i:5 	 global-step:22865	 l-p:0.1314399242401123
epoch£º1143	 i:6 	 global-step:22866	 l-p:0.10489821434020996
epoch£º1143	 i:7 	 global-step:22867	 l-p:0.1310821920633316
epoch£º1143	 i:8 	 global-step:22868	 l-p:0.21872080862522125
epoch£º1143	 i:9 	 global-step:22869	 l-p:0.14739269018173218
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1633, 5.1633, 5.1633],
        [5.1633, 5.1633, 5.1633],
        [5.1633, 5.1631, 5.1633],
        [5.1633, 4.8633, 4.6908]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1144, step:0 
model_pd.l_p.mean(): 0.09722401201725006 
model_pd.l_d.mean(): -20.27268409729004 
model_pd.lagr.mean(): -20.175460815429688 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4584], device='cuda:0')), ('power', tensor([-21.1265], device='cuda:0'))])
epoch£º1144	 i:0 	 global-step:22880	 l-p:0.09722401201725006
epoch£º1144	 i:1 	 global-step:22881	 l-p:0.15388089418411255
epoch£º1144	 i:2 	 global-step:22882	 l-p:0.11479655653238297
epoch£º1144	 i:3 	 global-step:22883	 l-p:0.15330003201961517
epoch£º1144	 i:4 	 global-step:22884	 l-p:0.10939930379390717
epoch£º1144	 i:5 	 global-step:22885	 l-p:0.09454840421676636
epoch£º1144	 i:6 	 global-step:22886	 l-p:0.1814209520816803
epoch£º1144	 i:7 	 global-step:22887	 l-p:0.12214378267526627
epoch£º1144	 i:8 	 global-step:22888	 l-p:0.2511061131954193
epoch£º1144	 i:9 	 global-step:22889	 l-p:0.13654638826847076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1643, 5.0734, 5.1292],
        [5.1643, 5.0210, 5.0814],
        [5.1643, 5.1578, 5.1639],
        [5.1643, 5.1532, 5.1634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1145, step:0 
model_pd.l_p.mean(): 0.1175331249833107 
model_pd.l_d.mean(): -19.787731170654297 
model_pd.lagr.mean(): -19.670198440551758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5137], device='cuda:0')), ('power', tensor([-20.6898], device='cuda:0'))])
epoch£º1145	 i:0 	 global-step:22900	 l-p:0.1175331249833107
epoch£º1145	 i:1 	 global-step:22901	 l-p:0.13034264743328094
epoch£º1145	 i:2 	 global-step:22902	 l-p:0.12134301662445068
epoch£º1145	 i:3 	 global-step:22903	 l-p:0.17212624847888947
epoch£º1145	 i:4 	 global-step:22904	 l-p:0.1338578164577484
epoch£º1145	 i:5 	 global-step:22905	 l-p:0.1455855667591095
epoch£º1145	 i:6 	 global-step:22906	 l-p:0.0678814947605133
epoch£º1145	 i:7 	 global-step:22907	 l-p:0.17727245390415192
epoch£º1145	 i:8 	 global-step:22908	 l-p:0.15676596760749817
epoch£º1145	 i:9 	 global-step:22909	 l-p:0.18756753206253052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1663, 5.1663, 5.1663],
        [5.1663, 5.5599, 5.4666],
        [5.1663, 4.8672, 4.7053],
        [5.1663, 4.8690, 4.7233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1146, step:0 
model_pd.l_p.mean(): 0.17643806338310242 
model_pd.l_d.mean(): -19.721885681152344 
model_pd.lagr.mean(): -19.545448303222656 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5497], device='cuda:0')), ('power', tensor([-20.6600], device='cuda:0'))])
epoch£º1146	 i:0 	 global-step:22920	 l-p:0.17643806338310242
epoch£º1146	 i:1 	 global-step:22921	 l-p:0.15691933035850525
epoch£º1146	 i:2 	 global-step:22922	 l-p:0.10817621648311615
epoch£º1146	 i:3 	 global-step:22923	 l-p:0.10966356843709946
epoch£º1146	 i:4 	 global-step:22924	 l-p:0.14894093573093414
epoch£º1146	 i:5 	 global-step:22925	 l-p:0.11032058298587799
epoch£º1146	 i:6 	 global-step:22926	 l-p:0.17032752931118011
epoch£º1146	 i:7 	 global-step:22927	 l-p:0.14720822870731354
epoch£º1146	 i:8 	 global-step:22928	 l-p:0.12712202966213226
epoch£º1146	 i:9 	 global-step:22929	 l-p:0.1318548172712326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1709, 5.2930, 5.0385],
        [5.1709, 5.1704, 5.1709],
        [5.1709, 5.1698, 5.1709],
        [5.1709, 4.9800, 4.6382]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1147, step:0 
model_pd.l_p.mean(): 0.08758573979139328 
model_pd.l_d.mean(): -20.090784072875977 
model_pd.lagr.mean(): -20.003198623657227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4981], device='cuda:0')), ('power', tensor([-20.9823], device='cuda:0'))])
epoch£º1147	 i:0 	 global-step:22940	 l-p:0.08758573979139328
epoch£º1147	 i:1 	 global-step:22941	 l-p:0.14975705742835999
epoch£º1147	 i:2 	 global-step:22942	 l-p:0.1314118206501007
epoch£º1147	 i:3 	 global-step:22943	 l-p:0.14807195961475372
epoch£º1147	 i:4 	 global-step:22944	 l-p:0.1300099641084671
epoch£º1147	 i:5 	 global-step:22945	 l-p:0.15171605348587036
epoch£º1147	 i:6 	 global-step:22946	 l-p:0.14280444383621216
epoch£º1147	 i:7 	 global-step:22947	 l-p:0.13592034578323364
epoch£º1147	 i:8 	 global-step:22948	 l-p:0.1254734843969345
epoch£º1147	 i:9 	 global-step:22949	 l-p:0.18098506331443787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1686, 5.4191, 5.2359],
        [5.1686, 5.0860, 5.1391],
        [5.1686, 5.0510, 5.1118],
        [5.1686, 5.1641, 5.1684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1148, step:0 
model_pd.l_p.mean(): 0.1193886324763298 
model_pd.l_d.mean(): -19.603668212890625 
model_pd.lagr.mean(): -19.48427963256836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5173], device='cuda:0')), ('power', tensor([-20.5061], device='cuda:0'))])
epoch£º1148	 i:0 	 global-step:22960	 l-p:0.1193886324763298
epoch£º1148	 i:1 	 global-step:22961	 l-p:0.1612679660320282
epoch£º1148	 i:2 	 global-step:22962	 l-p:0.15333198010921478
epoch£º1148	 i:3 	 global-step:22963	 l-p:0.14919646084308624
epoch£º1148	 i:4 	 global-step:22964	 l-p:0.13965773582458496
epoch£º1148	 i:5 	 global-step:22965	 l-p:0.1381792575120926
epoch£º1148	 i:6 	 global-step:22966	 l-p:0.13035285472869873
epoch£º1148	 i:7 	 global-step:22967	 l-p:0.11643964797258377
epoch£º1148	 i:8 	 global-step:22968	 l-p:0.130144402384758
epoch£º1148	 i:9 	 global-step:22969	 l-p:0.17327262461185455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1621, 5.0056, 4.6593],
        [5.1621, 4.9007, 4.5948],
        [5.1621, 5.0264, 5.0875],
        [5.1621, 5.0438, 5.1048]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1149, step:0 
model_pd.l_p.mean(): 0.18033486604690552 
model_pd.l_d.mean(): -19.71925163269043 
model_pd.lagr.mean(): -19.538917541503906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5262], device='cuda:0')), ('power', tensor([-20.6330], device='cuda:0'))])
epoch£º1149	 i:0 	 global-step:22980	 l-p:0.18033486604690552
epoch£º1149	 i:1 	 global-step:22981	 l-p:0.07202144712209702
epoch£º1149	 i:2 	 global-step:22982	 l-p:0.1863090544939041
epoch£º1149	 i:3 	 global-step:22983	 l-p:0.1532648205757141
epoch£º1149	 i:4 	 global-step:22984	 l-p:0.09948325902223587
epoch£º1149	 i:5 	 global-step:22985	 l-p:0.12287990003824234
epoch£º1149	 i:6 	 global-step:22986	 l-p:0.1380799114704132
epoch£º1149	 i:7 	 global-step:22987	 l-p:0.17402468621730804
epoch£º1149	 i:8 	 global-step:22988	 l-p:0.18941737711429596
epoch£º1149	 i:9 	 global-step:22989	 l-p:0.08156868070363998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1717, 5.1717, 5.1717],
        [5.1717, 5.1714, 5.1717],
        [5.1717, 5.1715, 5.1717],
        [5.1717, 5.1712, 5.1717]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1150, step:0 
model_pd.l_p.mean(): 0.1149282231926918 
model_pd.l_d.mean(): -20.355972290039062 
model_pd.lagr.mean(): -20.241044998168945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4169], device='cuda:0')), ('power', tensor([-21.1684], device='cuda:0'))])
epoch£º1150	 i:0 	 global-step:23000	 l-p:0.1149282231926918
epoch£º1150	 i:1 	 global-step:23001	 l-p:0.11687406897544861
epoch£º1150	 i:2 	 global-step:23002	 l-p:0.09051692485809326
epoch£º1150	 i:3 	 global-step:23003	 l-p:0.14076431095600128
epoch£º1150	 i:4 	 global-step:23004	 l-p:0.2180825024843216
epoch£º1150	 i:5 	 global-step:23005	 l-p:0.14509424567222595
epoch£º1150	 i:6 	 global-step:23006	 l-p:0.10635437816381454
epoch£º1150	 i:7 	 global-step:23007	 l-p:0.1536327600479126
epoch£º1150	 i:8 	 global-step:23008	 l-p:0.15530066192150116
epoch£º1150	 i:9 	 global-step:23009	 l-p:0.14482204616069794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1731, 5.1731, 5.1731],
        [5.1731, 5.1731, 5.1731],
        [5.1731, 5.0827, 5.1384],
        [5.1731, 4.9020, 4.8471]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1151, step:0 
model_pd.l_p.mean(): 0.1854272484779358 
model_pd.l_d.mean(): -20.51888084411621 
model_pd.lagr.mean(): -20.333454132080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4257], device='cuda:0')), ('power', tensor([-21.3435], device='cuda:0'))])
epoch£º1151	 i:0 	 global-step:23020	 l-p:0.1854272484779358
epoch£º1151	 i:1 	 global-step:23021	 l-p:0.10260837525129318
epoch£º1151	 i:2 	 global-step:23022	 l-p:0.12552492320537567
epoch£º1151	 i:3 	 global-step:23023	 l-p:0.14064054191112518
epoch£º1151	 i:4 	 global-step:23024	 l-p:0.1210922971367836
epoch£º1151	 i:5 	 global-step:23025	 l-p:0.15464819967746735
epoch£º1151	 i:6 	 global-step:23026	 l-p:0.10562029480934143
epoch£º1151	 i:7 	 global-step:23027	 l-p:0.12928074598312378
epoch£º1151	 i:8 	 global-step:23028	 l-p:0.128957137465477
epoch£º1151	 i:9 	 global-step:23029	 l-p:0.15585064888000488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1152
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1871, 5.1864, 5.1871],
        [5.1871, 4.8890, 4.7089],
        [5.1871, 5.1871, 5.1871],
        [5.1871, 4.9939, 5.0354]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1152, step:0 
model_pd.l_p.mean(): 0.14331667125225067 
model_pd.l_d.mean(): -20.734983444213867 
model_pd.lagr.mean(): -20.59166717529297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3753], device='cuda:0')), ('power', tensor([-21.5114], device='cuda:0'))])
epoch£º1152	 i:0 	 global-step:23040	 l-p:0.14331667125225067
epoch£º1152	 i:1 	 global-step:23041	 l-p:0.09854518622159958
epoch£º1152	 i:2 	 global-step:23042	 l-p:0.13027501106262207
epoch£º1152	 i:3 	 global-step:23043	 l-p:0.05368621274828911
epoch£º1152	 i:4 	 global-step:23044	 l-p:0.1633126437664032
epoch£º1152	 i:5 	 global-step:23045	 l-p:0.13444676995277405
epoch£º1152	 i:6 	 global-step:23046	 l-p:0.11380809545516968
epoch£º1152	 i:7 	 global-step:23047	 l-p:0.14287374913692474
epoch£º1152	 i:8 	 global-step:23048	 l-p:0.17030158638954163
epoch£º1152	 i:9 	 global-step:23049	 l-p:0.16948272287845612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1908, 5.1897, 5.1908],
        [5.1908, 5.1376, 5.1775],
        [5.1908, 5.0667, 5.1276],
        [5.1908, 5.1765, 5.1894]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1153, step:0 
model_pd.l_p.mean(): 0.16343818604946136 
model_pd.l_d.mean(): -20.306232452392578 
model_pd.lagr.mean(): -20.142793655395508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4637], device='cuda:0')), ('power', tensor([-21.1662], device='cuda:0'))])
epoch£º1153	 i:0 	 global-step:23060	 l-p:0.16343818604946136
epoch£º1153	 i:1 	 global-step:23061	 l-p:0.1508561372756958
epoch£º1153	 i:2 	 global-step:23062	 l-p:0.12001506239175797
epoch£º1153	 i:3 	 global-step:23063	 l-p:0.1187998354434967
epoch£º1153	 i:4 	 global-step:23064	 l-p:0.1197466030716896
epoch£º1153	 i:5 	 global-step:23065	 l-p:0.08052144199609756
epoch£º1153	 i:6 	 global-step:23066	 l-p:0.1279674917459488
epoch£º1153	 i:7 	 global-step:23067	 l-p:0.10757597535848618
epoch£º1153	 i:8 	 global-step:23068	 l-p:0.15926994383335114
epoch£º1153	 i:9 	 global-step:23069	 l-p:0.17402894794940948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1806, 5.1405, 5.1725],
        [5.1806, 5.0511, 5.1123],
        [5.1806, 5.1806, 5.1806],
        [5.1806, 5.1465, 5.1745]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1154, step:0 
model_pd.l_p.mean(): 0.13758908212184906 
model_pd.l_d.mean(): -20.51461410522461 
model_pd.lagr.mean(): -20.377025604248047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4365], device='cuda:0')), ('power', tensor([-21.3503], device='cuda:0'))])
epoch£º1154	 i:0 	 global-step:23080	 l-p:0.13758908212184906
epoch£º1154	 i:1 	 global-step:23081	 l-p:0.1576766073703766
epoch£º1154	 i:2 	 global-step:23082	 l-p:0.12164115905761719
epoch£º1154	 i:3 	 global-step:23083	 l-p:0.13981294631958008
epoch£º1154	 i:4 	 global-step:23084	 l-p:0.1704978197813034
epoch£º1154	 i:5 	 global-step:23085	 l-p:0.13661649823188782
epoch£º1154	 i:6 	 global-step:23086	 l-p:0.06401042640209198
epoch£º1154	 i:7 	 global-step:23087	 l-p:0.158732071518898
epoch£º1154	 i:8 	 global-step:23088	 l-p:0.1356290578842163
epoch£º1154	 i:9 	 global-step:23089	 l-p:0.1309562772512436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1801, 5.1752, 5.1798],
        [5.1801, 5.1801, 5.1801],
        [5.1801, 5.1801, 5.1801],
        [5.1801, 5.4875, 5.3383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1155, step:0 
model_pd.l_p.mean(): 0.09445302933454514 
model_pd.l_d.mean(): -19.18880271911621 
model_pd.lagr.mean(): -19.094348907470703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5394], device='cuda:0')), ('power', tensor([-20.1063], device='cuda:0'))])
epoch£º1155	 i:0 	 global-step:23100	 l-p:0.09445302933454514
epoch£º1155	 i:1 	 global-step:23101	 l-p:0.12440307438373566
epoch£º1155	 i:2 	 global-step:23102	 l-p:0.09977328777313232
epoch£º1155	 i:3 	 global-step:23103	 l-p:0.21374863386154175
epoch£º1155	 i:4 	 global-step:23104	 l-p:0.1747949719429016
epoch£º1155	 i:5 	 global-step:23105	 l-p:0.14863061904907227
epoch£º1155	 i:6 	 global-step:23106	 l-p:0.14939512312412262
epoch£º1155	 i:7 	 global-step:23107	 l-p:0.11266238987445831
epoch£º1155	 i:8 	 global-step:23108	 l-p:0.11315459758043289
epoch£º1155	 i:9 	 global-step:23109	 l-p:0.12753431499004364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1762, 5.1306, 5.1660],
        [5.1762, 5.1762, 5.1762],
        [5.1762, 5.1361, 5.1681],
        [5.1762, 4.8981, 4.6175]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1156, step:0 
model_pd.l_p.mean(): 0.15953724086284637 
model_pd.l_d.mean(): -20.83987808227539 
model_pd.lagr.mean(): -20.680341720581055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3788], device='cuda:0')), ('power', tensor([-21.6219], device='cuda:0'))])
epoch£º1156	 i:0 	 global-step:23120	 l-p:0.15953724086284637
epoch£º1156	 i:1 	 global-step:23121	 l-p:0.13261838257312775
epoch£º1156	 i:2 	 global-step:23122	 l-p:0.10937166213989258
epoch£º1156	 i:3 	 global-step:23123	 l-p:0.19331413507461548
epoch£º1156	 i:4 	 global-step:23124	 l-p:0.17313207685947418
epoch£º1156	 i:5 	 global-step:23125	 l-p:0.14111995697021484
epoch£º1156	 i:6 	 global-step:23126	 l-p:0.15545415878295898
epoch£º1156	 i:7 	 global-step:23127	 l-p:0.1161608099937439
epoch£º1156	 i:8 	 global-step:23128	 l-p:0.096644826233387
epoch£º1156	 i:9 	 global-step:23129	 l-p:0.12222181260585785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1628, 5.1628, 5.1628],
        [5.1628, 4.9011, 4.8675],
        [5.1628, 5.1598, 5.1627],
        [5.1628, 5.0608, 5.1194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1157, step:0 
model_pd.l_p.mean(): 0.12762270867824554 
model_pd.l_d.mean(): -20.06851577758789 
model_pd.lagr.mean(): -19.940893173217773 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4962], device='cuda:0')), ('power', tensor([-20.9577], device='cuda:0'))])
epoch£º1157	 i:0 	 global-step:23140	 l-p:0.12762270867824554
epoch£º1157	 i:1 	 global-step:23141	 l-p:0.17887654900550842
epoch£º1157	 i:2 	 global-step:23142	 l-p:0.14182345569133759
epoch£º1157	 i:3 	 global-step:23143	 l-p:0.1731235831975937
epoch£º1157	 i:4 	 global-step:23144	 l-p:0.12903743982315063
epoch£º1157	 i:5 	 global-step:23145	 l-p:0.12850236892700195
epoch£º1157	 i:6 	 global-step:23146	 l-p:0.1914937049150467
epoch£º1157	 i:7 	 global-step:23147	 l-p:0.08753558993339539
epoch£º1157	 i:8 	 global-step:23148	 l-p:0.12373816967010498
epoch£º1157	 i:9 	 global-step:23149	 l-p:0.11356765776872635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1756, 5.0580, 5.1188],
        [5.1756, 5.1756, 5.1756],
        [5.1756, 5.1756, 5.1756],
        [5.1756, 4.9017, 4.8406]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1158, step:0 
model_pd.l_p.mean(): 0.1525478959083557 
model_pd.l_d.mean(): -19.898561477661133 
model_pd.lagr.mean(): -19.746013641357422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4964], device='cuda:0')), ('power', tensor([-20.7848], device='cuda:0'))])
epoch£º1158	 i:0 	 global-step:23160	 l-p:0.1525478959083557
epoch£º1158	 i:1 	 global-step:23161	 l-p:0.1229330450296402
epoch£º1158	 i:2 	 global-step:23162	 l-p:0.14047707617282867
epoch£º1158	 i:3 	 global-step:23163	 l-p:0.1518511027097702
epoch£º1158	 i:4 	 global-step:23164	 l-p:0.11853143572807312
epoch£º1158	 i:5 	 global-step:23165	 l-p:0.13064534962177277
epoch£º1158	 i:6 	 global-step:23166	 l-p:0.13131894171237946
epoch£º1158	 i:7 	 global-step:23167	 l-p:0.190082386136055
epoch£º1158	 i:8 	 global-step:23168	 l-p:0.11320405453443527
epoch£º1158	 i:9 	 global-step:23169	 l-p:0.13165244460105896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1629, 4.8631, 4.6455],
        [5.1629, 5.1628, 5.1629],
        [5.1629, 5.1625, 5.1629],
        [5.1629, 5.1052, 5.1475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1159, step:0 
model_pd.l_p.mean(): 0.19351410865783691 
model_pd.l_d.mean(): -20.390031814575195 
model_pd.lagr.mean(): -20.196517944335938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4460], device='cuda:0')), ('power', tensor([-21.2333], device='cuda:0'))])
epoch£º1159	 i:0 	 global-step:23180	 l-p:0.19351410865783691
epoch£º1159	 i:1 	 global-step:23181	 l-p:0.15095821022987366
epoch£º1159	 i:2 	 global-step:23182	 l-p:0.12030892819166183
epoch£º1159	 i:3 	 global-step:23183	 l-p:0.12504000961780548
epoch£º1159	 i:4 	 global-step:23184	 l-p:0.13245299458503723
epoch£º1159	 i:5 	 global-step:23185	 l-p:0.1097404807806015
epoch£º1159	 i:6 	 global-step:23186	 l-p:0.14698418974876404
epoch£º1159	 i:7 	 global-step:23187	 l-p:0.1473386287689209
epoch£º1159	 i:8 	 global-step:23188	 l-p:0.14234863221645355
epoch£º1159	 i:9 	 global-step:23189	 l-p:0.16270849108695984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1591, 4.8802, 4.5951],
        [5.1591, 5.1535, 5.1588],
        [5.1591, 4.9147, 4.5919],
        [5.1591, 5.1590, 5.1591]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1160, step:0 
model_pd.l_p.mean(): 0.12971395254135132 
model_pd.l_d.mean(): -19.95340919494629 
model_pd.lagr.mean(): -19.82369613647461 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4542], device='cuda:0')), ('power', tensor([-20.7969], device='cuda:0'))])
epoch£º1160	 i:0 	 global-step:23200	 l-p:0.12971395254135132
epoch£º1160	 i:1 	 global-step:23201	 l-p:0.17495089769363403
epoch£º1160	 i:2 	 global-step:23202	 l-p:0.15359017252922058
epoch£º1160	 i:3 	 global-step:23203	 l-p:0.12455722689628601
epoch£º1160	 i:4 	 global-step:23204	 l-p:0.16169704496860504
epoch£º1160	 i:5 	 global-step:23205	 l-p:0.0955091044306755
epoch£º1160	 i:6 	 global-step:23206	 l-p:0.13031117618083954
epoch£º1160	 i:7 	 global-step:23207	 l-p:0.10711586475372314
epoch£º1160	 i:8 	 global-step:23208	 l-p:0.2455877959728241
epoch£º1160	 i:9 	 global-step:23209	 l-p:0.1384851485490799
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1542, 4.9525, 4.6100],
        [5.1542, 5.1542, 5.1542],
        [5.1542, 5.0515, 5.1104],
        [5.1542, 4.9292, 4.9470]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1161, step:0 
model_pd.l_p.mean(): 0.086967334151268 
model_pd.l_d.mean(): -18.48299789428711 
model_pd.lagr.mean(): -18.39603042602539 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6374], device='cuda:0')), ('power', tensor([-19.4888], device='cuda:0'))])
epoch£º1161	 i:0 	 global-step:23220	 l-p:0.086967334151268
epoch£º1161	 i:1 	 global-step:23221	 l-p:0.1365935355424881
epoch£º1161	 i:2 	 global-step:23222	 l-p:0.11232779175043106
epoch£º1161	 i:3 	 global-step:23223	 l-p:0.13514167070388794
epoch£º1161	 i:4 	 global-step:23224	 l-p:0.10077321529388428
epoch£º1161	 i:5 	 global-step:23225	 l-p:0.2188391238451004
epoch£º1161	 i:6 	 global-step:23226	 l-p:0.18762733042240143
epoch£º1161	 i:7 	 global-step:23227	 l-p:0.18925738334655762
epoch£º1161	 i:8 	 global-step:23228	 l-p:0.11857414245605469
epoch£º1161	 i:9 	 global-step:23229	 l-p:0.1474827378988266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1162
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228]], device='cuda:0')
 pt:tensor([[5.1685, 5.1017, 4.7684],
        [5.1685, 5.0267, 5.0874],
        [5.1685, 4.9097, 4.8812],
        [5.1685, 4.8985, 4.6019]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1162, step:0 
model_pd.l_p.mean(): 0.2153238207101822 
model_pd.l_d.mean(): -20.056543350219727 
model_pd.lagr.mean(): -19.841218948364258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5185], device='cuda:0')), ('power', tensor([-20.9687], device='cuda:0'))])
epoch£º1162	 i:0 	 global-step:23240	 l-p:0.2153238207101822
epoch£º1162	 i:1 	 global-step:23241	 l-p:0.09433601796627045
epoch£º1162	 i:2 	 global-step:23242	 l-p:0.12430455535650253
epoch£º1162	 i:3 	 global-step:23243	 l-p:0.14923128485679626
epoch£º1162	 i:4 	 global-step:23244	 l-p:0.12159886956214905
epoch£º1162	 i:5 	 global-step:23245	 l-p:0.08008889108896255
epoch£º1162	 i:6 	 global-step:23246	 l-p:0.19093723595142365
epoch£º1162	 i:7 	 global-step:23247	 l-p:0.11525620520114899
epoch£º1162	 i:8 	 global-step:23248	 l-p:0.13532185554504395
epoch£º1162	 i:9 	 global-step:23249	 l-p:0.1418159306049347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1867, 5.0393, 4.6939],
        [5.1867, 5.1867, 5.1867],
        [5.1867, 5.1867, 5.1867],
        [5.1867, 5.5119, 5.3734]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1163, step:0 
model_pd.l_p.mean(): 0.11423199623823166 
model_pd.l_d.mean(): -19.696624755859375 
model_pd.lagr.mean(): -19.582393646240234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4856], device='cuda:0')), ('power', tensor([-20.5679], device='cuda:0'))])
epoch£º1163	 i:0 	 global-step:23260	 l-p:0.11423199623823166
epoch£º1163	 i:1 	 global-step:23261	 l-p:0.1014985665678978
epoch£º1163	 i:2 	 global-step:23262	 l-p:0.18756654858589172
epoch£º1163	 i:3 	 global-step:23263	 l-p:0.11183252185583115
epoch£º1163	 i:4 	 global-step:23264	 l-p:0.15945562720298767
epoch£º1163	 i:5 	 global-step:23265	 l-p:0.1445475071668625
epoch£º1163	 i:6 	 global-step:23266	 l-p:0.0966779962182045
epoch£º1163	 i:7 	 global-step:23267	 l-p:0.08077196031808853
epoch£º1163	 i:8 	 global-step:23268	 l-p:0.1593925952911377
epoch£º1163	 i:9 	 global-step:23269	 l-p:0.15902358293533325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1164
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1270,  0.0638,  1.0000,  0.0321,
          1.0000,  0.5026, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1497,  0.0795,  1.0000,  0.0422,
          1.0000,  0.5310, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6301, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]], device='cuda:0')
 pt:tensor([[5.1899, 5.0547, 5.1156],
        [5.1899, 5.0238, 5.0786],
        [5.1899, 4.9218, 4.8724],
        [5.1899, 5.5648, 5.4580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1164, step:0 
model_pd.l_p.mean(): 0.15482649207115173 
model_pd.l_d.mean(): -19.930532455444336 
model_pd.lagr.mean(): -19.775705337524414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4972], device='cuda:0')), ('power', tensor([-20.8181], device='cuda:0'))])
epoch£º1164	 i:0 	 global-step:23280	 l-p:0.15482649207115173
epoch£º1164	 i:1 	 global-step:23281	 l-p:0.1053367406129837
epoch£º1164	 i:2 	 global-step:23282	 l-p:0.18898643553256989
epoch£º1164	 i:3 	 global-step:23283	 l-p:0.04350799322128296
epoch£º1164	 i:4 	 global-step:23284	 l-p:0.1314689964056015
epoch£º1164	 i:5 	 global-step:23285	 l-p:0.15640339255332947
epoch£º1164	 i:6 	 global-step:23286	 l-p:0.13071250915527344
epoch£º1164	 i:7 	 global-step:23287	 l-p:0.1389206051826477
epoch£º1164	 i:8 	 global-step:23288	 l-p:0.14896923303604126
epoch£º1164	 i:9 	 global-step:23289	 l-p:0.11477271467447281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1890, 4.9375, 4.6247],
        [5.1890, 5.0585, 4.7139],
        [5.1890, 4.9655, 4.9828],
        [5.1890, 5.1890, 5.1890]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1165, step:0 
model_pd.l_p.mean(): 0.11708999425172806 
model_pd.l_d.mean(): -19.79937171936035 
model_pd.lagr.mean(): -19.682281494140625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5143], device='cuda:0')), ('power', tensor([-20.7023], device='cuda:0'))])
epoch£º1165	 i:0 	 global-step:23300	 l-p:0.11708999425172806
epoch£º1165	 i:1 	 global-step:23301	 l-p:0.07671411335468292
epoch£º1165	 i:2 	 global-step:23302	 l-p:0.15099473297595978
epoch£º1165	 i:3 	 global-step:23303	 l-p:0.17433814704418182
epoch£º1165	 i:4 	 global-step:23304	 l-p:0.1049627736210823
epoch£º1165	 i:5 	 global-step:23305	 l-p:0.10590670257806778
epoch£º1165	 i:6 	 global-step:23306	 l-p:0.09027921408414841
epoch£º1165	 i:7 	 global-step:23307	 l-p:0.21871311962604523
epoch£º1165	 i:8 	 global-step:23308	 l-p:0.1937829554080963
epoch£º1165	 i:9 	 global-step:23309	 l-p:0.11316566169261932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1772, 5.1772, 5.1772],
        [5.1772, 5.0337, 5.0942],
        [5.1772, 5.1627, 5.1758],
        [5.1772, 5.0416, 5.1028]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1166, step:0 
model_pd.l_p.mean(): 0.17451192438602448 
model_pd.l_d.mean(): -20.361059188842773 
model_pd.lagr.mean(): -20.186546325683594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4501], device='cuda:0')), ('power', tensor([-21.2080], device='cuda:0'))])
epoch£º1166	 i:0 	 global-step:23320	 l-p:0.17451192438602448
epoch£º1166	 i:1 	 global-step:23321	 l-p:0.15551424026489258
epoch£º1166	 i:2 	 global-step:23322	 l-p:0.15091612935066223
epoch£º1166	 i:3 	 global-step:23323	 l-p:0.10825759917497635
epoch£º1166	 i:4 	 global-step:23324	 l-p:0.141469806432724
epoch£º1166	 i:5 	 global-step:23325	 l-p:0.10769248753786087
epoch£º1166	 i:6 	 global-step:23326	 l-p:0.12222070246934891
epoch£º1166	 i:7 	 global-step:23327	 l-p:0.11864671111106873
epoch£º1166	 i:8 	 global-step:23328	 l-p:0.14638425409793854
epoch£º1166	 i:9 	 global-step:23329	 l-p:0.14149945974349976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1672, 5.0606, 5.1202],
        [5.1672, 5.1446, 5.1642],
        [5.1672, 4.8972, 4.8481],
        [5.1672, 4.9409, 4.9569]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1167, step:0 
model_pd.l_p.mean(): 0.1444951444864273 
model_pd.l_d.mean(): -20.296667098999023 
model_pd.lagr.mean(): -20.152172088623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4967], device='cuda:0')), ('power', tensor([-21.1906], device='cuda:0'))])
epoch£º1167	 i:0 	 global-step:23340	 l-p:0.1444951444864273
epoch£º1167	 i:1 	 global-step:23341	 l-p:0.08293122798204422
epoch£º1167	 i:2 	 global-step:23342	 l-p:0.18357637524604797
epoch£º1167	 i:3 	 global-step:23343	 l-p:0.2084471434354782
epoch£º1167	 i:4 	 global-step:23344	 l-p:0.08742718398571014
epoch£º1167	 i:5 	 global-step:23345	 l-p:0.12718142569065094
epoch£º1167	 i:6 	 global-step:23346	 l-p:0.15655061602592468
epoch£º1167	 i:7 	 global-step:23347	 l-p:0.13146734237670898
epoch£º1167	 i:8 	 global-step:23348	 l-p:0.1269833743572235
epoch£º1167	 i:9 	 global-step:23349	 l-p:0.1878914088010788
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1542, 5.1496, 5.1540],
        [5.1542, 5.1618, 4.8535],
        [5.1542, 5.1481, 5.1539],
        [5.1542, 5.5813, 5.5099]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1168, step:0 
model_pd.l_p.mean(): 0.17797419428825378 
model_pd.l_d.mean(): -17.26693344116211 
model_pd.lagr.mean(): -17.088958740234375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6635], device='cuda:0')), ('power', tensor([-18.2770], device='cuda:0'))])
epoch£º1168	 i:0 	 global-step:23360	 l-p:0.17797419428825378
epoch£º1168	 i:1 	 global-step:23361	 l-p:0.11924423277378082
epoch£º1168	 i:2 	 global-step:23362	 l-p:0.1316457837820053
epoch£º1168	 i:3 	 global-step:23363	 l-p:0.2767808735370636
epoch£º1168	 i:4 	 global-step:23364	 l-p:0.12744133174419403
epoch£º1168	 i:5 	 global-step:23365	 l-p:0.15926750004291534
epoch£º1168	 i:6 	 global-step:23366	 l-p:0.11692120879888535
epoch£º1168	 i:7 	 global-step:23367	 l-p:0.12848812341690063
epoch£º1168	 i:8 	 global-step:23368	 l-p:0.13056401908397675
epoch£º1168	 i:9 	 global-step:23369	 l-p:0.11252893507480621
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1560, 4.9995, 4.6512],
        [5.1560, 5.1536, 5.1559],
        [5.1560, 5.0814, 4.7453],
        [5.1560, 5.0491, 5.1089]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1169, step:0 
model_pd.l_p.mean(): 0.15152046084403992 
model_pd.l_d.mean(): -20.50291633605957 
model_pd.lagr.mean(): -20.351396560668945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4266], device='cuda:0')), ('power', tensor([-21.3282], device='cuda:0'))])
epoch£º1169	 i:0 	 global-step:23380	 l-p:0.15152046084403992
epoch£º1169	 i:1 	 global-step:23381	 l-p:0.1499083787202835
epoch£º1169	 i:2 	 global-step:23382	 l-p:0.14204631745815277
epoch£º1169	 i:3 	 global-step:23383	 l-p:0.2049606293439865
epoch£º1169	 i:4 	 global-step:23384	 l-p:0.13497856259346008
epoch£º1169	 i:5 	 global-step:23385	 l-p:0.11661792546510696
epoch£º1169	 i:6 	 global-step:23386	 l-p:0.10207986831665039
epoch£º1169	 i:7 	 global-step:23387	 l-p:0.1669590324163437
epoch£º1169	 i:8 	 global-step:23388	 l-p:0.17225314676761627
epoch£º1169	 i:9 	 global-step:23389	 l-p:0.11936311423778534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1611, 5.1612, 5.1611],
        [5.1611, 4.8832, 4.8176],
        [5.1611, 4.9173, 4.9132],
        [5.1611, 4.9043, 4.5902]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1170, step:0 
model_pd.l_p.mean(): 0.14349792897701263 
model_pd.l_d.mean(): -19.903305053710938 
model_pd.lagr.mean(): -19.759807586669922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5116], device='cuda:0')), ('power', tensor([-20.8054], device='cuda:0'))])
epoch£º1170	 i:0 	 global-step:23400	 l-p:0.14349792897701263
epoch£º1170	 i:1 	 global-step:23401	 l-p:0.12864616513252258
epoch£º1170	 i:2 	 global-step:23402	 l-p:0.12608422338962555
epoch£º1170	 i:3 	 global-step:23403	 l-p:0.09288126975297928
epoch£º1170	 i:4 	 global-step:23404	 l-p:0.12397348135709763
epoch£º1170	 i:5 	 global-step:23405	 l-p:0.17100493609905243
epoch£º1170	 i:6 	 global-step:23406	 l-p:0.18158374726772308
epoch£º1170	 i:7 	 global-step:23407	 l-p:0.2032463401556015
epoch£º1170	 i:8 	 global-step:23408	 l-p:0.15512840449810028
epoch£º1170	 i:9 	 global-step:23409	 l-p:0.09639246761798859
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1670, 5.1663, 5.1670],
        [5.1670, 5.0417, 4.6959],
        [5.1670, 5.0309, 4.6840],
        [5.1670, 5.0389, 5.1005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1171, step:0 
model_pd.l_p.mean(): 0.13346725702285767 
model_pd.l_d.mean(): -20.25731658935547 
model_pd.lagr.mean(): -20.123849868774414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4495], device='cuda:0')), ('power', tensor([-21.1017], device='cuda:0'))])
epoch£º1171	 i:0 	 global-step:23420	 l-p:0.13346725702285767
epoch£º1171	 i:1 	 global-step:23421	 l-p:0.19396130740642548
epoch£º1171	 i:2 	 global-step:23422	 l-p:0.11158528923988342
epoch£º1171	 i:3 	 global-step:23423	 l-p:0.1365906149148941
epoch£º1171	 i:4 	 global-step:23424	 l-p:0.16762402653694153
epoch£º1171	 i:5 	 global-step:23425	 l-p:0.08632918447256088
epoch£º1171	 i:6 	 global-step:23426	 l-p:0.19845977425575256
epoch£º1171	 i:7 	 global-step:23427	 l-p:0.12022222578525543
epoch£º1171	 i:8 	 global-step:23428	 l-p:0.11788304150104523
epoch£º1171	 i:9 	 global-step:23429	 l-p:0.14544206857681274
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1640, 5.1610, 5.1639],
        [5.1640, 5.1608, 5.1639],
        [5.1640, 5.1599, 5.1638],
        [5.1640, 5.0199, 5.0807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1172, step:0 
model_pd.l_p.mean(): 0.1310913860797882 
model_pd.l_d.mean(): -20.640316009521484 
model_pd.lagr.mean(): -20.50922393798828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4135], device='cuda:0')), ('power', tensor([-21.4545], device='cuda:0'))])
epoch£º1172	 i:0 	 global-step:23440	 l-p:0.1310913860797882
epoch£º1172	 i:1 	 global-step:23441	 l-p:0.1787385493516922
epoch£º1172	 i:2 	 global-step:23442	 l-p:0.14628270268440247
epoch£º1172	 i:3 	 global-step:23443	 l-p:0.0902995690703392
epoch£º1172	 i:4 	 global-step:23444	 l-p:0.14605973660945892
epoch£º1172	 i:5 	 global-step:23445	 l-p:0.1457383930683136
epoch£º1172	 i:6 	 global-step:23446	 l-p:0.1323075294494629
epoch£º1172	 i:7 	 global-step:23447	 l-p:0.21013838052749634
epoch£º1172	 i:8 	 global-step:23448	 l-p:0.13592614233493805
epoch£º1172	 i:9 	 global-step:23449	 l-p:0.11498139053583145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1617, 4.8588, 4.6641],
        [5.1617, 5.1617, 5.1617],
        [5.1617, 4.8767, 4.7927],
        [5.1617, 5.1617, 5.1617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1173, step:0 
model_pd.l_p.mean(): 0.13172629475593567 
model_pd.l_d.mean(): -19.94685935974121 
model_pd.lagr.mean(): -19.81513214111328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4637], device='cuda:0')), ('power', tensor([-20.8001], device='cuda:0'))])
epoch£º1173	 i:0 	 global-step:23460	 l-p:0.13172629475593567
epoch£º1173	 i:1 	 global-step:23461	 l-p:0.10307034105062485
epoch£º1173	 i:2 	 global-step:23462	 l-p:0.16005805134773254
epoch£º1173	 i:3 	 global-step:23463	 l-p:0.12815505266189575
epoch£º1173	 i:4 	 global-step:23464	 l-p:0.11505404859781265
epoch£º1173	 i:5 	 global-step:23465	 l-p:0.11458241939544678
epoch£º1173	 i:6 	 global-step:23466	 l-p:0.15681402385234833
epoch£º1173	 i:7 	 global-step:23467	 l-p:0.14911764860153198
epoch£º1173	 i:8 	 global-step:23468	 l-p:0.19785282015800476
epoch£º1173	 i:9 	 global-step:23469	 l-p:0.16234740614891052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1692, 4.9969, 5.0504],
        [5.1692, 5.0389, 5.1005],
        [5.1692, 4.8763, 4.6228],
        [5.1692, 5.2191, 4.9286]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1174, step:0 
model_pd.l_p.mean(): 0.16069693863391876 
model_pd.l_d.mean(): -19.526748657226562 
model_pd.lagr.mean(): -19.366052627563477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5082], device='cuda:0')), ('power', tensor([-20.4182], device='cuda:0'))])
epoch£º1174	 i:0 	 global-step:23480	 l-p:0.16069693863391876
epoch£º1174	 i:1 	 global-step:23481	 l-p:0.18173278868198395
epoch£º1174	 i:2 	 global-step:23482	 l-p:0.12894077599048615
epoch£º1174	 i:3 	 global-step:23483	 l-p:0.13251128792762756
epoch£º1174	 i:4 	 global-step:23484	 l-p:0.1561100333929062
epoch£º1174	 i:5 	 global-step:23485	 l-p:0.11879327148199081
epoch£º1174	 i:6 	 global-step:23486	 l-p:0.11284174025058746
epoch£º1174	 i:7 	 global-step:23487	 l-p:0.10747277736663818
epoch£º1174	 i:8 	 global-step:23488	 l-p:0.13273105025291443
epoch£º1174	 i:9 	 global-step:23489	 l-p:0.15646019577980042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1719, 5.5400, 5.4288],
        [5.1719, 5.0069, 5.0629],
        [5.1719, 4.8725, 4.7228],
        [5.1719, 5.1718, 5.1719]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1175, step:0 
model_pd.l_p.mean(): 0.11728852242231369 
model_pd.l_d.mean(): -18.872299194335938 
model_pd.lagr.mean(): -18.7550106048584 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5898], device='cuda:0')), ('power', tensor([-19.8360], device='cuda:0'))])
epoch£º1175	 i:0 	 global-step:23500	 l-p:0.11728852242231369
epoch£º1175	 i:1 	 global-step:23501	 l-p:0.21074187755584717
epoch£º1175	 i:2 	 global-step:23502	 l-p:0.13169193267822266
epoch£º1175	 i:3 	 global-step:23503	 l-p:0.12775149941444397
epoch£º1175	 i:4 	 global-step:23504	 l-p:0.08233684301376343
epoch£º1175	 i:5 	 global-step:23505	 l-p:0.11033610254526138
epoch£º1175	 i:6 	 global-step:23506	 l-p:0.127908855676651
epoch£º1175	 i:7 	 global-step:23507	 l-p:0.15296198427677155
epoch£º1175	 i:8 	 global-step:23508	 l-p:0.1944536417722702
epoch£º1175	 i:9 	 global-step:23509	 l-p:0.13514180481433868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1703, 4.9108, 4.8823],
        [5.1703, 4.8682, 4.6877],
        [5.1703, 5.1703, 5.1703],
        [5.1703, 4.9131, 4.8884]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1176, step:0 
model_pd.l_p.mean(): 0.12322565913200378 
model_pd.l_d.mean(): -19.795812606811523 
model_pd.lagr.mean(): -19.67258644104004 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4349], device='cuda:0')), ('power', tensor([-20.6164], device='cuda:0'))])
epoch£º1176	 i:0 	 global-step:23520	 l-p:0.12322565913200378
epoch£º1176	 i:1 	 global-step:23521	 l-p:0.12267056852579117
epoch£º1176	 i:2 	 global-step:23522	 l-p:0.1814069300889969
epoch£º1176	 i:3 	 global-step:23523	 l-p:0.11043643951416016
epoch£º1176	 i:4 	 global-step:23524	 l-p:0.09138482809066772
epoch£º1176	 i:5 	 global-step:23525	 l-p:0.12172363698482513
epoch£º1176	 i:6 	 global-step:23526	 l-p:0.17981092631816864
epoch£º1176	 i:7 	 global-step:23527	 l-p:0.14395157992839813
epoch£º1176	 i:8 	 global-step:23528	 l-p:0.16003601253032684
epoch£º1176	 i:9 	 global-step:23529	 l-p:0.14729437232017517
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1757, 5.1461, 5.1709],
        [5.1757, 5.1179, 5.1603],
        [5.1757, 5.1730, 5.1756],
        [5.1757, 5.2473, 4.9667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1177, step:0 
model_pd.l_p.mean(): 0.18572299182415009 
model_pd.l_d.mean(): -20.228025436401367 
model_pd.lagr.mean(): -20.04230308532715 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4671], device='cuda:0')), ('power', tensor([-21.0900], device='cuda:0'))])
epoch£º1177	 i:0 	 global-step:23540	 l-p:0.18572299182415009
epoch£º1177	 i:1 	 global-step:23541	 l-p:0.08711995929479599
epoch£º1177	 i:2 	 global-step:23542	 l-p:0.149382084608078
epoch£º1177	 i:3 	 global-step:23543	 l-p:0.13594907522201538
epoch£º1177	 i:4 	 global-step:23544	 l-p:0.10667067021131516
epoch£º1177	 i:5 	 global-step:23545	 l-p:0.12378966808319092
epoch£º1177	 i:6 	 global-step:23546	 l-p:0.1417747586965561
epoch£º1177	 i:7 	 global-step:23547	 l-p:0.17084014415740967
epoch£º1177	 i:8 	 global-step:23548	 l-p:0.13718447089195251
epoch£º1177	 i:9 	 global-step:23549	 l-p:0.13669045269489288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1750, 5.1750, 5.1750],
        [5.1750, 5.1475, 5.1708],
        [5.1750, 4.8774, 4.7398],
        [5.1750, 5.1750, 5.1750]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1178, step:0 
model_pd.l_p.mean(): 0.19074207544326782 
model_pd.l_d.mean(): -20.773786544799805 
model_pd.lagr.mean(): -20.583044052124023 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3897], device='cuda:0')), ('power', tensor([-21.5659], device='cuda:0'))])
epoch£º1178	 i:0 	 global-step:23560	 l-p:0.19074207544326782
epoch£º1178	 i:1 	 global-step:23561	 l-p:0.10084881633520126
epoch£º1178	 i:2 	 global-step:23562	 l-p:0.11962966620922089
epoch£º1178	 i:3 	 global-step:23563	 l-p:0.14829608798027039
epoch£º1178	 i:4 	 global-step:23564	 l-p:0.11041639000177383
epoch£º1178	 i:5 	 global-step:23565	 l-p:0.11099559813737869
epoch£º1178	 i:6 	 global-step:23566	 l-p:0.16757282614707947
epoch£º1178	 i:7 	 global-step:23567	 l-p:0.1196260079741478
epoch£º1178	 i:8 	 global-step:23568	 l-p:0.16155968606472015
epoch£º1178	 i:9 	 global-step:23569	 l-p:0.1381244659423828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1821, 5.0028, 5.0531],
        [5.1821, 5.1624, 5.1797],
        [5.1821, 5.1129, 4.7785],
        [5.1821, 5.1478, 5.1759]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1179, step:0 
model_pd.l_p.mean(): 0.13521507382392883 
model_pd.l_d.mean(): -20.530786514282227 
model_pd.lagr.mean(): -20.395570755004883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4232], device='cuda:0')), ('power', tensor([-21.3530], device='cuda:0'))])
epoch£º1179	 i:0 	 global-step:23580	 l-p:0.13521507382392883
epoch£º1179	 i:1 	 global-step:23581	 l-p:0.11183355003595352
epoch£º1179	 i:2 	 global-step:23582	 l-p:0.15286056697368622
epoch£º1179	 i:3 	 global-step:23583	 l-p:0.12353616207838058
epoch£º1179	 i:4 	 global-step:23584	 l-p:0.12688399851322174
epoch£º1179	 i:5 	 global-step:23585	 l-p:0.12956377863883972
epoch£º1179	 i:6 	 global-step:23586	 l-p:0.14493142068386078
epoch£º1179	 i:7 	 global-step:23587	 l-p:0.1808651238679886
epoch£º1179	 i:8 	 global-step:23588	 l-p:0.12773734331130981
epoch£º1179	 i:9 	 global-step:23589	 l-p:0.11278755962848663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1849, 5.1758, 5.1843],
        [5.1849, 5.1272, 5.1696],
        [5.1849, 5.1102, 5.1605],
        [5.1849, 5.1652, 5.1825]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1180, step:0 
model_pd.l_p.mean(): 0.1403978019952774 
model_pd.l_d.mean(): -19.574356079101562 
model_pd.lagr.mean(): -19.433958053588867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4670], device='cuda:0')), ('power', tensor([-20.4241], device='cuda:0'))])
epoch£º1180	 i:0 	 global-step:23600	 l-p:0.1403978019952774
epoch£º1180	 i:1 	 global-step:23601	 l-p:0.12378007918596268
epoch£º1180	 i:2 	 global-step:23602	 l-p:0.1319102793931961
epoch£º1180	 i:3 	 global-step:23603	 l-p:0.14818640053272247
epoch£º1180	 i:4 	 global-step:23604	 l-p:0.13470512628555298
epoch£º1180	 i:5 	 global-step:23605	 l-p:0.1544618308544159
epoch£º1180	 i:6 	 global-step:23606	 l-p:0.08959007263183594
epoch£º1180	 i:7 	 global-step:23607	 l-p:0.11608171463012695
epoch£º1180	 i:8 	 global-step:23608	 l-p:0.15253159403800964
epoch£º1180	 i:9 	 global-step:23609	 l-p:0.19881011545658112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1662, 5.4616, 5.3042],
        [5.1662, 5.1575, 5.1656],
        [5.1662, 5.1436, 5.1632],
        [5.1662, 5.4121, 5.2248]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1181, step:0 
model_pd.l_p.mean(): 0.12473899871110916 
model_pd.l_d.mean(): -19.862960815429688 
model_pd.lagr.mean(): -19.738222122192383 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5321], device='cuda:0')), ('power', tensor([-20.7855], device='cuda:0'))])
epoch£º1181	 i:0 	 global-step:23620	 l-p:0.12473899871110916
epoch£º1181	 i:1 	 global-step:23621	 l-p:0.18673577904701233
epoch£º1181	 i:2 	 global-step:23622	 l-p:0.1257837414741516
epoch£º1181	 i:3 	 global-step:23623	 l-p:0.19588644802570343
epoch£º1181	 i:4 	 global-step:23624	 l-p:0.16692468523979187
epoch£º1181	 i:5 	 global-step:23625	 l-p:0.09353822469711304
epoch£º1181	 i:6 	 global-step:23626	 l-p:0.11616212874650955
epoch£º1181	 i:7 	 global-step:23627	 l-p:0.08548086881637573
epoch£º1181	 i:8 	 global-step:23628	 l-p:0.17850078642368317
epoch£º1181	 i:9 	 global-step:23629	 l-p:0.15242280066013336
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1602, 5.0272, 4.6795],
        [5.1602, 4.9800, 5.0307],
        [5.1602, 4.9532, 4.9879],
        [5.1602, 5.1601, 5.1602]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1182, step:0 
model_pd.l_p.mean(): 0.12714023888111115 
model_pd.l_d.mean(): -20.471435546875 
model_pd.lagr.mean(): -20.344295501708984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4285], device='cuda:0')), ('power', tensor([-21.2980], device='cuda:0'))])
epoch£º1182	 i:0 	 global-step:23640	 l-p:0.12714023888111115
epoch£º1182	 i:1 	 global-step:23641	 l-p:0.14963465929031372
epoch£º1182	 i:2 	 global-step:23642	 l-p:0.19881290197372437
epoch£º1182	 i:3 	 global-step:23643	 l-p:0.12773720920085907
epoch£º1182	 i:4 	 global-step:23644	 l-p:0.1075943261384964
epoch£º1182	 i:5 	 global-step:23645	 l-p:0.218397855758667
epoch£º1182	 i:6 	 global-step:23646	 l-p:0.08042984455823898
epoch£º1182	 i:7 	 global-step:23647	 l-p:0.16766303777694702
epoch£º1182	 i:8 	 global-step:23648	 l-p:0.1300104707479477
epoch£º1182	 i:9 	 global-step:23649	 l-p:0.16927020251750946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1558, 5.4094, 5.2268],
        [5.1558, 5.3062, 5.0652],
        [5.1558, 5.1558, 5.1558],
        [5.1558, 4.8602, 4.6062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1183, step:0 
model_pd.l_p.mean(): 0.10158123075962067 
model_pd.l_d.mean(): -20.094694137573242 
model_pd.lagr.mean(): -19.993112564086914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5082], device='cuda:0')), ('power', tensor([-20.9968], device='cuda:0'))])
epoch£º1183	 i:0 	 global-step:23660	 l-p:0.10158123075962067
epoch£º1183	 i:1 	 global-step:23661	 l-p:0.14139197766780853
epoch£º1183	 i:2 	 global-step:23662	 l-p:0.09248162060976028
epoch£º1183	 i:3 	 global-step:23663	 l-p:0.1462794989347458
epoch£º1183	 i:4 	 global-step:23664	 l-p:0.19901244342327118
epoch£º1183	 i:5 	 global-step:23665	 l-p:0.21630752086639404
epoch£º1183	 i:6 	 global-step:23666	 l-p:0.1404736042022705
epoch£º1183	 i:7 	 global-step:23667	 l-p:0.13566997647285461
epoch£º1183	 i:8 	 global-step:23668	 l-p:0.11815578490495682
epoch£º1183	 i:9 	 global-step:23669	 l-p:0.16028501093387604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1613, 5.0901, 4.7542],
        [5.1613, 5.1528, 5.1607],
        [5.1613, 5.1971, 4.9000],
        [5.1613, 4.9166, 4.9126]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1184, step:0 
model_pd.l_p.mean(): 0.17142711579799652 
model_pd.l_d.mean(): -20.472917556762695 
model_pd.lagr.mean(): -20.301490783691406 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4396], device='cuda:0')), ('power', tensor([-21.3111], device='cuda:0'))])
epoch£º1184	 i:0 	 global-step:23680	 l-p:0.17142711579799652
epoch£º1184	 i:1 	 global-step:23681	 l-p:0.16967912018299103
epoch£º1184	 i:2 	 global-step:23682	 l-p:0.1767265647649765
epoch£º1184	 i:3 	 global-step:23683	 l-p:0.11657197773456573
epoch£º1184	 i:4 	 global-step:23684	 l-p:0.10208440572023392
epoch£º1184	 i:5 	 global-step:23685	 l-p:0.14870868623256683
epoch£º1184	 i:6 	 global-step:23686	 l-p:0.13941502571105957
epoch£º1184	 i:7 	 global-step:23687	 l-p:0.07468162477016449
epoch£º1184	 i:8 	 global-step:23688	 l-p:0.19685488939285278
epoch£º1184	 i:9 	 global-step:23689	 l-p:0.11408504843711853
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1762, 4.8785, 4.7430],
        [5.1762, 4.9484, 4.9634],
        [5.1762, 4.9450, 4.9562],
        [5.1762, 5.0509, 5.1125]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1185, step:0 
model_pd.l_p.mean(): 0.15289534628391266 
model_pd.l_d.mean(): -20.344898223876953 
model_pd.lagr.mean(): -20.19200325012207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4483], device='cuda:0')), ('power', tensor([-21.1897], device='cuda:0'))])
epoch£º1185	 i:0 	 global-step:23700	 l-p:0.15289534628391266
epoch£º1185	 i:1 	 global-step:23701	 l-p:0.12648431956768036
epoch£º1185	 i:2 	 global-step:23702	 l-p:0.15813347697257996
epoch£º1185	 i:3 	 global-step:23703	 l-p:0.12131115049123764
epoch£º1185	 i:4 	 global-step:23704	 l-p:0.12379865348339081
epoch£º1185	 i:5 	 global-step:23705	 l-p:0.1858631670475006
epoch£º1185	 i:6 	 global-step:23706	 l-p:0.13411207497119904
epoch£º1185	 i:7 	 global-step:23707	 l-p:0.13919422030448914
epoch£º1185	 i:8 	 global-step:23708	 l-p:0.10891608148813248
epoch£º1185	 i:9 	 global-step:23709	 l-p:0.10808753967285156
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1836, 5.1395, 5.1740],
        [5.1836, 5.1307, 5.1704],
        [5.1836, 4.9110, 4.8562],
        [5.1836, 5.1379, 4.8099]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1186, step:0 
model_pd.l_p.mean(): 0.08747456967830658 
model_pd.l_d.mean(): -20.329326629638672 
model_pd.lagr.mean(): -20.241851806640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4537], device='cuda:0')), ('power', tensor([-21.1793], device='cuda:0'))])
epoch£º1186	 i:0 	 global-step:23720	 l-p:0.08747456967830658
epoch£º1186	 i:1 	 global-step:23721	 l-p:0.11961307376623154
epoch£º1186	 i:2 	 global-step:23722	 l-p:0.1362699419260025
epoch£º1186	 i:3 	 global-step:23723	 l-p:0.14730633795261383
epoch£º1186	 i:4 	 global-step:23724	 l-p:0.13658174872398376
epoch£º1186	 i:5 	 global-step:23725	 l-p:0.1866304874420166
epoch£º1186	 i:6 	 global-step:23726	 l-p:0.11029788851737976
epoch£º1186	 i:7 	 global-step:23727	 l-p:0.13747508823871613
epoch£º1186	 i:8 	 global-step:23728	 l-p:0.117665134370327
epoch£º1186	 i:9 	 global-step:23729	 l-p:0.1563437432050705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1872, 5.0328, 5.0915],
        [5.1872, 5.3095, 5.0532],
        [5.1872, 5.1042, 5.1576],
        [5.1872, 5.1857, 5.1871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1187, step:0 
model_pd.l_p.mean(): 0.17860637605190277 
model_pd.l_d.mean(): -20.31969451904297 
model_pd.lagr.mean(): -20.141088485717773 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4520], device='cuda:0')), ('power', tensor([-21.1678], device='cuda:0'))])
epoch£º1187	 i:0 	 global-step:23740	 l-p:0.17860637605190277
epoch£º1187	 i:1 	 global-step:23741	 l-p:0.1464383453130722
epoch£º1187	 i:2 	 global-step:23742	 l-p:0.11520503461360931
epoch£º1187	 i:3 	 global-step:23743	 l-p:0.1520843654870987
epoch£º1187	 i:4 	 global-step:23744	 l-p:0.12156438827514648
epoch£º1187	 i:5 	 global-step:23745	 l-p:0.1440400928258896
epoch£º1187	 i:6 	 global-step:23746	 l-p:0.09529607743024826
epoch£º1187	 i:7 	 global-step:23747	 l-p:0.15506593883037567
epoch£º1187	 i:8 	 global-step:23748	 l-p:0.06906682997941971
epoch£º1187	 i:9 	 global-step:23749	 l-p:0.15607614815235138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1897, 5.0959, 5.1527],
        [5.1897, 5.1897, 5.1897],
        [5.1897, 5.6091, 5.5309],
        [5.1897, 5.1439, 5.1795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1188, step:0 
model_pd.l_p.mean(): 0.12465633451938629 
model_pd.l_d.mean(): -20.272945404052734 
model_pd.lagr.mean(): -20.14828872680664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3988], device='cuda:0')), ('power', tensor([-21.0650], device='cuda:0'))])
epoch£º1188	 i:0 	 global-step:23760	 l-p:0.12465633451938629
epoch£º1188	 i:1 	 global-step:23761	 l-p:0.16262656450271606
epoch£º1188	 i:2 	 global-step:23762	 l-p:0.11205276101827621
epoch£º1188	 i:3 	 global-step:23763	 l-p:0.12242764979600906
epoch£º1188	 i:4 	 global-step:23764	 l-p:0.10994235426187515
epoch£º1188	 i:5 	 global-step:23765	 l-p:0.18779267370700836
epoch£º1188	 i:6 	 global-step:23766	 l-p:0.06308143585920334
epoch£º1188	 i:7 	 global-step:23767	 l-p:0.1818559765815735
epoch£º1188	 i:8 	 global-step:23768	 l-p:0.13587306439876556
epoch£º1188	 i:9 	 global-step:23769	 l-p:0.12031058967113495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1943, 5.0524, 5.1131],
        [5.1943, 4.9699, 4.9874],
        [5.1943, 5.1943, 5.1943],
        [5.1943, 5.1924, 5.1943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1189, step:0 
model_pd.l_p.mean(): 0.11236201226711273 
model_pd.l_d.mean(): -20.831531524658203 
model_pd.lagr.mean(): -20.71916961669922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3622], device='cuda:0')), ('power', tensor([-21.5962], device='cuda:0'))])
epoch£º1189	 i:0 	 global-step:23780	 l-p:0.11236201226711273
epoch£º1189	 i:1 	 global-step:23781	 l-p:0.19779306650161743
epoch£º1189	 i:2 	 global-step:23782	 l-p:0.16842028498649597
epoch£º1189	 i:3 	 global-step:23783	 l-p:0.05448406934738159
epoch£º1189	 i:4 	 global-step:23784	 l-p:0.1392333209514618
epoch£º1189	 i:5 	 global-step:23785	 l-p:0.12751106917858124
epoch£º1189	 i:6 	 global-step:23786	 l-p:0.19209781289100647
epoch£º1189	 i:7 	 global-step:23787	 l-p:0.09163389354944229
epoch£º1189	 i:8 	 global-step:23788	 l-p:0.12134908139705658
epoch£º1189	 i:9 	 global-step:23789	 l-p:0.10118116438388824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1946, 5.1946, 5.1946],
        [5.1946, 5.1753, 5.1923],
        [5.1946, 5.1946, 5.1946],
        [5.1946, 5.1783, 5.1929]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1190, step:0 
model_pd.l_p.mean(): 0.08588431030511856 
model_pd.l_d.mean(): -19.540082931518555 
model_pd.lagr.mean(): -19.454198837280273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4918], device='cuda:0')), ('power', tensor([-20.4148], device='cuda:0'))])
epoch£º1190	 i:0 	 global-step:23800	 l-p:0.08588431030511856
epoch£º1190	 i:1 	 global-step:23801	 l-p:0.16506986320018768
epoch£º1190	 i:2 	 global-step:23802	 l-p:0.07199934124946594
epoch£º1190	 i:3 	 global-step:23803	 l-p:0.16153842210769653
epoch£º1190	 i:4 	 global-step:23804	 l-p:0.15722326934337616
epoch£º1190	 i:5 	 global-step:23805	 l-p:0.19296500086784363
epoch£º1190	 i:6 	 global-step:23806	 l-p:0.1334535777568817
epoch£º1190	 i:7 	 global-step:23807	 l-p:0.12884503602981567
epoch£º1190	 i:8 	 global-step:23808	 l-p:0.08875421434640884
epoch£º1190	 i:9 	 global-step:23809	 l-p:0.11971433460712433
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1968, 5.5577, 5.4405],
        [5.1968, 5.1967, 5.1968],
        [5.1968, 5.1528, 5.1873],
        [5.1968, 4.9708, 4.9866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1191, step:0 
model_pd.l_p.mean(): 0.07327643036842346 
model_pd.l_d.mean(): -19.114046096801758 
model_pd.lagr.mean(): -19.040769577026367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5694], device='cuda:0')), ('power', tensor([-20.0612], device='cuda:0'))])
epoch£º1191	 i:0 	 global-step:23820	 l-p:0.07327643036842346
epoch£º1191	 i:1 	 global-step:23821	 l-p:0.12345247715711594
epoch£º1191	 i:2 	 global-step:23822	 l-p:0.10610955953598022
epoch£º1191	 i:3 	 global-step:23823	 l-p:0.15014751255512238
epoch£º1191	 i:4 	 global-step:23824	 l-p:0.08939012140035629
epoch£º1191	 i:5 	 global-step:23825	 l-p:0.1552625447511673
epoch£º1191	 i:6 	 global-step:23826	 l-p:0.15089133381843567
epoch£º1191	 i:7 	 global-step:23827	 l-p:0.12682554125785828
epoch£º1191	 i:8 	 global-step:23828	 l-p:0.14078402519226074
epoch£º1191	 i:9 	 global-step:23829	 l-p:0.19036254286766052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1943, 5.1151, 5.1672],
        [5.1943, 5.1020, 5.1584],
        [5.1943, 5.0315, 5.0881],
        [5.1943, 5.1892, 5.1941]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1192, step:0 
model_pd.l_p.mean(): 0.14211232960224152 
model_pd.l_d.mean(): -19.602720260620117 
model_pd.lagr.mean(): -19.460607528686523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5160], device='cuda:0')), ('power', tensor([-20.5037], device='cuda:0'))])
epoch£º1192	 i:0 	 global-step:23840	 l-p:0.14211232960224152
epoch£º1192	 i:1 	 global-step:23841	 l-p:0.18548965454101562
epoch£º1192	 i:2 	 global-step:23842	 l-p:0.13479532301425934
epoch£º1192	 i:3 	 global-step:23843	 l-p:0.08626522123813629
epoch£º1192	 i:4 	 global-step:23844	 l-p:0.13299036026000977
epoch£º1192	 i:5 	 global-step:23845	 l-p:0.15394511818885803
epoch£º1192	 i:6 	 global-step:23846	 l-p:0.13250714540481567
epoch£º1192	 i:7 	 global-step:23847	 l-p:0.10811448097229004
epoch£º1192	 i:8 	 global-step:23848	 l-p:0.14611588418483734
epoch£º1192	 i:9 	 global-step:23849	 l-p:0.09318224340677261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1902, 4.8967, 4.6482],
        [5.1902, 5.1338, 5.1755],
        [5.1902, 5.1854, 5.1900],
        [5.1902, 5.1674, 5.1871]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1193, step:0 
model_pd.l_p.mean(): 0.1407242715358734 
model_pd.l_d.mean(): -20.006784439086914 
model_pd.lagr.mean(): -19.866060256958008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5147], device='cuda:0')), ('power', tensor([-20.9139], device='cuda:0'))])
epoch£º1193	 i:0 	 global-step:23860	 l-p:0.1407242715358734
epoch£º1193	 i:1 	 global-step:23861	 l-p:0.0997229740023613
epoch£º1193	 i:2 	 global-step:23862	 l-p:0.108723945915699
epoch£º1193	 i:3 	 global-step:23863	 l-p:0.1457035094499588
epoch£º1193	 i:4 	 global-step:23864	 l-p:0.1420130878686905
epoch£º1193	 i:5 	 global-step:23865	 l-p:0.14819659292697906
epoch£º1193	 i:6 	 global-step:23866	 l-p:0.13175275921821594
epoch£º1193	 i:7 	 global-step:23867	 l-p:0.15213778614997864
epoch£º1193	 i:8 	 global-step:23868	 l-p:0.12709049880504608
epoch£º1193	 i:9 	 global-step:23869	 l-p:0.1323651522397995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1864, 4.9315, 4.6170],
        [5.1864, 5.1853, 5.1864],
        [5.1864, 5.0338, 4.6860],
        [5.1864, 4.8861, 4.6689]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1194, step:0 
model_pd.l_p.mean(): 0.11966904997825623 
model_pd.l_d.mean(): -20.33787727355957 
model_pd.lagr.mean(): -20.21820831298828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4466], device='cuda:0')), ('power', tensor([-21.1807], device='cuda:0'))])
epoch£º1194	 i:0 	 global-step:23880	 l-p:0.11966904997825623
epoch£º1194	 i:1 	 global-step:23881	 l-p:0.13609477877616882
epoch£º1194	 i:2 	 global-step:23882	 l-p:0.12976552546024323
epoch£º1194	 i:3 	 global-step:23883	 l-p:0.16011270880699158
epoch£º1194	 i:4 	 global-step:23884	 l-p:0.14110788702964783
epoch£º1194	 i:5 	 global-step:23885	 l-p:0.1619063913822174
epoch£º1194	 i:6 	 global-step:23886	 l-p:0.10931853950023651
epoch£º1194	 i:7 	 global-step:23887	 l-p:0.12324243038892746
epoch£º1194	 i:8 	 global-step:23888	 l-p:0.14957629144191742
epoch£º1194	 i:9 	 global-step:23889	 l-p:0.17043176293373108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1655, 5.1602, 5.1652],
        [5.1655, 5.1651, 5.1655],
        [5.1655, 5.0287, 5.0905],
        [5.1655, 4.8882, 4.8272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1195, step:0 
model_pd.l_p.mean(): 0.14121413230895996 
model_pd.l_d.mean(): -20.34667205810547 
model_pd.lagr.mean(): -20.20545768737793 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4519], device='cuda:0')), ('power', tensor([-21.1951], device='cuda:0'))])
epoch£º1195	 i:0 	 global-step:23900	 l-p:0.14121413230895996
epoch£º1195	 i:1 	 global-step:23901	 l-p:0.11842698603868484
epoch£º1195	 i:2 	 global-step:23902	 l-p:0.17072078585624695
epoch£º1195	 i:3 	 global-step:23903	 l-p:0.16818878054618835
epoch£º1195	 i:4 	 global-step:23904	 l-p:0.12412466108798981
epoch£º1195	 i:5 	 global-step:23905	 l-p:0.08770687133073807
epoch£º1195	 i:6 	 global-step:23906	 l-p:0.1348247081041336
epoch£º1195	 i:7 	 global-step:23907	 l-p:0.13927465677261353
epoch£º1195	 i:8 	 global-step:23908	 l-p:0.15604691207408905
epoch£º1195	 i:9 	 global-step:23909	 l-p:0.1787644475698471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1684, 5.0850, 5.1387],
        [5.1684, 5.1366, 5.1630],
        [5.1684, 5.3820, 5.1753],
        [5.1684, 5.2380, 4.9556]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1196, step:0 
model_pd.l_p.mean(): 0.1348007321357727 
model_pd.l_d.mean(): -20.138933181762695 
model_pd.lagr.mean(): -20.004133224487305 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4849], device='cuda:0')), ('power', tensor([-21.0177], device='cuda:0'))])
epoch£º1196	 i:0 	 global-step:23920	 l-p:0.1348007321357727
epoch£º1196	 i:1 	 global-step:23921	 l-p:0.14653995633125305
epoch£º1196	 i:2 	 global-step:23922	 l-p:0.18783503770828247
epoch£º1196	 i:3 	 global-step:23923	 l-p:0.14956268668174744
epoch£º1196	 i:4 	 global-step:23924	 l-p:0.10135640949010849
epoch£º1196	 i:5 	 global-step:23925	 l-p:0.14417243003845215
epoch£º1196	 i:6 	 global-step:23926	 l-p:0.139878511428833
epoch£º1196	 i:7 	 global-step:23927	 l-p:0.12283547967672348
epoch£º1196	 i:8 	 global-step:23928	 l-p:0.1349969208240509
epoch£º1196	 i:9 	 global-step:23929	 l-p:0.13679565489292145
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1706, 4.8846, 4.8002],
        [5.1706, 5.0857, 5.1400],
        [5.1706, 5.4864, 5.3409],
        [5.1706, 4.9561, 4.6157]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1197, step:0 
model_pd.l_p.mean(): 0.1477118879556656 
model_pd.l_d.mean(): -19.990930557250977 
model_pd.lagr.mean(): -19.843217849731445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5082], device='cuda:0')), ('power', tensor([-20.8911], device='cuda:0'))])
epoch£º1197	 i:0 	 global-step:23940	 l-p:0.1477118879556656
epoch£º1197	 i:1 	 global-step:23941	 l-p:0.140031099319458
epoch£º1197	 i:2 	 global-step:23942	 l-p:0.11617501825094223
epoch£º1197	 i:3 	 global-step:23943	 l-p:0.14348316192626953
epoch£º1197	 i:4 	 global-step:23944	 l-p:0.14966867864131927
epoch£º1197	 i:5 	 global-step:23945	 l-p:0.20814773440361023
epoch£º1197	 i:6 	 global-step:23946	 l-p:0.13842535018920898
epoch£º1197	 i:7 	 global-step:23947	 l-p:0.1625203788280487
epoch£º1197	 i:8 	 global-step:23948	 l-p:0.12810039520263672
epoch£º1197	 i:9 	 global-step:23949	 l-p:0.07319632172584534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1718, 5.1425, 4.8191],
        [5.1718, 5.1718, 5.1718],
        [5.1718, 5.1673, 5.1716],
        [5.1718, 5.1547, 5.1699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1198, step:0 
model_pd.l_p.mean(): 0.10624519735574722 
model_pd.l_d.mean(): -19.890148162841797 
model_pd.lagr.mean(): -19.783903121948242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4957], device='cuda:0')), ('power', tensor([-20.7755], device='cuda:0'))])
epoch£º1198	 i:0 	 global-step:23960	 l-p:0.10624519735574722
epoch£º1198	 i:1 	 global-step:23961	 l-p:0.12853196263313293
epoch£º1198	 i:2 	 global-step:23962	 l-p:0.09562404453754425
epoch£º1198	 i:3 	 global-step:23963	 l-p:0.16659554839134216
epoch£º1198	 i:4 	 global-step:23964	 l-p:0.14211110770702362
epoch£º1198	 i:5 	 global-step:23965	 l-p:0.12024127691984177
epoch£º1198	 i:6 	 global-step:23966	 l-p:0.1362568587064743
epoch£º1198	 i:7 	 global-step:23967	 l-p:0.14113381505012512
epoch£º1198	 i:8 	 global-step:23968	 l-p:0.21395009756088257
epoch£º1198	 i:9 	 global-step:23969	 l-p:0.14564217627048492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1729, 5.1714, 5.1729],
        [5.1729, 5.1729, 5.1729],
        [5.1729, 5.1720, 5.1729],
        [5.1729, 5.1729, 5.1729]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1199, step:0 
model_pd.l_p.mean(): 0.10630765557289124 
model_pd.l_d.mean(): -19.120695114135742 
model_pd.lagr.mean(): -19.014387130737305 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5525], device='cuda:0')), ('power', tensor([-20.0505], device='cuda:0'))])
epoch£º1199	 i:0 	 global-step:23980	 l-p:0.10630765557289124
epoch£º1199	 i:1 	 global-step:23981	 l-p:0.18821069598197937
epoch£º1199	 i:2 	 global-step:23982	 l-p:0.09292419254779816
epoch£º1199	 i:3 	 global-step:23983	 l-p:0.14288705587387085
epoch£º1199	 i:4 	 global-step:23984	 l-p:0.13156722486019135
epoch£º1199	 i:5 	 global-step:23985	 l-p:0.1501946598291397
epoch£º1199	 i:6 	 global-step:23986	 l-p:0.09244882315397263
epoch£º1199	 i:7 	 global-step:23987	 l-p:0.18649478256702423
epoch£º1199	 i:8 	 global-step:23988	 l-p:0.13375471532344818
epoch£º1199	 i:9 	 global-step:23989	 l-p:0.16227003931999207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1725, 5.0915, 5.1444],
        [5.1725, 5.1725, 5.1725],
        [5.1725, 4.9390, 4.9489],
        [5.1725, 4.9337, 4.9373]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1200, step:0 
model_pd.l_p.mean(): 0.04986779764294624 
model_pd.l_d.mean(): -20.13484001159668 
model_pd.lagr.mean(): -20.084972381591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4660], device='cuda:0')), ('power', tensor([-20.9940], device='cuda:0'))])
epoch£º1200	 i:0 	 global-step:24000	 l-p:0.04986779764294624
epoch£º1200	 i:1 	 global-step:24001	 l-p:0.11995638906955719
epoch£º1200	 i:2 	 global-step:24002	 l-p:0.11206968128681183
epoch£º1200	 i:3 	 global-step:24003	 l-p:0.13534009456634521
epoch£º1200	 i:4 	 global-step:24004	 l-p:0.1682589054107666
epoch£º1200	 i:5 	 global-step:24005	 l-p:0.12987542152404785
epoch£º1200	 i:6 	 global-step:24006	 l-p:0.16656802594661713
epoch£º1200	 i:7 	 global-step:24007	 l-p:0.21815241873264313
epoch£º1200	 i:8 	 global-step:24008	 l-p:0.11310301721096039
epoch£º1200	 i:9 	 global-step:24009	 l-p:0.1826101839542389
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1715, 5.1716, 5.1715],
        [5.1715, 5.1468, 5.1680],
        [5.1715, 4.8749, 4.6259],
        [5.1715, 4.8697, 4.6461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1201, step:0 
model_pd.l_p.mean(): 0.13720685243606567 
model_pd.l_d.mean(): -18.87566375732422 
model_pd.lagr.mean(): -18.73845672607422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5381], device='cuda:0')), ('power', tensor([-19.7860], device='cuda:0'))])
epoch£º1201	 i:0 	 global-step:24020	 l-p:0.13720685243606567
epoch£º1201	 i:1 	 global-step:24021	 l-p:0.13943181931972504
epoch£º1201	 i:2 	 global-step:24022	 l-p:0.18663203716278076
epoch£º1201	 i:3 	 global-step:24023	 l-p:0.10689862817525864
epoch£º1201	 i:4 	 global-step:24024	 l-p:0.13226717710494995
epoch£º1201	 i:5 	 global-step:24025	 l-p:0.12209758162498474
epoch£º1201	 i:6 	 global-step:24026	 l-p:0.13399171829223633
epoch£º1201	 i:7 	 global-step:24027	 l-p:0.14435310661792755
epoch£º1201	 i:8 	 global-step:24028	 l-p:0.1868673861026764
epoch£º1201	 i:9 	 global-step:24029	 l-p:0.11227336525917053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1694, 5.0844, 5.1386],
        [5.1694, 4.8792, 4.7830],
        [5.1694, 5.1693, 5.1694],
        [5.1694, 5.4447, 5.2743]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1202, step:0 
model_pd.l_p.mean(): 0.0873996913433075 
model_pd.l_d.mean(): -19.667133331298828 
model_pd.lagr.mean(): -19.57973289489746 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5119], device='cuda:0')), ('power', tensor([-20.5651], device='cuda:0'))])
epoch£º1202	 i:0 	 global-step:24040	 l-p:0.0873996913433075
epoch£º1202	 i:1 	 global-step:24041	 l-p:0.14846035838127136
epoch£º1202	 i:2 	 global-step:24042	 l-p:0.14221884310245514
epoch£º1202	 i:3 	 global-step:24043	 l-p:0.14296430349349976
epoch£º1202	 i:4 	 global-step:24044	 l-p:0.09372773766517639
epoch£º1202	 i:5 	 global-step:24045	 l-p:0.10225566476583481
epoch£º1202	 i:6 	 global-step:24046	 l-p:0.1500406414270401
epoch£º1202	 i:7 	 global-step:24047	 l-p:0.27122265100479126
epoch£º1202	 i:8 	 global-step:24048	 l-p:0.1516381800174713
epoch£º1202	 i:9 	 global-step:24049	 l-p:0.1395946592092514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1623, 5.1382, 5.1590],
        [5.1623, 5.1620, 5.1623],
        [5.1623, 4.9785, 4.6299],
        [5.1623, 4.9026, 4.5869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1203, step:0 
model_pd.l_p.mean(): 0.22625769674777985 
model_pd.l_d.mean(): -20.27436637878418 
model_pd.lagr.mean(): -20.04810905456543 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4930], device='cuda:0')), ('power', tensor([-21.1641], device='cuda:0'))])
epoch£º1203	 i:0 	 global-step:24060	 l-p:0.22625769674777985
epoch£º1203	 i:1 	 global-step:24061	 l-p:0.15681225061416626
epoch£º1203	 i:2 	 global-step:24062	 l-p:0.11403168737888336
epoch£º1203	 i:3 	 global-step:24063	 l-p:0.17116901278495789
epoch£º1203	 i:4 	 global-step:24064	 l-p:0.12187483161687851
epoch£º1203	 i:5 	 global-step:24065	 l-p:0.15349732339382172
epoch£º1203	 i:6 	 global-step:24066	 l-p:0.1058933213353157
epoch£º1203	 i:7 	 global-step:24067	 l-p:0.13559383153915405
epoch£º1203	 i:8 	 global-step:24068	 l-p:0.13141870498657227
epoch£º1203	 i:9 	 global-step:24069	 l-p:0.1266278475522995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1608, 5.1516, 5.1601],
        [5.1608, 5.1310, 5.1560],
        [5.1608, 4.8668, 4.7598],
        [5.1608, 4.9980, 5.0556]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1204, step:0 
model_pd.l_p.mean(): 0.1661401093006134 
model_pd.l_d.mean(): -20.564956665039062 
model_pd.lagr.mean(): -20.39881706237793 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4278], device='cuda:0')), ('power', tensor([-21.3926], device='cuda:0'))])
epoch£º1204	 i:0 	 global-step:24080	 l-p:0.1661401093006134
epoch£º1204	 i:1 	 global-step:24081	 l-p:0.1414068639278412
epoch£º1204	 i:2 	 global-step:24082	 l-p:0.12855158746242523
epoch£º1204	 i:3 	 global-step:24083	 l-p:0.12369901686906815
epoch£º1204	 i:4 	 global-step:24084	 l-p:0.12949013710021973
epoch£º1204	 i:5 	 global-step:24085	 l-p:0.26404377818107605
epoch£º1204	 i:6 	 global-step:24086	 l-p:0.08171946555376053
epoch£º1204	 i:7 	 global-step:24087	 l-p:0.14381957054138184
epoch£º1204	 i:8 	 global-step:24088	 l-p:0.1126461774110794
epoch£º1204	 i:9 	 global-step:24089	 l-p:0.16027171909809113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1606, 4.8987, 4.5846],
        [5.1606, 5.1066, 5.1470],
        [5.1606, 5.2733, 5.0118],
        [5.1606, 4.9235, 4.5911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1205, step:0 
model_pd.l_p.mean(): 0.11623141169548035 
model_pd.l_d.mean(): -20.381824493408203 
model_pd.lagr.mean(): -20.265592575073242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4669], device='cuda:0')), ('power', tensor([-21.2465], device='cuda:0'))])
epoch£º1205	 i:0 	 global-step:24100	 l-p:0.11623141169548035
epoch£º1205	 i:1 	 global-step:24101	 l-p:0.14994075894355774
epoch£º1205	 i:2 	 global-step:24102	 l-p:0.12700389325618744
epoch£º1205	 i:3 	 global-step:24103	 l-p:0.2202608436346054
epoch£º1205	 i:4 	 global-step:24104	 l-p:0.10305242240428925
epoch£º1205	 i:5 	 global-step:24105	 l-p:0.1461758315563202
epoch£º1205	 i:6 	 global-step:24106	 l-p:0.1644221693277359
epoch£º1205	 i:7 	 global-step:24107	 l-p:0.1296035200357437
epoch£º1205	 i:8 	 global-step:24108	 l-p:0.19315937161445618
epoch£º1205	 i:9 	 global-step:24109	 l-p:0.11610591411590576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1569, 5.1569, 5.1569],
        [5.1569, 4.8511, 4.6518],
        [5.1569, 5.1052, 5.1444],
        [5.1569, 4.8766, 4.8114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1206, step:0 
model_pd.l_p.mean(): 0.1689542680978775 
model_pd.l_d.mean(): -20.59926414489746 
model_pd.lagr.mean(): -20.430309295654297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4176], device='cuda:0')), ('power', tensor([-21.4170], device='cuda:0'))])
epoch£º1206	 i:0 	 global-step:24120	 l-p:0.1689542680978775
epoch£º1206	 i:1 	 global-step:24121	 l-p:0.12676794826984406
epoch£º1206	 i:2 	 global-step:24122	 l-p:0.07339641451835632
epoch£º1206	 i:3 	 global-step:24123	 l-p:0.1414547860622406
epoch£º1206	 i:4 	 global-step:24124	 l-p:0.1623874008655548
epoch£º1206	 i:5 	 global-step:24125	 l-p:0.1835981160402298
epoch£º1206	 i:6 	 global-step:24126	 l-p:0.17447105050086975
epoch£º1206	 i:7 	 global-step:24127	 l-p:0.09240832924842834
epoch£º1206	 i:8 	 global-step:24128	 l-p:0.1268853098154068
epoch£º1206	 i:9 	 global-step:24129	 l-p:0.212324857711792
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1611, 4.9782, 5.0283],
        [5.1611, 4.8782, 4.5912],
        [5.1611, 5.1599, 5.1611],
        [5.1611, 4.8567, 4.6913]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1207, step:0 
model_pd.l_p.mean(): 0.17123474180698395 
model_pd.l_d.mean(): -20.0443058013916 
model_pd.lagr.mean(): -19.873071670532227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5068], device='cuda:0')), ('power', tensor([-20.9440], device='cuda:0'))])
epoch£º1207	 i:0 	 global-step:24140	 l-p:0.17123474180698395
epoch£º1207	 i:1 	 global-step:24141	 l-p:0.13284710049629211
epoch£º1207	 i:2 	 global-step:24142	 l-p:0.11819417774677277
epoch£º1207	 i:3 	 global-step:24143	 l-p:0.169536754488945
epoch£º1207	 i:4 	 global-step:24144	 l-p:0.17259465157985687
epoch£º1207	 i:5 	 global-step:24145	 l-p:0.12843550741672516
epoch£º1207	 i:6 	 global-step:24146	 l-p:0.14641104638576508
epoch£º1207	 i:7 	 global-step:24147	 l-p:0.151055708527565
epoch£º1207	 i:8 	 global-step:24148	 l-p:0.12426263839006424
epoch£º1207	 i:9 	 global-step:24149	 l-p:0.11860749870538712
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1672, 4.9848, 4.6363],
        [5.1672, 5.0335, 5.0954],
        [5.1672, 5.1672, 5.1672],
        [5.1672, 5.0148, 5.0747]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1208, step:0 
model_pd.l_p.mean(): 0.12919452786445618 
model_pd.l_d.mean(): -19.9296875 
model_pd.lagr.mean(): -19.800493240356445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5120], device='cuda:0')), ('power', tensor([-20.8327], device='cuda:0'))])
epoch£º1208	 i:0 	 global-step:24160	 l-p:0.12919452786445618
epoch£º1208	 i:1 	 global-step:24161	 l-p:0.20516996085643768
epoch£º1208	 i:2 	 global-step:24162	 l-p:0.17595577239990234
epoch£º1208	 i:3 	 global-step:24163	 l-p:0.12372852861881256
epoch£º1208	 i:4 	 global-step:24164	 l-p:0.1358516961336136
epoch£º1208	 i:5 	 global-step:24165	 l-p:0.18450194597244263
epoch£º1208	 i:6 	 global-step:24166	 l-p:0.13811339437961578
epoch£º1208	 i:7 	 global-step:24167	 l-p:0.08462728559970856
epoch£º1208	 i:8 	 global-step:24168	 l-p:0.1318914145231247
epoch£º1208	 i:9 	 global-step:24169	 l-p:0.10869121551513672
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1664, 5.0706, 5.1282],
        [5.1664, 5.1538, 5.1653],
        [5.1664, 5.1664, 5.1664],
        [5.1664, 5.4396, 5.2678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1209, step:0 
model_pd.l_p.mean(): 0.16522619128227234 
model_pd.l_d.mean(): -19.50490951538086 
model_pd.lagr.mean(): -19.339683532714844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4731], device='cuda:0')), ('power', tensor([-20.3597], device='cuda:0'))])
epoch£º1209	 i:0 	 global-step:24180	 l-p:0.16522619128227234
epoch£º1209	 i:1 	 global-step:24181	 l-p:0.17140960693359375
epoch£º1209	 i:2 	 global-step:24182	 l-p:0.16106516122817993
epoch£º1209	 i:3 	 global-step:24183	 l-p:0.10437311977148056
epoch£º1209	 i:4 	 global-step:24184	 l-p:0.13990116119384766
epoch£º1209	 i:5 	 global-step:24185	 l-p:0.11462320387363434
epoch£º1209	 i:6 	 global-step:24186	 l-p:0.11641331762075424
epoch£º1209	 i:7 	 global-step:24187	 l-p:0.21173322200775146
epoch£º1209	 i:8 	 global-step:24188	 l-p:0.1212707906961441
epoch£º1209	 i:9 	 global-step:24189	 l-p:0.12376602739095688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1625, 5.1608, 5.1625],
        [5.1625, 5.3735, 5.1652],
        [5.1625, 5.1571, 5.1622],
        [5.1625, 5.1098, 5.1495]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1210, step:0 
model_pd.l_p.mean(): 0.15289275348186493 
model_pd.l_d.mean(): -19.90047836303711 
model_pd.lagr.mean(): -19.74758529663086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5274], device='cuda:0')), ('power', tensor([-20.8188], device='cuda:0'))])
epoch£º1210	 i:0 	 global-step:24200	 l-p:0.15289275348186493
epoch£º1210	 i:1 	 global-step:24201	 l-p:0.12613363564014435
epoch£º1210	 i:2 	 global-step:24202	 l-p:0.12278687953948975
epoch£º1210	 i:3 	 global-step:24203	 l-p:0.1710093915462494
epoch£º1210	 i:4 	 global-step:24204	 l-p:0.10233952105045319
epoch£º1210	 i:5 	 global-step:24205	 l-p:0.10227300226688385
epoch£º1210	 i:6 	 global-step:24206	 l-p:0.14097809791564941
epoch£º1210	 i:7 	 global-step:24207	 l-p:0.16189372539520264
epoch£º1210	 i:8 	 global-step:24208	 l-p:0.25289589166641235
epoch£º1210	 i:9 	 global-step:24209	 l-p:0.12080002576112747
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1594, 5.1594, 5.1594],
        [5.1594, 5.1449, 5.1580],
        [5.1594, 5.1594, 5.1594],
        [5.1594, 5.1594, 5.1594]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1211, step:0 
model_pd.l_p.mean(): 0.1931925117969513 
model_pd.l_d.mean(): -20.428661346435547 
model_pd.lagr.mean(): -20.2354679107666 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4591], device='cuda:0')), ('power', tensor([-21.2862], device='cuda:0'))])
epoch£º1211	 i:0 	 global-step:24220	 l-p:0.1931925117969513
epoch£º1211	 i:1 	 global-step:24221	 l-p:0.11831386387348175
epoch£º1211	 i:2 	 global-step:24222	 l-p:0.15126332640647888
epoch£º1211	 i:3 	 global-step:24223	 l-p:0.11878051608800888
epoch£º1211	 i:4 	 global-step:24224	 l-p:0.23924186825752258
epoch£º1211	 i:5 	 global-step:24225	 l-p:0.13688892126083374
epoch£º1211	 i:6 	 global-step:24226	 l-p:0.12758289277553558
epoch£º1211	 i:7 	 global-step:24227	 l-p:0.07599936425685883
epoch£º1211	 i:8 	 global-step:24228	 l-p:0.1570666879415512
epoch£º1211	 i:9 	 global-step:24229	 l-p:0.13981400430202484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1599, 5.1536, 5.1595],
        [5.1599, 5.1580, 5.1598],
        [5.1599, 4.9868, 5.0411],
        [5.1599, 5.1598, 5.1599]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1212, step:0 
model_pd.l_p.mean(): 0.18700793385505676 
model_pd.l_d.mean(): -19.149953842163086 
model_pd.lagr.mean(): -18.96294593811035 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5550], device='cuda:0')), ('power', tensor([-20.0829], device='cuda:0'))])
epoch£º1212	 i:0 	 global-step:24240	 l-p:0.18700793385505676
epoch£º1212	 i:1 	 global-step:24241	 l-p:0.0933838039636612
epoch£º1212	 i:2 	 global-step:24242	 l-p:0.13909916579723358
epoch£º1212	 i:3 	 global-step:24243	 l-p:0.08388786762952805
epoch£º1212	 i:4 	 global-step:24244	 l-p:0.15998581051826477
epoch£º1212	 i:5 	 global-step:24245	 l-p:0.21473661065101624
epoch£º1212	 i:6 	 global-step:24246	 l-p:0.19569522142410278
epoch£º1212	 i:7 	 global-step:24247	 l-p:0.12378107011318207
epoch£º1212	 i:8 	 global-step:24248	 l-p:0.10963767021894455
epoch£º1212	 i:9 	 global-step:24249	 l-p:0.13936103880405426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1655, 5.1653, 5.1655],
        [5.1655, 5.1654, 5.1655],
        [5.1655, 5.0797, 5.1343],
        [5.1655, 5.0343, 5.0964]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1213, step:0 
model_pd.l_p.mean(): 0.15377536416053772 
model_pd.l_d.mean(): -20.236574172973633 
model_pd.lagr.mean(): -20.08279800415039 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4712], device='cuda:0')), ('power', tensor([-21.1030], device='cuda:0'))])
epoch£º1213	 i:0 	 global-step:24260	 l-p:0.15377536416053772
epoch£º1213	 i:1 	 global-step:24261	 l-p:0.13734908401966095
epoch£º1213	 i:2 	 global-step:24262	 l-p:0.11610126495361328
epoch£º1213	 i:3 	 global-step:24263	 l-p:0.10016078501939774
epoch£º1213	 i:4 	 global-step:24264	 l-p:0.10147438943386078
epoch£º1213	 i:5 	 global-step:24265	 l-p:0.13036027550697327
epoch£º1213	 i:6 	 global-step:24266	 l-p:0.15075726807117462
epoch£º1213	 i:7 	 global-step:24267	 l-p:0.1322060525417328
epoch£º1213	 i:8 	 global-step:24268	 l-p:0.1921401023864746
epoch£º1213	 i:9 	 global-step:24269	 l-p:0.20675738155841827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1673, 5.0131, 5.0727],
        [5.1673, 4.9514, 4.6103],
        [5.1673, 5.1024, 5.1485],
        [5.1673, 5.0887, 5.1407]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1214, step:0 
model_pd.l_p.mean(): 0.1379375457763672 
model_pd.l_d.mean(): -20.310380935668945 
model_pd.lagr.mean(): -20.172443389892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4563], device='cuda:0')), ('power', tensor([-21.1628], device='cuda:0'))])
epoch£º1214	 i:0 	 global-step:24280	 l-p:0.1379375457763672
epoch£º1214	 i:1 	 global-step:24281	 l-p:0.12037745863199234
epoch£º1214	 i:2 	 global-step:24282	 l-p:0.17912369966506958
epoch£º1214	 i:3 	 global-step:24283	 l-p:0.14227953553199768
epoch£º1214	 i:4 	 global-step:24284	 l-p:0.1494154930114746
epoch£º1214	 i:5 	 global-step:24285	 l-p:0.128867506980896
epoch£º1214	 i:6 	 global-step:24286	 l-p:0.11432095617055893
epoch£º1214	 i:7 	 global-step:24287	 l-p:0.1609359085559845
epoch£º1214	 i:8 	 global-step:24288	 l-p:0.11558349430561066
epoch£º1214	 i:9 	 global-step:24289	 l-p:0.1708795577287674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1676, 4.9651, 4.6201],
        [5.1676, 4.9843, 4.6357],
        [5.1676, 4.9240, 4.9225],
        [5.1676, 5.1676, 5.1676]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1215, step:0 
model_pd.l_p.mean(): 0.13103656470775604 
model_pd.l_d.mean(): -20.507652282714844 
model_pd.lagr.mean(): -20.376615524291992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4393], device='cuda:0')), ('power', tensor([-21.3461], device='cuda:0'))])
epoch£º1215	 i:0 	 global-step:24300	 l-p:0.13103656470775604
epoch£º1215	 i:1 	 global-step:24301	 l-p:0.14888912439346313
epoch£º1215	 i:2 	 global-step:24302	 l-p:0.14206071197986603
epoch£º1215	 i:3 	 global-step:24303	 l-p:0.15791326761245728
epoch£º1215	 i:4 	 global-step:24304	 l-p:0.16680079698562622
epoch£º1215	 i:5 	 global-step:24305	 l-p:0.112286277115345
epoch£º1215	 i:6 	 global-step:24306	 l-p:0.14925479888916016
epoch£º1215	 i:7 	 global-step:24307	 l-p:0.16575275361537933
epoch£º1215	 i:8 	 global-step:24308	 l-p:0.10408875346183777
epoch£º1215	 i:9 	 global-step:24309	 l-p:0.1359841376543045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1696, 4.9868, 4.6384],
        [5.1696, 5.1182, 4.7872],
        [5.1696, 5.0739, 5.1314],
        [5.1696, 5.1693, 5.1696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1216, step:0 
model_pd.l_p.mean(): 0.12181121855974197 
model_pd.l_d.mean(): -19.308124542236328 
model_pd.lagr.mean(): -19.18631362915039 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4756], device='cuda:0')), ('power', tensor([-20.1617], device='cuda:0'))])
epoch£º1216	 i:0 	 global-step:24320	 l-p:0.12181121855974197
epoch£º1216	 i:1 	 global-step:24321	 l-p:0.14412909746170044
epoch£º1216	 i:2 	 global-step:24322	 l-p:0.14708678424358368
epoch£º1216	 i:3 	 global-step:24323	 l-p:0.13298186659812927
epoch£º1216	 i:4 	 global-step:24324	 l-p:0.11038767546415329
epoch£º1216	 i:5 	 global-step:24325	 l-p:0.08556308597326279
epoch£º1216	 i:6 	 global-step:24326	 l-p:0.12364131957292557
epoch£º1216	 i:7 	 global-step:24327	 l-p:0.1493140459060669
epoch£º1216	 i:8 	 global-step:24328	 l-p:0.23972506821155548
epoch£º1216	 i:9 	 global-step:24329	 l-p:0.16648463904857635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1669, 5.1638, 5.1668],
        [5.1669, 4.8667, 4.7311],
        [5.1669, 5.0095, 4.6593],
        [5.1669, 4.9783, 5.0253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1217, step:0 
model_pd.l_p.mean(): 0.1542862504720688 
model_pd.l_d.mean(): -19.335886001586914 
model_pd.lagr.mean(): -19.18160057067871 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5392], device='cuda:0')), ('power', tensor([-20.2559], device='cuda:0'))])
epoch£º1217	 i:0 	 global-step:24340	 l-p:0.1542862504720688
epoch£º1217	 i:1 	 global-step:24341	 l-p:0.13033850491046906
epoch£º1217	 i:2 	 global-step:24342	 l-p:0.14151433110237122
epoch£º1217	 i:3 	 global-step:24343	 l-p:0.1782163828611374
epoch£º1217	 i:4 	 global-step:24344	 l-p:0.1118474155664444
epoch£º1217	 i:5 	 global-step:24345	 l-p:0.2087634801864624
epoch£º1217	 i:6 	 global-step:24346	 l-p:0.1491762101650238
epoch£º1217	 i:7 	 global-step:24347	 l-p:0.1288175880908966
epoch£º1217	 i:8 	 global-step:24348	 l-p:0.054821185767650604
epoch£º1217	 i:9 	 global-step:24349	 l-p:0.15413495898246765
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1708, 4.9689, 4.6240],
        [5.1708, 4.8663, 4.6858],
        [5.1708, 5.1708, 5.1708],
        [5.1708, 5.0531, 5.1145]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1218, step:0 
model_pd.l_p.mean(): 0.16629832983016968 
model_pd.l_d.mean(): -19.741004943847656 
model_pd.lagr.mean(): -19.57470703125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4990], device='cuda:0')), ('power', tensor([-20.6270], device='cuda:0'))])
epoch£º1218	 i:0 	 global-step:24360	 l-p:0.16629832983016968
epoch£º1218	 i:1 	 global-step:24361	 l-p:0.1601954847574234
epoch£º1218	 i:2 	 global-step:24362	 l-p:0.17122894525527954
epoch£º1218	 i:3 	 global-step:24363	 l-p:0.14177268743515015
epoch£º1218	 i:4 	 global-step:24364	 l-p:0.17323806881904602
epoch£º1218	 i:5 	 global-step:24365	 l-p:0.07281331717967987
epoch£º1218	 i:6 	 global-step:24366	 l-p:0.14956405758857727
epoch£º1218	 i:7 	 global-step:24367	 l-p:0.10195519030094147
epoch£º1218	 i:8 	 global-step:24368	 l-p:0.13034752011299133
epoch£º1218	 i:9 	 global-step:24369	 l-p:0.12880684435367584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1734, 4.8697, 4.6610],
        [5.1734, 5.0884, 5.1427],
        [5.1734, 5.1336, 5.1655],
        [5.1734, 5.1734, 5.1734]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1219, step:0 
model_pd.l_p.mean(): 0.13145709037780762 
model_pd.l_d.mean(): -19.185827255249023 
model_pd.lagr.mean(): -19.054370880126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5054], device='cuda:0')), ('power', tensor([-20.0681], device='cuda:0'))])
epoch£º1219	 i:0 	 global-step:24380	 l-p:0.13145709037780762
epoch£º1219	 i:1 	 global-step:24381	 l-p:0.13220763206481934
epoch£º1219	 i:2 	 global-step:24382	 l-p:0.06952612847089767
epoch£º1219	 i:3 	 global-step:24383	 l-p:0.17263467609882355
epoch£º1219	 i:4 	 global-step:24384	 l-p:0.11165961623191833
epoch£º1219	 i:5 	 global-step:24385	 l-p:0.17551679909229279
epoch£º1219	 i:6 	 global-step:24386	 l-p:0.16533510386943817
epoch£º1219	 i:7 	 global-step:24387	 l-p:0.1208987757563591
epoch£º1219	 i:8 	 global-step:24388	 l-p:0.20420201122760773
epoch£º1219	 i:9 	 global-step:24389	 l-p:0.10999561846256256
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1739, 5.1738, 5.1739],
        [5.1739, 5.3882, 5.1815],
        [5.1739, 5.0791, 4.7366],
        [5.1739, 4.9932, 4.6448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1220, step:0 
model_pd.l_p.mean(): 0.17613250017166138 
model_pd.l_d.mean(): -20.588659286499023 
model_pd.lagr.mean(): -20.412527084350586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4257], device='cuda:0')), ('power', tensor([-21.4145], device='cuda:0'))])
epoch£º1220	 i:0 	 global-step:24400	 l-p:0.17613250017166138
epoch£º1220	 i:1 	 global-step:24401	 l-p:0.13666950166225433
epoch£º1220	 i:2 	 global-step:24402	 l-p:0.12799639999866486
epoch£º1220	 i:3 	 global-step:24403	 l-p:0.12247277051210403
epoch£º1220	 i:4 	 global-step:24404	 l-p:0.1858559250831604
epoch£º1220	 i:5 	 global-step:24405	 l-p:0.12078309059143066
epoch£º1220	 i:6 	 global-step:24406	 l-p:0.07435683161020279
epoch£º1220	 i:7 	 global-step:24407	 l-p:0.14181804656982422
epoch£º1220	 i:8 	 global-step:24408	 l-p:0.1324903815984726
epoch£º1220	 i:9 	 global-step:24409	 l-p:0.17541922628879547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1716, 5.1705, 5.1716],
        [5.1716, 5.1716, 5.1716],
        [5.1716, 5.1413, 4.8172],
        [5.1716, 4.8672, 4.6654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1221, step:0 
model_pd.l_p.mean(): 0.10970066487789154 
model_pd.l_d.mean(): -20.259984970092773 
model_pd.lagr.mean(): -20.150283813476562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4837], device='cuda:0')), ('power', tensor([-21.1399], device='cuda:0'))])
epoch£º1221	 i:0 	 global-step:24420	 l-p:0.10970066487789154
epoch£º1221	 i:1 	 global-step:24421	 l-p:0.10936561226844788
epoch£º1221	 i:2 	 global-step:24422	 l-p:0.15335282683372498
epoch£º1221	 i:3 	 global-step:24423	 l-p:0.17419566214084625
epoch£º1221	 i:4 	 global-step:24424	 l-p:0.16394644975662231
epoch£º1221	 i:5 	 global-step:24425	 l-p:0.13383355736732483
epoch£º1221	 i:6 	 global-step:24426	 l-p:0.15283209085464478
epoch£º1221	 i:7 	 global-step:24427	 l-p:0.17687763273715973
epoch£º1221	 i:8 	 global-step:24428	 l-p:0.1092926487326622
epoch£º1221	 i:9 	 global-step:24429	 l-p:0.12359792739152908
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1222
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1689, 5.1689, 5.1689],
        [5.1689, 5.1122, 5.1541],
        [5.1689, 5.1518, 5.1671],
        [5.1689, 5.1684, 5.1689]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1222, step:0 
model_pd.l_p.mean(): 0.13452191650867462 
model_pd.l_d.mean(): -20.632944107055664 
model_pd.lagr.mean(): -20.498422622680664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4111], device='cuda:0')), ('power', tensor([-21.4445], device='cuda:0'))])
epoch£º1222	 i:0 	 global-step:24440	 l-p:0.13452191650867462
epoch£º1222	 i:1 	 global-step:24441	 l-p:0.13217729330062866
epoch£º1222	 i:2 	 global-step:24442	 l-p:0.1386176198720932
epoch£º1222	 i:3 	 global-step:24443	 l-p:0.1076311245560646
epoch£º1222	 i:4 	 global-step:24444	 l-p:0.11472691595554352
epoch£º1222	 i:5 	 global-step:24445	 l-p:0.14922934770584106
epoch£º1222	 i:6 	 global-step:24446	 l-p:0.13952207565307617
epoch£º1222	 i:7 	 global-step:24447	 l-p:0.16281253099441528
epoch£º1222	 i:8 	 global-step:24448	 l-p:0.23343198001384735
epoch£º1222	 i:9 	 global-step:24449	 l-p:0.1296541392803192
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1614, 5.1101, 5.1490],
        [5.1614, 5.1474, 5.1601],
        [5.1614, 5.0801, 5.1331],
        [5.1614, 4.9371, 4.9581]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1223, step:0 
model_pd.l_p.mean(): 0.11725050956010818 
model_pd.l_d.mean(): -19.292098999023438 
model_pd.lagr.mean(): -19.174848556518555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5030], device='cuda:0')), ('power', tensor([-20.1738], device='cuda:0'))])
epoch£º1223	 i:0 	 global-step:24460	 l-p:0.11725050956010818
epoch£º1223	 i:1 	 global-step:24461	 l-p:0.19143567979335785
epoch£º1223	 i:2 	 global-step:24462	 l-p:0.1390797197818756
epoch£º1223	 i:3 	 global-step:24463	 l-p:0.14732326567173004
epoch£º1223	 i:4 	 global-step:24464	 l-p:0.198993980884552
epoch£º1223	 i:5 	 global-step:24465	 l-p:0.16365878283977509
epoch£º1223	 i:6 	 global-step:24466	 l-p:0.12034550309181213
epoch£º1223	 i:7 	 global-step:24467	 l-p:0.10777068883180618
epoch£º1223	 i:8 	 global-step:24468	 l-p:0.1381392478942871
epoch£º1223	 i:9 	 global-step:24469	 l-p:0.13087163865566254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1597, 5.0775, 5.1309],
        [5.1597, 5.1597, 5.1597],
        [5.1597, 5.5188, 5.4008],
        [5.1597, 4.8552, 4.6360]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1224, step:0 
model_pd.l_p.mean(): 0.20966237783432007 
model_pd.l_d.mean(): -19.027786254882812 
model_pd.lagr.mean(): -18.818124771118164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5229], device='cuda:0')), ('power', tensor([-19.9251], device='cuda:0'))])
epoch£º1224	 i:0 	 global-step:24480	 l-p:0.20966237783432007
epoch£º1224	 i:1 	 global-step:24481	 l-p:0.16696828603744507
epoch£º1224	 i:2 	 global-step:24482	 l-p:0.13226431608200073
epoch£º1224	 i:3 	 global-step:24483	 l-p:0.1073262169957161
epoch£º1224	 i:4 	 global-step:24484	 l-p:0.20139840245246887
epoch£º1224	 i:5 	 global-step:24485	 l-p:0.1297217756509781
epoch£º1224	 i:6 	 global-step:24486	 l-p:0.13497471809387207
epoch£º1224	 i:7 	 global-step:24487	 l-p:0.14808465540409088
epoch£º1224	 i:8 	 global-step:24488	 l-p:0.09326809644699097
epoch£º1224	 i:9 	 global-step:24489	 l-p:0.13362394273281097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1597, 5.1367, 5.1566],
        [5.1597, 5.1595, 5.1597],
        [5.1597, 5.1409, 5.1575],
        [5.1597, 5.1597, 5.1597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1225, step:0 
model_pd.l_p.mean(): 0.10965491086244583 
model_pd.l_d.mean(): -20.30387306213379 
model_pd.lagr.mean(): -20.194217681884766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4402], device='cuda:0')), ('power', tensor([-21.1395], device='cuda:0'))])
epoch£º1225	 i:0 	 global-step:24500	 l-p:0.10965491086244583
epoch£º1225	 i:1 	 global-step:24501	 l-p:0.22247150540351868
epoch£º1225	 i:2 	 global-step:24502	 l-p:0.13578511774539948
epoch£º1225	 i:3 	 global-step:24503	 l-p:0.12521947920322418
epoch£º1225	 i:4 	 global-step:24504	 l-p:0.12252947688102722
epoch£º1225	 i:5 	 global-step:24505	 l-p:0.12917274236679077
epoch£º1225	 i:6 	 global-step:24506	 l-p:0.16820797324180603
epoch£º1225	 i:7 	 global-step:24507	 l-p:0.19783060252666473
epoch£º1225	 i:8 	 global-step:24508	 l-p:0.11290565878152847
epoch£º1225	 i:9 	 global-step:24509	 l-p:0.15045346319675446
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1561, 5.0220, 5.0842],
        [5.1561, 4.8795, 4.5803],
        [5.1561, 5.1561, 5.1561],
        [5.1561, 5.1561, 5.1561]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1226, step:0 
model_pd.l_p.mean(): 0.12552514672279358 
model_pd.l_d.mean(): -18.979684829711914 
model_pd.lagr.mean(): -18.85416030883789 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5718], device='cuda:0')), ('power', tensor([-19.9268], device='cuda:0'))])
epoch£º1226	 i:0 	 global-step:24520	 l-p:0.12552514672279358
epoch£º1226	 i:1 	 global-step:24521	 l-p:0.16655941307544708
epoch£º1226	 i:2 	 global-step:24522	 l-p:0.11663901805877686
epoch£º1226	 i:3 	 global-step:24523	 l-p:0.17738032341003418
epoch£º1226	 i:4 	 global-step:24524	 l-p:0.20044612884521484
epoch£º1226	 i:5 	 global-step:24525	 l-p:0.16467832028865814
epoch£º1226	 i:6 	 global-step:24526	 l-p:0.11692661792039871
epoch£º1226	 i:7 	 global-step:24527	 l-p:0.09734002500772476
epoch£º1226	 i:8 	 global-step:24528	 l-p:0.15916894376277924
epoch£º1226	 i:9 	 global-step:24529	 l-p:0.15484540164470673
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1566, 5.1511, 5.1564],
        [5.1566, 5.1554, 5.1566],
        [5.1566, 5.0646, 5.1212],
        [5.1566, 5.1566, 5.1567]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1227, step:0 
model_pd.l_p.mean(): 0.0995909571647644 
model_pd.l_d.mean(): -20.030447006225586 
model_pd.lagr.mean(): -19.930856704711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5018], device='cuda:0')), ('power', tensor([-20.9248], device='cuda:0'))])
epoch£º1227	 i:0 	 global-step:24540	 l-p:0.0995909571647644
epoch£º1227	 i:1 	 global-step:24541	 l-p:0.18307244777679443
epoch£º1227	 i:2 	 global-step:24542	 l-p:0.1220845952630043
epoch£º1227	 i:3 	 global-step:24543	 l-p:0.14495974779129028
epoch£º1227	 i:4 	 global-step:24544	 l-p:0.09432271122932434
epoch£º1227	 i:5 	 global-step:24545	 l-p:0.14138421416282654
epoch£º1227	 i:6 	 global-step:24546	 l-p:0.16680335998535156
epoch£º1227	 i:7 	 global-step:24547	 l-p:0.204273521900177
epoch£º1227	 i:8 	 global-step:24548	 l-p:0.14060381054878235
epoch£º1227	 i:9 	 global-step:24549	 l-p:0.1705378144979477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1602, 4.9935, 4.6427],
        [5.1602, 5.1490, 5.1593],
        [5.1602, 5.1570, 5.1601],
        [5.1602, 4.9749, 4.6255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1228, step:0 
model_pd.l_p.mean(): 0.13000395894050598 
model_pd.l_d.mean(): -20.34824562072754 
model_pd.lagr.mean(): -20.21824073791504 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4492], device='cuda:0')), ('power', tensor([-21.1940], device='cuda:0'))])
epoch£º1228	 i:0 	 global-step:24560	 l-p:0.13000395894050598
epoch£º1228	 i:1 	 global-step:24561	 l-p:0.1436040997505188
epoch£º1228	 i:2 	 global-step:24562	 l-p:0.14894697070121765
epoch£º1228	 i:3 	 global-step:24563	 l-p:0.12377616763114929
epoch£º1228	 i:4 	 global-step:24564	 l-p:0.1305227130651474
epoch£º1228	 i:5 	 global-step:24565	 l-p:0.169057697057724
epoch£º1228	 i:6 	 global-step:24566	 l-p:0.09508494287729263
epoch£º1228	 i:7 	 global-step:24567	 l-p:0.21827350556850433
epoch£º1228	 i:8 	 global-step:24568	 l-p:0.13190330564975739
epoch£º1228	 i:9 	 global-step:24569	 l-p:0.14869076013565063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1669, 5.1142, 5.1539],
        [5.1669, 5.1590, 5.1664],
        [5.1669, 4.8616, 4.6815],
        [5.1669, 4.8664, 4.7306]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1229, step:0 
model_pd.l_p.mean(): 0.0927635058760643 
model_pd.l_d.mean(): -19.231830596923828 
model_pd.lagr.mean(): -19.139066696166992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4912], device='cuda:0')), ('power', tensor([-20.1002], device='cuda:0'))])
epoch£º1229	 i:0 	 global-step:24580	 l-p:0.0927635058760643
epoch£º1229	 i:1 	 global-step:24581	 l-p:0.1977860927581787
epoch£º1229	 i:2 	 global-step:24582	 l-p:0.1252366602420807
epoch£º1229	 i:3 	 global-step:24583	 l-p:0.1381330043077469
epoch£º1229	 i:4 	 global-step:24584	 l-p:0.18803931772708893
epoch£º1229	 i:5 	 global-step:24585	 l-p:0.11602558195590973
epoch£º1229	 i:6 	 global-step:24586	 l-p:0.13004349172115326
epoch£º1229	 i:7 	 global-step:24587	 l-p:0.15731030702590942
epoch£º1229	 i:8 	 global-step:24588	 l-p:0.13443022966384888
epoch£º1229	 i:9 	 global-step:24589	 l-p:0.1276427060365677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1721, 5.1161, 5.1577],
        [5.1721, 5.1633, 5.1715],
        [5.1721, 5.1716, 5.1721],
        [5.1721, 5.0388, 4.6899]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1230, step:0 
model_pd.l_p.mean(): 0.12528379261493683 
model_pd.l_d.mean(): -20.758922576904297 
model_pd.lagr.mean(): -20.633638381958008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3949], device='cuda:0')), ('power', tensor([-21.5561], device='cuda:0'))])
epoch£º1230	 i:0 	 global-step:24600	 l-p:0.12528379261493683
epoch£º1230	 i:1 	 global-step:24601	 l-p:0.14301669597625732
epoch£º1230	 i:2 	 global-step:24602	 l-p:0.19809532165527344
epoch£º1230	 i:3 	 global-step:24603	 l-p:0.12322286516427994
epoch£º1230	 i:4 	 global-step:24604	 l-p:0.1225886344909668
epoch£º1230	 i:5 	 global-step:24605	 l-p:0.10183189064264297
epoch£º1230	 i:6 	 global-step:24606	 l-p:0.13295595347881317
epoch£º1230	 i:7 	 global-step:24607	 l-p:0.18710015714168549
epoch£º1230	 i:8 	 global-step:24608	 l-p:0.11741001158952713
epoch£º1230	 i:9 	 global-step:24609	 l-p:0.1447349190711975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1715, 4.9177, 4.5972],
        [5.1715, 5.1714, 5.1715],
        [5.1715, 5.1715, 5.1715],
        [5.1715, 5.0339, 4.6845]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1231, step:0 
model_pd.l_p.mean(): 0.09127415716648102 
model_pd.l_d.mean(): -20.48705291748047 
model_pd.lagr.mean(): -20.39577865600586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4331], device='cuda:0')), ('power', tensor([-21.3187], device='cuda:0'))])
epoch£º1231	 i:0 	 global-step:24620	 l-p:0.09127415716648102
epoch£º1231	 i:1 	 global-step:24621	 l-p:0.13584163784980774
epoch£º1231	 i:2 	 global-step:24622	 l-p:0.20117318630218506
epoch£º1231	 i:3 	 global-step:24623	 l-p:0.14999930560588837
epoch£º1231	 i:4 	 global-step:24624	 l-p:0.2539307773113251
epoch£º1231	 i:5 	 global-step:24625	 l-p:0.14752764999866486
epoch£º1231	 i:6 	 global-step:24626	 l-p:0.12182814627885818
epoch£º1231	 i:7 	 global-step:24627	 l-p:0.11688485741615295
epoch£º1231	 i:8 	 global-step:24628	 l-p:0.1263969987630844
epoch£º1231	 i:9 	 global-step:24629	 l-p:0.06790415197610855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1636, 4.8960, 4.5869],
        [5.1636, 5.1471, 5.1618],
        [5.1636, 4.8759, 4.5967],
        [5.1636, 4.8879, 4.8331]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1232, step:0 
model_pd.l_p.mean(): 0.1725146323442459 
model_pd.l_d.mean(): -20.62788200378418 
model_pd.lagr.mean(): -20.455368041992188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4243], device='cuda:0')), ('power', tensor([-21.4531], device='cuda:0'))])
epoch£º1232	 i:0 	 global-step:24640	 l-p:0.1725146323442459
epoch£º1232	 i:1 	 global-step:24641	 l-p:0.07006015628576279
epoch£º1232	 i:2 	 global-step:24642	 l-p:0.13124284148216248
epoch£º1232	 i:3 	 global-step:24643	 l-p:0.15246279537677765
epoch£º1232	 i:4 	 global-step:24644	 l-p:0.1492345631122589
epoch£º1232	 i:5 	 global-step:24645	 l-p:0.13667736947536469
epoch£º1232	 i:6 	 global-step:24646	 l-p:0.23956698179244995
epoch£º1232	 i:7 	 global-step:24647	 l-p:0.1394176483154297
epoch£º1232	 i:8 	 global-step:24648	 l-p:0.14125731587409973
epoch£º1232	 i:9 	 global-step:24649	 l-p:0.12383769452571869
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1574, 4.8694, 4.7853],
        [5.1574, 5.0313, 4.6824],
        [5.1574, 5.1574, 5.1574],
        [5.1574, 5.1483, 5.1568]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1233, step:0 
model_pd.l_p.mean(): 0.10837607085704803 
model_pd.l_d.mean(): -20.44622039794922 
model_pd.lagr.mean(): -20.337844848632812 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4309], device='cuda:0')), ('power', tensor([-21.2749], device='cuda:0'))])
epoch£º1233	 i:0 	 global-step:24660	 l-p:0.10837607085704803
epoch£º1233	 i:1 	 global-step:24661	 l-p:0.11715450137853622
epoch£º1233	 i:2 	 global-step:24662	 l-p:0.09644679725170135
epoch£º1233	 i:3 	 global-step:24663	 l-p:0.12827248871326447
epoch£º1233	 i:4 	 global-step:24664	 l-p:0.19053183495998383
epoch£º1233	 i:5 	 global-step:24665	 l-p:0.24495942890644073
epoch£º1233	 i:6 	 global-step:24666	 l-p:0.24401776492595673
epoch£º1233	 i:7 	 global-step:24667	 l-p:0.11654189974069595
epoch£º1233	 i:8 	 global-step:24668	 l-p:0.15528923273086548
epoch£º1233	 i:9 	 global-step:24669	 l-p:0.1019740104675293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1523, 5.1506, 5.1522],
        [5.1523, 5.1522, 5.1523],
        [5.1523, 5.1523, 5.1523],
        [5.1523, 4.8784, 4.8296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1234, step:0 
model_pd.l_p.mean(): 0.1265832930803299 
model_pd.l_d.mean(): -20.603099822998047 
model_pd.lagr.mean(): -20.476516723632812 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4023], device='cuda:0')), ('power', tensor([-21.4050], device='cuda:0'))])
epoch£º1234	 i:0 	 global-step:24680	 l-p:0.1265832930803299
epoch£º1234	 i:1 	 global-step:24681	 l-p:0.1157861277461052
epoch£º1234	 i:2 	 global-step:24682	 l-p:0.1193636879324913
epoch£º1234	 i:3 	 global-step:24683	 l-p:0.15594734251499176
epoch£º1234	 i:4 	 global-step:24684	 l-p:0.10856102406978607
epoch£º1234	 i:5 	 global-step:24685	 l-p:0.24869422614574432
epoch£º1234	 i:6 	 global-step:24686	 l-p:0.3160493075847626
epoch£º1234	 i:7 	 global-step:24687	 l-p:0.10928141325712204
epoch£º1234	 i:8 	 global-step:24688	 l-p:0.10459791868925095
epoch£º1234	 i:9 	 global-step:24689	 l-p:0.12111827731132507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1480, 5.1037, 5.1384],
        [5.1480, 5.1448, 5.1478],
        [5.1480, 5.1122, 5.1414],
        [5.1480, 5.1395, 5.1474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1235, step:0 
model_pd.l_p.mean(): 0.13880065083503723 
model_pd.l_d.mean(): -20.54788589477539 
model_pd.lagr.mean(): -20.40908432006836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4159], device='cuda:0')), ('power', tensor([-21.3628], device='cuda:0'))])
epoch£º1235	 i:0 	 global-step:24700	 l-p:0.13880065083503723
epoch£º1235	 i:1 	 global-step:24701	 l-p:0.18491972982883453
epoch£º1235	 i:2 	 global-step:24702	 l-p:0.20701144635677338
epoch£º1235	 i:3 	 global-step:24703	 l-p:0.25257328152656555
epoch£º1235	 i:4 	 global-step:24704	 l-p:0.10800641775131226
epoch£º1235	 i:5 	 global-step:24705	 l-p:0.11836821585893631
epoch£º1235	 i:6 	 global-step:24706	 l-p:0.09185293316841125
epoch£º1235	 i:7 	 global-step:24707	 l-p:0.09526266902685165
epoch£º1235	 i:8 	 global-step:24708	 l-p:0.13148514926433563
epoch£º1235	 i:9 	 global-step:24709	 l-p:0.22307445108890533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1459, 4.8687, 4.8142],
        [5.1459, 4.9726, 5.0274],
        [5.1459, 4.9788, 5.0358],
        [5.1459, 5.1459, 5.1459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1236, step:0 
model_pd.l_p.mean(): 0.0900953933596611 
model_pd.l_d.mean(): -20.156044006347656 
model_pd.lagr.mean(): -20.065948486328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4961], device='cuda:0')), ('power', tensor([-21.0468], device='cuda:0'))])
epoch£º1236	 i:0 	 global-step:24720	 l-p:0.0900953933596611
epoch£º1236	 i:1 	 global-step:24721	 l-p:0.13450482487678528
epoch£º1236	 i:2 	 global-step:24722	 l-p:0.19815029203891754
epoch£º1236	 i:3 	 global-step:24723	 l-p:0.12260417640209198
epoch£º1236	 i:4 	 global-step:24724	 l-p:0.1680402308702469
epoch£º1236	 i:5 	 global-step:24725	 l-p:0.26840972900390625
epoch£º1236	 i:6 	 global-step:24726	 l-p:0.1390381157398224
epoch£º1236	 i:7 	 global-step:24727	 l-p:0.13606975972652435
epoch£º1236	 i:8 	 global-step:24728	 l-p:0.19324098527431488
epoch£º1236	 i:9 	 global-step:24729	 l-p:0.10521182417869568
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1499, 5.1499, 5.1499],
        [5.1499, 5.1499, 5.1499],
        [5.1499, 5.1494, 5.1499],
        [5.1499, 5.1036, 5.1396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1237, step:0 
model_pd.l_p.mean(): 0.12148790061473846 
model_pd.l_d.mean(): -19.882802963256836 
model_pd.lagr.mean(): -19.761314392089844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4598], device='cuda:0')), ('power', tensor([-20.7309], device='cuda:0'))])
epoch£º1237	 i:0 	 global-step:24740	 l-p:0.12148790061473846
epoch£º1237	 i:1 	 global-step:24741	 l-p:0.23211611807346344
epoch£º1237	 i:2 	 global-step:24742	 l-p:0.13423432409763336
epoch£º1237	 i:3 	 global-step:24743	 l-p:0.10142583400011063
epoch£º1237	 i:4 	 global-step:24744	 l-p:0.1430174708366394
epoch£º1237	 i:5 	 global-step:24745	 l-p:0.1642182171344757
epoch£º1237	 i:6 	 global-step:24746	 l-p:0.14677169919013977
epoch£º1237	 i:7 	 global-step:24747	 l-p:0.09459703415632248
epoch£º1237	 i:8 	 global-step:24748	 l-p:0.1034889742732048
epoch£º1237	 i:9 	 global-step:24749	 l-p:0.2752135694026947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1523, 4.8451, 4.6637],
        [5.1523, 4.8784, 4.8297],
        [5.1523, 5.0871, 5.1334],
        [5.1523, 5.1059, 5.1420]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1238, step:0 
model_pd.l_p.mean(): 0.12898069620132446 
model_pd.l_d.mean(): -20.93917465209961 
model_pd.lagr.mean(): -20.81019401550293 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3587], device='cuda:0')), ('power', tensor([-21.7023], device='cuda:0'))])
epoch£º1238	 i:0 	 global-step:24760	 l-p:0.12898069620132446
epoch£º1238	 i:1 	 global-step:24761	 l-p:0.10679306834936142
epoch£º1238	 i:2 	 global-step:24762	 l-p:0.14986610412597656
epoch£º1238	 i:3 	 global-step:24763	 l-p:0.15198786556720734
epoch£º1238	 i:4 	 global-step:24764	 l-p:0.1537749469280243
epoch£º1238	 i:5 	 global-step:24765	 l-p:0.12874561548233032
epoch£º1238	 i:6 	 global-step:24766	 l-p:0.28107672929763794
epoch£º1238	 i:7 	 global-step:24767	 l-p:0.13822299242019653
epoch£º1238	 i:8 	 global-step:24768	 l-p:0.13736744225025177
epoch£º1238	 i:9 	 global-step:24769	 l-p:0.11580894142389297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1579, 5.1574, 5.1579],
        [5.1579, 5.1500, 5.1574],
        [5.1579, 5.2620, 4.9958],
        [5.1579, 5.1445, 5.1567]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1239, step:0 
model_pd.l_p.mean(): 0.11864912509918213 
model_pd.l_d.mean(): -20.802696228027344 
model_pd.lagr.mean(): -20.68404769897461 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3889], device='cuda:0')), ('power', tensor([-21.5944], device='cuda:0'))])
epoch£º1239	 i:0 	 global-step:24780	 l-p:0.11864912509918213
epoch£º1239	 i:1 	 global-step:24781	 l-p:0.13734260201454163
epoch£º1239	 i:2 	 global-step:24782	 l-p:0.12361495941877365
epoch£º1239	 i:3 	 global-step:24783	 l-p:0.12914296984672546
epoch£º1239	 i:4 	 global-step:24784	 l-p:0.10743489116430283
epoch£º1239	 i:5 	 global-step:24785	 l-p:0.3133469820022583
epoch£º1239	 i:6 	 global-step:24786	 l-p:0.11789482086896896
epoch£º1239	 i:7 	 global-step:24787	 l-p:0.18189628422260284
epoch£º1239	 i:8 	 global-step:24788	 l-p:0.12391140311956406
epoch£º1239	 i:9 	 global-step:24789	 l-p:0.1272187978029251
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1557, 5.5379, 5.4347],
        [5.1557, 5.4326, 5.2629],
        [5.1557, 5.0332, 5.0953],
        [5.1557, 4.9306, 4.5905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1240, step:0 
model_pd.l_p.mean(): 0.12111535668373108 
model_pd.l_d.mean(): -19.972835540771484 
model_pd.lagr.mean(): -19.851720809936523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4967], device='cuda:0')), ('power', tensor([-20.8607], device='cuda:0'))])
epoch£º1240	 i:0 	 global-step:24800	 l-p:0.12111535668373108
epoch£º1240	 i:1 	 global-step:24801	 l-p:0.21487566828727722
epoch£º1240	 i:2 	 global-step:24802	 l-p:0.17797642946243286
epoch£º1240	 i:3 	 global-step:24803	 l-p:0.13149693608283997
epoch£º1240	 i:4 	 global-step:24804	 l-p:0.1370229572057724
epoch£º1240	 i:5 	 global-step:24805	 l-p:0.11496219038963318
epoch£º1240	 i:6 	 global-step:24806	 l-p:0.1292375773191452
epoch£º1240	 i:7 	 global-step:24807	 l-p:0.1614966094493866
epoch£º1240	 i:8 	 global-step:24808	 l-p:0.15468963980674744
epoch£º1240	 i:9 	 global-step:24809	 l-p:0.13908402621746063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1577, 5.0282, 5.0905],
        [5.1577, 4.8655, 4.7693],
        [5.1577, 4.9471, 4.9806],
        [5.1577, 5.0888, 5.1368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1241, step:0 
model_pd.l_p.mean(): 0.13720740377902985 
model_pd.l_d.mean(): -18.45304298400879 
model_pd.lagr.mean(): -18.31583595275879 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5986], device='cuda:0')), ('power', tensor([-19.4181], device='cuda:0'))])
epoch£º1241	 i:0 	 global-step:24820	 l-p:0.13720740377902985
epoch£º1241	 i:1 	 global-step:24821	 l-p:0.16102807223796844
epoch£º1241	 i:2 	 global-step:24822	 l-p:0.24350352585315704
epoch£º1241	 i:3 	 global-step:24823	 l-p:0.12827754020690918
epoch£º1241	 i:4 	 global-step:24824	 l-p:0.11591146141290665
epoch£º1241	 i:5 	 global-step:24825	 l-p:0.1846018135547638
epoch£º1241	 i:6 	 global-step:24826	 l-p:0.12324025481939316
epoch£º1241	 i:7 	 global-step:24827	 l-p:0.09315232932567596
epoch£º1241	 i:8 	 global-step:24828	 l-p:0.10317663103342056
epoch£º1241	 i:9 	 global-step:24829	 l-p:0.1802467554807663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1589, 5.0332, 4.6841],
        [5.1589, 5.1470, 4.8290],
        [5.1589, 5.1589, 5.1589],
        [5.1589, 5.1545, 5.1587]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1242, step:0 
model_pd.l_p.mean(): 0.17648100852966309 
model_pd.l_d.mean(): -19.990507125854492 
model_pd.lagr.mean(): -19.81402587890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5088], device='cuda:0')), ('power', tensor([-20.8913], device='cuda:0'))])
epoch£º1242	 i:0 	 global-step:24840	 l-p:0.17648100852966309
epoch£º1242	 i:1 	 global-step:24841	 l-p:0.1116628348827362
epoch£º1242	 i:2 	 global-step:24842	 l-p:0.14549601078033447
epoch£º1242	 i:3 	 global-step:24843	 l-p:0.09305372089147568
epoch£º1242	 i:4 	 global-step:24844	 l-p:0.14085179567337036
epoch£º1242	 i:5 	 global-step:24845	 l-p:0.17559830844402313
epoch£º1242	 i:6 	 global-step:24846	 l-p:0.2016657590866089
epoch£º1242	 i:7 	 global-step:24847	 l-p:0.15300951898097992
epoch£º1242	 i:8 	 global-step:24848	 l-p:0.11496537178754807
epoch£º1242	 i:9 	 global-step:24849	 l-p:0.14530158042907715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1635, 5.1631, 5.1635],
        [5.1635, 5.0812, 5.1346],
        [5.1635, 5.0320, 5.0942],
        [5.1635, 4.8661, 4.7485]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1243, step:0 
model_pd.l_p.mean(): 0.17196348309516907 
model_pd.l_d.mean(): -20.18646812438965 
model_pd.lagr.mean(): -20.01450538635254 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4749], device='cuda:0')), ('power', tensor([-21.0558], device='cuda:0'))])
epoch£º1243	 i:0 	 global-step:24860	 l-p:0.17196348309516907
epoch£º1243	 i:1 	 global-step:24861	 l-p:0.13892298936843872
epoch£º1243	 i:2 	 global-step:24862	 l-p:0.12584814429283142
epoch£º1243	 i:3 	 global-step:24863	 l-p:0.12242294102907181
epoch£º1243	 i:4 	 global-step:24864	 l-p:0.16791503131389618
epoch£º1243	 i:5 	 global-step:24865	 l-p:0.12611904740333557
epoch£º1243	 i:6 	 global-step:24866	 l-p:0.1341017186641693
epoch£º1243	 i:7 	 global-step:24867	 l-p:0.15324944257736206
epoch£º1243	 i:8 	 global-step:24868	 l-p:0.13087518513202667
epoch£º1243	 i:9 	 global-step:24869	 l-p:0.16196613013744354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1681, 5.0960, 5.1454],
        [5.1681, 5.0154, 5.0755],
        [5.1681, 4.9341, 4.9447],
        [5.1681, 5.1681, 5.1681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1244, step:0 
model_pd.l_p.mean(): 0.17546409368515015 
model_pd.l_d.mean(): -20.149919509887695 
model_pd.lagr.mean(): -19.974454879760742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4280], device='cuda:0')), ('power', tensor([-20.9700], device='cuda:0'))])
epoch£º1244	 i:0 	 global-step:24880	 l-p:0.17546409368515015
epoch£º1244	 i:1 	 global-step:24881	 l-p:0.13697761297225952
epoch£º1244	 i:2 	 global-step:24882	 l-p:0.232695072889328
epoch£º1244	 i:3 	 global-step:24883	 l-p:0.1335742473602295
epoch£º1244	 i:4 	 global-step:24884	 l-p:0.1206921711564064
epoch£º1244	 i:5 	 global-step:24885	 l-p:0.1149986982345581
epoch£º1244	 i:6 	 global-step:24886	 l-p:0.13604484498500824
epoch£º1244	 i:7 	 global-step:24887	 l-p:0.14216077327728271
epoch£º1244	 i:8 	 global-step:24888	 l-p:0.07533151656389236
epoch£º1244	 i:9 	 global-step:24889	 l-p:0.13901033997535706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1245
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6075,  0.5145,  1.0000,  0.4357,
          1.0000,  0.8469, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[5.1738, 5.0450, 4.6963],
        [5.1738, 5.0177, 4.6673],
        [5.1738, 5.2203, 4.9266],
        [5.1738, 4.9146, 4.5984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1245, step:0 
model_pd.l_p.mean(): 0.1608409285545349 
model_pd.l_d.mean(): -20.423267364501953 
model_pd.lagr.mean(): -20.262426376342773 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4596], device='cuda:0')), ('power', tensor([-21.2811], device='cuda:0'))])
epoch£º1245	 i:0 	 global-step:24900	 l-p:0.1608409285545349
epoch£º1245	 i:1 	 global-step:24901	 l-p:0.22018949687480927
epoch£º1245	 i:2 	 global-step:24902	 l-p:0.11366283148527145
epoch£º1245	 i:3 	 global-step:24903	 l-p:0.06031080707907677
epoch£º1245	 i:4 	 global-step:24904	 l-p:0.16225486993789673
epoch£º1245	 i:5 	 global-step:24905	 l-p:0.1634640395641327
epoch£º1245	 i:6 	 global-step:24906	 l-p:0.12809857726097107
epoch£º1245	 i:7 	 global-step:24907	 l-p:0.10184057801961899
epoch£º1245	 i:8 	 global-step:24908	 l-p:0.15710990130901337
epoch£º1245	 i:9 	 global-step:24909	 l-p:0.12346984446048737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1725, 4.9129, 4.5967],
        [5.1725, 5.1725, 5.1725],
        [5.1725, 5.1281, 5.1629],
        [5.1725, 5.1725, 5.1725]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1246, step:0 
model_pd.l_p.mean(): 0.2105567306280136 
model_pd.l_d.mean(): -19.977500915527344 
model_pd.lagr.mean(): -19.766944885253906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5277], device='cuda:0')), ('power', tensor([-20.8976], device='cuda:0'))])
epoch£º1246	 i:0 	 global-step:24920	 l-p:0.2105567306280136
epoch£º1246	 i:1 	 global-step:24921	 l-p:0.11540275067090988
epoch£º1246	 i:2 	 global-step:24922	 l-p:0.14399009943008423
epoch£º1246	 i:3 	 global-step:24923	 l-p:0.13394713401794434
epoch£º1246	 i:4 	 global-step:24924	 l-p:0.13182635605335236
epoch£º1246	 i:5 	 global-step:24925	 l-p:0.11470277607440948
epoch£º1246	 i:6 	 global-step:24926	 l-p:0.11724425107240677
epoch£º1246	 i:7 	 global-step:24927	 l-p:0.11764752864837646
epoch£º1246	 i:8 	 global-step:24928	 l-p:0.1513271927833557
epoch£º1246	 i:9 	 global-step:24929	 l-p:0.17341746389865875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1666, 4.8633, 4.7131],
        [5.1666, 5.1666, 5.1666],
        [5.1666, 4.9912, 5.0448],
        [5.1666, 5.2730, 5.0077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1247, step:0 
model_pd.l_p.mean(): 0.14935141801834106 
model_pd.l_d.mean(): -19.906545639038086 
model_pd.lagr.mean(): -19.75719451904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4846], device='cuda:0')), ('power', tensor([-20.7807], device='cuda:0'))])
epoch£º1247	 i:0 	 global-step:24940	 l-p:0.14935141801834106
epoch£º1247	 i:1 	 global-step:24941	 l-p:0.11855898797512054
epoch£º1247	 i:2 	 global-step:24942	 l-p:0.10732604563236237
epoch£º1247	 i:3 	 global-step:24943	 l-p:0.14890146255493164
epoch£º1247	 i:4 	 global-step:24944	 l-p:0.15691949427127838
epoch£º1247	 i:5 	 global-step:24945	 l-p:0.12126170843839645
epoch£º1247	 i:6 	 global-step:24946	 l-p:0.1677008420228958
epoch£º1247	 i:7 	 global-step:24947	 l-p:0.13786812126636505
epoch£º1247	 i:8 	 global-step:24948	 l-p:0.14344041049480438
epoch£º1247	 i:9 	 global-step:24949	 l-p:0.1749918907880783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1668, 5.0747, 5.1313],
        [5.1668, 5.1668, 5.1668],
        [5.1668, 4.8797, 4.7971],
        [5.1668, 4.8615, 4.6932]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1248, step:0 
model_pd.l_p.mean(): 0.13388265669345856 
model_pd.l_d.mean(): -20.86631965637207 
model_pd.lagr.mean(): -20.732437133789062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3787], device='cuda:0')), ('power', tensor([-21.6487], device='cuda:0'))])
epoch£º1248	 i:0 	 global-step:24960	 l-p:0.13388265669345856
epoch£º1248	 i:1 	 global-step:24961	 l-p:0.16944144666194916
epoch£º1248	 i:2 	 global-step:24962	 l-p:0.12754929065704346
epoch£º1248	 i:3 	 global-step:24963	 l-p:0.1329699158668518
epoch£º1248	 i:4 	 global-step:24964	 l-p:0.20841114223003387
epoch£º1248	 i:5 	 global-step:24965	 l-p:0.16990961134433746
epoch£º1248	 i:6 	 global-step:24966	 l-p:0.1322799026966095
epoch£º1248	 i:7 	 global-step:24967	 l-p:0.1547454297542572
epoch£º1248	 i:8 	 global-step:24968	 l-p:0.07777687162160873
epoch£º1248	 i:9 	 global-step:24969	 l-p:0.11970249563455582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1675, 4.8629, 4.6449],
        [5.1675, 5.1675, 5.1675],
        [5.1675, 4.8841, 4.5966],
        [5.1675, 5.1675, 5.1675]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1249, step:0 
model_pd.l_p.mean(): 0.1750454604625702 
model_pd.l_d.mean(): -20.1591854095459 
model_pd.lagr.mean(): -19.984140396118164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4837], device='cuda:0')), ('power', tensor([-21.0372], device='cuda:0'))])
epoch£º1249	 i:0 	 global-step:24980	 l-p:0.1750454604625702
epoch£º1249	 i:1 	 global-step:24981	 l-p:0.08808989077806473
epoch£º1249	 i:2 	 global-step:24982	 l-p:0.18033580482006073
epoch£º1249	 i:3 	 global-step:24983	 l-p:0.162104532122612
epoch£º1249	 i:4 	 global-step:24984	 l-p:0.10071305930614471
epoch£º1249	 i:5 	 global-step:24985	 l-p:0.17345096170902252
epoch£º1249	 i:6 	 global-step:24986	 l-p:0.12284263223409653
epoch£º1249	 i:7 	 global-step:24987	 l-p:0.16370320320129395
epoch£º1249	 i:8 	 global-step:24988	 l-p:0.10902193933725357
epoch£º1249	 i:9 	 global-step:24989	 l-p:0.1448122262954712
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1250
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1828,  0.1038,  1.0000,  0.0589,
          1.0000,  0.5675, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1353,  0.0695,  1.0000,  0.0357,
          1.0000,  0.5134, 31.6228]], device='cuda:0')
 pt:tensor([[5.1694, 4.9593, 4.9926],
        [5.1694, 4.9348, 4.9449],
        [5.1694, 5.0136, 5.0730],
        [5.1694, 5.0205, 5.0812]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1250, step:0 
model_pd.l_p.mean(): 0.13292908668518066 
model_pd.l_d.mean(): -20.23079490661621 
model_pd.lagr.mean(): -20.09786605834961 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4501], device='cuda:0')), ('power', tensor([-21.0753], device='cuda:0'))])
epoch£º1250	 i:0 	 global-step:25000	 l-p:0.13292908668518066
epoch£º1250	 i:1 	 global-step:25001	 l-p:0.17154693603515625
epoch£º1250	 i:2 	 global-step:25002	 l-p:0.13567347824573517
epoch£º1250	 i:3 	 global-step:25003	 l-p:0.12792059779167175
epoch£º1250	 i:4 	 global-step:25004	 l-p:0.12675011157989502
epoch£º1250	 i:5 	 global-step:25005	 l-p:0.2214692085981369
epoch£º1250	 i:6 	 global-step:25006	 l-p:0.07518250495195389
epoch£º1250	 i:7 	 global-step:25007	 l-p:0.12209292501211166
epoch£º1250	 i:8 	 global-step:25008	 l-p:0.11054793000221252
epoch£º1250	 i:9 	 global-step:25009	 l-p:0.1981961578130722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1672, 4.8801, 4.7973],
        [5.1672, 5.1663, 5.1672],
        [5.1672, 5.1672, 5.1672],
        [5.1672, 4.9569, 4.9903]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1251, step:0 
model_pd.l_p.mean(): 0.09463731199502945 
model_pd.l_d.mean(): -19.931697845458984 
model_pd.lagr.mean(): -19.837060928344727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4957], device='cuda:0')), ('power', tensor([-20.8178], device='cuda:0'))])
epoch£º1251	 i:0 	 global-step:25020	 l-p:0.09463731199502945
epoch£º1251	 i:1 	 global-step:25021	 l-p:0.13253726065158844
epoch£º1251	 i:2 	 global-step:25022	 l-p:0.18722201883792877
epoch£º1251	 i:3 	 global-step:25023	 l-p:0.13181263208389282
epoch£º1251	 i:4 	 global-step:25024	 l-p:0.17659319937229156
epoch£º1251	 i:5 	 global-step:25025	 l-p:0.11355891078710556
epoch£º1251	 i:6 	 global-step:25026	 l-p:0.1128510907292366
epoch£º1251	 i:7 	 global-step:25027	 l-p:0.22770734131336212
epoch£º1251	 i:8 	 global-step:25028	 l-p:0.121095672249794
epoch£º1251	 i:9 	 global-step:25029	 l-p:0.12486881017684937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1682, 5.0192, 5.0800],
        [5.1682, 4.9843, 4.6348],
        [5.1682, 5.1682, 5.1682],
        [5.1682, 4.8887, 4.5949]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1252, step:0 
model_pd.l_p.mean(): 0.15882277488708496 
model_pd.l_d.mean(): -19.93889617919922 
model_pd.lagr.mean(): -19.780073165893555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5067], device='cuda:0')), ('power', tensor([-20.8366], device='cuda:0'))])
epoch£º1252	 i:0 	 global-step:25040	 l-p:0.15882277488708496
epoch£º1252	 i:1 	 global-step:25041	 l-p:0.20548665523529053
epoch£º1252	 i:2 	 global-step:25042	 l-p:0.13829205930233002
epoch£º1252	 i:3 	 global-step:25043	 l-p:0.12341111153364182
epoch£º1252	 i:4 	 global-step:25044	 l-p:0.11040516942739487
epoch£º1252	 i:5 	 global-step:25045	 l-p:0.14735595881938934
epoch£º1252	 i:6 	 global-step:25046	 l-p:0.09952569007873535
epoch£º1252	 i:7 	 global-step:25047	 l-p:0.16506046056747437
epoch£º1252	 i:8 	 global-step:25048	 l-p:0.14260762929916382
epoch£º1252	 i:9 	 global-step:25049	 l-p:0.12184362858533859
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1701, 5.1701, 5.1701],
        [5.1701, 5.1695, 5.1700],
        [5.1701, 5.1701, 5.1701],
        [5.1701, 5.1701, 5.1701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1253, step:0 
model_pd.l_p.mean(): 0.12480044364929199 
model_pd.l_d.mean(): -20.054912567138672 
model_pd.lagr.mean(): -19.930112838745117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4296], device='cuda:0')), ('power', tensor([-20.8749], device='cuda:0'))])
epoch£º1253	 i:0 	 global-step:25060	 l-p:0.12480044364929199
epoch£º1253	 i:1 	 global-step:25061	 l-p:0.1346314251422882
epoch£º1253	 i:2 	 global-step:25062	 l-p:0.15098537504673004
epoch£º1253	 i:3 	 global-step:25063	 l-p:0.16351598501205444
epoch£º1253	 i:4 	 global-step:25064	 l-p:0.16155429184436798
epoch£º1253	 i:5 	 global-step:25065	 l-p:0.1332952082157135
epoch£º1253	 i:6 	 global-step:25066	 l-p:0.09782199561595917
epoch£º1253	 i:7 	 global-step:25067	 l-p:0.09188573807477951
epoch£º1253	 i:8 	 global-step:25068	 l-p:0.11325886845588684
epoch£º1253	 i:9 	 global-step:25069	 l-p:0.25282564759254456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1661, 5.4192, 5.2346],
        [5.1661, 4.9907, 5.0443],
        [5.1661, 5.1630, 5.1660],
        [5.1661, 4.8609, 4.6952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1254, step:0 
model_pd.l_p.mean(): 0.12304162979125977 
model_pd.l_d.mean(): -19.149890899658203 
model_pd.lagr.mean(): -19.0268497467041 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5403], device='cuda:0')), ('power', tensor([-20.0676], device='cuda:0'))])
epoch£º1254	 i:0 	 global-step:25080	 l-p:0.12304162979125977
epoch£º1254	 i:1 	 global-step:25081	 l-p:0.10728609561920166
epoch£º1254	 i:2 	 global-step:25082	 l-p:0.111236073076725
epoch£º1254	 i:3 	 global-step:25083	 l-p:0.11540709435939789
epoch£º1254	 i:4 	 global-step:25084	 l-p:0.14037860929965973
epoch£º1254	 i:5 	 global-step:25085	 l-p:0.1223190501332283
epoch£º1254	 i:6 	 global-step:25086	 l-p:0.25644102692604065
epoch£º1254	 i:7 	 global-step:25087	 l-p:0.12167831510305405
epoch£º1254	 i:8 	 global-step:25088	 l-p:0.21186648309230804
epoch£º1254	 i:9 	 global-step:25089	 l-p:0.1309136301279068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1634, 4.9800, 5.0303],
        [5.1634, 4.8619, 4.7260],
        [5.1634, 5.0292, 5.0914],
        [5.1634, 5.1634, 5.1634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1255, step:0 
model_pd.l_p.mean(): 0.11453823000192642 
model_pd.l_d.mean(): -19.064489364624023 
model_pd.lagr.mean(): -18.949951171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5650], device='cuda:0')), ('power', tensor([-20.0062], device='cuda:0'))])
epoch£º1255	 i:0 	 global-step:25100	 l-p:0.11453823000192642
epoch£º1255	 i:1 	 global-step:25101	 l-p:0.18154652416706085
epoch£º1255	 i:2 	 global-step:25102	 l-p:0.11942905187606812
epoch£º1255	 i:3 	 global-step:25103	 l-p:0.1759667992591858
epoch£º1255	 i:4 	 global-step:25104	 l-p:0.11675156652927399
epoch£º1255	 i:5 	 global-step:25105	 l-p:0.20282775163650513
epoch£º1255	 i:6 	 global-step:25106	 l-p:0.160687655210495
epoch£º1255	 i:7 	 global-step:25107	 l-p:0.11864539980888367
epoch£º1255	 i:8 	 global-step:25108	 l-p:0.143101304769516
epoch£º1255	 i:9 	 global-step:25109	 l-p:0.10598476976156235
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1655, 5.0623, 5.1217],
        [5.1655, 4.8848, 4.8194],
        [5.1655, 5.4366, 5.2629],
        [5.1655, 5.1568, 5.1649]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1256, step:0 
model_pd.l_p.mean(): 0.08615697920322418 
model_pd.l_d.mean(): -20.3934326171875 
model_pd.lagr.mean(): -20.307275772094727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4413], device='cuda:0')), ('power', tensor([-21.2318], device='cuda:0'))])
epoch£º1256	 i:0 	 global-step:25120	 l-p:0.08615697920322418
epoch£º1256	 i:1 	 global-step:25121	 l-p:0.14801301062107086
epoch£º1256	 i:2 	 global-step:25122	 l-p:0.1660057008266449
epoch£º1256	 i:3 	 global-step:25123	 l-p:0.17808757722377777
epoch£º1256	 i:4 	 global-step:25124	 l-p:0.17157308757305145
epoch£º1256	 i:5 	 global-step:25125	 l-p:0.15164324641227722
epoch£º1256	 i:6 	 global-step:25126	 l-p:0.1017412319779396
epoch£º1256	 i:7 	 global-step:25127	 l-p:0.1318327933549881
epoch£º1256	 i:8 	 global-step:25128	 l-p:0.16187620162963867
epoch£º1256	 i:9 	 global-step:25129	 l-p:0.14084051549434662
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1636, 4.8658, 4.7482],
        [5.1636, 5.0404, 4.6916],
        [5.1636, 5.1632, 5.1636],
        [5.1636, 5.1636, 5.1636]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1257, step:0 
model_pd.l_p.mean(): 0.1156509593129158 
model_pd.l_d.mean(): -20.26542091369629 
model_pd.lagr.mean(): -20.149770736694336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4717], device='cuda:0')), ('power', tensor([-21.1329], device='cuda:0'))])
epoch£º1257	 i:0 	 global-step:25140	 l-p:0.1156509593129158
epoch£º1257	 i:1 	 global-step:25141	 l-p:0.214726984500885
epoch£º1257	 i:2 	 global-step:25142	 l-p:0.11820199340581894
epoch£º1257	 i:3 	 global-step:25143	 l-p:0.2256590723991394
epoch£º1257	 i:4 	 global-step:25144	 l-p:0.10208982229232788
epoch£º1257	 i:5 	 global-step:25145	 l-p:0.11136170476675034
epoch£º1257	 i:6 	 global-step:25146	 l-p:0.14788052439689636
epoch£º1257	 i:7 	 global-step:25147	 l-p:0.10594645887613297
epoch£º1257	 i:8 	 global-step:25148	 l-p:0.14980721473693848
epoch£º1257	 i:9 	 global-step:25149	 l-p:0.15512821078300476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1621, 5.1617, 5.1621],
        [5.1621, 5.1621, 5.1621],
        [5.1621, 4.8996, 4.8717],
        [5.1621, 4.8707, 4.5967]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1258, step:0 
model_pd.l_p.mean(): 0.13514064252376556 
model_pd.l_d.mean(): -20.670923233032227 
model_pd.lagr.mean(): -20.535781860351562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4160], device='cuda:0')), ('power', tensor([-21.4884], device='cuda:0'))])
epoch£º1258	 i:0 	 global-step:25160	 l-p:0.13514064252376556
epoch£º1258	 i:1 	 global-step:25161	 l-p:0.18583305180072784
epoch£º1258	 i:2 	 global-step:25162	 l-p:0.197883740067482
epoch£º1258	 i:3 	 global-step:25163	 l-p:0.12760870158672333
epoch£º1258	 i:4 	 global-step:25164	 l-p:0.12113950401544571
epoch£º1258	 i:5 	 global-step:25165	 l-p:0.22736787796020508
epoch£º1258	 i:6 	 global-step:25166	 l-p:0.06537745893001556
epoch£º1258	 i:7 	 global-step:25167	 l-p:0.12197158485651016
epoch£º1258	 i:8 	 global-step:25168	 l-p:0.14441604912281036
epoch£º1258	 i:9 	 global-step:25169	 l-p:0.12242794036865234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1655, 5.1610, 5.1653],
        [5.1655, 4.8845, 4.8188],
        [5.1655, 5.1612, 5.1653],
        [5.1655, 4.9815, 4.6315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1259, step:0 
model_pd.l_p.mean(): 0.14392443001270294 
model_pd.l_d.mean(): -19.8468074798584 
model_pd.lagr.mean(): -19.702882766723633 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5142], device='cuda:0')), ('power', tensor([-20.7505], device='cuda:0'))])
epoch£º1259	 i:0 	 global-step:25180	 l-p:0.14392443001270294
epoch£º1259	 i:1 	 global-step:25181	 l-p:0.16332878172397614
epoch£º1259	 i:2 	 global-step:25182	 l-p:0.13417184352874756
epoch£º1259	 i:3 	 global-step:25183	 l-p:0.12065520137548447
epoch£º1259	 i:4 	 global-step:25184	 l-p:0.07807929068803787
epoch£º1259	 i:5 	 global-step:25185	 l-p:0.1749284714460373
epoch£º1259	 i:6 	 global-step:25186	 l-p:0.13057726621627808
epoch£º1259	 i:7 	 global-step:25187	 l-p:0.13760215044021606
epoch£º1259	 i:8 	 global-step:25188	 l-p:0.19805768132209778
epoch£º1259	 i:9 	 global-step:25189	 l-p:0.14467070996761322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1683, 5.1639, 5.1681],
        [5.1683, 4.9009, 4.8634],
        [5.1683, 5.1683, 5.1683],
        [5.1683, 5.0811, 5.1362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1260, step:0 
model_pd.l_p.mean(): 0.16390357911586761 
model_pd.l_d.mean(): -20.335477828979492 
model_pd.lagr.mean(): -20.171573638916016 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4588], device='cuda:0')), ('power', tensor([-21.1909], device='cuda:0'))])
epoch£º1260	 i:0 	 global-step:25200	 l-p:0.16390357911586761
epoch£º1260	 i:1 	 global-step:25201	 l-p:0.1906019151210785
epoch£º1260	 i:2 	 global-step:25202	 l-p:0.15932893753051758
epoch£º1260	 i:3 	 global-step:25203	 l-p:0.11736255139112473
epoch£º1260	 i:4 	 global-step:25204	 l-p:0.16555564105510712
epoch£º1260	 i:5 	 global-step:25205	 l-p:0.12874466180801392
epoch£º1260	 i:6 	 global-step:25206	 l-p:0.10160484164953232
epoch£º1260	 i:7 	 global-step:25207	 l-p:0.18724390864372253
epoch£º1260	 i:8 	 global-step:25208	 l-p:0.06289530545473099
epoch£º1260	 i:9 	 global-step:25209	 l-p:0.13426609337329865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1726, 5.1726, 5.1726],
        [5.1726, 5.1725, 5.1726],
        [5.1726, 5.1555, 5.1707],
        [5.1726, 5.1677, 5.1724]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1261, step:0 
model_pd.l_p.mean(): 0.17052587866783142 
model_pd.l_d.mean(): -20.654857635498047 
model_pd.lagr.mean(): -20.484331130981445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4136], device='cuda:0')), ('power', tensor([-21.4695], device='cuda:0'))])
epoch£º1261	 i:0 	 global-step:25220	 l-p:0.17052587866783142
epoch£º1261	 i:1 	 global-step:25221	 l-p:0.15986232459545135
epoch£º1261	 i:2 	 global-step:25222	 l-p:0.21411173045635223
epoch£º1261	 i:3 	 global-step:25223	 l-p:0.11541496217250824
epoch£º1261	 i:4 	 global-step:25224	 l-p:0.10342615097761154
epoch£º1261	 i:5 	 global-step:25225	 l-p:0.09550470858812332
epoch£º1261	 i:6 	 global-step:25226	 l-p:0.11434781551361084
epoch£º1261	 i:7 	 global-step:25227	 l-p:0.12546895444393158
epoch£º1261	 i:8 	 global-step:25228	 l-p:0.18069720268249512
epoch£º1261	 i:9 	 global-step:25229	 l-p:0.11409058421850204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1749, 5.1821, 4.8715],
        [5.1749, 5.1501, 5.1714],
        [5.1749, 5.1804, 4.8690],
        [5.1749, 5.0518, 4.7035]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1262, step:0 
model_pd.l_p.mean(): 0.18083640933036804 
model_pd.l_d.mean(): -19.471899032592773 
model_pd.lagr.mean(): -19.29106330871582 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4904], device='cuda:0')), ('power', tensor([-20.3439], device='cuda:0'))])
epoch£º1262	 i:0 	 global-step:25240	 l-p:0.18083640933036804
epoch£º1262	 i:1 	 global-step:25241	 l-p:0.1626560240983963
epoch£º1262	 i:2 	 global-step:25242	 l-p:0.16156141459941864
epoch£º1262	 i:3 	 global-step:25243	 l-p:0.17999911308288574
epoch£º1262	 i:4 	 global-step:25244	 l-p:0.11591268330812454
epoch£º1262	 i:5 	 global-step:25245	 l-p:0.11490458250045776
epoch£º1262	 i:6 	 global-step:25246	 l-p:0.14822527766227722
epoch£º1262	 i:7 	 global-step:25247	 l-p:0.08047179877758026
epoch£º1262	 i:8 	 global-step:25248	 l-p:0.10132873058319092
epoch£º1262	 i:9 	 global-step:25249	 l-p:0.14448703825473785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1752, 5.3018, 5.0465],
        [5.1752, 5.1261, 5.1638],
        [5.1752, 5.0776, 5.1357],
        [5.1752, 5.0108, 5.0681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1263, step:0 
model_pd.l_p.mean(): 0.0805787742137909 
model_pd.l_d.mean(): -18.820104598999023 
model_pd.lagr.mean(): -18.739526748657227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5324], device='cuda:0')), ('power', tensor([-19.7235], device='cuda:0'))])
epoch£º1263	 i:0 	 global-step:25260	 l-p:0.0805787742137909
epoch£º1263	 i:1 	 global-step:25261	 l-p:0.14017316699028015
epoch£º1263	 i:2 	 global-step:25262	 l-p:0.20928795635700226
epoch£º1263	 i:3 	 global-step:25263	 l-p:0.1169082522392273
epoch£º1263	 i:4 	 global-step:25264	 l-p:0.11454137414693832
epoch£º1263	 i:5 	 global-step:25265	 l-p:0.19661647081375122
epoch£º1263	 i:6 	 global-step:25266	 l-p:0.13827352225780487
epoch£º1263	 i:7 	 global-step:25267	 l-p:0.10485050827264786
epoch£º1263	 i:8 	 global-step:25268	 l-p:0.13002178072929382
epoch£º1263	 i:9 	 global-step:25269	 l-p:0.15689608454704285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1759, 4.8727, 4.6485],
        [5.1759, 4.9555, 4.9799],
        [5.1759, 5.1759, 5.1759],
        [5.1759, 5.1759, 5.1759]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1264, step:0 
model_pd.l_p.mean(): 0.1045924723148346 
model_pd.l_d.mean(): -19.043123245239258 
model_pd.lagr.mean(): -18.93852996826172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5209], device='cuda:0')), ('power', tensor([-19.9387], device='cuda:0'))])
epoch£º1264	 i:0 	 global-step:25280	 l-p:0.1045924723148346
epoch£º1264	 i:1 	 global-step:25281	 l-p:0.21506564319133759
epoch£º1264	 i:2 	 global-step:25282	 l-p:0.14333565533161163
epoch£º1264	 i:3 	 global-step:25283	 l-p:0.16272230446338654
epoch£º1264	 i:4 	 global-step:25284	 l-p:0.12430854886770248
epoch£º1264	 i:5 	 global-step:25285	 l-p:0.10377265512943268
epoch£º1264	 i:6 	 global-step:25286	 l-p:0.19205279648303986
epoch£º1264	 i:7 	 global-step:25287	 l-p:0.10209865123033524
epoch£º1264	 i:8 	 global-step:25288	 l-p:0.1330302655696869
epoch£º1264	 i:9 	 global-step:25289	 l-p:0.10775155574083328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1733, 5.1681, 5.1731],
        [5.1733, 4.9604, 4.9914],
        [5.1733, 5.1189, 5.1596],
        [5.1733, 5.1733, 5.1733]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1265, step:0 
model_pd.l_p.mean(): 0.10731080174446106 
model_pd.l_d.mean(): -19.249732971191406 
model_pd.lagr.mean(): -19.14242172241211 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5284], device='cuda:0')), ('power', tensor([-20.1570], device='cuda:0'))])
epoch£º1265	 i:0 	 global-step:25300	 l-p:0.10731080174446106
epoch£º1265	 i:1 	 global-step:25301	 l-p:0.13858246803283691
epoch£º1265	 i:2 	 global-step:25302	 l-p:0.1437046080827713
epoch£º1265	 i:3 	 global-step:25303	 l-p:0.15691043436527252
epoch£º1265	 i:4 	 global-step:25304	 l-p:0.12557289004325867
epoch£º1265	 i:5 	 global-step:25305	 l-p:0.11662773042917252
epoch£º1265	 i:6 	 global-step:25306	 l-p:0.21351797878742218
epoch£º1265	 i:7 	 global-step:25307	 l-p:0.1944018453359604
epoch£º1265	 i:8 	 global-step:25308	 l-p:0.11848817765712738
epoch£º1265	 i:9 	 global-step:25309	 l-p:0.10155034810304642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1266
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1995,  0.1166,  1.0000,  0.0681,
          1.0000,  0.5843, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2321,  0.1426,  1.0000,  0.0876,
          1.0000,  0.6145, 31.6228]], device='cuda:0')
 pt:tensor([[5.1683, 4.9393, 4.9559],
        [5.1683, 5.0122, 5.0718],
        [5.1683, 4.9558, 4.9876],
        [5.1683, 4.9086, 4.8849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1266, step:0 
model_pd.l_p.mean(): 0.20472536981105804 
model_pd.l_d.mean(): -19.706161499023438 
model_pd.lagr.mean(): -19.501436233520508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4744], device='cuda:0')), ('power', tensor([-20.5660], device='cuda:0'))])
epoch£º1266	 i:0 	 global-step:25320	 l-p:0.20472536981105804
epoch£º1266	 i:1 	 global-step:25321	 l-p:0.15816842019557953
epoch£º1266	 i:2 	 global-step:25322	 l-p:0.1310746669769287
epoch£º1266	 i:3 	 global-step:25323	 l-p:0.13886213302612305
epoch£º1266	 i:4 	 global-step:25324	 l-p:0.10680778324604034
epoch£º1266	 i:5 	 global-step:25325	 l-p:0.1938808709383011
epoch£º1266	 i:6 	 global-step:25326	 l-p:0.1040041521191597
epoch£º1266	 i:7 	 global-step:25327	 l-p:0.08967366069555283
epoch£º1266	 i:8 	 global-step:25328	 l-p:0.13016611337661743
epoch£º1266	 i:9 	 global-step:25329	 l-p:0.16364836692810059
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1678, 4.8918, 4.8372],
        [5.1678, 5.1672, 5.1678],
        [5.1678, 5.1613, 5.1674],
        [5.1678, 5.1678, 5.1678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1267, step:0 
model_pd.l_p.mean(): 0.1359974890947342 
model_pd.l_d.mean(): -20.640758514404297 
model_pd.lagr.mean(): -20.5047607421875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4231], device='cuda:0')), ('power', tensor([-21.4649], device='cuda:0'))])
epoch£º1267	 i:0 	 global-step:25340	 l-p:0.1359974890947342
epoch£º1267	 i:1 	 global-step:25341	 l-p:0.12298464775085449
epoch£º1267	 i:2 	 global-step:25342	 l-p:0.12531574070453644
epoch£º1267	 i:3 	 global-step:25343	 l-p:0.10034070909023285
epoch£º1267	 i:4 	 global-step:25344	 l-p:0.16795583069324493
epoch£º1267	 i:5 	 global-step:25345	 l-p:0.17881163954734802
epoch£º1267	 i:6 	 global-step:25346	 l-p:0.1557665765285492
epoch£º1267	 i:7 	 global-step:25347	 l-p:0.131927952170372
epoch£º1267	 i:8 	 global-step:25348	 l-p:0.1818145513534546
epoch£º1267	 i:9 	 global-step:25349	 l-p:0.13302280008792877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1655, 5.1637, 5.1654],
        [5.1655, 4.9651, 5.0058],
        [5.1655, 5.1655, 5.1655],
        [5.1655, 4.9072, 4.8861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1268, step:0 
model_pd.l_p.mean(): 0.1336963176727295 
model_pd.l_d.mean(): -19.399234771728516 
model_pd.lagr.mean(): -19.265539169311523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4697], device='cuda:0')), ('power', tensor([-20.2485], device='cuda:0'))])
epoch£º1268	 i:0 	 global-step:25360	 l-p:0.1336963176727295
epoch£º1268	 i:1 	 global-step:25361	 l-p:0.1265573352575302
epoch£º1268	 i:2 	 global-step:25362	 l-p:0.15482419729232788
epoch£º1268	 i:3 	 global-step:25363	 l-p:0.16897901892662048
epoch£º1268	 i:4 	 global-step:25364	 l-p:0.12989646196365356
epoch£º1268	 i:5 	 global-step:25365	 l-p:0.12770472466945648
epoch£º1268	 i:6 	 global-step:25366	 l-p:0.2000717669725418
epoch£º1268	 i:7 	 global-step:25367	 l-p:0.09042906761169434
epoch£º1268	 i:8 	 global-step:25368	 l-p:0.13276703655719757
epoch£º1268	 i:9 	 global-step:25369	 l-p:0.16639934480190277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1678, 4.9865, 5.0376],
        [5.1678, 5.1599, 5.1673],
        [5.1678, 5.1678, 5.1679],
        [5.1678, 5.1678, 5.1678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1269, step:0 
model_pd.l_p.mean(): 0.0697828009724617 
model_pd.l_d.mean(): -20.125579833984375 
model_pd.lagr.mean(): -20.055797576904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4682], device='cuda:0')), ('power', tensor([-20.9868], device='cuda:0'))])
epoch£º1269	 i:0 	 global-step:25380	 l-p:0.0697828009724617
epoch£º1269	 i:1 	 global-step:25381	 l-p:0.17062224447727203
epoch£º1269	 i:2 	 global-step:25382	 l-p:0.1280146688222885
epoch£º1269	 i:3 	 global-step:25383	 l-p:0.17634879052639008
epoch£º1269	 i:4 	 global-step:25384	 l-p:0.11656419187784195
epoch£º1269	 i:5 	 global-step:25385	 l-p:0.16504749655723572
epoch£º1269	 i:6 	 global-step:25386	 l-p:0.15732544660568237
epoch£º1269	 i:7 	 global-step:25387	 l-p:0.1434037685394287
epoch£º1269	 i:8 	 global-step:25388	 l-p:0.12831081449985504
epoch£º1269	 i:9 	 global-step:25389	 l-p:0.1529209464788437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1772, 5.2859, 5.0214],
        [5.1772, 5.1760, 5.1771],
        [5.1772, 4.8724, 4.6632],
        [5.1772, 4.8791, 4.6293]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1270, step:0 
model_pd.l_p.mean(): 0.12977829575538635 
model_pd.l_d.mean(): -20.332799911499023 
model_pd.lagr.mean(): -20.203022003173828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4519], device='cuda:0')), ('power', tensor([-21.1810], device='cuda:0'))])
epoch£º1270	 i:0 	 global-step:25400	 l-p:0.12977829575538635
epoch£º1270	 i:1 	 global-step:25401	 l-p:0.13805045187473297
epoch£º1270	 i:2 	 global-step:25402	 l-p:0.04800301417708397
epoch£º1270	 i:3 	 global-step:25403	 l-p:0.1880180835723877
epoch£º1270	 i:4 	 global-step:25404	 l-p:0.16116797924041748
epoch£º1270	 i:5 	 global-step:25405	 l-p:0.14657950401306152
epoch£º1270	 i:6 	 global-step:25406	 l-p:0.14938059449195862
epoch£º1270	 i:7 	 global-step:25407	 l-p:0.1412459909915924
epoch£º1270	 i:8 	 global-step:25408	 l-p:0.1445656716823578
epoch£º1270	 i:9 	 global-step:25409	 l-p:0.1238846629858017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1838, 5.1775, 4.8614],
        [5.1838, 4.9903, 4.6434],
        [5.1838, 5.1305, 5.1706],
        [5.1838, 4.8839, 4.7478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1271, step:0 
model_pd.l_p.mean(): 0.13148806989192963 
model_pd.l_d.mean(): -19.27590560913086 
model_pd.lagr.mean(): -19.14441680908203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5102], device='cuda:0')), ('power', tensor([-20.1648], device='cuda:0'))])
epoch£º1271	 i:0 	 global-step:25420	 l-p:0.13148806989192963
epoch£º1271	 i:1 	 global-step:25421	 l-p:0.15652815997600555
epoch£º1271	 i:2 	 global-step:25422	 l-p:0.14934805035591125
epoch£º1271	 i:3 	 global-step:25423	 l-p:0.12015701085329056
epoch£º1271	 i:4 	 global-step:25424	 l-p:0.13752049207687378
epoch£º1271	 i:5 	 global-step:25425	 l-p:0.1479211151599884
epoch£º1271	 i:6 	 global-step:25426	 l-p:0.18736757338047028
epoch£º1271	 i:7 	 global-step:25427	 l-p:0.12159079313278198
epoch£º1271	 i:8 	 global-step:25428	 l-p:0.1239396184682846
epoch£º1271	 i:9 	 global-step:25429	 l-p:0.08865344524383545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1272
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228]], device='cuda:0')
 pt:tensor([[5.1812, 4.9794, 4.6339],
        [5.1812, 4.8824, 4.6376],
        [5.1812, 4.9045, 4.6088],
        [5.1812, 5.3959, 5.1886]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1272, step:0 
model_pd.l_p.mean(): 0.1352568417787552 
model_pd.l_d.mean(): -19.908281326293945 
model_pd.lagr.mean(): -19.77302360534668 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5261], device='cuda:0')), ('power', tensor([-20.8255], device='cuda:0'))])
epoch£º1272	 i:0 	 global-step:25440	 l-p:0.1352568417787552
epoch£º1272	 i:1 	 global-step:25441	 l-p:0.13114352524280548
epoch£º1272	 i:2 	 global-step:25442	 l-p:0.12173080444335938
epoch£º1272	 i:3 	 global-step:25443	 l-p:0.1336212456226349
epoch£º1272	 i:4 	 global-step:25444	 l-p:0.10008537769317627
epoch£º1272	 i:5 	 global-step:25445	 l-p:0.1427101492881775
epoch£º1272	 i:6 	 global-step:25446	 l-p:0.14448659121990204
epoch£º1272	 i:7 	 global-step:25447	 l-p:0.139936164021492
epoch£º1272	 i:8 	 global-step:25448	 l-p:0.13248048722743988
epoch£º1272	 i:9 	 global-step:25449	 l-p:0.17681217193603516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1857, 4.8817, 4.6753],
        [5.1857, 5.1296, 5.1712],
        [5.1857, 5.1853, 5.1857],
        [5.1857, 5.1810, 5.1855]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1273, step:0 
model_pd.l_p.mean(): 0.15629751980304718 
model_pd.l_d.mean(): -19.72461700439453 
model_pd.lagr.mean(): -19.56831932067871 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5366], device='cuda:0')), ('power', tensor([-20.6493], device='cuda:0'))])
epoch£º1273	 i:0 	 global-step:25460	 l-p:0.15629751980304718
epoch£º1273	 i:1 	 global-step:25461	 l-p:0.19128406047821045
epoch£º1273	 i:2 	 global-step:25462	 l-p:0.08440795540809631
epoch£º1273	 i:3 	 global-step:25463	 l-p:0.1468123197555542
epoch£º1273	 i:4 	 global-step:25464	 l-p:0.11337944120168686
epoch£º1273	 i:5 	 global-step:25465	 l-p:0.09938500821590424
epoch£º1273	 i:6 	 global-step:25466	 l-p:0.17613887786865234
epoch£º1273	 i:7 	 global-step:25467	 l-p:0.12543152272701263
epoch£º1273	 i:8 	 global-step:25468	 l-p:0.10831470042467117
epoch£º1273	 i:9 	 global-step:25469	 l-p:0.1431782990694046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1867, 5.1868, 5.1868],
        [5.1867, 5.0060, 5.0568],
        [5.1867, 4.9743, 5.0051],
        [5.1867, 5.1845, 5.1867]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1274, step:0 
model_pd.l_p.mean(): 0.14874909818172455 
model_pd.l_d.mean(): -19.895008087158203 
model_pd.lagr.mean(): -19.746259689331055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4610], device='cuda:0')), ('power', tensor([-20.7445], device='cuda:0'))])
epoch£º1274	 i:0 	 global-step:25480	 l-p:0.14874909818172455
epoch£º1274	 i:1 	 global-step:25481	 l-p:0.12702900171279907
epoch£º1274	 i:2 	 global-step:25482	 l-p:0.14339032769203186
epoch£º1274	 i:3 	 global-step:25483	 l-p:0.10848019272089005
epoch£º1274	 i:4 	 global-step:25484	 l-p:0.17440754175186157
epoch£º1274	 i:5 	 global-step:25485	 l-p:0.12539687752723694
epoch£º1274	 i:6 	 global-step:25486	 l-p:0.12023500353097916
epoch£º1274	 i:7 	 global-step:25487	 l-p:0.08003539592027664
epoch£º1274	 i:8 	 global-step:25488	 l-p:0.19818218052387238
epoch£º1274	 i:9 	 global-step:25489	 l-p:0.1390443742275238
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1779, 5.0487, 5.1108],
        [5.1779, 5.1759, 5.1778],
        [5.1779, 5.1772, 5.1779],
        [5.1779, 5.5283, 5.4035]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1275, step:0 
model_pd.l_p.mean(): 0.09558317065238953 
model_pd.l_d.mean(): -20.08165168762207 
model_pd.lagr.mean(): -19.986068725585938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4889], device='cuda:0')), ('power', tensor([-20.9635], device='cuda:0'))])
epoch£º1275	 i:0 	 global-step:25500	 l-p:0.09558317065238953
epoch£º1275	 i:1 	 global-step:25501	 l-p:0.14657197892665863
epoch£º1275	 i:2 	 global-step:25502	 l-p:0.10150095075368881
epoch£º1275	 i:3 	 global-step:25503	 l-p:0.13652974367141724
epoch£º1275	 i:4 	 global-step:25504	 l-p:0.12055627256631851
epoch£º1275	 i:5 	 global-step:25505	 l-p:0.13653028011322021
epoch£º1275	 i:6 	 global-step:25506	 l-p:0.16965480148792267
epoch£º1275	 i:7 	 global-step:25507	 l-p:0.15524329245090485
epoch£º1275	 i:8 	 global-step:25508	 l-p:0.1306113749742508
epoch£º1275	 i:9 	 global-step:25509	 l-p:0.199174165725708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1737, 5.1733, 5.1737],
        [5.1737, 4.9748, 4.6277],
        [5.1737, 5.0166, 4.6654],
        [5.1737, 4.8723, 4.7343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1276, step:0 
model_pd.l_p.mean(): 0.12049726396799088 
model_pd.l_d.mean(): -20.355911254882812 
model_pd.lagr.mean(): -20.235414505004883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4686], device='cuda:0')), ('power', tensor([-21.2219], device='cuda:0'))])
epoch£º1276	 i:0 	 global-step:25520	 l-p:0.12049726396799088
epoch£º1276	 i:1 	 global-step:25521	 l-p:0.14199814200401306
epoch£º1276	 i:2 	 global-step:25522	 l-p:0.14756369590759277
epoch£º1276	 i:3 	 global-step:25523	 l-p:0.17528057098388672
epoch£º1276	 i:4 	 global-step:25524	 l-p:0.1309407651424408
epoch£º1276	 i:5 	 global-step:25525	 l-p:0.1237698495388031
epoch£º1276	 i:6 	 global-step:25526	 l-p:0.12136554718017578
epoch£º1276	 i:7 	 global-step:25527	 l-p:0.11764977872371674
epoch£º1276	 i:8 	 global-step:25528	 l-p:0.1255306750535965
epoch£º1276	 i:9 	 global-step:25529	 l-p:0.19108399748802185
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1747, 4.9285, 4.6008],
        [5.1747, 5.1732, 5.1746],
        [5.1747, 5.4342, 5.2532],
        [5.1747, 5.1741, 5.1747]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1277, step:0 
model_pd.l_p.mean(): 0.1840101033449173 
model_pd.l_d.mean(): -20.236881256103516 
model_pd.lagr.mean(): -20.052871704101562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5073], device='cuda:0')), ('power', tensor([-21.1407], device='cuda:0'))])
epoch£º1277	 i:0 	 global-step:25540	 l-p:0.1840101033449173
epoch£º1277	 i:1 	 global-step:25541	 l-p:0.11881308257579803
epoch£º1277	 i:2 	 global-step:25542	 l-p:0.14409422874450684
epoch£º1277	 i:3 	 global-step:25543	 l-p:0.12006492167711258
epoch£º1277	 i:4 	 global-step:25544	 l-p:0.15217648446559906
epoch£º1277	 i:5 	 global-step:25545	 l-p:0.11103036999702454
epoch£º1277	 i:6 	 global-step:25546	 l-p:0.12395833432674408
epoch£º1277	 i:7 	 global-step:25547	 l-p:0.11107949167490005
epoch£º1277	 i:8 	 global-step:25548	 l-p:0.12606242299079895
epoch£º1277	 i:9 	 global-step:25549	 l-p:0.19418539106845856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1783, 5.1783, 5.1783],
        [5.1783, 5.1840, 4.8725],
        [5.1783, 5.0812, 4.7372],
        [5.1783, 5.1634, 5.1768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1278, step:0 
model_pd.l_p.mean(): 0.14248351752758026 
model_pd.l_d.mean(): -20.39783477783203 
model_pd.lagr.mean(): -20.255352020263672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4368], device='cuda:0')), ('power', tensor([-21.2317], device='cuda:0'))])
epoch£º1278	 i:0 	 global-step:25560	 l-p:0.14248351752758026
epoch£º1278	 i:1 	 global-step:25561	 l-p:0.16055917739868164
epoch£º1278	 i:2 	 global-step:25562	 l-p:0.11351508647203445
epoch£º1278	 i:3 	 global-step:25563	 l-p:0.16661393642425537
epoch£º1278	 i:4 	 global-step:25564	 l-p:0.13295023143291473
epoch£º1278	 i:5 	 global-step:25565	 l-p:0.15899153053760529
epoch£º1278	 i:6 	 global-step:25566	 l-p:0.11989469081163406
epoch£º1278	 i:7 	 global-step:25567	 l-p:0.0942537933588028
epoch£º1278	 i:8 	 global-step:25568	 l-p:0.15078219771385193
epoch£º1278	 i:9 	 global-step:25569	 l-p:0.13290229439735413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1803, 5.0861, 5.1433],
        [5.1803, 5.1801, 5.1803],
        [5.1803, 4.9852, 4.6379],
        [5.1803, 5.1803, 5.1803]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1279, step:0 
model_pd.l_p.mean(): 0.13473710417747498 
model_pd.l_d.mean(): -17.93729591369629 
model_pd.lagr.mean(): -17.80255889892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6610], device='cuda:0')), ('power', tensor([-18.9573], device='cuda:0'))])
epoch£º1279	 i:0 	 global-step:25580	 l-p:0.13473710417747498
epoch£º1279	 i:1 	 global-step:25581	 l-p:0.1305907517671585
epoch£º1279	 i:2 	 global-step:25582	 l-p:0.15216612815856934
epoch£º1279	 i:3 	 global-step:25583	 l-p:0.12024842947721481
epoch£º1279	 i:4 	 global-step:25584	 l-p:0.1738164871931076
epoch£º1279	 i:5 	 global-step:25585	 l-p:0.10689122974872589
epoch£º1279	 i:6 	 global-step:25586	 l-p:0.15624819695949554
epoch£º1279	 i:7 	 global-step:25587	 l-p:0.12977319955825806
epoch£º1279	 i:8 	 global-step:25588	 l-p:0.14172860980033875
epoch£º1279	 i:9 	 global-step:25589	 l-p:0.12788324058055878
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1776, 4.8733, 4.6551],
        [5.1776, 5.0216, 5.0811],
        [5.1776, 5.1747, 5.1775],
        [5.1776, 5.1759, 5.1775]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1280, step:0 
model_pd.l_p.mean(): 0.14648132026195526 
model_pd.l_d.mean(): -19.608945846557617 
model_pd.lagr.mean(): -19.462465286254883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4996], device='cuda:0')), ('power', tensor([-20.4931], device='cuda:0'))])
epoch£º1280	 i:0 	 global-step:25600	 l-p:0.14648132026195526
epoch£º1280	 i:1 	 global-step:25601	 l-p:0.14280560612678528
epoch£º1280	 i:2 	 global-step:25602	 l-p:0.1335177719593048
epoch£º1280	 i:3 	 global-step:25603	 l-p:0.14169296622276306
epoch£º1280	 i:4 	 global-step:25604	 l-p:0.12981997430324554
epoch£º1280	 i:5 	 global-step:25605	 l-p:0.12123309820890427
epoch£º1280	 i:6 	 global-step:25606	 l-p:0.1553061306476593
epoch£º1280	 i:7 	 global-step:25607	 l-p:0.12869510054588318
epoch£º1280	 i:8 	 global-step:25608	 l-p:0.1491163820028305
epoch£º1280	 i:9 	 global-step:25609	 l-p:0.1395011693239212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1743, 5.1527, 5.1715],
        [5.1743, 4.8984, 4.8437],
        [5.1743, 5.0364, 5.0984],
        [5.1743, 4.8871, 4.8042]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1281, step:0 
model_pd.l_p.mean(): 0.089541956782341 
model_pd.l_d.mean(): -20.416933059692383 
model_pd.lagr.mean(): -20.327390670776367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4341], device='cuda:0')), ('power', tensor([-21.2484], device='cuda:0'))])
epoch£º1281	 i:0 	 global-step:25620	 l-p:0.089541956782341
epoch£º1281	 i:1 	 global-step:25621	 l-p:0.17599627375602722
epoch£º1281	 i:2 	 global-step:25622	 l-p:0.14264152944087982
epoch£º1281	 i:3 	 global-step:25623	 l-p:0.16072535514831543
epoch£º1281	 i:4 	 global-step:25624	 l-p:0.16779766976833344
epoch£º1281	 i:5 	 global-step:25625	 l-p:0.1273750513792038
epoch£º1281	 i:6 	 global-step:25626	 l-p:0.1715540587902069
epoch£º1281	 i:7 	 global-step:25627	 l-p:0.1235017254948616
epoch£º1281	 i:8 	 global-step:25628	 l-p:0.09264686703681946
epoch£º1281	 i:9 	 global-step:25629	 l-p:0.13241437077522278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1805, 5.1804, 5.1805],
        [5.1805, 5.1782, 5.1804],
        [5.1805, 4.9340, 4.9291],
        [5.1805, 5.0437, 4.6938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1282, step:0 
model_pd.l_p.mean(): 0.10545358061790466 
model_pd.l_d.mean(): -20.22779083251953 
model_pd.lagr.mean(): -20.122337341308594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4693], device='cuda:0')), ('power', tensor([-21.0921], device='cuda:0'))])
epoch£º1282	 i:0 	 global-step:25640	 l-p:0.10545358061790466
epoch£º1282	 i:1 	 global-step:25641	 l-p:0.08012473583221436
epoch£º1282	 i:2 	 global-step:25642	 l-p:0.15050797164440155
epoch£º1282	 i:3 	 global-step:25643	 l-p:0.15989911556243896
epoch£º1282	 i:4 	 global-step:25644	 l-p:0.11021807789802551
epoch£º1282	 i:5 	 global-step:25645	 l-p:0.13576602935791016
epoch£º1282	 i:6 	 global-step:25646	 l-p:0.098790243268013
epoch£º1282	 i:7 	 global-step:25647	 l-p:0.19045284390449524
epoch£º1282	 i:8 	 global-step:25648	 l-p:0.2076471894979477
epoch£º1282	 i:9 	 global-step:25649	 l-p:0.14008599519729614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1778, 4.8766, 4.7383],
        [5.1778, 5.1771, 5.1778],
        [5.1778, 5.1754, 5.1777],
        [5.1778, 5.1778, 5.1778]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1283, step:0 
model_pd.l_p.mean(): 0.08056320995092392 
model_pd.l_d.mean(): -20.071399688720703 
model_pd.lagr.mean(): -19.99083709716797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4746], device='cuda:0')), ('power', tensor([-20.9382], device='cuda:0'))])
epoch£º1283	 i:0 	 global-step:25660	 l-p:0.08056320995092392
epoch£º1283	 i:1 	 global-step:25661	 l-p:0.13785165548324585
epoch£º1283	 i:2 	 global-step:25662	 l-p:0.16362738609313965
epoch£º1283	 i:3 	 global-step:25663	 l-p:0.14649656414985657
epoch£º1283	 i:4 	 global-step:25664	 l-p:0.18398210406303406
epoch£º1283	 i:5 	 global-step:25665	 l-p:0.08383552730083466
epoch£º1283	 i:6 	 global-step:25666	 l-p:0.12278373539447784
epoch£º1283	 i:7 	 global-step:25667	 l-p:0.1536962240934372
epoch£º1283	 i:8 	 global-step:25668	 l-p:0.15074610710144043
epoch£º1283	 i:9 	 global-step:25669	 l-p:0.1498851478099823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1823, 5.2469, 4.9608],
        [5.1823, 5.1793, 5.1822],
        [5.1823, 5.0971, 5.1515],
        [5.1823, 5.1823, 5.1823]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1284, step:0 
model_pd.l_p.mean(): 0.128285214304924 
model_pd.l_d.mean(): -20.655105590820312 
model_pd.lagr.mean(): -20.52682113647461 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4159], device='cuda:0')), ('power', tensor([-21.4721], device='cuda:0'))])
epoch£º1284	 i:0 	 global-step:25680	 l-p:0.128285214304924
epoch£º1284	 i:1 	 global-step:25681	 l-p:0.15133750438690186
epoch£º1284	 i:2 	 global-step:25682	 l-p:0.1251567006111145
epoch£º1284	 i:3 	 global-step:25683	 l-p:0.12527431547641754
epoch£º1284	 i:4 	 global-step:25684	 l-p:0.14838221669197083
epoch£º1284	 i:5 	 global-step:25685	 l-p:0.19292961061000824
epoch£º1284	 i:6 	 global-step:25686	 l-p:0.13555721938610077
epoch£º1284	 i:7 	 global-step:25687	 l-p:0.14093643426895142
epoch£º1284	 i:8 	 global-step:25688	 l-p:0.09477647393941879
epoch£º1284	 i:9 	 global-step:25689	 l-p:0.12981973588466644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1783, 4.9615, 4.6196],
        [5.1783, 5.1727, 5.1780],
        [5.1783, 4.9052, 4.8560],
        [5.1783, 5.0550, 4.7064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1285, step:0 
model_pd.l_p.mean(): 0.20942555367946625 
model_pd.l_d.mean(): -20.52006721496582 
model_pd.lagr.mean(): -20.31064224243164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4408], device='cuda:0')), ('power', tensor([-21.3603], device='cuda:0'))])
epoch£º1285	 i:0 	 global-step:25700	 l-p:0.20942555367946625
epoch£º1285	 i:1 	 global-step:25701	 l-p:0.16200990974903107
epoch£º1285	 i:2 	 global-step:25702	 l-p:0.11208280175924301
epoch£º1285	 i:3 	 global-step:25703	 l-p:0.09820754081010818
epoch£º1285	 i:4 	 global-step:25704	 l-p:0.12506458163261414
epoch£º1285	 i:5 	 global-step:25705	 l-p:0.13429853320121765
epoch£º1285	 i:6 	 global-step:25706	 l-p:0.1029462218284607
epoch£º1285	 i:7 	 global-step:25707	 l-p:0.16462565958499908
epoch£º1285	 i:8 	 global-step:25708	 l-p:0.1409967541694641
epoch£º1285	 i:9 	 global-step:25709	 l-p:0.13519255816936493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1743, 4.8870, 4.8041],
        [5.1743, 5.1299, 5.1647],
        [5.1743, 5.4915, 5.3456],
        [5.1743, 5.2998, 5.0436]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1286, step:0 
model_pd.l_p.mean(): 0.12102407962083817 
model_pd.l_d.mean(): -19.414695739746094 
model_pd.lagr.mean(): -19.293672561645508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5142], device='cuda:0')), ('power', tensor([-20.3103], device='cuda:0'))])
epoch£º1286	 i:0 	 global-step:25720	 l-p:0.12102407962083817
epoch£º1286	 i:1 	 global-step:25721	 l-p:0.1296444833278656
epoch£º1286	 i:2 	 global-step:25722	 l-p:0.11974435299634933
epoch£º1286	 i:3 	 global-step:25723	 l-p:0.1939934492111206
epoch£º1286	 i:4 	 global-step:25724	 l-p:0.16104644536972046
epoch£º1286	 i:5 	 global-step:25725	 l-p:0.18660561740398407
epoch£º1286	 i:6 	 global-step:25726	 l-p:0.14390648901462555
epoch£º1286	 i:7 	 global-step:25727	 l-p:0.11834587901830673
epoch£º1286	 i:8 	 global-step:25728	 l-p:0.12314727157354355
epoch£º1286	 i:9 	 global-step:25729	 l-p:0.09697645157575607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1759, 5.1032, 4.7649],
        [5.1759, 4.8708, 4.7048],
        [5.1759, 5.1759, 5.1759],
        [5.1759, 5.1759, 5.1759]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1287, step:0 
model_pd.l_p.mean(): 0.12580758333206177 
model_pd.l_d.mean(): -20.272552490234375 
model_pd.lagr.mean(): -20.146745681762695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4924], device='cuda:0')), ('power', tensor([-21.1616], device='cuda:0'))])
epoch£º1287	 i:0 	 global-step:25740	 l-p:0.12580758333206177
epoch£º1287	 i:1 	 global-step:25741	 l-p:0.11663716286420822
epoch£º1287	 i:2 	 global-step:25742	 l-p:0.15139731764793396
epoch£º1287	 i:3 	 global-step:25743	 l-p:0.14346642792224884
epoch£º1287	 i:4 	 global-step:25744	 l-p:0.13463087379932404
epoch£º1287	 i:5 	 global-step:25745	 l-p:0.1229705885052681
epoch£º1287	 i:6 	 global-step:25746	 l-p:0.11205604672431946
epoch£º1287	 i:7 	 global-step:25747	 l-p:0.12260503321886063
epoch£º1287	 i:8 	 global-step:25748	 l-p:0.1840524673461914
epoch£º1287	 i:9 	 global-step:25749	 l-p:0.18377463519573212
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1718, 5.1718, 5.1718],
        [5.1718, 5.1717, 5.1718],
        [5.1718, 4.8837, 4.7993],
        [5.1718, 5.0410, 4.6911]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1288, step:0 
model_pd.l_p.mean(): 0.17357607185840607 
model_pd.l_d.mean(): -19.791481018066406 
model_pd.lagr.mean(): -19.617904663085938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4799], device='cuda:0')), ('power', tensor([-20.6587], device='cuda:0'))])
epoch£º1288	 i:0 	 global-step:25760	 l-p:0.17357607185840607
epoch£º1288	 i:1 	 global-step:25761	 l-p:0.15962249040603638
epoch£º1288	 i:2 	 global-step:25762	 l-p:0.13237106800079346
epoch£º1288	 i:3 	 global-step:25763	 l-p:0.19274401664733887
epoch£º1288	 i:4 	 global-step:25764	 l-p:0.12032951414585114
epoch£º1288	 i:5 	 global-step:25765	 l-p:0.0811324492096901
epoch£º1288	 i:6 	 global-step:25766	 l-p:0.13206933438777924
epoch£º1288	 i:7 	 global-step:25767	 l-p:0.1302395910024643
epoch£º1288	 i:8 	 global-step:25768	 l-p:0.10458268970251083
epoch£º1288	 i:9 	 global-step:25769	 l-p:0.1794782280921936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1730, 5.2959, 5.0385],
        [5.1730, 5.0786, 5.1359],
        [5.1730, 5.1730, 5.1730],
        [5.1730, 5.1287, 5.1635]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1289, step:0 
model_pd.l_p.mean(): 0.1069580465555191 
model_pd.l_d.mean(): -20.159704208374023 
model_pd.lagr.mean(): -20.052745819091797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4340], device='cuda:0')), ('power', tensor([-20.9862], device='cuda:0'))])
epoch£º1289	 i:0 	 global-step:25780	 l-p:0.1069580465555191
epoch£º1289	 i:1 	 global-step:25781	 l-p:0.10616572201251984
epoch£º1289	 i:2 	 global-step:25782	 l-p:0.11644131690263748
epoch£º1289	 i:3 	 global-step:25783	 l-p:0.223666250705719
epoch£º1289	 i:4 	 global-step:25784	 l-p:0.13033607602119446
epoch£º1289	 i:5 	 global-step:25785	 l-p:0.1315862238407135
epoch£º1289	 i:6 	 global-step:25786	 l-p:0.12267711013555527
epoch£º1289	 i:7 	 global-step:25787	 l-p:0.21106068789958954
epoch£º1289	 i:8 	 global-step:25788	 l-p:0.13738451898097992
epoch£º1289	 i:9 	 global-step:25789	 l-p:0.11779536306858063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1719, 5.1719, 5.1719],
        [5.1719, 4.9444, 4.9626],
        [5.1719, 4.9681, 5.0065],
        [5.1719, 5.0808, 5.1371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1290, step:0 
model_pd.l_p.mean(): 0.10335241258144379 
model_pd.l_d.mean(): -19.487781524658203 
model_pd.lagr.mean(): -19.384429931640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5229], device='cuda:0')), ('power', tensor([-20.3937], device='cuda:0'))])
epoch£º1290	 i:0 	 global-step:25800	 l-p:0.10335241258144379
epoch£º1290	 i:1 	 global-step:25801	 l-p:0.22962245345115662
epoch£º1290	 i:2 	 global-step:25802	 l-p:0.14905114471912384
epoch£º1290	 i:3 	 global-step:25803	 l-p:0.1243637278676033
epoch£º1290	 i:4 	 global-step:25804	 l-p:0.2280406355857849
epoch£º1290	 i:5 	 global-step:25805	 l-p:0.13512545824050903
epoch£º1290	 i:6 	 global-step:25806	 l-p:0.08157135546207428
epoch£º1290	 i:7 	 global-step:25807	 l-p:0.09589576721191406
epoch£º1290	 i:8 	 global-step:25808	 l-p:0.09556964784860611
epoch£º1290	 i:9 	 global-step:25809	 l-p:0.16650407016277313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1701, 4.9395, 4.9547],
        [5.1701, 4.9295, 4.9334],
        [5.1701, 4.8661, 4.7156],
        [5.1701, 5.1701, 5.1701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1291, step:0 
model_pd.l_p.mean(): 0.12962402403354645 
model_pd.l_d.mean(): -20.73146629333496 
model_pd.lagr.mean(): -20.601842880249023 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4006], device='cuda:0')), ('power', tensor([-21.5341], device='cuda:0'))])
epoch£º1291	 i:0 	 global-step:25820	 l-p:0.12962402403354645
epoch£º1291	 i:1 	 global-step:25821	 l-p:0.13677279651165009
epoch£º1291	 i:2 	 global-step:25822	 l-p:0.1266125589609146
epoch£º1291	 i:3 	 global-step:25823	 l-p:0.23581723868846893
epoch£º1291	 i:4 	 global-step:25824	 l-p:0.13587616384029388
epoch£º1291	 i:5 	 global-step:25825	 l-p:0.1755753606557846
epoch£º1291	 i:6 	 global-step:25826	 l-p:0.14049072563648224
epoch£º1291	 i:7 	 global-step:25827	 l-p:0.06959737837314606
epoch£º1291	 i:8 	 global-step:25828	 l-p:0.12773165106773376
epoch£º1291	 i:9 	 global-step:25829	 l-p:0.1453099101781845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1699, 5.1696, 5.1699],
        [5.1699, 5.5554, 5.4534],
        [5.1699, 5.1687, 5.1699],
        [5.1699, 5.0875, 5.1410]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1292, step:0 
model_pd.l_p.mean(): 0.12072166055440903 
model_pd.l_d.mean(): -18.811487197875977 
model_pd.lagr.mean(): -18.690765380859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5304], device='cuda:0')), ('power', tensor([-19.7126], device='cuda:0'))])
epoch£º1292	 i:0 	 global-step:25840	 l-p:0.12072166055440903
epoch£º1292	 i:1 	 global-step:25841	 l-p:0.1958686262369156
epoch£º1292	 i:2 	 global-step:25842	 l-p:0.13996843993663788
epoch£º1292	 i:3 	 global-step:25843	 l-p:0.125838503241539
epoch£º1292	 i:4 	 global-step:25844	 l-p:0.1138276681303978
epoch£º1292	 i:5 	 global-step:25845	 l-p:0.14128397405147552
epoch£º1292	 i:6 	 global-step:25846	 l-p:0.1707756221294403
epoch£º1292	 i:7 	 global-step:25847	 l-p:0.13550563156604767
epoch£º1292	 i:8 	 global-step:25848	 l-p:0.13885582983493805
epoch£º1292	 i:9 	 global-step:25849	 l-p:0.1278614103794098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1717, 5.1674, 5.1715],
        [5.1717, 4.8682, 4.7211],
        [5.1717, 5.0079, 5.0656],
        [5.1717, 5.0522, 5.1141]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1293, step:0 
model_pd.l_p.mean(): 0.09917966276407242 
model_pd.l_d.mean(): -20.176671981811523 
model_pd.lagr.mean(): -20.077491760253906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4774], device='cuda:0')), ('power', tensor([-21.0484], device='cuda:0'))])
epoch£º1293	 i:0 	 global-step:25860	 l-p:0.09917966276407242
epoch£º1293	 i:1 	 global-step:25861	 l-p:0.12022958695888519
epoch£º1293	 i:2 	 global-step:25862	 l-p:0.24585773050785065
epoch£º1293	 i:3 	 global-step:25863	 l-p:0.15108929574489594
epoch£º1293	 i:4 	 global-step:25864	 l-p:0.15815898776054382
epoch£º1293	 i:5 	 global-step:25865	 l-p:0.10653813928365707
epoch£º1293	 i:6 	 global-step:25866	 l-p:0.12614022195339203
epoch£º1293	 i:7 	 global-step:25867	 l-p:0.081483855843544
epoch£º1293	 i:8 	 global-step:25868	 l-p:0.19850440323352814
epoch£º1293	 i:9 	 global-step:25869	 l-p:0.13221235573291779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1681, 5.1671, 5.1681],
        [5.1681, 5.1177, 5.1561],
        [5.1681, 4.8622, 4.6988],
        [5.1681, 5.1676, 5.1681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1294, step:0 
model_pd.l_p.mean(): 0.09342828392982483 
model_pd.l_d.mean(): -19.185977935791016 
model_pd.lagr.mean(): -19.09255027770996 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5340], device='cuda:0')), ('power', tensor([-20.0978], device='cuda:0'))])
epoch£º1294	 i:0 	 global-step:25880	 l-p:0.09342828392982483
epoch£º1294	 i:1 	 global-step:25881	 l-p:0.15682227909564972
epoch£º1294	 i:2 	 global-step:25882	 l-p:0.11346746236085892
epoch£º1294	 i:3 	 global-step:25883	 l-p:0.2380167543888092
epoch£º1294	 i:4 	 global-step:25884	 l-p:0.12575995922088623
epoch£º1294	 i:5 	 global-step:25885	 l-p:0.14299924671649933
epoch£º1294	 i:6 	 global-step:25886	 l-p:0.15402527153491974
epoch£º1294	 i:7 	 global-step:25887	 l-p:0.12354335188865662
epoch£º1294	 i:8 	 global-step:25888	 l-p:0.1489739865064621
epoch£º1294	 i:9 	 global-step:25889	 l-p:0.1300657093524933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1674, 5.1651, 5.1673],
        [5.1674, 5.1533, 5.1661],
        [5.1674, 5.1159, 5.1550],
        [5.1674, 5.2361, 4.9518]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1295, step:0 
model_pd.l_p.mean(): 0.16676118969917297 
model_pd.l_d.mean(): -20.285171508789062 
model_pd.lagr.mean(): -20.118410110473633 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4603], device='cuda:0')), ('power', tensor([-21.1413], device='cuda:0'))])
epoch£º1295	 i:0 	 global-step:25900	 l-p:0.16676118969917297
epoch£º1295	 i:1 	 global-step:25901	 l-p:0.07382605224847794
epoch£º1295	 i:2 	 global-step:25902	 l-p:0.20396338403224945
epoch£º1295	 i:3 	 global-step:25903	 l-p:0.12634631991386414
epoch£º1295	 i:4 	 global-step:25904	 l-p:0.1254393756389618
epoch£º1295	 i:5 	 global-step:25905	 l-p:0.10768260806798935
epoch£º1295	 i:6 	 global-step:25906	 l-p:0.16494809091091156
epoch£º1295	 i:7 	 global-step:25907	 l-p:0.16671983897686005
epoch£º1295	 i:8 	 global-step:25908	 l-p:0.1506102979183197
epoch£º1295	 i:9 	 global-step:25909	 l-p:0.13856098055839539
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1687, 5.0813, 5.1365],
        [5.1687, 5.0997, 5.1478],
        [5.1687, 5.1687, 5.1687],
        [5.1687, 5.1487, 5.1663]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1296, step:0 
model_pd.l_p.mean(): 0.1453954130411148 
model_pd.l_d.mean(): -19.257675170898438 
model_pd.lagr.mean(): -19.112279891967773 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5204], device='cuda:0')), ('power', tensor([-20.1567], device='cuda:0'))])
epoch£º1296	 i:0 	 global-step:25920	 l-p:0.1453954130411148
epoch£º1296	 i:1 	 global-step:25921	 l-p:0.10737025737762451
epoch£º1296	 i:2 	 global-step:25922	 l-p:0.15572726726531982
epoch£º1296	 i:3 	 global-step:25923	 l-p:0.11917664855718613
epoch£º1296	 i:4 	 global-step:25924	 l-p:0.17302455008029938
epoch£º1296	 i:5 	 global-step:25925	 l-p:0.157548189163208
epoch£º1296	 i:6 	 global-step:25926	 l-p:0.1812196522951126
epoch£º1296	 i:7 	 global-step:25927	 l-p:0.13990959525108337
epoch£º1296	 i:8 	 global-step:25928	 l-p:0.12214677780866623
epoch£º1296	 i:9 	 global-step:25929	 l-p:0.11187932640314102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1297
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4712,  0.3667,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2304,  0.1412,  1.0000,  0.0866,
          1.0000,  0.6130, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2326,  0.1431,  1.0000,  0.0880,
          1.0000,  0.6150, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]], device='cuda:0')
 pt:tensor([[5.1736, 4.9126, 4.5956],
        [5.1736, 4.9152, 4.8937],
        [5.1736, 4.9133, 4.8888],
        [5.1736, 4.8672, 4.6867]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1297, step:0 
model_pd.l_p.mean(): 0.1217866837978363 
model_pd.l_d.mean(): -18.71720314025879 
model_pd.lagr.mean(): -18.595417022705078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5369], device='cuda:0')), ('power', tensor([-19.6233], device='cuda:0'))])
epoch£º1297	 i:0 	 global-step:25940	 l-p:0.1217866837978363
epoch£º1297	 i:1 	 global-step:25941	 l-p:0.12083286046981812
epoch£º1297	 i:2 	 global-step:25942	 l-p:0.20052531361579895
epoch£º1297	 i:3 	 global-step:25943	 l-p:0.14722809195518494
epoch£º1297	 i:4 	 global-step:25944	 l-p:0.197581484913826
epoch£º1297	 i:5 	 global-step:25945	 l-p:0.18521952629089355
epoch£º1297	 i:6 	 global-step:25946	 l-p:0.09759443253278732
epoch£º1297	 i:7 	 global-step:25947	 l-p:0.08520551770925522
epoch£º1297	 i:8 	 global-step:25948	 l-p:0.14709332585334778
epoch£º1297	 i:9 	 global-step:25949	 l-p:0.10279381275177002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1720, 5.1718, 5.1720],
        [5.1720, 4.8878, 4.5997],
        [5.1720, 4.9719, 4.6242],
        [5.1720, 4.8745, 4.6172]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1298, step:0 
model_pd.l_p.mean(): 0.10706125199794769 
model_pd.l_d.mean(): -20.310073852539062 
model_pd.lagr.mean(): -20.203012466430664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4752], device='cuda:0')), ('power', tensor([-21.1820], device='cuda:0'))])
epoch£º1298	 i:0 	 global-step:25960	 l-p:0.10706125199794769
epoch£º1298	 i:1 	 global-step:25961	 l-p:0.17637702822685242
epoch£º1298	 i:2 	 global-step:25962	 l-p:0.20510061085224152
epoch£º1298	 i:3 	 global-step:25963	 l-p:0.08122902363538742
epoch£º1298	 i:4 	 global-step:25964	 l-p:0.1070379838347435
epoch£º1298	 i:5 	 global-step:25965	 l-p:0.12296421080827713
epoch£º1298	 i:6 	 global-step:25966	 l-p:0.1914360523223877
epoch£º1298	 i:7 	 global-step:25967	 l-p:0.13969796895980835
epoch£º1298	 i:8 	 global-step:25968	 l-p:0.20456819236278534
epoch£º1298	 i:9 	 global-step:25969	 l-p:0.08222479373216629
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1680, 5.1674, 5.1680],
        [5.1680, 4.8608, 4.6609],
        [5.1680, 5.1680, 5.1680],
        [5.1680, 5.1353, 5.1624]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1299, step:0 
model_pd.l_p.mean(): 0.12341274321079254 
model_pd.l_d.mean(): -20.76453971862793 
model_pd.lagr.mean(): -20.64112663269043 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4013], device='cuda:0')), ('power', tensor([-21.5685], device='cuda:0'))])
epoch£º1299	 i:0 	 global-step:25980	 l-p:0.12341274321079254
epoch£º1299	 i:1 	 global-step:25981	 l-p:0.12318295985460281
epoch£º1299	 i:2 	 global-step:25982	 l-p:0.21388526260852814
epoch£º1299	 i:3 	 global-step:25983	 l-p:0.14908367395401
epoch£º1299	 i:4 	 global-step:25984	 l-p:0.12678439915180206
epoch£º1299	 i:5 	 global-step:25985	 l-p:0.12131161242723465
epoch£º1299	 i:6 	 global-step:25986	 l-p:0.12072130292654037
epoch£º1299	 i:7 	 global-step:25987	 l-p:0.15691448748111725
epoch£º1299	 i:8 	 global-step:25988	 l-p:0.14151950180530548
epoch£º1299	 i:9 	 global-step:25989	 l-p:0.15540581941604614
