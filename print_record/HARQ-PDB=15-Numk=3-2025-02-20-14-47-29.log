
bounds:tensor([-1.], device='cuda:0')	db:15	Pt_max:31.62277603149414
model init: 
lambdas:{'pout': tensor([1.], device='cuda:0'), 'power': tensor([1.], device='cuda:0')},
vars:{'pout': tensor([0.], device='cuda:0'), 'power': tensor([0.], device='cuda:0')}

====================================================================================================
====================================================================================================
====================================================================================================

epoch:0
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[2.3753, 2.3753, 2.3753],
        [2.3753, 2.4873, 2.4568],
        [2.3753, 2.3756, 2.3753],
        [2.3753, 2.5082, 2.4827]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:0, step:0 
model_pd.l_p.mean(): -0.1705528199672699 
model_pd.l_d.mean(): -19.24897003173828 
model_pd.lagr.mean(): -19.419523239135742 
model_pd.lambdas: dict_items([('pout', tensor([1.0014], device='cuda:0')), ('power', tensor([0.9990], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([1.3984], device='cuda:0')), ('power', tensor([-20.6474], device='cuda:0'))])
epoch£º0	 i:0 	 global-step:0	 l-p:-0.1705528199672699
epoch£º0	 i:1 	 global-step:1	 l-p:-0.20870685577392578
epoch£º0	 i:2 	 global-step:2	 l-p:-0.2769811153411865
epoch£º0	 i:3 	 global-step:3	 l-p:-0.4038802981376648
epoch£º0	 i:4 	 global-step:4	 l-p:-0.710580050945282
epoch£º0	 i:5 	 global-step:5	 l-p:-2.5309364795684814
epoch£º0	 i:6 	 global-step:6	 l-p:1.7505427598953247
epoch£º0	 i:7 	 global-step:7	 l-p:0.49168166518211365
epoch£º0	 i:8 	 global-step:8	 l-p:0.6606587171554565
epoch£º0	 i:9 	 global-step:9	 l-p:0.056809842586517334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[3.5667, 3.5671, 3.5667],
        [3.5667, 3.7813, 3.7378],
        [3.5667, 3.6280, 3.5888],
        [3.5667, 4.5730, 5.2699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1, step:0 
model_pd.l_p.mean(): 0.37725773453712463 
model_pd.l_d.mean(): -18.738479614257812 
model_pd.lagr.mean(): -18.361221313476562 
model_pd.lambdas: dict_items([('pout', tensor([1.0124], device='cuda:0')), ('power', tensor([0.9889], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.9113], device='cuda:0')), ('power', tensor([-19.8624], device='cuda:0'))])
epoch£º1	 i:0 	 global-step:20	 l-p:0.37725773453712463
epoch£º1	 i:1 	 global-step:21	 l-p:0.2680797576904297
epoch£º1	 i:2 	 global-step:22	 l-p:0.1878846138715744
epoch£º1	 i:3 	 global-step:23	 l-p:0.24290166795253754
epoch£º1	 i:4 	 global-step:24	 l-p:0.1398099660873413
epoch£º1	 i:5 	 global-step:25	 l-p:0.18688258528709412
epoch£º1	 i:6 	 global-step:26	 l-p:0.1773836463689804
epoch£º1	 i:7 	 global-step:27	 l-p:0.218707874417305
epoch£º1	 i:8 	 global-step:28	 l-p:0.22566206753253937
epoch£º1	 i:9 	 global-step:29	 l-p:0.11669803410768509
====================================================================================================
====================================================================================================
====================================================================================================

epoch:2
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9155, 5.9890, 6.5151],
        [4.9155, 5.0133, 4.9532],
        [4.9155, 5.7273, 5.9884],
        [4.9155, 4.9975, 4.9437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:2, step:0 
model_pd.l_p.mean(): 0.13359054923057556 
model_pd.l_d.mean(): -20.127248764038086 
model_pd.lagr.mean(): -19.9936580657959 
model_pd.lambdas: dict_items([('pout', tensor([1.0162], device='cuda:0')), ('power', tensor([0.9832], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4446], device='cuda:0')), ('power', tensor([-20.9244], device='cuda:0'))])
epoch£º2	 i:0 	 global-step:40	 l-p:0.13359054923057556
epoch£º2	 i:1 	 global-step:41	 l-p:0.10821323841810226
epoch£º2	 i:2 	 global-step:42	 l-p:0.10349391400814056
epoch£º2	 i:3 	 global-step:43	 l-p:0.1066986471414566
epoch£º2	 i:4 	 global-step:44	 l-p:0.11863960325717926
epoch£º2	 i:5 	 global-step:45	 l-p:0.1057601273059845
epoch£º2	 i:6 	 global-step:46	 l-p:0.11227868497371674
epoch£º2	 i:7 	 global-step:47	 l-p:1.5558156967163086
epoch£º2	 i:8 	 global-step:48	 l-p:0.10760611295700073
epoch£º2	 i:9 	 global-step:49	 l-p:0.11721697449684143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:3
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.4074, 6.4997, 6.9733],
        [5.4074, 5.4586, 5.4196],
        [5.4074, 5.5137, 5.4478],
        [5.4074, 6.9835, 8.0210]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:3, step:0 
model_pd.l_p.mean(): 0.0807473286986351 
model_pd.l_d.mean(): -18.569534301757812 
model_pd.lagr.mean(): -18.488786697387695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9818], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4170], device='cuda:0')), ('power', tensor([-19.3446], device='cuda:0'))])
epoch£º3	 i:0 	 global-step:60	 l-p:0.0807473286986351
epoch£º3	 i:1 	 global-step:61	 l-p:0.08568032830953598
epoch£º3	 i:2 	 global-step:62	 l-p:-0.047054242342710495
epoch£º3	 i:3 	 global-step:63	 l-p:0.12177307158708572
epoch£º3	 i:4 	 global-step:64	 l-p:0.11827287077903748
epoch£º3	 i:5 	 global-step:65	 l-p:0.11643493175506592
epoch£º3	 i:6 	 global-step:66	 l-p:0.11279250681400299
epoch£º3	 i:7 	 global-step:67	 l-p:0.11705859005451202
epoch£º3	 i:8 	 global-step:68	 l-p:0.1252945214509964
epoch£º3	 i:9 	 global-step:69	 l-p:0.12358066439628601
====================================================================================================
====================================================================================================
====================================================================================================

epoch:4
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9978, 5.7240, 5.9021],
        [4.9978, 5.8848, 6.2076],
        [4.9978, 6.5444, 7.6349],
        [4.9978, 5.0864, 5.0295]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:4, step:0 
model_pd.l_p.mean(): 0.11452260613441467 
model_pd.l_d.mean(): -18.904130935668945 
model_pd.lagr.mean(): -18.789608001708984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9817], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4799], device='cuda:0')), ('power', tensor([-19.7545], device='cuda:0'))])
epoch£º4	 i:0 	 global-step:80	 l-p:0.11452260613441467
epoch£º4	 i:1 	 global-step:81	 l-p:0.1306811422109604
epoch£º4	 i:2 	 global-step:82	 l-p:0.16477511823177338
epoch£º4	 i:3 	 global-step:83	 l-p:0.13171032071113586
epoch£º4	 i:4 	 global-step:84	 l-p:0.15259011089801788
epoch£º4	 i:5 	 global-step:85	 l-p:0.1260961890220642
epoch£º4	 i:6 	 global-step:86	 l-p:0.12957435846328735
epoch£º4	 i:7 	 global-step:87	 l-p:0.13372306525707245
epoch£º4	 i:8 	 global-step:88	 l-p:0.1398295909166336
epoch£º4	 i:9 	 global-step:89	 l-p:0.15763939917087555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:5
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6637, 4.6639, 4.6637],
        [4.6637, 4.6699, 4.6642],
        [4.6637, 4.7574, 4.7003],
        [4.6637, 4.7980, 4.7299]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:5, step:0 
model_pd.l_p.mean(): 0.18277549743652344 
model_pd.l_d.mean(): -19.944305419921875 
model_pd.lagr.mean(): -19.76152992248535 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5379], device='cuda:0')), ('power', tensor([-20.8743], device='cuda:0'))])
epoch£º5	 i:0 	 global-step:100	 l-p:0.18277549743652344
epoch£º5	 i:1 	 global-step:101	 l-p:0.14999276399612427
epoch£º5	 i:2 	 global-step:102	 l-p:0.16478438675403595
epoch£º5	 i:3 	 global-step:103	 l-p:0.07728911936283112
epoch£º5	 i:4 	 global-step:104	 l-p:0.1327490508556366
epoch£º5	 i:5 	 global-step:105	 l-p:0.15239235758781433
epoch£º5	 i:6 	 global-step:106	 l-p:0.12010861188173294
epoch£º5	 i:7 	 global-step:107	 l-p:0.13868780434131622
epoch£º5	 i:8 	 global-step:108	 l-p:0.1365223228931427
epoch£º5	 i:9 	 global-step:109	 l-p:0.13613934814929962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:6
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8715, 4.8993, 4.8764],
        [4.8715, 4.8715, 4.8715],
        [4.8715, 6.2817, 7.2225],
        [4.8715, 4.8763, 4.8718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:6, step:0 
model_pd.l_p.mean(): 0.13218234479427338 
model_pd.l_d.mean(): -20.418516159057617 
model_pd.lagr.mean(): -20.286333084106445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4251], device='cuda:0')), ('power', tensor([-21.2406], device='cuda:0'))])
epoch£º6	 i:0 	 global-step:120	 l-p:0.13218234479427338
epoch£º6	 i:1 	 global-step:121	 l-p:0.1315602958202362
epoch£º6	 i:2 	 global-step:122	 l-p:0.13860024511814117
epoch£º6	 i:3 	 global-step:123	 l-p:0.14717987179756165
epoch£º6	 i:4 	 global-step:124	 l-p:0.1291673183441162
epoch£º6	 i:5 	 global-step:125	 l-p:0.15118077397346497
epoch£º6	 i:6 	 global-step:126	 l-p:0.13693656027317047
epoch£º6	 i:7 	 global-step:127	 l-p:0.12906843423843384
epoch£º6	 i:8 	 global-step:128	 l-p:0.13058078289031982
epoch£º6	 i:9 	 global-step:129	 l-p:0.12362633645534515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:7
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8997, 5.5526, 5.6828],
        [4.8997, 4.9624, 4.9180],
        [4.8997, 4.8997, 4.8997],
        [4.8997, 4.9209, 4.9028]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:7, step:0 
model_pd.l_p.mean(): 0.1269131600856781 
model_pd.l_d.mean(): -20.290233612060547 
model_pd.lagr.mean(): -20.163320541381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4325], device='cuda:0')), ('power', tensor([-21.1176], device='cuda:0'))])
epoch£º7	 i:0 	 global-step:140	 l-p:0.1269131600856781
epoch£º7	 i:1 	 global-step:141	 l-p:0.11748652905225754
epoch£º7	 i:2 	 global-step:142	 l-p:0.1348828822374344
epoch£º7	 i:3 	 global-step:143	 l-p:0.13659480214118958
epoch£º7	 i:4 	 global-step:144	 l-p:0.14762327075004578
epoch£º7	 i:5 	 global-step:145	 l-p:3.593973398208618
epoch£º7	 i:6 	 global-step:146	 l-p:0.12868492305278778
epoch£º7	 i:7 	 global-step:147	 l-p:0.13810276985168457
epoch£º7	 i:8 	 global-step:148	 l-p:0.1635686457157135
epoch£º7	 i:9 	 global-step:149	 l-p:0.13958828151226044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:8
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7086, 4.7086, 4.7086],
        [4.7086, 4.7087, 4.7086],
        [4.7086, 5.5089, 5.7876],
        [4.7086, 4.8519, 4.7821]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:8, step:0 
model_pd.l_p.mean(): 0.13321112096309662 
model_pd.l_d.mean(): -18.60624885559082 
model_pd.lagr.mean(): -18.473037719726562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5675], device='cuda:0')), ('power', tensor([-19.5419], device='cuda:0'))])
epoch£º8	 i:0 	 global-step:160	 l-p:0.13321112096309662
epoch£º8	 i:1 	 global-step:161	 l-p:0.09418945014476776
epoch£º8	 i:2 	 global-step:162	 l-p:0.13871243596076965
epoch£º8	 i:3 	 global-step:163	 l-p:0.1417110562324524
epoch£º8	 i:4 	 global-step:164	 l-p:0.1269940584897995
epoch£º8	 i:5 	 global-step:165	 l-p:0.14698320627212524
epoch£º8	 i:6 	 global-step:166	 l-p:0.12891386449337006
epoch£º8	 i:7 	 global-step:167	 l-p:0.1298888772726059
epoch£º8	 i:8 	 global-step:168	 l-p:0.13997717201709747
epoch£º8	 i:9 	 global-step:169	 l-p:0.13983552157878876
====================================================================================================
====================================================================================================
====================================================================================================

epoch:9
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8737, 4.8755, 4.8738],
        [4.8737, 4.8757, 4.8738],
        [4.8737, 5.0219, 4.9496],
        [4.8737, 4.8945, 4.8768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:9, step:0 
model_pd.l_p.mean(): 0.13453277945518494 
model_pd.l_d.mean(): -17.52029037475586 
model_pd.lagr.mean(): -17.385757446289062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5526], device='cuda:0')), ('power', tensor([-18.4202], device='cuda:0'))])
epoch£º9	 i:0 	 global-step:180	 l-p:0.13453277945518494
epoch£º9	 i:1 	 global-step:181	 l-p:0.12755951285362244
epoch£º9	 i:2 	 global-step:182	 l-p:0.13793876767158508
epoch£º9	 i:3 	 global-step:183	 l-p:0.12984102964401245
epoch£º9	 i:4 	 global-step:184	 l-p:0.12048095464706421
epoch£º9	 i:5 	 global-step:185	 l-p:0.14611761271953583
epoch£º9	 i:6 	 global-step:186	 l-p:0.16321857273578644
epoch£º9	 i:7 	 global-step:187	 l-p:0.12874849140644073
epoch£º9	 i:8 	 global-step:188	 l-p:0.12757398188114166
epoch£º9	 i:9 	 global-step:189	 l-p:0.10954371839761734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:10
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8109, 6.0359, 6.7620],
        [4.8109, 5.6466, 5.9486],
        [4.8109, 4.8160, 4.8113],
        [4.8109, 4.8109, 4.8109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:10, step:0 
model_pd.l_p.mean(): 0.15186400711536407 
model_pd.l_d.mean(): -20.063068389892578 
model_pd.lagr.mean(): -19.911205291748047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4818], device='cuda:0')), ('power', tensor([-20.9373], device='cuda:0'))])
epoch£º10	 i:0 	 global-step:200	 l-p:0.15186400711536407
epoch£º10	 i:1 	 global-step:201	 l-p:0.13907670974731445
epoch£º10	 i:2 	 global-step:202	 l-p:0.11484933644533157
epoch£º10	 i:3 	 global-step:203	 l-p:0.1323915421962738
epoch£º10	 i:4 	 global-step:204	 l-p:0.1326514333486557
epoch£º10	 i:5 	 global-step:205	 l-p:0.13942541182041168
epoch£º10	 i:6 	 global-step:206	 l-p:0.14921295642852783
epoch£º10	 i:7 	 global-step:207	 l-p:0.16363048553466797
epoch£º10	 i:8 	 global-step:208	 l-p:-0.12437297403812408
epoch£º10	 i:9 	 global-step:209	 l-p:0.14140941202640533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:11
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8006, 5.3208, 5.3671],
        [4.8006, 6.1096, 6.9424],
        [4.8006, 5.5127, 5.7048],
        [4.8006, 4.8237, 4.8043]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:11, step:0 
model_pd.l_p.mean(): 0.11777819693088531 
model_pd.l_d.mean(): -19.93014907836914 
model_pd.lagr.mean(): -19.81237030029297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4915], device='cuda:0')), ('power', tensor([-20.8119], device='cuda:0'))])
epoch£º11	 i:0 	 global-step:220	 l-p:0.11777819693088531
epoch£º11	 i:1 	 global-step:221	 l-p:0.1492534726858139
epoch£º11	 i:2 	 global-step:222	 l-p:0.1240251213312149
epoch£º11	 i:3 	 global-step:223	 l-p:0.12629689276218414
epoch£º11	 i:4 	 global-step:224	 l-p:0.1416632980108261
epoch£º11	 i:5 	 global-step:225	 l-p:0.13777081668376923
epoch£º11	 i:6 	 global-step:226	 l-p:0.12515591084957123
epoch£º11	 i:7 	 global-step:227	 l-p:0.19592593610286713
epoch£º11	 i:8 	 global-step:228	 l-p:0.12264569848775864
epoch£º11	 i:9 	 global-step:229	 l-p:0.15399609506130219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:12
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9297, 5.3333, 5.3075],
        [4.9297, 4.9664, 4.9374],
        [4.9297, 4.9609, 4.9356],
        [4.9297, 5.1005, 5.0252]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:12, step:0 
model_pd.l_p.mean(): 0.12222376465797424 
model_pd.l_d.mean(): -19.18491554260254 
model_pd.lagr.mean(): -19.062692642211914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5152], device='cuda:0')), ('power', tensor([-20.0772], device='cuda:0'))])
epoch£º12	 i:0 	 global-step:240	 l-p:0.12222376465797424
epoch£º12	 i:1 	 global-step:241	 l-p:0.13547231256961823
epoch£º12	 i:2 	 global-step:242	 l-p:0.11961772292852402
epoch£º12	 i:3 	 global-step:243	 l-p:0.13583286106586456
epoch£º12	 i:4 	 global-step:244	 l-p:0.13756383955478668
epoch£º12	 i:5 	 global-step:245	 l-p:0.1381254941225052
epoch£º12	 i:6 	 global-step:246	 l-p:0.13514664769172668
epoch£º12	 i:7 	 global-step:247	 l-p:0.1331748366355896
epoch£º12	 i:8 	 global-step:248	 l-p:0.1384812444448471
epoch£º12	 i:9 	 global-step:249	 l-p:0.09729291498661041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:13
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8578, 6.0011, 6.6264],
        [4.8578, 4.8578, 4.8578],
        [4.8578, 4.8611, 4.8579],
        [4.8578, 5.2529, 5.2275]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:13, step:0 
model_pd.l_p.mean(): 0.12300750613212585 
model_pd.l_d.mean(): -18.557350158691406 
model_pd.lagr.mean(): -18.434343338012695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5242], device='cuda:0')), ('power', tensor([-19.4473], device='cuda:0'))])
epoch£º13	 i:0 	 global-step:260	 l-p:0.12300750613212585
epoch£º13	 i:1 	 global-step:261	 l-p:0.1868564635515213
epoch£º13	 i:2 	 global-step:262	 l-p:0.12797711789608002
epoch£º13	 i:3 	 global-step:263	 l-p:0.1303241103887558
epoch£º13	 i:4 	 global-step:264	 l-p:0.13789187371730804
epoch£º13	 i:5 	 global-step:265	 l-p:0.1422492414712906
epoch£º13	 i:6 	 global-step:266	 l-p:0.11201068758964539
epoch£º13	 i:7 	 global-step:267	 l-p:0.1340658962726593
epoch£º13	 i:8 	 global-step:268	 l-p:0.13783618807792664
epoch£º13	 i:9 	 global-step:269	 l-p:0.12308825552463531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:14
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8394, 4.8394, 4.8394],
        [4.8394, 5.7370, 6.1009],
        [4.8394, 4.8735, 4.8464],
        [4.8394, 5.6711, 5.9705]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:14, step:0 
model_pd.l_p.mean(): 0.1499348133802414 
model_pd.l_d.mean(): -18.42837905883789 
model_pd.lagr.mean(): -18.278444290161133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5182], device='cuda:0')), ('power', tensor([-19.3097], device='cuda:0'))])
epoch£º14	 i:0 	 global-step:280	 l-p:0.1499348133802414
epoch£º14	 i:1 	 global-step:281	 l-p:0.12439269572496414
epoch£º14	 i:2 	 global-step:282	 l-p:0.16560618579387665
epoch£º14	 i:3 	 global-step:283	 l-p:0.1400095671415329
epoch£º14	 i:4 	 global-step:284	 l-p:0.13504065573215485
epoch£º14	 i:5 	 global-step:285	 l-p:0.12264902144670486
epoch£º14	 i:6 	 global-step:286	 l-p:0.11743035912513733
epoch£º14	 i:7 	 global-step:287	 l-p:0.13086532056331635
epoch£º14	 i:8 	 global-step:288	 l-p:0.13037782907485962
epoch£º14	 i:9 	 global-step:289	 l-p:-0.02604619972407818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:15
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7564, 4.7874, 4.7625],
        [4.7564, 4.7601, 4.7566],
        [4.7564, 4.9276, 4.8556],
        [4.7564, 4.7670, 4.7575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:15, step:0 
model_pd.l_p.mean(): 0.15474554896354675 
model_pd.l_d.mean(): -20.384502410888672 
model_pd.lagr.mean(): -20.22975730895996 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4647], device='cuda:0')), ('power', tensor([-21.2470], device='cuda:0'))])
epoch£º15	 i:0 	 global-step:300	 l-p:0.15474554896354675
epoch£º15	 i:1 	 global-step:301	 l-p:0.1601480096578598
epoch£º15	 i:2 	 global-step:302	 l-p:0.12358193844556808
epoch£º15	 i:3 	 global-step:303	 l-p:0.1438024789094925
epoch£º15	 i:4 	 global-step:304	 l-p:0.15779408812522888
epoch£º15	 i:5 	 global-step:305	 l-p:0.1456502377986908
epoch£º15	 i:6 	 global-step:306	 l-p:0.11899888515472412
epoch£º15	 i:7 	 global-step:307	 l-p:0.1423283815383911
epoch£º15	 i:8 	 global-step:308	 l-p:0.13303370773792267
epoch£º15	 i:9 	 global-step:309	 l-p:0.20450639724731445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:16
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8643, 4.8662, 4.8644],
        [4.8643, 4.8904, 4.8689],
        [4.8643, 4.8647, 4.8643],
        [4.8643, 5.2214, 5.1815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:16, step:0 
model_pd.l_p.mean(): 0.12048551440238953 
model_pd.l_d.mean(): -18.84018325805664 
model_pd.lagr.mean(): -18.719697952270508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4854], device='cuda:0')), ('power', tensor([-19.6953], device='cuda:0'))])
epoch£º16	 i:0 	 global-step:320	 l-p:0.12048551440238953
epoch£º16	 i:1 	 global-step:321	 l-p:0.11616601794958115
epoch£º16	 i:2 	 global-step:322	 l-p:0.1507081687450409
epoch£º16	 i:3 	 global-step:323	 l-p:0.13504715263843536
epoch£º16	 i:4 	 global-step:324	 l-p:0.12404438108205795
epoch£º16	 i:5 	 global-step:325	 l-p:0.11437845230102539
epoch£º16	 i:6 	 global-step:326	 l-p:0.1331661492586136
epoch£º16	 i:7 	 global-step:327	 l-p:0.15229082107543945
epoch£º16	 i:8 	 global-step:328	 l-p:0.1295456886291504
epoch£º16	 i:9 	 global-step:329	 l-p:0.1283268928527832
====================================================================================================
====================================================================================================
====================================================================================================

epoch:17
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9148, 5.8438, 6.2328],
        [4.9148, 5.2293, 5.1732],
        [4.9148, 4.9149, 4.9148],
        [4.9148, 5.8316, 6.2083]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:17, step:0 
model_pd.l_p.mean(): 0.15870122611522675 
model_pd.l_d.mean(): -20.015905380249023 
model_pd.lagr.mean(): -19.85720443725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4497], device='cuda:0')), ('power', tensor([-20.8559], device='cuda:0'))])
epoch£º17	 i:0 	 global-step:340	 l-p:0.15870122611522675
epoch£º17	 i:1 	 global-step:341	 l-p:0.12112598866224289
epoch£º17	 i:2 	 global-step:342	 l-p:0.13697052001953125
epoch£º17	 i:3 	 global-step:343	 l-p:0.13912618160247803
epoch£º17	 i:4 	 global-step:344	 l-p:0.13112735748291016
epoch£º17	 i:5 	 global-step:345	 l-p:0.12582305073738098
epoch£º17	 i:6 	 global-step:346	 l-p:0.13766804337501526
epoch£º17	 i:7 	 global-step:347	 l-p:0.14249174296855927
epoch£º17	 i:8 	 global-step:348	 l-p:0.13055182993412018
epoch£º17	 i:9 	 global-step:349	 l-p:0.13331276178359985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:18
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7533, 4.8014, 4.7659],
        [4.7533, 4.7536, 4.7533],
        [4.7533, 4.8647, 4.8029],
        [4.7533, 4.7657, 4.7547]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:18, step:0 
model_pd.l_p.mean(): 0.14220760762691498 
model_pd.l_d.mean(): -19.897676467895508 
model_pd.lagr.mean(): -19.755468368530273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5123], device='cuda:0')), ('power', tensor([-20.8004], device='cuda:0'))])
epoch£º18	 i:0 	 global-step:360	 l-p:0.14220760762691498
epoch£º18	 i:1 	 global-step:361	 l-p:0.19414973258972168
epoch£º18	 i:2 	 global-step:362	 l-p:0.13479334115982056
epoch£º18	 i:3 	 global-step:363	 l-p:0.1730833798646927
epoch£º18	 i:4 	 global-step:364	 l-p:0.13130290806293488
epoch£º18	 i:5 	 global-step:365	 l-p:0.13695691525936127
epoch£º18	 i:6 	 global-step:366	 l-p:0.1332370489835739
epoch£º18	 i:7 	 global-step:367	 l-p:0.0429278165102005
epoch£º18	 i:8 	 global-step:368	 l-p:0.13950838148593903
epoch£º18	 i:9 	 global-step:369	 l-p:0.1410810500383377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:19
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7806, 4.9532, 4.8817],
        [4.7806, 4.8101, 4.7863],
        [4.7806, 4.7806, 4.7806],
        [4.7806, 4.9857, 4.9143]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:19, step:0 
model_pd.l_p.mean(): 0.1465989500284195 
model_pd.l_d.mean(): -20.357336044311523 
model_pd.lagr.mean(): -20.210737228393555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4510], device='cuda:0')), ('power', tensor([-21.2052], device='cuda:0'))])
epoch£º19	 i:0 	 global-step:380	 l-p:0.1465989500284195
epoch£º19	 i:1 	 global-step:381	 l-p:0.14527656137943268
epoch£º19	 i:2 	 global-step:382	 l-p:0.13275626301765442
epoch£º19	 i:3 	 global-step:383	 l-p:0.13093316555023193
epoch£º19	 i:4 	 global-step:384	 l-p:0.12443291395902634
epoch£º19	 i:5 	 global-step:385	 l-p:0.14446644484996796
epoch£º19	 i:6 	 global-step:386	 l-p:0.12928687036037445
epoch£º19	 i:7 	 global-step:387	 l-p:0.1242438405752182
epoch£º19	 i:8 	 global-step:388	 l-p:0.10942995548248291
epoch£º19	 i:9 	 global-step:389	 l-p:0.16883300244808197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:20
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9303, 4.9330, 4.9305],
        [4.9303, 5.3530, 5.3416],
        [4.9303, 4.9726, 4.9403],
        [4.9303, 4.9913, 4.9485]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:20, step:0 
model_pd.l_p.mean(): 0.12406431138515472 
model_pd.l_d.mean(): -20.310691833496094 
model_pd.lagr.mean(): -20.186628341674805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4214], device='cuda:0')), ('power', tensor([-21.1269], device='cuda:0'))])
epoch£º20	 i:0 	 global-step:400	 l-p:0.12406431138515472
epoch£º20	 i:1 	 global-step:401	 l-p:0.12725573778152466
epoch£º20	 i:2 	 global-step:402	 l-p:0.1217140331864357
epoch£º20	 i:3 	 global-step:403	 l-p:0.18929226696491241
epoch£º20	 i:4 	 global-step:404	 l-p:0.1399947851896286
epoch£º20	 i:5 	 global-step:405	 l-p:0.13850504159927368
epoch£º20	 i:6 	 global-step:406	 l-p:0.12436217814683914
epoch£º20	 i:7 	 global-step:407	 l-p:0.13034431636333466
epoch£º20	 i:8 	 global-step:408	 l-p:0.12495024502277374
epoch£º20	 i:9 	 global-step:409	 l-p:0.1272892951965332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:21
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9305, 6.3456, 7.2993],
        [4.9305, 4.9921, 4.9490],
        [4.9305, 4.9305, 4.9305],
        [4.9305, 5.0307, 4.9714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:21, step:0 
model_pd.l_p.mean(): 0.1321074664592743 
model_pd.l_d.mean(): -20.15224838256836 
model_pd.lagr.mean(): -20.0201416015625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4383], device='cuda:0')), ('power', tensor([-20.9830], device='cuda:0'))])
epoch£º21	 i:0 	 global-step:420	 l-p:0.1321074664592743
epoch£º21	 i:1 	 global-step:421	 l-p:0.12820328772068024
epoch£º21	 i:2 	 global-step:422	 l-p:0.13581880927085876
epoch£º21	 i:3 	 global-step:423	 l-p:0.1299152672290802
epoch£º21	 i:4 	 global-step:424	 l-p:0.16044454276561737
epoch£º21	 i:5 	 global-step:425	 l-p:0.13445840775966644
epoch£º21	 i:6 	 global-step:426	 l-p:0.15661878883838654
epoch£º21	 i:7 	 global-step:427	 l-p:0.16106374561786652
epoch£º21	 i:8 	 global-step:428	 l-p:0.18458291888237
epoch£º21	 i:9 	 global-step:429	 l-p:0.13455374538898468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:22
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7049, 4.8011, 4.7447],
        [4.7049, 4.9854, 4.9294],
        [4.7049, 4.7070, 4.7049],
        [4.7049, 4.7350, 4.7108]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:22, step:0 
model_pd.l_p.mean(): 0.18089957535266876 
model_pd.l_d.mean(): -20.365995407104492 
model_pd.lagr.mean(): -20.185096740722656 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4818], device='cuda:0')), ('power', tensor([-21.2458], device='cuda:0'))])
epoch£º22	 i:0 	 global-step:440	 l-p:0.18089957535266876
epoch£º22	 i:1 	 global-step:441	 l-p:0.13948877155780792
epoch£º22	 i:2 	 global-step:442	 l-p:0.1341520994901657
epoch£º22	 i:3 	 global-step:443	 l-p:0.13339610397815704
epoch£º22	 i:4 	 global-step:444	 l-p:0.11401382088661194
epoch£º22	 i:5 	 global-step:445	 l-p:0.14585691690444946
epoch£º22	 i:6 	 global-step:446	 l-p:0.11964737623929977
epoch£º22	 i:7 	 global-step:447	 l-p:0.139297753572464
epoch£º22	 i:8 	 global-step:448	 l-p:0.135220468044281
epoch£º22	 i:9 	 global-step:449	 l-p:0.12017128616571426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:23
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0081, 6.4428, 7.4071],
        [5.0081, 5.5637, 5.6280],
        [5.0081, 5.0087, 5.0081],
        [5.0081, 5.9409, 6.3278]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:23, step:0 
model_pd.l_p.mean(): 0.1045539379119873 
model_pd.l_d.mean(): -19.83930015563965 
model_pd.lagr.mean(): -19.7347469329834 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4411], device='cuda:0')), ('power', tensor([-20.6671], device='cuda:0'))])
epoch£º23	 i:0 	 global-step:460	 l-p:0.1045539379119873
epoch£º23	 i:1 	 global-step:461	 l-p:0.13121925294399261
epoch£º23	 i:2 	 global-step:462	 l-p:0.13759589195251465
epoch£º23	 i:3 	 global-step:463	 l-p:0.11523927748203278
epoch£º23	 i:4 	 global-step:464	 l-p:0.13728076219558716
epoch£º23	 i:5 	 global-step:465	 l-p:0.13085097074508667
epoch£º23	 i:6 	 global-step:466	 l-p:0.15012741088867188
epoch£º23	 i:7 	 global-step:467	 l-p:0.12758654356002808
epoch£º23	 i:8 	 global-step:468	 l-p:0.1421702653169632
epoch£º23	 i:9 	 global-step:469	 l-p:0.14453595876693726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:24
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8415, 5.2905, 5.3007],
        [4.8415, 5.1257, 5.0669],
        [4.8415, 4.9219, 4.8707],
        [4.8415, 4.8415, 4.8415]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:24, step:0 
model_pd.l_p.mean(): 0.13746458292007446 
model_pd.l_d.mean(): -20.788225173950195 
model_pd.lagr.mean(): -20.650760650634766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3978], device='cuda:0')), ('power', tensor([-21.5889], device='cuda:0'))])
epoch£º24	 i:0 	 global-step:480	 l-p:0.13746458292007446
epoch£º24	 i:1 	 global-step:481	 l-p:0.14466260373592377
epoch£º24	 i:2 	 global-step:482	 l-p:0.12076643854379654
epoch£º24	 i:3 	 global-step:483	 l-p:0.11947614699602127
epoch£º24	 i:4 	 global-step:484	 l-p:0.1403537094593048
epoch£º24	 i:5 	 global-step:485	 l-p:0.11619937419891357
epoch£º24	 i:6 	 global-step:486	 l-p:0.1423410326242447
epoch£º24	 i:7 	 global-step:487	 l-p:0.14003777503967285
epoch£º24	 i:8 	 global-step:488	 l-p:0.18100424110889435
epoch£º24	 i:9 	 global-step:489	 l-p:0.12379541248083115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:25
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8980, 4.8980, 4.8980],
        [4.8980, 4.8980, 4.8980],
        [4.8980, 4.8980, 4.8980],
        [4.8980, 6.0289, 6.6475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:25, step:0 
model_pd.l_p.mean(): 0.1330510526895523 
model_pd.l_d.mean(): -18.20371437072754 
model_pd.lagr.mean(): -18.070663452148438 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5730], device='cuda:0')), ('power', tensor([-19.1376], device='cuda:0'))])
epoch£º25	 i:0 	 global-step:500	 l-p:0.1330510526895523
epoch£º25	 i:1 	 global-step:501	 l-p:0.13396243751049042
epoch£º25	 i:2 	 global-step:502	 l-p:0.14122946560382843
epoch£º25	 i:3 	 global-step:503	 l-p:0.12349500507116318
epoch£º25	 i:4 	 global-step:504	 l-p:0.28167957067489624
epoch£º25	 i:5 	 global-step:505	 l-p:0.14605234563350677
epoch£º25	 i:6 	 global-step:506	 l-p:0.12947280704975128
epoch£º25	 i:7 	 global-step:507	 l-p:0.12791021168231964
epoch£º25	 i:8 	 global-step:508	 l-p:0.1480344980955124
epoch£º25	 i:9 	 global-step:509	 l-p:0.12818944454193115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:26
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8380, 5.0279, 4.9567],
        [4.8380, 5.4360, 5.5466],
        [4.8380, 4.8970, 4.8557],
        [4.8380, 5.3537, 5.4062]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:26, step:0 
model_pd.l_p.mean(): 0.15482550859451294 
model_pd.l_d.mean(): -20.40583038330078 
model_pd.lagr.mean(): -20.251005172729492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4437], device='cuda:0')), ('power', tensor([-21.2469], device='cuda:0'))])
epoch£º26	 i:0 	 global-step:520	 l-p:0.15482550859451294
epoch£º26	 i:1 	 global-step:521	 l-p:0.13785654306411743
epoch£º26	 i:2 	 global-step:522	 l-p:0.13874399662017822
epoch£º26	 i:3 	 global-step:523	 l-p:0.12667307257652283
epoch£º26	 i:4 	 global-step:524	 l-p:-0.006640257779508829
epoch£º26	 i:5 	 global-step:525	 l-p:0.15591943264007568
epoch£º26	 i:6 	 global-step:526	 l-p:0.14105840027332306
epoch£º26	 i:7 	 global-step:527	 l-p:0.14322654902935028
epoch£º26	 i:8 	 global-step:528	 l-p:0.1402582973241806
epoch£º26	 i:9 	 global-step:529	 l-p:0.1292801797389984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:27
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8569, 4.9725, 4.9099],
        [4.8569, 5.1866, 5.1424],
        [4.8569, 4.8569, 4.8569],
        [4.8569, 4.8594, 4.8570]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:27, step:0 
model_pd.l_p.mean(): 0.12456659227609634 
model_pd.l_d.mean(): -18.112218856811523 
model_pd.lagr.mean(): -17.987651824951172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5407], device='cuda:0')), ('power', tensor([-19.0109], device='cuda:0'))])
epoch£º27	 i:0 	 global-step:540	 l-p:0.12456659227609634
epoch£º27	 i:1 	 global-step:541	 l-p:0.13089346885681152
epoch£º27	 i:2 	 global-step:542	 l-p:0.11914839595556259
epoch£º27	 i:3 	 global-step:543	 l-p:0.13946670293807983
epoch£º27	 i:4 	 global-step:544	 l-p:0.12975473701953888
epoch£º27	 i:5 	 global-step:545	 l-p:0.13794125616550446
epoch£º27	 i:6 	 global-step:546	 l-p:0.13376441597938538
epoch£º27	 i:7 	 global-step:547	 l-p:0.1461104154586792
epoch£º27	 i:8 	 global-step:548	 l-p:0.28027287125587463
epoch£º27	 i:9 	 global-step:549	 l-p:0.13422837853431702
====================================================================================================
====================================================================================================
====================================================================================================

epoch:28
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8425, 5.0409, 4.9704],
        [4.8425, 5.4369, 5.5459],
        [4.8425, 4.8905, 4.8552],
        [4.8425, 6.1483, 6.9892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:28, step:0 
model_pd.l_p.mean(): 0.13190031051635742 
model_pd.l_d.mean(): -19.747665405273438 
model_pd.lagr.mean(): -19.615764617919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4967], device='cuda:0')), ('power', tensor([-20.6314], device='cuda:0'))])
epoch£º28	 i:0 	 global-step:560	 l-p:0.13190031051635742
epoch£º28	 i:1 	 global-step:561	 l-p:0.15929752588272095
epoch£º28	 i:2 	 global-step:562	 l-p:0.12898124754428864
epoch£º28	 i:3 	 global-step:563	 l-p:0.12719665467739105
epoch£º28	 i:4 	 global-step:564	 l-p:0.13586978614330292
epoch£º28	 i:5 	 global-step:565	 l-p:0.1562623679637909
epoch£º28	 i:6 	 global-step:566	 l-p:0.12274210900068283
epoch£º28	 i:7 	 global-step:567	 l-p:0.12279241532087326
epoch£º28	 i:8 	 global-step:568	 l-p:0.12731574475765228
epoch£º28	 i:9 	 global-step:569	 l-p:0.1377333253622055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:29
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0190, 5.0190, 5.0190],
        [5.0190, 5.2241, 5.1505],
        [5.0190, 5.6689, 5.8055],
        [5.0190, 5.0233, 5.0192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:29, step:0 
model_pd.l_p.mean(): 0.1358269602060318 
model_pd.l_d.mean(): -19.511545181274414 
model_pd.lagr.mean(): -19.37571907043457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4460], device='cuda:0')), ('power', tensor([-20.3384], device='cuda:0'))])
epoch£º29	 i:0 	 global-step:580	 l-p:0.1358269602060318
epoch£º29	 i:1 	 global-step:581	 l-p:0.1070399284362793
epoch£º29	 i:2 	 global-step:582	 l-p:0.11940096318721771
epoch£º29	 i:3 	 global-step:583	 l-p:0.13809190690517426
epoch£º29	 i:4 	 global-step:584	 l-p:0.13165901601314545
epoch£º29	 i:5 	 global-step:585	 l-p:0.13963903486728668
epoch£º29	 i:6 	 global-step:586	 l-p:0.20349346101284027
epoch£º29	 i:7 	 global-step:587	 l-p:0.12456328421831131
epoch£º29	 i:8 	 global-step:588	 l-p:0.1283804029226303
epoch£º29	 i:9 	 global-step:589	 l-p:0.13200713694095612
====================================================================================================
====================================================================================================
====================================================================================================

epoch:30
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8416, 4.8432, 4.8417],
        [4.8416, 6.1302, 6.9518],
        [4.8416, 5.0536, 4.9844],
        [4.8416, 6.0115, 6.6901]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:30, step:0 
model_pd.l_p.mean(): 0.1495833694934845 
model_pd.l_d.mean(): -18.799638748168945 
model_pd.lagr.mean(): -18.650054931640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5591], device='cuda:0')), ('power', tensor([-19.7302], device='cuda:0'))])
epoch£º30	 i:0 	 global-step:600	 l-p:0.1495833694934845
epoch£º30	 i:1 	 global-step:601	 l-p:0.12171147018671036
epoch£º30	 i:2 	 global-step:602	 l-p:0.12703467905521393
epoch£º30	 i:3 	 global-step:603	 l-p:-0.1003183126449585
epoch£º30	 i:4 	 global-step:604	 l-p:0.13050203025341034
epoch£º30	 i:5 	 global-step:605	 l-p:0.1659737378358841
epoch£º30	 i:6 	 global-step:606	 l-p:0.13714724779129028
epoch£º30	 i:7 	 global-step:607	 l-p:0.13003601133823395
epoch£º30	 i:8 	 global-step:608	 l-p:0.12919752299785614
epoch£º30	 i:9 	 global-step:609	 l-p:0.15413497388362885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:31
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7297, 4.7298, 4.7297],
        [4.7297, 4.7377, 4.7305],
        [4.7297, 6.0483, 6.9312],
        [4.7297, 4.7836, 4.7455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:31, step:0 
model_pd.l_p.mean(): 0.13342317938804626 
model_pd.l_d.mean(): -17.924278259277344 
model_pd.lagr.mean(): -17.790855407714844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6312], device='cuda:0')), ('power', tensor([-18.9132], device='cuda:0'))])
epoch£º31	 i:0 	 global-step:620	 l-p:0.13342317938804626
epoch£º31	 i:1 	 global-step:621	 l-p:0.16901908814907074
epoch£º31	 i:2 	 global-step:622	 l-p:0.1203412339091301
epoch£º31	 i:3 	 global-step:623	 l-p:0.13260024785995483
epoch£º31	 i:4 	 global-step:624	 l-p:0.13140980899333954
epoch£º31	 i:5 	 global-step:625	 l-p:0.12628047168254852
epoch£º31	 i:6 	 global-step:626	 l-p:0.1482732594013214
epoch£º31	 i:7 	 global-step:627	 l-p:0.1943303644657135
epoch£º31	 i:8 	 global-step:628	 l-p:0.14663638174533844
epoch£º31	 i:9 	 global-step:629	 l-p:0.16265937685966492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:32
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8204, 6.2530, 7.2642],
        [4.8204, 5.7503, 6.1683],
        [4.8204, 4.8769, 4.8372],
        [4.8204, 4.8219, 4.8205]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:32, step:0 
model_pd.l_p.mean(): 0.13449984788894653 
model_pd.l_d.mean(): -19.75826072692871 
model_pd.lagr.mean(): -19.623760223388672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4763], device='cuda:0')), ('power', tensor([-20.6210], device='cuda:0'))])
epoch£º32	 i:0 	 global-step:640	 l-p:0.13449984788894653
epoch£º32	 i:1 	 global-step:641	 l-p:0.1399911344051361
epoch£º32	 i:2 	 global-step:642	 l-p:0.15120895206928253
epoch£º32	 i:3 	 global-step:643	 l-p:0.12232216447591782
epoch£º32	 i:4 	 global-step:644	 l-p:0.1764354705810547
epoch£º32	 i:5 	 global-step:645	 l-p:0.13882817327976227
epoch£º32	 i:6 	 global-step:646	 l-p:0.1536041647195816
epoch£º32	 i:7 	 global-step:647	 l-p:0.26236337423324585
epoch£º32	 i:8 	 global-step:648	 l-p:0.1190275177359581
epoch£º32	 i:9 	 global-step:649	 l-p:0.1314990520477295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:33
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9114, 4.9117, 4.9114],
        [4.9114, 5.1562, 5.0899],
        [4.9114, 4.9121, 4.9114],
        [4.9114, 5.1564, 5.0902]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:33, step:0 
model_pd.l_p.mean(): 0.11754690110683441 
model_pd.l_d.mean(): -20.188016891479492 
model_pd.lagr.mean(): -20.070470809936523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4443], device='cuda:0')), ('power', tensor([-21.0256], device='cuda:0'))])
epoch£º33	 i:0 	 global-step:660	 l-p:0.11754690110683441
epoch£º33	 i:1 	 global-step:661	 l-p:0.19598256051540375
epoch£º33	 i:2 	 global-step:662	 l-p:0.13850443065166473
epoch£º33	 i:3 	 global-step:663	 l-p:0.11721782386302948
epoch£º33	 i:4 	 global-step:664	 l-p:0.1396005004644394
epoch£º33	 i:5 	 global-step:665	 l-p:0.1327798068523407
epoch£º33	 i:6 	 global-step:666	 l-p:0.14769713580608368
epoch£º33	 i:7 	 global-step:667	 l-p:0.15611153841018677
epoch£º33	 i:8 	 global-step:668	 l-p:0.1344776302576065
epoch£º33	 i:9 	 global-step:669	 l-p:0.1917855441570282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:34
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7650, 5.3446, 5.4528],
        [4.7650, 4.7652, 4.7650],
        [4.7650, 5.0357, 4.9794],
        [4.7650, 4.8187, 4.7807]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:34, step:0 
model_pd.l_p.mean(): 0.14855217933654785 
model_pd.l_d.mean(): -20.245502471923828 
model_pd.lagr.mean(): -20.09695053100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4809], device='cuda:0')), ('power', tensor([-21.1221], device='cuda:0'))])
epoch£º34	 i:0 	 global-step:680	 l-p:0.14855217933654785
epoch£º34	 i:1 	 global-step:681	 l-p:0.14329160749912262
epoch£º34	 i:2 	 global-step:682	 l-p:0.13197389245033264
epoch£º34	 i:3 	 global-step:683	 l-p:0.12682463228702545
epoch£º34	 i:4 	 global-step:684	 l-p:0.15594103932380676
epoch£º34	 i:5 	 global-step:685	 l-p:0.14071789383888245
epoch£º34	 i:6 	 global-step:686	 l-p:0.13909009099006653
epoch£º34	 i:7 	 global-step:687	 l-p:0.1361132562160492
epoch£º34	 i:8 	 global-step:688	 l-p:0.12391094118356705
epoch£º34	 i:9 	 global-step:689	 l-p:0.16998505592346191
====================================================================================================
====================================================================================================
====================================================================================================

epoch:35
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9343, 5.3890, 5.4026],
        [4.9343, 4.9343, 4.9342],
        [4.9343, 5.0663, 5.0002],
        [4.9343, 6.2903, 7.1812]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:35, step:0 
model_pd.l_p.mean(): 0.11928943544626236 
model_pd.l_d.mean(): -19.74420928955078 
model_pd.lagr.mean(): -19.624919891357422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4773], device='cuda:0')), ('power', tensor([-20.6077], device='cuda:0'))])
epoch£º35	 i:0 	 global-step:700	 l-p:0.11928943544626236
epoch£º35	 i:1 	 global-step:701	 l-p:0.13338232040405273
epoch£º35	 i:2 	 global-step:702	 l-p:0.1513938456773758
epoch£º35	 i:3 	 global-step:703	 l-p:0.1280922293663025
epoch£º35	 i:4 	 global-step:704	 l-p:0.11593813449144363
epoch£º35	 i:5 	 global-step:705	 l-p:0.1318429708480835
epoch£º35	 i:6 	 global-step:706	 l-p:0.14819787442684174
epoch£º35	 i:7 	 global-step:707	 l-p:0.1271934062242508
epoch£º35	 i:8 	 global-step:708	 l-p:0.10716982930898666
epoch£º35	 i:9 	 global-step:709	 l-p:0.12216419726610184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:36
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8912, 4.8912, 4.8912],
        [4.8912, 4.8912, 4.8912],
        [4.8912, 5.5273, 5.6691],
        [4.8912, 4.9370, 4.9031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:36, step:0 
model_pd.l_p.mean(): 0.1349772810935974 
model_pd.l_d.mean(): -18.880870819091797 
model_pd.lagr.mean(): -18.745893478393555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5108], device='cuda:0')), ('power', tensor([-19.7630], device='cuda:0'))])
epoch£º36	 i:0 	 global-step:720	 l-p:0.1349772810935974
epoch£º36	 i:1 	 global-step:721	 l-p:0.11782009154558182
epoch£º36	 i:2 	 global-step:722	 l-p:0.14313450455665588
epoch£º36	 i:3 	 global-step:723	 l-p:0.11882004141807556
epoch£º36	 i:4 	 global-step:724	 l-p:0.13415098190307617
epoch£º36	 i:5 	 global-step:725	 l-p:0.15038686990737915
epoch£º36	 i:6 	 global-step:726	 l-p:0.15227636694908142
epoch£º36	 i:7 	 global-step:727	 l-p:0.13849739730358124
epoch£º36	 i:8 	 global-step:728	 l-p:-1.9902576208114624
epoch£º36	 i:9 	 global-step:729	 l-p:0.1311369091272354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:37
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8249, 4.8309, 4.8254],
        [4.8249, 6.1056, 6.9268],
        [4.8249, 5.2139, 5.2003],
        [4.8249, 5.3129, 5.3547]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:37, step:0 
model_pd.l_p.mean(): 0.128378227353096 
model_pd.l_d.mean(): -18.38649559020996 
model_pd.lagr.mean(): -18.25811767578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5705], device='cuda:0')), ('power', tensor([-19.3212], device='cuda:0'))])
epoch£º37	 i:0 	 global-step:740	 l-p:0.128378227353096
epoch£º37	 i:1 	 global-step:741	 l-p:0.1266232430934906
epoch£º37	 i:2 	 global-step:742	 l-p:0.22842992842197418
epoch£º37	 i:3 	 global-step:743	 l-p:0.1260949820280075
epoch£º37	 i:4 	 global-step:744	 l-p:0.14017097651958466
epoch£º37	 i:5 	 global-step:745	 l-p:0.13201020658016205
epoch£º37	 i:6 	 global-step:746	 l-p:0.12672758102416992
epoch£º37	 i:7 	 global-step:747	 l-p:0.11788610368967056
epoch£º37	 i:8 	 global-step:748	 l-p:0.12281244248151779
epoch£º37	 i:9 	 global-step:749	 l-p:0.11865804344415665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:38
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9127, 5.0225, 4.9621],
        [4.9127, 4.9779, 4.9339],
        [4.9127, 4.9379, 4.9173],
        [4.9127, 4.9749, 4.9323]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:38, step:0 
model_pd.l_p.mean(): 0.15942484140396118 
model_pd.l_d.mean(): -19.731544494628906 
model_pd.lagr.mean(): -19.572118759155273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4822], device='cuda:0')), ('power', tensor([-20.6000], device='cuda:0'))])
epoch£º38	 i:0 	 global-step:760	 l-p:0.15942484140396118
epoch£º38	 i:1 	 global-step:761	 l-p:0.14100037515163422
epoch£º38	 i:2 	 global-step:762	 l-p:0.12883219122886658
epoch£º38	 i:3 	 global-step:763	 l-p:0.12412212789058685
epoch£º38	 i:4 	 global-step:764	 l-p:0.13281235098838806
epoch£º38	 i:5 	 global-step:765	 l-p:0.12320210039615631
epoch£º38	 i:6 	 global-step:766	 l-p:0.1745428591966629
epoch£º38	 i:7 	 global-step:767	 l-p:0.13884514570236206
epoch£º38	 i:8 	 global-step:768	 l-p:0.14511659741401672
epoch£º38	 i:9 	 global-step:769	 l-p:0.1223621740937233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:39
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7759, 4.7759, 4.7759],
        [4.7759, 5.8211, 6.3756],
        [4.7759, 4.7759, 4.7759],
        [4.7759, 4.7778, 4.7760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:39, step:0 
model_pd.l_p.mean(): 0.13733190298080444 
model_pd.l_d.mean(): -20.61395835876465 
model_pd.lagr.mean(): -20.476627349853516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4396], device='cuda:0')), ('power', tensor([-21.4548], device='cuda:0'))])
epoch£º39	 i:0 	 global-step:780	 l-p:0.13733190298080444
epoch£º39	 i:1 	 global-step:781	 l-p:0.1329183727502823
epoch£º39	 i:2 	 global-step:782	 l-p:0.10598346590995789
epoch£º39	 i:3 	 global-step:783	 l-p:0.1383701115846634
epoch£º39	 i:4 	 global-step:784	 l-p:0.16713540256023407
epoch£º39	 i:5 	 global-step:785	 l-p:0.1900617927312851
epoch£º39	 i:6 	 global-step:786	 l-p:0.14471220970153809
epoch£º39	 i:7 	 global-step:787	 l-p:0.1435856968164444
epoch£º39	 i:8 	 global-step:788	 l-p:0.13650628924369812
epoch£º39	 i:9 	 global-step:789	 l-p:0.12330319732427597
====================================================================================================
====================================================================================================
====================================================================================================

epoch:40
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9677, 4.9679, 4.9677],
        [4.9677, 4.9863, 4.9705],
        [4.9677, 5.0802, 5.0188],
        [4.9677, 4.9752, 4.9684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:40, step:0 
model_pd.l_p.mean(): 0.1116958037018776 
model_pd.l_d.mean(): -20.385908126831055 
model_pd.lagr.mean(): -20.274211883544922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4098], device='cuda:0')), ('power', tensor([-21.1916], device='cuda:0'))])
epoch£º40	 i:0 	 global-step:800	 l-p:0.1116958037018776
epoch£º40	 i:1 	 global-step:801	 l-p:0.129723459482193
epoch£º40	 i:2 	 global-step:802	 l-p:0.11579789966344833
epoch£º40	 i:3 	 global-step:803	 l-p:0.15086959302425385
epoch£º40	 i:4 	 global-step:804	 l-p:0.1231541782617569
epoch£º40	 i:5 	 global-step:805	 l-p:0.11637752503156662
epoch£º40	 i:6 	 global-step:806	 l-p:0.125308096408844
epoch£º40	 i:7 	 global-step:807	 l-p:0.19042091071605682
epoch£º40	 i:8 	 global-step:808	 l-p:0.1286517083644867
epoch£º40	 i:9 	 global-step:809	 l-p:0.14410525560379028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:41
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9452, 4.9478, 4.9453],
        [4.9452, 4.9455, 4.9452],
        [4.9452, 5.1288, 5.0586],
        [4.9452, 5.0598, 4.9981]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:41, step:0 
model_pd.l_p.mean(): 0.14146535098552704 
model_pd.l_d.mean(): -20.310747146606445 
model_pd.lagr.mean(): -20.169281005859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4177], device='cuda:0')), ('power', tensor([-21.1231], device='cuda:0'))])
epoch£º41	 i:0 	 global-step:820	 l-p:0.14146535098552704
epoch£º41	 i:1 	 global-step:821	 l-p:0.12122321128845215
epoch£º41	 i:2 	 global-step:822	 l-p:0.11717050522565842
epoch£º41	 i:3 	 global-step:823	 l-p:0.13195504248142242
epoch£º41	 i:4 	 global-step:824	 l-p:0.20661792159080505
epoch£º41	 i:5 	 global-step:825	 l-p:0.12286628782749176
epoch£º41	 i:6 	 global-step:826	 l-p:0.13028188049793243
epoch£º41	 i:7 	 global-step:827	 l-p:0.1217661052942276
epoch£º41	 i:8 	 global-step:828	 l-p:0.13030794262886047
epoch£º41	 i:9 	 global-step:829	 l-p:0.1616370677947998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:42
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8443, 5.1233, 5.0689],
        [4.8443, 4.8443, 4.8443],
        [4.8443, 6.2088, 7.1351],
        [4.8443, 4.8599, 4.8464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:42, step:0 
model_pd.l_p.mean(): 0.1217503622174263 
model_pd.l_d.mean(): -18.8397159576416 
model_pd.lagr.mean(): -18.717966079711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5206], device='cuda:0')), ('power', tensor([-19.7312], device='cuda:0'))])
epoch£º42	 i:0 	 global-step:840	 l-p:0.1217503622174263
epoch£º42	 i:1 	 global-step:841	 l-p:0.1343744844198227
epoch£º42	 i:2 	 global-step:842	 l-p:0.13887149095535278
epoch£º42	 i:3 	 global-step:843	 l-p:0.20727424323558807
epoch£º42	 i:4 	 global-step:844	 l-p:0.15114736557006836
epoch£º42	 i:5 	 global-step:845	 l-p:0.1461758017539978
epoch£º42	 i:6 	 global-step:846	 l-p:0.13443422317504883
epoch£º42	 i:7 	 global-step:847	 l-p:0.0913650393486023
epoch£º42	 i:8 	 global-step:848	 l-p:0.13778936862945557
epoch£º42	 i:9 	 global-step:849	 l-p:0.1585918515920639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:43
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4331,  0.3277,  1.0000,  0.2480,
          1.0000,  0.7566, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[4.7761, 5.9809, 6.7238],
        [4.7761, 5.3184, 5.4036],
        [4.7761, 5.0750, 5.0292],
        [4.7761, 5.9210, 6.5927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:43, step:0 
model_pd.l_p.mean(): 0.05469410866498947 
model_pd.l_d.mean(): -19.768604278564453 
model_pd.lagr.mean(): -19.713911056518555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5163], device='cuda:0')), ('power', tensor([-20.6730], device='cuda:0'))])
epoch£º43	 i:0 	 global-step:860	 l-p:0.05469410866498947
epoch£º43	 i:1 	 global-step:861	 l-p:0.1478758454322815
epoch£º43	 i:2 	 global-step:862	 l-p:0.13405950367450714
epoch£º43	 i:3 	 global-step:863	 l-p:0.13170680403709412
epoch£º43	 i:4 	 global-step:864	 l-p:0.13965938985347748
epoch£º43	 i:5 	 global-step:865	 l-p:0.12777991592884064
epoch£º43	 i:6 	 global-step:866	 l-p:0.12974010407924652
epoch£º43	 i:7 	 global-step:867	 l-p:0.14066356420516968
epoch£º43	 i:8 	 global-step:868	 l-p:0.11958061158657074
epoch£º43	 i:9 	 global-step:869	 l-p:0.13988259434700012
====================================================================================================
====================================================================================================
====================================================================================================

epoch:44
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8964, 5.0224, 4.9589],
        [4.8964, 4.9034, 4.8970],
        [4.8964, 4.8987, 4.8965],
        [4.8964, 4.8965, 4.8964]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:44, step:0 
model_pd.l_p.mean(): 0.13603360950946808 
model_pd.l_d.mean(): -20.27545928955078 
model_pd.lagr.mean(): -20.13942527770996 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4302], device='cuda:0')), ('power', tensor([-21.1002], device='cuda:0'))])
epoch£º44	 i:0 	 global-step:880	 l-p:0.13603360950946808
epoch£º44	 i:1 	 global-step:881	 l-p:0.1375376582145691
epoch£º44	 i:2 	 global-step:882	 l-p:-0.1387024074792862
epoch£º44	 i:3 	 global-step:883	 l-p:0.13938714563846588
epoch£º44	 i:4 	 global-step:884	 l-p:0.13581444323062897
epoch£º44	 i:5 	 global-step:885	 l-p:0.1290464550256729
epoch£º44	 i:6 	 global-step:886	 l-p:0.11224673688411713
epoch£º44	 i:7 	 global-step:887	 l-p:0.15116360783576965
epoch£º44	 i:8 	 global-step:888	 l-p:0.1651667356491089
epoch£º44	 i:9 	 global-step:889	 l-p:0.1267378181219101
====================================================================================================
====================================================================================================
====================================================================================================

epoch:45
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8645, 4.8646, 4.8645],
        [4.8645, 4.8656, 4.8646],
        [4.8645, 5.0966, 5.0325],
        [4.8645, 4.8645, 4.8645]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:45, step:0 
model_pd.l_p.mean(): 0.13244184851646423 
model_pd.l_d.mean(): -20.662809371948242 
model_pd.lagr.mean(): -20.530366897583008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4106], device='cuda:0')), ('power', tensor([-21.4744], device='cuda:0'))])
epoch£º45	 i:0 	 global-step:900	 l-p:0.13244184851646423
epoch£º45	 i:1 	 global-step:901	 l-p:0.13065017759799957
epoch£º45	 i:2 	 global-step:902	 l-p:0.1114446222782135
epoch£º45	 i:3 	 global-step:903	 l-p:0.12557141482830048
epoch£º45	 i:4 	 global-step:904	 l-p:0.14918076992034912
epoch£º45	 i:5 	 global-step:905	 l-p:-0.04025977849960327
epoch£º45	 i:6 	 global-step:906	 l-p:0.14079919457435608
epoch£º45	 i:7 	 global-step:907	 l-p:0.1413465291261673
epoch£º45	 i:8 	 global-step:908	 l-p:0.15947136282920837
epoch£º45	 i:9 	 global-step:909	 l-p:0.12821626663208008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:46
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8932, 4.8974, 4.8934],
        [4.8932, 4.9196, 4.8981],
        [4.8932, 4.8932, 4.8932],
        [4.8932, 4.9190, 4.8980]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:46, step:0 
model_pd.l_p.mean(): 0.11778851598501205 
model_pd.l_d.mean(): -19.52469253540039 
model_pd.lagr.mean(): -19.406904220581055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4899], device='cuda:0')), ('power', tensor([-20.3972], device='cuda:0'))])
epoch£º46	 i:0 	 global-step:920	 l-p:0.11778851598501205
epoch£º46	 i:1 	 global-step:921	 l-p:0.21427570283412933
epoch£º46	 i:2 	 global-step:922	 l-p:0.12700368463993073
epoch£º46	 i:3 	 global-step:923	 l-p:0.1249251738190651
epoch£º46	 i:4 	 global-step:924	 l-p:0.13933168351650238
epoch£º46	 i:5 	 global-step:925	 l-p:0.11550125479698181
epoch£º46	 i:6 	 global-step:926	 l-p:0.1442698836326599
epoch£º46	 i:7 	 global-step:927	 l-p:0.13146445155143738
epoch£º46	 i:8 	 global-step:928	 l-p:0.1277398020029068
epoch£º46	 i:9 	 global-step:929	 l-p:0.15421901643276215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:47
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8377, 4.8935, 4.8546],
        [4.8377, 4.9442, 4.8860],
        [4.8377, 4.8395, 4.8378],
        [4.8377, 5.1796, 5.1481]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:47, step:0 
model_pd.l_p.mean(): 0.11969244480133057 
model_pd.l_d.mean(): -18.279401779174805 
model_pd.lagr.mean(): -18.159709930419922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5298], device='cuda:0')), ('power', tensor([-19.1700], device='cuda:0'))])
epoch£º47	 i:0 	 global-step:940	 l-p:0.11969244480133057
epoch£º47	 i:1 	 global-step:941	 l-p:0.1418762356042862
epoch£º47	 i:2 	 global-step:942	 l-p:0.16347655653953552
epoch£º47	 i:3 	 global-step:943	 l-p:0.1305561065673828
epoch£º47	 i:4 	 global-step:944	 l-p:0.13770513236522675
epoch£º47	 i:5 	 global-step:945	 l-p:0.11224504560232162
epoch£º47	 i:6 	 global-step:946	 l-p:0.13481613993644714
epoch£º47	 i:7 	 global-step:947	 l-p:0.135152667760849
epoch£º47	 i:8 	 global-step:948	 l-p:0.13378162682056427
epoch£º47	 i:9 	 global-step:949	 l-p:0.17036907374858856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:48
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7858, 5.1238, 5.0934],
        [4.7858, 4.9662, 4.9001],
        [4.7858, 4.7858, 4.7858],
        [4.7858, 5.0157, 4.9540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:48, step:0 
model_pd.l_p.mean(): 0.17284931242465973 
model_pd.l_d.mean(): -20.68732261657715 
model_pd.lagr.mean(): -20.51447296142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4347], device='cuda:0')), ('power', tensor([-21.5244], device='cuda:0'))])
epoch£º48	 i:0 	 global-step:960	 l-p:0.17284931242465973
epoch£º48	 i:1 	 global-step:961	 l-p:0.12777139246463776
epoch£º48	 i:2 	 global-step:962	 l-p:0.13522042334079742
epoch£º48	 i:3 	 global-step:963	 l-p:0.11988922208547592
epoch£º48	 i:4 	 global-step:964	 l-p:0.13764768838882446
epoch£º48	 i:5 	 global-step:965	 l-p:0.2058575451374054
epoch£º48	 i:6 	 global-step:966	 l-p:0.12332204729318619
epoch£º48	 i:7 	 global-step:967	 l-p:0.12908506393432617
epoch£º48	 i:8 	 global-step:968	 l-p:0.11771709471940994
epoch£º48	 i:9 	 global-step:969	 l-p:0.11680164188146591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:49
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9362, 4.9707, 4.9439],
        [4.9362, 4.9376, 4.9363],
        [4.9362, 5.0952, 5.0274],
        [4.9362, 4.9424, 4.9367]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:49, step:0 
model_pd.l_p.mean(): 0.12528258562088013 
model_pd.l_d.mean(): -20.676599502563477 
model_pd.lagr.mean(): -20.55131721496582 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3882], device='cuda:0')), ('power', tensor([-21.4653], device='cuda:0'))])
epoch£º49	 i:0 	 global-step:980	 l-p:0.12528258562088013
epoch£º49	 i:1 	 global-step:981	 l-p:0.13822820782661438
epoch£º49	 i:2 	 global-step:982	 l-p:0.15642081201076508
epoch£º49	 i:3 	 global-step:983	 l-p:0.11825933307409286
epoch£º49	 i:4 	 global-step:984	 l-p:0.14523406326770782
epoch£º49	 i:5 	 global-step:985	 l-p:0.14262281358242035
epoch£º49	 i:6 	 global-step:986	 l-p:0.13741977512836456
epoch£º49	 i:7 	 global-step:987	 l-p:0.1843917965888977
epoch£º49	 i:8 	 global-step:988	 l-p:0.11888474225997925
epoch£º49	 i:9 	 global-step:989	 l-p:0.13849444687366486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:50
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8264, 5.6596, 5.9924],
        [4.8264, 4.8265, 4.8264],
        [4.8264, 5.1448, 5.1057],
        [4.8264, 4.8282, 4.8265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:50, step:0 
model_pd.l_p.mean(): 0.1373373419046402 
model_pd.l_d.mean(): -19.932411193847656 
model_pd.lagr.mean(): -19.795074462890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4906], device='cuda:0')), ('power', tensor([-20.8133], device='cuda:0'))])
epoch£º50	 i:0 	 global-step:1000	 l-p:0.1373373419046402
epoch£º50	 i:1 	 global-step:1001	 l-p:0.13769303262233734
epoch£º50	 i:2 	 global-step:1002	 l-p:0.14258241653442383
epoch£º50	 i:3 	 global-step:1003	 l-p:0.1397445797920227
epoch£º50	 i:4 	 global-step:1004	 l-p:0.12561772763729095
epoch£º50	 i:5 	 global-step:1005	 l-p:0.12176542729139328
epoch£º50	 i:6 	 global-step:1006	 l-p:0.13377417623996735
epoch£º50	 i:7 	 global-step:1007	 l-p:0.14555400609970093
epoch£º50	 i:8 	 global-step:1008	 l-p:0.12408789992332458
epoch£º50	 i:9 	 global-step:1009	 l-p:0.13597321510314941
====================================================================================================
====================================================================================================
====================================================================================================

epoch:51
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9639, 4.9639, 4.9639],
        [4.9639, 5.5696, 5.6896],
        [4.9639, 6.2923, 7.1554],
        [4.9639, 4.9640, 4.9639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:51, step:0 
model_pd.l_p.mean(): 0.11435072869062424 
model_pd.l_d.mean(): -18.53336524963379 
model_pd.lagr.mean(): -18.41901397705078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5101], device='cuda:0')), ('power', tensor([-19.4082], device='cuda:0'))])
epoch£º51	 i:0 	 global-step:1020	 l-p:0.11435072869062424
epoch£º51	 i:1 	 global-step:1021	 l-p:0.1095992848277092
epoch£º51	 i:2 	 global-step:1022	 l-p:0.11614038050174713
epoch£º51	 i:3 	 global-step:1023	 l-p:0.12675301730632782
epoch£º51	 i:4 	 global-step:1024	 l-p:0.12326554954051971
epoch£º51	 i:5 	 global-step:1025	 l-p:0.1245914027094841
epoch£º51	 i:6 	 global-step:1026	 l-p:0.13788671791553497
epoch£º51	 i:7 	 global-step:1027	 l-p:0.18677276372909546
epoch£º51	 i:8 	 global-step:1028	 l-p:0.1530396193265915
epoch£º51	 i:9 	 global-step:1029	 l-p:0.153777077794075
====================================================================================================
====================================================================================================
====================================================================================================

epoch:52
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7097, 5.2888, 5.4120],
        [4.7097, 5.1261, 5.1396],
        [4.7097, 4.7582, 4.7236],
        [4.7097, 4.7097, 4.7097]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:52, step:0 
model_pd.l_p.mean(): 0.1430409997701645 
model_pd.l_d.mean(): -20.38134765625 
model_pd.lagr.mean(): -20.238306045532227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4676], device='cuda:0')), ('power', tensor([-21.2468], device='cuda:0'))])
epoch£º52	 i:0 	 global-step:1040	 l-p:0.1430409997701645
epoch£º52	 i:1 	 global-step:1041	 l-p:0.15623609721660614
epoch£º52	 i:2 	 global-step:1042	 l-p:0.1389693170785904
epoch£º52	 i:3 	 global-step:1043	 l-p:0.1475730687379837
epoch£º52	 i:4 	 global-step:1044	 l-p:0.03257538378238678
epoch£º52	 i:5 	 global-step:1045	 l-p:0.14539997279644012
epoch£º52	 i:6 	 global-step:1046	 l-p:0.16257557272911072
epoch£º52	 i:7 	 global-step:1047	 l-p:-0.932316243648529
epoch£º52	 i:8 	 global-step:1048	 l-p:0.1738450527191162
epoch£º52	 i:9 	 global-step:1049	 l-p:0.10678206384181976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:53
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6565, 4.6645, 4.6573],
        [4.6565, 4.6566, 4.6565],
        [4.6565, 4.6571, 4.6565],
        [4.6565, 4.6570, 4.6565]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:53, step:0 
model_pd.l_p.mean(): 0.15208719670772552 
model_pd.l_d.mean(): -18.917325973510742 
model_pd.lagr.mean(): -18.765239715576172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5585], device='cuda:0')), ('power', tensor([-19.8496], device='cuda:0'))])
epoch£º53	 i:0 	 global-step:1060	 l-p:0.15208719670772552
epoch£º53	 i:1 	 global-step:1061	 l-p:0.13184744119644165
epoch£º53	 i:2 	 global-step:1062	 l-p:0.13219404220581055
epoch£º53	 i:3 	 global-step:1063	 l-p:0.649499773979187
epoch£º53	 i:4 	 global-step:1064	 l-p:0.1215999573469162
epoch£º53	 i:5 	 global-step:1065	 l-p:0.11829858273267746
epoch£º53	 i:6 	 global-step:1066	 l-p:0.1233876496553421
epoch£º53	 i:7 	 global-step:1067	 l-p:0.12010179460048676
epoch£º53	 i:8 	 global-step:1068	 l-p:0.13307274878025055
epoch£º53	 i:9 	 global-step:1069	 l-p:0.1354483664035797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:54
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0276, 5.0276, 5.0276],
        [5.0276, 5.7461, 5.9528],
        [5.0276, 5.7983, 6.0506],
        [5.0276, 5.1352, 5.0756]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:54, step:0 
model_pd.l_p.mean(): 0.1276344209909439 
model_pd.l_d.mean(): -20.427297592163086 
model_pd.lagr.mean(): -20.299663543701172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3862], device='cuda:0')), ('power', tensor([-21.2093], device='cuda:0'))])
epoch£º54	 i:0 	 global-step:1080	 l-p:0.1276344209909439
epoch£º54	 i:1 	 global-step:1081	 l-p:0.1221705973148346
epoch£º54	 i:2 	 global-step:1082	 l-p:0.1259172260761261
epoch£º54	 i:3 	 global-step:1083	 l-p:0.11591541767120361
epoch£º54	 i:4 	 global-step:1084	 l-p:0.12452251464128494
epoch£º54	 i:5 	 global-step:1085	 l-p:0.13933907449245453
epoch£º54	 i:6 	 global-step:1086	 l-p:0.007503199391067028
epoch£º54	 i:7 	 global-step:1087	 l-p:0.1552942842245102
epoch£º54	 i:8 	 global-step:1088	 l-p:0.13197574019432068
epoch£º54	 i:9 	 global-step:1089	 l-p:0.1410582959651947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:55
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5530,  0.4539,  1.0000,  0.3726,
          1.0000,  0.8208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7399,  0.6692,  1.0000,  0.6053,
          1.0000,  0.9045, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5585,  0.4600,  1.0000,  0.3788,
          1.0000,  0.8235, 31.6228]], device='cuda:0')
 pt:tensor([[4.8630, 5.5984, 5.8383],
        [4.8630, 5.8996, 6.4419],
        [4.8630, 5.2967, 5.3115],
        [4.8630, 5.6075, 5.8555]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:55, step:0 
model_pd.l_p.mean(): 0.15535804629325867 
model_pd.l_d.mean(): -19.877473831176758 
model_pd.lagr.mean(): -19.722116470336914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4834], device='cuda:0')), ('power', tensor([-20.7498], device='cuda:0'))])
epoch£º55	 i:0 	 global-step:1100	 l-p:0.15535804629325867
epoch£º55	 i:1 	 global-step:1101	 l-p:-1.0355684757232666
epoch£º55	 i:2 	 global-step:1102	 l-p:0.1492752879858017
epoch£º55	 i:3 	 global-step:1103	 l-p:0.15566273033618927
epoch£º55	 i:4 	 global-step:1104	 l-p:0.10966111719608307
epoch£º55	 i:5 	 global-step:1105	 l-p:0.11469386518001556
epoch£º55	 i:6 	 global-step:1106	 l-p:0.11336981505155563
epoch£º55	 i:7 	 global-step:1107	 l-p:0.12109436839818954
epoch£º55	 i:8 	 global-step:1108	 l-p:0.11315369606018066
epoch£º55	 i:9 	 global-step:1109	 l-p:0.1495051234960556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:56
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9288, 6.2385, 7.0891],
        [4.9288, 4.9289, 4.9288],
        [4.9288, 5.1675, 5.1050],
        [4.9288, 5.0056, 4.9572]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:56, step:0 
model_pd.l_p.mean(): 0.17878329753875732 
model_pd.l_d.mean(): -19.840106964111328 
model_pd.lagr.mean(): -19.66132354736328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4742], device='cuda:0')), ('power', tensor([-20.7023], device='cuda:0'))])
epoch£º56	 i:0 	 global-step:1120	 l-p:0.17878329753875732
epoch£º56	 i:1 	 global-step:1121	 l-p:0.13520090281963348
epoch£º56	 i:2 	 global-step:1122	 l-p:0.12750087678432465
epoch£º56	 i:3 	 global-step:1123	 l-p:0.12373373657464981
epoch£º56	 i:4 	 global-step:1124	 l-p:0.12157823145389557
epoch£º56	 i:5 	 global-step:1125	 l-p:0.12857334315776825
epoch£º56	 i:6 	 global-step:1126	 l-p:0.13364256918430328
epoch£º56	 i:7 	 global-step:1127	 l-p:0.14273397624492645
epoch£º56	 i:8 	 global-step:1128	 l-p:0.162504643201828
epoch£º56	 i:9 	 global-step:1129	 l-p:0.14107368886470795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:57
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8344, 5.8146, 6.3017],
        [4.8344, 5.9296, 6.5439],
        [4.8344, 4.8344, 4.8344],
        [4.8344, 4.9005, 4.8571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:57, step:0 
model_pd.l_p.mean(): 0.16009002923965454 
model_pd.l_d.mean(): -20.181177139282227 
model_pd.lagr.mean(): -20.021087646484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4718], device='cuda:0')), ('power', tensor([-21.0472], device='cuda:0'))])
epoch£º57	 i:0 	 global-step:1140	 l-p:0.16009002923965454
epoch£º57	 i:1 	 global-step:1141	 l-p:0.13855643570423126
epoch£º57	 i:2 	 global-step:1142	 l-p:0.1626170575618744
epoch£º57	 i:3 	 global-step:1143	 l-p:0.060032349079847336
epoch£º57	 i:4 	 global-step:1144	 l-p:0.12505148351192474
epoch£º57	 i:5 	 global-step:1145	 l-p:0.14716508984565735
epoch£º57	 i:6 	 global-step:1146	 l-p:0.14769525825977325
epoch£º57	 i:7 	 global-step:1147	 l-p:0.11327201873064041
epoch£º57	 i:8 	 global-step:1148	 l-p:0.12699121236801147
epoch£º57	 i:9 	 global-step:1149	 l-p:0.1140241026878357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:58
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9302, 5.2420, 5.1987],
        [4.9302, 5.1485, 5.0835],
        [4.9302, 6.1832, 6.9661],
        [4.9302, 4.9302, 4.9302]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:58, step:0 
model_pd.l_p.mean(): 0.13071654736995697 
model_pd.l_d.mean(): -20.451658248901367 
model_pd.lagr.mean(): -20.320941925048828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4154], device='cuda:0')), ('power', tensor([-21.2643], device='cuda:0'))])
epoch£º58	 i:0 	 global-step:1160	 l-p:0.13071654736995697
epoch£º58	 i:1 	 global-step:1161	 l-p:0.13971035182476044
epoch£º58	 i:2 	 global-step:1162	 l-p:0.12397567182779312
epoch£º58	 i:3 	 global-step:1163	 l-p:0.12997891008853912
epoch£º58	 i:4 	 global-step:1164	 l-p:0.14994049072265625
epoch£º58	 i:5 	 global-step:1165	 l-p:0.13225187361240387
epoch£º58	 i:6 	 global-step:1166	 l-p:0.1340317279100418
epoch£º58	 i:7 	 global-step:1167	 l-p:0.14073817431926727
epoch£º58	 i:8 	 global-step:1168	 l-p:0.15375515818595886
epoch£º58	 i:9 	 global-step:1169	 l-p:0.04063527658581734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:59
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6297, 4.9493, 4.9222],
        [4.6297, 4.6317, 4.6298],
        [4.6297, 4.6297, 4.6297],
        [4.6297, 4.7830, 4.7216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:59, step:0 
model_pd.l_p.mean(): 0.13011546432971954 
model_pd.l_d.mean(): -18.68512725830078 
model_pd.lagr.mean(): -18.555011749267578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5757], device='cuda:0')), ('power', tensor([-19.6308], device='cuda:0'))])
epoch£º59	 i:0 	 global-step:1180	 l-p:0.13011546432971954
epoch£º59	 i:1 	 global-step:1181	 l-p:0.14477649331092834
epoch£º59	 i:2 	 global-step:1182	 l-p:0.16381607949733734
epoch£º59	 i:3 	 global-step:1183	 l-p:0.22349406778812408
epoch£º59	 i:4 	 global-step:1184	 l-p:0.14975328743457794
epoch£º59	 i:5 	 global-step:1185	 l-p:0.1761024445295334
epoch£º59	 i:6 	 global-step:1186	 l-p:0.13960251212120056
epoch£º59	 i:7 	 global-step:1187	 l-p:0.12437893450260162
epoch£º59	 i:8 	 global-step:1188	 l-p:0.13224825263023376
epoch£º59	 i:9 	 global-step:1189	 l-p:0.13542133569717407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:60
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9808, 5.0885, 5.0298],
        [4.9808, 4.9808, 4.9808],
        [4.9808, 4.9828, 4.9808],
        [4.9808, 5.3254, 5.2926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:60, step:0 
model_pd.l_p.mean(): 0.15861590206623077 
model_pd.l_d.mean(): -18.05537223815918 
model_pd.lagr.mean(): -17.896757125854492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5649], device='cuda:0')), ('power', tensor([-18.9781], device='cuda:0'))])
epoch£º60	 i:0 	 global-step:1200	 l-p:0.15861590206623077
epoch£º60	 i:1 	 global-step:1201	 l-p:0.1340503841638565
epoch£º60	 i:2 	 global-step:1202	 l-p:0.1163032129406929
epoch£º60	 i:3 	 global-step:1203	 l-p:0.11693158745765686
epoch£º60	 i:4 	 global-step:1204	 l-p:0.13140566647052765
epoch£º60	 i:5 	 global-step:1205	 l-p:0.12279333174228668
epoch£º60	 i:6 	 global-step:1206	 l-p:0.12751145660877228
epoch£º60	 i:7 	 global-step:1207	 l-p:0.1147933229804039
epoch£º60	 i:8 	 global-step:1208	 l-p:0.14725837111473083
epoch£º60	 i:9 	 global-step:1209	 l-p:0.11725641041994095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:61
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8354, 4.8354, 4.8354],
        [4.8354, 5.4566, 5.6058],
        [4.8354, 4.8419, 4.8360],
        [4.8354, 5.5440, 5.7667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:61, step:0 
model_pd.l_p.mean(): 0.15280720591545105 
model_pd.l_d.mean(): -19.077335357666016 
model_pd.lagr.mean(): -18.924528121948242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5089], device='cuda:0')), ('power', tensor([-19.9611], device='cuda:0'))])
epoch£º61	 i:0 	 global-step:1220	 l-p:0.15280720591545105
epoch£º61	 i:1 	 global-step:1221	 l-p:0.14592863619327545
epoch£º61	 i:2 	 global-step:1222	 l-p:0.12451349943876266
epoch£º61	 i:3 	 global-step:1223	 l-p:0.17252002656459808
epoch£º61	 i:4 	 global-step:1224	 l-p:0.13715365529060364
epoch£º61	 i:5 	 global-step:1225	 l-p:0.11607103049755096
epoch£º61	 i:6 	 global-step:1226	 l-p:0.13367536664009094
epoch£º61	 i:7 	 global-step:1227	 l-p:0.1272994726896286
epoch£º61	 i:8 	 global-step:1228	 l-p:0.1356457769870758
epoch£º61	 i:9 	 global-step:1229	 l-p:0.14618191123008728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:62
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8446, 4.8762, 4.8515],
        [4.8446, 5.7739, 6.2091],
        [4.8446, 4.9756, 4.9136],
        [4.8446, 5.0579, 4.9952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:62, step:0 
model_pd.l_p.mean(): 0.157131627202034 
model_pd.l_d.mean(): -19.79358673095703 
model_pd.lagr.mean(): -19.636455535888672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5026], device='cuda:0')), ('power', tensor([-20.6842], device='cuda:0'))])
epoch£º62	 i:0 	 global-step:1240	 l-p:0.157131627202034
epoch£º62	 i:1 	 global-step:1241	 l-p:0.13051356375217438
epoch£º62	 i:2 	 global-step:1242	 l-p:0.1083846464753151
epoch£º62	 i:3 	 global-step:1243	 l-p:0.12306894361972809
epoch£º62	 i:4 	 global-step:1244	 l-p:0.2035880982875824
epoch£º62	 i:5 	 global-step:1245	 l-p:0.12748922407627106
epoch£º62	 i:6 	 global-step:1246	 l-p:0.1358506679534912
epoch£º62	 i:7 	 global-step:1247	 l-p:0.14141535758972168
epoch£º62	 i:8 	 global-step:1248	 l-p:0.1400785595178604
epoch£º62	 i:9 	 global-step:1249	 l-p:0.1390695720911026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:63
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9073, 4.9099, 4.9074],
        [4.9073, 6.2424, 7.1342],
        [4.9073, 6.2471, 7.1448],
        [4.9073, 4.9073, 4.9073]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:63, step:0 
model_pd.l_p.mean(): 0.13424299657344818 
model_pd.l_d.mean(): -19.88812828063965 
model_pd.lagr.mean(): -19.75388526916504 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4665], device='cuda:0')), ('power', tensor([-20.7432], device='cuda:0'))])
epoch£º63	 i:0 	 global-step:1260	 l-p:0.13424299657344818
epoch£º63	 i:1 	 global-step:1261	 l-p:0.15174919366836548
epoch£º63	 i:2 	 global-step:1262	 l-p:0.07273724675178528
epoch£º63	 i:3 	 global-step:1263	 l-p:0.15375778079032898
epoch£º63	 i:4 	 global-step:1264	 l-p:0.11572717130184174
epoch£º63	 i:5 	 global-step:1265	 l-p:0.1383800506591797
epoch£º63	 i:6 	 global-step:1266	 l-p:0.1421516239643097
epoch£º63	 i:7 	 global-step:1267	 l-p:0.17358483374118805
epoch£º63	 i:8 	 global-step:1268	 l-p:0.11361265927553177
epoch£º63	 i:9 	 global-step:1269	 l-p:2.114698648452759
====================================================================================================
====================================================================================================
====================================================================================================

epoch:64
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6897, 4.6913, 4.6897],
        [4.6897, 4.6897, 4.6897],
        [4.6897, 4.7159, 4.6950],
        [4.6897, 4.6897, 4.6897]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:64, step:0 
model_pd.l_p.mean(): 0.12976114451885223 
model_pd.l_d.mean(): -19.628812789916992 
model_pd.lagr.mean(): -19.499052047729492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5419], device='cuda:0')), ('power', tensor([-20.5571], device='cuda:0'))])
epoch£º64	 i:0 	 global-step:1280	 l-p:0.12976114451885223
epoch£º64	 i:1 	 global-step:1281	 l-p:0.14558450877666473
epoch£º64	 i:2 	 global-step:1282	 l-p:0.189787819981575
epoch£º64	 i:3 	 global-step:1283	 l-p:0.20318129658699036
epoch£º64	 i:4 	 global-step:1284	 l-p:0.09818780422210693
epoch£º64	 i:5 	 global-step:1285	 l-p:0.16379058361053467
epoch£º64	 i:6 	 global-step:1286	 l-p:0.13702695071697235
epoch£º64	 i:7 	 global-step:1287	 l-p:0.1006915271282196
epoch£º64	 i:8 	 global-step:1288	 l-p:0.14261527359485626
epoch£º64	 i:9 	 global-step:1289	 l-p:0.11549003422260284
====================================================================================================
====================================================================================================
====================================================================================================

epoch:65
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9464, 4.9469, 4.9464],
        [4.9464, 4.9464, 4.9464],
        [4.9464, 6.0186, 6.5938],
        [4.9464, 5.0665, 5.0056]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:65, step:0 
model_pd.l_p.mean(): 0.12497756630182266 
model_pd.l_d.mean(): -19.849607467651367 
model_pd.lagr.mean(): -19.72463035583496 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4640], device='cuda:0')), ('power', tensor([-20.7013], device='cuda:0'))])
epoch£º65	 i:0 	 global-step:1300	 l-p:0.12497756630182266
epoch£º65	 i:1 	 global-step:1301	 l-p:0.13952235877513885
epoch£º65	 i:2 	 global-step:1302	 l-p:0.13534896075725555
epoch£º65	 i:3 	 global-step:1303	 l-p:0.13732409477233887
epoch£º65	 i:4 	 global-step:1304	 l-p:0.12193265557289124
epoch£º65	 i:5 	 global-step:1305	 l-p:0.17988406121730804
epoch£º65	 i:6 	 global-step:1306	 l-p:0.13642719388008118
epoch£º65	 i:7 	 global-step:1307	 l-p:0.14618630707263947
epoch£º65	 i:8 	 global-step:1308	 l-p:0.14925365149974823
epoch£º65	 i:9 	 global-step:1309	 l-p:0.3151931166648865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:66
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7078, 4.7078, 4.7078],
        [4.7078, 5.6954, 6.2163],
        [4.7078, 5.2357, 5.3273],
        [4.7078, 4.7315, 4.7123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:66, step:0 
model_pd.l_p.mean(): 0.14089688658714294 
model_pd.l_d.mean(): -20.30460548400879 
model_pd.lagr.mean(): -20.163707733154297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4912], device='cuda:0')), ('power', tensor([-21.1930], device='cuda:0'))])
epoch£º66	 i:0 	 global-step:1320	 l-p:0.14089688658714294
epoch£º66	 i:1 	 global-step:1321	 l-p:0.13924060761928558
epoch£º66	 i:2 	 global-step:1322	 l-p:0.1316373199224472
epoch£º66	 i:3 	 global-step:1323	 l-p:0.12313012778759003
epoch£º66	 i:4 	 global-step:1324	 l-p:0.16344276070594788
epoch£º66	 i:5 	 global-step:1325	 l-p:0.13264498114585876
epoch£º66	 i:6 	 global-step:1326	 l-p:0.1355074793100357
epoch£º66	 i:7 	 global-step:1327	 l-p:0.17552459239959717
epoch£º66	 i:8 	 global-step:1328	 l-p:0.1292826384305954
epoch£º66	 i:9 	 global-step:1329	 l-p:0.15075793862342834
====================================================================================================
====================================================================================================
====================================================================================================

epoch:67
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9135, 4.9135, 4.9135],
        [4.9135, 4.9135, 4.9135],
        [4.9135, 5.8940, 6.3761],
        [4.9135, 5.6866, 5.9620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:67, step:0 
model_pd.l_p.mean(): 0.16386647522449493 
model_pd.l_d.mean(): -19.842605590820312 
model_pd.lagr.mean(): -19.678739547729492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4755], device='cuda:0')), ('power', tensor([-20.7062], device='cuda:0'))])
epoch£º67	 i:0 	 global-step:1340	 l-p:0.16386647522449493
epoch£º67	 i:1 	 global-step:1341	 l-p:0.11248449236154556
epoch£º67	 i:2 	 global-step:1342	 l-p:0.12852182984352112
epoch£º67	 i:3 	 global-step:1343	 l-p:0.12928134202957153
epoch£º67	 i:4 	 global-step:1344	 l-p:0.13667701184749603
epoch£º67	 i:5 	 global-step:1345	 l-p:0.20825175940990448
epoch£º67	 i:6 	 global-step:1346	 l-p:0.13390867412090302
epoch£º67	 i:7 	 global-step:1347	 l-p:0.12883614003658295
epoch£º67	 i:8 	 global-step:1348	 l-p:0.14497010409832
epoch£º67	 i:9 	 global-step:1349	 l-p:0.11003778874874115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:68
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8873, 6.1528, 6.9659],
        [4.8873, 6.2097, 7.0931],
        [4.8873, 4.8971, 4.8884],
        [4.8873, 5.9363, 6.4973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:68, step:0 
model_pd.l_p.mean(): 0.12098395079374313 
model_pd.l_d.mean(): -18.9372615814209 
model_pd.lagr.mean(): -18.8162784576416 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5415], device='cuda:0')), ('power', tensor([-19.8522], device='cuda:0'))])
epoch£º68	 i:0 	 global-step:1360	 l-p:0.12098395079374313
epoch£º68	 i:1 	 global-step:1361	 l-p:0.36786162853240967
epoch£º68	 i:2 	 global-step:1362	 l-p:0.15059204399585724
epoch£º68	 i:3 	 global-step:1363	 l-p:0.11869897693395615
epoch£º68	 i:4 	 global-step:1364	 l-p:0.127463236451149
epoch£º68	 i:5 	 global-step:1365	 l-p:0.12340765446424484
epoch£º68	 i:6 	 global-step:1366	 l-p:0.13037334382534027
epoch£º68	 i:7 	 global-step:1367	 l-p:0.1545732617378235
epoch£º68	 i:8 	 global-step:1368	 l-p:0.11792800575494766
epoch£º68	 i:9 	 global-step:1369	 l-p:0.1586635559797287
====================================================================================================
====================================================================================================
====================================================================================================

epoch:69
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8449, 5.0097, 4.9457],
        [4.8449, 5.9188, 6.5154],
        [4.8449, 5.0673, 5.0072],
        [4.8449, 4.8449, 4.8449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:69, step:0 
model_pd.l_p.mean(): 0.15082918107509613 
model_pd.l_d.mean(): -18.46632957458496 
model_pd.lagr.mean(): -18.315500259399414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5681], device='cuda:0')), ('power', tensor([-19.4000], device='cuda:0'))])
epoch£º69	 i:0 	 global-step:1380	 l-p:0.15082918107509613
epoch£º69	 i:1 	 global-step:1381	 l-p:0.1301332265138626
epoch£º69	 i:2 	 global-step:1382	 l-p:-1.2330436706542969
epoch£º69	 i:3 	 global-step:1383	 l-p:0.1529397964477539
epoch£º69	 i:4 	 global-step:1384	 l-p:0.11988089233636856
epoch£º69	 i:5 	 global-step:1385	 l-p:0.12412011623382568
epoch£º69	 i:6 	 global-step:1386	 l-p:0.11886390298604965
epoch£º69	 i:7 	 global-step:1387	 l-p:0.1322626918554306
epoch£º69	 i:8 	 global-step:1388	 l-p:0.12689633667469025
epoch£º69	 i:9 	 global-step:1389	 l-p:0.12079112976789474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:70
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9366, 6.3239, 7.2802],
        [4.9366, 4.9643, 4.9422],
        [4.9366, 4.9366, 4.9366],
        [4.9366, 6.2341, 7.0784]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:70, step:0 
model_pd.l_p.mean(): 0.10369468480348587 
model_pd.l_d.mean(): -17.842063903808594 
model_pd.lagr.mean(): -17.73836898803711 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5746], device='cuda:0')), ('power', tensor([-18.7708], device='cuda:0'))])
epoch£º70	 i:0 	 global-step:1400	 l-p:0.10369468480348587
epoch£º70	 i:1 	 global-step:1401	 l-p:0.12618176639080048
epoch£º70	 i:2 	 global-step:1402	 l-p:0.12830814719200134
epoch£º70	 i:3 	 global-step:1403	 l-p:0.1401529163122177
epoch£º70	 i:4 	 global-step:1404	 l-p:0.15187518298625946
epoch£º70	 i:5 	 global-step:1405	 l-p:0.1833384782075882
epoch£º70	 i:6 	 global-step:1406	 l-p:0.16357210278511047
epoch£º70	 i:7 	 global-step:1407	 l-p:0.09943040460348129
epoch£º70	 i:8 	 global-step:1408	 l-p:0.12622712552547455
epoch£º70	 i:9 	 global-step:1409	 l-p:0.14934305846691132
====================================================================================================
====================================================================================================
====================================================================================================

epoch:71
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8291, 5.0207, 4.9579],
        [4.8291, 4.8322, 4.8293],
        [4.8291, 4.8291, 4.8291],
        [4.8291, 5.4334, 5.5749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:71, step:0 
model_pd.l_p.mean(): 0.13691991567611694 
model_pd.l_d.mean(): -20.163827896118164 
model_pd.lagr.mean(): -20.02690887451172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4759], device='cuda:0')), ('power', tensor([-21.0338], device='cuda:0'))])
epoch£º71	 i:0 	 global-step:1420	 l-p:0.13691991567611694
epoch£º71	 i:1 	 global-step:1421	 l-p:0.12872718274593353
epoch£º71	 i:2 	 global-step:1422	 l-p:0.08989354968070984
epoch£º71	 i:3 	 global-step:1423	 l-p:0.12103601545095444
epoch£º71	 i:4 	 global-step:1424	 l-p:0.15930883586406708
epoch£º71	 i:5 	 global-step:1425	 l-p:0.13836175203323364
epoch£º71	 i:6 	 global-step:1426	 l-p:0.14022648334503174
epoch£º71	 i:7 	 global-step:1427	 l-p:0.15915724635124207
epoch£º71	 i:8 	 global-step:1428	 l-p:0.1428961604833603
epoch£º71	 i:9 	 global-step:1429	 l-p:0.15425752103328705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:72
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8530, 4.8530, 4.8530],
        [4.8530, 4.8531, 4.8530],
        [4.8530, 4.8575, 4.8534],
        [4.8530, 4.8531, 4.8530]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:72, step:0 
model_pd.l_p.mean(): 0.12221159040927887 
model_pd.l_d.mean(): -19.043914794921875 
model_pd.lagr.mean(): -18.921703338623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4902], device='cuda:0')), ('power', tensor([-19.9077], device='cuda:0'))])
epoch£º72	 i:0 	 global-step:1440	 l-p:0.12221159040927887
epoch£º72	 i:1 	 global-step:1441	 l-p:0.14671675860881805
epoch£º72	 i:2 	 global-step:1442	 l-p:0.1358989030122757
epoch£º72	 i:3 	 global-step:1443	 l-p:0.14229995012283325
epoch£º72	 i:4 	 global-step:1444	 l-p:0.14747342467308044
epoch£º72	 i:5 	 global-step:1445	 l-p:0.11708594113588333
epoch£º72	 i:6 	 global-step:1446	 l-p:0.15363921225070953
epoch£º72	 i:7 	 global-step:1447	 l-p:0.1328875720500946
epoch£º72	 i:8 	 global-step:1448	 l-p:0.15581119060516357
epoch£º72	 i:9 	 global-step:1449	 l-p:-0.08720579743385315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:73
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8933, 4.8933, 4.8933],
        [4.8933, 5.0509, 4.9869],
        [4.8933, 5.8630, 6.3406],
        [4.8933, 5.1147, 5.0539]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:73, step:0 
model_pd.l_p.mean(): 0.1314225047826767 
model_pd.l_d.mean(): -19.15789031982422 
model_pd.lagr.mean(): -19.02646827697754 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4689], device='cuda:0')), ('power', tensor([-20.0018], device='cuda:0'))])
epoch£º73	 i:0 	 global-step:1460	 l-p:0.1314225047826767
epoch£º73	 i:1 	 global-step:1461	 l-p:0.14315608143806458
epoch£º73	 i:2 	 global-step:1462	 l-p:0.2059546262025833
epoch£º73	 i:3 	 global-step:1463	 l-p:0.11429790407419205
epoch£º73	 i:4 	 global-step:1464	 l-p:0.1279105395078659
epoch£º73	 i:5 	 global-step:1465	 l-p:0.12018951028585434
epoch£º73	 i:6 	 global-step:1466	 l-p:0.13284577429294586
epoch£º73	 i:7 	 global-step:1467	 l-p:0.14369939267635345
epoch£º73	 i:8 	 global-step:1468	 l-p:0.10545698553323746
epoch£º73	 i:9 	 global-step:1469	 l-p:0.12764282524585724
====================================================================================================
====================================================================================================
====================================================================================================

epoch:74
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9139, 4.9319, 4.9167],
        [4.9139, 5.2843, 5.2704],
        [4.9139, 5.6455, 5.8873],
        [4.9139, 5.1350, 5.0738]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:74, step:0 
model_pd.l_p.mean(): 0.13360632956027985 
model_pd.l_d.mean(): -19.59657096862793 
model_pd.lagr.mean(): -19.46296501159668 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4987], device='cuda:0')), ('power', tensor([-20.4795], device='cuda:0'))])
epoch£º74	 i:0 	 global-step:1480	 l-p:0.13360632956027985
epoch£º74	 i:1 	 global-step:1481	 l-p:0.11731618642807007
epoch£º74	 i:2 	 global-step:1482	 l-p:0.14399293065071106
epoch£º74	 i:3 	 global-step:1483	 l-p:0.13353215157985687
epoch£º74	 i:4 	 global-step:1484	 l-p:0.12479083985090256
epoch£º74	 i:5 	 global-step:1485	 l-p:0.17726370692253113
epoch£º74	 i:6 	 global-step:1486	 l-p:0.18185950815677643
epoch£º74	 i:7 	 global-step:1487	 l-p:0.10620962083339691
epoch£º74	 i:8 	 global-step:1488	 l-p:0.16122664511203766
epoch£º74	 i:9 	 global-step:1489	 l-p:0.13233965635299683
====================================================================================================
====================================================================================================
====================================================================================================

epoch:75
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8988, 5.2322, 5.2032],
        [4.8988, 5.1090, 5.0471],
        [4.8988, 4.8988, 4.8988],
        [4.8988, 5.6822, 5.9739]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:75, step:0 
model_pd.l_p.mean(): 0.13600605726242065 
model_pd.l_d.mean(): -19.93600845336914 
model_pd.lagr.mean(): -19.800003051757812 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4664], device='cuda:0')), ('power', tensor([-20.7919], device='cuda:0'))])
epoch£º75	 i:0 	 global-step:1500	 l-p:0.13600605726242065
epoch£º75	 i:1 	 global-step:1501	 l-p:0.12752200663089752
epoch£º75	 i:2 	 global-step:1502	 l-p:0.1397332102060318
epoch£º75	 i:3 	 global-step:1503	 l-p:0.14123588800430298
epoch£º75	 i:4 	 global-step:1504	 l-p:0.13728514313697815
epoch£º75	 i:5 	 global-step:1505	 l-p:0.11924748867750168
epoch£º75	 i:6 	 global-step:1506	 l-p:0.1624264419078827
epoch£º75	 i:7 	 global-step:1507	 l-p:0.13571152091026306
epoch£º75	 i:8 	 global-step:1508	 l-p:0.09755304455757141
epoch£º75	 i:9 	 global-step:1509	 l-p:0.14521482586860657
====================================================================================================
====================================================================================================
====================================================================================================

epoch:76
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8382, 4.8387, 4.8382],
        [4.8382, 4.8382, 4.8382],
        [4.8382, 5.5727, 5.8270],
        [4.8382, 5.0656, 5.0078]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:76, step:0 
model_pd.l_p.mean(): 0.12407089024782181 
model_pd.l_d.mean(): -19.26133918762207 
model_pd.lagr.mean(): -19.13726806640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5277], device='cuda:0')), ('power', tensor([-20.1680], device='cuda:0'))])
epoch£º76	 i:0 	 global-step:1520	 l-p:0.12407089024782181
epoch£º76	 i:1 	 global-step:1521	 l-p:0.13845501840114594
epoch£º76	 i:2 	 global-step:1522	 l-p:0.26796755194664
epoch£º76	 i:3 	 global-step:1523	 l-p:0.1271076798439026
epoch£º76	 i:4 	 global-step:1524	 l-p:0.11960790306329727
epoch£º76	 i:5 	 global-step:1525	 l-p:0.11825977265834808
epoch£º76	 i:6 	 global-step:1526	 l-p:0.11106598377227783
epoch£º76	 i:7 	 global-step:1527	 l-p:0.11725761741399765
epoch£º76	 i:8 	 global-step:1528	 l-p:0.12485747784376144
epoch£º76	 i:9 	 global-step:1529	 l-p:0.16207991540431976
====================================================================================================
====================================================================================================
====================================================================================================

epoch:77
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9392, 4.9563, 4.9418],
        [4.9392, 4.9396, 4.9392],
        [4.9392, 4.9811, 4.9502],
        [4.9392, 5.0402, 4.9849]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:77, step:0 
model_pd.l_p.mean(): 0.12608706951141357 
model_pd.l_d.mean(): -20.28995704650879 
model_pd.lagr.mean(): -20.163869857788086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4306], device='cuda:0')), ('power', tensor([-21.1153], device='cuda:0'))])
epoch£º77	 i:0 	 global-step:1540	 l-p:0.12608706951141357
epoch£º77	 i:1 	 global-step:1541	 l-p:0.11670618504285812
epoch£º77	 i:2 	 global-step:1542	 l-p:0.1503300815820694
epoch£º77	 i:3 	 global-step:1543	 l-p:0.10294103622436523
epoch£º77	 i:4 	 global-step:1544	 l-p:0.13665185868740082
epoch£º77	 i:5 	 global-step:1545	 l-p:0.18643103539943695
epoch£º77	 i:6 	 global-step:1546	 l-p:0.1261066198348999
epoch£º77	 i:7 	 global-step:1547	 l-p:0.17263874411582947
epoch£º77	 i:8 	 global-step:1548	 l-p:0.1901833713054657
epoch£º77	 i:9 	 global-step:1549	 l-p:0.1394917368888855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:78
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8195, 5.4109, 5.5467],
        [4.8195, 5.6876, 6.0738],
        [4.8195, 4.9196, 4.8656],
        [4.8195, 4.8703, 4.8349]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:78, step:0 
model_pd.l_p.mean(): 0.14249160885810852 
model_pd.l_d.mean(): -19.762418746948242 
model_pd.lagr.mean(): -19.61992645263672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5105], device='cuda:0')), ('power', tensor([-20.6607], device='cuda:0'))])
epoch£º78	 i:0 	 global-step:1560	 l-p:0.14249160885810852
epoch£º78	 i:1 	 global-step:1561	 l-p:0.13443458080291748
epoch£º78	 i:2 	 global-step:1562	 l-p:0.1661682426929474
epoch£º78	 i:3 	 global-step:1563	 l-p:0.13384804129600525
epoch£º78	 i:4 	 global-step:1564	 l-p:0.16234903037548065
epoch£º78	 i:5 	 global-step:1565	 l-p:0.12497908622026443
epoch£º78	 i:6 	 global-step:1566	 l-p:0.08677155524492264
epoch£º78	 i:7 	 global-step:1567	 l-p:0.13335511088371277
epoch£º78	 i:8 	 global-step:1568	 l-p:0.1458100825548172
epoch£º78	 i:9 	 global-step:1569	 l-p:0.1412615180015564
====================================================================================================
====================================================================================================
====================================================================================================

epoch:79
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8202, 5.4614, 5.6377],
        [4.8202, 4.8203, 4.8202],
        [4.8202, 4.8218, 4.8203],
        [4.8202, 4.8245, 4.8205]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:79, step:0 
model_pd.l_p.mean(): 0.14820590615272522 
model_pd.l_d.mean(): -20.290103912353516 
model_pd.lagr.mean(): -20.141897201538086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4700], device='cuda:0')), ('power', tensor([-21.1563], device='cuda:0'))])
epoch£º79	 i:0 	 global-step:1580	 l-p:0.14820590615272522
epoch£º79	 i:1 	 global-step:1581	 l-p:0.1737671196460724
epoch£º79	 i:2 	 global-step:1582	 l-p:0.1371311992406845
epoch£º79	 i:3 	 global-step:1583	 l-p:0.13736781477928162
epoch£º79	 i:4 	 global-step:1584	 l-p:0.12785984575748444
epoch£º79	 i:5 	 global-step:1585	 l-p:0.195622518658638
epoch£º79	 i:6 	 global-step:1586	 l-p:0.2335711568593979
epoch£º79	 i:7 	 global-step:1587	 l-p:0.14394330978393555
epoch£º79	 i:8 	 global-step:1588	 l-p:0.15501286089420319
epoch£º79	 i:9 	 global-step:1589	 l-p:0.10060729831457138
====================================================================================================
====================================================================================================
====================================================================================================

epoch:80
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8186, 6.1566, 7.0821],
        [4.8186, 4.8347, 4.8210],
        [4.8186, 4.8186, 4.8186],
        [4.8186, 5.7545, 6.2120]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:80, step:0 
model_pd.l_p.mean(): 0.16898801922798157 
model_pd.l_d.mean(): -18.92485809326172 
model_pd.lagr.mean(): -18.755870819091797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5187], device='cuda:0')), ('power', tensor([-19.8160], device='cuda:0'))])
epoch£º80	 i:0 	 global-step:1600	 l-p:0.16898801922798157
epoch£º80	 i:1 	 global-step:1601	 l-p:0.12668275833129883
epoch£º80	 i:2 	 global-step:1602	 l-p:0.13870400190353394
epoch£º80	 i:3 	 global-step:1603	 l-p:0.12022987753152847
epoch£º80	 i:4 	 global-step:1604	 l-p:0.21688435971736908
epoch£º80	 i:5 	 global-step:1605	 l-p:0.13036280870437622
epoch£º80	 i:6 	 global-step:1606	 l-p:0.12778174877166748
epoch£º80	 i:7 	 global-step:1607	 l-p:0.13124680519104004
epoch£º80	 i:8 	 global-step:1608	 l-p:0.12617304921150208
epoch£º80	 i:9 	 global-step:1609	 l-p:0.13020405173301697
====================================================================================================
====================================================================================================
====================================================================================================

epoch:81
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9240, 4.9299, 4.9245],
        [4.9240, 4.9240, 4.9240],
        [4.9240, 5.8905, 6.3652],
        [4.9240, 4.9943, 4.9497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:81, step:0 
model_pd.l_p.mean(): 0.12173096090555191 
model_pd.l_d.mean(): -20.057641983032227 
model_pd.lagr.mean(): -19.935911178588867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4577], device='cuda:0')), ('power', tensor([-20.9068], device='cuda:0'))])
epoch£º81	 i:0 	 global-step:1620	 l-p:0.12173096090555191
epoch£º81	 i:1 	 global-step:1621	 l-p:0.14275720715522766
epoch£º81	 i:2 	 global-step:1622	 l-p:0.13062725961208344
epoch£º81	 i:3 	 global-step:1623	 l-p:0.15365320444107056
epoch£º81	 i:4 	 global-step:1624	 l-p:0.1636621057987213
epoch£º81	 i:5 	 global-step:1625	 l-p:0.35760557651519775
epoch£º81	 i:6 	 global-step:1626	 l-p:0.1323918253183365
epoch£º81	 i:7 	 global-step:1627	 l-p:-0.12056294828653336
epoch£º81	 i:8 	 global-step:1628	 l-p:0.8029140830039978
epoch£º81	 i:9 	 global-step:1629	 l-p:0.1356772631406784
====================================================================================================
====================================================================================================
====================================================================================================

epoch:82
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7377, 4.8035, 4.7617],
        [4.7377, 5.3809, 5.5688],
        [4.7377, 4.9379, 4.8798],
        [4.7377, 5.7665, 6.3387]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:82, step:0 
model_pd.l_p.mean(): 0.21993239223957062 
model_pd.l_d.mean(): -20.132558822631836 
model_pd.lagr.mean(): -19.912626266479492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5059], device='cuda:0')), ('power', tensor([-21.0330], device='cuda:0'))])
epoch£º82	 i:0 	 global-step:1640	 l-p:0.21993239223957062
epoch£º82	 i:1 	 global-step:1641	 l-p:0.12980693578720093
epoch£º82	 i:2 	 global-step:1642	 l-p:0.15229946374893188
epoch£º82	 i:3 	 global-step:1643	 l-p:0.16252893209457397
epoch£º82	 i:4 	 global-step:1644	 l-p:0.1354241818189621
epoch£º82	 i:5 	 global-step:1645	 l-p:0.1367146521806717
epoch£º82	 i:6 	 global-step:1646	 l-p:0.14887739717960358
epoch£º82	 i:7 	 global-step:1647	 l-p:0.13629558682441711
epoch£º82	 i:8 	 global-step:1648	 l-p:0.12808452546596527
epoch£º82	 i:9 	 global-step:1649	 l-p:0.1366811841726303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:83
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9572, 5.8774, 6.3001],
        [4.9572, 5.1926, 5.1342],
        [4.9572, 4.9578, 4.9572],
        [4.9572, 5.4633, 5.5274]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:83, step:0 
model_pd.l_p.mean(): 0.11839063465595245 
model_pd.l_d.mean(): -19.09731674194336 
model_pd.lagr.mean(): -18.978925704956055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4957], device='cuda:0')), ('power', tensor([-19.9678], device='cuda:0'))])
epoch£º83	 i:0 	 global-step:1660	 l-p:0.11839063465595245
epoch£º83	 i:1 	 global-step:1661	 l-p:0.105852872133255
epoch£º83	 i:2 	 global-step:1662	 l-p:0.13651233911514282
epoch£º83	 i:3 	 global-step:1663	 l-p:0.13026389479637146
epoch£º83	 i:4 	 global-step:1664	 l-p:0.126668319106102
epoch£º83	 i:5 	 global-step:1665	 l-p:0.12581805884838104
epoch£º83	 i:6 	 global-step:1666	 l-p:0.13185568153858185
epoch£º83	 i:7 	 global-step:1667	 l-p:0.1534423530101776
epoch£º83	 i:8 	 global-step:1668	 l-p:-0.5630128979682922
epoch£º83	 i:9 	 global-step:1669	 l-p:0.1373457908630371
====================================================================================================
====================================================================================================
====================================================================================================

epoch:84
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8858, 4.8860, 4.8858],
        [4.8858, 4.8942, 4.8867],
        [4.8858, 4.9589, 4.9135],
        [4.8858, 5.1153, 5.0580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:84, step:0 
model_pd.l_p.mean(): 0.13545915484428406 
model_pd.l_d.mean(): -20.12385368347168 
model_pd.lagr.mean(): -19.988393783569336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4722], device='cuda:0')), ('power', tensor([-20.9892], device='cuda:0'))])
epoch£º84	 i:0 	 global-step:1680	 l-p:0.13545915484428406
epoch£º84	 i:1 	 global-step:1681	 l-p:0.12253990769386292
epoch£º84	 i:2 	 global-step:1682	 l-p:0.13937799632549286
epoch£º84	 i:3 	 global-step:1683	 l-p:0.14818327128887177
epoch£º84	 i:4 	 global-step:1684	 l-p:0.12514443695545197
epoch£º84	 i:5 	 global-step:1685	 l-p:0.14622311294078827
epoch£º84	 i:6 	 global-step:1686	 l-p:0.18305686116218567
epoch£º84	 i:7 	 global-step:1687	 l-p:0.06393469870090485
epoch£º84	 i:8 	 global-step:1688	 l-p:0.13630664348602295
epoch£º84	 i:9 	 global-step:1689	 l-p:0.13465853035449982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:85
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6507,  0.5638,  1.0000,  0.4886,
          1.0000,  0.8665, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[4.8298, 5.5561, 5.8084],
        [4.8298, 5.2920, 5.3378],
        [4.8298, 5.6605, 6.0117],
        [4.8298, 5.1531, 5.1259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:85, step:0 
model_pd.l_p.mean(): 0.16135993599891663 
model_pd.l_d.mean(): -19.855619430541992 
model_pd.lagr.mean(): -19.694259643554688 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5066], device='cuda:0')), ('power', tensor([-20.7516], device='cuda:0'))])
epoch£º85	 i:0 	 global-step:1700	 l-p:0.16135993599891663
epoch£º85	 i:1 	 global-step:1701	 l-p:0.13553206622600555
epoch£º85	 i:2 	 global-step:1702	 l-p:0.09099464118480682
epoch£º85	 i:3 	 global-step:1703	 l-p:0.15790320932865143
epoch£º85	 i:4 	 global-step:1704	 l-p:0.15277116000652313
epoch£º85	 i:5 	 global-step:1705	 l-p:0.16673927009105682
epoch£º85	 i:6 	 global-step:1706	 l-p:0.11853966116905212
epoch£º85	 i:7 	 global-step:1707	 l-p:0.11984750628471375
epoch£º85	 i:8 	 global-step:1708	 l-p:0.13867110013961792
epoch£º85	 i:9 	 global-step:1709	 l-p:0.1243283823132515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:86
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9116, 5.7653, 6.1289],
        [4.9116, 4.9212, 4.9127],
        [4.9116, 4.9117, 4.9116],
        [4.9116, 5.2227, 5.1875]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:86, step:0 
model_pd.l_p.mean(): 0.4365776777267456 
model_pd.l_d.mean(): -17.214256286621094 
model_pd.lagr.mean(): -16.777679443359375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6587], device='cuda:0')), ('power', tensor([-18.2184], device='cuda:0'))])
epoch£º86	 i:0 	 global-step:1720	 l-p:0.4365776777267456
epoch£º86	 i:1 	 global-step:1721	 l-p:0.11653291434049606
epoch£º86	 i:2 	 global-step:1722	 l-p:0.1309950202703476
epoch£º86	 i:3 	 global-step:1723	 l-p:0.14011745154857635
epoch£º86	 i:4 	 global-step:1724	 l-p:0.12523874640464783
epoch£º86	 i:5 	 global-step:1725	 l-p:0.12507334351539612
epoch£º86	 i:6 	 global-step:1726	 l-p:0.13031978905200958
epoch£º86	 i:7 	 global-step:1727	 l-p:0.13566897809505463
epoch£º86	 i:8 	 global-step:1728	 l-p:0.14014723896980286
epoch£º86	 i:9 	 global-step:1729	 l-p:0.18963053822517395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:87
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6958, 4.7833, 4.7343],
        [4.6958, 4.6998, 4.6961],
        [4.6958, 4.6962, 4.6958],
        [4.6958, 5.1780, 5.2478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:87, step:0 
model_pd.l_p.mean(): 0.15051086246967316 
model_pd.l_d.mean(): -19.512590408325195 
model_pd.lagr.mean(): -19.362079620361328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5771], device='cuda:0')), ('power', tensor([-20.4752], device='cuda:0'))])
epoch£º87	 i:0 	 global-step:1740	 l-p:0.15051086246967316
epoch£º87	 i:1 	 global-step:1741	 l-p:0.1356448084115982
epoch£º87	 i:2 	 global-step:1742	 l-p:0.13549724221229553
epoch£º87	 i:3 	 global-step:1743	 l-p:0.16933505237102509
epoch£º87	 i:4 	 global-step:1744	 l-p:0.03602366894483566
epoch£º87	 i:5 	 global-step:1745	 l-p:-0.06205058842897415
epoch£º87	 i:6 	 global-step:1746	 l-p:0.13015085458755493
epoch£º87	 i:7 	 global-step:1747	 l-p:0.20438586175441742
epoch£º87	 i:8 	 global-step:1748	 l-p:0.14734944701194763
epoch£º87	 i:9 	 global-step:1749	 l-p:0.14569471776485443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:88
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8984, 4.8984, 4.8984],
        [4.8984, 5.1920, 5.1516],
        [4.8984, 4.9993, 4.9451],
        [4.8984, 4.9036, 4.8988]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:88, step:0 
model_pd.l_p.mean(): 0.15165109932422638 
model_pd.l_d.mean(): -20.48173713684082 
model_pd.lagr.mean(): -20.33008575439453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4267], device='cuda:0')), ('power', tensor([-21.3067], device='cuda:0'))])
epoch£º88	 i:0 	 global-step:1760	 l-p:0.15165109932422638
epoch£º88	 i:1 	 global-step:1761	 l-p:0.1260908544063568
epoch£º88	 i:2 	 global-step:1762	 l-p:0.12855683267116547
epoch£º88	 i:3 	 global-step:1763	 l-p:0.13853144645690918
epoch£º88	 i:4 	 global-step:1764	 l-p:0.1309606283903122
epoch£º88	 i:5 	 global-step:1765	 l-p:0.14677369594573975
epoch£º88	 i:6 	 global-step:1766	 l-p:0.14818687736988068
epoch£º88	 i:7 	 global-step:1767	 l-p:0.14774076640605927
epoch£º88	 i:8 	 global-step:1768	 l-p:0.11605382710695267
epoch£º88	 i:9 	 global-step:1769	 l-p:0.1177670955657959
====================================================================================================
====================================================================================================
====================================================================================================

epoch:89
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7831, 5.7478, 6.2463],
        [4.7831, 4.8071, 4.7879],
        [4.7831, 4.8155, 4.7908],
        [4.7831, 4.9324, 4.8724]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:89, step:0 
model_pd.l_p.mean(): 0.11368481069803238 
model_pd.l_d.mean(): -19.204280853271484 
model_pd.lagr.mean(): -19.090595245361328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5817], device='cuda:0')), ('power', tensor([-20.1658], device='cuda:0'))])
epoch£º89	 i:0 	 global-step:1780	 l-p:0.11368481069803238
epoch£º89	 i:1 	 global-step:1781	 l-p:0.15314839780330658
epoch£º89	 i:2 	 global-step:1782	 l-p:0.1324307769536972
epoch£º89	 i:3 	 global-step:1783	 l-p:0.14957758784294128
epoch£º89	 i:4 	 global-step:1784	 l-p:0.2120889276266098
epoch£º89	 i:5 	 global-step:1785	 l-p:0.14058683812618256
epoch£º89	 i:6 	 global-step:1786	 l-p:0.14483365416526794
epoch£º89	 i:7 	 global-step:1787	 l-p:-0.5807417035102844
epoch£º89	 i:8 	 global-step:1788	 l-p:0.1175289899110794
epoch£º89	 i:9 	 global-step:1789	 l-p:0.1271936595439911
====================================================================================================
====================================================================================================
====================================================================================================

epoch:90
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9849, 6.0989, 6.7304],
        [4.9849, 4.9849, 4.9848],
        [4.9849, 4.9849, 4.9848],
        [4.9849, 5.0228, 4.9944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:90, step:0 
model_pd.l_p.mean(): 0.16279853880405426 
model_pd.l_d.mean(): -19.22429847717285 
model_pd.lagr.mean(): -19.061500549316406 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4657], device='cuda:0')), ('power', tensor([-20.0661], device='cuda:0'))])
epoch£º90	 i:0 	 global-step:1800	 l-p:0.16279853880405426
epoch£º90	 i:1 	 global-step:1801	 l-p:0.11141839623451233
epoch£º90	 i:2 	 global-step:1802	 l-p:0.12177573889493942
epoch£º90	 i:3 	 global-step:1803	 l-p:0.11865373700857162
epoch£º90	 i:4 	 global-step:1804	 l-p:0.1198936402797699
epoch£º90	 i:5 	 global-step:1805	 l-p:0.13576149940490723
epoch£º90	 i:6 	 global-step:1806	 l-p:0.12247096747159958
epoch£º90	 i:7 	 global-step:1807	 l-p:0.15549981594085693
epoch£º90	 i:8 	 global-step:1808	 l-p:0.14576929807662964
epoch£º90	 i:9 	 global-step:1809	 l-p:0.1379633992910385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:91
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7803, 4.7803, 4.7803],
        [4.7803, 4.7807, 4.7803],
        [4.7803, 4.7862, 4.7808],
        [4.7803, 5.5333, 5.8193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:91, step:0 
model_pd.l_p.mean(): 0.13191814720630646 
model_pd.l_d.mean(): -19.168643951416016 
model_pd.lagr.mean(): -19.036725997924805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5006], device='cuda:0')), ('power', tensor([-20.0456], device='cuda:0'))])
epoch£º91	 i:0 	 global-step:1820	 l-p:0.13191814720630646
epoch£º91	 i:1 	 global-step:1821	 l-p:0.18529605865478516
epoch£º91	 i:2 	 global-step:1822	 l-p:0.2659892737865448
epoch£º91	 i:3 	 global-step:1823	 l-p:0.1359686255455017
epoch£º91	 i:4 	 global-step:1824	 l-p:0.1411457061767578
epoch£º91	 i:5 	 global-step:1825	 l-p:0.16560541093349457
epoch£º91	 i:6 	 global-step:1826	 l-p:0.10238049924373627
epoch£º91	 i:7 	 global-step:1827	 l-p:0.12973876297473907
epoch£º91	 i:8 	 global-step:1828	 l-p:0.13120858371257782
epoch£º91	 i:9 	 global-step:1829	 l-p:0.15394416451454163
====================================================================================================
====================================================================================================
====================================================================================================

epoch:92
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8941, 4.8941, 4.8941],
        [4.8941, 5.9321, 6.4946],
        [4.8941, 5.0397, 4.9786],
        [4.8941, 4.9530, 4.9139]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:92, step:0 
model_pd.l_p.mean(): 0.1330622285604477 
model_pd.l_d.mean(): -19.45956802368164 
model_pd.lagr.mean(): -19.326505661010742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5361], device='cuda:0')), ('power', tensor([-20.3787], device='cuda:0'))])
epoch£º92	 i:0 	 global-step:1840	 l-p:0.1330622285604477
epoch£º92	 i:1 	 global-step:1841	 l-p:0.13222329318523407
epoch£º92	 i:2 	 global-step:1842	 l-p:0.12015756964683533
epoch£º92	 i:3 	 global-step:1843	 l-p:-0.6382534503936768
epoch£º92	 i:4 	 global-step:1844	 l-p:0.13794198632240295
epoch£º92	 i:5 	 global-step:1845	 l-p:0.14407792687416077
epoch£º92	 i:6 	 global-step:1846	 l-p:0.15452659130096436
epoch£º92	 i:7 	 global-step:1847	 l-p:0.14241935312747955
epoch£º92	 i:8 	 global-step:1848	 l-p:0.1054496094584465
epoch£º92	 i:9 	 global-step:1849	 l-p:0.12788067758083344
====================================================================================================
====================================================================================================
====================================================================================================

epoch:93
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9151, 4.9151, 4.9151],
        [4.9151, 5.2812, 5.2708],
        [4.9151, 5.3151, 5.3218],
        [4.9151, 5.1894, 5.1436]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:93, step:0 
model_pd.l_p.mean(): 0.12549300491809845 
model_pd.l_d.mean(): -19.516698837280273 
model_pd.lagr.mean(): -19.391206741333008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4816], device='cuda:0')), ('power', tensor([-20.3804], device='cuda:0'))])
epoch£º93	 i:0 	 global-step:1860	 l-p:0.12549300491809845
epoch£º93	 i:1 	 global-step:1861	 l-p:0.12939795851707458
epoch£º93	 i:2 	 global-step:1862	 l-p:0.12386495620012283
epoch£º93	 i:3 	 global-step:1863	 l-p:0.14073309302330017
epoch£º93	 i:4 	 global-step:1864	 l-p:0.1523660570383072
epoch£º93	 i:5 	 global-step:1865	 l-p:0.1820743829011917
epoch£º93	 i:6 	 global-step:1866	 l-p:0.1418405920267105
epoch£º93	 i:7 	 global-step:1867	 l-p:0.14880533516407013
epoch£º93	 i:8 	 global-step:1868	 l-p:-0.14079272747039795
epoch£º93	 i:9 	 global-step:1869	 l-p:0.12688638269901276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:94
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9764, 5.5207, 5.6144],
        [4.9764, 4.9764, 4.9764],
        [4.9764, 5.0208, 4.9888],
        [4.9764, 4.9909, 4.9785]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:94, step:0 
model_pd.l_p.mean(): 0.1499616950750351 
model_pd.l_d.mean(): -20.534931182861328 
model_pd.lagr.mean(): -20.38496971130371 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3970], device='cuda:0')), ('power', tensor([-21.3301], device='cuda:0'))])
epoch£º94	 i:0 	 global-step:1880	 l-p:0.1499616950750351
epoch£º94	 i:1 	 global-step:1881	 l-p:0.11379991471767426
epoch£º94	 i:2 	 global-step:1882	 l-p:0.1773577332496643
epoch£º94	 i:3 	 global-step:1883	 l-p:0.12487872689962387
epoch£º94	 i:4 	 global-step:1884	 l-p:0.12431786954402924
epoch£º94	 i:5 	 global-step:1885	 l-p:0.11674708127975464
epoch£º94	 i:6 	 global-step:1886	 l-p:0.11379043012857437
epoch£º94	 i:7 	 global-step:1887	 l-p:0.12135353684425354
epoch£º94	 i:8 	 global-step:1888	 l-p:0.13112352788448334
epoch£º94	 i:9 	 global-step:1889	 l-p:0.15852861106395721
====================================================================================================
====================================================================================================
====================================================================================================

epoch:95
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8891, 4.8892, 4.8891],
        [4.8891, 4.8891, 4.8891],
        [4.8891, 4.8905, 4.8892],
        [4.8891, 4.9045, 4.8914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:95, step:0 
model_pd.l_p.mean(): 0.12313015013933182 
model_pd.l_d.mean(): -19.1104679107666 
model_pd.lagr.mean(): -18.987337112426758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5234], device='cuda:0')), ('power', tensor([-20.0099], device='cuda:0'))])
epoch£º95	 i:0 	 global-step:1900	 l-p:0.12313015013933182
epoch£º95	 i:1 	 global-step:1901	 l-p:0.1487772911787033
epoch£º95	 i:2 	 global-step:1902	 l-p:0.13629043102264404
epoch£º95	 i:3 	 global-step:1903	 l-p:0.12910810112953186
epoch£º95	 i:4 	 global-step:1904	 l-p:0.1271217316389084
epoch£º95	 i:5 	 global-step:1905	 l-p:0.21034623682498932
epoch£º95	 i:6 	 global-step:1906	 l-p:0.13453663885593414
epoch£º95	 i:7 	 global-step:1907	 l-p:0.13363207876682281
epoch£º95	 i:8 	 global-step:1908	 l-p:0.1342274695634842
epoch£º95	 i:9 	 global-step:1909	 l-p:0.2869046628475189
====================================================================================================
====================================================================================================
====================================================================================================

epoch:96
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8113, 5.9460, 6.6353],
        [4.8113, 4.8119, 4.8113],
        [4.8113, 5.5592, 5.8395],
        [4.8113, 4.8113, 4.8113]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:96, step:0 
model_pd.l_p.mean(): 0.10279186069965363 
model_pd.l_d.mean(): -18.21607208251953 
model_pd.lagr.mean(): -18.113279342651367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6397], device='cuda:0')), ('power', tensor([-19.2193], device='cuda:0'))])
epoch£º96	 i:0 	 global-step:1920	 l-p:0.10279186069965363
epoch£º96	 i:1 	 global-step:1921	 l-p:0.14725537598133087
epoch£º96	 i:2 	 global-step:1922	 l-p:0.12060423940420151
epoch£º96	 i:3 	 global-step:1923	 l-p:0.13362106680870056
epoch£º96	 i:4 	 global-step:1924	 l-p:0.1382628232240677
epoch£º96	 i:5 	 global-step:1925	 l-p:0.13662321865558624
epoch£º96	 i:6 	 global-step:1926	 l-p:0.181923970580101
epoch£º96	 i:7 	 global-step:1927	 l-p:0.12055926024913788
epoch£º96	 i:8 	 global-step:1928	 l-p:0.14104805886745453
epoch£º96	 i:9 	 global-step:1929	 l-p:0.12763100862503052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:97
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0220, 5.5009, 5.5484],
        [5.0220, 5.8777, 6.2356],
        [5.0220, 5.0220, 5.0220],
        [5.0220, 6.3316, 7.1909]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:97, step:0 
model_pd.l_p.mean(): 0.1531805694103241 
model_pd.l_d.mean(): -20.14373779296875 
model_pd.lagr.mean(): -19.990556716918945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4198], device='cuda:0')), ('power', tensor([-20.9552], device='cuda:0'))])
epoch£º97	 i:0 	 global-step:1940	 l-p:0.1531805694103241
epoch£º97	 i:1 	 global-step:1941	 l-p:0.12259449064731598
epoch£º97	 i:2 	 global-step:1942	 l-p:0.10759726166725159
epoch£º97	 i:3 	 global-step:1943	 l-p:0.14675284922122955
epoch£º97	 i:4 	 global-step:1944	 l-p:0.1794503778219223
epoch£º97	 i:5 	 global-step:1945	 l-p:0.1372629702091217
epoch£º97	 i:6 	 global-step:1946	 l-p:0.11438813805580139
epoch£º97	 i:7 	 global-step:1947	 l-p:0.13089148700237274
epoch£º97	 i:8 	 global-step:1948	 l-p:0.14155778288841248
epoch£º97	 i:9 	 global-step:1949	 l-p:0.12589341402053833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:98
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7536, 5.1185, 5.1182],
        [4.7536, 4.7536, 4.7536],
        [4.7536, 4.7931, 4.7644],
        [4.7536, 5.3507, 5.5067]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:98, step:0 
model_pd.l_p.mean(): 0.13104739785194397 
model_pd.l_d.mean(): -20.154741287231445 
model_pd.lagr.mean(): -20.023693084716797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5045], device='cuda:0')), ('power', tensor([-21.0542], device='cuda:0'))])
epoch£º98	 i:0 	 global-step:1960	 l-p:0.13104739785194397
epoch£º98	 i:1 	 global-step:1961	 l-p:0.14675264060497284
epoch£º98	 i:2 	 global-step:1962	 l-p:0.1273004710674286
epoch£º98	 i:3 	 global-step:1963	 l-p:-0.20573684573173523
epoch£º98	 i:4 	 global-step:1964	 l-p:0.16805095970630646
epoch£º98	 i:5 	 global-step:1965	 l-p:0.1755315512418747
epoch£º98	 i:6 	 global-step:1966	 l-p:0.13555021584033966
epoch£º98	 i:7 	 global-step:1967	 l-p:0.1452154666185379
epoch£º98	 i:8 	 global-step:1968	 l-p:0.12316364049911499
epoch£º98	 i:9 	 global-step:1969	 l-p:0.13719616830348969
====================================================================================================
====================================================================================================
====================================================================================================

epoch:99
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9218, 4.9218, 4.9218],
        [4.9218, 4.9219, 4.9218],
        [4.9218, 5.3827, 5.4269],
        [4.9218, 4.9218, 4.9218]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:99, step:0 
model_pd.l_p.mean(): 0.14549626410007477 
model_pd.l_d.mean(): -19.245912551879883 
model_pd.lagr.mean(): -19.10041618347168 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4575], device='cuda:0')), ('power', tensor([-20.0796], device='cuda:0'))])
epoch£º99	 i:0 	 global-step:1980	 l-p:0.14549626410007477
epoch£º99	 i:1 	 global-step:1981	 l-p:0.14154790341854095
epoch£º99	 i:2 	 global-step:1982	 l-p:0.11253118515014648
epoch£º99	 i:3 	 global-step:1983	 l-p:0.14440174400806427
epoch£º99	 i:4 	 global-step:1984	 l-p:0.12545621395111084
epoch£º99	 i:5 	 global-step:1985	 l-p:-0.020252756774425507
epoch£º99	 i:6 	 global-step:1986	 l-p:0.12308815866708755
epoch£º99	 i:7 	 global-step:1987	 l-p:0.1601591408252716
epoch£º99	 i:8 	 global-step:1988	 l-p:0.1265559196472168
epoch£º99	 i:9 	 global-step:1989	 l-p:0.1393345594406128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8812, 4.8818, 4.8812],
        [4.8812, 5.3794, 5.4513],
        [4.8812, 4.9688, 4.9193],
        [4.8812, 4.8812, 4.8812]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:100, step:0 
model_pd.l_p.mean(): 0.16113349795341492 
model_pd.l_d.mean(): -20.5778751373291 
model_pd.lagr.mean(): -20.4167423248291 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4220], device='cuda:0')), ('power', tensor([-21.3997], device='cuda:0'))])
epoch£º100	 i:0 	 global-step:2000	 l-p:0.16113349795341492
epoch£º100	 i:1 	 global-step:2001	 l-p:0.13307920098304749
epoch£º100	 i:2 	 global-step:2002	 l-p:0.10963118076324463
epoch£º100	 i:3 	 global-step:2003	 l-p:0.11720352619886398
epoch£º100	 i:4 	 global-step:2004	 l-p:0.13889101147651672
epoch£º100	 i:5 	 global-step:2005	 l-p:0.1433415710926056
epoch£º100	 i:6 	 global-step:2006	 l-p:0.13752765953540802
epoch£º100	 i:7 	 global-step:2007	 l-p:0.47711262106895447
epoch£º100	 i:8 	 global-step:2008	 l-p:0.7777506709098816
epoch£º100	 i:9 	 global-step:2009	 l-p:0.16667065024375916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8264, 4.8854, 4.8469],
        [4.8264, 4.8265, 4.8264],
        [4.8264, 4.8285, 4.8265],
        [4.8264, 4.8264, 4.8264]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:101, step:0 
model_pd.l_p.mean(): 0.13523083925247192 
model_pd.l_d.mean(): -19.943328857421875 
model_pd.lagr.mean(): -19.80809783935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4805], device='cuda:0')), ('power', tensor([-20.8139], device='cuda:0'))])
epoch£º101	 i:0 	 global-step:2020	 l-p:0.13523083925247192
epoch£º101	 i:1 	 global-step:2021	 l-p:0.11621108651161194
epoch£º101	 i:2 	 global-step:2022	 l-p:0.10479407012462616
epoch£º101	 i:3 	 global-step:2023	 l-p:-0.06051579490303993
epoch£º101	 i:4 	 global-step:2024	 l-p:0.13449493050575256
epoch£º101	 i:5 	 global-step:2025	 l-p:0.13281668722629547
epoch£º101	 i:6 	 global-step:2026	 l-p:0.11729686707258224
epoch£º101	 i:7 	 global-step:2027	 l-p:0.12430445104837418
epoch£º101	 i:8 	 global-step:2028	 l-p:0.14241717755794525
epoch£º101	 i:9 	 global-step:2029	 l-p:0.1373659074306488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:102
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5760,  0.4793,  1.0000,  0.3988,
          1.0000,  0.8321, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5791,  0.4826,  1.0000,  0.4023,
          1.0000,  0.8335, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9034,  0.8733,  1.0000,  0.8442,
          1.0000,  0.9667, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2169,  0.1303,  1.0000,  0.0783,
          1.0000,  0.6008, 31.6228]], device='cuda:0')
 pt:tensor([[4.9909, 5.7091, 5.9456],
        [4.9909, 5.7140, 5.9547],
        [4.9909, 6.2236, 6.9995],
        [4.9909, 5.1622, 5.1000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:102, step:0 
model_pd.l_p.mean(): 0.11455141007900238 
model_pd.l_d.mean(): -19.91588592529297 
model_pd.lagr.mean(): -19.801334381103516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4601], device='cuda:0')), ('power', tensor([-20.7648], device='cuda:0'))])
epoch£º102	 i:0 	 global-step:2040	 l-p:0.11455141007900238
epoch£º102	 i:1 	 global-step:2041	 l-p:0.16053582727909088
epoch£º102	 i:2 	 global-step:2042	 l-p:-0.02214888483285904
epoch£º102	 i:3 	 global-step:2043	 l-p:0.14532452821731567
epoch£º102	 i:4 	 global-step:2044	 l-p:0.13984431326389313
epoch£º102	 i:5 	 global-step:2045	 l-p:0.13473853468894958
epoch£º102	 i:6 	 global-step:2046	 l-p:0.14285023510456085
epoch£º102	 i:7 	 global-step:2047	 l-p:0.12777705490589142
epoch£º102	 i:8 	 global-step:2048	 l-p:0.1453656405210495
epoch£º102	 i:9 	 global-step:2049	 l-p:0.2454943060874939
====================================================================================================
====================================================================================================
====================================================================================================

epoch:103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6799, 4.7704, 4.7218],
        [4.6799, 5.0051, 4.9911],
        [4.6799, 4.7642, 4.7173],
        [4.6799, 4.6898, 4.6812]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:103, step:0 
model_pd.l_p.mean(): 0.1251544952392578 
model_pd.l_d.mean(): -19.7410888671875 
model_pd.lagr.mean(): -19.615934371948242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5633], device='cuda:0')), ('power', tensor([-20.6936], device='cuda:0'))])
epoch£º103	 i:0 	 global-step:2060	 l-p:0.1251544952392578
epoch£º103	 i:1 	 global-step:2061	 l-p:0.08715184032917023
epoch£º103	 i:2 	 global-step:2062	 l-p:0.15404637157917023
epoch£º103	 i:3 	 global-step:2063	 l-p:0.163038969039917
epoch£º103	 i:4 	 global-step:2064	 l-p:0.1326133757829666
epoch£º103	 i:5 	 global-step:2065	 l-p:0.13881325721740723
epoch£º103	 i:6 	 global-step:2066	 l-p:0.13010886311531067
epoch£º103	 i:7 	 global-step:2067	 l-p:0.1515791267156601
epoch£º103	 i:8 	 global-step:2068	 l-p:0.12999442219734192
epoch£º103	 i:9 	 global-step:2069	 l-p:0.12749215960502625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9762, 4.9780, 4.9763],
        [4.9762, 5.1235, 5.0625],
        [4.9762, 5.0344, 4.9958],
        [4.9762, 5.9084, 6.3535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:104, step:0 
model_pd.l_p.mean(): 0.12842687964439392 
model_pd.l_d.mean(): -20.549699783325195 
model_pd.lagr.mean(): -20.42127227783203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3946], device='cuda:0')), ('power', tensor([-21.3427], device='cuda:0'))])
epoch£º104	 i:0 	 global-step:2080	 l-p:0.12842687964439392
epoch£º104	 i:1 	 global-step:2081	 l-p:0.2763088047504425
epoch£º104	 i:2 	 global-step:2082	 l-p:0.15158018469810486
epoch£º104	 i:3 	 global-step:2083	 l-p:0.1559964120388031
epoch£º104	 i:4 	 global-step:2084	 l-p:0.12848976254463196
epoch£º104	 i:5 	 global-step:2085	 l-p:0.12579144537448883
epoch£º104	 i:6 	 global-step:2086	 l-p:0.12470557540655136
epoch£º104	 i:7 	 global-step:2087	 l-p:0.1430474817752838
epoch£º104	 i:8 	 global-step:2088	 l-p:0.12391063570976257
epoch£º104	 i:9 	 global-step:2089	 l-p:0.13200080394744873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8467, 4.8467, 4.8467],
        [4.8467, 4.8467, 4.8467],
        [4.8467, 5.0496, 4.9932],
        [4.8467, 5.3681, 5.4611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:105, step:0 
model_pd.l_p.mean(): 0.15877088904380798 
model_pd.l_d.mean(): -20.185224533081055 
model_pd.lagr.mean(): -20.026453018188477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4673], device='cuda:0')), ('power', tensor([-21.0466], device='cuda:0'))])
epoch£º105	 i:0 	 global-step:2100	 l-p:0.15877088904380798
epoch£º105	 i:1 	 global-step:2101	 l-p:0.16085006296634674
epoch£º105	 i:2 	 global-step:2102	 l-p:0.12373700737953186
epoch£º105	 i:3 	 global-step:2103	 l-p:0.14029574394226074
epoch£º105	 i:4 	 global-step:2104	 l-p:0.14363108575344086
epoch£º105	 i:5 	 global-step:2105	 l-p:0.18810561299324036
epoch£º105	 i:6 	 global-step:2106	 l-p:0.16599559783935547
epoch£º105	 i:7 	 global-step:2107	 l-p:0.17462627589702606
epoch£º105	 i:8 	 global-step:2108	 l-p:0.07795026153326035
epoch£º105	 i:9 	 global-step:2109	 l-p:0.12919853627681732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9053, 4.9081, 4.9054],
        [4.9053, 5.6099, 5.8457],
        [4.9053, 4.9236, 4.9084],
        [4.9053, 5.8172, 6.2526]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:106, step:0 
model_pd.l_p.mean(): 0.13373254239559174 
model_pd.l_d.mean(): -18.371158599853516 
model_pd.lagr.mean(): -18.2374267578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5652], device='cuda:0')), ('power', tensor([-19.3000], device='cuda:0'))])
epoch£º106	 i:0 	 global-step:2120	 l-p:0.13373254239559174
epoch£º106	 i:1 	 global-step:2121	 l-p:0.14318834245204926
epoch£º106	 i:2 	 global-step:2122	 l-p:0.1331077367067337
epoch£º106	 i:3 	 global-step:2123	 l-p:0.13347285985946655
epoch£º106	 i:4 	 global-step:2124	 l-p:0.12373141944408417
epoch£º106	 i:5 	 global-step:2125	 l-p:0.12906484305858612
epoch£º106	 i:6 	 global-step:2126	 l-p:-1.6980481147766113
epoch£º106	 i:7 	 global-step:2127	 l-p:0.1345648616552353
epoch£º106	 i:8 	 global-step:2128	 l-p:0.13244011998176575
epoch£º106	 i:9 	 global-step:2129	 l-p:0.11528056114912033
====================================================================================================
====================================================================================================
====================================================================================================

epoch:107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9065, 4.9091, 4.9067],
        [4.9065, 4.9116, 4.9069],
        [4.9065, 5.9336, 6.4918],
        [4.9065, 5.7484, 6.1133]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:107, step:0 
model_pd.l_p.mean(): 0.1443273276090622 
model_pd.l_d.mean(): -19.220703125 
model_pd.lagr.mean(): -19.07637596130371 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5040], device='cuda:0')), ('power', tensor([-20.1022], device='cuda:0'))])
epoch£º107	 i:0 	 global-step:2140	 l-p:0.1443273276090622
epoch£º107	 i:1 	 global-step:2141	 l-p:0.12064369767904282
epoch£º107	 i:2 	 global-step:2142	 l-p:-0.11078473180532455
epoch£º107	 i:3 	 global-step:2143	 l-p:0.1222730204463005
epoch£º107	 i:4 	 global-step:2144	 l-p:0.12821197509765625
epoch£º107	 i:5 	 global-step:2145	 l-p:0.13112863898277283
epoch£º107	 i:6 	 global-step:2146	 l-p:0.17041625082492828
epoch£º107	 i:7 	 global-step:2147	 l-p:0.1491759866476059
epoch£º107	 i:8 	 global-step:2148	 l-p:0.12664727866649628
epoch£º107	 i:9 	 global-step:2149	 l-p:0.1380816549062729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8255, 4.8356, 4.8267],
        [4.8255, 4.8256, 4.8255],
        [4.8255, 4.9202, 4.8696],
        [4.8255, 4.8255, 4.8255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:108, step:0 
model_pd.l_p.mean(): 0.1306043267250061 
model_pd.l_d.mean(): -18.70357894897461 
model_pd.lagr.mean(): -18.572975158691406 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5405], device='cuda:0')), ('power', tensor([-19.6132], device='cuda:0'))])
epoch£º108	 i:0 	 global-step:2160	 l-p:0.1306043267250061
epoch£º108	 i:1 	 global-step:2161	 l-p:0.16381947696208954
epoch£º108	 i:2 	 global-step:2162	 l-p:0.10568228363990784
epoch£º108	 i:3 	 global-step:2163	 l-p:0.13614319264888763
epoch£º108	 i:4 	 global-step:2164	 l-p:0.13892248272895813
epoch£º108	 i:5 	 global-step:2165	 l-p:0.15474224090576172
epoch£º108	 i:6 	 global-step:2166	 l-p:0.1277403086423874
epoch£º108	 i:7 	 global-step:2167	 l-p:0.1266641467809677
epoch£º108	 i:8 	 global-step:2168	 l-p:0.1327863484621048
epoch£º108	 i:9 	 global-step:2169	 l-p:0.1613410860300064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9348, 5.0183, 4.9702],
        [4.9348, 4.9363, 4.9349],
        [4.9348, 6.1931, 7.0144],
        [4.9348, 4.9348, 4.9348]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:109, step:0 
model_pd.l_p.mean(): 0.11570652574300766 
model_pd.l_d.mean(): -19.854351043701172 
model_pd.lagr.mean(): -19.738643646240234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4730], device='cuda:0')), ('power', tensor([-20.7155], device='cuda:0'))])
epoch£º109	 i:0 	 global-step:2180	 l-p:0.11570652574300766
epoch£º109	 i:1 	 global-step:2181	 l-p:0.13407263159751892
epoch£º109	 i:2 	 global-step:2182	 l-p:0.13086365163326263
epoch£º109	 i:3 	 global-step:2183	 l-p:0.14565224945545197
epoch£º109	 i:4 	 global-step:2184	 l-p:0.32376641035079956
epoch£º109	 i:5 	 global-step:2185	 l-p:0.13928455114364624
epoch£º109	 i:6 	 global-step:2186	 l-p:0.14242681860923767
epoch£º109	 i:7 	 global-step:2187	 l-p:0.12323945760726929
epoch£º109	 i:8 	 global-step:2188	 l-p:0.13373635709285736
epoch£º109	 i:9 	 global-step:2189	 l-p:0.11671864986419678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:110
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9473, 4.9477, 4.9473],
        [4.9473, 4.9473, 4.9473],
        [4.9473, 4.9483, 4.9473],
        [4.9473, 4.9678, 4.9511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:110, step:0 
model_pd.l_p.mean(): 0.4301885962486267 
model_pd.l_d.mean(): -20.64302635192871 
model_pd.lagr.mean(): -20.21283721923828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3933], device='cuda:0')), ('power', tensor([-21.4364], device='cuda:0'))])
epoch£º110	 i:0 	 global-step:2200	 l-p:0.4301885962486267
epoch£º110	 i:1 	 global-step:2201	 l-p:0.13120509684085846
epoch£º110	 i:2 	 global-step:2202	 l-p:0.09824392199516296
epoch£º110	 i:3 	 global-step:2203	 l-p:0.13433893024921417
epoch£º110	 i:4 	 global-step:2204	 l-p:0.14354781806468964
epoch£º110	 i:5 	 global-step:2205	 l-p:0.13453997671604156
epoch£º110	 i:6 	 global-step:2206	 l-p:0.13874056935310364
epoch£º110	 i:7 	 global-step:2207	 l-p:0.13304835557937622
epoch£º110	 i:8 	 global-step:2208	 l-p:0.22281000018119812
epoch£º110	 i:9 	 global-step:2209	 l-p:0.21226519346237183
====================================================================================================
====================================================================================================
====================================================================================================

epoch:111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8684, 4.8727, 4.8687],
        [4.8684, 5.5470, 5.7669],
        [4.8684, 5.3822, 5.4709],
        [4.8684, 5.7335, 6.1294]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:111, step:0 
model_pd.l_p.mean(): 0.16930921375751495 
model_pd.l_d.mean(): -19.927440643310547 
model_pd.lagr.mean(): -19.75813102722168 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4912], device='cuda:0')), ('power', tensor([-20.8088], device='cuda:0'))])
epoch£º111	 i:0 	 global-step:2220	 l-p:0.16930921375751495
epoch£º111	 i:1 	 global-step:2221	 l-p:0.13469848036766052
epoch£º111	 i:2 	 global-step:2222	 l-p:0.37045130133628845
epoch£º111	 i:3 	 global-step:2223	 l-p:0.12724407017230988
epoch£º111	 i:4 	 global-step:2224	 l-p:0.1179097592830658
epoch£º111	 i:5 	 global-step:2225	 l-p:0.12331700325012207
epoch£º111	 i:6 	 global-step:2226	 l-p:0.11520052701234818
epoch£º111	 i:7 	 global-step:2227	 l-p:0.14073991775512695
epoch£º111	 i:8 	 global-step:2228	 l-p:0.13631175458431244
epoch£º111	 i:9 	 global-step:2229	 l-p:0.11079830676317215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:112
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9584, 5.0225, 4.9817],
        [4.9584, 5.0205, 4.9805],
        [4.9584, 4.9585, 4.9584],
        [4.9584, 4.9805, 4.9627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:112, step:0 
model_pd.l_p.mean(): 0.14768674969673157 
model_pd.l_d.mean(): -20.386571884155273 
model_pd.lagr.mean(): -20.2388858795166 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4249], device='cuda:0')), ('power', tensor([-21.2079], device='cuda:0'))])
epoch£º112	 i:0 	 global-step:2240	 l-p:0.14768674969673157
epoch£º112	 i:1 	 global-step:2241	 l-p:-0.08561042696237564
epoch£º112	 i:2 	 global-step:2242	 l-p:0.15538497269153595
epoch£º112	 i:3 	 global-step:2243	 l-p:0.13483309745788574
epoch£º112	 i:4 	 global-step:2244	 l-p:0.13445881009101868
epoch£º112	 i:5 	 global-step:2245	 l-p:0.153907909989357
epoch£º112	 i:6 	 global-step:2246	 l-p:0.11893392354249954
epoch£º112	 i:7 	 global-step:2247	 l-p:0.16327840089797974
epoch£º112	 i:8 	 global-step:2248	 l-p:0.13059388101100922
epoch£º112	 i:9 	 global-step:2249	 l-p:0.13301385939121246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8155, 4.8155, 4.8155],
        [4.8155, 5.4855, 5.7048],
        [4.8155, 4.8756, 4.8371],
        [4.8155, 4.8155, 4.8155]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:113, step:0 
model_pd.l_p.mean(): 0.0946178212761879 
model_pd.l_d.mean(): -19.342649459838867 
model_pd.lagr.mean(): -19.248031616210938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5209], device='cuda:0')), ('power', tensor([-20.2439], device='cuda:0'))])
epoch£º113	 i:0 	 global-step:2260	 l-p:0.0946178212761879
epoch£º113	 i:1 	 global-step:2261	 l-p:0.13559302687644958
epoch£º113	 i:2 	 global-step:2262	 l-p:0.12899155914783478
epoch£º113	 i:3 	 global-step:2263	 l-p:0.14230534434318542
epoch£º113	 i:4 	 global-step:2264	 l-p:0.1376352310180664
epoch£º113	 i:5 	 global-step:2265	 l-p:0.12665332853794098
epoch£º113	 i:6 	 global-step:2266	 l-p:0.30303841829299927
epoch£º113	 i:7 	 global-step:2267	 l-p:0.13657128810882568
epoch£º113	 i:8 	 global-step:2268	 l-p:0.14618784189224243
epoch£º113	 i:9 	 global-step:2269	 l-p:0.5625423192977905
====================================================================================================
====================================================================================================
====================================================================================================

epoch:114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8173, 4.8173, 4.8173],
        [4.8173, 4.8188, 4.8174],
        [4.8173, 4.8181, 4.8173],
        [4.8173, 5.6866, 6.0957]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:114, step:0 
model_pd.l_p.mean(): 0.12508508563041687 
model_pd.l_d.mean(): -19.43882179260254 
model_pd.lagr.mean(): -19.313735961914062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5487], device='cuda:0')), ('power', tensor([-20.3706], device='cuda:0'))])
epoch£º114	 i:0 	 global-step:2280	 l-p:0.12508508563041687
epoch£º114	 i:1 	 global-step:2281	 l-p:0.14055640995502472
epoch£º114	 i:2 	 global-step:2282	 l-p:0.14101621508598328
epoch£º114	 i:3 	 global-step:2283	 l-p:0.15525661408901215
epoch£º114	 i:4 	 global-step:2284	 l-p:0.13034874200820923
epoch£º114	 i:5 	 global-step:2285	 l-p:0.13000808656215668
epoch£º114	 i:6 	 global-step:2286	 l-p:0.12732307612895966
epoch£º114	 i:7 	 global-step:2287	 l-p:0.1328553557395935
epoch£º114	 i:8 	 global-step:2288	 l-p:0.08132311701774597
epoch£º114	 i:9 	 global-step:2289	 l-p:0.13581666350364685
====================================================================================================
====================================================================================================
====================================================================================================

epoch:115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8624, 4.8624, 4.8624],
        [4.8624, 4.8624, 4.8624],
        [4.8624, 4.8624, 4.8624],
        [4.8624, 5.0558, 4.9996]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:115, step:0 
model_pd.l_p.mean(): 0.1271410435438156 
model_pd.l_d.mean(): -20.103099822998047 
model_pd.lagr.mean(): -19.9759578704834 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4756], device='cuda:0')), ('power', tensor([-20.9717], device='cuda:0'))])
epoch£º115	 i:0 	 global-step:2300	 l-p:0.1271410435438156
epoch£º115	 i:1 	 global-step:2301	 l-p:0.11588622629642487
epoch£º115	 i:2 	 global-step:2302	 l-p:0.12097124010324478
epoch£º115	 i:3 	 global-step:2303	 l-p:0.5434173345565796
epoch£º115	 i:4 	 global-step:2304	 l-p:0.1707300841808319
epoch£º115	 i:5 	 global-step:2305	 l-p:0.13094092905521393
epoch£º115	 i:6 	 global-step:2306	 l-p:0.1363850235939026
epoch£º115	 i:7 	 global-step:2307	 l-p:0.13609974086284637
epoch£º115	 i:8 	 global-step:2308	 l-p:0.37368717789649963
epoch£º115	 i:9 	 global-step:2309	 l-p:0.19024476408958435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7837, 4.8588, 4.8148],
        [4.7837, 4.8181, 4.7927],
        [4.7837, 4.7837, 4.7837],
        [4.7837, 4.9754, 4.9211]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:116, step:0 
model_pd.l_p.mean(): 0.11764809489250183 
model_pd.l_d.mean(): -19.593963623046875 
model_pd.lagr.mean(): -19.476316452026367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5360], device='cuda:0')), ('power', tensor([-20.5155], device='cuda:0'))])
epoch£º116	 i:0 	 global-step:2320	 l-p:0.11764809489250183
epoch£º116	 i:1 	 global-step:2321	 l-p:0.1406383514404297
epoch£º116	 i:2 	 global-step:2322	 l-p:0.3359704911708832
epoch£º116	 i:3 	 global-step:2323	 l-p:0.15048091113567352
epoch£º116	 i:4 	 global-step:2324	 l-p:0.10235664993524551
epoch£º116	 i:5 	 global-step:2325	 l-p:0.17233474552631378
epoch£º116	 i:6 	 global-step:2326	 l-p:0.15219159424304962
epoch£º116	 i:7 	 global-step:2327	 l-p:0.13702242076396942
epoch£º116	 i:8 	 global-step:2328	 l-p:0.10875162482261658
epoch£º116	 i:9 	 global-step:2329	 l-p:0.1326645463705063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0349, 5.0359, 5.0350],
        [5.0349, 5.0968, 5.0568],
        [5.0349, 5.0349, 5.0349],
        [5.0349, 5.0349, 5.0349]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:117, step:0 
model_pd.l_p.mean(): 0.12902937829494476 
model_pd.l_d.mean(): -20.59890365600586 
model_pd.lagr.mean(): -20.469873428344727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3750], device='cuda:0')), ('power', tensor([-21.3725], device='cuda:0'))])
epoch£º117	 i:0 	 global-step:2340	 l-p:0.12902937829494476
epoch£º117	 i:1 	 global-step:2341	 l-p:0.1332962065935135
epoch£º117	 i:2 	 global-step:2342	 l-p:0.13380646705627441
epoch£º117	 i:3 	 global-step:2343	 l-p:0.1141878291964531
epoch£º117	 i:4 	 global-step:2344	 l-p:0.14192207157611847
epoch£º117	 i:5 	 global-step:2345	 l-p:0.124033123254776
epoch£º117	 i:6 	 global-step:2346	 l-p:0.25640174746513367
epoch£º117	 i:7 	 global-step:2347	 l-p:0.13965360820293427
epoch£º117	 i:8 	 global-step:2348	 l-p:-0.9669086933135986
epoch£º117	 i:9 	 global-step:2349	 l-p:0.1537133902311325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7193, 5.8697, 6.6082],
        [4.7193, 5.0131, 4.9883],
        [4.7193, 4.7383, 4.7229],
        [4.7193, 4.7193, 4.7193]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:118, step:0 
model_pd.l_p.mean(): 0.1217663362622261 
model_pd.l_d.mean(): -19.584644317626953 
model_pd.lagr.mean(): -19.46287727355957 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5592], device='cuda:0')), ('power', tensor([-20.5301], device='cuda:0'))])
epoch£º118	 i:0 	 global-step:2360	 l-p:0.1217663362622261
epoch£º118	 i:1 	 global-step:2361	 l-p:0.14084994792938232
epoch£º118	 i:2 	 global-step:2362	 l-p:0.14085721969604492
epoch£º118	 i:3 	 global-step:2363	 l-p:0.19289083778858185
epoch£º118	 i:4 	 global-step:2364	 l-p:0.14375455677509308
epoch£º118	 i:5 	 global-step:2365	 l-p:0.150954470038414
epoch£º118	 i:6 	 global-step:2366	 l-p:0.150457963347435
epoch£º118	 i:7 	 global-step:2367	 l-p:0.6112078428268433
epoch£º118	 i:8 	 global-step:2368	 l-p:0.31662845611572266
epoch£º118	 i:9 	 global-step:2369	 l-p:0.17345231771469116
====================================================================================================
====================================================================================================
====================================================================================================

epoch:119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9001, 5.0930, 5.0365],
        [4.9001, 4.9079, 4.9010],
        [4.9001, 5.4349, 5.5395],
        [4.9001, 5.5705, 5.7839]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:119, step:0 
model_pd.l_p.mean(): 0.16443969309329987 
model_pd.l_d.mean(): -20.5570011138916 
model_pd.lagr.mean(): -20.392560958862305 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4229], device='cuda:0')), ('power', tensor([-21.3794], device='cuda:0'))])
epoch£º119	 i:0 	 global-step:2380	 l-p:0.16443969309329987
epoch£º119	 i:1 	 global-step:2381	 l-p:0.13239999115467072
epoch£º119	 i:2 	 global-step:2382	 l-p:0.12263715267181396
epoch£º119	 i:3 	 global-step:2383	 l-p:0.11799009144306183
epoch£º119	 i:4 	 global-step:2384	 l-p:0.11435823142528534
epoch£º119	 i:5 	 global-step:2385	 l-p:0.11652535200119019
epoch£º119	 i:6 	 global-step:2386	 l-p:0.12513503432273865
epoch£º119	 i:7 	 global-step:2387	 l-p:0.14097543060779572
epoch£º119	 i:8 	 global-step:2388	 l-p:-0.09323421120643616
epoch£º119	 i:9 	 global-step:2389	 l-p:0.14657677710056305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9100, 4.9101, 4.9100],
        [4.9100, 4.9102, 4.9100],
        [4.9100, 4.9109, 4.9101],
        [4.9100, 5.3161, 5.3362]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:120, step:0 
model_pd.l_p.mean(): 0.1313047707080841 
model_pd.l_d.mean(): -20.145450592041016 
model_pd.lagr.mean(): -20.014144897460938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4632], device='cuda:0')), ('power', tensor([-21.0019], device='cuda:0'))])
epoch£º120	 i:0 	 global-step:2400	 l-p:0.1313047707080841
epoch£º120	 i:1 	 global-step:2401	 l-p:0.12622886896133423
epoch£º120	 i:2 	 global-step:2402	 l-p:0.12779481709003448
epoch£º120	 i:3 	 global-step:2403	 l-p:0.15211592614650726
epoch£º120	 i:4 	 global-step:2404	 l-p:0.14730091392993927
epoch£º120	 i:5 	 global-step:2405	 l-p:0.12693561613559723
epoch£º120	 i:6 	 global-step:2406	 l-p:0.1455266922712326
epoch£º120	 i:7 	 global-step:2407	 l-p:0.0877978578209877
epoch£º120	 i:8 	 global-step:2408	 l-p:0.07648926228284836
epoch£º120	 i:9 	 global-step:2409	 l-p:0.030426830053329468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7523, 4.9324, 4.8783],
        [4.7523, 4.7780, 4.7581],
        [4.7523, 4.7523, 4.7523],
        [4.7523, 4.7528, 4.7523]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:121, step:0 
model_pd.l_p.mean(): 0.14620289206504822 
model_pd.l_d.mean(): -19.357053756713867 
model_pd.lagr.mean(): -19.210851669311523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4942], device='cuda:0')), ('power', tensor([-20.2308], device='cuda:0'))])
epoch£º121	 i:0 	 global-step:2420	 l-p:0.14620289206504822
epoch£º121	 i:1 	 global-step:2421	 l-p:0.10382145643234253
epoch£º121	 i:2 	 global-step:2422	 l-p:0.13255077600479126
epoch£º121	 i:3 	 global-step:2423	 l-p:0.13337716460227966
epoch£º121	 i:4 	 global-step:2424	 l-p:0.18889747560024261
epoch£º121	 i:5 	 global-step:2425	 l-p:0.1266007423400879
epoch£º121	 i:6 	 global-step:2426	 l-p:0.13651782274246216
epoch£º121	 i:7 	 global-step:2427	 l-p:0.1584194302558899
epoch£º121	 i:8 	 global-step:2428	 l-p:0.12420739978551865
epoch£º121	 i:9 	 global-step:2429	 l-p:0.1317308396100998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9523, 4.9788, 4.9581],
        [4.9523, 4.9619, 4.9535],
        [4.9523, 4.9524, 4.9523],
        [4.9523, 4.9591, 4.9530]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:122, step:0 
model_pd.l_p.mean(): 0.132553368806839 
model_pd.l_d.mean(): -18.468402862548828 
model_pd.lagr.mean(): -18.33584976196289 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5176], device='cuda:0')), ('power', tensor([-19.3499], device='cuda:0'))])
epoch£º122	 i:0 	 global-step:2440	 l-p:0.132553368806839
epoch£º122	 i:1 	 global-step:2441	 l-p:0.11412816494703293
epoch£º122	 i:2 	 global-step:2442	 l-p:0.14973527193069458
epoch£º122	 i:3 	 global-step:2443	 l-p:0.13589051365852356
epoch£º122	 i:4 	 global-step:2444	 l-p:0.1346777081489563
epoch£º122	 i:5 	 global-step:2445	 l-p:0.112052321434021
epoch£º122	 i:6 	 global-step:2446	 l-p:0.1457817554473877
epoch£º122	 i:7 	 global-step:2447	 l-p:0.1320997029542923
epoch£º122	 i:8 	 global-step:2448	 l-p:0.14210428297519684
epoch£º122	 i:9 	 global-step:2449	 l-p:0.1363079845905304
====================================================================================================
====================================================================================================
====================================================================================================

epoch:123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7553, 4.7558, 4.7554],
        [4.7553, 4.8058, 4.7722],
        [4.7553, 4.7609, 4.7559],
        [4.7553, 4.7553, 4.7553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:123, step:0 
model_pd.l_p.mean(): 0.1418210119009018 
model_pd.l_d.mean(): -20.368589401245117 
model_pd.lagr.mean(): -20.226768493652344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4872], device='cuda:0')), ('power', tensor([-21.2541], device='cuda:0'))])
epoch£º123	 i:0 	 global-step:2460	 l-p:0.1418210119009018
epoch£º123	 i:1 	 global-step:2461	 l-p:0.13768316805362701
epoch£º123	 i:2 	 global-step:2462	 l-p:0.21630661189556122
epoch£º123	 i:3 	 global-step:2463	 l-p:0.14473174512386322
epoch£º123	 i:4 	 global-step:2464	 l-p:0.07654698193073273
epoch£º123	 i:5 	 global-step:2465	 l-p:0.14734801650047302
epoch£º123	 i:6 	 global-step:2466	 l-p:0.21830669045448303
epoch£º123	 i:7 	 global-step:2467	 l-p:0.14760762453079224
epoch£º123	 i:8 	 global-step:2468	 l-p:-0.5418103933334351
epoch£º123	 i:9 	 global-step:2469	 l-p:0.08989545702934265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6816, 4.6819, 4.6816],
        [4.6816, 5.3729, 5.6324],
        [4.6816, 4.7131, 4.6897],
        [4.6816, 4.7285, 4.6968]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:124, step:0 
model_pd.l_p.mean(): 0.21345847845077515 
model_pd.l_d.mean(): -19.983291625976562 
model_pd.lagr.mean(): -19.769832611083984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5286], device='cuda:0')), ('power', tensor([-20.9044], device='cuda:0'))])
epoch£º124	 i:0 	 global-step:2480	 l-p:0.21345847845077515
epoch£º124	 i:1 	 global-step:2481	 l-p:-2.230465888977051
epoch£º124	 i:2 	 global-step:2482	 l-p:0.1731031984090805
epoch£º124	 i:3 	 global-step:2483	 l-p:0.13000184297561646
epoch£º124	 i:4 	 global-step:2484	 l-p:0.1379079967737198
epoch£º124	 i:5 	 global-step:2485	 l-p:0.10202699154615402
epoch£º124	 i:6 	 global-step:2486	 l-p:0.1159573346376419
epoch£º124	 i:7 	 global-step:2487	 l-p:0.10599308460950851
epoch£º124	 i:8 	 global-step:2488	 l-p:0.11509332060813904
epoch£º124	 i:9 	 global-step:2489	 l-p:0.1108485758304596
====================================================================================================
====================================================================================================
====================================================================================================

epoch:125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3182, 5.3334, 5.3205],
        [5.3182, 6.7445, 7.7061],
        [5.3182, 5.3374, 5.3215],
        [5.3182, 5.7763, 5.8026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:125, step:0 
model_pd.l_p.mean(): 0.10472134500741959 
model_pd.l_d.mean(): -18.867963790893555 
model_pd.lagr.mean(): -18.763242721557617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3911], device='cuda:0')), ('power', tensor([-19.6258], device='cuda:0'))])
epoch£º125	 i:0 	 global-step:2500	 l-p:0.10472134500741959
epoch£º125	 i:1 	 global-step:2501	 l-p:0.11111398786306381
epoch£º125	 i:2 	 global-step:2502	 l-p:0.11137203127145767
epoch£º125	 i:3 	 global-step:2503	 l-p:0.12739087641239166
epoch£º125	 i:4 	 global-step:2504	 l-p:0.11360421776771545
epoch£º125	 i:5 	 global-step:2505	 l-p:0.12803952395915985
epoch£º125	 i:6 	 global-step:2506	 l-p:0.123372882604599
epoch£º125	 i:7 	 global-step:2507	 l-p:0.18978911638259888
epoch£º125	 i:8 	 global-step:2508	 l-p:0.14300251007080078
epoch£º125	 i:9 	 global-step:2509	 l-p:0.157185360789299
====================================================================================================
====================================================================================================
====================================================================================================

epoch:126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6962, 5.3588, 5.5910],
        [4.6962, 5.1141, 5.1571],
        [4.6962, 4.6962, 4.6962],
        [4.6962, 4.7012, 4.6966]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:126, step:0 
model_pd.l_p.mean(): 0.14176620543003082 
model_pd.l_d.mean(): -20.530872344970703 
model_pd.lagr.mean(): -20.38910675048828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4757], device='cuda:0')), ('power', tensor([-21.4075], device='cuda:0'))])
epoch£º126	 i:0 	 global-step:2520	 l-p:0.14176620543003082
epoch£º126	 i:1 	 global-step:2521	 l-p:0.16378726065158844
epoch£º126	 i:2 	 global-step:2522	 l-p:0.16905654966831207
epoch£º126	 i:3 	 global-step:2523	 l-p:0.17665980756282806
epoch£º126	 i:4 	 global-step:2524	 l-p:0.14755211770534515
epoch£º126	 i:5 	 global-step:2525	 l-p:0.15634238719940186
epoch£º126	 i:6 	 global-step:2526	 l-p:0.11422935128211975
epoch£º126	 i:7 	 global-step:2527	 l-p:0.17043307423591614
epoch£º126	 i:8 	 global-step:2528	 l-p:-0.06514208018779755
epoch£º126	 i:9 	 global-step:2529	 l-p:-0.518539309501648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:127
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2616,  0.1673,  1.0000,  0.1070,
          1.0000,  0.6396, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6901,  0.6098,  1.0000,  0.5389,
          1.0000,  0.8837, 31.6228]], device='cuda:0')
 pt:tensor([[4.6959, 4.8052, 4.7543],
        [4.6959, 4.7707, 4.7279],
        [4.6959, 4.8807, 4.8291],
        [4.6959, 5.4680, 5.8033]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:127, step:0 
model_pd.l_p.mean(): 0.09281963855028152 
model_pd.l_d.mean(): -19.574377059936523 
model_pd.lagr.mean(): -19.481557846069336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5353], device='cuda:0')), ('power', tensor([-20.4949], device='cuda:0'))])
epoch£º127	 i:0 	 global-step:2540	 l-p:0.09281963855028152
epoch£º127	 i:1 	 global-step:2541	 l-p:0.21945112943649292
epoch£º127	 i:2 	 global-step:2542	 l-p:0.13661983609199524
epoch£º127	 i:3 	 global-step:2543	 l-p:0.1398262083530426
epoch£º127	 i:4 	 global-step:2544	 l-p:0.11282379925251007
epoch£º127	 i:5 	 global-step:2545	 l-p:0.14295212924480438
epoch£º127	 i:6 	 global-step:2546	 l-p:0.11857911944389343
epoch£º127	 i:7 	 global-step:2547	 l-p:0.11662496626377106
epoch£º127	 i:8 	 global-step:2548	 l-p:0.11878546327352524
epoch£º127	 i:9 	 global-step:2549	 l-p:0.11205864697694778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2023, 5.6782, 5.7234],
        [5.2023, 5.3898, 5.3274],
        [5.2023, 5.2052, 5.2025],
        [5.2023, 5.2030, 5.2023]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:128, step:0 
model_pd.l_p.mean(): 0.09645656496286392 
model_pd.l_d.mean(): -19.50374984741211 
model_pd.lagr.mean(): -19.40729331970215 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4358], device='cuda:0')), ('power', tensor([-20.3198], device='cuda:0'))])
epoch£º128	 i:0 	 global-step:2560	 l-p:0.09645656496286392
epoch£º128	 i:1 	 global-step:2561	 l-p:0.12358743697404861
epoch£º128	 i:2 	 global-step:2562	 l-p:0.12217824906110764
epoch£º128	 i:3 	 global-step:2563	 l-p:0.20868268609046936
epoch£º128	 i:4 	 global-step:2564	 l-p:0.14288534224033356
epoch£º128	 i:5 	 global-step:2565	 l-p:0.1457899659872055
epoch£º128	 i:6 	 global-step:2566	 l-p:0.1657787710428238
epoch£º128	 i:7 	 global-step:2567	 l-p:0.12751717865467072
epoch£º128	 i:8 	 global-step:2568	 l-p:0.14621226489543915
epoch£º128	 i:9 	 global-step:2569	 l-p:0.042532894760370255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7409, 4.7416, 4.7409],
        [4.7409, 4.7573, 4.7438],
        [4.7409, 4.7409, 4.7409],
        [4.7409, 4.8654, 4.8120]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:129, step:0 
model_pd.l_p.mean(): 0.12254156172275543 
model_pd.l_d.mean(): -19.828149795532227 
model_pd.lagr.mean(): -19.705608367919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5523], device='cuda:0')), ('power', tensor([-20.7709], device='cuda:0'))])
epoch£º129	 i:0 	 global-step:2580	 l-p:0.12254156172275543
epoch£º129	 i:1 	 global-step:2581	 l-p:0.3378615975379944
epoch£º129	 i:2 	 global-step:2582	 l-p:0.1474534571170807
epoch£º129	 i:3 	 global-step:2583	 l-p:0.157706156373024
epoch£º129	 i:4 	 global-step:2584	 l-p:0.13060952723026276
epoch£º129	 i:5 	 global-step:2585	 l-p:0.12578067183494568
epoch£º129	 i:6 	 global-step:2586	 l-p:0.152597576379776
epoch£º129	 i:7 	 global-step:2587	 l-p:0.14920105040073395
epoch£º129	 i:8 	 global-step:2588	 l-p:0.14381183683872223
epoch£º129	 i:9 	 global-step:2589	 l-p:0.12665052711963654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9159, 5.0551, 4.9982],
        [4.9159, 4.9159, 4.9159],
        [4.9159, 4.9159, 4.9159],
        [4.9159, 4.9169, 4.9159]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:130, step:0 
model_pd.l_p.mean(): 0.025403425097465515 
model_pd.l_d.mean(): -18.60327911376953 
model_pd.lagr.mean(): -18.5778751373291 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5293], device='cuda:0')), ('power', tensor([-19.4993], device='cuda:0'))])
epoch£º130	 i:0 	 global-step:2600	 l-p:0.025403425097465515
epoch£º130	 i:1 	 global-step:2601	 l-p:0.15718422830104828
epoch£º130	 i:2 	 global-step:2602	 l-p:0.1308944672346115
epoch£º130	 i:3 	 global-step:2603	 l-p:0.13636432588100433
epoch£º130	 i:4 	 global-step:2604	 l-p:0.12690134346485138
epoch£º130	 i:5 	 global-step:2605	 l-p:0.1508995145559311
epoch£º130	 i:6 	 global-step:2606	 l-p:0.12294511497020721
epoch£º130	 i:7 	 global-step:2607	 l-p:0.20119567215442657
epoch£º130	 i:8 	 global-step:2608	 l-p:0.12892630696296692
epoch£º130	 i:9 	 global-step:2609	 l-p:0.13687576353549957
====================================================================================================
====================================================================================================
====================================================================================================

epoch:131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9100, 4.9100, 4.9100],
        [4.9100, 4.9848, 4.9408],
        [4.9100, 4.9101, 4.9100],
        [4.9100, 4.9100, 4.9100]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:131, step:0 
model_pd.l_p.mean(): 0.14009280502796173 
model_pd.l_d.mean(): -19.560813903808594 
model_pd.lagr.mean(): -19.42072105407715 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4999], device='cuda:0')), ('power', tensor([-20.4443], device='cuda:0'))])
epoch£º131	 i:0 	 global-step:2620	 l-p:0.14009280502796173
epoch£º131	 i:1 	 global-step:2621	 l-p:0.16242514550685883
epoch£º131	 i:2 	 global-step:2622	 l-p:0.1522497683763504
epoch£º131	 i:3 	 global-step:2623	 l-p:0.08232852816581726
epoch£º131	 i:4 	 global-step:2624	 l-p:0.12569096684455872
epoch£º131	 i:5 	 global-step:2625	 l-p:0.14866863191127777
epoch£º131	 i:6 	 global-step:2626	 l-p:0.13244308531284332
epoch£º131	 i:7 	 global-step:2627	 l-p:0.15719294548034668
epoch£º131	 i:8 	 global-step:2628	 l-p:0.12485688924789429
epoch£º131	 i:9 	 global-step:2629	 l-p:0.1539870649576187
====================================================================================================
====================================================================================================
====================================================================================================

epoch:132
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9374, 4.9374, 4.9374],
        [4.9374, 6.0227, 6.6542],
        [4.9374, 4.9374, 4.9374],
        [4.9374, 5.3614, 5.3938]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:132, step:0 
model_pd.l_p.mean(): 0.11696416884660721 
model_pd.l_d.mean(): -18.96693229675293 
model_pd.lagr.mean(): -18.84996795654297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5279], device='cuda:0')), ('power', tensor([-19.8684], device='cuda:0'))])
epoch£º132	 i:0 	 global-step:2640	 l-p:0.11696416884660721
epoch£º132	 i:1 	 global-step:2641	 l-p:0.11800504475831985
epoch£º132	 i:2 	 global-step:2642	 l-p:0.12227369099855423
epoch£º132	 i:3 	 global-step:2643	 l-p:0.12849721312522888
epoch£º132	 i:4 	 global-step:2644	 l-p:0.14034733176231384
epoch£º132	 i:5 	 global-step:2645	 l-p:0.13031227886676788
epoch£º132	 i:6 	 global-step:2646	 l-p:0.13287170231342316
epoch£º132	 i:7 	 global-step:2647	 l-p:-0.19370651245117188
epoch£º132	 i:8 	 global-step:2648	 l-p:0.12544631958007812
epoch£º132	 i:9 	 global-step:2649	 l-p:0.16980786621570587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9641, 4.9641, 4.9641],
        [4.9641, 4.9641, 4.9641],
        [4.9641, 5.2692, 5.2417],
        [4.9641, 5.0749, 5.0212]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:133, step:0 
model_pd.l_p.mean(): 0.14210747182369232 
model_pd.l_d.mean(): -18.43720817565918 
model_pd.lagr.mean(): -18.295101165771484 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5124], device='cuda:0')), ('power', tensor([-19.3127], device='cuda:0'))])
epoch£º133	 i:0 	 global-step:2660	 l-p:0.14210747182369232
epoch£º133	 i:1 	 global-step:2661	 l-p:0.12322457879781723
epoch£º133	 i:2 	 global-step:2662	 l-p:0.13525532186031342
epoch£º133	 i:3 	 global-step:2663	 l-p:0.13401657342910767
epoch£º133	 i:4 	 global-step:2664	 l-p:0.13327597081661224
epoch£º133	 i:5 	 global-step:2665	 l-p:0.12331654876470566
epoch£º133	 i:6 	 global-step:2666	 l-p:0.15417413413524628
epoch£º133	 i:7 	 global-step:2667	 l-p:0.10319726914167404
epoch£º133	 i:8 	 global-step:2668	 l-p:0.15837299823760986
epoch£º133	 i:9 	 global-step:2669	 l-p:0.13632743060588837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9537, 5.2429, 5.2102],
        [4.9537, 5.0321, 4.9868],
        [4.9537, 4.9692, 4.9563],
        [4.9537, 4.9562, 4.9539]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:134, step:0 
model_pd.l_p.mean(): 0.13941454887390137 
model_pd.l_d.mean(): -20.17563819885254 
model_pd.lagr.mean(): -20.036224365234375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4421], device='cuda:0')), ('power', tensor([-21.0109], device='cuda:0'))])
epoch£º134	 i:0 	 global-step:2680	 l-p:0.13941454887390137
epoch£º134	 i:1 	 global-step:2681	 l-p:0.12912625074386597
epoch£º134	 i:2 	 global-step:2682	 l-p:0.13009439408779144
epoch£º134	 i:3 	 global-step:2683	 l-p:0.20381022989749908
epoch£º134	 i:4 	 global-step:2684	 l-p:0.1281621903181076
epoch£º134	 i:5 	 global-step:2685	 l-p:0.11584383994340897
epoch£º134	 i:6 	 global-step:2686	 l-p:0.13130110502243042
epoch£º134	 i:7 	 global-step:2687	 l-p:0.11744377762079239
epoch£º134	 i:8 	 global-step:2688	 l-p:0.1153799444437027
epoch£º134	 i:9 	 global-step:2689	 l-p:0.15074549615383148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9433, 4.9433, 4.9433],
        [4.9433, 6.0500, 6.7067],
        [4.9433, 5.0671, 5.0118],
        [4.9433, 5.8600, 6.3110]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:135, step:0 
model_pd.l_p.mean(): 0.12583228945732117 
model_pd.l_d.mean(): -18.602764129638672 
model_pd.lagr.mean(): -18.476932525634766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5507], device='cuda:0')), ('power', tensor([-19.5210], device='cuda:0'))])
epoch£º135	 i:0 	 global-step:2700	 l-p:0.12583228945732117
epoch£º135	 i:1 	 global-step:2701	 l-p:0.10123804211616516
epoch£º135	 i:2 	 global-step:2702	 l-p:0.1252232939004898
epoch£º135	 i:3 	 global-step:2703	 l-p:0.17937763035297394
epoch£º135	 i:4 	 global-step:2704	 l-p:0.1287069320678711
epoch£º135	 i:5 	 global-step:2705	 l-p:0.1548694521188736
epoch£º135	 i:6 	 global-step:2706	 l-p:0.16504637897014618
epoch£º135	 i:7 	 global-step:2707	 l-p:0.11500663310289383
epoch£º135	 i:8 	 global-step:2708	 l-p:0.12809529900550842
epoch£º135	 i:9 	 global-step:2709	 l-p:0.1397276073694229
====================================================================================================
====================================================================================================
====================================================================================================

epoch:136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9491, 4.9492, 4.9491],
        [4.9491, 5.0921, 5.0352],
        [4.9491, 4.9498, 4.9491],
        [4.9491, 5.1213, 5.0643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:136, step:0 
model_pd.l_p.mean(): 0.1398649513721466 
model_pd.l_d.mean(): -20.76788330078125 
model_pd.lagr.mean(): -20.62801742553711 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3830], device='cuda:0')), ('power', tensor([-21.5529], device='cuda:0'))])
epoch£º136	 i:0 	 global-step:2720	 l-p:0.1398649513721466
epoch£º136	 i:1 	 global-step:2721	 l-p:0.1396140307188034
epoch£º136	 i:2 	 global-step:2722	 l-p:0.15271401405334473
epoch£º136	 i:3 	 global-step:2723	 l-p:0.11970315873622894
epoch£º136	 i:4 	 global-step:2724	 l-p:0.14636826515197754
epoch£º136	 i:5 	 global-step:2725	 l-p:0.11529715359210968
epoch£º136	 i:6 	 global-step:2726	 l-p:0.1739608347415924
epoch£º136	 i:7 	 global-step:2727	 l-p:0.1339719593524933
epoch£º136	 i:8 	 global-step:2728	 l-p:0.17495062947273254
epoch£º136	 i:9 	 global-step:2729	 l-p:0.11609136313199997
====================================================================================================
====================================================================================================
====================================================================================================

epoch:137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9656, 4.9656, 4.9656],
        [4.9656, 4.9820, 4.9684],
        [4.9656, 4.9656, 4.9656],
        [4.9656, 5.9701, 6.5117]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:137, step:0 
model_pd.l_p.mean(): -0.2822629511356354 
model_pd.l_d.mean(): -20.475366592407227 
model_pd.lagr.mean(): -20.75762939453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4164], device='cuda:0')), ('power', tensor([-21.2895], device='cuda:0'))])
epoch£º137	 i:0 	 global-step:2740	 l-p:-0.2822629511356354
epoch£º137	 i:1 	 global-step:2741	 l-p:0.13479922711849213
epoch£º137	 i:2 	 global-step:2742	 l-p:0.11641594022512436
epoch£º137	 i:3 	 global-step:2743	 l-p:0.12558209896087646
epoch£º137	 i:4 	 global-step:2744	 l-p:0.11685485392808914
epoch£º137	 i:5 	 global-step:2745	 l-p:0.1309947520494461
epoch£º137	 i:6 	 global-step:2746	 l-p:0.11402091383934021
epoch£º137	 i:7 	 global-step:2747	 l-p:0.10933364927768707
epoch£º137	 i:8 	 global-step:2748	 l-p:0.141676664352417
epoch£º137	 i:9 	 global-step:2749	 l-p:0.1459171324968338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9466, 5.0165, 4.9744],
        [4.9466, 4.9466, 4.9466],
        [4.9466, 4.9466, 4.9466],
        [4.9466, 4.9466, 4.9466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:138, step:0 
model_pd.l_p.mean(): 0.13191938400268555 
model_pd.l_d.mean(): -20.481063842773438 
model_pd.lagr.mean(): -20.349143981933594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4150], device='cuda:0')), ('power', tensor([-21.2938], device='cuda:0'))])
epoch£º138	 i:0 	 global-step:2760	 l-p:0.13191938400268555
epoch£º138	 i:1 	 global-step:2761	 l-p:0.12639768421649933
epoch£º138	 i:2 	 global-step:2762	 l-p:0.1390611082315445
epoch£º138	 i:3 	 global-step:2763	 l-p:0.24270722270011902
epoch£º138	 i:4 	 global-step:2764	 l-p:0.1500956416130066
epoch£º138	 i:5 	 global-step:2765	 l-p:0.1330632120370865
epoch£º138	 i:6 	 global-step:2766	 l-p:0.04191575571894646
epoch£º138	 i:7 	 global-step:2767	 l-p:-0.21970336139202118
epoch£º138	 i:8 	 global-step:2768	 l-p:0.1351184993982315
epoch£º138	 i:9 	 global-step:2769	 l-p:0.19326581060886383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9249, 5.9112, 6.4406],
        [4.9249, 6.1012, 6.8434],
        [4.9249, 4.9249, 4.9249],
        [4.9249, 4.9270, 4.9250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:139, step:0 
model_pd.l_p.mean(): 0.06017030403017998 
model_pd.l_d.mean(): -18.568058013916016 
model_pd.lagr.mean(): -18.50788688659668 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5337], device='cuda:0')), ('power', tensor([-19.4681], device='cuda:0'))])
epoch£º139	 i:0 	 global-step:2780	 l-p:0.06017030403017998
epoch£º139	 i:1 	 global-step:2781	 l-p:0.14582721889019012
epoch£º139	 i:2 	 global-step:2782	 l-p:0.13030306994915009
epoch£º139	 i:3 	 global-step:2783	 l-p:0.1330818086862564
epoch£º139	 i:4 	 global-step:2784	 l-p:0.12701593339443207
epoch£º139	 i:5 	 global-step:2785	 l-p:0.11529548466205597
epoch£º139	 i:6 	 global-step:2786	 l-p:0.13211382925510406
epoch£º139	 i:7 	 global-step:2787	 l-p:0.11907299607992172
epoch£º139	 i:8 	 global-step:2788	 l-p:0.16249485313892365
epoch£º139	 i:9 	 global-step:2789	 l-p:0.15729741752147675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8889, 5.9202, 6.5045],
        [4.8889, 4.9225, 4.8978],
        [4.8889, 5.4863, 5.6495],
        [4.8889, 5.9396, 6.5453]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:140, step:0 
model_pd.l_p.mean(): 0.14048534631729126 
model_pd.l_d.mean(): -19.9287052154541 
model_pd.lagr.mean(): -19.788219451904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4915], device='cuda:0')), ('power', tensor([-20.8104], device='cuda:0'))])
epoch£º140	 i:0 	 global-step:2800	 l-p:0.14048534631729126
epoch£º140	 i:1 	 global-step:2801	 l-p:0.13618019223213196
epoch£º140	 i:2 	 global-step:2802	 l-p:0.13428069651126862
epoch£º140	 i:3 	 global-step:2803	 l-p:0.17512498795986176
epoch£º140	 i:4 	 global-step:2804	 l-p:0.11239773780107498
epoch£º140	 i:5 	 global-step:2805	 l-p:0.13122853636741638
epoch£º140	 i:6 	 global-step:2806	 l-p:0.13852477073669434
epoch£º140	 i:7 	 global-step:2807	 l-p:0.1386917382478714
epoch£º140	 i:8 	 global-step:2808	 l-p:0.12668687105178833
epoch£º140	 i:9 	 global-step:2809	 l-p:0.1297716647386551
====================================================================================================
====================================================================================================
====================================================================================================

epoch:141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0322, 5.0323, 5.0322],
        [5.0322, 5.0323, 5.0322],
        [5.0322, 5.0322, 5.0322],
        [5.0322, 5.1609, 5.1045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:141, step:0 
model_pd.l_p.mean(): 0.13614647090435028 
model_pd.l_d.mean(): -19.98208999633789 
model_pd.lagr.mean(): -19.845943450927734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4394], device='cuda:0')), ('power', tensor([-20.8108], device='cuda:0'))])
epoch£º141	 i:0 	 global-step:2820	 l-p:0.13614647090435028
epoch£º141	 i:1 	 global-step:2821	 l-p:0.11856979876756668
epoch£º141	 i:2 	 global-step:2822	 l-p:0.332387775182724
epoch£º141	 i:3 	 global-step:2823	 l-p:0.12777365744113922
epoch£º141	 i:4 	 global-step:2824	 l-p:0.1352657973766327
epoch£º141	 i:5 	 global-step:2825	 l-p:0.1289556622505188
epoch£º141	 i:6 	 global-step:2826	 l-p:0.12923280894756317
epoch£º141	 i:7 	 global-step:2827	 l-p:0.1251245141029358
epoch£º141	 i:8 	 global-step:2828	 l-p:0.006511430721729994
epoch£º141	 i:9 	 global-step:2829	 l-p:0.1871824562549591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7221, 4.8146, 4.7675],
        [4.7221, 4.8806, 4.8282],
        [4.7221, 4.8640, 4.8113],
        [4.7221, 4.7225, 4.7221]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:142, step:0 
model_pd.l_p.mean(): 0.2943905293941498 
model_pd.l_d.mean(): -20.10194206237793 
model_pd.lagr.mean(): -19.807552337646484 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5206], device='cuda:0')), ('power', tensor([-21.0170], device='cuda:0'))])
epoch£º142	 i:0 	 global-step:2840	 l-p:0.2943905293941498
epoch£º142	 i:1 	 global-step:2841	 l-p:0.1602589339017868
epoch£º142	 i:2 	 global-step:2842	 l-p:0.14169944822788239
epoch£º142	 i:3 	 global-step:2843	 l-p:0.6408265829086304
epoch£º142	 i:4 	 global-step:2844	 l-p:0.14986643195152283
epoch£º142	 i:5 	 global-step:2845	 l-p:0.15084820985794067
epoch£º142	 i:6 	 global-step:2846	 l-p:0.1126394048333168
epoch£º142	 i:7 	 global-step:2847	 l-p:0.11839564144611359
epoch£º142	 i:8 	 global-step:2848	 l-p:0.12860743701457977
epoch£º142	 i:9 	 global-step:2849	 l-p:0.11324945092201233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1462, 5.1462, 5.1462],
        [5.1462, 5.1462, 5.1462],
        [5.1462, 5.1463, 5.1462],
        [5.1462, 5.9120, 6.1954]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:143, step:0 
model_pd.l_p.mean(): 0.11867479234933853 
model_pd.l_d.mean(): -18.80428123474121 
model_pd.lagr.mean(): -18.685606002807617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4661], device='cuda:0')), ('power', tensor([-19.6387], device='cuda:0'))])
epoch£º143	 i:0 	 global-step:2860	 l-p:0.11867479234933853
epoch£º143	 i:1 	 global-step:2861	 l-p:0.11474044620990753
epoch£º143	 i:2 	 global-step:2862	 l-p:0.11817197501659393
epoch£º143	 i:3 	 global-step:2863	 l-p:0.13474221527576447
epoch£º143	 i:4 	 global-step:2864	 l-p:0.1321941763162613
epoch£º143	 i:5 	 global-step:2865	 l-p:0.1265331655740738
epoch£º143	 i:6 	 global-step:2866	 l-p:0.1657930463552475
epoch£º143	 i:7 	 global-step:2867	 l-p:0.14155301451683044
epoch£º143	 i:8 	 global-step:2868	 l-p:0.1003701388835907
epoch£º143	 i:9 	 global-step:2869	 l-p:0.16305293142795563
====================================================================================================
====================================================================================================
====================================================================================================

epoch:144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7797, 5.9506, 6.7155],
        [4.7797, 5.3297, 5.4672],
        [4.7797, 4.8703, 4.8233],
        [4.7797, 4.7850, 4.7803]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:144, step:0 
model_pd.l_p.mean(): 0.1347251832485199 
model_pd.l_d.mean(): -19.389169692993164 
model_pd.lagr.mean(): -19.254444122314453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5130], device='cuda:0')), ('power', tensor([-20.2830], device='cuda:0'))])
epoch£º144	 i:0 	 global-step:2880	 l-p:0.1347251832485199
epoch£º144	 i:1 	 global-step:2881	 l-p:0.1345258355140686
epoch£º144	 i:2 	 global-step:2882	 l-p:0.21110816299915314
epoch£º144	 i:3 	 global-step:2883	 l-p:-0.009290837682783604
epoch£º144	 i:4 	 global-step:2884	 l-p:0.24556583166122437
epoch£º144	 i:5 	 global-step:2885	 l-p:0.15093186497688293
epoch£º144	 i:6 	 global-step:2886	 l-p:0.14586268365383148
epoch£º144	 i:7 	 global-step:2887	 l-p:0.09365902841091156
epoch£º144	 i:8 	 global-step:2888	 l-p:0.1604243367910385
epoch£º144	 i:9 	 global-step:2889	 l-p:0.12439515441656113
====================================================================================================
====================================================================================================
====================================================================================================

epoch:145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9828, 5.6828, 5.9261],
        [4.9828, 5.7767, 6.1064],
        [4.9828, 4.9913, 4.9838],
        [4.9828, 6.0489, 6.6593]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:145, step:0 
model_pd.l_p.mean(): 0.12556743621826172 
model_pd.l_d.mean(): -20.323780059814453 
model_pd.lagr.mean(): -20.198211669921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4303], device='cuda:0')), ('power', tensor([-21.1495], device='cuda:0'))])
epoch£º145	 i:0 	 global-step:2900	 l-p:0.12556743621826172
epoch£º145	 i:1 	 global-step:2901	 l-p:-2.07588791847229
epoch£º145	 i:2 	 global-step:2902	 l-p:0.12425989657640457
epoch£º145	 i:3 	 global-step:2903	 l-p:0.12902961671352386
epoch£º145	 i:4 	 global-step:2904	 l-p:0.13880974054336548
epoch£º145	 i:5 	 global-step:2905	 l-p:0.11724277585744858
epoch£º145	 i:6 	 global-step:2906	 l-p:0.14128881692886353
epoch£º145	 i:7 	 global-step:2907	 l-p:0.10773232579231262
epoch£º145	 i:8 	 global-step:2908	 l-p:0.14711925387382507
epoch£º145	 i:9 	 global-step:2909	 l-p:0.1448393166065216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8819, 4.8819, 4.8819],
        [4.8819, 4.9128, 4.8897],
        [4.8819, 5.1297, 5.0891],
        [4.8819, 4.9402, 4.9032]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:146, step:0 
model_pd.l_p.mean(): 0.15658658742904663 
model_pd.l_d.mean(): -19.482593536376953 
model_pd.lagr.mean(): -19.326007843017578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4993], device='cuda:0')), ('power', tensor([-20.3641], device='cuda:0'))])
epoch£º146	 i:0 	 global-step:2920	 l-p:0.15658658742904663
epoch£º146	 i:1 	 global-step:2921	 l-p:0.170388862490654
epoch£º146	 i:2 	 global-step:2922	 l-p:0.13860610127449036
epoch£º146	 i:3 	 global-step:2923	 l-p:0.1348460465669632
epoch£º146	 i:4 	 global-step:2924	 l-p:-1.7516064643859863
epoch£º146	 i:5 	 global-step:2925	 l-p:0.13896377384662628
epoch£º146	 i:6 	 global-step:2926	 l-p:0.1581028699874878
epoch£º146	 i:7 	 global-step:2927	 l-p:0.13855193555355072
epoch£º146	 i:8 	 global-step:2928	 l-p:0.1587141454219818
epoch£º146	 i:9 	 global-step:2929	 l-p:0.17896302044391632
====================================================================================================
====================================================================================================
====================================================================================================

epoch:147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8391, 5.0073, 4.9536],
        [4.8391, 4.8391, 4.8391],
        [4.8391, 4.8827, 4.8527],
        [4.8391, 5.0871, 5.0481]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:147, step:0 
model_pd.l_p.mean(): 0.13568657636642456 
model_pd.l_d.mean(): -20.61569595336914 
model_pd.lagr.mean(): -20.480009078979492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4361], device='cuda:0')), ('power', tensor([-21.4529], device='cuda:0'))])
epoch£º147	 i:0 	 global-step:2940	 l-p:0.13568657636642456
epoch£º147	 i:1 	 global-step:2941	 l-p:0.1300099492073059
epoch£º147	 i:2 	 global-step:2942	 l-p:0.1331978738307953
epoch£º147	 i:3 	 global-step:2943	 l-p:0.16400882601737976
epoch£º147	 i:4 	 global-step:2944	 l-p:0.13979093730449677
epoch£º147	 i:5 	 global-step:2945	 l-p:0.1480412632226944
epoch£º147	 i:6 	 global-step:2946	 l-p:0.0516030378639698
epoch£º147	 i:7 	 global-step:2947	 l-p:0.11923231184482574
epoch£º147	 i:8 	 global-step:2948	 l-p:0.14121070504188538
epoch£º147	 i:9 	 global-step:2949	 l-p:0.12225181609392166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0396, 5.0607, 5.0438],
        [5.0396, 5.0493, 5.0408],
        [5.0396, 6.1464, 6.7946],
        [5.0396, 5.0396, 5.0396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:148, step:0 
model_pd.l_p.mean(): 0.12211047112941742 
model_pd.l_d.mean(): -20.333650588989258 
model_pd.lagr.mean(): -20.21154022216797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4060], device='cuda:0')), ('power', tensor([-21.1344], device='cuda:0'))])
epoch£º148	 i:0 	 global-step:2960	 l-p:0.12211047112941742
epoch£º148	 i:1 	 global-step:2961	 l-p:0.14633764326572418
epoch£º148	 i:2 	 global-step:2962	 l-p:0.12663845717906952
epoch£º148	 i:3 	 global-step:2963	 l-p:0.16389383375644684
epoch£º148	 i:4 	 global-step:2964	 l-p:0.13562576472759247
epoch£º148	 i:5 	 global-step:2965	 l-p:0.11808492243289948
epoch£º148	 i:6 	 global-step:2966	 l-p:0.14149832725524902
epoch£º148	 i:7 	 global-step:2967	 l-p:0.10527368634939194
epoch£º148	 i:8 	 global-step:2968	 l-p:0.12073010951280594
epoch£º148	 i:9 	 global-step:2969	 l-p:0.14026649296283722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8322, 4.8921, 4.8547],
        [4.8322, 4.8606, 4.8392],
        [4.8322, 5.8513, 6.4364],
        [4.8322, 5.2747, 5.3323]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:149, step:0 
model_pd.l_p.mean(): 0.21222737431526184 
model_pd.l_d.mean(): -18.60541534423828 
model_pd.lagr.mean(): -18.3931884765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5853], device='cuda:0')), ('power', tensor([-19.5595], device='cuda:0'))])
epoch£º149	 i:0 	 global-step:2980	 l-p:0.21222737431526184
epoch£º149	 i:1 	 global-step:2981	 l-p:0.14358113706111908
epoch£º149	 i:2 	 global-step:2982	 l-p:0.25129616260528564
epoch£º149	 i:3 	 global-step:2983	 l-p:0.140147402882576
epoch£º149	 i:4 	 global-step:2984	 l-p:0.1094873696565628
epoch£º149	 i:5 	 global-step:2985	 l-p:0.14346331357955933
epoch£º149	 i:6 	 global-step:2986	 l-p:0.13817299902439117
epoch£º149	 i:7 	 global-step:2987	 l-p:0.1873539686203003
epoch£º149	 i:8 	 global-step:2988	 l-p:0.11678370088338852
epoch£º149	 i:9 	 global-step:2989	 l-p:0.14816999435424805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8474, 5.1334, 5.1076],
        [4.8474, 5.2568, 5.2930],
        [4.8474, 5.1272, 5.0991],
        [4.8474, 4.8492, 4.8475]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:150, step:0 
model_pd.l_p.mean(): 0.13506752252578735 
model_pd.l_d.mean(): -18.513137817382812 
model_pd.lagr.mean(): -18.378070831298828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5547], device='cuda:0')), ('power', tensor([-19.4338], device='cuda:0'))])
epoch£º150	 i:0 	 global-step:3000	 l-p:0.13506752252578735
epoch£º150	 i:1 	 global-step:3001	 l-p:0.17710140347480774
epoch£º150	 i:2 	 global-step:3002	 l-p:0.0712265744805336
epoch£º150	 i:3 	 global-step:3003	 l-p:0.12189611792564392
epoch£º150	 i:4 	 global-step:3004	 l-p:0.1210411787033081
epoch£º150	 i:5 	 global-step:3005	 l-p:0.1267092525959015
epoch£º150	 i:6 	 global-step:3006	 l-p:0.12315420061349869
epoch£º150	 i:7 	 global-step:3007	 l-p:0.12715564668178558
epoch£º150	 i:8 	 global-step:3008	 l-p:0.12825964391231537
epoch£º150	 i:9 	 global-step:3009	 l-p:0.16343769431114197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9275, 5.2746, 5.2721],
        [4.9275, 4.9275, 4.9275],
        [4.9275, 5.6932, 6.0056],
        [4.9275, 5.3931, 5.4598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:151, step:0 
model_pd.l_p.mean(): 0.12991982698440552 
model_pd.l_d.mean(): -20.571439743041992 
model_pd.lagr.mean(): -20.44152069091797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4166], device='cuda:0')), ('power', tensor([-21.3876], device='cuda:0'))])
epoch£º151	 i:0 	 global-step:3020	 l-p:0.12991982698440552
epoch£º151	 i:1 	 global-step:3021	 l-p:0.1406756490468979
epoch£º151	 i:2 	 global-step:3022	 l-p:0.2167297899723053
epoch£º151	 i:3 	 global-step:3023	 l-p:0.015998290851712227
epoch£º151	 i:4 	 global-step:3024	 l-p:0.3210057020187378
epoch£º151	 i:5 	 global-step:3025	 l-p:0.15802110731601715
epoch£º151	 i:6 	 global-step:3026	 l-p:0.14703013002872467
epoch£º151	 i:7 	 global-step:3027	 l-p:0.062001075595617294
epoch£º151	 i:8 	 global-step:3028	 l-p:0.1435823291540146
epoch£º151	 i:9 	 global-step:3029	 l-p:0.12895509600639343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:152
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2225,  0.1348,  1.0000,  0.0817,
          1.0000,  0.6059, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2584,  0.1646,  1.0000,  0.1048,
          1.0000,  0.6369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228]], device='cuda:0')
 pt:tensor([[4.9232, 5.0553, 5.0008],
        [4.9232, 5.8538, 6.3311],
        [4.9232, 5.0946, 5.0399],
        [4.9232, 5.0897, 5.0347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:152, step:0 
model_pd.l_p.mean(): 0.08862666040658951 
model_pd.l_d.mean(): -19.69965934753418 
model_pd.lagr.mean(): -19.611032485961914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5195], device='cuda:0')), ('power', tensor([-20.6060], device='cuda:0'))])
epoch£º152	 i:0 	 global-step:3040	 l-p:0.08862666040658951
epoch£º152	 i:1 	 global-step:3041	 l-p:0.11843948811292648
epoch£º152	 i:2 	 global-step:3042	 l-p:0.12977595627307892
epoch£º152	 i:3 	 global-step:3043	 l-p:0.12515053153038025
epoch£º152	 i:4 	 global-step:3044	 l-p:0.13371816277503967
epoch£º152	 i:5 	 global-step:3045	 l-p:0.12178876250982285
epoch£º152	 i:6 	 global-step:3046	 l-p:0.12784147262573242
epoch£º152	 i:7 	 global-step:3047	 l-p:0.1415853500366211
epoch£º152	 i:8 	 global-step:3048	 l-p:0.16004678606987
epoch£º152	 i:9 	 global-step:3049	 l-p:0.12454141676425934
====================================================================================================
====================================================================================================
====================================================================================================

epoch:153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9725, 4.9763, 4.9728],
        [4.9725, 4.9726, 4.9725],
        [4.9725, 4.9827, 4.9739],
        [4.9725, 4.9725, 4.9725]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:153, step:0 
model_pd.l_p.mean(): 0.17316806316375732 
model_pd.l_d.mean(): -19.22088623046875 
model_pd.lagr.mean(): -19.047718048095703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4593], device='cuda:0')), ('power', tensor([-20.0560], device='cuda:0'))])
epoch£º153	 i:0 	 global-step:3060	 l-p:0.17316806316375732
epoch£º153	 i:1 	 global-step:3061	 l-p:0.1291615217924118
epoch£º153	 i:2 	 global-step:3062	 l-p:0.051235370337963104
epoch£º153	 i:3 	 global-step:3063	 l-p:0.14101415872573853
epoch£º153	 i:4 	 global-step:3064	 l-p:0.13661152124404907
epoch£º153	 i:5 	 global-step:3065	 l-p:0.11786256730556488
epoch£º153	 i:6 	 global-step:3066	 l-p:0.14326165616512299
epoch£º153	 i:7 	 global-step:3067	 l-p:0.12165280431509018
epoch£º153	 i:8 	 global-step:3068	 l-p:0.14098843932151794
epoch£º153	 i:9 	 global-step:3069	 l-p:0.17211371660232544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7427, 5.3099, 5.4690],
        [4.7427, 4.8748, 4.8228],
        [4.7427, 4.7637, 4.7471],
        [4.7427, 4.7465, 4.7430]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:154, step:0 
model_pd.l_p.mean(): 0.1412612646818161 
model_pd.l_d.mean(): -18.595199584960938 
model_pd.lagr.mean(): -18.453937530517578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5887], device='cuda:0')), ('power', tensor([-19.5527], device='cuda:0'))])
epoch£º154	 i:0 	 global-step:3080	 l-p:0.1412612646818161
epoch£º154	 i:1 	 global-step:3081	 l-p:0.13037528097629547
epoch£º154	 i:2 	 global-step:3082	 l-p:0.19854716956615448
epoch£º154	 i:3 	 global-step:3083	 l-p:0.13954050838947296
epoch£º154	 i:4 	 global-step:3084	 l-p:0.12053897231817245
epoch£º154	 i:5 	 global-step:3085	 l-p:0.15402929484844208
epoch£º154	 i:6 	 global-step:3086	 l-p:0.13376040756702423
epoch£º154	 i:7 	 global-step:3087	 l-p:0.1418742835521698
epoch£º154	 i:8 	 global-step:3088	 l-p:0.4737224578857422
epoch£º154	 i:9 	 global-step:3089	 l-p:0.1311839520931244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0442, 5.6797, 5.8661],
        [5.0442, 5.5915, 5.7085],
        [5.0442, 5.0859, 5.0565],
        [5.0442, 5.3629, 5.3429]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:155, step:0 
model_pd.l_p.mean(): 0.13061338663101196 
model_pd.l_d.mean(): -19.794780731201172 
model_pd.lagr.mean(): -19.664167404174805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4491], device='cuda:0')), ('power', tensor([-20.6301], device='cuda:0'))])
epoch£º155	 i:0 	 global-step:3100	 l-p:0.13061338663101196
epoch£º155	 i:1 	 global-step:3101	 l-p:0.31389474868774414
epoch£º155	 i:2 	 global-step:3102	 l-p:0.1316576600074768
epoch£º155	 i:3 	 global-step:3103	 l-p:0.12627147138118744
epoch£º155	 i:4 	 global-step:3104	 l-p:0.11895354837179184
epoch£º155	 i:5 	 global-step:3105	 l-p:0.15506649017333984
epoch£º155	 i:6 	 global-step:3106	 l-p:0.12806163728237152
epoch£º155	 i:7 	 global-step:3107	 l-p:0.17098380625247955
epoch£º155	 i:8 	 global-step:3108	 l-p:0.12235669791698456
epoch£º155	 i:9 	 global-step:3109	 l-p:0.1380801796913147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:156
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9160,  0.8896,  1.0000,  0.8640,
          1.0000,  0.9712, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228]], device='cuda:0')
 pt:tensor([[4.8869, 5.9743, 6.6295],
        [4.8869, 5.9974, 6.6788],
        [4.8869, 5.0330, 4.9787],
        [4.8869, 5.9710, 6.6225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:156, step:0 
model_pd.l_p.mean(): 0.1262732595205307 
model_pd.l_d.mean(): -19.824609756469727 
model_pd.lagr.mean(): -19.698335647583008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5021], device='cuda:0')), ('power', tensor([-20.7154], device='cuda:0'))])
epoch£º156	 i:0 	 global-step:3120	 l-p:0.1262732595205307
epoch£º156	 i:1 	 global-step:3121	 l-p:0.1508595049381256
epoch£º156	 i:2 	 global-step:3122	 l-p:0.1248372420668602
epoch£º156	 i:3 	 global-step:3123	 l-p:0.071942999958992
epoch£º156	 i:4 	 global-step:3124	 l-p:0.14377759397029877
epoch£º156	 i:5 	 global-step:3125	 l-p:0.13755899667739868
epoch£º156	 i:6 	 global-step:3126	 l-p:0.15223704278469086
epoch£º156	 i:7 	 global-step:3127	 l-p:0.12948104739189148
epoch£º156	 i:8 	 global-step:3128	 l-p:0.12161916494369507
epoch£º156	 i:9 	 global-step:3129	 l-p:0.1456623375415802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:157
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1482,  0.0784,  1.0000,  0.0415,
          1.0000,  0.5292, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228]], device='cuda:0')
 pt:tensor([[4.9446, 5.0055, 4.9675],
        [4.9446, 5.4287, 5.5090],
        [4.9446, 5.8185, 6.2373],
        [4.9446, 5.1697, 5.1231]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:157, step:0 
model_pd.l_p.mean(): 0.08174560964107513 
model_pd.l_d.mean(): -20.010238647460938 
model_pd.lagr.mean(): -19.92849349975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4786], device='cuda:0')), ('power', tensor([-20.8801], device='cuda:0'))])
epoch£º157	 i:0 	 global-step:3140	 l-p:0.08174560964107513
epoch£º157	 i:1 	 global-step:3141	 l-p:0.11531095206737518
epoch£º157	 i:2 	 global-step:3142	 l-p:0.16986532509326935
epoch£º157	 i:3 	 global-step:3143	 l-p:0.12756453454494476
epoch£º157	 i:4 	 global-step:3144	 l-p:0.1459321528673172
epoch£º157	 i:5 	 global-step:3145	 l-p:0.20318610966205597
epoch£º157	 i:6 	 global-step:3146	 l-p:0.1339593231678009
epoch£º157	 i:7 	 global-step:3147	 l-p:0.1563965380191803
epoch£º157	 i:8 	 global-step:3148	 l-p:0.14170798659324646
epoch£º157	 i:9 	 global-step:3149	 l-p:0.21375489234924316
====================================================================================================
====================================================================================================
====================================================================================================

epoch:158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9010, 4.9010, 4.9010],
        [4.9010, 4.9087, 4.9019],
        [4.9010, 4.9010, 4.9010],
        [4.9010, 5.2945, 5.3203]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:158, step:0 
model_pd.l_p.mean(): 0.14730092883110046 
model_pd.l_d.mean(): -19.428543090820312 
model_pd.lagr.mean(): -19.28124237060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4816], device='cuda:0')), ('power', tensor([-20.2906], device='cuda:0'))])
epoch£º158	 i:0 	 global-step:3160	 l-p:0.14730092883110046
epoch£º158	 i:1 	 global-step:3161	 l-p:0.13415615260601044
epoch£º158	 i:2 	 global-step:3162	 l-p:0.1207156851887703
epoch£º158	 i:3 	 global-step:3163	 l-p:0.11967700719833374
epoch£º158	 i:4 	 global-step:3164	 l-p:0.12494048476219177
epoch£º158	 i:5 	 global-step:3165	 l-p:0.12882070243358612
epoch£º158	 i:6 	 global-step:3166	 l-p:0.14894960820674896
epoch£º158	 i:7 	 global-step:3167	 l-p:0.1391141563653946
epoch£º158	 i:8 	 global-step:3168	 l-p:0.1366024613380432
epoch£º158	 i:9 	 global-step:3169	 l-p:0.22044332325458527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8477, 4.9810, 4.9278],
        [4.8477, 4.9055, 4.8691],
        [4.8477, 4.9473, 4.8982],
        [4.8477, 4.8794, 4.8560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:159, step:0 
model_pd.l_p.mean(): 0.2403883934020996 
model_pd.l_d.mean(): -20.31931495666504 
model_pd.lagr.mean(): -20.07892608642578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4756], device='cuda:0')), ('power', tensor([-21.1919], device='cuda:0'))])
epoch£º159	 i:0 	 global-step:3180	 l-p:0.2403883934020996
epoch£º159	 i:1 	 global-step:3181	 l-p:0.13721834123134613
epoch£º159	 i:2 	 global-step:3182	 l-p:0.13479800522327423
epoch£º159	 i:3 	 global-step:3183	 l-p:-0.126179501414299
epoch£º159	 i:4 	 global-step:3184	 l-p:0.15141309797763824
epoch£º159	 i:5 	 global-step:3185	 l-p:0.13127067685127258
epoch£º159	 i:6 	 global-step:3186	 l-p:0.11718818545341492
epoch£º159	 i:7 	 global-step:3187	 l-p:0.13445816934108734
epoch£º159	 i:8 	 global-step:3188	 l-p:0.12308476865291595
epoch£º159	 i:9 	 global-step:3189	 l-p:0.17765818536281586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8854, 4.9397, 4.9047],
        [4.8854, 5.5741, 5.8250],
        [4.8854, 4.8854, 4.8854],
        [4.8854, 4.8859, 4.8854]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:160, step:0 
model_pd.l_p.mean(): 0.18677809834480286 
model_pd.l_d.mean(): -20.718996047973633 
model_pd.lagr.mean(): -20.532217025756836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4133], device='cuda:0')), ('power', tensor([-21.5345], device='cuda:0'))])
epoch£º160	 i:0 	 global-step:3200	 l-p:0.18677809834480286
epoch£º160	 i:1 	 global-step:3201	 l-p:0.11581455171108246
epoch£º160	 i:2 	 global-step:3202	 l-p:0.12458793073892593
epoch£º160	 i:3 	 global-step:3203	 l-p:0.13099539279937744
epoch£º160	 i:4 	 global-step:3204	 l-p:0.13994257152080536
epoch£º160	 i:5 	 global-step:3205	 l-p:-0.21122778952121735
epoch£º160	 i:6 	 global-step:3206	 l-p:0.12787027657032013
epoch£º160	 i:7 	 global-step:3207	 l-p:0.1324501782655716
epoch£º160	 i:8 	 global-step:3208	 l-p:0.1338607519865036
epoch£º160	 i:9 	 global-step:3209	 l-p:0.1575131118297577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:161
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2302,  0.1411,  1.0000,  0.0865,
          1.0000,  0.6129, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1536,  0.0823,  1.0000,  0.0441,
          1.0000,  0.5356, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228]], device='cuda:0')
 pt:tensor([[4.9399, 5.0717, 5.0174],
        [4.9399, 5.0024, 4.9638],
        [4.9399, 5.5120, 5.6594],
        [4.9399, 5.5860, 5.7943]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:161, step:0 
model_pd.l_p.mean(): 0.1605142056941986 
model_pd.l_d.mean(): -20.704622268676758 
model_pd.lagr.mean(): -20.54410743713379 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4006], device='cuda:0')), ('power', tensor([-21.5067], device='cuda:0'))])
epoch£º161	 i:0 	 global-step:3220	 l-p:0.1605142056941986
epoch£º161	 i:1 	 global-step:3221	 l-p:0.1311820149421692
epoch£º161	 i:2 	 global-step:3222	 l-p:0.29004067182540894
epoch£º161	 i:3 	 global-step:3223	 l-p:0.11710986495018005
epoch£º161	 i:4 	 global-step:3224	 l-p:0.0360659584403038
epoch£º161	 i:5 	 global-step:3225	 l-p:0.19051681458950043
epoch£º161	 i:6 	 global-step:3226	 l-p:0.13052873313426971
epoch£º161	 i:7 	 global-step:3227	 l-p:0.14530245959758759
epoch£º161	 i:8 	 global-step:3228	 l-p:0.16932910680770874
epoch£º161	 i:9 	 global-step:3229	 l-p:0.16564011573791504
====================================================================================================
====================================================================================================
====================================================================================================

epoch:162
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6488, 5.1075, 5.1950],
        [4.6488, 4.6493, 4.6489],
        [4.6488, 4.7230, 4.6816],
        [4.6488, 5.2229, 5.4010]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:162, step:0 
model_pd.l_p.mean(): 0.12029149383306503 
model_pd.l_d.mean(): -20.17251968383789 
model_pd.lagr.mean(): -20.052228927612305 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5505], device='cuda:0')), ('power', tensor([-21.1199], device='cuda:0'))])
epoch£º162	 i:0 	 global-step:3240	 l-p:0.12029149383306503
epoch£º162	 i:1 	 global-step:3241	 l-p:0.1534823775291443
epoch£º162	 i:2 	 global-step:3242	 l-p:0.15924681723117828
epoch£º162	 i:3 	 global-step:3243	 l-p:0.15792275965213776
epoch£º162	 i:4 	 global-step:3244	 l-p:0.04624701291322708
epoch£º162	 i:5 	 global-step:3245	 l-p:0.12786921858787537
epoch£º162	 i:6 	 global-step:3246	 l-p:0.18761275708675385
epoch£º162	 i:7 	 global-step:3247	 l-p:0.07216101139783859
epoch£º162	 i:8 	 global-step:3248	 l-p:0.32858917117118835
epoch£º162	 i:9 	 global-step:3249	 l-p:0.12451757490634918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9598, 5.8245, 6.2346],
        [4.9598, 4.9598, 4.9598],
        [4.9598, 5.2732, 5.2564],
        [4.9598, 4.9598, 4.9598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:163, step:0 
model_pd.l_p.mean(): 0.14264768362045288 
model_pd.l_d.mean(): -18.397737503051758 
model_pd.lagr.mean(): -18.255090713500977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5616], device='cuda:0')), ('power', tensor([-19.3234], device='cuda:0'))])
epoch£º163	 i:0 	 global-step:3260	 l-p:0.14264768362045288
epoch£º163	 i:1 	 global-step:3261	 l-p:0.11653220653533936
epoch£º163	 i:2 	 global-step:3262	 l-p:0.11873923242092133
epoch£º163	 i:3 	 global-step:3263	 l-p:0.15873725712299347
epoch£º163	 i:4 	 global-step:3264	 l-p:0.13054880499839783
epoch£º163	 i:5 	 global-step:3265	 l-p:0.13630175590515137
epoch£º163	 i:6 	 global-step:3266	 l-p:0.11609910428524017
epoch£º163	 i:7 	 global-step:3267	 l-p:0.13939134776592255
epoch£º163	 i:8 	 global-step:3268	 l-p:0.12521584331989288
epoch£º163	 i:9 	 global-step:3269	 l-p:0.1310962289571762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:164
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9667, 5.1861, 5.1384],
        [4.9667, 4.9707, 4.9671],
        [4.9667, 4.9675, 4.9668],
        [4.9667, 4.9891, 4.9715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:164, step:0 
model_pd.l_p.mean(): 0.1268417090177536 
model_pd.l_d.mean(): -20.00238609313965 
model_pd.lagr.mean(): -19.87554359436035 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4593], device='cuda:0')), ('power', tensor([-20.8521], device='cuda:0'))])
epoch£º164	 i:0 	 global-step:3280	 l-p:0.1268417090177536
epoch£º164	 i:1 	 global-step:3281	 l-p:0.1277635395526886
epoch£º164	 i:2 	 global-step:3282	 l-p:0.1459560990333557
epoch£º164	 i:3 	 global-step:3283	 l-p:0.11097560822963715
epoch£º164	 i:4 	 global-step:3284	 l-p:0.14655785262584686
epoch£º164	 i:5 	 global-step:3285	 l-p:0.12940393388271332
epoch£º164	 i:6 	 global-step:3286	 l-p:0.1295151561498642
epoch£º164	 i:7 	 global-step:3287	 l-p:0.14458638429641724
epoch£º164	 i:8 	 global-step:3288	 l-p:0.14991030097007751
epoch£º164	 i:9 	 global-step:3289	 l-p:0.14764653146266937
====================================================================================================
====================================================================================================
====================================================================================================

epoch:165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8978, 4.8978, 4.8978],
        [4.8978, 5.6363, 5.9333],
        [4.8978, 5.4146, 5.5247],
        [4.8978, 5.0782, 5.0259]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:165, step:0 
model_pd.l_p.mean(): 0.13089855015277863 
model_pd.l_d.mean(): -20.044029235839844 
model_pd.lagr.mean(): -19.913129806518555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4774], device='cuda:0')), ('power', tensor([-20.9133], device='cuda:0'))])
epoch£º165	 i:0 	 global-step:3300	 l-p:0.13089855015277863
epoch£º165	 i:1 	 global-step:3301	 l-p:0.13206727802753448
epoch£º165	 i:2 	 global-step:3302	 l-p:0.16267819702625275
epoch£º165	 i:3 	 global-step:3303	 l-p:0.1432076096534729
epoch£º165	 i:4 	 global-step:3304	 l-p:-0.022544141858816147
epoch£º165	 i:5 	 global-step:3305	 l-p:0.369148850440979
epoch£º165	 i:6 	 global-step:3306	 l-p:0.12381207942962646
epoch£º165	 i:7 	 global-step:3307	 l-p:0.14196740090847015
epoch£º165	 i:8 	 global-step:3308	 l-p:-0.033147554844617844
epoch£º165	 i:9 	 global-step:3309	 l-p:0.13009658455848694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8856, 4.8872, 4.8857],
        [4.8856, 4.9521, 4.9122],
        [4.8856, 4.9613, 4.9182],
        [4.8856, 4.8856, 4.8856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:166, step:0 
model_pd.l_p.mean(): 0.14085668325424194 
model_pd.l_d.mean(): -18.828521728515625 
model_pd.lagr.mean(): -18.687665939331055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5416], device='cuda:0')), ('power', tensor([-19.7416], device='cuda:0'))])
epoch£º166	 i:0 	 global-step:3320	 l-p:0.14085668325424194
epoch£º166	 i:1 	 global-step:3321	 l-p:0.130179762840271
epoch£º166	 i:2 	 global-step:3322	 l-p:0.14248207211494446
epoch£º166	 i:3 	 global-step:3323	 l-p:0.14104433357715607
epoch£º166	 i:4 	 global-step:3324	 l-p:0.125564843416214
epoch£º166	 i:5 	 global-step:3325	 l-p:0.11059417575597763
epoch£º166	 i:6 	 global-step:3326	 l-p:0.12316251546144485
epoch£º166	 i:7 	 global-step:3327	 l-p:0.12944960594177246
epoch£º166	 i:8 	 global-step:3328	 l-p:0.11851336807012558
epoch£º166	 i:9 	 global-step:3329	 l-p:0.5260671973228455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0103, 5.0103, 5.0103],
        [5.0103, 5.1612, 5.1055],
        [5.0103, 6.2028, 6.9613],
        [5.0103, 5.0218, 5.0120]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:167, step:0 
model_pd.l_p.mean(): 0.1346389800310135 
model_pd.l_d.mean(): -19.303730010986328 
model_pd.lagr.mean(): -19.169090270996094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4810], device='cuda:0')), ('power', tensor([-20.1628], device='cuda:0'))])
epoch£º167	 i:0 	 global-step:3340	 l-p:0.1346389800310135
epoch£º167	 i:1 	 global-step:3341	 l-p:0.1299271434545517
epoch£º167	 i:2 	 global-step:3342	 l-p:0.1270674467086792
epoch£º167	 i:3 	 global-step:3343	 l-p:0.13896560668945312
epoch£º167	 i:4 	 global-step:3344	 l-p:0.1327439695596695
epoch£º167	 i:5 	 global-step:3345	 l-p:0.1215139850974083
epoch£º167	 i:6 	 global-step:3346	 l-p:0.4343585968017578
epoch£º167	 i:7 	 global-step:3347	 l-p:0.0780627578496933
epoch£º167	 i:8 	 global-step:3348	 l-p:0.055212609469890594
epoch£º167	 i:9 	 global-step:3349	 l-p:0.12085286527872086
====================================================================================================
====================================================================================================
====================================================================================================

epoch:168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8224, 4.8229, 4.8224],
        [4.8224, 5.8337, 6.4204],
        [4.8224, 4.8224, 4.8224],
        [4.8224, 4.8224, 4.8224]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:168, step:0 
model_pd.l_p.mean(): 0.12856721878051758 
model_pd.l_d.mean(): -18.50955581665039 
model_pd.lagr.mean(): -18.38098907470703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5759], device='cuda:0')), ('power', tensor([-19.4521], device='cuda:0'))])
epoch£º168	 i:0 	 global-step:3360	 l-p:0.12856721878051758
epoch£º168	 i:1 	 global-step:3361	 l-p:0.12690486013889313
epoch£º168	 i:2 	 global-step:3362	 l-p:0.146742582321167
epoch£º168	 i:3 	 global-step:3363	 l-p:0.14480645954608917
epoch£º168	 i:4 	 global-step:3364	 l-p:0.14134536683559418
epoch£º168	 i:5 	 global-step:3365	 l-p:0.11977998167276382
epoch£º168	 i:6 	 global-step:3366	 l-p:0.11622999608516693
epoch£º168	 i:7 	 global-step:3367	 l-p:0.14006711542606354
epoch£º168	 i:8 	 global-step:3368	 l-p:0.1506177932024002
epoch£º168	 i:9 	 global-step:3369	 l-p:0.12784543633460999
====================================================================================================
====================================================================================================
====================================================================================================

epoch:169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8985, 4.8987, 4.8985],
        [4.8985, 4.9166, 4.9020],
        [4.8985, 5.0277, 4.9744],
        [4.8985, 4.9483, 4.9153]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:169, step:0 
model_pd.l_p.mean(): 0.13707435131072998 
model_pd.l_d.mean(): -20.550640106201172 
model_pd.lagr.mean(): -20.41356658935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4272], device='cuda:0')), ('power', tensor([-21.3774], device='cuda:0'))])
epoch£º169	 i:0 	 global-step:3380	 l-p:0.13707435131072998
epoch£º169	 i:1 	 global-step:3381	 l-p:0.1415747106075287
epoch£º169	 i:2 	 global-step:3382	 l-p:0.13314242660999298
epoch£º169	 i:3 	 global-step:3383	 l-p:-0.1876201629638672
epoch£º169	 i:4 	 global-step:3384	 l-p:0.13553476333618164
epoch£º169	 i:5 	 global-step:3385	 l-p:0.09304633736610413
epoch£º169	 i:6 	 global-step:3386	 l-p:0.03840282931923866
epoch£º169	 i:7 	 global-step:3387	 l-p:0.15152765810489655
epoch£º169	 i:8 	 global-step:3388	 l-p:0.12490642070770264
epoch£º169	 i:9 	 global-step:3389	 l-p:0.14755719900131226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9601, 5.1542, 5.1027],
        [4.9601, 4.9609, 4.9601],
        [4.9601, 4.9834, 4.9651],
        [4.9601, 4.9607, 4.9601]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:170, step:0 
model_pd.l_p.mean(): 0.1496751606464386 
model_pd.l_d.mean(): -18.643831253051758 
model_pd.lagr.mean(): -18.494155883789062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5315], device='cuda:0')), ('power', tensor([-19.5429], device='cuda:0'))])
epoch£º170	 i:0 	 global-step:3400	 l-p:0.1496751606464386
epoch£º170	 i:1 	 global-step:3401	 l-p:0.13907361030578613
epoch£º170	 i:2 	 global-step:3402	 l-p:0.13736660778522491
epoch£º170	 i:3 	 global-step:3403	 l-p:0.12428329885005951
epoch£º170	 i:4 	 global-step:3404	 l-p:0.12301451712846756
epoch£º170	 i:5 	 global-step:3405	 l-p:0.30050069093704224
epoch£º170	 i:6 	 global-step:3406	 l-p:0.11999040096998215
epoch£º170	 i:7 	 global-step:3407	 l-p:0.12206239253282547
epoch£º170	 i:8 	 global-step:3408	 l-p:0.12789180874824524
epoch£º170	 i:9 	 global-step:3409	 l-p:0.15006259083747864
====================================================================================================
====================================================================================================
====================================================================================================

epoch:171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9603, 5.4463, 5.5317],
        [4.9603, 4.9616, 4.9604],
        [4.9603, 4.9849, 4.9658],
        [4.9603, 4.9603, 4.9603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:171, step:0 
model_pd.l_p.mean(): 0.11712026596069336 
model_pd.l_d.mean(): -18.537031173706055 
model_pd.lagr.mean(): -18.419910430908203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5485], device='cuda:0')), ('power', tensor([-19.4518], device='cuda:0'))])
epoch£º171	 i:0 	 global-step:3420	 l-p:0.11712026596069336
epoch£º171	 i:1 	 global-step:3421	 l-p:0.12483036518096924
epoch£º171	 i:2 	 global-step:3422	 l-p:0.16646356880664825
epoch£º171	 i:3 	 global-step:3423	 l-p:0.14447037875652313
epoch£º171	 i:4 	 global-step:3424	 l-p:0.0778636708855629
epoch£º171	 i:5 	 global-step:3425	 l-p:0.11658231168985367
epoch£º171	 i:6 	 global-step:3426	 l-p:0.13424566388130188
epoch£º171	 i:7 	 global-step:3427	 l-p:0.24019554257392883
epoch£º171	 i:8 	 global-step:3428	 l-p:0.13771484792232513
epoch£º171	 i:9 	 global-step:3429	 l-p:0.15457794070243835
====================================================================================================
====================================================================================================
====================================================================================================

epoch:172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8199, 4.8204, 4.8200],
        [4.8199, 4.8252, 4.8205],
        [4.8199, 5.0659, 5.0309],
        [4.8199, 4.8199, 4.8199]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:172, step:0 
model_pd.l_p.mean(): 0.1144961267709732 
model_pd.l_d.mean(): -20.624401092529297 
model_pd.lagr.mean(): -20.509904861450195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4473], device='cuda:0')), ('power', tensor([-21.4733], device='cuda:0'))])
epoch£º172	 i:0 	 global-step:3440	 l-p:0.1144961267709732
epoch£º172	 i:1 	 global-step:3441	 l-p:0.18389397859573364
epoch£º172	 i:2 	 global-step:3442	 l-p:0.09504895657300949
epoch£º172	 i:3 	 global-step:3443	 l-p:-1.0386042594909668
epoch£º172	 i:4 	 global-step:3444	 l-p:0.12588801980018616
epoch£º172	 i:5 	 global-step:3445	 l-p:0.13554920256137848
epoch£º172	 i:6 	 global-step:3446	 l-p:0.13939855992794037
epoch£º172	 i:7 	 global-step:3447	 l-p:0.1744910180568695
epoch£º172	 i:8 	 global-step:3448	 l-p:0.12032045423984528
epoch£º172	 i:9 	 global-step:3449	 l-p:0.12997737526893616
====================================================================================================
====================================================================================================
====================================================================================================

epoch:173
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1980,  0.1154,  1.0000,  0.0672,
          1.0000,  0.5828, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2106,  0.1253,  1.0000,  0.0745,
          1.0000,  0.5949, 31.6228]], device='cuda:0')
 pt:tensor([[5.1254, 6.2466, 6.9083],
        [5.1254, 5.2239, 5.1731],
        [5.1254, 5.8157, 6.0485],
        [5.1254, 5.2359, 5.1826]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:173, step:0 
model_pd.l_p.mean(): 0.11865244805812836 
model_pd.l_d.mean(): -19.316242218017578 
model_pd.lagr.mean(): -19.197589874267578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4771], device='cuda:0')), ('power', tensor([-20.1716], device='cuda:0'))])
epoch£º173	 i:0 	 global-step:3460	 l-p:0.11865244805812836
epoch£º173	 i:1 	 global-step:3461	 l-p:0.10033410787582397
epoch£º173	 i:2 	 global-step:3462	 l-p:0.13001887500286102
epoch£º173	 i:3 	 global-step:3463	 l-p:0.14521071314811707
epoch£º173	 i:4 	 global-step:3464	 l-p:0.1267079859972
epoch£º173	 i:5 	 global-step:3465	 l-p:0.14462319016456604
epoch£º173	 i:6 	 global-step:3466	 l-p:0.1343545913696289
epoch£º173	 i:7 	 global-step:3467	 l-p:0.10543125122785568
epoch£º173	 i:8 	 global-step:3468	 l-p:0.10149981081485748
epoch£º173	 i:9 	 global-step:3469	 l-p:0.1436983197927475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6634, 5.4109, 5.7510],
        [4.6634, 4.6745, 4.6650],
        [4.6634, 4.7277, 4.6890],
        [4.6634, 4.7803, 4.7304]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:174, step:0 
model_pd.l_p.mean(): 0.1581507921218872 
model_pd.l_d.mean(): -20.09773063659668 
model_pd.lagr.mean(): -19.939579010009766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5430], device='cuda:0')), ('power', tensor([-21.0360], device='cuda:0'))])
epoch£º174	 i:0 	 global-step:3480	 l-p:0.1581507921218872
epoch£º174	 i:1 	 global-step:3481	 l-p:0.13909584283828735
epoch£º174	 i:2 	 global-step:3482	 l-p:0.10326247662305832
epoch£º174	 i:3 	 global-step:3483	 l-p:0.15481851994991302
epoch£º174	 i:4 	 global-step:3484	 l-p:0.15464884042739868
epoch£º174	 i:5 	 global-step:3485	 l-p:0.3072650730609894
epoch£º174	 i:6 	 global-step:3486	 l-p:0.16608673334121704
epoch£º174	 i:7 	 global-step:3487	 l-p:0.13386227190494537
epoch£º174	 i:8 	 global-step:3488	 l-p:0.1264512836933136
epoch£º174	 i:9 	 global-step:3489	 l-p:0.11638621240854263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1786, 5.1812, 5.1787],
        [5.1786, 5.1901, 5.1802],
        [5.1786, 5.2328, 5.1969],
        [5.1786, 5.2625, 5.2153]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:175, step:0 
model_pd.l_p.mean(): 0.12009971588850021 
model_pd.l_d.mean(): -20.433345794677734 
model_pd.lagr.mean(): -20.31324577331543 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3486], device='cuda:0')), ('power', tensor([-21.1765], device='cuda:0'))])
epoch£º175	 i:0 	 global-step:3500	 l-p:0.12009971588850021
epoch£º175	 i:1 	 global-step:3501	 l-p:0.11928587406873703
epoch£º175	 i:2 	 global-step:3502	 l-p:0.15308333933353424
epoch£º175	 i:3 	 global-step:3503	 l-p:0.1758052408695221
epoch£º175	 i:4 	 global-step:3504	 l-p:0.13002587854862213
epoch£º175	 i:5 	 global-step:3505	 l-p:0.11786680668592453
epoch£º175	 i:6 	 global-step:3506	 l-p:0.11526830494403839
epoch£º175	 i:7 	 global-step:3507	 l-p:0.12464997917413712
epoch£º175	 i:8 	 global-step:3508	 l-p:0.12617337703704834
epoch£º175	 i:9 	 global-step:3509	 l-p:0.1252153068780899
====================================================================================================
====================================================================================================
====================================================================================================

epoch:176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9098, 4.9098, 4.9098],
        [4.9098, 4.9275, 4.9131],
        [4.9098, 5.4311, 5.5481],
        [4.9098, 5.1124, 5.0639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:176, step:0 
model_pd.l_p.mean(): 0.09986856579780579 
model_pd.l_d.mean(): -19.159143447875977 
model_pd.lagr.mean(): -19.059274673461914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5124], device='cuda:0')), ('power', tensor([-20.0481], device='cuda:0'))])
epoch£º176	 i:0 	 global-step:3520	 l-p:0.09986856579780579
epoch£º176	 i:1 	 global-step:3521	 l-p:0.1792064756155014
epoch£º176	 i:2 	 global-step:3522	 l-p:0.1370258927345276
epoch£º176	 i:3 	 global-step:3523	 l-p:0.13644500076770782
epoch£º176	 i:4 	 global-step:3524	 l-p:0.09027718007564545
epoch£º176	 i:5 	 global-step:3525	 l-p:0.19034802913665771
epoch£º176	 i:6 	 global-step:3526	 l-p:0.13394249975681305
epoch£º176	 i:7 	 global-step:3527	 l-p:0.13098494708538055
epoch£º176	 i:8 	 global-step:3528	 l-p:0.15334442257881165
epoch£º176	 i:9 	 global-step:3529	 l-p:0.14749754965305328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8623, 5.9445, 6.6069],
        [4.8623, 4.8629, 4.8623],
        [4.8623, 4.9598, 4.9109],
        [4.8623, 4.8625, 4.8623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:177, step:0 
model_pd.l_p.mean(): 0.10611467063426971 
model_pd.l_d.mean(): -19.665966033935547 
model_pd.lagr.mean(): -19.559850692749023 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5110], device='cuda:0')), ('power', tensor([-20.5630], device='cuda:0'))])
epoch£º177	 i:0 	 global-step:3540	 l-p:0.10611467063426971
epoch£º177	 i:1 	 global-step:3541	 l-p:0.12885791063308716
epoch£º177	 i:2 	 global-step:3542	 l-p:0.15985114872455597
epoch£º177	 i:3 	 global-step:3543	 l-p:0.10898877680301666
epoch£º177	 i:4 	 global-step:3544	 l-p:0.13846473395824432
epoch£º177	 i:5 	 global-step:3545	 l-p:0.10921275615692139
epoch£º177	 i:6 	 global-step:3546	 l-p:0.13302825391292572
epoch£º177	 i:7 	 global-step:3547	 l-p:0.1283317357301712
epoch£º177	 i:8 	 global-step:3548	 l-p:0.15713481605052948
epoch£º177	 i:9 	 global-step:3549	 l-p:0.45171254873275757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8525, 4.8527, 4.8525],
        [4.8525, 4.8525, 4.8525],
        [4.8525, 4.8525, 4.8525],
        [4.8525, 4.9162, 4.8770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:178, step:0 
model_pd.l_p.mean(): -1.021936297416687 
model_pd.l_d.mean(): -18.163206100463867 
model_pd.lagr.mean(): -19.185142517089844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5643], device='cuda:0')), ('power', tensor([-19.0873], device='cuda:0'))])
epoch£º178	 i:0 	 global-step:3560	 l-p:-1.021936297416687
epoch£º178	 i:1 	 global-step:3561	 l-p:0.155425563454628
epoch£º178	 i:2 	 global-step:3562	 l-p:0.14136654138565063
epoch£º178	 i:3 	 global-step:3563	 l-p:0.11585015058517456
epoch£º178	 i:4 	 global-step:3564	 l-p:0.13169410824775696
epoch£º178	 i:5 	 global-step:3565	 l-p:0.15180397033691406
epoch£º178	 i:6 	 global-step:3566	 l-p:0.1226804256439209
epoch£º178	 i:7 	 global-step:3567	 l-p:0.1985846310853958
epoch£º178	 i:8 	 global-step:3568	 l-p:0.119407519698143
epoch£º178	 i:9 	 global-step:3569	 l-p:0.25413021445274353
====================================================================================================
====================================================================================================
====================================================================================================

epoch:179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9121, 4.9131, 4.9121],
        [4.9121, 4.9335, 4.9164],
        [4.9121, 4.9322, 4.9160],
        [4.9121, 4.9125, 4.9121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:179, step:0 
model_pd.l_p.mean(): 0.15105852484703064 
model_pd.l_d.mean(): -20.384042739868164 
model_pd.lagr.mean(): -20.23298454284668 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4428], device='cuda:0')), ('power', tensor([-21.2238], device='cuda:0'))])
epoch£º179	 i:0 	 global-step:3580	 l-p:0.15105852484703064
epoch£º179	 i:1 	 global-step:3581	 l-p:0.2085331231355667
epoch£º179	 i:2 	 global-step:3582	 l-p:0.12464165687561035
epoch£º179	 i:3 	 global-step:3583	 l-p:0.12562167644500732
epoch£º179	 i:4 	 global-step:3584	 l-p:0.15830694139003754
epoch£º179	 i:5 	 global-step:3585	 l-p:0.1334669440984726
epoch£º179	 i:6 	 global-step:3586	 l-p:0.287197083234787
epoch£º179	 i:7 	 global-step:3587	 l-p:0.13058148324489594
epoch£º179	 i:8 	 global-step:3588	 l-p:0.11701729148626328
epoch£º179	 i:9 	 global-step:3589	 l-p:-0.1261875182390213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8123, 4.8129, 4.8123],
        [4.8123, 4.8127, 4.8123],
        [4.8123, 4.9897, 4.9394],
        [4.8123, 4.8123, 4.8123]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:180, step:0 
model_pd.l_p.mean(): 0.13293464481830597 
model_pd.l_d.mean(): -20.7485294342041 
model_pd.lagr.mean(): -20.6155948638916 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4357], device='cuda:0')), ('power', tensor([-21.5878], device='cuda:0'))])
epoch£º180	 i:0 	 global-step:3600	 l-p:0.13293464481830597
epoch£º180	 i:1 	 global-step:3601	 l-p:0.1617557853460312
epoch£º180	 i:2 	 global-step:3602	 l-p:0.09532085061073303
epoch£º180	 i:3 	 global-step:3603	 l-p:0.09378992021083832
epoch£º180	 i:4 	 global-step:3604	 l-p:0.11027943342924118
epoch£º180	 i:5 	 global-step:3605	 l-p:0.21167349815368652
epoch£º180	 i:6 	 global-step:3606	 l-p:0.08095619082450867
epoch£º180	 i:7 	 global-step:3607	 l-p:0.13230472803115845
epoch£º180	 i:8 	 global-step:3608	 l-p:0.13938845694065094
epoch£º180	 i:9 	 global-step:3609	 l-p:0.13366208970546722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9725, 4.9790, 4.9732],
        [4.9725, 5.0452, 5.0023],
        [4.9725, 5.3678, 5.3969],
        [4.9725, 5.6330, 5.8590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:181, step:0 
model_pd.l_p.mean(): 0.14427275955677032 
model_pd.l_d.mean(): -20.305810928344727 
model_pd.lagr.mean(): -20.16153907775879 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4402], device='cuda:0')), ('power', tensor([-21.1415], device='cuda:0'))])
epoch£º181	 i:0 	 global-step:3620	 l-p:0.14427275955677032
epoch£º181	 i:1 	 global-step:3621	 l-p:0.1704729050397873
epoch£º181	 i:2 	 global-step:3622	 l-p:0.11988313496112823
epoch£º181	 i:3 	 global-step:3623	 l-p:0.10347827523946762
epoch£º181	 i:4 	 global-step:3624	 l-p:0.13222649693489075
epoch£º181	 i:5 	 global-step:3625	 l-p:0.10389212518930435
epoch£º181	 i:6 	 global-step:3626	 l-p:0.13116209208965302
epoch£º181	 i:7 	 global-step:3627	 l-p:0.12507587671279907
epoch£º181	 i:8 	 global-step:3628	 l-p:0.1618489772081375
epoch£º181	 i:9 	 global-step:3629	 l-p:0.18512064218521118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9492, 4.9570, 4.9501],
        [4.9492, 4.9979, 4.9648],
        [4.9492, 5.0694, 5.0161],
        [4.9492, 4.9492, 4.9492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:182, step:0 
model_pd.l_p.mean(): 0.1238994374871254 
model_pd.l_d.mean(): -20.095979690551758 
model_pd.lagr.mean(): -19.97208023071289 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4366], device='cuda:0')), ('power', tensor([-20.9239], device='cuda:0'))])
epoch£º182	 i:0 	 global-step:3640	 l-p:0.1238994374871254
epoch£º182	 i:1 	 global-step:3641	 l-p:0.1425868421792984
epoch£º182	 i:2 	 global-step:3642	 l-p:0.12852145731449127
epoch£º182	 i:3 	 global-step:3643	 l-p:0.12190333753824234
epoch£º182	 i:4 	 global-step:3644	 l-p:0.2459898442029953
epoch£º182	 i:5 	 global-step:3645	 l-p:0.08751287311315536
epoch£º182	 i:6 	 global-step:3646	 l-p:0.18721577525138855
epoch£º182	 i:7 	 global-step:3647	 l-p:0.21141380071640015
epoch£º182	 i:8 	 global-step:3648	 l-p:0.1425836682319641
epoch£º182	 i:9 	 global-step:3649	 l-p:0.1131371259689331
====================================================================================================
====================================================================================================
====================================================================================================

epoch:183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0789, 5.3957, 5.3791],
        [5.0789, 5.2184, 5.1622],
        [5.0789, 5.0870, 5.0799],
        [5.0789, 5.8080, 6.0841]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:183, step:0 
model_pd.l_p.mean(): 0.12538251280784607 
model_pd.l_d.mean(): -20.17618179321289 
model_pd.lagr.mean(): -20.050798416137695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4124], device='cuda:0')), ('power', tensor([-20.9806], device='cuda:0'))])
epoch£º183	 i:0 	 global-step:3660	 l-p:0.12538251280784607
epoch£º183	 i:1 	 global-step:3661	 l-p:0.12486825883388519
epoch£º183	 i:2 	 global-step:3662	 l-p:0.12886559963226318
epoch£º183	 i:3 	 global-step:3663	 l-p:0.1233048290014267
epoch£º183	 i:4 	 global-step:3664	 l-p:0.17133086919784546
epoch£º183	 i:5 	 global-step:3665	 l-p:0.21948017179965973
epoch£º183	 i:6 	 global-step:3666	 l-p:0.13720180094242096
epoch£º183	 i:7 	 global-step:3667	 l-p:0.26311102509498596
epoch£º183	 i:8 	 global-step:3668	 l-p:0.1144590750336647
epoch£º183	 i:9 	 global-step:3669	 l-p:0.12567298114299774
====================================================================================================
====================================================================================================
====================================================================================================

epoch:184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9519, 5.0213, 4.9794],
        [4.9519, 5.7272, 6.0607],
        [4.9519, 4.9840, 4.9599],
        [4.9519, 4.9519, 4.9519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:184, step:0 
model_pd.l_p.mean(): 0.13699622452259064 
model_pd.l_d.mean(): -18.871442794799805 
model_pd.lagr.mean(): -18.734447479248047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5184], device='cuda:0')), ('power', tensor([-19.7612], device='cuda:0'))])
epoch£º184	 i:0 	 global-step:3680	 l-p:0.13699622452259064
epoch£º184	 i:1 	 global-step:3681	 l-p:0.15108934044837952
epoch£º184	 i:2 	 global-step:3682	 l-p:0.12737801671028137
epoch£º184	 i:3 	 global-step:3683	 l-p:0.12049199640750885
epoch£º184	 i:4 	 global-step:3684	 l-p:0.12847276031970978
epoch£º184	 i:5 	 global-step:3685	 l-p:0.09954346716403961
epoch£º184	 i:6 	 global-step:3686	 l-p:0.13395677506923676
epoch£º184	 i:7 	 global-step:3687	 l-p:0.12063644081354141
epoch£º184	 i:8 	 global-step:3688	 l-p:0.11419668793678284
epoch£º184	 i:9 	 global-step:3689	 l-p:0.09945380687713623
====================================================================================================
====================================================================================================
====================================================================================================

epoch:185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9262, 5.0404, 4.9878],
        [4.9262, 4.9262, 4.9262],
        [4.9262, 4.9400, 4.9283],
        [4.9262, 5.6483, 5.9357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:185, step:0 
model_pd.l_p.mean(): 0.12988685071468353 
model_pd.l_d.mean(): -19.219308853149414 
model_pd.lagr.mean(): -19.08942222595215 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5234], device='cuda:0')), ('power', tensor([-20.1208], device='cuda:0'))])
epoch£º185	 i:0 	 global-step:3700	 l-p:0.12988685071468353
epoch£º185	 i:1 	 global-step:3701	 l-p:0.2642807364463806
epoch£º185	 i:2 	 global-step:3702	 l-p:0.13771037757396698
epoch£º185	 i:3 	 global-step:3703	 l-p:0.15080198645591736
epoch£º185	 i:4 	 global-step:3704	 l-p:0.1456315964460373
epoch£º185	 i:5 	 global-step:3705	 l-p:0.021754421293735504
epoch£º185	 i:6 	 global-step:3706	 l-p:0.14312274754047394
epoch£º185	 i:7 	 global-step:3707	 l-p:0.16118042171001434
epoch£º185	 i:8 	 global-step:3708	 l-p:0.11478077620267868
epoch£º185	 i:9 	 global-step:3709	 l-p:0.4356693923473358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9150, 4.9150, 4.9150],
        [4.9150, 5.2614, 5.2675],
        [4.9150, 5.2192, 5.2047],
        [4.9150, 5.0166, 4.9660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:186, step:0 
model_pd.l_p.mean(): 0.13095593452453613 
model_pd.l_d.mean(): -20.05732536315918 
model_pd.lagr.mean(): -19.926368713378906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4810], device='cuda:0')), ('power', tensor([-20.9305], device='cuda:0'))])
epoch£º186	 i:0 	 global-step:3720	 l-p:0.13095593452453613
epoch£º186	 i:1 	 global-step:3721	 l-p:0.13619327545166016
epoch£º186	 i:2 	 global-step:3722	 l-p:0.17558440566062927
epoch£º186	 i:3 	 global-step:3723	 l-p:0.17569303512573242
epoch£º186	 i:4 	 global-step:3724	 l-p:0.103951595723629
epoch£º186	 i:5 	 global-step:3725	 l-p:0.11252710968255997
epoch£º186	 i:6 	 global-step:3726	 l-p:0.1325622946023941
epoch£º186	 i:7 	 global-step:3727	 l-p:0.13747791945934296
epoch£º186	 i:8 	 global-step:3728	 l-p:0.11996085196733475
epoch£º186	 i:9 	 global-step:3729	 l-p:0.11548931896686554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1270, 5.1271, 5.1270],
        [5.1270, 5.3045, 5.2484],
        [5.1270, 5.4417, 5.4228],
        [5.1270, 5.1270, 5.1270]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:187, step:0 
model_pd.l_p.mean(): 0.18660537898540497 
model_pd.l_d.mean(): -19.242067337036133 
model_pd.lagr.mean(): -19.055461883544922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4505], device='cuda:0')), ('power', tensor([-20.0685], device='cuda:0'))])
epoch£º187	 i:0 	 global-step:3740	 l-p:0.18660537898540497
epoch£º187	 i:1 	 global-step:3741	 l-p:0.1401212066411972
epoch£º187	 i:2 	 global-step:3742	 l-p:0.11662318557500839
epoch£º187	 i:3 	 global-step:3743	 l-p:0.13602693378925323
epoch£º187	 i:4 	 global-step:3744	 l-p:0.13947872817516327
epoch£º187	 i:5 	 global-step:3745	 l-p:0.1267968714237213
epoch£º187	 i:6 	 global-step:3746	 l-p:0.07430427521467209
epoch£º187	 i:7 	 global-step:3747	 l-p:0.10616803914308548
epoch£º187	 i:8 	 global-step:3748	 l-p:0.13248758018016815
epoch£º187	 i:9 	 global-step:3749	 l-p:0.145974263548851
====================================================================================================
====================================================================================================
====================================================================================================

epoch:188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6720, 5.3556, 5.6397],
        [4.6720, 4.7308, 4.6926],
        [4.6720, 4.6720, 4.6720],
        [4.6720, 5.3445, 5.6185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:188, step:0 
model_pd.l_p.mean(): 0.15283696353435516 
model_pd.l_d.mean(): -20.530597686767578 
model_pd.lagr.mean(): -20.37775993347168 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5062], device='cuda:0')), ('power', tensor([-21.4388], device='cuda:0'))])
epoch£º188	 i:0 	 global-step:3760	 l-p:0.15283696353435516
epoch£º188	 i:1 	 global-step:3761	 l-p:0.1337449550628662
epoch£º188	 i:2 	 global-step:3762	 l-p:0.13187175989151
epoch£º188	 i:3 	 global-step:3763	 l-p:0.14359484612941742
epoch£º188	 i:4 	 global-step:3764	 l-p:0.0959484651684761
epoch£º188	 i:5 	 global-step:3765	 l-p:0.18785187602043152
epoch£º188	 i:6 	 global-step:3766	 l-p:0.1675182282924652
epoch£º188	 i:7 	 global-step:3767	 l-p:0.10882380604743958
epoch£º188	 i:8 	 global-step:3768	 l-p:0.17112430930137634
epoch£º188	 i:9 	 global-step:3769	 l-p:0.11645650863647461
====================================================================================================
====================================================================================================
====================================================================================================

epoch:189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0586, 6.0570, 6.6020],
        [5.0586, 6.0732, 6.6352],
        [5.0586, 5.0587, 5.0586],
        [5.0586, 5.0586, 5.0586]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:189, step:0 
model_pd.l_p.mean(): 0.10192838311195374 
model_pd.l_d.mean(): -19.516151428222656 
model_pd.lagr.mean(): -19.414222717285156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4626], device='cuda:0')), ('power', tensor([-20.3602], device='cuda:0'))])
epoch£º189	 i:0 	 global-step:3780	 l-p:0.10192838311195374
epoch£º189	 i:1 	 global-step:3781	 l-p:0.18755576014518738
epoch£º189	 i:2 	 global-step:3782	 l-p:0.11339446157217026
epoch£º189	 i:3 	 global-step:3783	 l-p:0.12508387863636017
epoch£º189	 i:4 	 global-step:3784	 l-p:0.12126535922288895
epoch£º189	 i:5 	 global-step:3785	 l-p:0.1033211275935173
epoch£º189	 i:6 	 global-step:3786	 l-p:0.12176889926195145
epoch£º189	 i:7 	 global-step:3787	 l-p:0.10959216952323914
epoch£º189	 i:8 	 global-step:3788	 l-p:0.11679252237081528
epoch£º189	 i:9 	 global-step:3789	 l-p:0.1326599270105362
====================================================================================================
====================================================================================================
====================================================================================================

epoch:190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0394, 5.0394, 5.0394],
        [5.0394, 5.0394, 5.0394],
        [5.0394, 5.0394, 5.0394],
        [5.0394, 5.0532, 5.0414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:190, step:0 
model_pd.l_p.mean(): 0.13203562796115875 
model_pd.l_d.mean(): -20.057090759277344 
model_pd.lagr.mean(): -19.9250545501709 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4309], device='cuda:0')), ('power', tensor([-20.8785], device='cuda:0'))])
epoch£º190	 i:0 	 global-step:3800	 l-p:0.13203562796115875
epoch£º190	 i:1 	 global-step:3801	 l-p:0.1382375806570053
epoch£º190	 i:2 	 global-step:3802	 l-p:0.1322784721851349
epoch£º190	 i:3 	 global-step:3803	 l-p:0.06788424402475357
epoch£º190	 i:4 	 global-step:3804	 l-p:0.23530428111553192
epoch£º190	 i:5 	 global-step:3805	 l-p:0.13851720094680786
epoch£º190	 i:6 	 global-step:3806	 l-p:0.2007269561290741
epoch£º190	 i:7 	 global-step:3807	 l-p:0.1513364315032959
epoch£º190	 i:8 	 global-step:3808	 l-p:0.1505141258239746
epoch£º190	 i:9 	 global-step:3809	 l-p:0.15752412378787994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.6857, 5.5155, 5.9404],
        [4.6857, 4.7229, 4.6948],
        [4.6857, 4.6857, 4.6857],
        [4.6857, 4.6857, 4.6857]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:191, step:0 
model_pd.l_p.mean(): 0.16406501829624176 
model_pd.l_d.mean(): -18.694499969482422 
model_pd.lagr.mean(): -18.53043556213379 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6502], device='cuda:0')), ('power', tensor([-19.7175], device='cuda:0'))])
epoch£º191	 i:0 	 global-step:3820	 l-p:0.16406501829624176
epoch£º191	 i:1 	 global-step:3821	 l-p:0.19882215559482574
epoch£º191	 i:2 	 global-step:3822	 l-p:-0.004922971595078707
epoch£º191	 i:3 	 global-step:3823	 l-p:0.134891539812088
epoch£º191	 i:4 	 global-step:3824	 l-p:0.15919964015483856
epoch£º191	 i:5 	 global-step:3825	 l-p:0.10903768986463547
epoch£º191	 i:6 	 global-step:3826	 l-p:0.12160329520702362
epoch£º191	 i:7 	 global-step:3827	 l-p:0.14184704422950745
epoch£º191	 i:8 	 global-step:3828	 l-p:0.1551900953054428
epoch£º191	 i:9 	 global-step:3829	 l-p:0.12966041266918182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9348, 4.9499, 4.9370],
        [4.9348, 4.9438, 4.9358],
        [4.9348, 5.7622, 6.1510],
        [4.9348, 4.9565, 4.9388]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:192, step:0 
model_pd.l_p.mean(): 0.19458000361919403 
model_pd.l_d.mean(): -20.043554306030273 
model_pd.lagr.mean(): -19.848974227905273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4746], device='cuda:0')), ('power', tensor([-20.9100], device='cuda:0'))])
epoch£º192	 i:0 	 global-step:3840	 l-p:0.19458000361919403
epoch£º192	 i:1 	 global-step:3841	 l-p:0.14732810854911804
epoch£º192	 i:2 	 global-step:3842	 l-p:0.5154253840446472
epoch£º192	 i:3 	 global-step:3843	 l-p:0.12873011827468872
epoch£º192	 i:4 	 global-step:3844	 l-p:0.5323066115379333
epoch£º192	 i:5 	 global-step:3845	 l-p:0.13041144609451294
epoch£º192	 i:6 	 global-step:3846	 l-p:0.14437676966190338
epoch£º192	 i:7 	 global-step:3847	 l-p:0.1546536535024643
epoch£º192	 i:8 	 global-step:3848	 l-p:0.12705962359905243
epoch£º192	 i:9 	 global-step:3849	 l-p:0.1314035952091217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9437, 5.7082, 6.0357],
        [4.9437, 4.9573, 4.9456],
        [4.9437, 4.9453, 4.9438],
        [4.9437, 4.9438, 4.9437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:193, step:0 
model_pd.l_p.mean(): 0.13592183589935303 
model_pd.l_d.mean(): -20.381765365600586 
model_pd.lagr.mean(): -20.2458438873291 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4425], device='cuda:0')), ('power', tensor([-21.2212], device='cuda:0'))])
epoch£º193	 i:0 	 global-step:3860	 l-p:0.13592183589935303
epoch£º193	 i:1 	 global-step:3861	 l-p:0.13772821426391602
epoch£º193	 i:2 	 global-step:3862	 l-p:0.06047813221812248
epoch£º193	 i:3 	 global-step:3863	 l-p:-0.06724383682012558
epoch£º193	 i:4 	 global-step:3864	 l-p:0.1329178661108017
epoch£º193	 i:5 	 global-step:3865	 l-p:0.1468178778886795
epoch£º193	 i:6 	 global-step:3866	 l-p:0.1380910873413086
epoch£º193	 i:7 	 global-step:3867	 l-p:0.10963638871908188
epoch£º193	 i:8 	 global-step:3868	 l-p:0.15081365406513214
epoch£º193	 i:9 	 global-step:3869	 l-p:0.11369306594133377
====================================================================================================
====================================================================================================
====================================================================================================

epoch:194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7170, 4.7170, 4.7170],
        [4.7170, 5.1835, 5.2811],
        [4.7170, 4.8720, 4.8203],
        [4.7170, 4.7395, 4.7206]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:194, step:0 
model_pd.l_p.mean(): 0.0999516099691391 
model_pd.l_d.mean(): -20.75826644897461 
model_pd.lagr.mean(): -20.658315658569336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4698], device='cuda:0')), ('power', tensor([-21.6330], device='cuda:0'))])
epoch£º194	 i:0 	 global-step:3880	 l-p:0.0999516099691391
epoch£º194	 i:1 	 global-step:3881	 l-p:0.14364582300186157
epoch£º194	 i:2 	 global-step:3882	 l-p:-0.0047808075323700905
epoch£º194	 i:3 	 global-step:3883	 l-p:0.13323210179805756
epoch£º194	 i:4 	 global-step:3884	 l-p:0.13552764058113098
epoch£º194	 i:5 	 global-step:3885	 l-p:0.11619985103607178
epoch£º194	 i:6 	 global-step:3886	 l-p:0.12080421298742294
epoch£º194	 i:7 	 global-step:3887	 l-p:0.1400785744190216
epoch£º194	 i:8 	 global-step:3888	 l-p:0.12981092929840088
epoch£º194	 i:9 	 global-step:3889	 l-p:0.44117286801338196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1154, 5.1831, 5.1408],
        [5.1154, 5.6189, 5.7124],
        [5.1154, 5.1177, 5.1156],
        [5.1154, 5.5214, 5.5514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:195, step:0 
model_pd.l_p.mean(): 0.11424749344587326 
model_pd.l_d.mean(): -20.142311096191406 
model_pd.lagr.mean(): -20.02806282043457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4099], device='cuda:0')), ('power', tensor([-20.9435], device='cuda:0'))])
epoch£º195	 i:0 	 global-step:3900	 l-p:0.11424749344587326
epoch£º195	 i:1 	 global-step:3901	 l-p:0.11829651892185211
epoch£º195	 i:2 	 global-step:3902	 l-p:0.11932642757892609
epoch£º195	 i:3 	 global-step:3903	 l-p:0.11979871243238449
epoch£º195	 i:4 	 global-step:3904	 l-p:0.15927466750144958
epoch£º195	 i:5 	 global-step:3905	 l-p:0.1339559257030487
epoch£º195	 i:6 	 global-step:3906	 l-p:0.14193299412727356
epoch£º195	 i:7 	 global-step:3907	 l-p:0.23344217240810394
epoch£º195	 i:8 	 global-step:3908	 l-p:0.11524902284145355
epoch£º195	 i:9 	 global-step:3909	 l-p:0.1244686022400856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7869, 4.7869, 4.7869],
        [4.7869, 4.7907, 4.7870],
        [4.7869, 4.7870, 4.7869],
        [4.7869, 4.7896, 4.7870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:196, step:0 
model_pd.l_p.mean(): 0.13849978148937225 
model_pd.l_d.mean(): -19.46961784362793 
model_pd.lagr.mean(): -19.331117630004883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5764], device='cuda:0')), ('power', tensor([-20.4307], device='cuda:0'))])
epoch£º196	 i:0 	 global-step:3920	 l-p:0.13849978148937225
epoch£º196	 i:1 	 global-step:3921	 l-p:0.10519887506961823
epoch£º196	 i:2 	 global-step:3922	 l-p:0.09513634443283081
epoch£º196	 i:3 	 global-step:3923	 l-p:0.1051972284913063
epoch£º196	 i:4 	 global-step:3924	 l-p:0.13155995309352875
epoch£º196	 i:5 	 global-step:3925	 l-p:0.15588437020778656
epoch£º196	 i:6 	 global-step:3926	 l-p:0.12386155873537064
epoch£º196	 i:7 	 global-step:3927	 l-p:0.12602956593036652
epoch£º196	 i:8 	 global-step:3928	 l-p:0.13520970940589905
epoch£º196	 i:9 	 global-step:3929	 l-p:0.11927799880504608
====================================================================================================
====================================================================================================
====================================================================================================

epoch:197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9529, 4.9532, 4.9530],
        [4.9529, 5.3565, 5.3953],
        [4.9529, 4.9529, 4.9529],
        [4.9529, 4.9653, 4.9544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:197, step:0 
model_pd.l_p.mean(): 0.16880498826503754 
model_pd.l_d.mean(): -18.917903900146484 
model_pd.lagr.mean(): -18.749099731445312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5329], device='cuda:0')), ('power', tensor([-19.8236], device='cuda:0'))])
epoch£º197	 i:0 	 global-step:3940	 l-p:0.16880498826503754
epoch£º197	 i:1 	 global-step:3941	 l-p:0.18441101908683777
epoch£º197	 i:2 	 global-step:3942	 l-p:0.14765822887420654
epoch£º197	 i:3 	 global-step:3943	 l-p:0.11310435831546783
epoch£º197	 i:4 	 global-step:3944	 l-p:-0.009362315759062767
epoch£º197	 i:5 	 global-step:3945	 l-p:0.1344657987356186
epoch£º197	 i:6 	 global-step:3946	 l-p:0.12944522500038147
epoch£º197	 i:7 	 global-step:3947	 l-p:0.1343529373407364
epoch£º197	 i:8 	 global-step:3948	 l-p:0.1348988115787506
epoch£º197	 i:9 	 global-step:3949	 l-p:0.12212884426116943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9477, 4.9657, 4.9504],
        [4.9477, 4.9478, 4.9477],
        [4.9477, 4.9477, 4.9477],
        [4.9477, 4.9484, 4.9477]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:198, step:0 
model_pd.l_p.mean(): 0.12934763729572296 
model_pd.l_d.mean(): -20.370906829833984 
model_pd.lagr.mean(): -20.241559982299805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4349], device='cuda:0')), ('power', tensor([-21.2022], device='cuda:0'))])
epoch£º198	 i:0 	 global-step:3960	 l-p:0.12934763729572296
epoch£º198	 i:1 	 global-step:3961	 l-p:0.1407051682472229
epoch£º198	 i:2 	 global-step:3962	 l-p:0.15250466763973236
epoch£º198	 i:3 	 global-step:3963	 l-p:0.1458158642053604
epoch£º198	 i:4 	 global-step:3964	 l-p:0.1123574897646904
epoch£º198	 i:5 	 global-step:3965	 l-p:0.12883462011814117
epoch£º198	 i:6 	 global-step:3966	 l-p:0.13985596597194672
epoch£º198	 i:7 	 global-step:3967	 l-p:0.23098129034042358
epoch£º198	 i:8 	 global-step:3968	 l-p:0.1335117667913437
epoch£º198	 i:9 	 global-step:3969	 l-p:0.12272382527589798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0250, 5.0257, 5.0250],
        [5.0250, 5.0348, 5.0260],
        [5.0250, 5.0250, 5.0250],
        [5.0250, 5.1373, 5.0831]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:199, step:0 
model_pd.l_p.mean(): 0.1569902002811432 
model_pd.l_d.mean(): -18.896095275878906 
model_pd.lagr.mean(): -18.739105224609375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4858], device='cuda:0')), ('power', tensor([-19.7526], device='cuda:0'))])
epoch£º199	 i:0 	 global-step:3980	 l-p:0.1569902002811432
epoch£º199	 i:1 	 global-step:3981	 l-p:0.1156894639134407
epoch£º199	 i:2 	 global-step:3982	 l-p:0.11794701218605042
epoch£º199	 i:3 	 global-step:3983	 l-p:0.12516191601753235
epoch£º199	 i:4 	 global-step:3984	 l-p:0.13585835695266724
epoch£º199	 i:5 	 global-step:3985	 l-p:0.1476879119873047
epoch£º199	 i:6 	 global-step:3986	 l-p:0.11206036061048508
epoch£º199	 i:7 	 global-step:3987	 l-p:0.11619498580694199
epoch£º199	 i:8 	 global-step:3988	 l-p:0.11312519758939743
epoch£º199	 i:9 	 global-step:3989	 l-p:0.16380390524864197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9871, 4.9871, 4.9871],
        [4.9871, 4.9874, 4.9871],
        [4.9871, 4.9976, 4.9881],
        [4.9871, 5.3113, 5.3047]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:200, step:0 
model_pd.l_p.mean(): 0.14988718926906586 
model_pd.l_d.mean(): -20.375492095947266 
model_pd.lagr.mean(): -20.225605010986328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4263], device='cuda:0')), ('power', tensor([-21.1980], device='cuda:0'))])
epoch£º200	 i:0 	 global-step:4000	 l-p:0.14988718926906586
epoch£º200	 i:1 	 global-step:4001	 l-p:0.14233799278736115
epoch£º200	 i:2 	 global-step:4002	 l-p:0.1280640810728073
epoch£º200	 i:3 	 global-step:4003	 l-p:0.4309188723564148
epoch£º200	 i:4 	 global-step:4004	 l-p:0.13650935888290405
epoch£º200	 i:5 	 global-step:4005	 l-p:-0.01398633886128664
epoch£º200	 i:6 	 global-step:4006	 l-p:0.1748860478401184
epoch£º200	 i:7 	 global-step:4007	 l-p:0.17625346779823303
epoch£º200	 i:8 	 global-step:4008	 l-p:0.12273965775966644
epoch£º200	 i:9 	 global-step:4009	 l-p:0.0201729666441679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8834, 4.8834, 4.8834],
        [4.8834, 5.3383, 5.4161],
        [4.8834, 4.8834, 4.8834],
        [4.8834, 4.9673, 4.9190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:201, step:0 
model_pd.l_p.mean(): -0.9858276844024658 
model_pd.l_d.mean(): -18.40871238708496 
model_pd.lagr.mean(): -19.394540786743164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5712], device='cuda:0')), ('power', tensor([-19.3445], device='cuda:0'))])
epoch£º201	 i:0 	 global-step:4020	 l-p:-0.9858276844024658
epoch£º201	 i:1 	 global-step:4021	 l-p:0.11126645654439926
epoch£º201	 i:2 	 global-step:4022	 l-p:0.09075399488210678
epoch£º201	 i:3 	 global-step:4023	 l-p:0.13343030214309692
epoch£º201	 i:4 	 global-step:4024	 l-p:0.15776051580905914
epoch£º201	 i:5 	 global-step:4025	 l-p:0.13729262351989746
epoch£º201	 i:6 	 global-step:4026	 l-p:0.12528344988822937
epoch£º201	 i:7 	 global-step:4027	 l-p:0.14076241850852966
epoch£º201	 i:8 	 global-step:4028	 l-p:0.1251884549856186
epoch£º201	 i:9 	 global-step:4029	 l-p:0.15375705063343048
====================================================================================================
====================================================================================================
====================================================================================================

epoch:202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9831, 4.9833, 4.9831],
        [4.9831, 4.9832, 4.9831],
        [4.9831, 5.0669, 5.0185],
        [4.9831, 4.9944, 4.9843]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:202, step:0 
model_pd.l_p.mean(): 0.13775669038295746 
model_pd.l_d.mean(): -19.908267974853516 
model_pd.lagr.mean(): -19.770511627197266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4591], device='cuda:0')), ('power', tensor([-20.7561], device='cuda:0'))])
epoch£º202	 i:0 	 global-step:4040	 l-p:0.13775669038295746
epoch£º202	 i:1 	 global-step:4041	 l-p:0.15998774766921997
epoch£º202	 i:2 	 global-step:4042	 l-p:0.13222795724868774
epoch£º202	 i:3 	 global-step:4043	 l-p:0.1518167108297348
epoch£º202	 i:4 	 global-step:4044	 l-p:0.2114991396665573
epoch£º202	 i:5 	 global-step:4045	 l-p:0.1455433964729309
epoch£º202	 i:6 	 global-step:4046	 l-p:0.13166110217571259
epoch£º202	 i:7 	 global-step:4047	 l-p:0.13273827731609344
epoch£º202	 i:8 	 global-step:4048	 l-p:-0.014657297171652317
epoch£º202	 i:9 	 global-step:4049	 l-p:0.1960311084985733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8550, 5.3796, 5.5113],
        [4.8550, 4.9376, 4.8895],
        [4.8550, 4.9052, 4.8693],
        [4.8550, 4.8551, 4.8550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:203, step:0 
model_pd.l_p.mean(): 0.021545181050896645 
model_pd.l_d.mean(): -18.78151512145996 
model_pd.lagr.mean(): -18.75996971130371 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5364], device='cuda:0')), ('power', tensor([-19.6883], device='cuda:0'))])
epoch£º203	 i:0 	 global-step:4060	 l-p:0.021545181050896645
epoch£º203	 i:1 	 global-step:4061	 l-p:0.10288289189338684
epoch£º203	 i:2 	 global-step:4062	 l-p:1.7289249897003174
epoch£º203	 i:3 	 global-step:4063	 l-p:0.13660821318626404
epoch£º203	 i:4 	 global-step:4064	 l-p:0.4736635386943817
epoch£º203	 i:5 	 global-step:4065	 l-p:0.175902858376503
epoch£º203	 i:6 	 global-step:4066	 l-p:0.131706103682518
epoch£º203	 i:7 	 global-step:4067	 l-p:0.13316838443279266
epoch£º203	 i:8 	 global-step:4068	 l-p:0.12500469386577606
epoch£º203	 i:9 	 global-step:4069	 l-p:0.13253925740718842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9900, 5.5022, 5.6135],
        [4.9900, 4.9900, 4.9900],
        [4.9900, 5.4926, 5.5969],
        [4.9900, 5.0964, 5.0428]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:204, step:0 
model_pd.l_p.mean(): 0.12091529369354248 
model_pd.l_d.mean(): -20.203094482421875 
model_pd.lagr.mean(): -20.08218002319336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4536], device='cuda:0')), ('power', tensor([-21.0507], device='cuda:0'))])
epoch£º204	 i:0 	 global-step:4080	 l-p:0.12091529369354248
epoch£º204	 i:1 	 global-step:4081	 l-p:0.1488414853811264
epoch£º204	 i:2 	 global-step:4082	 l-p:0.24176913499832153
epoch£º204	 i:3 	 global-step:4083	 l-p:0.22839097678661346
epoch£º204	 i:4 	 global-step:4084	 l-p:0.14255496859550476
epoch£º204	 i:5 	 global-step:4085	 l-p:0.10153678804636002
epoch£º204	 i:6 	 global-step:4086	 l-p:0.14099431037902832
epoch£º204	 i:7 	 global-step:4087	 l-p:0.14717765152454376
epoch£º204	 i:8 	 global-step:4088	 l-p:0.1424051821231842
epoch£º204	 i:9 	 global-step:4089	 l-p:0.1335442066192627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9506, 5.0149, 4.9729],
        [4.9506, 4.9506, 4.9506],
        [4.9506, 4.9507, 4.9506],
        [4.9506, 4.9506, 4.9506]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:205, step:0 
model_pd.l_p.mean(): 0.19396887719631195 
model_pd.l_d.mean(): -19.536447525024414 
model_pd.lagr.mean(): -19.342477798461914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5061], device='cuda:0')), ('power', tensor([-20.4260], device='cuda:0'))])
epoch£º205	 i:0 	 global-step:4100	 l-p:0.19396887719631195
epoch£º205	 i:1 	 global-step:4101	 l-p:0.13689622282981873
epoch£º205	 i:2 	 global-step:4102	 l-p:0.1255229115486145
epoch£º205	 i:3 	 global-step:4103	 l-p:0.159947007894516
epoch£º205	 i:4 	 global-step:4104	 l-p:0.15700939297676086
epoch£º205	 i:5 	 global-step:4105	 l-p:0.07770281285047531
epoch£º205	 i:6 	 global-step:4106	 l-p:0.14299876987934113
epoch£º205	 i:7 	 global-step:4107	 l-p:0.1709495186805725
epoch£º205	 i:8 	 global-step:4108	 l-p:0.11196829378604889
epoch£º205	 i:9 	 global-step:4109	 l-p:0.1226174533367157
====================================================================================================
====================================================================================================
====================================================================================================

epoch:206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0432, 5.0809, 5.0521],
        [5.0432, 5.9892, 6.4857],
        [5.0432, 5.6081, 5.7556],
        [5.0432, 5.0432, 5.0432]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:206, step:0 
model_pd.l_p.mean(): 0.13776271045207977 
model_pd.l_d.mean(): -20.060779571533203 
model_pd.lagr.mean(): -19.923017501831055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4598], device='cuda:0')), ('power', tensor([-20.9121], device='cuda:0'))])
epoch£º206	 i:0 	 global-step:4120	 l-p:0.13776271045207977
epoch£º206	 i:1 	 global-step:4121	 l-p:0.0873393788933754
epoch£º206	 i:2 	 global-step:4122	 l-p:0.12079707533121109
epoch£º206	 i:3 	 global-step:4123	 l-p:0.14820967614650726
epoch£º206	 i:4 	 global-step:4124	 l-p:0.113999143242836
epoch£º206	 i:5 	 global-step:4125	 l-p:0.16016362607479095
epoch£º206	 i:6 	 global-step:4126	 l-p:0.13503165543079376
epoch£º206	 i:7 	 global-step:4127	 l-p:0.11255916208028793
epoch£º206	 i:8 	 global-step:4128	 l-p:0.16604654490947723
epoch£º206	 i:9 	 global-step:4129	 l-p:0.1284286230802536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9811, 5.0014, 4.9840],
        [4.9811, 4.9811, 4.9811],
        [4.9811, 5.2621, 5.2365],
        [4.9811, 5.1484, 5.0929]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:207, step:0 
model_pd.l_p.mean(): 0.13158653676509857 
model_pd.l_d.mean(): -19.684274673461914 
model_pd.lagr.mean(): -19.552688598632812 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4570], device='cuda:0')), ('power', tensor([-20.5256], device='cuda:0'))])
epoch£º207	 i:0 	 global-step:4140	 l-p:0.13158653676509857
epoch£º207	 i:1 	 global-step:4141	 l-p:0.1257542073726654
epoch£º207	 i:2 	 global-step:4142	 l-p:0.20363658666610718
epoch£º207	 i:3 	 global-step:4143	 l-p:0.16160838305950165
epoch£º207	 i:4 	 global-step:4144	 l-p:0.16552191972732544
epoch£º207	 i:5 	 global-step:4145	 l-p:0.1359628438949585
epoch£º207	 i:6 	 global-step:4146	 l-p:0.12281207740306854
epoch£º207	 i:7 	 global-step:4147	 l-p:0.5196058750152588
epoch£º207	 i:8 	 global-step:4148	 l-p:0.127097487449646
epoch£º207	 i:9 	 global-step:4149	 l-p:0.13449807465076447
====================================================================================================
====================================================================================================
====================================================================================================

epoch:208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9005, 4.9005, 4.9005],
        [4.9005, 5.3822, 5.4785],
        [4.9005, 5.0267, 4.9710],
        [4.9005, 4.9005, 4.9005]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:208, step:0 
model_pd.l_p.mean(): 0.1322222352027893 
model_pd.l_d.mean(): -20.70064926147461 
model_pd.lagr.mean(): -20.56842613220215 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4169], device='cuda:0')), ('power', tensor([-21.5195], device='cuda:0'))])
epoch£º208	 i:0 	 global-step:4160	 l-p:0.1322222352027893
epoch£º208	 i:1 	 global-step:4161	 l-p:-0.35488757491111755
epoch£º208	 i:2 	 global-step:4162	 l-p:0.1247515007853508
epoch£º208	 i:3 	 global-step:4163	 l-p:0.1284700483083725
epoch£º208	 i:4 	 global-step:4164	 l-p:0.13547556102275848
epoch£º208	 i:5 	 global-step:4165	 l-p:0.14205333590507507
epoch£º208	 i:6 	 global-step:4166	 l-p:0.18366456031799316
epoch£º208	 i:7 	 global-step:4167	 l-p:0.0908103808760643
epoch£º208	 i:8 	 global-step:4168	 l-p:0.1512308418750763
epoch£º208	 i:9 	 global-step:4169	 l-p:0.14938384294509888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:209
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228]], device='cuda:0')
 pt:tensor([[4.8919, 5.2278, 5.2320],
        [4.8919, 5.8171, 6.3137],
        [4.8919, 4.9407, 4.9050],
        [4.8919, 5.1402, 5.1054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:209, step:0 
model_pd.l_p.mean(): -0.9837805032730103 
model_pd.l_d.mean(): -20.07986831665039 
model_pd.lagr.mean(): -21.063648223876953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5038], device='cuda:0')), ('power', tensor([-20.9771], device='cuda:0'))])
epoch£º209	 i:0 	 global-step:4180	 l-p:-0.9837805032730103
epoch£º209	 i:1 	 global-step:4181	 l-p:0.13578693568706512
epoch£º209	 i:2 	 global-step:4182	 l-p:0.12789936363697052
epoch£º209	 i:3 	 global-step:4183	 l-p:0.13943618535995483
epoch£º209	 i:4 	 global-step:4184	 l-p:0.1419535130262375
epoch£º209	 i:5 	 global-step:4185	 l-p:0.09926799684762955
epoch£º209	 i:6 	 global-step:4186	 l-p:0.13355806469917297
epoch£º209	 i:7 	 global-step:4187	 l-p:0.189223051071167
epoch£º209	 i:8 	 global-step:4188	 l-p:0.15185979008674622
epoch£º209	 i:9 	 global-step:4189	 l-p:0.12344800680875778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0191, 5.0191, 5.0191],
        [5.0191, 5.0191, 5.0191],
        [5.0191, 5.1716, 5.1144],
        [5.0191, 5.0191, 5.0191]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:210, step:0 
model_pd.l_p.mean(): 0.16603176295757294 
model_pd.l_d.mean(): -20.458454132080078 
model_pd.lagr.mean(): -20.292423248291016 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4148], device='cuda:0')), ('power', tensor([-21.2706], device='cuda:0'))])
epoch£º210	 i:0 	 global-step:4200	 l-p:0.16603176295757294
epoch£º210	 i:1 	 global-step:4201	 l-p:0.1376427412033081
epoch£º210	 i:2 	 global-step:4202	 l-p:0.14752277731895447
epoch£º210	 i:3 	 global-step:4203	 l-p:0.12527461349964142
epoch£º210	 i:4 	 global-step:4204	 l-p:0.1359875202178955
epoch£º210	 i:5 	 global-step:4205	 l-p:0.12316936999559402
epoch£º210	 i:6 	 global-step:4206	 l-p:0.118776336312294
epoch£º210	 i:7 	 global-step:4207	 l-p:0.12411538511514664
epoch£º210	 i:8 	 global-step:4208	 l-p:-0.011966914869844913
epoch£º210	 i:9 	 global-step:4209	 l-p:0.12790897488594055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0469, 5.4434, 5.4736],
        [5.0469, 5.0469, 5.0469],
        [5.0469, 5.1838, 5.1263],
        [5.0469, 5.0469, 5.0469]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:211, step:0 
model_pd.l_p.mean(): 0.14094041287899017 
model_pd.l_d.mean(): -19.413381576538086 
model_pd.lagr.mean(): -19.272441864013672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4525], device='cuda:0')), ('power', tensor([-20.2450], device='cuda:0'))])
epoch£º211	 i:0 	 global-step:4220	 l-p:0.14094041287899017
epoch£º211	 i:1 	 global-step:4221	 l-p:0.12847310304641724
epoch£º211	 i:2 	 global-step:4222	 l-p:0.1592632383108139
epoch£º211	 i:3 	 global-step:4223	 l-p:0.18128199875354767
epoch£º211	 i:4 	 global-step:4224	 l-p:0.1260073184967041
epoch£º211	 i:5 	 global-step:4225	 l-p:0.16474148631095886
epoch£º211	 i:6 	 global-step:4226	 l-p:0.1153390035033226
epoch£º211	 i:7 	 global-step:4227	 l-p:0.11931677907705307
epoch£º211	 i:8 	 global-step:4228	 l-p:0.12532998621463776
epoch£º211	 i:9 	 global-step:4229	 l-p:0.13401900231838226
====================================================================================================
====================================================================================================
====================================================================================================

epoch:212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9597, 5.4234, 5.5029],
        [4.9597, 4.9598, 4.9597],
        [4.9597, 4.9597, 4.9597],
        [4.9597, 5.3931, 5.4521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:212, step:0 
model_pd.l_p.mean(): 0.1386944055557251 
model_pd.l_d.mean(): -20.78415298461914 
model_pd.lagr.mean(): -20.645458221435547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3910], device='cuda:0')), ('power', tensor([-21.5778], device='cuda:0'))])
epoch£º212	 i:0 	 global-step:4240	 l-p:0.1386944055557251
epoch£º212	 i:1 	 global-step:4241	 l-p:0.119005486369133
epoch£º212	 i:2 	 global-step:4242	 l-p:0.14550156891345978
epoch£º212	 i:3 	 global-step:4243	 l-p:0.12124147266149521
epoch£º212	 i:4 	 global-step:4244	 l-p:0.3406146466732025
epoch£º212	 i:5 	 global-step:4245	 l-p:0.15192802250385284
epoch£º212	 i:6 	 global-step:4246	 l-p:0.12060630321502686
epoch£º212	 i:7 	 global-step:4247	 l-p:0.20488622784614563
epoch£º212	 i:8 	 global-step:4248	 l-p:-0.23743514716625214
epoch£º212	 i:9 	 global-step:4249	 l-p:0.10968665778636932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7807, 4.7810, 4.7807],
        [4.7807, 4.7807, 4.7807],
        [4.7807, 4.9228, 4.8675],
        [4.7807, 4.7867, 4.7805]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:213, step:0 
model_pd.l_p.mean(): 0.12226759642362595 
model_pd.l_d.mean(): -19.372297286987305 
model_pd.lagr.mean(): -19.250030517578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5782], device='cuda:0')), ('power', tensor([-20.3334], device='cuda:0'))])
epoch£º213	 i:0 	 global-step:4260	 l-p:0.12226759642362595
epoch£º213	 i:1 	 global-step:4261	 l-p:0.13224664330482483
epoch£º213	 i:2 	 global-step:4262	 l-p:0.13859178125858307
epoch£º213	 i:3 	 global-step:4263	 l-p:0.13350999355316162
epoch£º213	 i:4 	 global-step:4264	 l-p:0.0005796837504021823
epoch£º213	 i:5 	 global-step:4265	 l-p:0.14267617464065552
epoch£º213	 i:6 	 global-step:4266	 l-p:0.1333802491426468
epoch£º213	 i:7 	 global-step:4267	 l-p:0.1187785342335701
epoch£º213	 i:8 	 global-step:4268	 l-p:0.34363478422164917
epoch£º213	 i:9 	 global-step:4269	 l-p:0.07510560005903244
====================================================================================================
====================================================================================================
====================================================================================================

epoch:214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8467, 4.8467, 4.8467],
        [4.8467, 4.8712, 4.8496],
        [4.8467, 4.9866, 4.9305],
        [4.8467, 4.8467, 4.8467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:214, step:0 
model_pd.l_p.mean(): 0.1387224942445755 
model_pd.l_d.mean(): -20.443603515625 
model_pd.lagr.mean(): -20.304880142211914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4627], device='cuda:0')), ('power', tensor([-21.3051], device='cuda:0'))])
epoch£º214	 i:0 	 global-step:4280	 l-p:0.1387224942445755
epoch£º214	 i:1 	 global-step:4281	 l-p:0.12213170528411865
epoch£º214	 i:2 	 global-step:4282	 l-p:0.0818047896027565
epoch£º214	 i:3 	 global-step:4283	 l-p:0.17715038359165192
epoch£º214	 i:4 	 global-step:4284	 l-p:0.13592593371868134
epoch£º214	 i:5 	 global-step:4285	 l-p:0.22935493290424347
epoch£º214	 i:6 	 global-step:4286	 l-p:0.16400374472141266
epoch£º214	 i:7 	 global-step:4287	 l-p:0.25516146421432495
epoch£º214	 i:8 	 global-step:4288	 l-p:0.14050206542015076
epoch£º214	 i:9 	 global-step:4289	 l-p:0.16679173707962036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0258, 5.1828, 5.1255],
        [5.0258, 5.0261, 5.0258],
        [5.0258, 5.0373, 5.0267],
        [5.0258, 5.0258, 5.0258]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:215, step:0 
model_pd.l_p.mean(): 0.1518128216266632 
model_pd.l_d.mean(): -20.354408264160156 
model_pd.lagr.mean(): -20.202594757080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4285], device='cuda:0')), ('power', tensor([-21.1789], device='cuda:0'))])
epoch£º215	 i:0 	 global-step:4300	 l-p:0.1518128216266632
epoch£º215	 i:1 	 global-step:4301	 l-p:0.12910638749599457
epoch£º215	 i:2 	 global-step:4302	 l-p:0.14362330734729767
epoch£º215	 i:3 	 global-step:4303	 l-p:0.11831288039684296
epoch£º215	 i:4 	 global-step:4304	 l-p:-0.1788433939218521
epoch£º215	 i:5 	 global-step:4305	 l-p:0.12627165019512177
epoch£º215	 i:6 	 global-step:4306	 l-p:0.1432855725288391
epoch£º215	 i:7 	 global-step:4307	 l-p:0.12001361697912216
epoch£º215	 i:8 	 global-step:4308	 l-p:0.1303928643465042
epoch£º215	 i:9 	 global-step:4309	 l-p:0.11262427270412445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0549, 5.0555, 5.0549],
        [5.0549, 5.3979, 5.3981],
        [5.0549, 5.0698, 5.0564],
        [5.0549, 5.0549, 5.0549]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:216, step:0 
model_pd.l_p.mean(): 0.11976367980241776 
model_pd.l_d.mean(): -18.969907760620117 
model_pd.lagr.mean(): -18.850143432617188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4678], device='cuda:0')), ('power', tensor([-19.8092], device='cuda:0'))])
epoch£º216	 i:0 	 global-step:4320	 l-p:0.11976367980241776
epoch£º216	 i:1 	 global-step:4321	 l-p:0.11983167380094528
epoch£º216	 i:2 	 global-step:4322	 l-p:0.16741637885570526
epoch£º216	 i:3 	 global-step:4323	 l-p:0.15326595306396484
epoch£º216	 i:4 	 global-step:4324	 l-p:0.1632746160030365
epoch£º216	 i:5 	 global-step:4325	 l-p:0.20901109278202057
epoch£º216	 i:6 	 global-step:4326	 l-p:0.09202715009450912
epoch£º216	 i:7 	 global-step:4327	 l-p:0.12789016962051392
epoch£º216	 i:8 	 global-step:4328	 l-p:0.12441276013851166
epoch£º216	 i:9 	 global-step:4329	 l-p:0.16318200528621674
====================================================================================================
====================================================================================================
====================================================================================================

epoch:217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9173, 4.9246, 4.9174],
        [4.9173, 5.3451, 5.4035],
        [4.9173, 5.0955, 5.0417],
        [4.9173, 4.9173, 4.9173]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:217, step:0 
model_pd.l_p.mean(): 0.13660994172096252 
model_pd.l_d.mean(): -20.47676658630371 
model_pd.lagr.mean(): -20.34015655517578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4318], device='cuda:0')), ('power', tensor([-21.3069], device='cuda:0'))])
epoch£º217	 i:0 	 global-step:4340	 l-p:0.13660994172096252
epoch£º217	 i:1 	 global-step:4341	 l-p:0.11049889028072357
epoch£º217	 i:2 	 global-step:4342	 l-p:-0.18454501032829285
epoch£º217	 i:3 	 global-step:4343	 l-p:0.1905471533536911
epoch£º217	 i:4 	 global-step:4344	 l-p:0.18322798609733582
epoch£º217	 i:5 	 global-step:4345	 l-p:0.13209019601345062
epoch£º217	 i:6 	 global-step:4346	 l-p:0.1350736767053604
epoch£º217	 i:7 	 global-step:4347	 l-p:0.4341389238834381
epoch£º217	 i:8 	 global-step:4348	 l-p:0.2192937731742859
epoch£º217	 i:9 	 global-step:4349	 l-p:0.13794833421707153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9782, 4.9803, 4.9782],
        [4.9782, 4.9782, 4.9782],
        [4.9782, 4.9782, 4.9782],
        [4.9782, 4.9782, 4.9782]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:218, step:0 
model_pd.l_p.mean(): 0.12182410806417465 
model_pd.l_d.mean(): -20.492820739746094 
model_pd.lagr.mean(): -20.370996475219727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4170], device='cuda:0')), ('power', tensor([-21.3079], device='cuda:0'))])
epoch£º218	 i:0 	 global-step:4360	 l-p:0.12182410806417465
epoch£º218	 i:1 	 global-step:4361	 l-p:0.13193824887275696
epoch£º218	 i:2 	 global-step:4362	 l-p:0.13290487229824066
epoch£º218	 i:3 	 global-step:4363	 l-p:0.16812250018119812
epoch£º218	 i:4 	 global-step:4364	 l-p:0.12555570900440216
epoch£º218	 i:5 	 global-step:4365	 l-p:0.2315170168876648
epoch£º218	 i:6 	 global-step:4366	 l-p:0.9146464467048645
epoch£º218	 i:7 	 global-step:4367	 l-p:0.12666252255439758
epoch£º218	 i:8 	 global-step:4368	 l-p:0.2322021871805191
epoch£º218	 i:9 	 global-step:4369	 l-p:0.09688500314950943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9817, 4.9853, 4.9817],
        [4.9817, 4.9905, 4.9820],
        [4.9817, 5.1323, 5.0748],
        [4.9817, 5.9866, 6.5545]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:219, step:0 
model_pd.l_p.mean(): 0.12192565947771072 
model_pd.l_d.mean(): -18.708419799804688 
model_pd.lagr.mean(): -18.58649444580078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5135], device='cuda:0')), ('power', tensor([-19.5901], device='cuda:0'))])
epoch£º219	 i:0 	 global-step:4380	 l-p:0.12192565947771072
epoch£º219	 i:1 	 global-step:4381	 l-p:0.08200865238904953
epoch£º219	 i:2 	 global-step:4382	 l-p:0.14534589648246765
epoch£º219	 i:3 	 global-step:4383	 l-p:0.11945842206478119
epoch£º219	 i:4 	 global-step:4384	 l-p:0.17927926778793335
epoch£º219	 i:5 	 global-step:4385	 l-p:0.1197492703795433
epoch£º219	 i:6 	 global-step:4386	 l-p:0.1549672931432724
epoch£º219	 i:7 	 global-step:4387	 l-p:0.13275285065174103
epoch£º219	 i:8 	 global-step:4388	 l-p:0.12108586728572845
epoch£º219	 i:9 	 global-step:4389	 l-p:0.1339167058467865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0007, 5.0009, 5.0007],
        [5.0007, 5.0585, 5.0179],
        [5.0007, 5.0007, 5.0007],
        [5.0007, 5.0110, 5.0012]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:220, step:0 
model_pd.l_p.mean(): 0.16246694326400757 
model_pd.l_d.mean(): -20.200122833251953 
model_pd.lagr.mean(): -20.037656784057617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4368], device='cuda:0')), ('power', tensor([-21.0303], device='cuda:0'))])
epoch£º220	 i:0 	 global-step:4400	 l-p:0.16246694326400757
epoch£º220	 i:1 	 global-step:4401	 l-p:0.13068550825119019
epoch£º220	 i:2 	 global-step:4402	 l-p:0.10809749364852905
epoch£º220	 i:3 	 global-step:4403	 l-p:0.13767531514167786
epoch£º220	 i:4 	 global-step:4404	 l-p:0.1456824094057083
epoch£º220	 i:5 	 global-step:4405	 l-p:-6.090899467468262
epoch£º220	 i:6 	 global-step:4406	 l-p:0.47641634941101074
epoch£º220	 i:7 	 global-step:4407	 l-p:0.13535623252391815
epoch£º220	 i:8 	 global-step:4408	 l-p:0.1457761973142624
epoch£º220	 i:9 	 global-step:4409	 l-p:0.15293624997138977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8793, 5.3158, 5.3830],
        [4.8793, 5.1483, 5.1217],
        [4.8793, 4.9792, 4.9252],
        [4.8793, 4.8793, 4.8793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:221, step:0 
model_pd.l_p.mean(): 0.14665137231349945 
model_pd.l_d.mean(): -20.01337242126465 
model_pd.lagr.mean(): -19.86672019958496 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4913], device='cuda:0')), ('power', tensor([-20.8964], device='cuda:0'))])
epoch£º221	 i:0 	 global-step:4420	 l-p:0.14665137231349945
epoch£º221	 i:1 	 global-step:4421	 l-p:0.015614161267876625
epoch£º221	 i:2 	 global-step:4422	 l-p:0.13350799679756165
epoch£º221	 i:3 	 global-step:4423	 l-p:0.14632593095302582
epoch£º221	 i:4 	 global-step:4424	 l-p:0.04207761585712433
epoch£º221	 i:5 	 global-step:4425	 l-p:2.4954824447631836
epoch£º221	 i:6 	 global-step:4426	 l-p:0.12656347453594208
epoch£º221	 i:7 	 global-step:4427	 l-p:0.13820384442806244
epoch£º221	 i:8 	 global-step:4428	 l-p:0.18023736774921417
epoch£º221	 i:9 	 global-step:4429	 l-p:0.13509730994701385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:222
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6146,  0.5225,  1.0000,  0.4442,
          1.0000,  0.8502, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228]], device='cuda:0')
 pt:tensor([[4.8696, 5.8419, 6.3922],
        [4.8696, 5.3769, 5.4961],
        [4.8696, 5.7985, 6.3034],
        [4.8696, 5.3348, 5.4229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:222, step:0 
model_pd.l_p.mean(): 0.12126180529594421 
model_pd.l_d.mean(): -19.447128295898438 
model_pd.lagr.mean(): -19.32586669921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5658], device='cuda:0')), ('power', tensor([-20.3968], device='cuda:0'))])
epoch£º222	 i:0 	 global-step:4440	 l-p:0.12126180529594421
epoch£º222	 i:1 	 global-step:4441	 l-p:0.12235326319932938
epoch£º222	 i:2 	 global-step:4442	 l-p:0.1524723470211029
epoch£º222	 i:3 	 global-step:4443	 l-p:0.15465499460697174
epoch£º222	 i:4 	 global-step:4444	 l-p:0.13278482854366302
epoch£º222	 i:5 	 global-step:4445	 l-p:0.1385754495859146
epoch£º222	 i:6 	 global-step:4446	 l-p:0.35969945788383484
epoch£º222	 i:7 	 global-step:4447	 l-p:0.11278651654720306
epoch£º222	 i:8 	 global-step:4448	 l-p:0.08983060717582703
epoch£º222	 i:9 	 global-step:4449	 l-p:-0.030636481940746307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7686, 4.7686, 4.7686],
        [4.7686, 4.8641, 4.8104],
        [4.7686, 4.9429, 4.8904],
        [4.7686, 4.7748, 4.7678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:223, step:0 
model_pd.l_p.mean(): -0.06916915625333786 
model_pd.l_d.mean(): -20.595170974731445 
model_pd.lagr.mean(): -20.66434097290039 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4811], device='cuda:0')), ('power', tensor([-21.4786], device='cuda:0'))])
epoch£º223	 i:0 	 global-step:4460	 l-p:-0.06916915625333786
epoch£º223	 i:1 	 global-step:4461	 l-p:0.14684468507766724
epoch£º223	 i:2 	 global-step:4462	 l-p:0.1378631442785263
epoch£º223	 i:3 	 global-step:4463	 l-p:0.12719303369522095
epoch£º223	 i:4 	 global-step:4464	 l-p:0.12468890845775604
epoch£º223	 i:5 	 global-step:4465	 l-p:0.27733314037323
epoch£º223	 i:6 	 global-step:4466	 l-p:-0.13099712133407593
epoch£º223	 i:7 	 global-step:4467	 l-p:0.721868097782135
epoch£º223	 i:8 	 global-step:4468	 l-p:0.13721294701099396
epoch£º223	 i:9 	 global-step:4469	 l-p:0.18058232963085175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0230, 5.0230, 5.0230],
        [5.0230, 5.0230, 5.0230],
        [5.0230, 5.0509, 5.0270],
        [5.0230, 5.0235, 5.0230]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:224, step:0 
model_pd.l_p.mean(): 0.12277078628540039 
model_pd.l_d.mean(): -20.415964126586914 
model_pd.lagr.mean(): -20.293193817138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4029], device='cuda:0')), ('power', tensor([-21.2151], device='cuda:0'))])
epoch£º224	 i:0 	 global-step:4480	 l-p:0.12277078628540039
epoch£º224	 i:1 	 global-step:4481	 l-p:0.1241832971572876
epoch£º224	 i:2 	 global-step:4482	 l-p:0.13500718772411346
epoch£º224	 i:3 	 global-step:4483	 l-p:0.14859895408153534
epoch£º224	 i:4 	 global-step:4484	 l-p:0.14246763288974762
epoch£º224	 i:5 	 global-step:4485	 l-p:0.075435571372509
epoch£º224	 i:6 	 global-step:4486	 l-p:0.1539948582649231
epoch£º224	 i:7 	 global-step:4487	 l-p:0.15223346650600433
epoch£º224	 i:8 	 global-step:4488	 l-p:0.10779225826263428
epoch£º224	 i:9 	 global-step:4489	 l-p:0.12079005688428879
====================================================================================================
====================================================================================================
====================================================================================================

epoch:225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1076, 5.1076, 5.1076],
        [5.1076, 5.1076, 5.1076],
        [5.1076, 5.1076, 5.1076],
        [5.1076, 5.2423, 5.1833]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:225, step:0 
model_pd.l_p.mean(): 0.11575587093830109 
model_pd.l_d.mean(): -19.58022117614746 
model_pd.lagr.mean(): -19.464466094970703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4345], device='cuda:0')), ('power', tensor([-20.3963], device='cuda:0'))])
epoch£º225	 i:0 	 global-step:4500	 l-p:0.11575587093830109
epoch£º225	 i:1 	 global-step:4501	 l-p:-0.1134047657251358
epoch£º225	 i:2 	 global-step:4502	 l-p:0.1222788617014885
epoch£º225	 i:3 	 global-step:4503	 l-p:0.1656169444322586
epoch£º225	 i:4 	 global-step:4504	 l-p:0.11915642768144608
epoch£º225	 i:5 	 global-step:4505	 l-p:0.1688898801803589
epoch£º225	 i:6 	 global-step:4506	 l-p:0.11876390874385834
epoch£º225	 i:7 	 global-step:4507	 l-p:0.1257798671722412
epoch£º225	 i:8 	 global-step:4508	 l-p:0.16905781626701355
epoch£º225	 i:9 	 global-step:4509	 l-p:0.12851600348949432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9584, 5.1114, 5.0537],
        [4.9584, 5.4793, 5.6018],
        [4.9584, 5.0192, 4.9763],
        [4.9584, 4.9584, 4.9584]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:226, step:0 
model_pd.l_p.mean(): 0.12589336931705475 
model_pd.l_d.mean(): -20.438112258911133 
model_pd.lagr.mean(): -20.312219619750977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4337], device='cuda:0')), ('power', tensor([-21.2695], device='cuda:0'))])
epoch£º226	 i:0 	 global-step:4520	 l-p:0.12589336931705475
epoch£º226	 i:1 	 global-step:4521	 l-p:0.2697921395301819
epoch£º226	 i:2 	 global-step:4522	 l-p:0.0975261777639389
epoch£º226	 i:3 	 global-step:4523	 l-p:-10.929638862609863
epoch£º226	 i:4 	 global-step:4524	 l-p:0.1513516902923584
epoch£º226	 i:5 	 global-step:4525	 l-p:0.1972501277923584
epoch£º226	 i:6 	 global-step:4526	 l-p:0.1266242414712906
epoch£º226	 i:7 	 global-step:4527	 l-p:0.11506219953298569
epoch£º226	 i:8 	 global-step:4528	 l-p:0.14475314319133759
epoch£º226	 i:9 	 global-step:4529	 l-p:0.24280522763729095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9684, 5.0780, 5.0216],
        [4.9684, 4.9691, 4.9684],
        [4.9684, 4.9880, 4.9697],
        [4.9684, 4.9684, 4.9684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:227, step:0 
model_pd.l_p.mean(): 0.1499708741903305 
model_pd.l_d.mean(): -19.84522247314453 
model_pd.lagr.mean(): -19.69525146484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4615], device='cuda:0')), ('power', tensor([-20.6943], device='cuda:0'))])
epoch£º227	 i:0 	 global-step:4540	 l-p:0.1499708741903305
epoch£º227	 i:1 	 global-step:4541	 l-p:0.12103483080863953
epoch£º227	 i:2 	 global-step:4542	 l-p:0.13114745914936066
epoch£º227	 i:3 	 global-step:4543	 l-p:0.17050385475158691
epoch£º227	 i:4 	 global-step:4544	 l-p:0.33048015832901
epoch£º227	 i:5 	 global-step:4545	 l-p:0.23890987038612366
epoch£º227	 i:6 	 global-step:4546	 l-p:0.1278168261051178
epoch£º227	 i:7 	 global-step:4547	 l-p:0.12480191141366959
epoch£º227	 i:8 	 global-step:4548	 l-p:0.12754344940185547
epoch£º227	 i:9 	 global-step:4549	 l-p:0.17714624106884003
====================================================================================================
====================================================================================================
====================================================================================================

epoch:228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0147, 5.0147, 5.0147],
        [5.0147, 5.0147, 5.0147],
        [5.0147, 5.0147, 5.0147],
        [5.0147, 5.0424, 5.0182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:228, step:0 
model_pd.l_p.mean(): 0.1322605162858963 
model_pd.l_d.mean(): -20.363235473632812 
model_pd.lagr.mean(): -20.230974197387695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4296], device='cuda:0')), ('power', tensor([-21.1889], device='cuda:0'))])
epoch£º228	 i:0 	 global-step:4560	 l-p:0.1322605162858963
epoch£º228	 i:1 	 global-step:4561	 l-p:0.1292250007390976
epoch£º228	 i:2 	 global-step:4562	 l-p:0.13870921730995178
epoch£º228	 i:3 	 global-step:4563	 l-p:0.12967966496944427
epoch£º228	 i:4 	 global-step:4564	 l-p:0.09451552480459213
epoch£º228	 i:5 	 global-step:4565	 l-p:0.19431620836257935
epoch£º228	 i:6 	 global-step:4566	 l-p:0.13404481112957
epoch£º228	 i:7 	 global-step:4567	 l-p:0.14173421263694763
epoch£º228	 i:8 	 global-step:4568	 l-p:0.17163264751434326
epoch£º228	 i:9 	 global-step:4569	 l-p:0.1745677888393402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9871, 4.9871, 4.9871],
        [4.9871, 5.5401, 5.6853],
        [4.9871, 4.9989, 4.9872],
        [4.9871, 4.9944, 4.9869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:229, step:0 
model_pd.l_p.mean(): 0.136483296751976 
model_pd.l_d.mean(): -18.49681854248047 
model_pd.lagr.mean(): -18.360334396362305 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5421], device='cuda:0')), ('power', tensor([-19.4042], device='cuda:0'))])
epoch£º229	 i:0 	 global-step:4580	 l-p:0.136483296751976
epoch£º229	 i:1 	 global-step:4581	 l-p:0.13926173746585846
epoch£º229	 i:2 	 global-step:4582	 l-p:0.15683582425117493
epoch£º229	 i:3 	 global-step:4583	 l-p:0.16954493522644043
epoch£º229	 i:4 	 global-step:4584	 l-p:0.11708435416221619
epoch£º229	 i:5 	 global-step:4585	 l-p:0.13284888863563538
epoch£º229	 i:6 	 global-step:4586	 l-p:-0.1780916005373001
epoch£º229	 i:7 	 global-step:4587	 l-p:0.12126760929822922
epoch£º229	 i:8 	 global-step:4588	 l-p:0.11190208792686462
epoch£º229	 i:9 	 global-step:4589	 l-p:0.12726959586143494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:230
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1466,  0.0773,  1.0000,  0.0408,
          1.0000,  0.5273, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2503,  1.0000,  0.1770,
          1.0000,  0.7073, 31.6228]], device='cuda:0')
 pt:tensor([[5.0990, 5.1191, 5.1009],
        [5.0990, 6.0789, 6.6050],
        [5.0990, 5.1274, 5.1030],
        [5.0990, 5.3012, 5.2474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:230, step:0 
model_pd.l_p.mean(): 0.10717368870973587 
model_pd.l_d.mean(): -18.889888763427734 
model_pd.lagr.mean(): -18.78271484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4891], device='cuda:0')), ('power', tensor([-19.7497], device='cuda:0'))])
epoch£º230	 i:0 	 global-step:4600	 l-p:0.10717368870973587
epoch£º230	 i:1 	 global-step:4601	 l-p:0.131605863571167
epoch£º230	 i:2 	 global-step:4602	 l-p:0.12163162231445312
epoch£º230	 i:3 	 global-step:4603	 l-p:0.1526995599269867
epoch£º230	 i:4 	 global-step:4604	 l-p:0.16783186793327332
epoch£º230	 i:5 	 global-step:4605	 l-p:0.1302679032087326
epoch£º230	 i:6 	 global-step:4606	 l-p:0.23136331140995026
epoch£º230	 i:7 	 global-step:4607	 l-p:0.15215745568275452
epoch£º230	 i:8 	 global-step:4608	 l-p:0.1409815549850464
epoch£º230	 i:9 	 global-step:4609	 l-p:-0.0761353075504303
====================================================================================================
====================================================================================================
====================================================================================================

epoch:231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8842, 4.8842, 4.8842],
        [4.8842, 4.8841, 4.8841],
        [4.8842, 5.7575, 6.2046],
        [4.8842, 4.9289, 4.8925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:231, step:0 
model_pd.l_p.mean(): 0.12919704616069794 
model_pd.l_d.mean(): -19.981151580810547 
model_pd.lagr.mean(): -19.85195541381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5091], device='cuda:0')), ('power', tensor([-20.8821], device='cuda:0'))])
epoch£º231	 i:0 	 global-step:4620	 l-p:0.12919704616069794
epoch£º231	 i:1 	 global-step:4621	 l-p:0.14285893738269806
epoch£º231	 i:2 	 global-step:4622	 l-p:0.1256943792104721
epoch£º231	 i:3 	 global-step:4623	 l-p:0.1345565766096115
epoch£º231	 i:4 	 global-step:4624	 l-p:0.1384294331073761
epoch£º231	 i:5 	 global-step:4625	 l-p:0.30796733498573303
epoch£º231	 i:6 	 global-step:4626	 l-p:0.1367381066083908
epoch£º231	 i:7 	 global-step:4627	 l-p:0.0703011080622673
epoch£º231	 i:8 	 global-step:4628	 l-p:0.049934763461351395
epoch£º231	 i:9 	 global-step:4629	 l-p:0.1695377379655838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7808, 4.7807, 4.7808],
        [4.7808, 4.8008, 4.7800],
        [4.7808, 5.1412, 5.1661],
        [4.7808, 4.7810, 4.7805]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:232, step:0 
model_pd.l_p.mean(): 0.1250278800725937 
model_pd.l_d.mean(): -20.516738891601562 
model_pd.lagr.mean(): -20.39171028137207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4872], device='cuda:0')), ('power', tensor([-21.4050], device='cuda:0'))])
epoch£º232	 i:0 	 global-step:4640	 l-p:0.1250278800725937
epoch£º232	 i:1 	 global-step:4641	 l-p:0.18829791247844696
epoch£º232	 i:2 	 global-step:4642	 l-p:0.05244120582938194
epoch£º232	 i:3 	 global-step:4643	 l-p:0.1401890516281128
epoch£º232	 i:4 	 global-step:4644	 l-p:0.13818886876106262
epoch£º232	 i:5 	 global-step:4645	 l-p:0.13287095725536346
epoch£º232	 i:6 	 global-step:4646	 l-p:0.14630503952503204
epoch£º232	 i:7 	 global-step:4647	 l-p:0.12307924032211304
epoch£º232	 i:8 	 global-step:4648	 l-p:0.11851529031991959
epoch£º232	 i:9 	 global-step:4649	 l-p:0.12075614929199219
====================================================================================================
====================================================================================================
====================================================================================================

epoch:233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0691, 5.1096, 5.0769],
        [5.0691, 5.0861, 5.0700],
        [5.0691, 5.3699, 5.3496],
        [5.0691, 5.8996, 6.2809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:233, step:0 
model_pd.l_p.mean(): 0.06831742823123932 
model_pd.l_d.mean(): -19.83868408203125 
model_pd.lagr.mean(): -19.770366668701172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4739], device='cuda:0')), ('power', tensor([-20.7004], device='cuda:0'))])
epoch£º233	 i:0 	 global-step:4660	 l-p:0.06831742823123932
epoch£º233	 i:1 	 global-step:4661	 l-p:0.13592346012592316
epoch£º233	 i:2 	 global-step:4662	 l-p:0.10976757854223251
epoch£º233	 i:3 	 global-step:4663	 l-p:0.13210679590702057
epoch£º233	 i:4 	 global-step:4664	 l-p:0.13628359138965607
epoch£º233	 i:5 	 global-step:4665	 l-p:0.15315242111682892
epoch£º233	 i:6 	 global-step:4666	 l-p:0.14580991864204407
epoch£º233	 i:7 	 global-step:4667	 l-p:0.1349681317806244
epoch£º233	 i:8 	 global-step:4668	 l-p:0.12040000408887863
epoch£º233	 i:9 	 global-step:4669	 l-p:0.12992142140865326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9686, 4.9701, 4.9684],
        [4.9686, 4.9755, 4.9681],
        [4.9686, 4.9803, 4.9682],
        [4.9686, 4.9686, 4.9686]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:234, step:0 
model_pd.l_p.mean(): 0.12340208142995834 
model_pd.l_d.mean(): -19.866798400878906 
model_pd.lagr.mean(): -19.743396759033203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4943], device='cuda:0')), ('power', tensor([-20.7502], device='cuda:0'))])
epoch£º234	 i:0 	 global-step:4680	 l-p:0.12340208142995834
epoch£º234	 i:1 	 global-step:4681	 l-p:3.1421258449554443
epoch£º234	 i:2 	 global-step:4682	 l-p:0.12512440979480743
epoch£º234	 i:3 	 global-step:4683	 l-p:0.12975403666496277
epoch£º234	 i:4 	 global-step:4684	 l-p:0.2113669514656067
epoch£º234	 i:5 	 global-step:4685	 l-p:0.05556115508079529
epoch£º234	 i:6 	 global-step:4686	 l-p:0.13102267682552338
epoch£º234	 i:7 	 global-step:4687	 l-p:0.146542489528656
epoch£º234	 i:8 	 global-step:4688	 l-p:0.14146824181079865
epoch£º234	 i:9 	 global-step:4689	 l-p:0.0928112044930458
====================================================================================================
====================================================================================================
====================================================================================================

epoch:235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7780, 5.4058, 5.6349],
        [4.7780, 4.7780, 4.7780],
        [4.7780, 4.7785, 4.7774],
        [4.7780, 4.7780, 4.7780]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:235, step:0 
model_pd.l_p.mean(): 0.13864168524742126 
model_pd.l_d.mean(): -20.38974952697754 
model_pd.lagr.mean(): -20.251108169555664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4840], device='cuda:0')), ('power', tensor([-21.2724], device='cuda:0'))])
epoch£º235	 i:0 	 global-step:4700	 l-p:0.13864168524742126
epoch£º235	 i:1 	 global-step:4701	 l-p:-0.05535229668021202
epoch£º235	 i:2 	 global-step:4702	 l-p:0.13554708659648895
epoch£º235	 i:3 	 global-step:4703	 l-p:0.06365034729242325
epoch£º235	 i:4 	 global-step:4704	 l-p:0.1650848388671875
epoch£º235	 i:5 	 global-step:4705	 l-p:0.18509772419929504
epoch£º235	 i:6 	 global-step:4706	 l-p:0.13107366859912872
epoch£º235	 i:7 	 global-step:4707	 l-p:0.1407240778207779
epoch£º235	 i:8 	 global-step:4708	 l-p:0.1238802969455719
epoch£º235	 i:9 	 global-step:4709	 l-p:0.12252531945705414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9012, 5.1039, 5.0540],
        [4.9012, 4.9011, 4.9011],
        [4.9012, 5.0309, 4.9719],
        [4.9012, 4.9211, 4.9010]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:236, step:0 
model_pd.l_p.mean(): 0.13083228468894958 
model_pd.l_d.mean(): -20.332725524902344 
model_pd.lagr.mean(): -20.201892852783203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4619], device='cuda:0')), ('power', tensor([-21.1913], device='cuda:0'))])
epoch£º236	 i:0 	 global-step:4720	 l-p:0.13083228468894958
epoch£º236	 i:1 	 global-step:4721	 l-p:0.13487765192985535
epoch£º236	 i:2 	 global-step:4722	 l-p:0.13491114974021912
epoch£º236	 i:3 	 global-step:4723	 l-p:0.13838937878608704
epoch£º236	 i:4 	 global-step:4724	 l-p:0.3495030403137207
epoch£º236	 i:5 	 global-step:4725	 l-p:0.11185768991708755
epoch£º236	 i:6 	 global-step:4726	 l-p:0.14896467328071594
epoch£º236	 i:7 	 global-step:4727	 l-p:0.11129516363143921
epoch£º236	 i:8 	 global-step:4728	 l-p:0.15168730914592743
epoch£º236	 i:9 	 global-step:4729	 l-p:0.12658262252807617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8330, 5.5596, 5.8717],
        [4.8330, 4.9070, 4.8564],
        [4.8330, 4.8330, 4.8330],
        [4.8330, 4.9552, 4.8965]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:237, step:0 
model_pd.l_p.mean(): 0.04777825251221657 
model_pd.l_d.mean(): -19.427013397216797 
model_pd.lagr.mean(): -19.379234313964844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5405], device='cuda:0')), ('power', tensor([-20.3501], device='cuda:0'))])
epoch£º237	 i:0 	 global-step:4740	 l-p:0.04777825251221657
epoch£º237	 i:1 	 global-step:4741	 l-p:0.13201262056827545
epoch£º237	 i:2 	 global-step:4742	 l-p:-0.09444878995418549
epoch£º237	 i:3 	 global-step:4743	 l-p:0.14609088003635406
epoch£º237	 i:4 	 global-step:4744	 l-p:0.12700635194778442
epoch£º237	 i:5 	 global-step:4745	 l-p:0.18383565545082092
epoch£º237	 i:6 	 global-step:4746	 l-p:0.12330915033817291
epoch£º237	 i:7 	 global-step:4747	 l-p:0.14339151978492737
epoch£º237	 i:8 	 global-step:4748	 l-p:0.13045313954353333
epoch£º237	 i:9 	 global-step:4749	 l-p:0.11947289854288101
====================================================================================================
====================================================================================================
====================================================================================================

epoch:238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9950, 5.8815, 6.3275],
        [4.9950, 5.0418, 5.0041],
        [4.9950, 5.5012, 5.6109],
        [4.9950, 4.9950, 4.9950]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:238, step:0 
model_pd.l_p.mean(): 0.1304837018251419 
model_pd.l_d.mean(): -17.157419204711914 
model_pd.lagr.mean(): -17.026935577392578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6305], device='cuda:0')), ('power', tensor([-18.1313], device='cuda:0'))])
epoch£º238	 i:0 	 global-step:4760	 l-p:0.1304837018251419
epoch£º238	 i:1 	 global-step:4761	 l-p:0.1883014291524887
epoch£º238	 i:2 	 global-step:4762	 l-p:0.12629811465740204
epoch£º238	 i:3 	 global-step:4763	 l-p:0.1323828101158142
epoch£º238	 i:4 	 global-step:4764	 l-p:0.1557418256998062
epoch£º238	 i:5 	 global-step:4765	 l-p:0.19195321202278137
epoch£º238	 i:6 	 global-step:4766	 l-p:0.09112654626369476
epoch£º238	 i:7 	 global-step:4767	 l-p:0.1764582246541977
epoch£º238	 i:8 	 global-step:4768	 l-p:0.12379828095436096
epoch£º238	 i:9 	 global-step:4769	 l-p:0.1260833740234375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:239
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[4.9829, 5.4745, 5.5744],
        [4.9829, 5.7775, 6.1355],
        [4.9829, 5.0664, 5.0136],
        [4.9829, 5.3361, 5.3461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:239, step:0 
model_pd.l_p.mean(): 0.15071667730808258 
model_pd.l_d.mean(): -20.183557510375977 
model_pd.lagr.mean(): -20.032840728759766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4660], device='cuda:0')), ('power', tensor([-21.0437], device='cuda:0'))])
epoch£º239	 i:0 	 global-step:4780	 l-p:0.15071667730808258
epoch£º239	 i:1 	 global-step:4781	 l-p:0.10841365903615952
epoch£º239	 i:2 	 global-step:4782	 l-p:0.12734413146972656
epoch£º239	 i:3 	 global-step:4783	 l-p:0.13462530076503754
epoch£º239	 i:4 	 global-step:4784	 l-p:1.2819470167160034
epoch£º239	 i:5 	 global-step:4785	 l-p:0.13059058785438538
epoch£º239	 i:6 	 global-step:4786	 l-p:0.12045146524906158
epoch£º239	 i:7 	 global-step:4787	 l-p:0.1558675318956375
epoch£º239	 i:8 	 global-step:4788	 l-p:0.10753883421421051
epoch£º239	 i:9 	 global-step:4789	 l-p:0.09895891696214676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7921, 5.1077, 5.1066],
        [4.7921, 4.8220, 4.7922],
        [4.7921, 4.8506, 4.8048],
        [4.7921, 4.7919, 4.7921]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:240, step:0 
model_pd.l_p.mean(): -0.9141759276390076 
model_pd.l_d.mean(): -20.546100616455078 
model_pd.lagr.mean(): -21.460275650024414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4647], device='cuda:0')), ('power', tensor([-21.4116], device='cuda:0'))])
epoch£º240	 i:0 	 global-step:4800	 l-p:-0.9141759276390076
epoch£º240	 i:1 	 global-step:4801	 l-p:0.1591087281703949
epoch£º240	 i:2 	 global-step:4802	 l-p:0.13908429443836212
epoch£º240	 i:3 	 global-step:4803	 l-p:0.05397063121199608
epoch£º240	 i:4 	 global-step:4804	 l-p:0.12907205522060394
epoch£º240	 i:5 	 global-step:4805	 l-p:0.12547221779823303
epoch£º240	 i:6 	 global-step:4806	 l-p:0.14421647787094116
epoch£º240	 i:7 	 global-step:4807	 l-p:0.1512552946805954
epoch£º240	 i:8 	 global-step:4808	 l-p:0.19735419750213623
epoch£º240	 i:9 	 global-step:4809	 l-p:0.008638877421617508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9266, 4.9733, 4.9345],
        [4.9266, 4.9266, 4.9266],
        [4.9266, 5.4231, 5.5310],
        [4.9266, 5.6748, 5.9961]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:241, step:0 
model_pd.l_p.mean(): 0.375631719827652 
model_pd.l_d.mean(): -20.287151336669922 
model_pd.lagr.mean(): -19.91152000427246 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4601], device='cuda:0')), ('power', tensor([-21.1430], device='cuda:0'))])
epoch£º241	 i:0 	 global-step:4820	 l-p:0.375631719827652
epoch£º241	 i:1 	 global-step:4821	 l-p:0.399056077003479
epoch£º241	 i:2 	 global-step:4822	 l-p:0.12032578885555267
epoch£º241	 i:3 	 global-step:4823	 l-p:0.1562483012676239
epoch£º241	 i:4 	 global-step:4824	 l-p:0.1343531459569931
epoch£º241	 i:5 	 global-step:4825	 l-p:0.12791287899017334
epoch£º241	 i:6 	 global-step:4826	 l-p:0.11859086900949478
epoch£º241	 i:7 	 global-step:4827	 l-p:0.11963232606649399
epoch£º241	 i:8 	 global-step:4828	 l-p:0.13469372689723969
epoch£º241	 i:9 	 global-step:4829	 l-p:0.07927119731903076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0484, 5.3387, 5.3146],
        [5.0484, 5.0639, 5.0482],
        [5.0484, 5.0484, 5.0484],
        [5.0484, 5.7434, 6.0029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:242, step:0 
model_pd.l_p.mean(): 0.1416442096233368 
model_pd.l_d.mean(): -20.11944580078125 
model_pd.lagr.mean(): -19.977802276611328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4415], device='cuda:0')), ('power', tensor([-20.9530], device='cuda:0'))])
epoch£º242	 i:0 	 global-step:4840	 l-p:0.1416442096233368
epoch£º242	 i:1 	 global-step:4841	 l-p:0.12539716064929962
epoch£º242	 i:2 	 global-step:4842	 l-p:0.16283008456230164
epoch£º242	 i:3 	 global-step:4843	 l-p:0.12737274169921875
epoch£º242	 i:4 	 global-step:4844	 l-p:0.15166321396827698
epoch£º242	 i:5 	 global-step:4845	 l-p:0.1343621015548706
epoch£º242	 i:6 	 global-step:4846	 l-p:0.0982472375035286
epoch£º242	 i:7 	 global-step:4847	 l-p:0.13467730581760406
epoch£º242	 i:8 	 global-step:4848	 l-p:0.12513212859630585
epoch£º242	 i:9 	 global-step:4849	 l-p:0.3323597013950348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9603, 4.9830, 4.9602],
        [4.9603, 4.9602, 4.9603],
        [4.9603, 4.9603, 4.9603],
        [4.9603, 5.5687, 5.7626]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:243, step:0 
model_pd.l_p.mean(): 0.15633299946784973 
model_pd.l_d.mean(): -19.906442642211914 
model_pd.lagr.mean(): -19.75010871887207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4571], device='cuda:0')), ('power', tensor([-20.7521], device='cuda:0'))])
epoch£º243	 i:0 	 global-step:4860	 l-p:0.15633299946784973
epoch£º243	 i:1 	 global-step:4861	 l-p:0.48622408509254456
epoch£º243	 i:2 	 global-step:4862	 l-p:0.1300586760044098
epoch£º243	 i:3 	 global-step:4863	 l-p:0.14039872586727142
epoch£º243	 i:4 	 global-step:4864	 l-p:0.1213257759809494
epoch£º243	 i:5 	 global-step:4865	 l-p:0.11900615692138672
epoch£º243	 i:6 	 global-step:4866	 l-p:0.20017148554325104
epoch£º243	 i:7 	 global-step:4867	 l-p:-0.0692242905497551
epoch£º243	 i:8 	 global-step:4868	 l-p:0.13122811913490295
epoch£º243	 i:9 	 global-step:4869	 l-p:0.13140669465065002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:244
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1771,  0.0994,  1.0000,  0.0558,
          1.0000,  0.5615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[4.8841, 4.8948, 4.8819],
        [4.8841, 5.1590, 5.1342],
        [4.8841, 4.9131, 4.8844],
        [4.8841, 5.4200, 5.5619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:244, step:0 
model_pd.l_p.mean(): 0.052473392337560654 
model_pd.l_d.mean(): -20.33234405517578 
model_pd.lagr.mean(): -20.279870986938477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4760], device='cuda:0')), ('power', tensor([-21.2056], device='cuda:0'))])
epoch£º244	 i:0 	 global-step:4880	 l-p:0.052473392337560654
epoch£º244	 i:1 	 global-step:4881	 l-p:0.1390354335308075
epoch£º244	 i:2 	 global-step:4882	 l-p:0.11365637183189392
epoch£º244	 i:3 	 global-step:4883	 l-p:0.12877343595027924
epoch£º244	 i:4 	 global-step:4884	 l-p:0.12468145787715912
epoch£º244	 i:5 	 global-step:4885	 l-p:-0.15334784984588623
epoch£º244	 i:6 	 global-step:4886	 l-p:0.13442373275756836
epoch£º244	 i:7 	 global-step:4887	 l-p:0.21510669589042664
epoch£º244	 i:8 	 global-step:4888	 l-p:0.07746077328920364
epoch£º244	 i:9 	 global-step:4889	 l-p:0.1561034619808197
====================================================================================================
====================================================================================================
====================================================================================================

epoch:245
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8797, 4.8797, 4.8797],
        [4.8797, 5.0194, 4.9593],
        [4.8797, 5.1603, 5.1382],
        [4.8797, 4.8797, 4.8797]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:245, step:0 
model_pd.l_p.mean(): 0.05860982835292816 
model_pd.l_d.mean(): -19.844511032104492 
model_pd.lagr.mean(): -19.78590202331543 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5247], device='cuda:0')), ('power', tensor([-20.7590], device='cuda:0'))])
epoch£º245	 i:0 	 global-step:4900	 l-p:0.05860982835292816
epoch£º245	 i:1 	 global-step:4901	 l-p:0.1981731802225113
epoch£º245	 i:2 	 global-step:4902	 l-p:1.423161268234253
epoch£º245	 i:3 	 global-step:4903	 l-p:0.14640149474143982
epoch£º245	 i:4 	 global-step:4904	 l-p:0.11519408226013184
epoch£º245	 i:5 	 global-step:4905	 l-p:0.13135413825511932
epoch£º245	 i:6 	 global-step:4906	 l-p:0.07715868949890137
epoch£º245	 i:7 	 global-step:4907	 l-p:0.1661115288734436
epoch£º245	 i:8 	 global-step:4908	 l-p:0.12388254702091217
epoch£º245	 i:9 	 global-step:4909	 l-p:0.12311319261789322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1391, 5.6229, 5.7064],
        [5.1391, 5.1391, 5.1391],
        [5.1391, 5.1887, 5.1495],
        [5.1391, 5.1391, 5.1391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:246, step:0 
model_pd.l_p.mean(): 0.12024644762277603 
model_pd.l_d.mean(): -19.303958892822266 
model_pd.lagr.mean(): -19.183712005615234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4700], device='cuda:0')), ('power', tensor([-20.1518], device='cuda:0'))])
epoch£º246	 i:0 	 global-step:4920	 l-p:0.12024644762277603
epoch£º246	 i:1 	 global-step:4921	 l-p:0.12976133823394775
epoch£º246	 i:2 	 global-step:4922	 l-p:0.12727253139019012
epoch£º246	 i:3 	 global-step:4923	 l-p:0.1370343267917633
epoch£º246	 i:4 	 global-step:4924	 l-p:0.13115964829921722
epoch£º246	 i:5 	 global-step:4925	 l-p:0.12983684241771698
epoch£º246	 i:6 	 global-step:4926	 l-p:0.04100782424211502
epoch£º246	 i:7 	 global-step:4927	 l-p:0.13977572321891785
epoch£º246	 i:8 	 global-step:4928	 l-p:0.15607404708862305
epoch£º246	 i:9 	 global-step:4929	 l-p:0.12098316103219986
====================================================================================================
====================================================================================================
====================================================================================================

epoch:247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0535, 5.0535, 5.0535],
        [5.0535, 5.0602, 5.0524],
        [5.0535, 5.1396, 5.0850],
        [5.0535, 5.0534, 5.0535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:247, step:0 
model_pd.l_p.mean(): 0.07674043625593185 
model_pd.l_d.mean(): -19.796863555908203 
model_pd.lagr.mean(): -19.720123291015625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4406], device='cuda:0')), ('power', tensor([-20.6234], device='cuda:0'))])
epoch£º247	 i:0 	 global-step:4940	 l-p:0.07674043625593185
epoch£º247	 i:1 	 global-step:4941	 l-p:0.13461217284202576
epoch£º247	 i:2 	 global-step:4942	 l-p:0.1677461564540863
epoch£º247	 i:3 	 global-step:4943	 l-p:0.13823236525058746
epoch£º247	 i:4 	 global-step:4944	 l-p:0.14068135619163513
epoch£º247	 i:5 	 global-step:4945	 l-p:0.18613672256469727
epoch£º247	 i:6 	 global-step:4946	 l-p:0.12790639698505402
epoch£º247	 i:7 	 global-step:4947	 l-p:0.1291292905807495
epoch£º247	 i:8 	 global-step:4948	 l-p:0.11909295618534088
epoch£º247	 i:9 	 global-step:4949	 l-p:0.11439863592386246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0656, 5.0656, 5.0656],
        [5.0656, 5.0656, 5.0656],
        [5.0656, 5.0718, 5.0646],
        [5.0656, 5.0656, 5.0656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:248, step:0 
model_pd.l_p.mean(): 0.09718528389930725 
model_pd.l_d.mean(): -20.446941375732422 
model_pd.lagr.mean(): -20.349756240844727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4085], device='cuda:0')), ('power', tensor([-21.2523], device='cuda:0'))])
epoch£º248	 i:0 	 global-step:4960	 l-p:0.09718528389930725
epoch£º248	 i:1 	 global-step:4961	 l-p:0.13810457289218903
epoch£º248	 i:2 	 global-step:4962	 l-p:0.12294570356607437
epoch£º248	 i:3 	 global-step:4963	 l-p:0.16896362602710724
epoch£º248	 i:4 	 global-step:4964	 l-p:0.1265447586774826
epoch£º248	 i:5 	 global-step:4965	 l-p:0.13565221428871155
epoch£º248	 i:6 	 global-step:4966	 l-p:0.13203738629817963
epoch£º248	 i:7 	 global-step:4967	 l-p:0.14123888313770294
epoch£º248	 i:8 	 global-step:4968	 l-p:0.1216423362493515
epoch£º248	 i:9 	 global-step:4969	 l-p:-0.5189972519874573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:249
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5828,  0.4868,  1.0000,  0.4066,
          1.0000,  0.8353, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2392,  0.1485,  1.0000,  0.0922,
          1.0000,  0.6208, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7935,  0.7346,  1.0000,  0.6801,
          1.0000,  0.9258, 31.6228]], device='cuda:0')
 pt:tensor([[4.9106, 5.3008, 5.3371],
        [4.9106, 5.3492, 5.4173],
        [4.9106, 4.9738, 4.9253],
        [4.9106, 5.6441, 5.9544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:249, step:0 
model_pd.l_p.mean(): -0.019563959911465645 
model_pd.l_d.mean(): -20.512962341308594 
model_pd.lagr.mean(): -20.53252601623535 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4488], device='cuda:0')), ('power', tensor([-21.3613], device='cuda:0'))])
epoch£º249	 i:0 	 global-step:4980	 l-p:-0.019563959911465645
epoch£º249	 i:1 	 global-step:4981	 l-p:0.15193088352680206
epoch£º249	 i:2 	 global-step:4982	 l-p:0.12276968359947205
epoch£º249	 i:3 	 global-step:4983	 l-p:0.18113568425178528
epoch£º249	 i:4 	 global-step:4984	 l-p:0.21189051866531372
epoch£º249	 i:5 	 global-step:4985	 l-p:0.13339829444885254
epoch£º249	 i:6 	 global-step:4986	 l-p:0.10743825882673264
epoch£º249	 i:7 	 global-step:4987	 l-p:0.12464220821857452
epoch£º249	 i:8 	 global-step:4988	 l-p:0.15062753856182098
epoch£º249	 i:9 	 global-step:4989	 l-p:0.6592642068862915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:250
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9441, 5.1162, 5.0578],
        [4.9441, 4.9441, 4.9441],
        [4.9441, 4.9529, 4.9417],
        [4.9441, 4.9441, 4.9441]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:250, step:0 
model_pd.l_p.mean(): 0.16078969836235046 
model_pd.l_d.mean(): -19.368616104125977 
model_pd.lagr.mean(): -19.207826614379883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5163], device='cuda:0')), ('power', tensor([-20.2656], device='cuda:0'))])
epoch£º250	 i:0 	 global-step:5000	 l-p:0.16078969836235046
epoch£º250	 i:1 	 global-step:5001	 l-p:0.16916701197624207
epoch£º250	 i:2 	 global-step:5002	 l-p:0.12778101861476898
epoch£º250	 i:3 	 global-step:5003	 l-p:0.13997051119804382
epoch£º250	 i:4 	 global-step:5004	 l-p:0.12806010246276855
epoch£º250	 i:5 	 global-step:5005	 l-p:0.1263173669576645
epoch£º250	 i:6 	 global-step:5006	 l-p:0.14339402318000793
epoch£º250	 i:7 	 global-step:5007	 l-p:0.12415876239538193
epoch£º250	 i:8 	 global-step:5008	 l-p:0.16256491839885712
epoch£º250	 i:9 	 global-step:5009	 l-p:0.08739491552114487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0583, 5.2116, 5.1499],
        [5.0583, 5.0582, 5.0583],
        [5.0583, 5.1328, 5.0810],
        [5.0583, 5.0583, 5.0583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:251, step:0 
model_pd.l_p.mean(): 0.18355052173137665 
model_pd.l_d.mean(): -19.997690200805664 
model_pd.lagr.mean(): -19.81414031982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4560], device='cuda:0')), ('power', tensor([-20.8439], device='cuda:0'))])
epoch£º251	 i:0 	 global-step:5020	 l-p:0.18355052173137665
epoch£º251	 i:1 	 global-step:5021	 l-p:0.134408101439476
epoch£º251	 i:2 	 global-step:5022	 l-p:0.12275539338588715
epoch£º251	 i:3 	 global-step:5023	 l-p:0.11940187960863113
epoch£º251	 i:4 	 global-step:5024	 l-p:0.14187577366828918
epoch£º251	 i:5 	 global-step:5025	 l-p:0.11568199098110199
epoch£º251	 i:6 	 global-step:5026	 l-p:0.07702308148145676
epoch£º251	 i:7 	 global-step:5027	 l-p:0.1256474256515503
epoch£º251	 i:8 	 global-step:5028	 l-p:0.12699128687381744
epoch£º251	 i:9 	 global-step:5029	 l-p:0.1303189992904663
====================================================================================================
====================================================================================================
====================================================================================================

epoch:252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0265, 5.0264, 5.0265],
        [5.0265, 5.9340, 6.3980],
        [5.0265, 5.0973, 5.0463],
        [5.0265, 5.1850, 5.1239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:252, step:0 
model_pd.l_p.mean(): 0.12805117666721344 
model_pd.l_d.mean(): -19.935762405395508 
model_pd.lagr.mean(): -19.807710647583008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4861], device='cuda:0')), ('power', tensor([-20.8120], device='cuda:0'))])
epoch£º252	 i:0 	 global-step:5040	 l-p:0.12805117666721344
epoch£º252	 i:1 	 global-step:5041	 l-p:0.21437831223011017
epoch£º252	 i:2 	 global-step:5042	 l-p:0.12706342339515686
epoch£º252	 i:3 	 global-step:5043	 l-p:0.10367582738399506
epoch£º252	 i:4 	 global-step:5044	 l-p:0.17792661488056183
epoch£º252	 i:5 	 global-step:5045	 l-p:0.12621872127056122
epoch£º252	 i:6 	 global-step:5046	 l-p:-0.5913623571395874
epoch£º252	 i:7 	 global-step:5047	 l-p:0.13409055769443512
epoch£º252	 i:8 	 global-step:5048	 l-p:0.1299063265323639
epoch£º252	 i:9 	 global-step:5049	 l-p:0.07405967265367508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:253
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228]], device='cuda:0')
 pt:tensor([[4.8457, 5.6747, 6.0847],
        [4.8457, 4.9118, 4.8605],
        [4.8457, 4.9037, 4.8557],
        [4.8457, 5.2340, 5.2731]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:253, step:0 
model_pd.l_p.mean(): 0.133326455950737 
model_pd.l_d.mean(): -20.43732452392578 
model_pd.lagr.mean(): -20.303998947143555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4710], device='cuda:0')), ('power', tensor([-21.3073], device='cuda:0'))])
epoch£º253	 i:0 	 global-step:5060	 l-p:0.133326455950737
epoch£º253	 i:1 	 global-step:5061	 l-p:0.20585979521274567
epoch£º253	 i:2 	 global-step:5062	 l-p:0.10942786931991577
epoch£º253	 i:3 	 global-step:5063	 l-p:0.13724090158939362
epoch£º253	 i:4 	 global-step:5064	 l-p:0.06380262225866318
epoch£º253	 i:5 	 global-step:5065	 l-p:0.14052219688892365
epoch£º253	 i:6 	 global-step:5066	 l-p:0.13074426352977753
epoch£º253	 i:7 	 global-step:5067	 l-p:0.2195054143667221
epoch£º253	 i:8 	 global-step:5068	 l-p:0.1417049616575241
epoch£º253	 i:9 	 global-step:5069	 l-p:0.12430586665868759
====================================================================================================
====================================================================================================
====================================================================================================

epoch:254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0299, 5.0380, 5.0279],
        [5.0299, 5.0682, 5.0330],
        [5.0299, 5.0299, 5.0299],
        [5.0299, 5.0299, 5.0299]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:254, step:0 
model_pd.l_p.mean(): 0.15237770974636078 
model_pd.l_d.mean(): -20.0383358001709 
model_pd.lagr.mean(): -19.885957717895508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4539], device='cuda:0')), ('power', tensor([-20.8832], device='cuda:0'))])
epoch£º254	 i:0 	 global-step:5080	 l-p:0.15237770974636078
epoch£º254	 i:1 	 global-step:5081	 l-p:0.09435955435037613
epoch£º254	 i:2 	 global-step:5082	 l-p:0.14092092216014862
epoch£º254	 i:3 	 global-step:5083	 l-p:0.13026730716228485
epoch£º254	 i:4 	 global-step:5084	 l-p:0.10339836031198502
epoch£º254	 i:5 	 global-step:5085	 l-p:0.11587398499250412
epoch£º254	 i:6 	 global-step:5086	 l-p:0.11366111785173416
epoch£º254	 i:7 	 global-step:5087	 l-p:0.145074263215065
epoch£º254	 i:8 	 global-step:5088	 l-p:0.1480763852596283
epoch£º254	 i:9 	 global-step:5089	 l-p:0.14796388149261475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:255
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5645,  0.4665,  1.0000,  0.3855,
          1.0000,  0.8264, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9847,  0.9796,  1.0000,  0.9746,
          1.0000,  0.9949, 31.6228]], device='cuda:0')
 pt:tensor([[5.0736, 5.9772, 6.4313],
        [5.0736, 5.2042, 5.1416],
        [5.0736, 5.5145, 5.5737],
        [5.0736, 6.1334, 6.7482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:255, step:0 
model_pd.l_p.mean(): 0.15257859230041504 
model_pd.l_d.mean(): -18.685977935791016 
model_pd.lagr.mean(): -18.53339958190918 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5105], device='cuda:0')), ('power', tensor([-19.5642], device='cuda:0'))])
epoch£º255	 i:0 	 global-step:5100	 l-p:0.15257859230041504
epoch£º255	 i:1 	 global-step:5101	 l-p:0.11521375179290771
epoch£º255	 i:2 	 global-step:5102	 l-p:0.0904901996254921
epoch£º255	 i:3 	 global-step:5103	 l-p:0.12309018522500992
epoch£º255	 i:4 	 global-step:5104	 l-p:0.1826586276292801
epoch£º255	 i:5 	 global-step:5105	 l-p:0.12698151171207428
epoch£º255	 i:6 	 global-step:5106	 l-p:0.11899805068969727
epoch£º255	 i:7 	 global-step:5107	 l-p:0.3056258261203766
epoch£º255	 i:8 	 global-step:5108	 l-p:0.2094597965478897
epoch£º255	 i:9 	 global-step:5109	 l-p:0.14586172997951508
====================================================================================================
====================================================================================================
====================================================================================================

epoch:256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9239, 4.9478, 4.9210],
        [4.9239, 4.9236, 4.9230],
        [4.9239, 4.9239, 4.9239],
        [4.9239, 5.2714, 5.2814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:256, step:0 
model_pd.l_p.mean(): 0.14717146754264832 
model_pd.l_d.mean(): -20.477195739746094 
model_pd.lagr.mean(): -20.33002471923828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4442], device='cuda:0')), ('power', tensor([-21.3201], device='cuda:0'))])
epoch£º256	 i:0 	 global-step:5120	 l-p:0.14717146754264832
epoch£º256	 i:1 	 global-step:5121	 l-p:-0.11634214967489243
epoch£º256	 i:2 	 global-step:5122	 l-p:0.13579075038433075
epoch£º256	 i:3 	 global-step:5123	 l-p:0.13519422709941864
epoch£º256	 i:4 	 global-step:5124	 l-p:0.11776851117610931
epoch£º256	 i:5 	 global-step:5125	 l-p:0.13520725071430206
epoch£º256	 i:6 	 global-step:5126	 l-p:0.14165976643562317
epoch£º256	 i:7 	 global-step:5127	 l-p:0.3777935802936554
epoch£º256	 i:8 	 global-step:5128	 l-p:0.1234913021326065
epoch£º256	 i:9 	 global-step:5129	 l-p:-0.6626229882240295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8773, 4.8773, 4.8773],
        [4.8773, 4.8773, 4.8773],
        [4.8773, 5.0419, 4.9823],
        [4.8773, 4.8786, 4.8749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:257, step:0 
model_pd.l_p.mean(): 0.1246999204158783 
model_pd.l_d.mean(): -19.457435607910156 
model_pd.lagr.mean(): -19.332735061645508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4968], device='cuda:0')), ('power', tensor([-20.3358], device='cuda:0'))])
epoch£º257	 i:0 	 global-step:5140	 l-p:0.1246999204158783
epoch£º257	 i:1 	 global-step:5141	 l-p:0.14080660045146942
epoch£º257	 i:2 	 global-step:5142	 l-p:0.12464054673910141
epoch£º257	 i:3 	 global-step:5143	 l-p:0.06611179560422897
epoch£º257	 i:4 	 global-step:5144	 l-p:0.16921621561050415
epoch£º257	 i:5 	 global-step:5145	 l-p:0.1257065385580063
epoch£º257	 i:6 	 global-step:5146	 l-p:0.12908603250980377
epoch£º257	 i:7 	 global-step:5147	 l-p:0.16356487572193146
epoch£º257	 i:8 	 global-step:5148	 l-p:0.25596606731414795
epoch£º257	 i:9 	 global-step:5149	 l-p:0.13143402338027954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0327, 5.0325, 5.0327],
        [5.0327, 5.0603, 5.0318],
        [5.0327, 5.0327, 5.0327],
        [5.0327, 5.4049, 5.4236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:258, step:0 
model_pd.l_p.mean(): 0.12395921349525452 
model_pd.l_d.mean(): -19.930845260620117 
model_pd.lagr.mean(): -19.806886672973633 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4606], device='cuda:0')), ('power', tensor([-20.7806], device='cuda:0'))])
epoch£º258	 i:0 	 global-step:5160	 l-p:0.12395921349525452
epoch£º258	 i:1 	 global-step:5161	 l-p:0.1268506944179535
epoch£º258	 i:2 	 global-step:5162	 l-p:0.195098876953125
epoch£º258	 i:3 	 global-step:5163	 l-p:0.11557766050100327
epoch£º258	 i:4 	 global-step:5164	 l-p:0.12056823819875717
epoch£º258	 i:5 	 global-step:5165	 l-p:0.17956885695457458
epoch£º258	 i:6 	 global-step:5166	 l-p:0.19139693677425385
epoch£º258	 i:7 	 global-step:5167	 l-p:0.12343493103981018
epoch£º258	 i:8 	 global-step:5168	 l-p:0.1194942519068718
epoch£º258	 i:9 	 global-step:5169	 l-p:0.14034387469291687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0222, 5.3910, 5.4082],
        [5.0222, 5.0222, 5.0222],
        [5.0222, 5.0222, 5.0222],
        [5.0222, 5.0867, 5.0368]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:259, step:0 
model_pd.l_p.mean(): 0.11502740532159805 
model_pd.l_d.mean(): -18.827857971191406 
model_pd.lagr.mean(): -18.712831497192383 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4956], device='cuda:0')), ('power', tensor([-19.6933], device='cuda:0'))])
epoch£º259	 i:0 	 global-step:5180	 l-p:0.11502740532159805
epoch£º259	 i:1 	 global-step:5181	 l-p:0.09859897196292877
epoch£º259	 i:2 	 global-step:5182	 l-p:0.15009385347366333
epoch£º259	 i:3 	 global-step:5183	 l-p:0.14336037635803223
epoch£º259	 i:4 	 global-step:5184	 l-p:0.2458711564540863
epoch£º259	 i:5 	 global-step:5185	 l-p:0.2656906843185425
epoch£º259	 i:6 	 global-step:5186	 l-p:0.11850358545780182
epoch£º259	 i:7 	 global-step:5187	 l-p:0.12348626554012299
epoch£º259	 i:8 	 global-step:5188	 l-p:0.2552427649497986
epoch£º259	 i:9 	 global-step:5189	 l-p:0.14506512880325317
====================================================================================================
====================================================================================================
====================================================================================================

epoch:260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9453, 5.4708, 5.6007],
        [4.9453, 5.2039, 5.1694],
        [4.9453, 4.9453, 4.9453],
        [4.9453, 5.0156, 4.9623]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:260, step:0 
model_pd.l_p.mean(): 0.15395082533359528 
model_pd.l_d.mean(): -19.979028701782227 
model_pd.lagr.mean(): -19.825077056884766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4994], device='cuda:0')), ('power', tensor([-20.8698], device='cuda:0'))])
epoch£º260	 i:0 	 global-step:5200	 l-p:0.15395082533359528
epoch£º260	 i:1 	 global-step:5201	 l-p:0.4800906181335449
epoch£º260	 i:2 	 global-step:5202	 l-p:0.16969934105873108
epoch£º260	 i:3 	 global-step:5203	 l-p:0.10825102776288986
epoch£º260	 i:4 	 global-step:5204	 l-p:-0.36617186665534973
epoch£º260	 i:5 	 global-step:5205	 l-p:0.13639144599437714
epoch£º260	 i:6 	 global-step:5206	 l-p:0.13439738750457764
epoch£º260	 i:7 	 global-step:5207	 l-p:0.18022418022155762
epoch£º260	 i:8 	 global-step:5208	 l-p:0.024508647620677948
epoch£º260	 i:9 	 global-step:5209	 l-p:0.13417893648147583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9264, 4.9264, 4.9264],
        [4.9264, 5.2331, 5.2210],
        [4.9264, 5.2740, 5.2839],
        [4.9264, 4.9288, 4.9234]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:261, step:0 
model_pd.l_p.mean(): -0.6506187915802002 
model_pd.l_d.mean(): -19.924488067626953 
model_pd.lagr.mean(): -20.57510757446289 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4677], device='cuda:0')), ('power', tensor([-20.7814], device='cuda:0'))])
epoch£º261	 i:0 	 global-step:5220	 l-p:-0.6506187915802002
epoch£º261	 i:1 	 global-step:5221	 l-p:0.11192819476127625
epoch£º261	 i:2 	 global-step:5222	 l-p:-0.0358060747385025
epoch£º261	 i:3 	 global-step:5223	 l-p:0.13803577423095703
epoch£º261	 i:4 	 global-step:5224	 l-p:0.4216559827327728
epoch£º261	 i:5 	 global-step:5225	 l-p:0.145847886800766
epoch£º261	 i:6 	 global-step:5226	 l-p:0.12755028903484344
epoch£º261	 i:7 	 global-step:5227	 l-p:0.14456695318222046
epoch£º261	 i:8 	 global-step:5228	 l-p:0.15498007833957672
epoch£º261	 i:9 	 global-step:5229	 l-p:0.14005130529403687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0217, 5.0214, 5.0212],
        [5.0217, 5.0297, 5.0185],
        [5.0217, 5.0213, 5.0216],
        [5.0217, 5.0213, 5.0217]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:262, step:0 
model_pd.l_p.mean(): 0.1253611296415329 
model_pd.l_d.mean(): -19.394807815551758 
model_pd.lagr.mean(): -19.269447326660156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4841], device='cuda:0')), ('power', tensor([-20.2589], device='cuda:0'))])
epoch£º262	 i:0 	 global-step:5240	 l-p:0.1253611296415329
epoch£º262	 i:1 	 global-step:5241	 l-p:0.130497545003891
epoch£º262	 i:2 	 global-step:5242	 l-p:0.1405189484357834
epoch£º262	 i:3 	 global-step:5243	 l-p:0.3772335648536682
epoch£º262	 i:4 	 global-step:5244	 l-p:-0.22511723637580872
epoch£º262	 i:5 	 global-step:5245	 l-p:0.15359124541282654
epoch£º262	 i:6 	 global-step:5246	 l-p:0.14975690841674805
epoch£º262	 i:7 	 global-step:5247	 l-p:0.13228915631771088
epoch£º262	 i:8 	 global-step:5248	 l-p:0.19961856305599213
epoch£º262	 i:9 	 global-step:5249	 l-p:0.12779386341571808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0461, 5.5222, 5.6078],
        [5.0461, 5.3343, 5.3087],
        [5.0461, 5.0478, 5.0444],
        [5.0461, 5.4647, 5.5113]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:263, step:0 
model_pd.l_p.mean(): 0.1264103353023529 
model_pd.l_d.mean(): -19.186464309692383 
model_pd.lagr.mean(): -19.060054779052734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4787], device='cuda:0')), ('power', tensor([-20.0411], device='cuda:0'))])
epoch£º263	 i:0 	 global-step:5260	 l-p:0.1264103353023529
epoch£º263	 i:1 	 global-step:5261	 l-p:0.17712052166461945
epoch£º263	 i:2 	 global-step:5262	 l-p:0.12387585639953613
epoch£º263	 i:3 	 global-step:5263	 l-p:0.12992244958877563
epoch£º263	 i:4 	 global-step:5264	 l-p:0.14443141222000122
epoch£º263	 i:5 	 global-step:5265	 l-p:0.13021451234817505
epoch£º263	 i:6 	 global-step:5266	 l-p:0.14642466604709625
epoch£º263	 i:7 	 global-step:5267	 l-p:0.12922661006450653
epoch£º263	 i:8 	 global-step:5268	 l-p:0.06737548112869263
epoch£º263	 i:9 	 global-step:5269	 l-p:0.12374266237020493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0908, 5.0905, 5.0905],
        [5.0908, 5.5934, 5.6950],
        [5.0908, 5.8578, 6.1795],
        [5.0908, 5.4342, 5.4338]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:264, step:0 
model_pd.l_p.mean(): 0.14783555269241333 
model_pd.l_d.mean(): -20.330286026000977 
model_pd.lagr.mean(): -20.182451248168945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4159], device='cuda:0')), ('power', tensor([-21.1412], device='cuda:0'))])
epoch£º264	 i:0 	 global-step:5280	 l-p:0.14783555269241333
epoch£º264	 i:1 	 global-step:5281	 l-p:0.1293971836566925
epoch£º264	 i:2 	 global-step:5282	 l-p:0.10623864829540253
epoch£º264	 i:3 	 global-step:5283	 l-p:0.12226612120866776
epoch£º264	 i:4 	 global-step:5284	 l-p:0.14626440405845642
epoch£º264	 i:5 	 global-step:5285	 l-p:0.14628876745700836
epoch£º264	 i:6 	 global-step:5286	 l-p:0.28811076283454895
epoch£º264	 i:7 	 global-step:5287	 l-p:0.5611174702644348
epoch£º264	 i:8 	 global-step:5288	 l-p:0.1552794873714447
epoch£º264	 i:9 	 global-step:5289	 l-p:0.12134530395269394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0121, 5.8377, 6.2240],
        [5.0121, 5.1571, 5.0932],
        [5.0121, 5.0121, 5.0121],
        [5.0121, 5.0121, 5.0121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:265, step:0 
model_pd.l_p.mean(): 0.11401033401489258 
model_pd.l_d.mean(): -19.647390365600586 
model_pd.lagr.mean(): -19.53338050842285 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5044], device='cuda:0')), ('power', tensor([-20.5372], device='cuda:0'))])
epoch£º265	 i:0 	 global-step:5300	 l-p:0.11401033401489258
epoch£º265	 i:1 	 global-step:5301	 l-p:0.1453249454498291
epoch£º265	 i:2 	 global-step:5302	 l-p:0.1345497965812683
epoch£º265	 i:3 	 global-step:5303	 l-p:0.2202623188495636
epoch£º265	 i:4 	 global-step:5304	 l-p:0.09046626836061478
epoch£º265	 i:5 	 global-step:5305	 l-p:0.15615218877792358
epoch£º265	 i:6 	 global-step:5306	 l-p:0.1295647770166397
epoch£º265	 i:7 	 global-step:5307	 l-p:0.13196896016597748
epoch£º265	 i:8 	 global-step:5308	 l-p:0.1449718326330185
epoch£º265	 i:9 	 global-step:5309	 l-p:0.12082996219396591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:266
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0619, 5.0614, 5.0618],
        [5.0619, 5.0618, 5.0619],
        [5.0619, 5.0626, 5.0605],
        [5.0619, 5.0618, 5.0619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:266, step:0 
model_pd.l_p.mean(): 0.12360160797834396 
model_pd.l_d.mean(): -20.18889045715332 
model_pd.lagr.mean(): -20.065288543701172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4371], device='cuda:0')), ('power', tensor([-21.0191], device='cuda:0'))])
epoch£º266	 i:0 	 global-step:5320	 l-p:0.12360160797834396
epoch£º266	 i:1 	 global-step:5321	 l-p:0.13010281324386597
epoch£º266	 i:2 	 global-step:5322	 l-p:0.12524276971817017
epoch£º266	 i:3 	 global-step:5323	 l-p:0.2911520004272461
epoch£º266	 i:4 	 global-step:5324	 l-p:0.14797770977020264
epoch£º266	 i:5 	 global-step:5325	 l-p:0.3534429967403412
epoch£º266	 i:6 	 global-step:5326	 l-p:0.13896583020687103
epoch£º266	 i:7 	 global-step:5327	 l-p:0.18823660910129547
epoch£º266	 i:8 	 global-step:5328	 l-p:0.12947867810726166
epoch£º266	 i:9 	 global-step:5329	 l-p:0.13280978798866272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0523, 5.0523, 5.0523],
        [5.0523, 5.1388, 5.0800],
        [5.0523, 5.0523, 5.0523],
        [5.0523, 5.0519, 5.0514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:267, step:0 
model_pd.l_p.mean(): 0.12930113077163696 
model_pd.l_d.mean(): -19.801048278808594 
model_pd.lagr.mean(): -19.6717472076416 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4823], device='cuda:0')), ('power', tensor([-20.6709], device='cuda:0'))])
epoch£º267	 i:0 	 global-step:5340	 l-p:0.12930113077163696
epoch£º267	 i:1 	 global-step:5341	 l-p:0.14451520144939423
epoch£º267	 i:2 	 global-step:5342	 l-p:0.11060734838247299
epoch£º267	 i:3 	 global-step:5343	 l-p:0.1337401419878006
epoch£º267	 i:4 	 global-step:5344	 l-p:0.11930447071790695
epoch£º267	 i:5 	 global-step:5345	 l-p:0.18210533261299133
epoch£º267	 i:6 	 global-step:5346	 l-p:0.3251892924308777
epoch£º267	 i:7 	 global-step:5347	 l-p:0.12439840286970139
epoch£º267	 i:8 	 global-step:5348	 l-p:0.13482308387756348
epoch£º267	 i:9 	 global-step:5349	 l-p:0.177047997713089
====================================================================================================
====================================================================================================
====================================================================================================

epoch:268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9794, 4.9793, 4.9794],
        [4.9794, 5.1817, 5.1268],
        [4.9794, 4.9784, 4.9783],
        [4.9794, 4.9794, 4.9794]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:268, step:0 
model_pd.l_p.mean(): 0.15004831552505493 
model_pd.l_d.mean(): -20.079076766967773 
model_pd.lagr.mean(): -19.929027557373047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4763], device='cuda:0')), ('power', tensor([-20.9478], device='cuda:0'))])
epoch£º268	 i:0 	 global-step:5360	 l-p:0.15004831552505493
epoch£º268	 i:1 	 global-step:5361	 l-p:0.13865336775779724
epoch£º268	 i:2 	 global-step:5362	 l-p:0.15357854962348938
epoch£º268	 i:3 	 global-step:5363	 l-p:0.12294204533100128
epoch£º268	 i:4 	 global-step:5364	 l-p:3.807932138442993
epoch£º268	 i:5 	 global-step:5365	 l-p:-0.352575421333313
epoch£º268	 i:6 	 global-step:5366	 l-p:0.25248876214027405
epoch£º268	 i:7 	 global-step:5367	 l-p:0.1339644193649292
epoch£º268	 i:8 	 global-step:5368	 l-p:0.1569713056087494
epoch£º268	 i:9 	 global-step:5369	 l-p:0.12851476669311523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0208, 5.0206, 5.0208],
        [5.0208, 5.0200, 5.0206],
        [5.0208, 5.0297, 5.0161],
        [5.0208, 5.0231, 5.0178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:269, step:0 
model_pd.l_p.mean(): 0.1916581690311432 
model_pd.l_d.mean(): -20.264297485351562 
model_pd.lagr.mean(): -20.07263946533203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4546], device='cuda:0')), ('power', tensor([-21.1141], device='cuda:0'))])
epoch£º269	 i:0 	 global-step:5380	 l-p:0.1916581690311432
epoch£º269	 i:1 	 global-step:5381	 l-p:0.2232077419757843
epoch£º269	 i:2 	 global-step:5382	 l-p:0.13947445154190063
epoch£º269	 i:3 	 global-step:5383	 l-p:0.11445461213588715
epoch£º269	 i:4 	 global-step:5384	 l-p:0.12277143448591232
epoch£º269	 i:5 	 global-step:5385	 l-p:0.12219175696372986
epoch£º269	 i:6 	 global-step:5386	 l-p:0.10637574642896652
epoch£º269	 i:7 	 global-step:5387	 l-p:0.1724286824464798
epoch£º269	 i:8 	 global-step:5388	 l-p:0.13902506232261658
epoch£º269	 i:9 	 global-step:5389	 l-p:0.09886131435632706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9837, 4.9827, 4.9835],
        [4.9837, 5.2370, 5.1984],
        [4.9837, 4.9835, 4.9837],
        [4.9837, 5.0913, 5.0274]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:270, step:0 
model_pd.l_p.mean(): 0.13212361931800842 
model_pd.l_d.mean(): -20.469375610351562 
model_pd.lagr.mean(): -20.337251663208008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4291], device='cuda:0')), ('power', tensor([-21.2965], device='cuda:0'))])
epoch£º270	 i:0 	 global-step:5400	 l-p:0.13212361931800842
epoch£º270	 i:1 	 global-step:5401	 l-p:0.3775240480899811
epoch£º270	 i:2 	 global-step:5402	 l-p:1.131801724433899
epoch£º270	 i:3 	 global-step:5403	 l-p:0.15267515182495117
epoch£º270	 i:4 	 global-step:5404	 l-p:0.14000265300273895
epoch£º270	 i:5 	 global-step:5405	 l-p:0.283572256565094
epoch£º270	 i:6 	 global-step:5406	 l-p:0.1678648740053177
epoch£º270	 i:7 	 global-step:5407	 l-p:0.12920750677585602
epoch£º270	 i:8 	 global-step:5408	 l-p:0.12981806695461273
epoch£º270	 i:9 	 global-step:5409	 l-p:0.15404660999774933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9506, 4.9506, 4.9506],
        [4.9506, 5.0381, 4.9771],
        [4.9506, 5.3370, 5.3688],
        [4.9506, 5.6667, 5.9583]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:271, step:0 
model_pd.l_p.mean(): -0.11571017652750015 
model_pd.l_d.mean(): -20.308595657348633 
model_pd.lagr.mean(): -20.424304962158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4653], device='cuda:0')), ('power', tensor([-21.1702], device='cuda:0'))])
epoch£º271	 i:0 	 global-step:5420	 l-p:-0.11571017652750015
epoch£º271	 i:1 	 global-step:5421	 l-p:0.11784616112709045
epoch£º271	 i:2 	 global-step:5422	 l-p:0.15270768105983734
epoch£º271	 i:3 	 global-step:5423	 l-p:0.1551690250635147
epoch£º271	 i:4 	 global-step:5424	 l-p:0.13786761462688446
epoch£º271	 i:5 	 global-step:5425	 l-p:0.12209693342447281
epoch£º271	 i:6 	 global-step:5426	 l-p:0.14873909950256348
epoch£º271	 i:7 	 global-step:5427	 l-p:0.1317146122455597
epoch£º271	 i:8 	 global-step:5428	 l-p:-0.012257690541446209
epoch£º271	 i:9 	 global-step:5429	 l-p:0.14368881285190582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:272
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9241, 4.9223, 4.9231],
        [4.9241, 5.3857, 5.4700],
        [4.9241, 4.9231, 4.9212],
        [4.9241, 4.9226, 4.9236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:272, step:0 
model_pd.l_p.mean(): -0.15138596296310425 
model_pd.l_d.mean(): -19.066783905029297 
model_pd.lagr.mean(): -19.218170166015625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5182], device='cuda:0')), ('power', tensor([-19.9601], device='cuda:0'))])
epoch£º272	 i:0 	 global-step:5440	 l-p:-0.15138596296310425
epoch£º272	 i:1 	 global-step:5441	 l-p:0.15633776783943176
epoch£º272	 i:2 	 global-step:5442	 l-p:0.11788325756788254
epoch£º272	 i:3 	 global-step:5443	 l-p:0.10193053632974625
epoch£º272	 i:4 	 global-step:5444	 l-p:0.14313548803329468
epoch£º272	 i:5 	 global-step:5445	 l-p:0.12857525050640106
epoch£º272	 i:6 	 global-step:5446	 l-p:-0.03321234509348869
epoch£º272	 i:7 	 global-step:5447	 l-p:0.135199636220932
epoch£º272	 i:8 	 global-step:5448	 l-p:0.2266433984041214
epoch£º272	 i:9 	 global-step:5449	 l-p:0.14435264468193054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9089, 4.9632, 4.9123],
        [4.9089, 4.9083, 4.9089],
        [4.9089, 4.9084, 4.9049],
        [4.9089, 4.9069, 4.9079]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:273, step:0 
model_pd.l_p.mean(): 0.2248559147119522 
model_pd.l_d.mean(): -18.36911392211914 
model_pd.lagr.mean(): -18.144258499145508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5711], device='cuda:0')), ('power', tensor([-19.3041], device='cuda:0'))])
epoch£º273	 i:0 	 global-step:5460	 l-p:0.2248559147119522
epoch£º273	 i:1 	 global-step:5461	 l-p:0.13891276717185974
epoch£º273	 i:2 	 global-step:5462	 l-p:0.14426036179065704
epoch£º273	 i:3 	 global-step:5463	 l-p:0.06650229543447495
epoch£º273	 i:4 	 global-step:5464	 l-p:0.12148341536521912
epoch£º273	 i:5 	 global-step:5465	 l-p:0.12225420027971268
epoch£º273	 i:6 	 global-step:5466	 l-p:0.1466955989599228
epoch£º273	 i:7 	 global-step:5467	 l-p:0.13551844656467438
epoch£º273	 i:8 	 global-step:5468	 l-p:0.13483022153377533
epoch£º273	 i:9 	 global-step:5469	 l-p:0.19272765517234802
====================================================================================================
====================================================================================================
====================================================================================================

epoch:274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8607, 5.0538, 4.9978],
        [4.8607, 5.5441, 5.8166],
        [4.8607, 4.8598, 4.8606],
        [4.8607, 5.5423, 5.8131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:274, step:0 
model_pd.l_p.mean(): 0.03371559455990791 
model_pd.l_d.mean(): -20.522762298583984 
model_pd.lagr.mean(): -20.489046096801758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4618], device='cuda:0')), ('power', tensor([-21.3848], device='cuda:0'))])
epoch£º274	 i:0 	 global-step:5480	 l-p:0.03371559455990791
epoch£º274	 i:1 	 global-step:5481	 l-p:0.1357942372560501
epoch£º274	 i:2 	 global-step:5482	 l-p:0.18633510172367096
epoch£º274	 i:3 	 global-step:5483	 l-p:0.21967212855815887
epoch£º274	 i:4 	 global-step:5484	 l-p:0.14482183754444122
epoch£º274	 i:5 	 global-step:5485	 l-p:0.13754287362098694
epoch£º274	 i:6 	 global-step:5486	 l-p:0.12226167321205139
epoch£º274	 i:7 	 global-step:5487	 l-p:0.1201934963464737
epoch£º274	 i:8 	 global-step:5488	 l-p:0.17991770803928375
epoch£º274	 i:9 	 global-step:5489	 l-p:0.5623136162757874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0111, 5.2926, 5.2647],
        [5.0111, 5.3164, 5.2996],
        [5.0111, 5.0111, 5.0111],
        [5.0111, 5.0096, 5.0101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:275, step:0 
model_pd.l_p.mean(): 0.1449250727891922 
model_pd.l_d.mean(): -20.58542823791504 
model_pd.lagr.mean(): -20.44050407409668 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4055], device='cuda:0')), ('power', tensor([-21.3903], device='cuda:0'))])
epoch£º275	 i:0 	 global-step:5500	 l-p:0.1449250727891922
epoch£º275	 i:1 	 global-step:5501	 l-p:0.12701183557510376
epoch£º275	 i:2 	 global-step:5502	 l-p:0.11955586820840836
epoch£º275	 i:3 	 global-step:5503	 l-p:0.10981706529855728
epoch£º275	 i:4 	 global-step:5504	 l-p:0.1544998288154602
epoch£º275	 i:5 	 global-step:5505	 l-p:0.15508292615413666
epoch£º275	 i:6 	 global-step:5506	 l-p:0.1154307946562767
epoch£º275	 i:7 	 global-step:5507	 l-p:0.10602805763483047
epoch£º275	 i:8 	 global-step:5508	 l-p:0.16138151288032532
epoch£º275	 i:9 	 global-step:5509	 l-p:0.13253389298915863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0191, 5.6697, 5.8960],
        [5.0191, 5.0176, 5.0180],
        [5.0191, 5.0191, 5.0191],
        [5.0191, 5.0187, 5.0191]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:276, step:0 
model_pd.l_p.mean(): 0.13001374900341034 
model_pd.l_d.mean(): -20.64908218383789 
model_pd.lagr.mean(): -20.519067764282227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3970], device='cuda:0')), ('power', tensor([-21.4464], device='cuda:0'))])
epoch£º276	 i:0 	 global-step:5520	 l-p:0.13001374900341034
epoch£º276	 i:1 	 global-step:5521	 l-p:0.16228297352790833
epoch£º276	 i:2 	 global-step:5522	 l-p:0.011461583897471428
epoch£º276	 i:3 	 global-step:5523	 l-p:0.14321669936180115
epoch£º276	 i:4 	 global-step:5524	 l-p:0.1110599935054779
epoch£º276	 i:5 	 global-step:5525	 l-p:0.10616077482700348
epoch£º276	 i:6 	 global-step:5526	 l-p:-0.09826882928609848
epoch£º276	 i:7 	 global-step:5527	 l-p:0.1267046481370926
epoch£º276	 i:8 	 global-step:5528	 l-p:0.12426092475652695
epoch£º276	 i:9 	 global-step:5529	 l-p:0.12628427147865295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7762, 5.1974, 5.2644],
        [4.7762, 5.2783, 5.4061],
        [4.7762, 4.7762, 4.7762],
        [4.7762, 5.0958, 5.0971]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:277, step:0 
model_pd.l_p.mean(): 0.17743022739887238 
model_pd.l_d.mean(): -20.092777252197266 
model_pd.lagr.mean(): -19.915346145629883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5400], device='cuda:0')), ('power', tensor([-21.0278], device='cuda:0'))])
epoch£º277	 i:0 	 global-step:5540	 l-p:0.17743022739887238
epoch£º277	 i:1 	 global-step:5541	 l-p:0.14848333597183228
epoch£º277	 i:2 	 global-step:5542	 l-p:0.13718345761299133
epoch£º277	 i:3 	 global-step:5543	 l-p:0.1604692041873932
epoch£º277	 i:4 	 global-step:5544	 l-p:0.06725228577852249
epoch£º277	 i:5 	 global-step:5545	 l-p:0.12101003527641296
epoch£º277	 i:6 	 global-step:5546	 l-p:0.14824527502059937
epoch£º277	 i:7 	 global-step:5547	 l-p:-0.3813723623752594
epoch£º277	 i:8 	 global-step:5548	 l-p:0.13425180315971375
epoch£º277	 i:9 	 global-step:5549	 l-p:0.13913187384605408
====================================================================================================
====================================================================================================
====================================================================================================

epoch:278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9146, 4.9129, 4.9142],
        [4.9146, 4.9122, 4.9117],
        [4.9146, 4.9254, 4.9052],
        [4.9146, 5.4131, 5.5256]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:278, step:0 
model_pd.l_p.mean(): 0.13012385368347168 
model_pd.l_d.mean(): -19.023887634277344 
model_pd.lagr.mean(): -18.89376449584961 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5170], device='cuda:0')), ('power', tensor([-19.9151], device='cuda:0'))])
epoch£º278	 i:0 	 global-step:5560	 l-p:0.13012385368347168
epoch£º278	 i:1 	 global-step:5561	 l-p:-0.06126399710774422
epoch£º278	 i:2 	 global-step:5562	 l-p:0.3379644751548767
epoch£º278	 i:3 	 global-step:5563	 l-p:0.11567918211221695
epoch£º278	 i:4 	 global-step:5564	 l-p:0.14297257363796234
epoch£º278	 i:5 	 global-step:5565	 l-p:0.1173027753829956
epoch£º278	 i:6 	 global-step:5566	 l-p:0.13550332188606262
epoch£º278	 i:7 	 global-step:5567	 l-p:0.11830936372280121
epoch£º278	 i:8 	 global-step:5568	 l-p:0.15632963180541992
epoch£º278	 i:9 	 global-step:5569	 l-p:0.1263745278120041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0677, 5.1086, 5.0662],
        [5.0677, 5.0681, 5.0644],
        [5.0677, 5.5542, 5.6462],
        [5.0677, 5.0994, 5.0633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:279, step:0 
model_pd.l_p.mean(): 0.13247622549533844 
model_pd.l_d.mean(): -20.314924240112305 
model_pd.lagr.mean(): -20.18244743347168 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4214], device='cuda:0')), ('power', tensor([-21.1312], device='cuda:0'))])
epoch£º279	 i:0 	 global-step:5580	 l-p:0.13247622549533844
epoch£º279	 i:1 	 global-step:5581	 l-p:0.21198074519634247
epoch£º279	 i:2 	 global-step:5582	 l-p:0.1082751527428627
epoch£º279	 i:3 	 global-step:5583	 l-p:0.10296253114938736
epoch£º279	 i:4 	 global-step:5584	 l-p:0.14385001361370087
epoch£º279	 i:5 	 global-step:5585	 l-p:0.19686496257781982
epoch£º279	 i:6 	 global-step:5586	 l-p:0.20994704961776733
epoch£º279	 i:7 	 global-step:5587	 l-p:0.13005685806274414
epoch£º279	 i:8 	 global-step:5588	 l-p:0.1427726000547409
epoch£º279	 i:9 	 global-step:5589	 l-p:0.13830392062664032
====================================================================================================
====================================================================================================
====================================================================================================

epoch:280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0412, 5.0409, 5.0411],
        [5.0412, 5.0408, 5.0378],
        [5.0412, 5.0496, 5.0342],
        [5.0412, 5.0731, 5.0359]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:280, step:0 
model_pd.l_p.mean(): 0.12617553770542145 
model_pd.l_d.mean(): -19.88981056213379 
model_pd.lagr.mean(): -19.763635635375977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4372], device='cuda:0')), ('power', tensor([-20.7145], device='cuda:0'))])
epoch£º280	 i:0 	 global-step:5600	 l-p:0.12617553770542145
epoch£º280	 i:1 	 global-step:5601	 l-p:0.15199103951454163
epoch£º280	 i:2 	 global-step:5602	 l-p:0.1477571278810501
epoch£º280	 i:3 	 global-step:5603	 l-p:0.1302533894777298
epoch£º280	 i:4 	 global-step:5604	 l-p:0.13128523528575897
epoch£º280	 i:5 	 global-step:5605	 l-p:0.04940143972635269
epoch£º280	 i:6 	 global-step:5606	 l-p:0.0570245198905468
epoch£º280	 i:7 	 global-step:5607	 l-p:0.12192442268133163
epoch£º280	 i:8 	 global-step:5608	 l-p:0.12957976758480072
epoch£º280	 i:9 	 global-step:5609	 l-p:0.05383314937353134
====================================================================================================
====================================================================================================
====================================================================================================

epoch:281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9041, 4.9942, 4.9294],
        [4.9041, 4.9040, 4.9041],
        [4.9041, 4.9016, 4.9032],
        [4.9041, 4.9067, 4.8954]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:281, step:0 
model_pd.l_p.mean(): 0.06597160547971725 
model_pd.l_d.mean(): -20.443767547607422 
model_pd.lagr.mean(): -20.377796173095703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4622], device='cuda:0')), ('power', tensor([-21.3047], device='cuda:0'))])
epoch£º281	 i:0 	 global-step:5620	 l-p:0.06597160547971725
epoch£º281	 i:1 	 global-step:5621	 l-p:0.036947451531887054
epoch£º281	 i:2 	 global-step:5622	 l-p:0.15118145942687988
epoch£º281	 i:3 	 global-step:5623	 l-p:0.16376587748527527
epoch£º281	 i:4 	 global-step:5624	 l-p:-0.2906677722930908
epoch£º281	 i:5 	 global-step:5625	 l-p:0.11595062166452408
epoch£º281	 i:6 	 global-step:5626	 l-p:0.15271294116973877
epoch£º281	 i:7 	 global-step:5627	 l-p:0.13445013761520386
epoch£º281	 i:8 	 global-step:5628	 l-p:0.13016287982463837
epoch£º281	 i:9 	 global-step:5629	 l-p:0.11975033581256866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:282
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1456,  0.0766,  1.0000,  0.0403,
          1.0000,  0.5261, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7601,  0.6936,  1.0000,  0.6330,
          1.0000,  0.9126, 31.6228]], device='cuda:0')
 pt:tensor([[4.9960, 5.0023, 4.9880],
        [4.9960, 5.1220, 5.0542],
        [4.9960, 5.0060, 4.9871],
        [4.9960, 5.6606, 5.9018]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:282, step:0 
model_pd.l_p.mean(): 0.09723063558340073 
model_pd.l_d.mean(): -18.434185028076172 
model_pd.lagr.mean(): -18.33695411682129 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5897], device='cuda:0')), ('power', tensor([-19.3897], device='cuda:0'))])
epoch£º282	 i:0 	 global-step:5640	 l-p:0.09723063558340073
epoch£º282	 i:1 	 global-step:5641	 l-p:0.13501453399658203
epoch£º282	 i:2 	 global-step:5642	 l-p:0.18395355343818665
epoch£º282	 i:3 	 global-step:5643	 l-p:0.2368222028017044
epoch£º282	 i:4 	 global-step:5644	 l-p:0.18399609625339508
epoch£º282	 i:5 	 global-step:5645	 l-p:0.1498621553182602
epoch£º282	 i:6 	 global-step:5646	 l-p:0.11401896923780441
epoch£º282	 i:7 	 global-step:5647	 l-p:0.1578860878944397
epoch£º282	 i:8 	 global-step:5648	 l-p:0.1325734555721283
epoch£º282	 i:9 	 global-step:5649	 l-p:0.1287948042154312
====================================================================================================
====================================================================================================
====================================================================================================

epoch:283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0575, 5.3021, 5.2569],
        [5.0575, 5.7387, 5.9880],
        [5.0575, 5.3021, 5.2569],
        [5.0575, 5.0566, 5.0574]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:283, step:0 
model_pd.l_p.mean(): 0.09459499269723892 
model_pd.l_d.mean(): -19.917877197265625 
model_pd.lagr.mean(): -19.82328224182129 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4699], device='cuda:0')), ('power', tensor([-20.7770], device='cuda:0'))])
epoch£º283	 i:0 	 global-step:5660	 l-p:0.09459499269723892
epoch£º283	 i:1 	 global-step:5661	 l-p:0.1226256713271141
epoch£º283	 i:2 	 global-step:5662	 l-p:0.12774501740932465
epoch£º283	 i:3 	 global-step:5663	 l-p:0.14774638414382935
epoch£º283	 i:4 	 global-step:5664	 l-p:0.3022391200065613
epoch£º283	 i:5 	 global-step:5665	 l-p:0.13463199138641357
epoch£º283	 i:6 	 global-step:5666	 l-p:0.12509755790233612
epoch£º283	 i:7 	 global-step:5667	 l-p:1.521946668624878
epoch£º283	 i:8 	 global-step:5668	 l-p:0.18396908044815063
epoch£º283	 i:9 	 global-step:5669	 l-p:0.47443926334381104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0080, 5.0057, 5.0070],
        [5.0080, 5.0064, 5.0076],
        [5.0080, 5.0078, 5.0079],
        [5.0080, 5.1444, 5.0763]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:284, step:0 
model_pd.l_p.mean(): 0.18379007279872894 
model_pd.l_d.mean(): -20.594816207885742 
model_pd.lagr.mean(): -20.411026000976562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4130], device='cuda:0')), ('power', tensor([-21.4077], device='cuda:0'))])
epoch£º284	 i:0 	 global-step:5680	 l-p:0.18379007279872894
epoch£º284	 i:1 	 global-step:5681	 l-p:0.17379474639892578
epoch£º284	 i:2 	 global-step:5682	 l-p:0.1356845498085022
epoch£º284	 i:3 	 global-step:5683	 l-p:0.14985641837120056
epoch£º284	 i:4 	 global-step:5684	 l-p:0.12198585271835327
epoch£º284	 i:5 	 global-step:5685	 l-p:0.007813539355993271
epoch£º284	 i:6 	 global-step:5686	 l-p:0.12731124460697174
epoch£º284	 i:7 	 global-step:5687	 l-p:0.13740143179893494
epoch£º284	 i:8 	 global-step:5688	 l-p:0.11661703139543533
epoch£º284	 i:9 	 global-step:5689	 l-p:0.10118339955806732
====================================================================================================
====================================================================================================
====================================================================================================

epoch:285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1267, 5.1267, 5.1267],
        [5.1267, 5.1252, 5.1262],
        [5.1267, 5.1363, 5.1200],
        [5.1267, 5.3632, 5.3132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:285, step:0 
model_pd.l_p.mean(): 0.1114683672785759 
model_pd.l_d.mean(): -20.29411506652832 
model_pd.lagr.mean(): -20.182645797729492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4072], device='cuda:0')), ('power', tensor([-21.0953], device='cuda:0'))])
epoch£º285	 i:0 	 global-step:5700	 l-p:0.1114683672785759
epoch£º285	 i:1 	 global-step:5701	 l-p:0.1259850263595581
epoch£º285	 i:2 	 global-step:5702	 l-p:0.13499721884727478
epoch£º285	 i:3 	 global-step:5703	 l-p:0.12192925810813904
epoch£º285	 i:4 	 global-step:5704	 l-p:0.12806683778762817
epoch£º285	 i:5 	 global-step:5705	 l-p:-0.18194031715393066
epoch£º285	 i:6 	 global-step:5706	 l-p:0.07631119340658188
epoch£º285	 i:7 	 global-step:5707	 l-p:0.004869508557021618
epoch£º285	 i:8 	 global-step:5708	 l-p:0.19619645178318024
epoch£º285	 i:9 	 global-step:5709	 l-p:0.12710857391357422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9439, 4.9407, 4.9422],
        [4.9439, 4.9439, 4.9439],
        [4.9439, 4.9439, 4.9439],
        [4.9439, 4.9432, 4.9439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:286, step:0 
model_pd.l_p.mean(): 0.1310242861509323 
model_pd.l_d.mean(): -20.464078903198242 
model_pd.lagr.mean(): -20.33305549621582 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4416], device='cuda:0')), ('power', tensor([-21.3041], device='cuda:0'))])
epoch£º286	 i:0 	 global-step:5720	 l-p:0.1310242861509323
epoch£º286	 i:1 	 global-step:5721	 l-p:0.13206201791763306
epoch£º286	 i:2 	 global-step:5722	 l-p:0.2999500334262848
epoch£º286	 i:3 	 global-step:5723	 l-p:0.12468661367893219
epoch£º286	 i:4 	 global-step:5724	 l-p:0.0862351804971695
epoch£º286	 i:5 	 global-step:5725	 l-p:0.025984348729252815
epoch£º286	 i:6 	 global-step:5726	 l-p:0.13115859031677246
epoch£º286	 i:7 	 global-step:5727	 l-p:0.13517579436302185
epoch£º286	 i:8 	 global-step:5728	 l-p:0.14075292646884918
epoch£º286	 i:9 	 global-step:5729	 l-p:0.13858424127101898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0084, 5.1973, 5.1359],
        [5.0084, 5.2749, 5.2396],
        [5.0084, 5.0302, 4.9982],
        [5.0084, 5.0059, 5.0049]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:287, step:0 
model_pd.l_p.mean(): 0.14021603763103485 
model_pd.l_d.mean(): -20.266918182373047 
model_pd.lagr.mean(): -20.12670135498047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4479], device='cuda:0')), ('power', tensor([-21.1098], device='cuda:0'))])
epoch£º287	 i:0 	 global-step:5740	 l-p:0.14021603763103485
epoch£º287	 i:1 	 global-step:5741	 l-p:0.1281326860189438
epoch£º287	 i:2 	 global-step:5742	 l-p:0.18461337685585022
epoch£º287	 i:3 	 global-step:5743	 l-p:-0.009297600015997887
epoch£º287	 i:4 	 global-step:5744	 l-p:0.1331276148557663
epoch£º287	 i:5 	 global-step:5745	 l-p:0.14535140991210938
epoch£º287	 i:6 	 global-step:5746	 l-p:0.13961604237556458
epoch£º287	 i:7 	 global-step:5747	 l-p:0.14255069196224213
epoch£º287	 i:8 	 global-step:5748	 l-p:0.14298982918262482
epoch£º287	 i:9 	 global-step:5749	 l-p:-0.043200645595788956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9436, 4.9402, 4.9420],
        [4.9436, 4.9436, 4.9436],
        [4.9436, 4.9431, 4.9436],
        [4.9436, 5.0765, 5.0074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:288, step:0 
model_pd.l_p.mean(): 0.1922358274459839 
model_pd.l_d.mean(): -20.408401489257812 
model_pd.lagr.mean(): -20.21616554260254 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4558], device='cuda:0')), ('power', tensor([-21.2621], device='cuda:0'))])
epoch£º288	 i:0 	 global-step:5760	 l-p:0.1922358274459839
epoch£º288	 i:1 	 global-step:5761	 l-p:0.1447029560804367
epoch£º288	 i:2 	 global-step:5762	 l-p:0.14626777172088623
epoch£º288	 i:3 	 global-step:5763	 l-p:0.44037696719169617
epoch£º288	 i:4 	 global-step:5764	 l-p:0.14035053551197052
epoch£º288	 i:5 	 global-step:5765	 l-p:0.1321556270122528
epoch£º288	 i:6 	 global-step:5766	 l-p:0.14759331941604614
epoch£º288	 i:7 	 global-step:5767	 l-p:0.12404002994298935
epoch£º288	 i:8 	 global-step:5768	 l-p:0.09964311122894287
epoch£º288	 i:9 	 global-step:5769	 l-p:0.40330612659454346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9937, 5.3246, 5.3211],
        [4.9937, 4.9933, 4.9937],
        [4.9937, 4.9934, 4.9937],
        [4.9937, 4.9907, 4.9923]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:289, step:0 
model_pd.l_p.mean(): 0.11718014627695084 
model_pd.l_d.mean(): -18.875715255737305 
model_pd.lagr.mean(): -18.758535385131836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5385], device='cuda:0')), ('power', tensor([-19.7864], device='cuda:0'))])
epoch£º289	 i:0 	 global-step:5780	 l-p:0.11718014627695084
epoch£º289	 i:1 	 global-step:5781	 l-p:0.09212059527635574
epoch£º289	 i:2 	 global-step:5782	 l-p:0.8742757439613342
epoch£º289	 i:3 	 global-step:5783	 l-p:0.13638904690742493
epoch£º289	 i:4 	 global-step:5784	 l-p:0.15118923783302307
epoch£º289	 i:5 	 global-step:5785	 l-p:0.7716188430786133
epoch£º289	 i:6 	 global-step:5786	 l-p:0.17787081003189087
epoch£º289	 i:7 	 global-step:5787	 l-p:0.14325574040412903
epoch£º289	 i:8 	 global-step:5788	 l-p:0.12285011261701584
epoch£º289	 i:9 	 global-step:5789	 l-p:0.14161548018455505
====================================================================================================
====================================================================================================
====================================================================================================

epoch:290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9999, 4.9999, 4.9999],
        [4.9999, 4.9987, 4.9997],
        [4.9999, 4.9999, 4.9999],
        [4.9999, 5.0180, 4.9879]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:290, step:0 
model_pd.l_p.mean(): 0.15201793611049652 
model_pd.l_d.mean(): -20.448678970336914 
model_pd.lagr.mean(): -20.296661376953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4192], device='cuda:0')), ('power', tensor([-21.2652], device='cuda:0'))])
epoch£º290	 i:0 	 global-step:5800	 l-p:0.15201793611049652
epoch£º290	 i:1 	 global-step:5801	 l-p:0.1283344328403473
epoch£º290	 i:2 	 global-step:5802	 l-p:0.12757961452007294
epoch£º290	 i:3 	 global-step:5803	 l-p:1.6254253387451172
epoch£º290	 i:4 	 global-step:5804	 l-p:0.1293824315071106
epoch£º290	 i:5 	 global-step:5805	 l-p:0.12815141677856445
epoch£º290	 i:6 	 global-step:5806	 l-p:0.14147570729255676
epoch£º290	 i:7 	 global-step:5807	 l-p:0.24903558194637299
epoch£º290	 i:8 	 global-step:5808	 l-p:0.1517835259437561
epoch£º290	 i:9 	 global-step:5809	 l-p:0.1487838178873062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8558, 4.8787, 4.8393],
        [4.8558, 5.0782, 5.0290],
        [4.8558, 4.8513, 4.8539],
        [4.8558, 4.8557, 4.8558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:291, step:0 
model_pd.l_p.mean(): 0.09881187975406647 
model_pd.l_d.mean(): -20.749759674072266 
model_pd.lagr.mean(): -20.65094757080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4329], device='cuda:0')), ('power', tensor([-21.5861], device='cuda:0'))])
epoch£º291	 i:0 	 global-step:5820	 l-p:0.09881187975406647
epoch£º291	 i:1 	 global-step:5821	 l-p:0.11735440790653229
epoch£º291	 i:2 	 global-step:5822	 l-p:0.12540438771247864
epoch£º291	 i:3 	 global-step:5823	 l-p:0.17259196937084198
epoch£º291	 i:4 	 global-step:5824	 l-p:0.1304149478673935
epoch£º291	 i:5 	 global-step:5825	 l-p:-0.0035656976979225874
epoch£º291	 i:6 	 global-step:5826	 l-p:0.19016334414482117
epoch£º291	 i:7 	 global-step:5827	 l-p:0.16751976311206818
epoch£º291	 i:8 	 global-step:5828	 l-p:0.10247987508773804
epoch£º291	 i:9 	 global-step:5829	 l-p:0.13342492282390594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1037, 5.1037, 5.1037],
        [5.1037, 5.1022, 5.1034],
        [5.1037, 5.1036, 5.1037],
        [5.1037, 5.1037, 5.1037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:292, step:0 
model_pd.l_p.mean(): 0.12945657968521118 
model_pd.l_d.mean(): -19.89073944091797 
model_pd.lagr.mean(): -19.761281967163086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4364], device='cuda:0')), ('power', tensor([-20.7147], device='cuda:0'))])
epoch£º292	 i:0 	 global-step:5840	 l-p:0.12945657968521118
epoch£º292	 i:1 	 global-step:5841	 l-p:0.12488517165184021
epoch£º292	 i:2 	 global-step:5842	 l-p:0.1186162531375885
epoch£º292	 i:3 	 global-step:5843	 l-p:0.11697137355804443
epoch£º292	 i:4 	 global-step:5844	 l-p:0.11576373875141144
epoch£º292	 i:5 	 global-step:5845	 l-p:0.16199882328510284
epoch£º292	 i:6 	 global-step:5846	 l-p:0.11563407629728317
epoch£º292	 i:7 	 global-step:5847	 l-p:0.18708576261997223
epoch£º292	 i:8 	 global-step:5848	 l-p:0.12850221991539001
epoch£º292	 i:9 	 global-step:5849	 l-p:0.12630760669708252
====================================================================================================
====================================================================================================
====================================================================================================

epoch:293
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2542,  0.1610,  1.0000,  0.1020,
          1.0000,  0.6334, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1464,  0.0772,  1.0000,  0.0407,
          1.0000,  0.5270, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1273,  0.0641,  1.0000,  0.0322,
          1.0000,  0.5031, 31.6228]], device='cuda:0')
 pt:tensor([[5.0772, 5.1317, 5.0772],
        [5.0772, 5.1289, 5.0758],
        [5.0772, 5.0820, 5.0679],
        [5.0772, 5.0781, 5.0698]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:293, step:0 
model_pd.l_p.mean(): 0.1276150345802307 
model_pd.l_d.mean(): -20.44383430480957 
model_pd.lagr.mean(): -20.316219329833984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3974], device='cuda:0')), ('power', tensor([-21.2377], device='cuda:0'))])
epoch£º293	 i:0 	 global-step:5860	 l-p:0.1276150345802307
epoch£º293	 i:1 	 global-step:5861	 l-p:0.11507371813058853
epoch£º293	 i:2 	 global-step:5862	 l-p:0.2084171622991562
epoch£º293	 i:3 	 global-step:5863	 l-p:0.2815625071525574
epoch£º293	 i:4 	 global-step:5864	 l-p:0.13101908564567566
epoch£º293	 i:5 	 global-step:5865	 l-p:0.12130342423915863
epoch£º293	 i:6 	 global-step:5866	 l-p:0.3350258469581604
epoch£º293	 i:7 	 global-step:5867	 l-p:0.12248694151639938
epoch£º293	 i:8 	 global-step:5868	 l-p:0.14572936296463013
epoch£º293	 i:9 	 global-step:5869	 l-p:0.2085849940776825
====================================================================================================
====================================================================================================
====================================================================================================

epoch:294
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228]], device='cuda:0')
 pt:tensor([[4.9823, 4.9883, 4.9695],
        [4.9823, 5.2582, 5.2271],
        [4.9823, 5.3302, 5.3365],
        [4.9823, 5.0480, 4.9866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:294, step:0 
model_pd.l_p.mean(): 0.1544206440448761 
model_pd.l_d.mean(): -19.329191207885742 
model_pd.lagr.mean(): -19.17477035522461 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4871], device='cuda:0')), ('power', tensor([-20.1952], device='cuda:0'))])
epoch£º294	 i:0 	 global-step:5880	 l-p:0.1544206440448761
epoch£º294	 i:1 	 global-step:5881	 l-p:0.14024607837200165
epoch£º294	 i:2 	 global-step:5882	 l-p:0.21930885314941406
epoch£º294	 i:3 	 global-step:5883	 l-p:0.12642177939414978
epoch£º294	 i:4 	 global-step:5884	 l-p:0.10751071572303772
epoch£º294	 i:5 	 global-step:5885	 l-p:0.2185366451740265
epoch£º294	 i:6 	 global-step:5886	 l-p:0.14761072397232056
epoch£º294	 i:7 	 global-step:5887	 l-p:0.11079850792884827
epoch£º294	 i:8 	 global-step:5888	 l-p:0.12481321394443512
epoch£º294	 i:9 	 global-step:5889	 l-p:0.19336968660354614
====================================================================================================
====================================================================================================
====================================================================================================

epoch:295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0335, 5.0587, 5.0211],
        [5.0335, 5.0334, 5.0335],
        [5.0335, 5.0329, 5.0335],
        [5.0335, 5.0335, 5.0335]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:295, step:0 
model_pd.l_p.mean(): 0.1370791345834732 
model_pd.l_d.mean(): -20.681262969970703 
model_pd.lagr.mean(): -20.5441837310791 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3942], device='cuda:0')), ('power', tensor([-21.4763], device='cuda:0'))])
epoch£º295	 i:0 	 global-step:5900	 l-p:0.1370791345834732
epoch£º295	 i:1 	 global-step:5901	 l-p:0.12230834364891052
epoch£º295	 i:2 	 global-step:5902	 l-p:0.3967384994029999
epoch£º295	 i:3 	 global-step:5903	 l-p:0.22700317203998566
epoch£º295	 i:4 	 global-step:5904	 l-p:0.2742232382297516
epoch£º295	 i:5 	 global-step:5905	 l-p:0.13492855429649353
epoch£º295	 i:6 	 global-step:5906	 l-p:0.14184756577014923
epoch£º295	 i:7 	 global-step:5907	 l-p:0.12290557473897934
epoch£º295	 i:8 	 global-step:5908	 l-p:0.13508832454681396
epoch£º295	 i:9 	 global-step:5909	 l-p:0.13579268753528595
====================================================================================================
====================================================================================================
====================================================================================================

epoch:296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0881, 5.1292, 5.0813],
        [5.0881, 5.0861, 5.0877],
        [5.0881, 5.0878, 5.0881],
        [5.0881, 5.0881, 5.0881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:296, step:0 
model_pd.l_p.mean(): 0.15172651410102844 
model_pd.l_d.mean(): -20.302330017089844 
model_pd.lagr.mean(): -20.150604248046875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4184], device='cuda:0')), ('power', tensor([-21.1153], device='cuda:0'))])
epoch£º296	 i:0 	 global-step:5920	 l-p:0.15172651410102844
epoch£º296	 i:1 	 global-step:5921	 l-p:0.13234570622444153
epoch£º296	 i:2 	 global-step:5922	 l-p:0.13736137747764587
epoch£º296	 i:3 	 global-step:5923	 l-p:0.22905129194259644
epoch£º296	 i:4 	 global-step:5924	 l-p:0.2144383192062378
epoch£º296	 i:5 	 global-step:5925	 l-p:0.1093854233622551
epoch£º296	 i:6 	 global-step:5926	 l-p:0.12977586686611176
epoch£º296	 i:7 	 global-step:5927	 l-p:0.14108672738075256
epoch£º296	 i:8 	 global-step:5928	 l-p:0.1385195106267929
epoch£º296	 i:9 	 global-step:5929	 l-p:0.1162320002913475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:297
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8968, 5.5553, 5.8025],
        [4.8968, 4.9068, 4.8785],
        [4.8968, 4.8920, 4.8949],
        [4.8968, 4.8964, 4.8968]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:297, step:0 
model_pd.l_p.mean(): 0.07465442270040512 
model_pd.l_d.mean(): -19.37114906311035 
model_pd.lagr.mean(): -19.29649543762207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5428], device='cuda:0')), ('power', tensor([-20.2956], device='cuda:0'))])
epoch£º297	 i:0 	 global-step:5940	 l-p:0.07465442270040512
epoch£º297	 i:1 	 global-step:5941	 l-p:0.12929625809192657
epoch£º297	 i:2 	 global-step:5942	 l-p:0.14230863749980927
epoch£º297	 i:3 	 global-step:5943	 l-p:0.14071546494960785
epoch£º297	 i:4 	 global-step:5944	 l-p:0.14235778152942657
epoch£º297	 i:5 	 global-step:5945	 l-p:0.1544555276632309
epoch£º297	 i:6 	 global-step:5946	 l-p:0.0824957937002182
epoch£º297	 i:7 	 global-step:5947	 l-p:0.12430791556835175
epoch£º297	 i:8 	 global-step:5948	 l-p:0.1426997184753418
epoch£º297	 i:9 	 global-step:5949	 l-p:0.1637321263551712
====================================================================================================
====================================================================================================
====================================================================================================

epoch:298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7989, 4.7989, 4.7989],
        [4.7989, 4.7909, 4.7886],
        [4.7989, 4.7988, 4.7989],
        [4.7989, 5.5969, 5.9841]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:298, step:0 
model_pd.l_p.mean(): 0.1408066302537918 
model_pd.l_d.mean(): -19.684053421020508 
model_pd.lagr.mean(): -19.54324722290039 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5312], device='cuda:0')), ('power', tensor([-20.6023], device='cuda:0'))])
epoch£º298	 i:0 	 global-step:5960	 l-p:0.1408066302537918
epoch£º298	 i:1 	 global-step:5961	 l-p:0.512911856174469
epoch£º298	 i:2 	 global-step:5962	 l-p:0.15193134546279907
epoch£º298	 i:3 	 global-step:5963	 l-p:0.12806034088134766
epoch£º298	 i:4 	 global-step:5964	 l-p:0.16505178809165955
epoch£º298	 i:5 	 global-step:5965	 l-p:0.12496557086706161
epoch£º298	 i:6 	 global-step:5966	 l-p:1.2016446590423584
epoch£º298	 i:7 	 global-step:5967	 l-p:0.14682568609714508
epoch£º298	 i:8 	 global-step:5968	 l-p:0.1270049512386322
epoch£º298	 i:9 	 global-step:5969	 l-p:0.17798751592636108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0765, 5.0734, 5.0755],
        [5.0765, 5.0836, 5.0638],
        [5.0765, 5.1258, 5.0714],
        [5.0765, 5.2096, 5.1374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:299, step:0 
model_pd.l_p.mean(): 0.11820285767316818 
model_pd.l_d.mean(): -20.294870376586914 
model_pd.lagr.mean(): -20.176668167114258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4289], device='cuda:0')), ('power', tensor([-21.1186], device='cuda:0'))])
epoch£º299	 i:0 	 global-step:5980	 l-p:0.11820285767316818
epoch£º299	 i:1 	 global-step:5981	 l-p:0.16006240248680115
epoch£º299	 i:2 	 global-step:5982	 l-p:0.17749692499637604
epoch£º299	 i:3 	 global-step:5983	 l-p:0.12658320367336273
epoch£º299	 i:4 	 global-step:5984	 l-p:0.1318213939666748
epoch£º299	 i:5 	 global-step:5985	 l-p:0.14570091664791107
epoch£º299	 i:6 	 global-step:5986	 l-p:0.1333538442850113
epoch£º299	 i:7 	 global-step:5987	 l-p:1.3947874307632446
epoch£º299	 i:8 	 global-step:5988	 l-p:0.10000539571046829
epoch£º299	 i:9 	 global-step:5989	 l-p:-2.726545572280884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:300
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9822, 5.6521, 5.9002],
        [4.9822, 4.9766, 4.9765],
        [4.9822, 4.9822, 4.9822],
        [4.9822, 4.9766, 4.9769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:300, step:0 
model_pd.l_p.mean(): 0.18706166744232178 
model_pd.l_d.mean(): -20.507728576660156 
model_pd.lagr.mean(): -20.320667266845703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4308], device='cuda:0')), ('power', tensor([-21.3374], device='cuda:0'))])
epoch£º300	 i:0 	 global-step:6000	 l-p:0.18706166744232178
epoch£º300	 i:1 	 global-step:6001	 l-p:0.6502202749252319
epoch£º300	 i:2 	 global-step:6002	 l-p:0.13861624896526337
epoch£º300	 i:3 	 global-step:6003	 l-p:0.13515758514404297
epoch£º300	 i:4 	 global-step:6004	 l-p:0.1341703087091446
epoch£º300	 i:5 	 global-step:6005	 l-p:0.12756145000457764
epoch£º300	 i:6 	 global-step:6006	 l-p:0.11850404739379883
epoch£º300	 i:7 	 global-step:6007	 l-p:0.2054058462381363
epoch£º300	 i:8 	 global-step:6008	 l-p:0.1269480437040329
epoch£º300	 i:9 	 global-step:6009	 l-p:0.14033040404319763
====================================================================================================
====================================================================================================
====================================================================================================

epoch:301
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9859, 4.9872, 4.9716],
        [4.9859, 4.9859, 4.9859],
        [4.9859, 5.1314, 5.0597],
        [4.9859, 4.9859, 4.9859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:301, step:0 
model_pd.l_p.mean(): -1.53016197681427 
model_pd.l_d.mean(): -19.07976531982422 
model_pd.lagr.mean(): -20.609928131103516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5100], device='cuda:0')), ('power', tensor([-19.9648], device='cuda:0'))])
epoch£º301	 i:0 	 global-step:6020	 l-p:-1.53016197681427
epoch£º301	 i:1 	 global-step:6021	 l-p:0.12423750013113022
epoch£º301	 i:2 	 global-step:6022	 l-p:0.11635606735944748
epoch£º301	 i:3 	 global-step:6023	 l-p:0.14428777992725372
epoch£º301	 i:4 	 global-step:6024	 l-p:0.08520889282226562
epoch£º301	 i:5 	 global-step:6025	 l-p:0.13672764599323273
epoch£º301	 i:6 	 global-step:6026	 l-p:0.139895960688591
epoch£º301	 i:7 	 global-step:6027	 l-p:1.6536195278167725
epoch£º301	 i:8 	 global-step:6028	 l-p:0.24159148335456848
epoch£º301	 i:9 	 global-step:6029	 l-p:0.044788897037506104
====================================================================================================
====================================================================================================
====================================================================================================

epoch:302
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8796, 4.8824, 4.8596],
        [4.8796, 4.8790, 4.8796],
        [4.8796, 5.4519, 5.6272],
        [4.8796, 4.8790, 4.8796]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:302, step:0 
model_pd.l_p.mean(): 0.14433488249778748 
model_pd.l_d.mean(): -20.57063865661621 
model_pd.lagr.mean(): -20.42630386352539 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4511], device='cuda:0')), ('power', tensor([-21.4226], device='cuda:0'))])
epoch£º302	 i:0 	 global-step:6040	 l-p:0.14433488249778748
epoch£º302	 i:1 	 global-step:6041	 l-p:0.18065644800662994
epoch£º302	 i:2 	 global-step:6042	 l-p:0.12157147377729416
epoch£º302	 i:3 	 global-step:6043	 l-p:0.07634810358285904
epoch£º302	 i:4 	 global-step:6044	 l-p:0.14944180846214294
epoch£º302	 i:5 	 global-step:6045	 l-p:0.13575883209705353
epoch£º302	 i:6 	 global-step:6046	 l-p:1.7239946126937866
epoch£º302	 i:7 	 global-step:6047	 l-p:0.14841832220554352
epoch£º302	 i:8 	 global-step:6048	 l-p:0.14777471125125885
epoch£º302	 i:9 	 global-step:6049	 l-p:0.12237199395895004
====================================================================================================
====================================================================================================
====================================================================================================

epoch:303
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8781, 4.8781, 4.8781],
        [4.8781, 4.8772, 4.8593],
        [4.8781, 5.6647, 6.0313],
        [4.8781, 4.9037, 4.8567]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:303, step:0 
model_pd.l_p.mean(): 0.14291128516197205 
model_pd.l_d.mean(): -19.014142990112305 
model_pd.lagr.mean(): -18.871231079101562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5330], device='cuda:0')), ('power', tensor([-19.9217], device='cuda:0'))])
epoch£º303	 i:0 	 global-step:6060	 l-p:0.14291128516197205
epoch£º303	 i:1 	 global-step:6061	 l-p:0.1824079155921936
epoch£º303	 i:2 	 global-step:6062	 l-p:0.14038167893886566
epoch£º303	 i:3 	 global-step:6063	 l-p:-0.11506367474794388
epoch£º303	 i:4 	 global-step:6064	 l-p:0.2996031641960144
epoch£º303	 i:5 	 global-step:6065	 l-p:0.09735427796840668
epoch£º303	 i:6 	 global-step:6066	 l-p:0.1427386850118637
epoch£º303	 i:7 	 global-step:6067	 l-p:0.13313762843608856
epoch£º303	 i:8 	 global-step:6068	 l-p:0.14111000299453735
epoch£º303	 i:9 	 global-step:6069	 l-p:0.09844646602869034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:304
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1380, 5.1380, 5.1380],
        [5.1380, 5.1380, 5.1380],
        [5.1380, 5.1376, 5.1380],
        [5.1380, 5.1379, 5.1380]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:304, step:0 
model_pd.l_p.mean(): 0.12620772421360016 
model_pd.l_d.mean(): -18.921220779418945 
model_pd.lagr.mean(): -18.795013427734375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4559], device='cuda:0')), ('power', tensor([-19.7472], device='cuda:0'))])
epoch£º304	 i:0 	 global-step:6080	 l-p:0.12620772421360016
epoch£º304	 i:1 	 global-step:6081	 l-p:0.11101676523685455
epoch£º304	 i:2 	 global-step:6082	 l-p:0.12703996896743774
epoch£º304	 i:3 	 global-step:6083	 l-p:0.1410532295703888
epoch£º304	 i:4 	 global-step:6084	 l-p:0.15245413780212402
epoch£º304	 i:5 	 global-step:6085	 l-p:0.16113467514514923
epoch£º304	 i:6 	 global-step:6086	 l-p:0.062115300446748734
epoch£º304	 i:7 	 global-step:6087	 l-p:0.14249998331069946
epoch£º304	 i:8 	 global-step:6088	 l-p:0.12488079071044922
epoch£º304	 i:9 	 global-step:6089	 l-p:0.13100068271160126
====================================================================================================
====================================================================================================
====================================================================================================

epoch:305
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1259, 5.1716, 5.1177],
        [5.1259, 5.1258, 5.1259],
        [5.1259, 5.8180, 6.0708],
        [5.1259, 5.1269, 5.1144]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:305, step:0 
model_pd.l_p.mean(): 0.13407202064990997 
model_pd.l_d.mean(): -19.993972778320312 
model_pd.lagr.mean(): -19.859901428222656 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4458], device='cuda:0')), ('power', tensor([-20.8296], device='cuda:0'))])
epoch£º305	 i:0 	 global-step:6100	 l-p:0.13407202064990997
epoch£º305	 i:1 	 global-step:6101	 l-p:0.12860889732837677
epoch£º305	 i:2 	 global-step:6102	 l-p:0.09972935169935226
epoch£º305	 i:3 	 global-step:6103	 l-p:0.33808475732803345
epoch£º305	 i:4 	 global-step:6104	 l-p:0.347158282995224
epoch£º305	 i:5 	 global-step:6105	 l-p:0.13162456452846527
epoch£º305	 i:6 	 global-step:6106	 l-p:0.14091967046260834
epoch£º305	 i:7 	 global-step:6107	 l-p:-4.337325096130371
epoch£º305	 i:8 	 global-step:6108	 l-p:0.11751758307218552
epoch£º305	 i:9 	 global-step:6109	 l-p:0.11896853893995285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:306
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9765, 4.9765, 4.9765],
        [4.9765, 5.5975, 5.8042],
        [4.9765, 4.9765, 4.9765],
        [4.9765, 5.0609, 4.9892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:306, step:0 
model_pd.l_p.mean(): -0.80903559923172 
model_pd.l_d.mean(): -18.893220901489258 
model_pd.lagr.mean(): -19.70225715637207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5389], device='cuda:0')), ('power', tensor([-19.8047], device='cuda:0'))])
epoch£º306	 i:0 	 global-step:6120	 l-p:-0.80903559923172
epoch£º306	 i:1 	 global-step:6121	 l-p:0.12900996208190918
epoch£º306	 i:2 	 global-step:6122	 l-p:0.13753874599933624
epoch£º306	 i:3 	 global-step:6123	 l-p:0.16627059876918793
epoch£º306	 i:4 	 global-step:6124	 l-p:0.17084941267967224
epoch£º306	 i:5 	 global-step:6125	 l-p:0.14831575751304626
epoch£º306	 i:6 	 global-step:6126	 l-p:0.15816178917884827
epoch£º306	 i:7 	 global-step:6127	 l-p:0.12795765697956085
epoch£º306	 i:8 	 global-step:6128	 l-p:0.11165334284305573
epoch£º306	 i:9 	 global-step:6129	 l-p:0.09626080095767975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:307
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9070, 4.9070, 4.9070],
        [4.9070, 4.8983, 4.8979],
        [4.9070, 5.3036, 5.3439],
        [4.9070, 4.8983, 4.8994]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:307, step:0 
model_pd.l_p.mean(): 0.14759400486946106 
model_pd.l_d.mean(): -20.451784133911133 
model_pd.lagr.mean(): -20.304189682006836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4641], device='cuda:0')), ('power', tensor([-21.3148], device='cuda:0'))])
epoch£º307	 i:0 	 global-step:6140	 l-p:0.14759400486946106
epoch£º307	 i:1 	 global-step:6141	 l-p:0.13760067522525787
epoch£º307	 i:2 	 global-step:6142	 l-p:0.12197158485651016
epoch£º307	 i:3 	 global-step:6143	 l-p:0.14099988341331482
epoch£º307	 i:4 	 global-step:6144	 l-p:0.13841398060321808
epoch£º307	 i:5 	 global-step:6145	 l-p:0.10233618319034576
epoch£º307	 i:6 	 global-step:6146	 l-p:0.23477639257907867
epoch£º307	 i:7 	 global-step:6147	 l-p:0.1357107013463974
epoch£º307	 i:8 	 global-step:6148	 l-p:0.14768086373806
epoch£º307	 i:9 	 global-step:6149	 l-p:0.1241922527551651
====================================================================================================
====================================================================================================
====================================================================================================

epoch:308
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9372, 4.9328, 4.9361],
        [4.9372, 4.9315, 4.9352],
        [4.9372, 4.9291, 4.9257],
        [4.9372, 5.0095, 4.9387]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:308, step:0 
model_pd.l_p.mean(): 0.12775306403636932 
model_pd.l_d.mean(): -20.800251007080078 
model_pd.lagr.mean(): -20.67249870300293 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4045], device='cuda:0')), ('power', tensor([-21.6081], device='cuda:0'))])
epoch£º308	 i:0 	 global-step:6160	 l-p:0.12775306403636932
epoch£º308	 i:1 	 global-step:6161	 l-p:0.06672250479459763
epoch£º308	 i:2 	 global-step:6162	 l-p:0.1314336210489273
epoch£º308	 i:3 	 global-step:6163	 l-p:0.1413983702659607
epoch£º308	 i:4 	 global-step:6164	 l-p:0.1455748975276947
epoch£º308	 i:5 	 global-step:6165	 l-p:0.17257992923259735
epoch£º308	 i:6 	 global-step:6166	 l-p:-0.04682915657758713
epoch£º308	 i:7 	 global-step:6167	 l-p:0.20306439697742462
epoch£º308	 i:8 	 global-step:6168	 l-p:0.12794817984104156
epoch£º308	 i:9 	 global-step:6169	 l-p:0.13205955922603607
====================================================================================================
====================================================================================================
====================================================================================================

epoch:309
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9582, 4.9635, 4.9356],
        [4.9582, 5.1848, 5.1316],
        [4.9582, 4.9581, 4.9582],
        [4.9582, 5.3186, 5.3326]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:309, step:0 
model_pd.l_p.mean(): 0.19937127828598022 
model_pd.l_d.mean(): -18.910133361816406 
model_pd.lagr.mean(): -18.71076202392578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5299], device='cuda:0')), ('power', tensor([-19.8126], device='cuda:0'))])
epoch£º309	 i:0 	 global-step:6180	 l-p:0.19937127828598022
epoch£º309	 i:1 	 global-step:6181	 l-p:-0.15949195623397827
epoch£º309	 i:2 	 global-step:6182	 l-p:0.1426805555820465
epoch£º309	 i:3 	 global-step:6183	 l-p:0.12558336555957794
epoch£º309	 i:4 	 global-step:6184	 l-p:0.13862745463848114
epoch£º309	 i:5 	 global-step:6185	 l-p:0.13584548234939575
epoch£º309	 i:6 	 global-step:6186	 l-p:0.14085745811462402
epoch£º309	 i:7 	 global-step:6187	 l-p:0.3495289385318756
epoch£º309	 i:8 	 global-step:6188	 l-p:0.2509053647518158
epoch£º309	 i:9 	 global-step:6189	 l-p:0.13565589487552643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:310
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0851, 5.0818, 5.0843],
        [5.0851, 5.0850, 5.0851],
        [5.0851, 5.3195, 5.2661],
        [5.0851, 5.5979, 5.7082]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:310, step:0 
model_pd.l_p.mean(): 0.1831873655319214 
model_pd.l_d.mean(): -20.575103759765625 
model_pd.lagr.mean(): -20.391916275024414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3967], device='cuda:0')), ('power', tensor([-21.3707], device='cuda:0'))])
epoch£º310	 i:0 	 global-step:6200	 l-p:0.1831873655319214
epoch£º310	 i:1 	 global-step:6201	 l-p:0.0830855667591095
epoch£º310	 i:2 	 global-step:6202	 l-p:0.12323584407567978
epoch£º310	 i:3 	 global-step:6203	 l-p:0.11442188918590546
epoch£º310	 i:4 	 global-step:6204	 l-p:0.14563654363155365
epoch£º310	 i:5 	 global-step:6205	 l-p:0.13548476994037628
epoch£º310	 i:6 	 global-step:6206	 l-p:0.1460530310869217
epoch£º310	 i:7 	 global-step:6207	 l-p:0.1296945959329605
epoch£º310	 i:8 	 global-step:6208	 l-p:0.10264910757541656
epoch£º310	 i:9 	 global-step:6209	 l-p:0.13257695734500885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:311
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2563,  0.1628,  1.0000,  0.1034,
          1.0000,  0.6352, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[5.1132, 5.1563, 5.1007],
        [5.1132, 5.1526, 5.0992],
        [5.1132, 5.1210, 5.0953],
        [5.1132, 5.1807, 5.1139]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:311, step:0 
model_pd.l_p.mean(): 0.1136312186717987 
model_pd.l_d.mean(): -18.916181564331055 
model_pd.lagr.mean(): -18.80255126953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4778], device='cuda:0')), ('power', tensor([-19.7648], device='cuda:0'))])
epoch£º311	 i:0 	 global-step:6220	 l-p:0.1136312186717987
epoch£º311	 i:1 	 global-step:6221	 l-p:0.1856415867805481
epoch£º311	 i:2 	 global-step:6222	 l-p:0.1548919975757599
epoch£º311	 i:3 	 global-step:6223	 l-p:0.12950144708156586
epoch£º311	 i:4 	 global-step:6224	 l-p:0.12005565315485
epoch£º311	 i:5 	 global-step:6225	 l-p:0.11064612865447998
epoch£º311	 i:6 	 global-step:6226	 l-p:0.13469471037387848
epoch£º311	 i:7 	 global-step:6227	 l-p:0.19998790323734283
epoch£º311	 i:8 	 global-step:6228	 l-p:0.14130868017673492
epoch£º311	 i:9 	 global-step:6229	 l-p:0.13277891278266907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:312
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0469, 5.0872, 5.0307],
        [5.0469, 5.0438, 5.0463],
        [5.0469, 5.0469, 5.0469],
        [5.0469, 5.5253, 5.6129]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:312, step:0 
model_pd.l_p.mean(): 0.22774343192577362 
model_pd.l_d.mean(): -20.58000373840332 
model_pd.lagr.mean(): -20.35226058959961 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4068], device='cuda:0')), ('power', tensor([-21.3861], device='cuda:0'))])
epoch£º312	 i:0 	 global-step:6240	 l-p:0.22774343192577362
epoch£º312	 i:1 	 global-step:6241	 l-p:0.10008148103952408
epoch£º312	 i:2 	 global-step:6242	 l-p:0.10529325157403946
epoch£º312	 i:3 	 global-step:6243	 l-p:0.1702180653810501
epoch£º312	 i:4 	 global-step:6244	 l-p:0.14175176620483398
epoch£º312	 i:5 	 global-step:6245	 l-p:-0.059399280697107315
epoch£º312	 i:6 	 global-step:6246	 l-p:0.15050746500492096
epoch£º312	 i:7 	 global-step:6247	 l-p:0.13086186349391937
epoch£º312	 i:8 	 global-step:6248	 l-p:0.1364513337612152
epoch£º312	 i:9 	 global-step:6249	 l-p:0.015803970396518707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:313
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9529, 4.9433, 4.9443],
        [4.9529, 5.5259, 5.6948],
        [4.9529, 4.9850, 4.9299],
        [4.9529, 4.9476, 4.9514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:313, step:0 
model_pd.l_p.mean(): 0.15298748016357422 
model_pd.l_d.mean(): -20.521743774414062 
model_pd.lagr.mean(): -20.368755340576172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4418], device='cuda:0')), ('power', tensor([-21.3631], device='cuda:0'))])
epoch£º313	 i:0 	 global-step:6260	 l-p:0.15298748016357422
epoch£º313	 i:1 	 global-step:6261	 l-p:0.1355365365743637
epoch£º313	 i:2 	 global-step:6262	 l-p:-0.03876945748925209
epoch£º313	 i:3 	 global-step:6263	 l-p:0.7353020310401917
epoch£º313	 i:4 	 global-step:6264	 l-p:0.15461209416389465
epoch£º313	 i:5 	 global-step:6265	 l-p:0.12468846142292023
epoch£º313	 i:6 	 global-step:6266	 l-p:0.23371827602386475
epoch£º313	 i:7 	 global-step:6267	 l-p:0.13651183247566223
epoch£º313	 i:8 	 global-step:6268	 l-p:0.10005216300487518
epoch£º313	 i:9 	 global-step:6269	 l-p:0.0755976065993309
====================================================================================================
====================================================================================================
====================================================================================================

epoch:314
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8843, 4.8843, 4.8843],
        [4.8843, 4.8773, 4.8819],
        [4.8843, 4.8738, 4.8675],
        [4.8843, 4.8786, 4.8604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:314, step:0 
model_pd.l_p.mean(): 0.09477800875902176 
model_pd.l_d.mean(): -19.84953498840332 
model_pd.lagr.mean(): -19.754756927490234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5428], device='cuda:0')), ('power', tensor([-20.7829], device='cuda:0'))])
epoch£º314	 i:0 	 global-step:6280	 l-p:0.09477800875902176
epoch£º314	 i:1 	 global-step:6281	 l-p:0.14411099255084991
epoch£º314	 i:2 	 global-step:6282	 l-p:0.17085540294647217
epoch£º314	 i:3 	 global-step:6283	 l-p:0.13008058071136475
epoch£º314	 i:4 	 global-step:6284	 l-p:0.059248704463243484
epoch£º314	 i:5 	 global-step:6285	 l-p:-0.0048258304595947266
epoch£º314	 i:6 	 global-step:6286	 l-p:0.14269696176052094
epoch£º314	 i:7 	 global-step:6287	 l-p:0.13113240897655487
epoch£º314	 i:8 	 global-step:6288	 l-p:0.11617901176214218
epoch£º314	 i:9 	 global-step:6289	 l-p:0.14345119893550873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:315
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0934, 5.3023, 5.2394],
        [5.0934, 5.1236, 5.0740],
        [5.0934, 5.0934, 5.0934],
        [5.0934, 5.5116, 5.5544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:315, step:0 
model_pd.l_p.mean(): 0.13035593926906586 
model_pd.l_d.mean(): -20.051820755004883 
model_pd.lagr.mean(): -19.921464920043945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4342], device='cuda:0')), ('power', tensor([-20.8765], device='cuda:0'))])
epoch£º315	 i:0 	 global-step:6300	 l-p:0.13035593926906586
epoch£º315	 i:1 	 global-step:6301	 l-p:0.14134827256202698
epoch£º315	 i:2 	 global-step:6302	 l-p:0.111370749771595
epoch£º315	 i:3 	 global-step:6303	 l-p:0.12980294227600098
epoch£º315	 i:4 	 global-step:6304	 l-p:0.13363789021968842
epoch£º315	 i:5 	 global-step:6305	 l-p:0.21358993649482727
epoch£º315	 i:6 	 global-step:6306	 l-p:0.22503745555877686
epoch£º315	 i:7 	 global-step:6307	 l-p:0.15601018071174622
epoch£º315	 i:8 	 global-step:6308	 l-p:0.1197008267045021
epoch£º315	 i:9 	 global-step:6309	 l-p:0.13527412712574005
====================================================================================================
====================================================================================================
====================================================================================================

epoch:316
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0114, 5.0068, 5.0102],
        [5.0114, 5.0105, 5.0113],
        [5.0114, 5.0614, 4.9970],
        [5.0114, 5.0023, 5.0020]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:316, step:0 
model_pd.l_p.mean(): 0.13764746487140656 
model_pd.l_d.mean(): -20.795194625854492 
model_pd.lagr.mean(): -20.657546997070312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3845], device='cuda:0')), ('power', tensor([-21.5823], device='cuda:0'))])
epoch£º316	 i:0 	 global-step:6320	 l-p:0.13764746487140656
epoch£º316	 i:1 	 global-step:6321	 l-p:0.033288147300481796
epoch£º316	 i:2 	 global-step:6322	 l-p:0.21585644781589508
epoch£º316	 i:3 	 global-step:6323	 l-p:0.1416563242673874
epoch£º316	 i:4 	 global-step:6324	 l-p:0.1187424436211586
epoch£º316	 i:5 	 global-step:6325	 l-p:0.14388751983642578
epoch£º316	 i:6 	 global-step:6326	 l-p:0.09087134897708893
epoch£º316	 i:7 	 global-step:6327	 l-p:0.23000676929950714
epoch£º316	 i:8 	 global-step:6328	 l-p:0.12785604596138
epoch£º316	 i:9 	 global-step:6329	 l-p:0.01859120838344097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:317
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8443, 4.8313, 4.8317],
        [4.8443, 4.8436, 4.8443],
        [4.8443, 4.8396, 4.8151],
        [4.8443, 4.8354, 4.8189]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:317, step:0 
model_pd.l_p.mean(): 0.10982271283864975 
model_pd.l_d.mean(): -20.212326049804688 
model_pd.lagr.mean(): -20.102502822875977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5100], device='cuda:0')), ('power', tensor([-21.1185], device='cuda:0'))])
epoch£º317	 i:0 	 global-step:6340	 l-p:0.10982271283864975
epoch£º317	 i:1 	 global-step:6341	 l-p:0.14433793723583221
epoch£º317	 i:2 	 global-step:6342	 l-p:0.1283358484506607
epoch£º317	 i:3 	 global-step:6343	 l-p:0.06863560527563095
epoch£º317	 i:4 	 global-step:6344	 l-p:0.14019176363945007
epoch£º317	 i:5 	 global-step:6345	 l-p:0.3807465732097626
epoch£º317	 i:6 	 global-step:6346	 l-p:0.12527737021446228
epoch£º317	 i:7 	 global-step:6347	 l-p:0.12402717024087906
epoch£º317	 i:8 	 global-step:6348	 l-p:0.14232030510902405
epoch£º317	 i:9 	 global-step:6349	 l-p:0.1255260556936264
====================================================================================================
====================================================================================================
====================================================================================================

epoch:318
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9749, 4.9649, 4.9625],
        [4.9749, 4.9748, 4.9749],
        [4.9749, 4.9699, 4.9736],
        [4.9749, 4.9745, 4.9749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:318, step:0 
model_pd.l_p.mean(): 0.20233440399169922 
model_pd.l_d.mean(): -20.070899963378906 
model_pd.lagr.mean(): -19.86856460571289 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4783], device='cuda:0')), ('power', tensor([-20.9416], device='cuda:0'))])
epoch£º318	 i:0 	 global-step:6360	 l-p:0.20233440399169922
epoch£º318	 i:1 	 global-step:6361	 l-p:0.1360604614019394
epoch£º318	 i:2 	 global-step:6362	 l-p:0.12253432720899582
epoch£º318	 i:3 	 global-step:6363	 l-p:0.15567173063755035
epoch£º318	 i:4 	 global-step:6364	 l-p:0.12850214540958405
epoch£º318	 i:5 	 global-step:6365	 l-p:0.34375816583633423
epoch£º318	 i:6 	 global-step:6366	 l-p:0.6804080009460449
epoch£º318	 i:7 	 global-step:6367	 l-p:0.126078262925148
epoch£º318	 i:8 	 global-step:6368	 l-p:0.12014790624380112
epoch£º318	 i:9 	 global-step:6369	 l-p:0.1228586807847023
====================================================================================================
====================================================================================================
====================================================================================================

epoch:319
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1207, 5.1230, 5.1010],
        [5.1207, 5.1466, 5.0995],
        [5.1207, 5.1207, 5.1207],
        [5.1207, 5.1207, 5.1207]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:319, step:0 
model_pd.l_p.mean(): 0.16117610037326813 
model_pd.l_d.mean(): -20.33989715576172 
model_pd.lagr.mean(): -20.178720474243164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4146], device='cuda:0')), ('power', tensor([-21.1497], device='cuda:0'))])
epoch£º319	 i:0 	 global-step:6380	 l-p:0.16117610037326813
epoch£º319	 i:1 	 global-step:6381	 l-p:0.11462534964084625
epoch£º319	 i:2 	 global-step:6382	 l-p:0.12566812336444855
epoch£º319	 i:3 	 global-step:6383	 l-p:0.13377587497234344
epoch£º319	 i:4 	 global-step:6384	 l-p:0.13946318626403809
epoch£º319	 i:5 	 global-step:6385	 l-p:0.12536795437335968
epoch£º319	 i:6 	 global-step:6386	 l-p:0.006040496751666069
epoch£º319	 i:7 	 global-step:6387	 l-p:0.05118757113814354
epoch£º319	 i:8 	 global-step:6388	 l-p:0.19947487115859985
epoch£º319	 i:9 	 global-step:6389	 l-p:0.4214552938938141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:320
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9517, 4.9633, 4.9213],
        [4.9517, 5.0083, 4.9375],
        [4.9517, 4.9956, 4.9303],
        [4.9517, 4.9492, 4.9514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:320, step:0 
model_pd.l_p.mean(): 0.1858072280883789 
model_pd.l_d.mean(): -20.017227172851562 
model_pd.lagr.mean(): -19.8314208984375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4552], device='cuda:0')), ('power', tensor([-20.8630], device='cuda:0'))])
epoch£º320	 i:0 	 global-step:6400	 l-p:0.1858072280883789
epoch£º320	 i:1 	 global-step:6401	 l-p:0.1492880880832672
epoch£º320	 i:2 	 global-step:6402	 l-p:0.1050659567117691
epoch£º320	 i:3 	 global-step:6403	 l-p:0.46374621987342834
epoch£º320	 i:4 	 global-step:6404	 l-p:0.12473393976688385
epoch£º320	 i:5 	 global-step:6405	 l-p:0.15249043703079224
epoch£º320	 i:6 	 global-step:6406	 l-p:0.12915346026420593
epoch£º320	 i:7 	 global-step:6407	 l-p:0.22617852687835693
epoch£º320	 i:8 	 global-step:6408	 l-p:0.21889346837997437
epoch£º320	 i:9 	 global-step:6409	 l-p:0.1413039267063141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:321
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0858, 5.0779, 5.0814],
        [5.0858, 5.4153, 5.4045],
        [5.0858, 5.0828, 5.0853],
        [5.0858, 5.0856, 5.0858]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:321, step:0 
model_pd.l_p.mean(): 0.11610903590917587 
model_pd.l_d.mean(): -19.419509887695312 
model_pd.lagr.mean(): -19.30340003967285 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4740], device='cuda:0')), ('power', tensor([-20.2736], device='cuda:0'))])
epoch£º321	 i:0 	 global-step:6420	 l-p:0.11610903590917587
epoch£º321	 i:1 	 global-step:6421	 l-p:0.23262667655944824
epoch£º321	 i:2 	 global-step:6422	 l-p:0.11193113029003143
epoch£º321	 i:3 	 global-step:6423	 l-p:0.11739669740200043
epoch£º321	 i:4 	 global-step:6424	 l-p:0.12217053025960922
epoch£º321	 i:5 	 global-step:6425	 l-p:0.19030559062957764
epoch£º321	 i:6 	 global-step:6426	 l-p:0.12168004363775253
epoch£º321	 i:7 	 global-step:6427	 l-p:0.18431594967842102
epoch£º321	 i:8 	 global-step:6428	 l-p:0.13483260571956635
epoch£º321	 i:9 	 global-step:6429	 l-p:0.14436785876750946
====================================================================================================
====================================================================================================
====================================================================================================

epoch:322
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0473, 5.8914, 6.2914],
        [5.0473, 5.0473, 5.0473],
        [5.0473, 5.0470, 5.0473],
        [5.0473, 5.0473, 5.0473]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:322, step:0 
model_pd.l_p.mean(): 0.1592467874288559 
model_pd.l_d.mean(): -20.46186637878418 
model_pd.lagr.mean(): -20.30261993408203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4168], device='cuda:0')), ('power', tensor([-21.2762], device='cuda:0'))])
epoch£º322	 i:0 	 global-step:6440	 l-p:0.1592467874288559
epoch£º322	 i:1 	 global-step:6441	 l-p:0.14181779325008392
epoch£º322	 i:2 	 global-step:6442	 l-p:0.11698459088802338
epoch£º322	 i:3 	 global-step:6443	 l-p:0.2400539368391037
epoch£º322	 i:4 	 global-step:6444	 l-p:0.11307206004858017
epoch£º322	 i:5 	 global-step:6445	 l-p:0.1458854079246521
epoch£º322	 i:6 	 global-step:6446	 l-p:0.13463547825813293
epoch£º322	 i:7 	 global-step:6447	 l-p:0.2040957361459732
epoch£º322	 i:8 	 global-step:6448	 l-p:0.0881962776184082
epoch£º322	 i:9 	 global-step:6449	 l-p:0.10800383239984512
====================================================================================================
====================================================================================================
====================================================================================================

epoch:323
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9091, 4.9090, 4.9091],
        [4.9091, 4.9054, 4.9085],
        [4.9091, 5.0405, 4.9603],
        [4.9091, 4.8977, 4.9026]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:323, step:0 
model_pd.l_p.mean(): 0.09692903608083725 
model_pd.l_d.mean(): -20.24623680114746 
model_pd.lagr.mean(): -20.149307250976562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4893], device='cuda:0')), ('power', tensor([-21.1316], device='cuda:0'))])
epoch£º323	 i:0 	 global-step:6460	 l-p:0.09692903608083725
epoch£º323	 i:1 	 global-step:6461	 l-p:0.19156648218631744
epoch£º323	 i:2 	 global-step:6462	 l-p:0.10640862584114075
epoch£º323	 i:3 	 global-step:6463	 l-p:0.43444591760635376
epoch£º323	 i:4 	 global-step:6464	 l-p:0.13578085601329803
epoch£º323	 i:5 	 global-step:6465	 l-p:0.12603870034217834
epoch£º323	 i:6 	 global-step:6466	 l-p:0.12482793629169464
epoch£º323	 i:7 	 global-step:6467	 l-p:0.12110599130392075
epoch£º323	 i:8 	 global-step:6468	 l-p:0.1195441335439682
epoch£º323	 i:9 	 global-step:6469	 l-p:0.13224318623542786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:324
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1473, 5.1473, 5.1473],
        [5.1473, 5.1473, 5.1473],
        [5.1473, 5.1418, 5.1456],
        [5.1473, 5.1486, 5.1255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:324, step:0 
model_pd.l_p.mean(): 0.14959672093391418 
model_pd.l_d.mean(): -19.53606605529785 
model_pd.lagr.mean(): -19.3864688873291 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5015], device='cuda:0')), ('power', tensor([-20.4208], device='cuda:0'))])
epoch£º324	 i:0 	 global-step:6480	 l-p:0.14959672093391418
epoch£º324	 i:1 	 global-step:6481	 l-p:0.12331875413656235
epoch£º324	 i:2 	 global-step:6482	 l-p:0.16648532450199127
epoch£º324	 i:3 	 global-step:6483	 l-p:0.13139678537845612
epoch£º324	 i:4 	 global-step:6484	 l-p:0.09020571410655975
epoch£º324	 i:5 	 global-step:6485	 l-p:0.1439133733510971
epoch£º324	 i:6 	 global-step:6486	 l-p:0.12658584117889404
epoch£º324	 i:7 	 global-step:6487	 l-p:0.12494309991598129
epoch£º324	 i:8 	 global-step:6488	 l-p:-0.031246494501829147
epoch£º324	 i:9 	 global-step:6489	 l-p:0.19503070414066315
====================================================================================================
====================================================================================================
====================================================================================================

epoch:325
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9627, 4.9797, 4.9290],
        [4.9627, 4.9559, 4.9608],
        [4.9627, 4.9626, 4.9627],
        [4.9627, 5.0507, 4.9700]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:325, step:0 
model_pd.l_p.mean(): 0.12824498116970062 
model_pd.l_d.mean(): -19.131771087646484 
model_pd.lagr.mean(): -19.00352668762207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5034], device='cuda:0')), ('power', tensor([-20.0109], device='cuda:0'))])
epoch£º325	 i:0 	 global-step:6500	 l-p:0.12824498116970062
epoch£º325	 i:1 	 global-step:6501	 l-p:0.07377922534942627
epoch£º325	 i:2 	 global-step:6502	 l-p:0.18655724823474884
epoch£º325	 i:3 	 global-step:6503	 l-p:0.1306016445159912
epoch£º325	 i:4 	 global-step:6504	 l-p:0.12338922172784805
epoch£º325	 i:5 	 global-step:6505	 l-p:0.1348106861114502
epoch£º325	 i:6 	 global-step:6506	 l-p:0.34154364466667175
epoch£º325	 i:7 	 global-step:6507	 l-p:0.09743498265743256
epoch£º325	 i:8 	 global-step:6508	 l-p:0.062121372669935226
epoch£º325	 i:9 	 global-step:6509	 l-p:0.11822281777858734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:326
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9800, 5.0570, 4.9778],
        [4.9800, 4.9792, 4.9799],
        [4.9800, 5.7559, 6.0995],
        [4.9800, 5.0437, 4.9678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:326, step:0 
model_pd.l_p.mean(): 0.12734220921993256 
model_pd.l_d.mean(): -20.141841888427734 
model_pd.lagr.mean(): -20.01449966430664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4695], device='cuda:0')), ('power', tensor([-21.0047], device='cuda:0'))])
epoch£º326	 i:0 	 global-step:6520	 l-p:0.12734220921993256
epoch£º326	 i:1 	 global-step:6521	 l-p:0.1264156848192215
epoch£º326	 i:2 	 global-step:6522	 l-p:0.14304141700267792
epoch£º326	 i:3 	 global-step:6523	 l-p:3.0210297107696533
epoch£º326	 i:4 	 global-step:6524	 l-p:0.1527671366930008
epoch£º326	 i:5 	 global-step:6525	 l-p:0.20459052920341492
epoch£º326	 i:6 	 global-step:6526	 l-p:0.12335427105426788
epoch£º326	 i:7 	 global-step:6527	 l-p:0.134176105260849
epoch£º326	 i:8 	 global-step:6528	 l-p:0.11336936056613922
epoch£º326	 i:9 	 global-step:6529	 l-p:0.13361352682113647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:327
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8859, 5.6450, 5.9838],
        [4.8859, 4.8858, 4.8859],
        [4.8859, 4.8701, 4.8657],
        [4.8859, 4.8850, 4.8859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:327, step:0 
model_pd.l_p.mean(): 0.13427422940731049 
model_pd.l_d.mean(): -20.004196166992188 
model_pd.lagr.mean(): -19.869922637939453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5013], device='cuda:0')), ('power', tensor([-20.8975], device='cuda:0'))])
epoch£º327	 i:0 	 global-step:6540	 l-p:0.13427422940731049
epoch£º327	 i:1 	 global-step:6541	 l-p:0.02928265929222107
epoch£º327	 i:2 	 global-step:6542	 l-p:0.1287088394165039
epoch£º327	 i:3 	 global-step:6543	 l-p:0.12507523596286774
epoch£º327	 i:4 	 global-step:6544	 l-p:0.09249468892812729
epoch£º327	 i:5 	 global-step:6545	 l-p:0.16665512323379517
epoch£º327	 i:6 	 global-step:6546	 l-p:0.22131885588169098
epoch£º327	 i:7 	 global-step:6547	 l-p:0.13436274230480194
epoch£º327	 i:8 	 global-step:6548	 l-p:0.13523255288600922
epoch£º327	 i:9 	 global-step:6549	 l-p:0.13096798956394196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:328
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9553, 4.9502, 4.9542],
        [4.9553, 4.9531, 4.9551],
        [4.9553, 5.0189, 4.9413],
        [4.9553, 5.4240, 5.5109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:328, step:0 
model_pd.l_p.mean(): 0.05887474864721298 
model_pd.l_d.mean(): -19.95768928527832 
model_pd.lagr.mean(): -19.898815155029297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5021], device='cuda:0')), ('power', tensor([-20.8509], device='cuda:0'))])
epoch£º328	 i:0 	 global-step:6560	 l-p:0.05887474864721298
epoch£º328	 i:1 	 global-step:6561	 l-p:0.12894418835639954
epoch£º328	 i:2 	 global-step:6562	 l-p:0.11521414667367935
epoch£º328	 i:3 	 global-step:6563	 l-p:0.14060813188552856
epoch£º328	 i:4 	 global-step:6564	 l-p:0.30953383445739746
epoch£º328	 i:5 	 global-step:6565	 l-p:0.11494532972574234
epoch£º328	 i:6 	 global-step:6566	 l-p:-0.028575019910931587
epoch£º328	 i:7 	 global-step:6567	 l-p:0.1790064573287964
epoch£º328	 i:8 	 global-step:6568	 l-p:0.1305372565984726
epoch£º328	 i:9 	 global-step:6569	 l-p:0.10597232729196548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:329
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1131, 5.1131, 5.1131],
        [5.1131, 5.3629, 5.3115],
        [5.1131, 5.1124, 5.1131],
        [5.1131, 5.1024, 5.1000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:329, step:0 
model_pd.l_p.mean(): 0.12918703258037567 
model_pd.l_d.mean(): -18.886367797851562 
model_pd.lagr.mean(): -18.75718116760254 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5145], device='cuda:0')), ('power', tensor([-19.7724], device='cuda:0'))])
epoch£º329	 i:0 	 global-step:6580	 l-p:0.12918703258037567
epoch£º329	 i:1 	 global-step:6581	 l-p:0.11103381216526031
epoch£º329	 i:2 	 global-step:6582	 l-p:0.1147686168551445
epoch£º329	 i:3 	 global-step:6583	 l-p:0.1388898491859436
epoch£º329	 i:4 	 global-step:6584	 l-p:0.11451566219329834
epoch£º329	 i:5 	 global-step:6585	 l-p:0.15062803030014038
epoch£º329	 i:6 	 global-step:6586	 l-p:0.12325900793075562
epoch£º329	 i:7 	 global-step:6587	 l-p:0.1747048944234848
epoch£º329	 i:8 	 global-step:6588	 l-p:0.10286839306354523
epoch£º329	 i:9 	 global-step:6589	 l-p:0.8543485999107361
====================================================================================================
====================================================================================================
====================================================================================================

epoch:330
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7924e-02, 4.6907e-03,
         1.0000e+00, 1.2276e-03, 1.0000e+00, 2.6170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0225, 5.0219, 5.0225],
        [5.0225, 5.0216, 5.0225],
        [5.0225, 5.0220, 5.0225],
        [5.0225, 5.0225, 5.0225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:330, step:0 
model_pd.l_p.mean(): 0.12643800675868988 
model_pd.l_d.mean(): -20.35798454284668 
model_pd.lagr.mean(): -20.23154640197754 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4325], device='cuda:0')), ('power', tensor([-21.1866], device='cuda:0'))])
epoch£º330	 i:0 	 global-step:6600	 l-p:0.12643800675868988
epoch£º330	 i:1 	 global-step:6601	 l-p:0.15185238420963287
epoch£º330	 i:2 	 global-step:6602	 l-p:0.143524631857872
epoch£º330	 i:3 	 global-step:6603	 l-p:-0.09507782012224197
epoch£º330	 i:4 	 global-step:6604	 l-p:0.15286847949028015
epoch£º330	 i:5 	 global-step:6605	 l-p:1.1473495960235596
epoch£º330	 i:6 	 global-step:6606	 l-p:0.1345272660255432
epoch£º330	 i:7 	 global-step:6607	 l-p:0.11957279592752457
epoch£º330	 i:8 	 global-step:6608	 l-p:0.1274261772632599
epoch£º330	 i:9 	 global-step:6609	 l-p:0.15076200664043427
====================================================================================================
====================================================================================================
====================================================================================================

epoch:331
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7910, 4.8195, 4.7480],
        [4.7910, 5.4547, 5.7172],
        [4.7910, 4.7879, 4.7907],
        [4.7910, 5.3521, 5.5266]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:331, step:0 
model_pd.l_p.mean(): 0.09982215613126755 
model_pd.l_d.mean(): -20.134681701660156 
model_pd.lagr.mean(): -20.03485870361328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5359], device='cuda:0')), ('power', tensor([-21.0663], device='cuda:0'))])
epoch£º331	 i:0 	 global-step:6620	 l-p:0.09982215613126755
epoch£º331	 i:1 	 global-step:6621	 l-p:0.1403171867132187
epoch£º331	 i:2 	 global-step:6622	 l-p:0.16601596772670746
epoch£º331	 i:3 	 global-step:6623	 l-p:0.13421286642551422
epoch£º331	 i:4 	 global-step:6624	 l-p:0.165599063038826
epoch£º331	 i:5 	 global-step:6625	 l-p:0.24676905572414398
epoch£º331	 i:6 	 global-step:6626	 l-p:0.13504181802272797
epoch£º331	 i:7 	 global-step:6627	 l-p:0.14356885850429535
epoch£º331	 i:8 	 global-step:6628	 l-p:0.13071972131729126
epoch£º331	 i:9 	 global-step:6629	 l-p:0.12148046493530273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:332
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0337, 5.0416, 4.9980],
        [5.0337, 5.1895, 5.1097],
        [5.0337, 5.0856, 5.0120],
        [5.0337, 5.0313, 5.0334]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:332, step:0 
model_pd.l_p.mean(): 0.1318587213754654 
model_pd.l_d.mean(): -20.146635055541992 
model_pd.lagr.mean(): -20.0147762298584 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4697], device='cuda:0')), ('power', tensor([-21.0099], device='cuda:0'))])
epoch£º332	 i:0 	 global-step:6640	 l-p:0.1318587213754654
epoch£º332	 i:1 	 global-step:6641	 l-p:0.12736700475215912
epoch£º332	 i:2 	 global-step:6642	 l-p:-0.11642452329397202
epoch£º332	 i:3 	 global-step:6643	 l-p:0.11220954358577728
epoch£º332	 i:4 	 global-step:6644	 l-p:0.040579695254564285
epoch£º332	 i:5 	 global-step:6645	 l-p:0.15553270280361176
epoch£º332	 i:6 	 global-step:6646	 l-p:0.13685864210128784
epoch£º332	 i:7 	 global-step:6647	 l-p:0.19252684712409973
epoch£º332	 i:8 	 global-step:6648	 l-p:0.06436559557914734
epoch£º332	 i:9 	 global-step:6649	 l-p:0.12602990865707397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:333
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9469, 4.9469, 4.9469],
        [4.9469, 5.1568, 5.0927],
        [4.9469, 4.9469, 4.9469],
        [4.9469, 4.9469, 4.9469]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:333, step:0 
model_pd.l_p.mean(): 0.23303110897541046 
model_pd.l_d.mean(): -20.24652099609375 
model_pd.lagr.mean(): -20.013490676879883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4690], device='cuda:0')), ('power', tensor([-21.1108], device='cuda:0'))])
epoch£º333	 i:0 	 global-step:6660	 l-p:0.23303110897541046
epoch£º333	 i:1 	 global-step:6661	 l-p:0.12756162881851196
epoch£º333	 i:2 	 global-step:6662	 l-p:0.12693528831005096
epoch£º333	 i:3 	 global-step:6663	 l-p:0.045763805508613586
epoch£º333	 i:4 	 global-step:6664	 l-p:0.2056068778038025
epoch£º333	 i:5 	 global-step:6665	 l-p:0.14369797706604004
epoch£º333	 i:6 	 global-step:6666	 l-p:0.12327202409505844
epoch£º333	 i:7 	 global-step:6667	 l-p:1.2264814376831055
epoch£º333	 i:8 	 global-step:6668	 l-p:0.13502755761146545
epoch£º333	 i:9 	 global-step:6669	 l-p:0.46585673093795776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:334
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0648, 5.0644, 5.0647],
        [5.0648, 5.0626, 5.0645],
        [5.0648, 5.0950, 5.0327],
        [5.0648, 5.0601, 5.0336]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:334, step:0 
model_pd.l_p.mean(): 0.14370760321617126 
model_pd.l_d.mean(): -20.702770233154297 
model_pd.lagr.mean(): -20.559062957763672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3856], device='cuda:0')), ('power', tensor([-21.4893], device='cuda:0'))])
epoch£º334	 i:0 	 global-step:6680	 l-p:0.14370760321617126
epoch£º334	 i:1 	 global-step:6681	 l-p:0.1437045931816101
epoch£º334	 i:2 	 global-step:6682	 l-p:0.1372639238834381
epoch£º334	 i:3 	 global-step:6683	 l-p:0.2044655978679657
epoch£º334	 i:4 	 global-step:6684	 l-p:0.1294116973876953
epoch£º334	 i:5 	 global-step:6685	 l-p:0.16800153255462646
epoch£º334	 i:6 	 global-step:6686	 l-p:0.12995007634162903
epoch£º334	 i:7 	 global-step:6687	 l-p:0.15971048176288605
epoch£º334	 i:8 	 global-step:6688	 l-p:0.12461134046316147
epoch£º334	 i:9 	 global-step:6689	 l-p:0.11038032174110413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:335
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1434, 5.1322, 5.1254],
        [5.1434, 5.9016, 6.2104],
        [5.1434, 5.1434, 5.1434],
        [5.1434, 5.9830, 6.3669]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:335, step:0 
model_pd.l_p.mean(): 0.10070974379777908 
model_pd.l_d.mean(): -19.09440803527832 
model_pd.lagr.mean(): -18.993698120117188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4990], device='cuda:0')), ('power', tensor([-19.9683], device='cuda:0'))])
epoch£º335	 i:0 	 global-step:6700	 l-p:0.10070974379777908
epoch£º335	 i:1 	 global-step:6701	 l-p:0.13015389442443848
epoch£º335	 i:2 	 global-step:6702	 l-p:0.17012286186218262
epoch£º335	 i:3 	 global-step:6703	 l-p:0.16130828857421875
epoch£º335	 i:4 	 global-step:6704	 l-p:0.13611583411693573
epoch£º335	 i:5 	 global-step:6705	 l-p:0.14432364702224731
epoch£º335	 i:6 	 global-step:6706	 l-p:-0.28954264521598816
epoch£º335	 i:7 	 global-step:6707	 l-p:0.12177741527557373
epoch£º335	 i:8 	 global-step:6708	 l-p:0.1383781135082245
epoch£º335	 i:9 	 global-step:6709	 l-p:0.12472265213727951
====================================================================================================
====================================================================================================
====================================================================================================

epoch:336
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2039,  0.1200,  1.0000,  0.0706,
          1.0000,  0.5886, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1532,  0.0820,  1.0000,  0.0439,
          1.0000,  0.5351, 31.6228]], device='cuda:0')
 pt:tensor([[5.0152, 4.9994, 4.9964],
        [5.0152, 5.0090, 4.9797],
        [5.0152, 5.2394, 5.1786],
        [5.0152, 5.0000, 4.9925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:336, step:0 
model_pd.l_p.mean(): 0.11919612437486649 
model_pd.l_d.mean(): -19.23280906677246 
model_pd.lagr.mean(): -19.11361312866211 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4675], device='cuda:0')), ('power', tensor([-20.0767], device='cuda:0'))])
epoch£º336	 i:0 	 global-step:6720	 l-p:0.11919612437486649
epoch£º336	 i:1 	 global-step:6721	 l-p:0.07048552483320236
epoch£º336	 i:2 	 global-step:6722	 l-p:0.13176728785037994
epoch£º336	 i:3 	 global-step:6723	 l-p:0.09402286261320114
epoch£º336	 i:4 	 global-step:6724	 l-p:0.14216987788677216
epoch£º336	 i:5 	 global-step:6725	 l-p:0.13368447124958038
epoch£º336	 i:6 	 global-step:6726	 l-p:0.9621796011924744
epoch£º336	 i:7 	 global-step:6727	 l-p:0.12056108564138412
epoch£º336	 i:8 	 global-step:6728	 l-p:0.14237694442272186
epoch£º336	 i:9 	 global-step:6729	 l-p:-0.8155746459960938
====================================================================================================
====================================================================================================
====================================================================================================

epoch:337
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8866, 5.7473, 6.1809],
        [4.8866, 4.8678, 4.8720],
        [4.8866, 4.9549, 4.8690],
        [4.8866, 5.3053, 5.3606]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:337, step:0 
model_pd.l_p.mean(): 0.13545416295528412 
model_pd.l_d.mean(): -18.321659088134766 
model_pd.lagr.mean(): -18.18620491027832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5666], device='cuda:0')), ('power', tensor([-19.2512], device='cuda:0'))])
epoch£º337	 i:0 	 global-step:6740	 l-p:0.13545416295528412
epoch£º337	 i:1 	 global-step:6741	 l-p:0.2584225833415985
epoch£º337	 i:2 	 global-step:6742	 l-p:0.0204453282058239
epoch£º337	 i:3 	 global-step:6743	 l-p:0.13076086342334747
epoch£º337	 i:4 	 global-step:6744	 l-p:0.10102750360965729
epoch£º337	 i:5 	 global-step:6745	 l-p:0.1295531839132309
epoch£º337	 i:6 	 global-step:6746	 l-p:0.12853924930095673
epoch£º337	 i:7 	 global-step:6747	 l-p:0.1475854516029358
epoch£º337	 i:8 	 global-step:6748	 l-p:0.14573509991168976
epoch£º337	 i:9 	 global-step:6749	 l-p:0.15776190161705017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:338
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9417, 4.9234, 4.9133],
        [4.9417, 4.9264, 4.9331],
        [4.9417, 4.9376, 4.9411],
        [4.9417, 4.9417, 4.9417]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:338, step:0 
model_pd.l_p.mean(): 0.12395595014095306 
model_pd.l_d.mean(): -20.11937141418457 
model_pd.lagr.mean(): -19.99541473388672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4898], device='cuda:0')), ('power', tensor([-21.0028], device='cuda:0'))])
epoch£º338	 i:0 	 global-step:6760	 l-p:0.12395595014095306
epoch£º338	 i:1 	 global-step:6761	 l-p:0.1222986951470375
epoch£º338	 i:2 	 global-step:6762	 l-p:0.09170468896627426
epoch£º338	 i:3 	 global-step:6763	 l-p:0.13072095811367035
epoch£º338	 i:4 	 global-step:6764	 l-p:-0.5474945902824402
epoch£º338	 i:5 	 global-step:6765	 l-p:0.154494971036911
epoch£º338	 i:6 	 global-step:6766	 l-p:0.10008220374584198
epoch£º338	 i:7 	 global-step:6767	 l-p:0.16950547695159912
epoch£º338	 i:8 	 global-step:6768	 l-p:0.10858196020126343
epoch£º338	 i:9 	 global-step:6769	 l-p:0.1490350067615509
====================================================================================================
====================================================================================================
====================================================================================================

epoch:339
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0490, 5.6074, 5.7550],
        [5.0490, 5.0327, 5.0326],
        [5.0490, 5.0349, 5.0403],
        [5.0490, 5.0490, 5.0490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:339, step:0 
model_pd.l_p.mean(): 0.12508618831634521 
model_pd.l_d.mean(): -20.251562118530273 
model_pd.lagr.mean(): -20.126476287841797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4488], device='cuda:0')), ('power', tensor([-21.0951], device='cuda:0'))])
epoch£º339	 i:0 	 global-step:6780	 l-p:0.12508618831634521
epoch£º339	 i:1 	 global-step:6781	 l-p:0.13677707314491272
epoch£º339	 i:2 	 global-step:6782	 l-p:0.12987017631530762
epoch£º339	 i:3 	 global-step:6783	 l-p:3.633655309677124
epoch£º339	 i:4 	 global-step:6784	 l-p:0.1577295958995819
epoch£º339	 i:5 	 global-step:6785	 l-p:-0.04163169860839844
epoch£º339	 i:6 	 global-step:6786	 l-p:0.14544148743152618
epoch£º339	 i:7 	 global-step:6787	 l-p:0.1256619542837143
epoch£º339	 i:8 	 global-step:6788	 l-p:0.15243376791477203
epoch£º339	 i:9 	 global-step:6789	 l-p:-9.80622386932373
====================================================================================================
====================================================================================================
====================================================================================================

epoch:340
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0734, 5.4494, 5.4641],
        [5.0734, 5.0715, 5.0732],
        [5.0734, 5.0733, 5.0734],
        [5.0734, 5.0585, 5.0486]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:340, step:0 
model_pd.l_p.mean(): 0.12134791910648346 
model_pd.l_d.mean(): -19.78070068359375 
model_pd.lagr.mean(): -19.659353256225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4791], device='cuda:0')), ('power', tensor([-20.6468], device='cuda:0'))])
epoch£º340	 i:0 	 global-step:6800	 l-p:0.12134791910648346
epoch£º340	 i:1 	 global-step:6801	 l-p:0.1340397745370865
epoch£º340	 i:2 	 global-step:6802	 l-p:0.2538636326789856
epoch£º340	 i:3 	 global-step:6803	 l-p:0.13946007192134857
epoch£º340	 i:4 	 global-step:6804	 l-p:0.1264442652463913
epoch£º340	 i:5 	 global-step:6805	 l-p:0.11026788502931595
epoch£º340	 i:6 	 global-step:6806	 l-p:0.20733582973480225
epoch£º340	 i:7 	 global-step:6807	 l-p:0.12318620085716248
epoch£º340	 i:8 	 global-step:6808	 l-p:0.17657211422920227
epoch£º340	 i:9 	 global-step:6809	 l-p:0.11686486750841141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:341
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1295, 5.1288, 5.1294],
        [5.1295, 5.1243, 5.0952],
        [5.1295, 5.6030, 5.6798],
        [5.1295, 5.6206, 5.7102]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:341, step:0 
model_pd.l_p.mean(): 0.128956139087677 
model_pd.l_d.mean(): -19.941883087158203 
model_pd.lagr.mean(): -19.81292724609375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4605], device='cuda:0')), ('power', tensor([-20.7917], device='cuda:0'))])
epoch£º341	 i:0 	 global-step:6820	 l-p:0.128956139087677
epoch£º341	 i:1 	 global-step:6821	 l-p:0.1443302035331726
epoch£º341	 i:2 	 global-step:6822	 l-p:0.13637731969356537
epoch£º341	 i:3 	 global-step:6823	 l-p:0.119218610227108
epoch£º341	 i:4 	 global-step:6824	 l-p:0.12960948050022125
epoch£º341	 i:5 	 global-step:6825	 l-p:0.12088318914175034
epoch£º341	 i:6 	 global-step:6826	 l-p:0.007193360012024641
epoch£º341	 i:7 	 global-step:6827	 l-p:0.1474313735961914
epoch£º341	 i:8 	 global-step:6828	 l-p:0.09508692473173141
epoch£º341	 i:9 	 global-step:6829	 l-p:0.33459609746932983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:342
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9341, 4.9341, 4.9341],
        [4.9341, 5.4529, 5.5790],
        [4.9341, 5.6484, 5.9395],
        [4.9341, 5.4538, 5.5806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:342, step:0 
model_pd.l_p.mean(): 0.14836733043193817 
model_pd.l_d.mean(): -20.561182022094727 
model_pd.lagr.mean(): -20.41281509399414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4485], device='cuda:0')), ('power', tensor([-21.4102], device='cuda:0'))])
epoch£º342	 i:0 	 global-step:6840	 l-p:0.14836733043193817
epoch£º342	 i:1 	 global-step:6841	 l-p:0.5104735493659973
epoch£º342	 i:2 	 global-step:6842	 l-p:0.14263516664505005
epoch£º342	 i:3 	 global-step:6843	 l-p:0.12596286833286285
epoch£º342	 i:4 	 global-step:6844	 l-p:0.1333962231874466
epoch£º342	 i:5 	 global-step:6845	 l-p:0.31864267587661743
epoch£º342	 i:6 	 global-step:6846	 l-p:0.04757498577237129
epoch£º342	 i:7 	 global-step:6847	 l-p:0.08289163559675217
epoch£º342	 i:8 	 global-step:6848	 l-p:0.06982783228158951
epoch£º342	 i:9 	 global-step:6849	 l-p:0.1374329775571823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:343
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0527, 5.6048, 5.7467],
        [5.0527, 5.0527, 5.0527],
        [5.0527, 5.7950, 6.0984],
        [5.0527, 5.0396, 5.0465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:343, step:0 
model_pd.l_p.mean(): 0.13640566170215607 
model_pd.l_d.mean(): -20.589141845703125 
model_pd.lagr.mean(): -20.452735900878906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4047], device='cuda:0')), ('power', tensor([-21.3933], device='cuda:0'))])
epoch£º343	 i:0 	 global-step:6860	 l-p:0.13640566170215607
epoch£º343	 i:1 	 global-step:6861	 l-p:0.1323009580373764
epoch£º343	 i:2 	 global-step:6862	 l-p:0.1331787407398224
epoch£º343	 i:3 	 global-step:6863	 l-p:0.12335802614688873
epoch£º343	 i:4 	 global-step:6864	 l-p:0.13264137506484985
epoch£º343	 i:5 	 global-step:6865	 l-p:-0.06524515151977539
epoch£º343	 i:6 	 global-step:6866	 l-p:0.1307196170091629
epoch£º343	 i:7 	 global-step:6867	 l-p:-0.785162627696991
epoch£º343	 i:8 	 global-step:6868	 l-p:0.1276651918888092
epoch£º343	 i:9 	 global-step:6869	 l-p:0.13037070631980896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:344
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1920,  0.1107,  1.0000,  0.0639,
          1.0000,  0.5769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228]], device='cuda:0')
 pt:tensor([[4.9952, 5.7172, 6.0085],
        [4.9952, 5.5489, 5.6970],
        [4.9952, 4.9795, 4.9576],
        [4.9952, 4.9782, 4.9597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:344, step:0 
model_pd.l_p.mean(): 0.14524686336517334 
model_pd.l_d.mean(): -20.61579132080078 
model_pd.lagr.mean(): -20.470544815063477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4187], device='cuda:0')), ('power', tensor([-21.4349], device='cuda:0'))])
epoch£º344	 i:0 	 global-step:6880	 l-p:0.14524686336517334
epoch£º344	 i:1 	 global-step:6881	 l-p:0.11184831708669662
epoch£º344	 i:2 	 global-step:6882	 l-p:0.13258732855319977
epoch£º344	 i:3 	 global-step:6883	 l-p:0.12786579132080078
epoch£º344	 i:4 	 global-step:6884	 l-p:0.1023091971874237
epoch£º344	 i:5 	 global-step:6885	 l-p:0.15071135759353638
epoch£º344	 i:6 	 global-step:6886	 l-p:0.1331370770931244
epoch£º344	 i:7 	 global-step:6887	 l-p:0.0892571434378624
epoch£º344	 i:8 	 global-step:6888	 l-p:0.14800511300563812
epoch£º344	 i:9 	 global-step:6889	 l-p:-0.204433411359787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:345
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9002, 4.8888, 4.8966],
        [4.9002, 5.5994, 5.8804],
        [4.9002, 4.8864, 4.8948],
        [4.9002, 4.9561, 4.8683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:345, step:0 
model_pd.l_p.mean(): 0.12366646528244019 
model_pd.l_d.mean(): -19.352432250976562 
model_pd.lagr.mean(): -19.2287654876709 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4971], device='cuda:0')), ('power', tensor([-20.2291], device='cuda:0'))])
epoch£º345	 i:0 	 global-step:6900	 l-p:0.12366646528244019
epoch£º345	 i:1 	 global-step:6901	 l-p:0.13964256644248962
epoch£º345	 i:2 	 global-step:6902	 l-p:0.11924391239881516
epoch£º345	 i:3 	 global-step:6903	 l-p:0.14549987018108368
epoch£º345	 i:4 	 global-step:6904	 l-p:0.11782119423151016
epoch£º345	 i:5 	 global-step:6905	 l-p:4.2145233154296875
epoch£º345	 i:6 	 global-step:6906	 l-p:0.08327123522758484
epoch£º345	 i:7 	 global-step:6907	 l-p:0.1822754442691803
epoch£º345	 i:8 	 global-step:6908	 l-p:0.13218820095062256
epoch£º345	 i:9 	 global-step:6909	 l-p:0.12327320128679276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:346
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0552, 5.0486, 5.0538],
        [5.0552, 5.0551, 5.0552],
        [5.0552, 5.0399, 5.0468],
        [5.0552, 5.1697, 5.0793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:346, step:0 
model_pd.l_p.mean(): 0.1438831239938736 
model_pd.l_d.mean(): -19.7391414642334 
model_pd.lagr.mean(): -19.595258712768555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5066], device='cuda:0')), ('power', tensor([-20.6329], device='cuda:0'))])
epoch£º346	 i:0 	 global-step:6920	 l-p:0.1438831239938736
epoch£º346	 i:1 	 global-step:6921	 l-p:0.1307820826768875
epoch£º346	 i:2 	 global-step:6922	 l-p:0.14052452147006989
epoch£º346	 i:3 	 global-step:6923	 l-p:0.21368560194969177
epoch£º346	 i:4 	 global-step:6924	 l-p:0.1302410513162613
epoch£º346	 i:5 	 global-step:6925	 l-p:0.10525398701429367
epoch£º346	 i:6 	 global-step:6926	 l-p:0.12169981747865677
epoch£º346	 i:7 	 global-step:6927	 l-p:0.12889926135540009
epoch£º346	 i:8 	 global-step:6928	 l-p:0.13574035465717316
epoch£º346	 i:9 	 global-step:6929	 l-p:-0.058432310819625854
====================================================================================================
====================================================================================================
====================================================================================================

epoch:347
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0020, 5.0020, 5.0020],
        [5.0020, 5.0711, 4.9820],
        [5.0020, 5.7835, 6.1275],
        [5.0020, 4.9989, 5.0017]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:347, step:0 
model_pd.l_p.mean(): 0.12156657874584198 
model_pd.l_d.mean(): -19.811845779418945 
model_pd.lagr.mean(): -19.690279006958008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5083], device='cuda:0')), ('power', tensor([-20.7088], device='cuda:0'))])
epoch£º347	 i:0 	 global-step:6940	 l-p:0.12156657874584198
epoch£º347	 i:1 	 global-step:6941	 l-p:0.08823953568935394
epoch£º347	 i:2 	 global-step:6942	 l-p:0.09220939129590988
epoch£º347	 i:3 	 global-step:6943	 l-p:0.09080827236175537
epoch£º347	 i:4 	 global-step:6944	 l-p:0.12793861329555511
epoch£º347	 i:5 	 global-step:6945	 l-p:0.17513172328472137
epoch£º347	 i:6 	 global-step:6946	 l-p:0.30504104495048523
epoch£º347	 i:7 	 global-step:6947	 l-p:0.12592852115631104
epoch£º347	 i:8 	 global-step:6948	 l-p:0.11960908770561218
epoch£º347	 i:9 	 global-step:6949	 l-p:0.137228861451149
====================================================================================================
====================================================================================================
====================================================================================================

epoch:348
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0346, 5.0140, 5.0145],
        [5.0346, 5.0192, 5.0268],
        [5.0346, 5.0145, 5.0171],
        [5.0346, 5.0346, 5.0346]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:348, step:0 
model_pd.l_p.mean(): 0.13712568581104279 
model_pd.l_d.mean(): -20.26488494873047 
model_pd.lagr.mean(): -20.12775993347168 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4374], device='cuda:0')), ('power', tensor([-21.0968], device='cuda:0'))])
epoch£º348	 i:0 	 global-step:6960	 l-p:0.13712568581104279
epoch£º348	 i:1 	 global-step:6961	 l-p:0.047626346349716187
epoch£º348	 i:2 	 global-step:6962	 l-p:0.16592885553836823
epoch£º348	 i:3 	 global-step:6963	 l-p:0.19478289783000946
epoch£º348	 i:4 	 global-step:6964	 l-p:0.1280280351638794
epoch£º348	 i:5 	 global-step:6965	 l-p:0.11886715888977051
epoch£º348	 i:6 	 global-step:6966	 l-p:0.1433904767036438
epoch£º348	 i:7 	 global-step:6967	 l-p:1.5140084028244019
epoch£º348	 i:8 	 global-step:6968	 l-p:0.13647933304309845
epoch£º348	 i:9 	 global-step:6969	 l-p:-0.0015785789582878351
====================================================================================================
====================================================================================================
====================================================================================================

epoch:349
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0138, 5.0138, 5.0138],
        [5.0138, 5.0128, 5.0138],
        [5.0138, 5.0125, 5.0138],
        [5.0138, 4.9980, 5.0060]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:349, step:0 
model_pd.l_p.mean(): 0.13541139662265778 
model_pd.l_d.mean(): -18.75339698791504 
model_pd.lagr.mean(): -18.617984771728516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5361], device='cuda:0')), ('power', tensor([-19.6593], device='cuda:0'))])
epoch£º349	 i:0 	 global-step:6980	 l-p:0.13541139662265778
epoch£º349	 i:1 	 global-step:6981	 l-p:0.12554658949375153
epoch£º349	 i:2 	 global-step:6982	 l-p:0.12823861837387085
epoch£º349	 i:3 	 global-step:6983	 l-p:0.1451110690832138
epoch£º349	 i:4 	 global-step:6984	 l-p:0.12161470204591751
epoch£º349	 i:5 	 global-step:6985	 l-p:0.14140714704990387
epoch£º349	 i:6 	 global-step:6986	 l-p:0.1527867168188095
epoch£º349	 i:7 	 global-step:6987	 l-p:0.4639657139778137
epoch£º349	 i:8 	 global-step:6988	 l-p:0.13476666808128357
epoch£º349	 i:9 	 global-step:6989	 l-p:0.05351344123482704
====================================================================================================
====================================================================================================
====================================================================================================

epoch:350
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8622, 4.8608, 4.8621],
        [4.8622, 5.3080, 5.3837],
        [4.8622, 5.1672, 5.1484],
        [4.8622, 5.3168, 5.3991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:350, step:0 
model_pd.l_p.mean(): 0.19665728509426117 
model_pd.l_d.mean(): -20.322738647460938 
model_pd.lagr.mean(): -20.126081466674805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5010], device='cuda:0')), ('power', tensor([-21.2216], device='cuda:0'))])
epoch£º350	 i:0 	 global-step:7000	 l-p:0.19665728509426117
epoch£º350	 i:1 	 global-step:7001	 l-p:0.09616471827030182
epoch£º350	 i:2 	 global-step:7002	 l-p:0.04767359793186188
epoch£º350	 i:3 	 global-step:7003	 l-p:0.11544597148895264
epoch£º350	 i:4 	 global-step:7004	 l-p:0.16967788338661194
epoch£º350	 i:5 	 global-step:7005	 l-p:0.13124655187129974
epoch£º350	 i:6 	 global-step:7006	 l-p:0.11879605799913406
epoch£º350	 i:7 	 global-step:7007	 l-p:1.948678731918335
epoch£º350	 i:8 	 global-step:7008	 l-p:0.1322697252035141
epoch£º350	 i:9 	 global-step:7009	 l-p:0.12037955224514008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:351
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0771, 5.0574, 5.0608],
        [5.0771, 5.0564, 5.0510],
        [5.0771, 5.2325, 5.1456],
        [5.0771, 5.2515, 5.1691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:351, step:0 
model_pd.l_p.mean(): 0.31483176350593567 
model_pd.l_d.mean(): -20.003297805786133 
model_pd.lagr.mean(): -19.688465118408203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4734], device='cuda:0')), ('power', tensor([-20.8676], device='cuda:0'))])
epoch£º351	 i:0 	 global-step:7020	 l-p:0.31483176350593567
epoch£º351	 i:1 	 global-step:7021	 l-p:0.23601405322551727
epoch£º351	 i:2 	 global-step:7022	 l-p:0.09991899132728577
epoch£º351	 i:3 	 global-step:7023	 l-p:0.13288968801498413
epoch£º351	 i:4 	 global-step:7024	 l-p:0.11989066749811172
epoch£º351	 i:5 	 global-step:7025	 l-p:0.12804071605205536
epoch£º351	 i:6 	 global-step:7026	 l-p:0.14340734481811523
epoch£º351	 i:7 	 global-step:7027	 l-p:0.11614122241735458
epoch£º351	 i:8 	 global-step:7028	 l-p:0.2977418899536133
epoch£º351	 i:9 	 global-step:7029	 l-p:0.12624521553516388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:352
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0369, 5.0169, 5.0228],
        [5.0369, 5.1168, 5.0244],
        [5.0369, 5.0154, 5.0022],
        [5.0369, 5.2448, 5.1731]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:352, step:0 
model_pd.l_p.mean(): -0.02338005043566227 
model_pd.l_d.mean(): -20.794647216796875 
model_pd.lagr.mean(): -20.81802749633789 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3829], device='cuda:0')), ('power', tensor([-21.5801], device='cuda:0'))])
epoch£º352	 i:0 	 global-step:7040	 l-p:-0.02338005043566227
epoch£º352	 i:1 	 global-step:7041	 l-p:0.1542906016111374
epoch£º352	 i:2 	 global-step:7042	 l-p:0.13455113768577576
epoch£º352	 i:3 	 global-step:7043	 l-p:0.13903874158859253
epoch£º352	 i:4 	 global-step:7044	 l-p:0.1359437108039856
epoch£º352	 i:5 	 global-step:7045	 l-p:0.16821929812431335
epoch£º352	 i:6 	 global-step:7046	 l-p:0.17691642045974731
epoch£º352	 i:7 	 global-step:7047	 l-p:0.11011453717947006
epoch£º352	 i:8 	 global-step:7048	 l-p:0.13134267926216125
epoch£º352	 i:9 	 global-step:7049	 l-p:0.11526036262512207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:353
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1861, 5.1810, 5.1852],
        [5.1861, 5.1841, 5.1859],
        [5.1861, 5.1843, 5.1859],
        [5.1861, 5.1861, 5.1861]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:353, step:0 
model_pd.l_p.mean(): 0.1227974072098732 
model_pd.l_d.mean(): -20.2752628326416 
model_pd.lagr.mean(): -20.1524658203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3997], device='cuda:0')), ('power', tensor([-21.0683], device='cuda:0'))])
epoch£º353	 i:0 	 global-step:7060	 l-p:0.1227974072098732
epoch£º353	 i:1 	 global-step:7061	 l-p:0.12834496796131134
epoch£º353	 i:2 	 global-step:7062	 l-p:0.14315028488636017
epoch£º353	 i:3 	 global-step:7063	 l-p:0.13670943677425385
epoch£º353	 i:4 	 global-step:7064	 l-p:0.10652771592140198
epoch£º353	 i:5 	 global-step:7065	 l-p:0.1548968404531479
epoch£º353	 i:6 	 global-step:7066	 l-p:0.1422659456729889
epoch£º353	 i:7 	 global-step:7067	 l-p:0.13973194360733032
epoch£º353	 i:8 	 global-step:7068	 l-p:-1.9348000288009644
epoch£º353	 i:9 	 global-step:7069	 l-p:0.11765163391828537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:354
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0234, 5.0044, 5.0123],
        [5.0234, 5.2823, 5.2319],
        [5.0234, 5.8383, 6.2101],
        [5.0234, 5.0178, 5.0225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:354, step:0 
model_pd.l_p.mean(): 0.12673744559288025 
model_pd.l_d.mean(): -20.583768844604492 
model_pd.lagr.mean(): -20.45703125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4006], device='cuda:0')), ('power', tensor([-21.3835], device='cuda:0'))])
epoch£º354	 i:0 	 global-step:7080	 l-p:0.12673744559288025
epoch£º354	 i:1 	 global-step:7081	 l-p:0.2642161548137665
epoch£º354	 i:2 	 global-step:7082	 l-p:0.08037595450878143
epoch£º354	 i:3 	 global-step:7083	 l-p:0.18403583765029907
epoch£º354	 i:4 	 global-step:7084	 l-p:0.1414402574300766
epoch£º354	 i:5 	 global-step:7085	 l-p:0.135508194565773
epoch£º354	 i:6 	 global-step:7086	 l-p:0.1445043683052063
epoch£º354	 i:7 	 global-step:7087	 l-p:0.13920369744300842
epoch£º354	 i:8 	 global-step:7088	 l-p:0.12475863099098206
epoch£º354	 i:9 	 global-step:7089	 l-p:-0.007023429498076439
====================================================================================================
====================================================================================================
====================================================================================================

epoch:355
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9836, 5.7910, 6.1601],
        [4.9836, 4.9696, 4.9785],
        [4.9836, 4.9803, 4.9833],
        [4.9836, 4.9824, 4.9835]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:355, step:0 
model_pd.l_p.mean(): 0.13157448172569275 
model_pd.l_d.mean(): -19.987850189208984 
model_pd.lagr.mean(): -19.85627555847168 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4758], device='cuda:0')), ('power', tensor([-20.8545], device='cuda:0'))])
epoch£º355	 i:0 	 global-step:7100	 l-p:0.13157448172569275
epoch£º355	 i:1 	 global-step:7101	 l-p:0.1290544867515564
epoch£º355	 i:2 	 global-step:7102	 l-p:0.3075445592403412
epoch£º355	 i:3 	 global-step:7103	 l-p:0.12469765543937683
epoch£º355	 i:4 	 global-step:7104	 l-p:0.1491927206516266
epoch£º355	 i:5 	 global-step:7105	 l-p:0.1692783385515213
epoch£º355	 i:6 	 global-step:7106	 l-p:0.15094149112701416
epoch£º355	 i:7 	 global-step:7107	 l-p:0.08662571012973785
epoch£º355	 i:8 	 global-step:7108	 l-p:0.14048051834106445
epoch£º355	 i:9 	 global-step:7109	 l-p:0.09084386378526688
====================================================================================================
====================================================================================================
====================================================================================================

epoch:356
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8278, 4.7958, 4.7955],
        [4.8278, 5.2262, 5.2696],
        [4.8278, 4.8277, 4.8278],
        [4.8278, 4.7954, 4.7931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:356, step:0 
model_pd.l_p.mean(): 0.16341406106948853 
model_pd.l_d.mean(): -19.734966278076172 
model_pd.lagr.mean(): -19.571552276611328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5176], device='cuda:0')), ('power', tensor([-20.6400], device='cuda:0'))])
epoch£º356	 i:0 	 global-step:7120	 l-p:0.16341406106948853
epoch£º356	 i:1 	 global-step:7121	 l-p:0.15056699514389038
epoch£º356	 i:2 	 global-step:7122	 l-p:0.1475866734981537
epoch£º356	 i:3 	 global-step:7123	 l-p:0.19708210229873657
epoch£º356	 i:4 	 global-step:7124	 l-p:0.07770511507987976
epoch£º356	 i:5 	 global-step:7125	 l-p:0.10827836394309998
epoch£º356	 i:6 	 global-step:7126	 l-p:0.18055692315101624
epoch£º356	 i:7 	 global-step:7127	 l-p:0.06555015593767166
epoch£º356	 i:8 	 global-step:7128	 l-p:0.08531543612480164
epoch£º356	 i:9 	 global-step:7129	 l-p:0.14696066081523895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:357
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9396, 5.6816, 5.9950],
        [4.9396, 4.9377, 4.9395],
        [4.9396, 4.9311, 4.9378],
        [4.9396, 4.9138, 4.8885]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:357, step:0 
model_pd.l_p.mean(): 0.4955216944217682 
model_pd.l_d.mean(): -19.85054588317871 
model_pd.lagr.mean(): -19.355024337768555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5373], device='cuda:0')), ('power', tensor([-20.7783], device='cuda:0'))])
epoch£º357	 i:0 	 global-step:7140	 l-p:0.4955216944217682
epoch£º357	 i:1 	 global-step:7141	 l-p:0.15318556129932404
epoch£º357	 i:2 	 global-step:7142	 l-p:0.13575628399848938
epoch£º357	 i:3 	 global-step:7143	 l-p:0.29197803139686584
epoch£º357	 i:4 	 global-step:7144	 l-p:0.04547124728560448
epoch£º357	 i:5 	 global-step:7145	 l-p:0.13641029596328735
epoch£º357	 i:6 	 global-step:7146	 l-p:0.1308034509420395
epoch£º357	 i:7 	 global-step:7147	 l-p:0.1260971575975418
epoch£º357	 i:8 	 global-step:7148	 l-p:0.11487874388694763
epoch£º357	 i:9 	 global-step:7149	 l-p:0.12748999893665314
====================================================================================================
====================================================================================================
====================================================================================================

epoch:358
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9549, 4.9540, 4.9549],
        [4.9549, 4.9549, 4.9549],
        [4.9549, 4.9514, 4.8891],
        [4.9549, 5.5296, 5.6970]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:358, step:0 
model_pd.l_p.mean(): 0.07503283023834229 
model_pd.l_d.mean(): -20.455801010131836 
model_pd.lagr.mean(): -20.380767822265625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4525], device='cuda:0')), ('power', tensor([-21.3070], device='cuda:0'))])
epoch£º358	 i:0 	 global-step:7160	 l-p:0.07503283023834229
epoch£º358	 i:1 	 global-step:7161	 l-p:0.02194295823574066
epoch£º358	 i:2 	 global-step:7162	 l-p:0.15462419390678406
epoch£º358	 i:3 	 global-step:7163	 l-p:0.13280798494815826
epoch£º358	 i:4 	 global-step:7164	 l-p:0.09372007101774216
epoch£º358	 i:5 	 global-step:7165	 l-p:0.12962310016155243
epoch£º358	 i:6 	 global-step:7166	 l-p:0.14753404259681702
epoch£º358	 i:7 	 global-step:7167	 l-p:0.13403619825839996
epoch£º358	 i:8 	 global-step:7168	 l-p:0.31911906599998474
epoch£º358	 i:9 	 global-step:7169	 l-p:0.19583208858966827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:359
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9728, 4.9540, 4.9640],
        [4.9728, 4.9728, 4.9728],
        [4.9728, 4.9557, 4.9115],
        [4.9728, 4.9728, 4.9728]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:359, step:0 
model_pd.l_p.mean(): 0.14177542924880981 
model_pd.l_d.mean(): -20.129356384277344 
model_pd.lagr.mean(): -19.987581253051758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4771], device='cuda:0')), ('power', tensor([-20.9999], device='cuda:0'))])
epoch£º359	 i:0 	 global-step:7180	 l-p:0.14177542924880981
epoch£º359	 i:1 	 global-step:7181	 l-p:0.1284598857164383
epoch£º359	 i:2 	 global-step:7182	 l-p:0.13241377472877502
epoch£º359	 i:3 	 global-step:7183	 l-p:0.12434268742799759
epoch£º359	 i:4 	 global-step:7184	 l-p:-1.7354295253753662
epoch£º359	 i:5 	 global-step:7185	 l-p:0.14506123960018158
epoch£º359	 i:6 	 global-step:7186	 l-p:0.31701919436454773
epoch£º359	 i:7 	 global-step:7187	 l-p:0.09697955846786499
epoch£º359	 i:8 	 global-step:7188	 l-p:0.1730516403913498
epoch£º359	 i:9 	 global-step:7189	 l-p:0.14313822984695435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:360
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8935, 4.8622, 4.8403],
        [4.8935, 4.8627, 4.8385],
        [4.8935, 4.8610, 4.8578],
        [4.8935, 4.8935, 4.8935]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:360, step:0 
model_pd.l_p.mean(): 0.13245174288749695 
model_pd.l_d.mean(): -19.976402282714844 
model_pd.lagr.mean(): -19.843950271606445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5272], device='cuda:0')), ('power', tensor([-20.8960], device='cuda:0'))])
epoch£º360	 i:0 	 global-step:7200	 l-p:0.13245174288749695
epoch£º360	 i:1 	 global-step:7201	 l-p:0.11619109660387039
epoch£º360	 i:2 	 global-step:7202	 l-p:0.1954430341720581
epoch£º360	 i:3 	 global-step:7203	 l-p:0.054878462105989456
epoch£º360	 i:4 	 global-step:7204	 l-p:0.17700210213661194
epoch£º360	 i:5 	 global-step:7205	 l-p:0.15494124591350555
epoch£º360	 i:6 	 global-step:7206	 l-p:0.14783446490764618
epoch£º360	 i:7 	 global-step:7207	 l-p:0.086397185921669
epoch£º360	 i:8 	 global-step:7208	 l-p:0.1309475600719452
epoch£º360	 i:9 	 global-step:7209	 l-p:0.13988836109638214
====================================================================================================
====================================================================================================
====================================================================================================

epoch:361
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8673, 4.8673, 4.8673],
        [4.8673, 4.8673, 4.8673],
        [4.8673, 4.8671, 4.8673],
        [4.8673, 4.8657, 4.8672]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:361, step:0 
model_pd.l_p.mean(): 0.17169365286827087 
model_pd.l_d.mean(): -20.47637176513672 
model_pd.lagr.mean(): -20.304677963256836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4876], device='cuda:0')), ('power', tensor([-21.3643], device='cuda:0'))])
epoch£º361	 i:0 	 global-step:7220	 l-p:0.17169365286827087
epoch£º361	 i:1 	 global-step:7221	 l-p:0.1850442886352539
epoch£º361	 i:2 	 global-step:7222	 l-p:1.0860991477966309
epoch£º361	 i:3 	 global-step:7223	 l-p:0.0760878175497055
epoch£º361	 i:4 	 global-step:7224	 l-p:0.11147360503673553
epoch£º361	 i:5 	 global-step:7225	 l-p:0.1144721582531929
epoch£º361	 i:6 	 global-step:7226	 l-p:0.12765544652938843
epoch£º361	 i:7 	 global-step:7227	 l-p:0.131763756275177
epoch£º361	 i:8 	 global-step:7228	 l-p:0.6746034622192383
epoch£º361	 i:9 	 global-step:7229	 l-p:0.24093842506408691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:362
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1135, 5.0958, 5.0615],
        [5.1135, 5.1135, 5.1135],
        [5.1135, 5.1135, 5.1135],
        [5.1135, 5.5511, 5.6012]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:362, step:0 
model_pd.l_p.mean(): 0.1297038495540619 
model_pd.l_d.mean(): -20.193267822265625 
model_pd.lagr.mean(): -20.06356430053711 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4370], device='cuda:0')), ('power', tensor([-21.0235], device='cuda:0'))])
epoch£º362	 i:0 	 global-step:7240	 l-p:0.1297038495540619
epoch£º362	 i:1 	 global-step:7241	 l-p:0.1781289279460907
epoch£º362	 i:2 	 global-step:7242	 l-p:0.13306909799575806
epoch£º362	 i:3 	 global-step:7243	 l-p:0.13566924631595612
epoch£º362	 i:4 	 global-step:7244	 l-p:0.1661209762096405
epoch£º362	 i:5 	 global-step:7245	 l-p:0.12959150969982147
epoch£º362	 i:6 	 global-step:7246	 l-p:0.13943222165107727
epoch£º362	 i:7 	 global-step:7247	 l-p:0.13300618529319763
epoch£º362	 i:8 	 global-step:7248	 l-p:0.12269982695579529
epoch£º362	 i:9 	 global-step:7249	 l-p:0.1363973170518875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:363
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1302, 5.1050, 5.1038],
        [5.1302, 5.1296, 5.1302],
        [5.1302, 5.1288, 5.1301],
        [5.1302, 5.1244, 5.0717]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:363, step:0 
model_pd.l_p.mean(): 0.1961176097393036 
model_pd.l_d.mean(): -20.53141212463379 
model_pd.lagr.mean(): -20.335294723510742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3913], device='cuda:0')), ('power', tensor([-21.3206], device='cuda:0'))])
epoch£º363	 i:0 	 global-step:7260	 l-p:0.1961176097393036
epoch£º363	 i:1 	 global-step:7261	 l-p:0.15057845413684845
epoch£º363	 i:2 	 global-step:7262	 l-p:0.0843643993139267
epoch£º363	 i:3 	 global-step:7263	 l-p:0.11463689804077148
epoch£º363	 i:4 	 global-step:7264	 l-p:0.12737445533275604
epoch£º363	 i:5 	 global-step:7265	 l-p:0.14284414052963257
epoch£º363	 i:6 	 global-step:7266	 l-p:0.11674781888723373
epoch£º363	 i:7 	 global-step:7267	 l-p:0.15803636610507965
epoch£º363	 i:8 	 global-step:7268	 l-p:0.12052064388990402
epoch£º363	 i:9 	 global-step:7269	 l-p:0.12390761822462082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:364
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1237, 6.0252, 6.4656],
        [5.1237, 5.0978, 5.0968],
        [5.1237, 5.1236, 5.1237],
        [5.1237, 5.1269, 5.0627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:364, step:0 
model_pd.l_p.mean(): 0.1492328941822052 
model_pd.l_d.mean(): -19.184532165527344 
model_pd.lagr.mean(): -19.03529930114746 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4681], device='cuda:0')), ('power', tensor([-20.0281], device='cuda:0'))])
epoch£º364	 i:0 	 global-step:7280	 l-p:0.1492328941822052
epoch£º364	 i:1 	 global-step:7281	 l-p:0.12165490537881851
epoch£º364	 i:2 	 global-step:7282	 l-p:-0.17243289947509766
epoch£º364	 i:3 	 global-step:7283	 l-p:0.11971103399991989
epoch£º364	 i:4 	 global-step:7284	 l-p:0.13874880969524384
epoch£º364	 i:5 	 global-step:7285	 l-p:0.09231003373861313
epoch£º364	 i:6 	 global-step:7286	 l-p:0.06246737390756607
epoch£º364	 i:7 	 global-step:7287	 l-p:0.17702510952949524
epoch£º364	 i:8 	 global-step:7288	 l-p:0.13269321620464325
epoch£º364	 i:9 	 global-step:7289	 l-p:0.042133063077926636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:365
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8745, 4.8745, 4.8745],
        [4.8745, 4.8477, 4.7993],
        [4.8745, 4.8386, 4.8158],
        [4.8745, 4.8679, 4.8735]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:365, step:0 
model_pd.l_p.mean(): 0.09412350505590439 
model_pd.l_d.mean(): -20.60939598083496 
model_pd.lagr.mean(): -20.51527214050293 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4618], device='cuda:0')), ('power', tensor([-21.4730], device='cuda:0'))])
epoch£º365	 i:0 	 global-step:7300	 l-p:0.09412350505590439
epoch£º365	 i:1 	 global-step:7301	 l-p:0.17349977791309357
epoch£º365	 i:2 	 global-step:7302	 l-p:0.14865054190158844
epoch£º365	 i:3 	 global-step:7303	 l-p:0.10600762814283371
epoch£º365	 i:4 	 global-step:7304	 l-p:0.1436293125152588
epoch£º365	 i:5 	 global-step:7305	 l-p:0.1360393911600113
epoch£º365	 i:6 	 global-step:7306	 l-p:0.4258115291595459
epoch£º365	 i:7 	 global-step:7307	 l-p:0.10812197625637054
epoch£º365	 i:8 	 global-step:7308	 l-p:0.12224621325731277
epoch£º365	 i:9 	 global-step:7309	 l-p:0.13309575617313385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:366
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9717, 4.9677, 4.9712],
        [4.9717, 5.6557, 5.9127],
        [4.9717, 5.7447, 6.0810],
        [4.9717, 4.9685, 4.9714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:366, step:0 
model_pd.l_p.mean(): 0.11464784294366837 
model_pd.l_d.mean(): -18.548322677612305 
model_pd.lagr.mean(): -18.43367576599121 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6046], device='cuda:0')), ('power', tensor([-19.5214], device='cuda:0'))])
epoch£º366	 i:0 	 global-step:7320	 l-p:0.11464784294366837
epoch£º366	 i:1 	 global-step:7321	 l-p:0.13449907302856445
epoch£º366	 i:2 	 global-step:7322	 l-p:0.13347014784812927
epoch£º366	 i:3 	 global-step:7323	 l-p:0.13494841754436493
epoch£º366	 i:4 	 global-step:7324	 l-p:0.25551852583885193
epoch£º366	 i:5 	 global-step:7325	 l-p:0.13127067685127258
epoch£º366	 i:6 	 global-step:7326	 l-p:0.010623998008668423
epoch£º366	 i:7 	 global-step:7327	 l-p:0.13753418624401093
epoch£º366	 i:8 	 global-step:7328	 l-p:-0.08790376782417297
epoch£º366	 i:9 	 global-step:7329	 l-p:0.12380465865135193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:367
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228]], device='cuda:0')
 pt:tensor([[5.1011, 5.8432, 6.1381],
        [5.1011, 5.0833, 5.0397],
        [5.1011, 5.0729, 5.0570],
        [5.1011, 5.1824, 5.0815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:367, step:0 
model_pd.l_p.mean(): 0.19787408411502838 
model_pd.l_d.mean(): -19.183940887451172 
model_pd.lagr.mean(): -18.986066818237305 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4974], device='cuda:0')), ('power', tensor([-20.0579], device='cuda:0'))])
epoch£º367	 i:0 	 global-step:7340	 l-p:0.19787408411502838
epoch£º367	 i:1 	 global-step:7341	 l-p:0.14188170433044434
epoch£º367	 i:2 	 global-step:7342	 l-p:0.13992153108119965
epoch£º367	 i:3 	 global-step:7343	 l-p:0.14401470124721527
epoch£º367	 i:4 	 global-step:7344	 l-p:0.13067330420017242
epoch£º367	 i:5 	 global-step:7345	 l-p:0.1300268918275833
epoch£º367	 i:6 	 global-step:7346	 l-p:0.14319483935832977
epoch£º367	 i:7 	 global-step:7347	 l-p:0.12046579271554947
epoch£º367	 i:8 	 global-step:7348	 l-p:0.1006721630692482
epoch£º367	 i:9 	 global-step:7349	 l-p:0.129579558968544
====================================================================================================
====================================================================================================
====================================================================================================

epoch:368
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2141, 5.2141, 5.2141],
        [5.2141, 5.2141, 5.2141],
        [5.2141, 5.2118, 5.2139],
        [5.2141, 5.2571, 5.1675]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:368, step:0 
model_pd.l_p.mean(): 0.15726253390312195 
model_pd.l_d.mean(): -20.13897705078125 
model_pd.lagr.mean(): -19.981714248657227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4214], device='cuda:0')), ('power', tensor([-20.9520], device='cuda:0'))])
epoch£º368	 i:0 	 global-step:7360	 l-p:0.15726253390312195
epoch£º368	 i:1 	 global-step:7361	 l-p:0.08747375011444092
epoch£º368	 i:2 	 global-step:7362	 l-p:0.12410201877355576
epoch£º368	 i:3 	 global-step:7363	 l-p:0.20916396379470825
epoch£º368	 i:4 	 global-step:7364	 l-p:0.11632543057203293
epoch£º368	 i:5 	 global-step:7365	 l-p:0.13578109443187714
epoch£º368	 i:6 	 global-step:7366	 l-p:0.15826421976089478
epoch£º368	 i:7 	 global-step:7367	 l-p:0.11757920682430267
epoch£º368	 i:8 	 global-step:7368	 l-p:0.10902925580739975
epoch£º368	 i:9 	 global-step:7369	 l-p:0.18942002952098846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:369
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0219, 5.1234, 5.0191],
        [5.0219, 5.0200, 5.0217],
        [5.0219, 5.0219, 5.0219],
        [5.0219, 5.7383, 6.0180]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:369, step:0 
model_pd.l_p.mean(): 0.15032148361206055 
model_pd.l_d.mean(): -19.580551147460938 
model_pd.lagr.mean(): -19.43022918701172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5405], device='cuda:0')), ('power', tensor([-20.5065], device='cuda:0'))])
epoch£º369	 i:0 	 global-step:7380	 l-p:0.15032148361206055
epoch£º369	 i:1 	 global-step:7381	 l-p:0.1236921176314354
epoch£º369	 i:2 	 global-step:7382	 l-p:0.08885029703378677
epoch£º369	 i:3 	 global-step:7383	 l-p:0.1781194806098938
epoch£º369	 i:4 	 global-step:7384	 l-p:0.12784500420093536
epoch£º369	 i:5 	 global-step:7385	 l-p:0.12351220101118088
epoch£º369	 i:6 	 global-step:7386	 l-p:0.17001739144325256
epoch£º369	 i:7 	 global-step:7387	 l-p:0.09550575911998749
epoch£º369	 i:8 	 global-step:7388	 l-p:-0.38479697704315186
epoch£º369	 i:9 	 global-step:7389	 l-p:0.12790042161941528
====================================================================================================
====================================================================================================
====================================================================================================

epoch:370
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0231, 5.0560, 4.9596],
        [5.0231, 5.0205, 5.0229],
        [5.0231, 5.5285, 5.6333],
        [5.0231, 4.9894, 4.9894]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:370, step:0 
model_pd.l_p.mean(): 0.1320725530385971 
model_pd.l_d.mean(): -17.821067810058594 
model_pd.lagr.mean(): -17.688995361328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5622], device='cuda:0')), ('power', tensor([-18.7366], device='cuda:0'))])
epoch£º370	 i:0 	 global-step:7400	 l-p:0.1320725530385971
epoch£º370	 i:1 	 global-step:7401	 l-p:0.13677950203418732
epoch£º370	 i:2 	 global-step:7402	 l-p:0.0892101302742958
epoch£º370	 i:3 	 global-step:7403	 l-p:0.13841615617275238
epoch£º370	 i:4 	 global-step:7404	 l-p:0.13345618546009064
epoch£º370	 i:5 	 global-step:7405	 l-p:0.1348160207271576
epoch£º370	 i:6 	 global-step:7406	 l-p:0.15243257582187653
epoch£º370	 i:7 	 global-step:7407	 l-p:0.11671265959739685
epoch£º370	 i:8 	 global-step:7408	 l-p:-0.4181952178478241
epoch£º370	 i:9 	 global-step:7409	 l-p:0.12221265584230423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:371
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0400, 5.0400, 5.0400],
        [5.0400, 5.0387, 5.0400],
        [5.0400, 5.7507, 6.0232],
        [5.0400, 5.0281, 5.0370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:371, step:0 
model_pd.l_p.mean(): -0.058371253311634064 
model_pd.l_d.mean(): -20.125112533569336 
model_pd.lagr.mean(): -20.183483123779297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4686], device='cuda:0')), ('power', tensor([-20.9867], device='cuda:0'))])
epoch£º371	 i:0 	 global-step:7420	 l-p:-0.058371253311634064
epoch£º371	 i:1 	 global-step:7421	 l-p:0.1291508972644806
epoch£º371	 i:2 	 global-step:7422	 l-p:0.12131905555725098
epoch£º371	 i:3 	 global-step:7423	 l-p:0.14575734734535217
epoch£º371	 i:4 	 global-step:7424	 l-p:0.11172088235616684
epoch£º371	 i:5 	 global-step:7425	 l-p:0.0034221457317471504
epoch£º371	 i:6 	 global-step:7426	 l-p:0.12022778391838074
epoch£º371	 i:7 	 global-step:7427	 l-p:0.20286089181900024
epoch£º371	 i:8 	 global-step:7428	 l-p:0.1303388476371765
epoch£º371	 i:9 	 global-step:7429	 l-p:0.1225564256310463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:372
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8515, 5.6272, 5.9769],
        [4.8515, 4.8515, 4.8515],
        [4.8515, 4.8500, 4.8514],
        [4.8515, 4.8492, 4.7590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:372, step:0 
model_pd.l_p.mean(): 0.1304258555173874 
model_pd.l_d.mean(): -19.27580451965332 
model_pd.lagr.mean(): -19.14537811279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5540], device='cuda:0')), ('power', tensor([-20.2101], device='cuda:0'))])
epoch£º372	 i:0 	 global-step:7440	 l-p:0.1304258555173874
epoch£º372	 i:1 	 global-step:7441	 l-p:0.07105018198490143
epoch£º372	 i:2 	 global-step:7442	 l-p:0.1345241367816925
epoch£º372	 i:3 	 global-step:7443	 l-p:0.15807364881038666
epoch£º372	 i:4 	 global-step:7444	 l-p:0.18096895515918732
epoch£º372	 i:5 	 global-step:7445	 l-p:0.13163360953330994
epoch£º372	 i:6 	 global-step:7446	 l-p:0.2025877684354782
epoch£º372	 i:7 	 global-step:7447	 l-p:0.1611585170030594
epoch£º372	 i:8 	 global-step:7448	 l-p:0.14109265804290771
epoch£º372	 i:9 	 global-step:7449	 l-p:0.07721962034702301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:373
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8683, 5.5162, 5.7504],
        [4.8683, 4.8483, 4.8612],
        [4.8683, 4.8683, 4.8683],
        [4.8683, 4.9002, 4.7937]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:373, step:0 
model_pd.l_p.mean(): 0.14260025322437286 
model_pd.l_d.mean(): -18.877222061157227 
model_pd.lagr.mean(): -18.734621047973633 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5709], device='cuda:0')), ('power', tensor([-19.8215], device='cuda:0'))])
epoch£º373	 i:0 	 global-step:7460	 l-p:0.14260025322437286
epoch£º373	 i:1 	 global-step:7461	 l-p:-0.16112056374549866
epoch£º373	 i:2 	 global-step:7462	 l-p:0.07570785284042358
epoch£º373	 i:3 	 global-step:7463	 l-p:0.15433621406555176
epoch£º373	 i:4 	 global-step:7464	 l-p:0.11731371283531189
epoch£º373	 i:5 	 global-step:7465	 l-p:0.13164396584033966
epoch£º373	 i:6 	 global-step:7466	 l-p:0.14294946193695068
epoch£º373	 i:7 	 global-step:7467	 l-p:0.12844979763031006
epoch£º373	 i:8 	 global-step:7468	 l-p:0.1448461264371872
epoch£º373	 i:9 	 global-step:7469	 l-p:0.10043981671333313
====================================================================================================
====================================================================================================
====================================================================================================

epoch:374
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9825, 4.9734, 4.8958],
        [4.9825, 4.9560, 4.9686],
        [4.9825, 4.9620, 4.9745],
        [4.9825, 5.7356, 6.0505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:374, step:0 
model_pd.l_p.mean(): 0.09201012551784515 
model_pd.l_d.mean(): -20.44258689880371 
model_pd.lagr.mean(): -20.350576400756836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4464], device='cuda:0')), ('power', tensor([-21.2872], device='cuda:0'))])
epoch£º374	 i:0 	 global-step:7480	 l-p:0.09201012551784515
epoch£º374	 i:1 	 global-step:7481	 l-p:0.2102273553609848
epoch£º374	 i:2 	 global-step:7482	 l-p:0.05330653116106987
epoch£º374	 i:3 	 global-step:7483	 l-p:0.1248088926076889
epoch£º374	 i:4 	 global-step:7484	 l-p:0.13689389824867249
epoch£º374	 i:5 	 global-step:7485	 l-p:0.1340159922838211
epoch£º374	 i:6 	 global-step:7486	 l-p:0.06884374469518661
epoch£º374	 i:7 	 global-step:7487	 l-p:0.10640659928321838
epoch£º374	 i:8 	 global-step:7488	 l-p:0.1392732560634613
epoch£º374	 i:9 	 global-step:7489	 l-p:0.028217215090990067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:375
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9263, 4.9222, 4.9259],
        [4.9263, 4.9263, 4.9263],
        [4.9263, 4.9214, 4.9258],
        [4.9263, 4.9262, 4.9263]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:375, step:0 
model_pd.l_p.mean(): 0.06187911331653595 
model_pd.l_d.mean(): -20.30550765991211 
model_pd.lagr.mean(): -20.243629455566406 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4582], device='cuda:0')), ('power', tensor([-21.1598], device='cuda:0'))])
epoch£º375	 i:0 	 global-step:7500	 l-p:0.06187911331653595
epoch£º375	 i:1 	 global-step:7501	 l-p:0.250184029340744
epoch£º375	 i:2 	 global-step:7502	 l-p:0.14692530035972595
epoch£º375	 i:3 	 global-step:7503	 l-p:0.11800576746463776
epoch£º375	 i:4 	 global-step:7504	 l-p:0.12639780342578888
epoch£º375	 i:5 	 global-step:7505	 l-p:0.13131703436374664
epoch£º375	 i:6 	 global-step:7506	 l-p:0.17589285969734192
epoch£º375	 i:7 	 global-step:7507	 l-p:0.13101553916931152
epoch£º375	 i:8 	 global-step:7508	 l-p:0.14489875733852386
epoch£º375	 i:9 	 global-step:7509	 l-p:0.11594881117343903
====================================================================================================
====================================================================================================
====================================================================================================

epoch:376
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0811, 5.0500, 5.0581],
        [5.0811, 5.1185, 5.0178],
        [5.0811, 5.0810, 5.0811],
        [5.0811, 5.0543, 5.0080]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:376, step:0 
model_pd.l_p.mean(): 0.1096988171339035 
model_pd.l_d.mean(): -19.52032470703125 
model_pd.lagr.mean(): -19.410625457763672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4936], device='cuda:0')), ('power', tensor([-20.3965], device='cuda:0'))])
epoch£º376	 i:0 	 global-step:7520	 l-p:0.1096988171339035
epoch£º376	 i:1 	 global-step:7521	 l-p:0.16507859528064728
epoch£º376	 i:2 	 global-step:7522	 l-p:0.16140958666801453
epoch£º376	 i:3 	 global-step:7523	 l-p:0.13195815682411194
epoch£º376	 i:4 	 global-step:7524	 l-p:0.28144365549087524
epoch£º376	 i:5 	 global-step:7525	 l-p:0.13529275357723236
epoch£º376	 i:6 	 global-step:7526	 l-p:0.1331511288881302
epoch£º376	 i:7 	 global-step:7527	 l-p:0.10950327664613724
epoch£º376	 i:8 	 global-step:7528	 l-p:0.13168279826641083
epoch£º376	 i:9 	 global-step:7529	 l-p:-0.2755753695964813
====================================================================================================
====================================================================================================
====================================================================================================

epoch:377
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0719, 5.0489, 4.9928],
        [5.0719, 5.0406, 5.0497],
        [5.0719, 5.0443, 5.0558],
        [5.0719, 5.7534, 5.9960]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:377, step:0 
model_pd.l_p.mean(): -0.07945435494184494 
model_pd.l_d.mean(): -20.118213653564453 
model_pd.lagr.mean(): -20.197668075561523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4423], device='cuda:0')), ('power', tensor([-20.9525], device='cuda:0'))])
epoch£º377	 i:0 	 global-step:7540	 l-p:-0.07945435494184494
epoch£º377	 i:1 	 global-step:7541	 l-p:-0.004314951598644257
epoch£º377	 i:2 	 global-step:7542	 l-p:0.16317006945610046
epoch£º377	 i:3 	 global-step:7543	 l-p:0.20959541201591492
epoch£º377	 i:4 	 global-step:7544	 l-p:0.1141868531703949
epoch£º377	 i:5 	 global-step:7545	 l-p:0.127093106508255
epoch£º377	 i:6 	 global-step:7546	 l-p:0.14038261771202087
epoch£º377	 i:7 	 global-step:7547	 l-p:0.1303846836090088
epoch£º377	 i:8 	 global-step:7548	 l-p:0.06483057141304016
epoch£º377	 i:9 	 global-step:7549	 l-p:0.12035553902387619
====================================================================================================
====================================================================================================
====================================================================================================

epoch:378
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2409, 5.2671, 5.1773],
        [5.2409, 5.2409, 5.2409],
        [5.2409, 5.2098, 5.2025],
        [5.2409, 5.2170, 5.2268]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:378, step:0 
model_pd.l_p.mean(): 0.13505321741104126 
model_pd.l_d.mean(): -19.737714767456055 
model_pd.lagr.mean(): -19.6026611328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4545], device='cuda:0')), ('power', tensor([-20.5776], device='cuda:0'))])
epoch£º378	 i:0 	 global-step:7560	 l-p:0.13505321741104126
epoch£º378	 i:1 	 global-step:7561	 l-p:0.10045342147350311
epoch£º378	 i:2 	 global-step:7562	 l-p:0.13089418411254883
epoch£º378	 i:3 	 global-step:7563	 l-p:0.13466238975524902
epoch£º378	 i:4 	 global-step:7564	 l-p:0.10561943799257278
epoch£º378	 i:5 	 global-step:7565	 l-p:0.15891651809215546
epoch£º378	 i:6 	 global-step:7566	 l-p:0.14426349103450775
epoch£º378	 i:7 	 global-step:7567	 l-p:0.12388934940099716
epoch£º378	 i:8 	 global-step:7568	 l-p:0.1238483116030693
epoch£º378	 i:9 	 global-step:7569	 l-p:0.13402724266052246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:379
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0210, 4.9831, 4.9494],
        [5.0210, 5.0210, 5.0210],
        [5.0210, 5.0112, 5.0191],
        [5.0210, 4.9922, 4.9369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:379, step:0 
model_pd.l_p.mean(): 0.14495083689689636 
model_pd.l_d.mean(): -18.960758209228516 
model_pd.lagr.mean(): -18.815807342529297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5300], device='cuda:0')), ('power', tensor([-19.8642], device='cuda:0'))])
epoch£º379	 i:0 	 global-step:7580	 l-p:0.14495083689689636
epoch£º379	 i:1 	 global-step:7581	 l-p:0.0896923840045929
epoch£º379	 i:2 	 global-step:7582	 l-p:0.11686190217733383
epoch£º379	 i:3 	 global-step:7583	 l-p:-0.007141503971070051
epoch£º379	 i:4 	 global-step:7584	 l-p:0.13273119926452637
epoch£º379	 i:5 	 global-step:7585	 l-p:0.1350095272064209
epoch£º379	 i:6 	 global-step:7586	 l-p:-0.8058869242668152
epoch£º379	 i:7 	 global-step:7587	 l-p:0.1311168223619461
epoch£º379	 i:8 	 global-step:7588	 l-p:0.14228323101997375
epoch£º379	 i:9 	 global-step:7589	 l-p:0.11735708266496658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:380
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8256, 4.9712, 4.8688],
        [4.8256, 4.8110, 4.8221],
        [4.8256, 4.8253, 4.8256],
        [4.8256, 4.8226, 4.8254]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:380, step:0 
model_pd.l_p.mean(): 0.15059056878089905 
model_pd.l_d.mean(): -20.24444007873535 
model_pd.lagr.mean(): -20.093849182128906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5062], device='cuda:0')), ('power', tensor([-21.1473], device='cuda:0'))])
epoch£º380	 i:0 	 global-step:7600	 l-p:0.15059056878089905
epoch£º380	 i:1 	 global-step:7601	 l-p:0.1705632209777832
epoch£º380	 i:2 	 global-step:7602	 l-p:0.12602180242538452
epoch£º380	 i:3 	 global-step:7603	 l-p:0.12486850470304489
epoch£º380	 i:4 	 global-step:7604	 l-p:0.11969073116779327
epoch£º380	 i:5 	 global-step:7605	 l-p:0.15885190665721893
epoch£º380	 i:6 	 global-step:7606	 l-p:0.1018846407532692
epoch£º380	 i:7 	 global-step:7607	 l-p:0.03134073317050934
epoch£º380	 i:8 	 global-step:7608	 l-p:0.03561769425868988
epoch£º380	 i:9 	 global-step:7609	 l-p:-0.2697426676750183
====================================================================================================
====================================================================================================
====================================================================================================

epoch:381
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.7195, 4.7194, 4.7195],
        [4.7195, 5.1008, 5.1349],
        [4.7195, 4.6797, 4.6957],
        [4.7195, 4.6607, 4.6435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:381, step:0 
model_pd.l_p.mean(): 0.6172029972076416 
model_pd.l_d.mean(): -19.953834533691406 
model_pd.lagr.mean(): -19.336631774902344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6011], device='cuda:0')), ('power', tensor([-20.9495], device='cuda:0'))])
epoch£º381	 i:0 	 global-step:7620	 l-p:0.6172029972076416
epoch£º381	 i:1 	 global-step:7621	 l-p:0.278766006231308
epoch£º381	 i:2 	 global-step:7622	 l-p:0.10700876265764236
epoch£º381	 i:3 	 global-step:7623	 l-p:0.10396614670753479
epoch£º381	 i:4 	 global-step:7624	 l-p:-0.12710216641426086
epoch£º381	 i:5 	 global-step:7625	 l-p:0.12833701074123383
epoch£º381	 i:6 	 global-step:7626	 l-p:0.13045914471149445
epoch£º381	 i:7 	 global-step:7627	 l-p:0.24247629940509796
epoch£º381	 i:8 	 global-step:7628	 l-p:0.11522958427667618
epoch£º381	 i:9 	 global-step:7629	 l-p:0.14207608997821808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:382
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1055, 5.0667, 5.0648],
        [5.1055, 5.1053, 5.1055],
        [5.1055, 5.1048, 5.1055],
        [5.1055, 5.0664, 5.0445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:382, step:0 
model_pd.l_p.mean(): 0.13103248178958893 
model_pd.l_d.mean(): -20.632131576538086 
model_pd.lagr.mean(): -20.5010986328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3782], device='cuda:0')), ('power', tensor([-21.4097], device='cuda:0'))])
epoch£º382	 i:0 	 global-step:7640	 l-p:0.13103248178958893
epoch£º382	 i:1 	 global-step:7641	 l-p:0.16211271286010742
epoch£º382	 i:2 	 global-step:7642	 l-p:0.15822626650333405
epoch£º382	 i:3 	 global-step:7643	 l-p:0.13730905950069427
epoch£º382	 i:4 	 global-step:7644	 l-p:0.038821615278720856
epoch£º382	 i:5 	 global-step:7645	 l-p:0.11619865149259567
epoch£º382	 i:6 	 global-step:7646	 l-p:0.0877194032073021
epoch£º382	 i:7 	 global-step:7647	 l-p:0.1930653154850006
epoch£º382	 i:8 	 global-step:7648	 l-p:0.0371130108833313
epoch£º382	 i:9 	 global-step:7649	 l-p:0.1102331355214119
====================================================================================================
====================================================================================================
====================================================================================================

epoch:383
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1089, 5.1060, 5.1087],
        [5.1089, 5.1007, 5.0204],
        [5.1089, 5.1089, 5.1089],
        [5.1089, 5.0772, 5.0881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:383, step:0 
model_pd.l_p.mean(): 0.12756459414958954 
model_pd.l_d.mean(): -20.033288955688477 
model_pd.lagr.mean(): -19.905723571777344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4497], device='cuda:0')), ('power', tensor([-20.8737], device='cuda:0'))])
epoch£º383	 i:0 	 global-step:7660	 l-p:0.12756459414958954
epoch£º383	 i:1 	 global-step:7661	 l-p:0.3226846158504486
epoch£º383	 i:2 	 global-step:7662	 l-p:0.12943634390830994
epoch£º383	 i:3 	 global-step:7663	 l-p:0.1212315708398819
epoch£º383	 i:4 	 global-step:7664	 l-p:0.08323009312152863
epoch£º383	 i:5 	 global-step:7665	 l-p:0.1309398114681244
epoch£º383	 i:6 	 global-step:7666	 l-p:0.1401354968547821
epoch£º383	 i:7 	 global-step:7667	 l-p:0.13370412588119507
epoch£º383	 i:8 	 global-step:7668	 l-p:0.1552327573299408
epoch£º383	 i:9 	 global-step:7669	 l-p:0.2793780267238617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:384
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0246, 5.5473, 5.6624],
        [5.0246, 4.9842, 4.9431],
        [5.0246, 5.2695, 5.2029],
        [5.0246, 4.9996, 5.0136]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:384, step:0 
model_pd.l_p.mean(): 0.22210632264614105 
model_pd.l_d.mean(): -20.59221839904785 
model_pd.lagr.mean(): -20.3701114654541 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4293], device='cuda:0')), ('power', tensor([-21.4219], device='cuda:0'))])
epoch£º384	 i:0 	 global-step:7680	 l-p:0.22210632264614105
epoch£º384	 i:1 	 global-step:7681	 l-p:0.15160199999809265
epoch£º384	 i:2 	 global-step:7682	 l-p:0.13101975619792938
epoch£º384	 i:3 	 global-step:7683	 l-p:0.12534305453300476
epoch£º384	 i:4 	 global-step:7684	 l-p:0.1366596817970276
epoch£º384	 i:5 	 global-step:7685	 l-p:0.10270065814256668
epoch£º384	 i:6 	 global-step:7686	 l-p:0.10058175027370453
epoch£º384	 i:7 	 global-step:7687	 l-p:0.97335284948349
epoch£º384	 i:8 	 global-step:7688	 l-p:0.10690310597419739
epoch£º384	 i:9 	 global-step:7689	 l-p:0.12900014221668243
====================================================================================================
====================================================================================================
====================================================================================================

epoch:385
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0768, 5.0685, 4.9834],
        [5.0768, 5.0748, 5.0767],
        [5.0768, 5.0768, 5.0768],
        [5.0768, 5.5745, 5.6664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:385, step:0 
model_pd.l_p.mean(): 0.13160498440265656 
model_pd.l_d.mean(): -20.055896759033203 
model_pd.lagr.mean(): -19.924291610717773 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4660], device='cuda:0')), ('power', tensor([-20.9136], device='cuda:0'))])
epoch£º385	 i:0 	 global-step:7700	 l-p:0.13160498440265656
epoch£º385	 i:1 	 global-step:7701	 l-p:0.11907119303941727
epoch£º385	 i:2 	 global-step:7702	 l-p:0.1376580148935318
epoch£º385	 i:3 	 global-step:7703	 l-p:0.09417036920785904
epoch£º385	 i:4 	 global-step:7704	 l-p:0.10733874887228012
epoch£º385	 i:5 	 global-step:7705	 l-p:0.13844186067581177
epoch£º385	 i:6 	 global-step:7706	 l-p:0.029075482860207558
epoch£º385	 i:7 	 global-step:7707	 l-p:0.10467220842838287
epoch£º385	 i:8 	 global-step:7708	 l-p:0.15224221348762512
epoch£º385	 i:9 	 global-step:7709	 l-p:0.19226111471652985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:386
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8888, 4.8484, 4.8617],
        [4.8888, 5.0765, 4.9863],
        [4.8888, 4.8888, 4.8888],
        [4.8888, 4.8568, 4.8727]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:386, step:0 
model_pd.l_p.mean(): 0.15291087329387665 
model_pd.l_d.mean(): -20.005901336669922 
model_pd.lagr.mean(): -19.852991104125977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5408], device='cuda:0')), ('power', tensor([-20.9402], device='cuda:0'))])
epoch£º386	 i:0 	 global-step:7720	 l-p:0.15291087329387665
epoch£º386	 i:1 	 global-step:7721	 l-p:0.12353309243917465
epoch£º386	 i:2 	 global-step:7722	 l-p:0.15606246888637543
epoch£º386	 i:3 	 global-step:7723	 l-p:1.3312698602676392
epoch£º386	 i:4 	 global-step:7724	 l-p:0.07656754553318024
epoch£º386	 i:5 	 global-step:7725	 l-p:0.10204313695430756
epoch£º386	 i:6 	 global-step:7726	 l-p:0.11485128104686737
epoch£º386	 i:7 	 global-step:7727	 l-p:0.12527428567409515
epoch£º386	 i:8 	 global-step:7728	 l-p:0.21259166300296783
epoch£º386	 i:9 	 global-step:7729	 l-p:0.14756402373313904
====================================================================================================
====================================================================================================
====================================================================================================

epoch:387
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2466, 5.2253, 5.2376],
        [5.2466, 5.7299, 5.7995],
        [5.2466, 5.2461, 5.2465],
        [5.2466, 5.2142, 5.2216]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:387, step:0 
model_pd.l_p.mean(): 0.12650622427463531 
model_pd.l_d.mean(): -20.442888259887695 
model_pd.lagr.mean(): -20.316381454467773 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3726], device='cuda:0')), ('power', tensor([-21.2110], device='cuda:0'))])
epoch£º387	 i:0 	 global-step:7740	 l-p:0.12650622427463531
epoch£º387	 i:1 	 global-step:7741	 l-p:0.1260218620300293
epoch£º387	 i:2 	 global-step:7742	 l-p:0.12200803309679031
epoch£º387	 i:3 	 global-step:7743	 l-p:0.2676510512828827
epoch£º387	 i:4 	 global-step:7744	 l-p:0.1140885129570961
epoch£º387	 i:5 	 global-step:7745	 l-p:0.1267811506986618
epoch£º387	 i:6 	 global-step:7746	 l-p:0.10465513169765472
epoch£º387	 i:7 	 global-step:7747	 l-p:0.12807773053646088
epoch£º387	 i:8 	 global-step:7748	 l-p:0.13369543850421906
epoch£º387	 i:9 	 global-step:7749	 l-p:0.11129222810268402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:388
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2121, 5.2091, 5.2118],
        [5.2121, 5.1896, 5.2025],
        [5.2121, 5.2085, 5.2117],
        [5.2121, 5.2016, 5.1255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:388, step:0 
model_pd.l_p.mean(): 0.10680213570594788 
model_pd.l_d.mean(): -19.267292022705078 
model_pd.lagr.mean(): -19.160490036010742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5248], device='cuda:0')), ('power', tensor([-20.1711], device='cuda:0'))])
epoch£º388	 i:0 	 global-step:7760	 l-p:0.10680213570594788
epoch£º388	 i:1 	 global-step:7761	 l-p:0.21367312967777252
epoch£º388	 i:2 	 global-step:7762	 l-p:0.48939406871795654
epoch£º388	 i:3 	 global-step:7763	 l-p:0.17035254836082458
epoch£º388	 i:4 	 global-step:7764	 l-p:0.13244298100471497
epoch£º388	 i:5 	 global-step:7765	 l-p:0.1557282656431198
epoch£º388	 i:6 	 global-step:7766	 l-p:-8.020642280578613
epoch£º388	 i:7 	 global-step:7767	 l-p:0.155448779463768
epoch£º388	 i:8 	 global-step:7768	 l-p:0.13485907018184662
epoch£º388	 i:9 	 global-step:7769	 l-p:0.1309005171060562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:389
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9017, 4.9431, 4.8222],
        [4.9017, 4.8568, 4.7937],
        [4.9017, 4.9015, 4.9017],
        [4.9017, 4.8612, 4.8753]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:389, step:0 
model_pd.l_p.mean(): 0.13389398157596588 
model_pd.l_d.mean(): -19.25383186340332 
model_pd.lagr.mean(): -19.119937896728516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5549], device='cuda:0')), ('power', tensor([-20.1886], device='cuda:0'))])
epoch£º389	 i:0 	 global-step:7780	 l-p:0.13389398157596588
epoch£º389	 i:1 	 global-step:7781	 l-p:0.11791283637285233
epoch£º389	 i:2 	 global-step:7782	 l-p:0.15849338471889496
epoch£º389	 i:3 	 global-step:7783	 l-p:0.1513652503490448
epoch£º389	 i:4 	 global-step:7784	 l-p:0.09521093219518661
epoch£º389	 i:5 	 global-step:7785	 l-p:0.15800690650939941
epoch£º389	 i:6 	 global-step:7786	 l-p:0.1696867197751999
epoch£º389	 i:7 	 global-step:7787	 l-p:0.15613579750061035
epoch£º389	 i:8 	 global-step:7788	 l-p:0.10530953854322433
epoch£º389	 i:9 	 global-step:7789	 l-p:0.08207057416439056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:390
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0251, 5.0248, 5.0251],
        [5.0251, 4.9755, 4.9563],
        [5.0251, 5.5534, 5.6715],
        [5.0251, 5.0248, 5.0251]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:390, step:0 
model_pd.l_p.mean(): 0.056378357112407684 
model_pd.l_d.mean(): -20.50571060180664 
model_pd.lagr.mean(): -20.44933319091797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4355], device='cuda:0')), ('power', tensor([-21.3402], device='cuda:0'))])
epoch£º390	 i:0 	 global-step:7800	 l-p:0.056378357112407684
epoch£º390	 i:1 	 global-step:7801	 l-p:0.11516469717025757
epoch£º390	 i:2 	 global-step:7802	 l-p:0.12230642139911652
epoch£º390	 i:3 	 global-step:7803	 l-p:0.1386622041463852
epoch£º390	 i:4 	 global-step:7804	 l-p:0.11892154067754745
epoch£º390	 i:5 	 global-step:7805	 l-p:0.1364727020263672
epoch£º390	 i:6 	 global-step:7806	 l-p:0.11632691323757172
epoch£º390	 i:7 	 global-step:7807	 l-p:0.12766481935977936
epoch£º390	 i:8 	 global-step:7808	 l-p:0.14058633148670197
epoch£º390	 i:9 	 global-step:7809	 l-p:0.13265873491764069
====================================================================================================
====================================================================================================
====================================================================================================

epoch:391
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1642, 5.1562, 5.1629],
        [5.1642, 5.1567, 5.0698],
        [5.1642, 5.1385, 5.0720],
        [5.1642, 5.1605, 5.1639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:391, step:0 
model_pd.l_p.mean(): 0.1499236524105072 
model_pd.l_d.mean(): -20.50038719177246 
model_pd.lagr.mean(): -20.3504638671875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3899], device='cuda:0')), ('power', tensor([-21.2876], device='cuda:0'))])
epoch£º391	 i:0 	 global-step:7820	 l-p:0.1499236524105072
epoch£º391	 i:1 	 global-step:7821	 l-p:0.3913705348968506
epoch£º391	 i:2 	 global-step:7822	 l-p:0.13182133436203003
epoch£º391	 i:3 	 global-step:7823	 l-p:0.12110196053981781
epoch£º391	 i:4 	 global-step:7824	 l-p:0.11355438083410263
epoch£º391	 i:5 	 global-step:7825	 l-p:0.13343380391597748
epoch£º391	 i:6 	 global-step:7826	 l-p:0.10595083981752396
epoch£º391	 i:7 	 global-step:7827	 l-p:0.1234261617064476
epoch£º391	 i:8 	 global-step:7828	 l-p:0.12701626121997833
epoch£º391	 i:9 	 global-step:7829	 l-p:0.23446066677570343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:392
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8279, 4.7873, 4.8044],
        [4.8279, 4.8276, 4.8279],
        [4.8279, 5.5484, 5.8438],
        [4.8279, 4.8279, 4.8279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:392, step:0 
model_pd.l_p.mean(): 0.13286344707012177 
model_pd.l_d.mean(): -19.767250061035156 
model_pd.lagr.mean(): -19.63438606262207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5177], device='cuda:0')), ('power', tensor([-20.6731], device='cuda:0'))])
epoch£º392	 i:0 	 global-step:7840	 l-p:0.13286344707012177
epoch£º392	 i:1 	 global-step:7841	 l-p:0.15985897183418274
epoch£º392	 i:2 	 global-step:7842	 l-p:0.15431199967861176
epoch£º392	 i:3 	 global-step:7843	 l-p:0.19145217537879944
epoch£º392	 i:4 	 global-step:7844	 l-p:0.1785135269165039
epoch£º392	 i:5 	 global-step:7845	 l-p:0.020150193944573402
epoch£º392	 i:6 	 global-step:7846	 l-p:0.15284854173660278
epoch£º392	 i:7 	 global-step:7847	 l-p:0.016083354130387306
epoch£º392	 i:8 	 global-step:7848	 l-p:0.13172666728496552
epoch£º392	 i:9 	 global-step:7849	 l-p:0.11646022647619247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:393
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9783, 5.5826, 5.7646],
        [4.9783, 4.9783, 4.9783],
        [4.9783, 5.1729, 5.0811],
        [4.9783, 4.9783, 4.9783]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:393, step:0 
model_pd.l_p.mean(): -0.4008702039718628 
model_pd.l_d.mean(): -20.17491340637207 
model_pd.lagr.mean(): -20.575782775878906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4974], device='cuda:0')), ('power', tensor([-21.0674], device='cuda:0'))])
epoch£º393	 i:0 	 global-step:7860	 l-p:-0.4008702039718628
epoch£º393	 i:1 	 global-step:7861	 l-p:0.1716107577085495
epoch£º393	 i:2 	 global-step:7862	 l-p:0.15986405313014984
epoch£º393	 i:3 	 global-step:7863	 l-p:0.1193760335445404
epoch£º393	 i:4 	 global-step:7864	 l-p:0.2624113857746124
epoch£º393	 i:5 	 global-step:7865	 l-p:0.1256503015756607
epoch£º393	 i:6 	 global-step:7866	 l-p:0.2573847472667694
epoch£º393	 i:7 	 global-step:7867	 l-p:0.13958491384983063
epoch£º393	 i:8 	 global-step:7868	 l-p:0.13128702342510223
epoch£º393	 i:9 	 global-step:7869	 l-p:0.11636554449796677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:394
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1326, 5.0959, 5.1082],
        [5.1326, 5.1322, 5.1326],
        [5.1326, 5.0966, 5.0376],
        [5.1326, 5.8670, 6.1456]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:394, step:0 
model_pd.l_p.mean(): 0.15262679755687714 
model_pd.l_d.mean(): -20.380558013916016 
model_pd.lagr.mean(): -20.22793197631836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4121], device='cuda:0')), ('power', tensor([-21.1884], device='cuda:0'))])
epoch£º394	 i:0 	 global-step:7880	 l-p:0.15262679755687714
epoch£º394	 i:1 	 global-step:7881	 l-p:0.11481975018978119
epoch£º394	 i:2 	 global-step:7882	 l-p:0.10637205839157104
epoch£º394	 i:3 	 global-step:7883	 l-p:0.03903362154960632
epoch£º394	 i:4 	 global-step:7884	 l-p:0.06930584460496902
epoch£º394	 i:5 	 global-step:7885	 l-p:0.03206847980618477
epoch£º394	 i:6 	 global-step:7886	 l-p:0.14531193673610687
epoch£º394	 i:7 	 global-step:7887	 l-p:0.13145840167999268
epoch£º394	 i:8 	 global-step:7888	 l-p:0.14539986848831177
epoch£º394	 i:9 	 global-step:7889	 l-p:0.14164380729198456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:395
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0798, 5.0782, 5.0797],
        [5.0798, 5.0335, 5.0379],
        [5.0798, 5.0798, 5.0798],
        [5.0798, 5.0420, 5.0555]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:395, step:0 
model_pd.l_p.mean(): 0.11433316022157669 
model_pd.l_d.mean(): -19.936241149902344 
model_pd.lagr.mean(): -19.82190704345703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4826], device='cuda:0')), ('power', tensor([-20.8088], device='cuda:0'))])
epoch£º395	 i:0 	 global-step:7900	 l-p:0.11433316022157669
epoch£º395	 i:1 	 global-step:7901	 l-p:0.12015649676322937
epoch£º395	 i:2 	 global-step:7902	 l-p:0.13637393712997437
epoch£º395	 i:3 	 global-step:7903	 l-p:0.15210801362991333
epoch£º395	 i:4 	 global-step:7904	 l-p:0.06628095358610153
epoch£º395	 i:5 	 global-step:7905	 l-p:0.05993999168276787
epoch£º395	 i:6 	 global-step:7906	 l-p:0.1460886150598526
epoch£º395	 i:7 	 global-step:7907	 l-p:0.11807959526777267
epoch£º395	 i:8 	 global-step:7908	 l-p:0.1810809224843979
epoch£º395	 i:9 	 global-step:7909	 l-p:0.17244885861873627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:396
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8901, 5.0598, 4.9580],
        [4.8901, 5.1186, 5.0434],
        [4.8901, 4.8704, 4.8845],
        [4.8901, 4.8779, 4.8878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:396, step:0 
model_pd.l_p.mean(): 0.14042603969573975 
model_pd.l_d.mean(): -20.90340805053711 
model_pd.lagr.mean(): -20.762981414794922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4217], device='cuda:0')), ('power', tensor([-21.7310], device='cuda:0'))])
epoch£º396	 i:0 	 global-step:7920	 l-p:0.14042603969573975
epoch£º396	 i:1 	 global-step:7921	 l-p:0.10616732388734818
epoch£º396	 i:2 	 global-step:7922	 l-p:0.12724661827087402
epoch£º396	 i:3 	 global-step:7923	 l-p:0.14699368178844452
epoch£º396	 i:4 	 global-step:7924	 l-p:0.09751620888710022
epoch£º396	 i:5 	 global-step:7925	 l-p:0.1737537682056427
epoch£º396	 i:6 	 global-step:7926	 l-p:0.12325362861156464
epoch£º396	 i:7 	 global-step:7927	 l-p:0.03937062993645668
epoch£º396	 i:8 	 global-step:7928	 l-p:0.15217630565166473
epoch£º396	 i:9 	 global-step:7929	 l-p:0.099784255027771
====================================================================================================
====================================================================================================
====================================================================================================

epoch:397
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0010, 5.6835, 5.9283],
        [5.0010, 4.9954, 5.0004],
        [5.0010, 4.9529, 4.9619],
        [5.0010, 4.9559, 4.9681]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:397, step:0 
model_pd.l_p.mean(): 0.11242149770259857 
model_pd.l_d.mean(): -19.1453857421875 
model_pd.lagr.mean(): -19.0329647064209 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5826], device='cuda:0')), ('power', tensor([-20.1068], device='cuda:0'))])
epoch£º397	 i:0 	 global-step:7940	 l-p:0.11242149770259857
epoch£º397	 i:1 	 global-step:7941	 l-p:0.32984432578086853
epoch£º397	 i:2 	 global-step:7942	 l-p:0.1580706685781479
epoch£º397	 i:3 	 global-step:7943	 l-p:0.10410922765731812
epoch£º397	 i:4 	 global-step:7944	 l-p:0.2928175628185272
epoch£º397	 i:5 	 global-step:7945	 l-p:0.1624777615070343
epoch£º397	 i:6 	 global-step:7946	 l-p:-2.7917301654815674
epoch£º397	 i:7 	 global-step:7947	 l-p:0.13403941690921783
epoch£º397	 i:8 	 global-step:7948	 l-p:0.1276712417602539
epoch£º397	 i:9 	 global-step:7949	 l-p:0.5734738111495972
====================================================================================================
====================================================================================================
====================================================================================================

epoch:398
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1478, 5.3985, 5.3267],
        [5.1478, 5.1478, 5.1478],
        [5.1478, 5.1463, 5.1478],
        [5.1478, 5.3279, 5.2265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:398, step:0 
model_pd.l_p.mean(): 0.2569664716720581 
model_pd.l_d.mean(): -19.158456802368164 
model_pd.lagr.mean(): -18.901491165161133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4987], device='cuda:0')), ('power', tensor([-20.0332], device='cuda:0'))])
epoch£º398	 i:0 	 global-step:7960	 l-p:0.2569664716720581
epoch£º398	 i:1 	 global-step:7961	 l-p:0.10698205977678299
epoch£º398	 i:2 	 global-step:7962	 l-p:0.1956864893436432
epoch£º398	 i:3 	 global-step:7963	 l-p:0.14228662848472595
epoch£º398	 i:4 	 global-step:7964	 l-p:0.14186984300613403
epoch£º398	 i:5 	 global-step:7965	 l-p:0.11737790703773499
epoch£º398	 i:6 	 global-step:7966	 l-p:0.13671381771564484
epoch£º398	 i:7 	 global-step:7967	 l-p:0.12618625164031982
epoch£º398	 i:8 	 global-step:7968	 l-p:0.1253911554813385
epoch£º398	 i:9 	 global-step:7969	 l-p:0.11822415888309479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:399
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0674, 5.0670, 5.0674],
        [5.0674, 5.0224, 5.0332],
        [5.0674, 5.1486, 5.0239],
        [5.0674, 5.0314, 4.9542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:399, step:0 
model_pd.l_p.mean(): 0.11576133221387863 
model_pd.l_d.mean(): -19.883453369140625 
model_pd.lagr.mean(): -19.76769256591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4784], device='cuda:0')), ('power', tensor([-20.7508], device='cuda:0'))])
epoch£º399	 i:0 	 global-step:7980	 l-p:0.11576133221387863
epoch£º399	 i:1 	 global-step:7981	 l-p:0.10299409925937653
epoch£º399	 i:2 	 global-step:7982	 l-p:0.12667754292488098
epoch£º399	 i:3 	 global-step:7983	 l-p:0.02314044162631035
epoch£º399	 i:4 	 global-step:7984	 l-p:0.13494424521923065
epoch£º399	 i:5 	 global-step:7985	 l-p:0.14831210672855377
epoch£º399	 i:6 	 global-step:7986	 l-p:0.14464466273784637
epoch£º399	 i:7 	 global-step:7987	 l-p:0.15984708070755005
epoch£º399	 i:8 	 global-step:7988	 l-p:0.18770703673362732
epoch£º399	 i:9 	 global-step:7989	 l-p:0.039524998515844345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:400
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9274, 4.9057, 4.7972],
        [4.9274, 5.0280, 4.9027],
        [4.9274, 4.9102, 4.9232],
        [4.9274, 4.8967, 4.7956]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:400, step:0 
model_pd.l_p.mean(): 0.1463877409696579 
model_pd.l_d.mean(): -19.50472640991211 
model_pd.lagr.mean(): -19.358339309692383 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5110], device='cuda:0')), ('power', tensor([-20.3987], device='cuda:0'))])
epoch£º400	 i:0 	 global-step:8000	 l-p:0.1463877409696579
epoch£º400	 i:1 	 global-step:8001	 l-p:0.10968126356601715
epoch£º400	 i:2 	 global-step:8002	 l-p:0.11942274123430252
epoch£º400	 i:3 	 global-step:8003	 l-p:0.14272479712963104
epoch£º400	 i:4 	 global-step:8004	 l-p:0.14469166100025177
epoch£º400	 i:5 	 global-step:8005	 l-p:9.366424560546875
epoch£º400	 i:6 	 global-step:8006	 l-p:0.14357967674732208
epoch£º400	 i:7 	 global-step:8007	 l-p:-0.030206240713596344
epoch£º400	 i:8 	 global-step:8008	 l-p:0.10920746624469757
epoch£º400	 i:9 	 global-step:8009	 l-p:0.13129042088985443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:401
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9971, 5.1552, 5.0457],
        [4.9971, 4.9968, 4.9971],
        [4.9971, 5.1025, 4.9786],
        [4.9971, 5.2963, 5.2543]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:401, step:0 
model_pd.l_p.mean(): 0.14717258512973785 
model_pd.l_d.mean(): -19.336925506591797 
model_pd.lagr.mean(): -19.18975257873535 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5804], device='cuda:0')), ('power', tensor([-20.2997], device='cuda:0'))])
epoch£º401	 i:0 	 global-step:8020	 l-p:0.14717258512973785
epoch£º401	 i:1 	 global-step:8021	 l-p:0.1507485806941986
epoch£º401	 i:2 	 global-step:8022	 l-p:0.14646045863628387
epoch£º401	 i:3 	 global-step:8023	 l-p:0.2609158158302307
epoch£º401	 i:4 	 global-step:8024	 l-p:0.11289018392562866
epoch£º401	 i:5 	 global-step:8025	 l-p:-0.0060801077634096146
epoch£º401	 i:6 	 global-step:8026	 l-p:0.14721103012561798
epoch£º401	 i:7 	 global-step:8027	 l-p:0.11548230051994324
epoch£º401	 i:8 	 global-step:8028	 l-p:0.08851586282253265
epoch£º401	 i:9 	 global-step:8029	 l-p:0.14855551719665527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:402
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9985, 4.9985, 4.9985],
        [4.9985, 4.9980, 4.9985],
        [4.9985, 4.9498, 4.9620],
        [4.9985, 4.9985, 4.9985]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:402, step:0 
model_pd.l_p.mean(): 0.1352246105670929 
model_pd.l_d.mean(): -20.58562660217285 
model_pd.lagr.mean(): -20.450401306152344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4216], device='cuda:0')), ('power', tensor([-21.4073], device='cuda:0'))])
epoch£º402	 i:0 	 global-step:8040	 l-p:0.1352246105670929
epoch£º402	 i:1 	 global-step:8041	 l-p:0.13847607374191284
epoch£º402	 i:2 	 global-step:8042	 l-p:0.13537821173667908
epoch£º402	 i:3 	 global-step:8043	 l-p:0.09472142159938812
epoch£º402	 i:4 	 global-step:8044	 l-p:0.11921453475952148
epoch£º402	 i:5 	 global-step:8045	 l-p:0.008095011115074158
epoch£º402	 i:6 	 global-step:8046	 l-p:0.14041689038276672
epoch£º402	 i:7 	 global-step:8047	 l-p:0.16384445130825043
epoch£º402	 i:8 	 global-step:8048	 l-p:0.1322556883096695
epoch£º402	 i:9 	 global-step:8049	 l-p:0.1550225019454956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:403
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0559, 5.0147, 5.0301],
        [5.0559, 5.0559, 5.0560],
        [5.0559, 5.0176, 5.0341],
        [5.0559, 5.0400, 5.0521]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:403, step:0 
model_pd.l_p.mean(): 0.1569979339838028 
model_pd.l_d.mean(): -20.35042381286621 
model_pd.lagr.mean(): -20.19342613220215 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4290], device='cuda:0')), ('power', tensor([-21.1753], device='cuda:0'))])
epoch£º403	 i:0 	 global-step:8060	 l-p:0.1569979339838028
epoch£º403	 i:1 	 global-step:8061	 l-p:0.12377621978521347
epoch£º403	 i:2 	 global-step:8062	 l-p:0.1321840137243271
epoch£º403	 i:3 	 global-step:8063	 l-p:0.12555354833602905
epoch£º403	 i:4 	 global-step:8064	 l-p:0.020934682339429855
epoch£º403	 i:5 	 global-step:8065	 l-p:0.13597092032432556
epoch£º403	 i:6 	 global-step:8066	 l-p:0.12163085490465164
epoch£º403	 i:7 	 global-step:8067	 l-p:0.12139545381069183
epoch£º403	 i:8 	 global-step:8068	 l-p:0.1212221086025238
epoch£º403	 i:9 	 global-step:8069	 l-p:0.038008544594049454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:404
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1036, 5.2252, 5.1048],
        [5.1036, 5.5772, 5.6451],
        [5.1036, 5.0748, 5.0914],
        [5.1036, 5.0987, 5.1031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:404, step:0 
model_pd.l_p.mean(): 0.15110912919044495 
model_pd.l_d.mean(): -19.021453857421875 
model_pd.lagr.mean(): -18.870344161987305 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5282], device='cuda:0')), ('power', tensor([-19.9242], device='cuda:0'))])
epoch£º404	 i:0 	 global-step:8080	 l-p:0.15110912919044495
epoch£º404	 i:1 	 global-step:8081	 l-p:0.11305330693721771
epoch£º404	 i:2 	 global-step:8082	 l-p:0.05897006765007973
epoch£º404	 i:3 	 global-step:8083	 l-p:0.1277438849210739
epoch£º404	 i:4 	 global-step:8084	 l-p:0.12011158466339111
epoch£º404	 i:5 	 global-step:8085	 l-p:0.009958906099200249
epoch£º404	 i:6 	 global-step:8086	 l-p:0.15851908922195435
epoch£º404	 i:7 	 global-step:8087	 l-p:0.12083817273378372
epoch£º404	 i:8 	 global-step:8088	 l-p:-0.1727692037820816
epoch£º404	 i:9 	 global-step:8089	 l-p:0.14844384789466858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:405
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1056, 5.0875, 4.9877],
        [5.1056, 5.1056, 5.1056],
        [5.1056, 5.1056, 5.1056],
        [5.1056, 5.0513, 5.0124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:405, step:0 
model_pd.l_p.mean(): 0.10065795481204987 
model_pd.l_d.mean(): -20.20578384399414 
model_pd.lagr.mean(): -20.105125427246094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4506], device='cuda:0')), ('power', tensor([-21.0503], device='cuda:0'))])
epoch£º405	 i:0 	 global-step:8100	 l-p:0.10065795481204987
epoch£º405	 i:1 	 global-step:8101	 l-p:0.17081695795059204
epoch£º405	 i:2 	 global-step:8102	 l-p:0.10531052947044373
epoch£º405	 i:3 	 global-step:8103	 l-p:0.031238440424203873
epoch£º405	 i:4 	 global-step:8104	 l-p:0.041599586606025696
epoch£º405	 i:5 	 global-step:8105	 l-p:0.06185043230652809
epoch£º405	 i:6 	 global-step:8106	 l-p:0.15817949175834656
epoch£º405	 i:7 	 global-step:8107	 l-p:0.1510983258485794
epoch£º405	 i:8 	 global-step:8108	 l-p:0.1259216070175171
epoch£º405	 i:9 	 global-step:8109	 l-p:0.13394062221050262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:406
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0920, 5.0447, 5.0555],
        [5.0920, 5.0729, 5.0866],
        [5.0920, 5.0363, 4.9988],
        [5.0920, 5.2510, 5.1400]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:406, step:0 
model_pd.l_p.mean(): 0.03554420918226242 
model_pd.l_d.mean(): -19.531753540039062 
model_pd.lagr.mean(): -19.4962100982666 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4920], device='cuda:0')), ('power', tensor([-20.4065], device='cuda:0'))])
epoch£º406	 i:0 	 global-step:8120	 l-p:0.03554420918226242
epoch£º406	 i:1 	 global-step:8121	 l-p:0.09995298087596893
epoch£º406	 i:2 	 global-step:8122	 l-p:0.13057711720466614
epoch£º406	 i:3 	 global-step:8123	 l-p:0.1763525903224945
epoch£º406	 i:4 	 global-step:8124	 l-p:0.1523100584745407
epoch£º406	 i:5 	 global-step:8125	 l-p:2.5413503646850586
epoch£º406	 i:6 	 global-step:8126	 l-p:0.12066225707530975
epoch£º406	 i:7 	 global-step:8127	 l-p:0.08101599663496017
epoch£º406	 i:8 	 global-step:8128	 l-p:0.12743398547172546
epoch£º406	 i:9 	 global-step:8129	 l-p:0.12936250865459442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:407
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0576, 5.0513, 5.0569],
        [5.0576, 5.0520, 5.0570],
        [5.0576, 5.1375, 5.0091],
        [5.0576, 5.0393, 4.9343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:407, step:0 
model_pd.l_p.mean(): 0.1748376041650772 
model_pd.l_d.mean(): -20.483951568603516 
model_pd.lagr.mean(): -20.309114456176758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4246], device='cuda:0')), ('power', tensor([-21.3068], device='cuda:0'))])
epoch£º407	 i:0 	 global-step:8140	 l-p:0.1748376041650772
epoch£º407	 i:1 	 global-step:8141	 l-p:0.12725020945072174
epoch£º407	 i:2 	 global-step:8142	 l-p:0.1543857902288437
epoch£º407	 i:3 	 global-step:8143	 l-p:0.14202982187271118
epoch£º407	 i:4 	 global-step:8144	 l-p:-0.12908564507961273
epoch£º407	 i:5 	 global-step:8145	 l-p:0.1465981900691986
epoch£º407	 i:6 	 global-step:8146	 l-p:0.14071738719940186
epoch£º407	 i:7 	 global-step:8147	 l-p:0.4233856201171875
epoch£º407	 i:8 	 global-step:8148	 l-p:0.11556345969438553
epoch£º407	 i:9 	 global-step:8149	 l-p:0.12211336195468903
====================================================================================================
====================================================================================================
====================================================================================================

epoch:408
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1546, 5.1243, 5.1408],
        [5.1546, 5.1278, 5.1439],
        [5.1546, 5.1023, 5.0625],
        [5.1546, 5.2292, 5.1025]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:408, step:0 
model_pd.l_p.mean(): 0.3765471875667572 
model_pd.l_d.mean(): -20.41820526123047 
model_pd.lagr.mean(): -20.041658401489258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4172], device='cuda:0')), ('power', tensor([-21.2321], device='cuda:0'))])
epoch£º408	 i:0 	 global-step:8160	 l-p:0.3765471875667572
epoch£º408	 i:1 	 global-step:8161	 l-p:0.13941717147827148
epoch£º408	 i:2 	 global-step:8162	 l-p:0.1456250250339508
epoch£º408	 i:3 	 global-step:8163	 l-p:0.1387639045715332
epoch£º408	 i:4 	 global-step:8164	 l-p:0.2605762183666229
epoch£º408	 i:5 	 global-step:8165	 l-p:0.20248298346996307
epoch£º408	 i:6 	 global-step:8166	 l-p:0.1198715940117836
epoch£º408	 i:7 	 global-step:8167	 l-p:0.12140874564647675
epoch£º408	 i:8 	 global-step:8168	 l-p:0.12866388261318207
epoch£º408	 i:9 	 global-step:8169	 l-p:0.09321815520524979
====================================================================================================
====================================================================================================
====================================================================================================

epoch:409
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1666, 5.1464, 5.1605],
        [5.1666, 5.1444, 5.1592],
        [5.1666, 5.1649, 5.1666],
        [5.1666, 5.1654, 5.1666]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:409, step:0 
model_pd.l_p.mean(): 0.12794825434684753 
model_pd.l_d.mean(): -20.437692642211914 
model_pd.lagr.mean(): -20.309743881225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3847], device='cuda:0')), ('power', tensor([-21.2183], device='cuda:0'))])
epoch£º409	 i:0 	 global-step:8180	 l-p:0.12794825434684753
epoch£º409	 i:1 	 global-step:8181	 l-p:0.1639453023672104
epoch£º409	 i:2 	 global-step:8182	 l-p:0.27974840998649597
epoch£º409	 i:3 	 global-step:8183	 l-p:0.14060693979263306
epoch£º409	 i:4 	 global-step:8184	 l-p:0.13988685607910156
epoch£º409	 i:5 	 global-step:8185	 l-p:0.12325898557901382
epoch£º409	 i:6 	 global-step:8186	 l-p:-0.0720076709985733
epoch£º409	 i:7 	 global-step:8187	 l-p:-0.3536643981933594
epoch£º409	 i:8 	 global-step:8188	 l-p:0.14227460324764252
epoch£º409	 i:9 	 global-step:8189	 l-p:0.11605869978666306
====================================================================================================
====================================================================================================
====================================================================================================

epoch:410
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1271, 5.0958, 5.1128],
        [5.1271, 5.0761, 5.0816],
        [5.1271, 5.1212, 5.1264],
        [5.1271, 5.1271, 5.1271]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:410, step:0 
model_pd.l_p.mean(): 0.15235672891139984 
model_pd.l_d.mean(): -20.301504135131836 
model_pd.lagr.mean(): -20.149147033691406 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4334], device='cuda:0')), ('power', tensor([-21.1300], device='cuda:0'))])
epoch£º410	 i:0 	 global-step:8200	 l-p:0.15235672891139984
epoch£º410	 i:1 	 global-step:8201	 l-p:0.13232146203517914
epoch£º410	 i:2 	 global-step:8202	 l-p:0.09575644880533218
epoch£º410	 i:3 	 global-step:8203	 l-p:0.12824657559394836
epoch£º410	 i:4 	 global-step:8204	 l-p:0.28958702087402344
epoch£º410	 i:5 	 global-step:8205	 l-p:0.13082721829414368
epoch£º410	 i:6 	 global-step:8206	 l-p:0.08475560694932938
epoch£º410	 i:7 	 global-step:8207	 l-p:0.12921395897865295
epoch£º410	 i:8 	 global-step:8208	 l-p:0.14491724967956543
epoch£º410	 i:9 	 global-step:8209	 l-p:0.07831278443336487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:411
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0182, 5.0183, 5.0182],
        [5.0182, 5.0176, 5.0182],
        [5.0182, 5.0181, 5.0182],
        [5.0182, 5.0170, 5.0182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:411, step:0 
model_pd.l_p.mean(): 0.3389410078525543 
model_pd.l_d.mean(): -20.204601287841797 
model_pd.lagr.mean(): -19.865659713745117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4760], device='cuda:0')), ('power', tensor([-21.0754], device='cuda:0'))])
epoch£º411	 i:0 	 global-step:8220	 l-p:0.3389410078525543
epoch£º411	 i:1 	 global-step:8221	 l-p:0.13491816818714142
epoch£º411	 i:2 	 global-step:8222	 l-p:0.15032747387886047
epoch£º411	 i:3 	 global-step:8223	 l-p:0.17155662178993225
epoch£º411	 i:4 	 global-step:8224	 l-p:0.08704677224159241
epoch£º411	 i:5 	 global-step:8225	 l-p:0.3190852701663971
epoch£º411	 i:6 	 global-step:8226	 l-p:0.07391760498285294
epoch£º411	 i:7 	 global-step:8227	 l-p:0.12182223796844482
epoch£º411	 i:8 	 global-step:8228	 l-p:0.1349368393421173
epoch£º411	 i:9 	 global-step:8229	 l-p:0.14176201820373535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:412
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9873, 4.9813, 4.9866],
        [4.9873, 5.7506, 6.0640],
        [4.9873, 4.9873, 4.9873],
        [4.9873, 4.9316, 4.9405]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:412, step:0 
model_pd.l_p.mean(): 0.2770080864429474 
model_pd.l_d.mean(): -19.034090042114258 
model_pd.lagr.mean(): -18.757081985473633 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5347], device='cuda:0')), ('power', tensor([-19.9439], device='cuda:0'))])
epoch£º412	 i:0 	 global-step:8240	 l-p:0.2770080864429474
epoch£º412	 i:1 	 global-step:8241	 l-p:0.10786452889442444
epoch£º412	 i:2 	 global-step:8242	 l-p:0.13653679192066193
epoch£º412	 i:3 	 global-step:8243	 l-p:0.14594650268554688
epoch£º412	 i:4 	 global-step:8244	 l-p:0.12234855443239212
epoch£º412	 i:5 	 global-step:8245	 l-p:0.15661098062992096
epoch£º412	 i:6 	 global-step:8246	 l-p:0.09164606034755707
epoch£º412	 i:7 	 global-step:8247	 l-p:0.07748042792081833
epoch£º412	 i:8 	 global-step:8248	 l-p:0.16368842124938965
epoch£º412	 i:9 	 global-step:8249	 l-p:0.11860736459493637
====================================================================================================
====================================================================================================
====================================================================================================

epoch:413
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9298, 4.8823, 4.9001],
        [4.9298, 4.9298, 4.9298],
        [4.9298, 5.6122, 5.8600],
        [4.9298, 4.9298, 4.9298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:413, step:0 
model_pd.l_p.mean(): 0.14948353171348572 
model_pd.l_d.mean(): -19.78382110595703 
model_pd.lagr.mean(): -19.63433837890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5238], device='cuda:0')), ('power', tensor([-20.6962], device='cuda:0'))])
epoch£º413	 i:0 	 global-step:8260	 l-p:0.14948353171348572
epoch£º413	 i:1 	 global-step:8261	 l-p:0.14724697172641754
epoch£º413	 i:2 	 global-step:8262	 l-p:0.1350684016942978
epoch£º413	 i:3 	 global-step:8263	 l-p:0.07513292133808136
epoch£º413	 i:4 	 global-step:8264	 l-p:0.11934951692819595
epoch£º413	 i:5 	 global-step:8265	 l-p:0.18152114748954773
epoch£º413	 i:6 	 global-step:8266	 l-p:0.1693337857723236
epoch£º413	 i:7 	 global-step:8267	 l-p:0.1546175628900528
epoch£º413	 i:8 	 global-step:8268	 l-p:0.024333834648132324
epoch£º413	 i:9 	 global-step:8269	 l-p:0.1303093284368515
====================================================================================================
====================================================================================================
====================================================================================================

epoch:414
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8959, 4.8921, 4.8956],
        [4.8959, 5.0009, 4.8719],
        [4.8959, 4.8959, 4.8959],
        [4.8959, 4.8705, 4.8878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:414, step:0 
model_pd.l_p.mean(): 0.0883917286992073 
model_pd.l_d.mean(): -19.407642364501953 
model_pd.lagr.mean(): -19.319250106811523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5473], device='cuda:0')), ('power', tensor([-20.3374], device='cuda:0'))])
epoch£º414	 i:0 	 global-step:8280	 l-p:0.0883917286992073
epoch£º414	 i:1 	 global-step:8281	 l-p:0.1442061960697174
epoch£º414	 i:2 	 global-step:8282	 l-p:0.17176876962184906
epoch£º414	 i:3 	 global-step:8283	 l-p:0.12519216537475586
epoch£º414	 i:4 	 global-step:8284	 l-p:0.15043802559375763
epoch£º414	 i:5 	 global-step:8285	 l-p:0.1732599288225174
epoch£º414	 i:6 	 global-step:8286	 l-p:0.08781015127897263
epoch£º414	 i:7 	 global-step:8287	 l-p:0.10428503155708313
epoch£º414	 i:8 	 global-step:8288	 l-p:0.13385869562625885
epoch£º414	 i:9 	 global-step:8289	 l-p:0.15948182344436646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:415
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9425, 5.0227, 4.8892],
        [4.9425, 5.3992, 5.4644],
        [4.9425, 4.9719, 4.8371],
        [4.9425, 4.9280, 4.9396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:415, step:0 
model_pd.l_p.mean(): 0.10590816289186478 
model_pd.l_d.mean(): -19.74392318725586 
model_pd.lagr.mean(): -19.638015747070312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5658], device='cuda:0')), ('power', tensor([-20.6991], device='cuda:0'))])
epoch£º415	 i:0 	 global-step:8300	 l-p:0.10590816289186478
epoch£º415	 i:1 	 global-step:8301	 l-p:0.12435708940029144
epoch£º415	 i:2 	 global-step:8302	 l-p:0.15788671374320984
epoch£º415	 i:3 	 global-step:8303	 l-p:0.12926329672336578
epoch£º415	 i:4 	 global-step:8304	 l-p:0.1412779688835144
epoch£º415	 i:5 	 global-step:8305	 l-p:-0.16340754926204681
epoch£º415	 i:6 	 global-step:8306	 l-p:0.1337951123714447
epoch£º415	 i:7 	 global-step:8307	 l-p:0.1328924000263214
epoch£º415	 i:8 	 global-step:8308	 l-p:0.05146614834666252
epoch£º415	 i:9 	 global-step:8309	 l-p:0.1278332769870758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:416
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9665, 4.9344, 4.9533],
        [4.9665, 4.9665, 4.9665],
        [4.9665, 5.0343, 4.8995],
        [4.9665, 4.9665, 4.9665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:416, step:0 
model_pd.l_p.mean(): 0.16938161849975586 
model_pd.l_d.mean(): -20.49634552001953 
model_pd.lagr.mean(): -20.326963424682617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4643], device='cuda:0')), ('power', tensor([-21.3605], device='cuda:0'))])
epoch£º416	 i:0 	 global-step:8320	 l-p:0.16938161849975586
epoch£º416	 i:1 	 global-step:8321	 l-p:-0.6199618577957153
epoch£º416	 i:2 	 global-step:8322	 l-p:0.0729670450091362
epoch£º416	 i:3 	 global-step:8323	 l-p:0.1138634905219078
epoch£º416	 i:4 	 global-step:8324	 l-p:0.12394966185092926
epoch£º416	 i:5 	 global-step:8325	 l-p:0.08264022320508957
epoch£º416	 i:6 	 global-step:8326	 l-p:0.11014717072248459
epoch£º416	 i:7 	 global-step:8327	 l-p:0.12481220811605453
epoch£º416	 i:8 	 global-step:8328	 l-p:0.11906758695840836
epoch£º416	 i:9 	 global-step:8329	 l-p:0.1273084580898285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:417
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0336, 5.0251, 5.0324],
        [5.0336, 5.0159, 5.0293],
        [5.0336, 4.9877, 5.0044],
        [5.0336, 5.0336, 5.0336]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:417, step:0 
model_pd.l_p.mean(): 0.32441675662994385 
model_pd.l_d.mean(): -20.2663516998291 
model_pd.lagr.mean(): -19.94193458557129 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4601], device='cuda:0')), ('power', tensor([-21.1218], device='cuda:0'))])
epoch£º417	 i:0 	 global-step:8340	 l-p:0.32441675662994385
epoch£º417	 i:1 	 global-step:8341	 l-p:0.14024756848812103
epoch£º417	 i:2 	 global-step:8342	 l-p:0.1286843866109848
epoch£º417	 i:3 	 global-step:8343	 l-p:0.12011589854955673
epoch£º417	 i:4 	 global-step:8344	 l-p:0.12414573132991791
epoch£º417	 i:5 	 global-step:8345	 l-p:0.14397132396697998
epoch£º417	 i:6 	 global-step:8346	 l-p:0.04241848737001419
epoch£º417	 i:7 	 global-step:8347	 l-p:0.10464375466108322
epoch£º417	 i:8 	 global-step:8348	 l-p:0.1402907371520996
epoch£º417	 i:9 	 global-step:8349	 l-p:0.14480258524417877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:418
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9511, 4.9509, 4.9511],
        [4.9511, 4.9510, 4.9511],
        [4.9511, 5.1331, 5.0292],
        [4.9511, 4.8844, 4.8794]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:418, step:0 
model_pd.l_p.mean(): -0.17239345610141754 
model_pd.l_d.mean(): -19.487957000732422 
model_pd.lagr.mean(): -19.660350799560547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5183], device='cuda:0')), ('power', tensor([-20.3892], device='cuda:0'))])
epoch£º418	 i:0 	 global-step:8360	 l-p:-0.17239345610141754
epoch£º418	 i:1 	 global-step:8361	 l-p:0.1503634750843048
epoch£º418	 i:2 	 global-step:8362	 l-p:0.10803572088479996
epoch£º418	 i:3 	 global-step:8363	 l-p:0.13659252226352692
epoch£º418	 i:4 	 global-step:8364	 l-p:0.1474457085132599
epoch£º418	 i:5 	 global-step:8365	 l-p:0.11845070868730545
epoch£º418	 i:6 	 global-step:8366	 l-p:0.12025568634271622
epoch£º418	 i:7 	 global-step:8367	 l-p:0.17976164817810059
epoch£º418	 i:8 	 global-step:8368	 l-p:0.11531452089548111
epoch£º418	 i:9 	 global-step:8369	 l-p:0.12914039194583893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:419
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9602, 4.8937, 4.8891],
        [4.9602, 4.9854, 4.8500],
        [4.9602, 4.9230, 4.8161],
        [4.9602, 4.9509, 4.9589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:419, step:0 
model_pd.l_p.mean(): 0.15350447595119476 
model_pd.l_d.mean(): -20.1573486328125 
model_pd.lagr.mean(): -20.003843307495117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5052], device='cuda:0')), ('power', tensor([-21.0575], device='cuda:0'))])
epoch£º419	 i:0 	 global-step:8380	 l-p:0.15350447595119476
epoch£º419	 i:1 	 global-step:8381	 l-p:0.12995649874210358
epoch£º419	 i:2 	 global-step:8382	 l-p:0.11529862880706787
epoch£º419	 i:3 	 global-step:8383	 l-p:0.13268859684467316
epoch£º419	 i:4 	 global-step:8384	 l-p:0.14015637338161469
epoch£º419	 i:5 	 global-step:8385	 l-p:0.08189161121845245
epoch£º419	 i:6 	 global-step:8386	 l-p:-0.035914402455091476
epoch£º419	 i:7 	 global-step:8387	 l-p:0.12757350504398346
epoch£º419	 i:8 	 global-step:8388	 l-p:0.13910649716854095
epoch£º419	 i:9 	 global-step:8389	 l-p:0.28803694248199463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:420
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9895, 5.1289, 5.0083],
        [4.9895, 4.9279, 4.9323],
        [4.9895, 4.9208, 4.8846],
        [4.9895, 4.9890, 4.9895]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:420, step:0 
model_pd.l_p.mean(): 0.07037780433893204 
model_pd.l_d.mean(): -19.484426498413086 
model_pd.lagr.mean(): -19.41404914855957 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5035], device='cuda:0')), ('power', tensor([-20.3703], device='cuda:0'))])
epoch£º420	 i:0 	 global-step:8400	 l-p:0.07037780433893204
epoch£º420	 i:1 	 global-step:8401	 l-p:0.12912507355213165
epoch£º420	 i:2 	 global-step:8402	 l-p:0.1344592124223709
epoch£º420	 i:3 	 global-step:8403	 l-p:0.13236835598945618
epoch£º420	 i:4 	 global-step:8404	 l-p:0.13038958609104156
epoch£º420	 i:5 	 global-step:8405	 l-p:0.24502992630004883
epoch£º420	 i:6 	 global-step:8406	 l-p:0.07550717890262604
epoch£º420	 i:7 	 global-step:8407	 l-p:0.1248326301574707
epoch£º420	 i:8 	 global-step:8408	 l-p:0.10981239378452301
epoch£º420	 i:9 	 global-step:8409	 l-p:0.23143328726291656
====================================================================================================
====================================================================================================
====================================================================================================

epoch:421
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0639, 5.0026, 4.9494],
        [5.0639, 5.6748, 5.8500],
        [5.0639, 5.0280, 5.0469],
        [5.0639, 5.0576, 5.0632]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:421, step:0 
model_pd.l_p.mean(): 0.0986868292093277 
model_pd.l_d.mean(): -20.152915954589844 
model_pd.lagr.mean(): -20.054229736328125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4617], device='cuda:0')), ('power', tensor([-21.0079], device='cuda:0'))])
epoch£º421	 i:0 	 global-step:8420	 l-p:0.0986868292093277
epoch£º421	 i:1 	 global-step:8421	 l-p:0.12552474439144135
epoch£º421	 i:2 	 global-step:8422	 l-p:0.11541656404733658
epoch£º421	 i:3 	 global-step:8423	 l-p:0.13689666986465454
epoch£º421	 i:4 	 global-step:8424	 l-p:0.17562703788280487
epoch£º421	 i:5 	 global-step:8425	 l-p:0.06611285358667374
epoch£º421	 i:6 	 global-step:8426	 l-p:0.9628052115440369
epoch£º421	 i:7 	 global-step:8427	 l-p:0.1418866366147995
epoch£º421	 i:8 	 global-step:8428	 l-p:0.13469259440898895
epoch£º421	 i:9 	 global-step:8429	 l-p:0.12624892592430115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:422
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0813, 5.0662, 5.0781],
        [5.0813, 5.0813, 5.0813],
        [5.0813, 5.0806, 5.0813],
        [5.0813, 5.0986, 4.9692]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:422, step:0 
model_pd.l_p.mean(): 0.2621927857398987 
model_pd.l_d.mean(): -20.115488052368164 
model_pd.lagr.mean(): -19.853294372558594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4681], device='cuda:0')), ('power', tensor([-20.9765], device='cuda:0'))])
epoch£º422	 i:0 	 global-step:8440	 l-p:0.2621927857398987
epoch£º422	 i:1 	 global-step:8441	 l-p:0.12924613058567047
epoch£º422	 i:2 	 global-step:8442	 l-p:0.1177300214767456
epoch£º422	 i:3 	 global-step:8443	 l-p:0.12281101197004318
epoch£º422	 i:4 	 global-step:8444	 l-p:0.16120143234729767
epoch£º422	 i:5 	 global-step:8445	 l-p:0.09083150327205658
epoch£º422	 i:6 	 global-step:8446	 l-p:0.0845583900809288
epoch£º422	 i:7 	 global-step:8447	 l-p:0.0066157556138932705
epoch£º422	 i:8 	 global-step:8448	 l-p:0.15807069838047028
epoch£º422	 i:9 	 global-step:8449	 l-p:0.12385984510183334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:423
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1458, 5.1458, 5.1458],
        [5.1458, 5.2203, 5.0880],
        [5.1458, 5.1284, 5.1415],
        [5.1458, 5.1209, 5.0179]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:423, step:0 
model_pd.l_p.mean(): 0.10775670409202576 
model_pd.l_d.mean(): -19.945695877075195 
model_pd.lagr.mean(): -19.83793830871582 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4661], device='cuda:0')), ('power', tensor([-20.8014], device='cuda:0'))])
epoch£º423	 i:0 	 global-step:8460	 l-p:0.10775670409202576
epoch£º423	 i:1 	 global-step:8461	 l-p:31.1166934967041
epoch£º423	 i:2 	 global-step:8462	 l-p:0.3008851408958435
epoch£º423	 i:3 	 global-step:8463	 l-p:0.14090867340564728
epoch£º423	 i:4 	 global-step:8464	 l-p:0.19042131304740906
epoch£º423	 i:5 	 global-step:8465	 l-p:0.11375749111175537
epoch£º423	 i:6 	 global-step:8466	 l-p:0.13421259820461273
epoch£º423	 i:7 	 global-step:8467	 l-p:0.1317024677991867
epoch£º423	 i:8 	 global-step:8468	 l-p:0.12370366603136063
epoch£º423	 i:9 	 global-step:8469	 l-p:0.12078426033258438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:424
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1832, 5.1259, 5.1213],
        [5.1832, 5.1810, 5.1831],
        [5.1832, 5.4570, 5.3908],
        [5.1832, 5.1832, 5.1832]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:424, step:0 
model_pd.l_p.mean(): 0.12872789800167084 
model_pd.l_d.mean(): -19.990537643432617 
model_pd.lagr.mean(): -19.8618106842041 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4618], device='cuda:0')), ('power', tensor([-20.8426], device='cuda:0'))])
epoch£º424	 i:0 	 global-step:8480	 l-p:0.12872789800167084
epoch£º424	 i:1 	 global-step:8481	 l-p:0.1334812045097351
epoch£º424	 i:2 	 global-step:8482	 l-p:-0.701890230178833
epoch£º424	 i:3 	 global-step:8483	 l-p:0.1030740737915039
epoch£º424	 i:4 	 global-step:8484	 l-p:0.049218833446502686
epoch£º424	 i:5 	 global-step:8485	 l-p:0.1439613252878189
epoch£º424	 i:6 	 global-step:8486	 l-p:0.13339178264141083
epoch£º424	 i:7 	 global-step:8487	 l-p:0.09115501493215561
epoch£º424	 i:8 	 global-step:8488	 l-p:-0.003309821942821145
epoch£º424	 i:9 	 global-step:8489	 l-p:0.3022480607032776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:425
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0343, 4.9745, 4.9815],
        [5.0343, 4.9703, 4.9121],
        [5.0343, 5.1781, 5.0577],
        [5.0343, 5.0343, 5.0343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:425, step:0 
model_pd.l_p.mean(): 0.16928596794605255 
model_pd.l_d.mean(): -19.36474609375 
model_pd.lagr.mean(): -19.195459365844727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4849], device='cuda:0')), ('power', tensor([-20.2291], device='cuda:0'))])
epoch£º425	 i:0 	 global-step:8500	 l-p:0.16928596794605255
epoch£º425	 i:1 	 global-step:8501	 l-p:0.14269837737083435
epoch£º425	 i:2 	 global-step:8502	 l-p:0.2559914290904999
epoch£º425	 i:3 	 global-step:8503	 l-p:-0.29743340611457825
epoch£º425	 i:4 	 global-step:8504	 l-p:0.09040900319814682
epoch£º425	 i:5 	 global-step:8505	 l-p:0.1183018833398819
epoch£º425	 i:6 	 global-step:8506	 l-p:0.11884061247110367
epoch£º425	 i:7 	 global-step:8507	 l-p:0.010003878735005856
epoch£º425	 i:8 	 global-step:8508	 l-p:0.07228211313486099
epoch£º425	 i:9 	 global-step:8509	 l-p:0.12274768203496933
====================================================================================================
====================================================================================================
====================================================================================================

epoch:426
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1243, 5.1159, 5.1231],
        [5.1243, 5.1236, 5.1243],
        [5.1243, 5.1217, 5.1241],
        [5.1243, 5.4782, 5.4600]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:426, step:0 
model_pd.l_p.mean(): -0.960611879825592 
model_pd.l_d.mean(): -18.896474838256836 
model_pd.lagr.mean(): -19.857086181640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5284], device='cuda:0')), ('power', tensor([-19.7971], device='cuda:0'))])
epoch£º426	 i:0 	 global-step:8520	 l-p:-0.960611879825592
epoch£º426	 i:1 	 global-step:8521	 l-p:0.1845235377550125
epoch£º426	 i:2 	 global-step:8522	 l-p:0.13564108312129974
epoch£º426	 i:3 	 global-step:8523	 l-p:0.1446676254272461
epoch£º426	 i:4 	 global-step:8524	 l-p:0.122595876455307
epoch£º426	 i:5 	 global-step:8525	 l-p:0.1262246072292328
epoch£º426	 i:6 	 global-step:8526	 l-p:0.12424636632204056
epoch£º426	 i:7 	 global-step:8527	 l-p:0.0989021435379982
epoch£º426	 i:8 	 global-step:8528	 l-p:0.1519371122121811
epoch£º426	 i:9 	 global-step:8529	 l-p:0.11367645114660263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:427
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0687, 5.0627, 5.0681],
        [5.0687, 5.0660, 5.0685],
        [5.0687, 5.0215, 5.0388],
        [5.0687, 5.0656, 4.9402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:427, step:0 
model_pd.l_p.mean(): 0.1605769693851471 
model_pd.l_d.mean(): -19.65743064880371 
model_pd.lagr.mean(): -19.49685287475586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4467], device='cuda:0')), ('power', tensor([-20.4877], device='cuda:0'))])
epoch£º427	 i:0 	 global-step:8540	 l-p:0.1605769693851471
epoch£º427	 i:1 	 global-step:8541	 l-p:0.10213487595319748
epoch£º427	 i:2 	 global-step:8542	 l-p:0.25204533338546753
epoch£º427	 i:3 	 global-step:8543	 l-p:0.11554925888776779
epoch£º427	 i:4 	 global-step:8544	 l-p:0.11993290483951569
epoch£º427	 i:5 	 global-step:8545	 l-p:-0.0021099853329360485
epoch£º427	 i:6 	 global-step:8546	 l-p:0.1733526885509491
epoch£º427	 i:7 	 global-step:8547	 l-p:0.07423177361488342
epoch£º427	 i:8 	 global-step:8548	 l-p:0.15397298336029053
epoch£º427	 i:9 	 global-step:8549	 l-p:0.14079326391220093
====================================================================================================
====================================================================================================
====================================================================================================

epoch:428
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6811,  0.5993,  1.0000,  0.5273,
          1.0000,  0.8799, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2913,  0.1931,  1.0000,  0.1280,
          1.0000,  0.6629, 31.6228]], device='cuda:0')
 pt:tensor([[5.0023, 5.4520, 5.5059],
        [5.0023, 4.9342, 4.8766],
        [5.0023, 5.3179, 5.2812],
        [5.0023, 4.9420, 4.8633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:428, step:0 
model_pd.l_p.mean(): 0.2531321346759796 
model_pd.l_d.mean(): -20.442745208740234 
model_pd.lagr.mean(): -20.189613342285156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4534], device='cuda:0')), ('power', tensor([-21.2946], device='cuda:0'))])
epoch£º428	 i:0 	 global-step:8560	 l-p:0.2531321346759796
epoch£º428	 i:1 	 global-step:8561	 l-p:0.11910540610551834
epoch£º428	 i:2 	 global-step:8562	 l-p:0.10626929998397827
epoch£º428	 i:3 	 global-step:8563	 l-p:0.13381099700927734
epoch£º428	 i:4 	 global-step:8564	 l-p:0.12260668724775314
epoch£º428	 i:5 	 global-step:8565	 l-p:0.006269330624490976
epoch£º428	 i:6 	 global-step:8566	 l-p:0.1602129340171814
epoch£º428	 i:7 	 global-step:8567	 l-p:0.1316826343536377
epoch£º428	 i:8 	 global-step:8568	 l-p:0.1719018965959549
epoch£º428	 i:9 	 global-step:8569	 l-p:0.11789733916521072
====================================================================================================
====================================================================================================
====================================================================================================

epoch:429
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9563, 4.9059, 4.8034],
        [4.9563, 4.9562, 4.9563],
        [4.9563, 5.1250, 5.0129],
        [4.9563, 5.6240, 5.8539]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:429, step:0 
model_pd.l_p.mean(): 0.11032670736312866 
model_pd.l_d.mean(): -19.25377655029297 
model_pd.lagr.mean(): -19.143449783325195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5908], device='cuda:0')), ('power', tensor([-20.2257], device='cuda:0'))])
epoch£º429	 i:0 	 global-step:8580	 l-p:0.11032670736312866
epoch£º429	 i:1 	 global-step:8581	 l-p:0.10574086010456085
epoch£º429	 i:2 	 global-step:8582	 l-p:0.1502152383327484
epoch£º429	 i:3 	 global-step:8583	 l-p:0.1566242128610611
epoch£º429	 i:4 	 global-step:8584	 l-p:0.16796131432056427
epoch£º429	 i:5 	 global-step:8585	 l-p:-0.01181632000952959
epoch£º429	 i:6 	 global-step:8586	 l-p:0.11027912050485611
epoch£º429	 i:7 	 global-step:8587	 l-p:0.1120162308216095
epoch£º429	 i:8 	 global-step:8588	 l-p:0.07627002149820328
epoch£º429	 i:9 	 global-step:8589	 l-p:0.11241254210472107
====================================================================================================
====================================================================================================
====================================================================================================

epoch:430
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9906, 4.9590, 4.9786],
        [4.9906, 5.6647, 5.8969],
        [4.9906, 4.9294, 4.9401],
        [4.9906, 4.9893, 4.9906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:430, step:0 
model_pd.l_p.mean(): 0.2750837206840515 
model_pd.l_d.mean(): -19.196992874145508 
model_pd.lagr.mean(): -18.92190933227539 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5935], device='cuda:0')), ('power', tensor([-20.1707], device='cuda:0'))])
epoch£º430	 i:0 	 global-step:8600	 l-p:0.2750837206840515
epoch£º430	 i:1 	 global-step:8601	 l-p:0.14836221933364868
epoch£º430	 i:2 	 global-step:8602	 l-p:0.1385841816663742
epoch£º430	 i:3 	 global-step:8603	 l-p:0.14887703955173492
epoch£º430	 i:4 	 global-step:8604	 l-p:0.13142834603786469
epoch£º430	 i:5 	 global-step:8605	 l-p:0.12164025753736496
epoch£º430	 i:6 	 global-step:8606	 l-p:0.11942422389984131
epoch£º430	 i:7 	 global-step:8607	 l-p:0.12091978639364243
epoch£º430	 i:8 	 global-step:8608	 l-p:0.11243358254432678
epoch£º430	 i:9 	 global-step:8609	 l-p:0.1478717029094696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:431
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9326, 5.0334, 4.8983],
        [4.9326, 5.5968, 5.8254],
        [4.9326, 4.9202, 4.9305],
        [4.9326, 4.8700, 4.7791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:431, step:0 
model_pd.l_p.mean(): 0.0354427769780159 
model_pd.l_d.mean(): -19.959205627441406 
model_pd.lagr.mean(): -19.923763275146484 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5245], device='cuda:0')), ('power', tensor([-20.8756], device='cuda:0'))])
epoch£º431	 i:0 	 global-step:8620	 l-p:0.0354427769780159
epoch£º431	 i:1 	 global-step:8621	 l-p:0.10674764215946198
epoch£º431	 i:2 	 global-step:8622	 l-p:0.1372271478176117
epoch£º431	 i:3 	 global-step:8623	 l-p:0.13183051347732544
epoch£º431	 i:4 	 global-step:8624	 l-p:0.16450569033622742
epoch£º431	 i:5 	 global-step:8625	 l-p:0.08286859095096588
epoch£º431	 i:6 	 global-step:8626	 l-p:0.12728983163833618
epoch£º431	 i:7 	 global-step:8627	 l-p:0.13854734599590302
epoch£º431	 i:8 	 global-step:8628	 l-p:0.14052502810955048
epoch£º431	 i:9 	 global-step:8629	 l-p:0.1734611988067627
====================================================================================================
====================================================================================================
====================================================================================================

epoch:432
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9375, 5.5298, 5.6982],
        [4.9375, 4.9374, 4.9375],
        [4.9375, 4.9188, 4.9331],
        [4.9375, 4.9035, 4.9241]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:432, step:0 
model_pd.l_p.mean(): 0.14454752206802368 
model_pd.l_d.mean(): -20.02387046813965 
model_pd.lagr.mean(): -19.879322052001953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5198], device='cuda:0')), ('power', tensor([-20.9366], device='cuda:0'))])
epoch£º432	 i:0 	 global-step:8640	 l-p:0.14454752206802368
epoch£º432	 i:1 	 global-step:8641	 l-p:0.15381063520908356
epoch£º432	 i:2 	 global-step:8642	 l-p:0.09783664345741272
epoch£º432	 i:3 	 global-step:8643	 l-p:0.1226743683218956
epoch£º432	 i:4 	 global-step:8644	 l-p:0.11247479915618896
epoch£º432	 i:5 	 global-step:8645	 l-p:0.13226398825645447
epoch£º432	 i:6 	 global-step:8646	 l-p:0.13715757429599762
epoch£º432	 i:7 	 global-step:8647	 l-p:0.04428331181406975
epoch£º432	 i:8 	 global-step:8648	 l-p:0.2802063524723053
epoch£º432	 i:9 	 global-step:8649	 l-p:0.1457860767841339
====================================================================================================
====================================================================================================
====================================================================================================

epoch:433
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0282, 4.9577, 4.9049],
        [5.0282, 5.2115, 5.1034],
        [5.0282, 5.0283, 5.0283],
        [5.0282, 5.0023, 5.0199]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:433, step:0 
model_pd.l_p.mean(): 0.13973508775234222 
model_pd.l_d.mean(): -19.94208335876465 
model_pd.lagr.mean(): -19.802349090576172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4993], device='cuda:0')), ('power', tensor([-20.8322], device='cuda:0'))])
epoch£º433	 i:0 	 global-step:8660	 l-p:0.13973508775234222
epoch£º433	 i:1 	 global-step:8661	 l-p:0.1448720097541809
epoch£º433	 i:2 	 global-step:8662	 l-p:0.13168638944625854
epoch£º433	 i:3 	 global-step:8663	 l-p:0.3662974536418915
epoch£º433	 i:4 	 global-step:8664	 l-p:0.11350879073143005
epoch£º433	 i:5 	 global-step:8665	 l-p:0.04977556690573692
epoch£º433	 i:6 	 global-step:8666	 l-p:-0.030602149665355682
epoch£º433	 i:7 	 global-step:8667	 l-p:0.18735282123088837
epoch£º433	 i:8 	 global-step:8668	 l-p:0.14589621126651764
epoch£º433	 i:9 	 global-step:8669	 l-p:0.13885609805583954
====================================================================================================
====================================================================================================
====================================================================================================

epoch:434
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6197,  0.5284,  1.0000,  0.4505,
          1.0000,  0.8526, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6300, 31.6228]], device='cuda:0')
 pt:tensor([[5.0676, 5.0355, 4.9223],
        [5.0676, 5.3108, 5.2297],
        [5.0676, 4.9985, 4.9869],
        [5.0676, 4.9988, 4.9485]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:434, step:0 
model_pd.l_p.mean(): 0.14527010917663574 
model_pd.l_d.mean(): -20.724382400512695 
model_pd.lagr.mean(): -20.579113006591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4023], device='cuda:0')), ('power', tensor([-21.5286], device='cuda:0'))])
epoch£º434	 i:0 	 global-step:8680	 l-p:0.14527010917663574
epoch£º434	 i:1 	 global-step:8681	 l-p:0.14634737372398376
epoch£º434	 i:2 	 global-step:8682	 l-p:0.11791464686393738
epoch£º434	 i:3 	 global-step:8683	 l-p:0.5078136920928955
epoch£º434	 i:4 	 global-step:8684	 l-p:0.045792512595653534
epoch£º434	 i:5 	 global-step:8685	 l-p:0.19671303033828735
epoch£º434	 i:6 	 global-step:8686	 l-p:0.1219421848654747
epoch£º434	 i:7 	 global-step:8687	 l-p:0.1511448621749878
epoch£º434	 i:8 	 global-step:8688	 l-p:0.14018338918685913
epoch£º434	 i:9 	 global-step:8689	 l-p:0.09869684278964996
====================================================================================================
====================================================================================================
====================================================================================================

epoch:435
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0052, 5.0050, 5.0052],
        [5.0052, 5.0052, 5.0052],
        [5.0052, 5.0051, 5.0052],
        [5.0052, 5.5937, 5.7531]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:435, step:0 
model_pd.l_p.mean(): 0.15377862751483917 
model_pd.l_d.mean(): -20.65775489807129 
model_pd.lagr.mean(): -20.503976821899414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4203], device='cuda:0')), ('power', tensor([-21.4794], device='cuda:0'))])
epoch£º435	 i:0 	 global-step:8700	 l-p:0.15377862751483917
epoch£º435	 i:1 	 global-step:8701	 l-p:0.0933169350028038
epoch£º435	 i:2 	 global-step:8702	 l-p:0.10867606848478317
epoch£º435	 i:3 	 global-step:8703	 l-p:0.12592639029026031
epoch£º435	 i:4 	 global-step:8704	 l-p:0.1364259272813797
epoch£º435	 i:5 	 global-step:8705	 l-p:0.09884344786405563
epoch£º435	 i:6 	 global-step:8706	 l-p:0.08144130557775497
epoch£º435	 i:7 	 global-step:8707	 l-p:0.17923499643802643
epoch£º435	 i:8 	 global-step:8708	 l-p:0.1697995364665985
epoch£º435	 i:9 	 global-step:8709	 l-p:0.131978377699852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:436
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9249, 5.4642, 5.5903],
        [4.9249, 5.2159, 5.1662],
        [4.9249, 4.9167, 4.9239],
        [4.9249, 4.9925, 4.8490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:436, step:0 
model_pd.l_p.mean(): 0.10618363320827484 
model_pd.l_d.mean(): -20.325490951538086 
model_pd.lagr.mean(): -20.21930694580078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5115], device='cuda:0')), ('power', tensor([-21.2354], device='cuda:0'))])
epoch£º436	 i:0 	 global-step:8720	 l-p:0.10618363320827484
epoch£º436	 i:1 	 global-step:8721	 l-p:0.10260089486837387
epoch£º436	 i:2 	 global-step:8722	 l-p:0.09580931812524796
epoch£º436	 i:3 	 global-step:8723	 l-p:-0.24979430437088013
epoch£º436	 i:4 	 global-step:8724	 l-p:0.12169795483350754
epoch£º436	 i:5 	 global-step:8725	 l-p:0.13873252272605896
epoch£º436	 i:6 	 global-step:8726	 l-p:0.1484009474515915
epoch£º436	 i:7 	 global-step:8727	 l-p:0.162052184343338
epoch£º436	 i:8 	 global-step:8728	 l-p:0.1432131677865982
epoch£º436	 i:9 	 global-step:8729	 l-p:0.11420710384845734
====================================================================================================
====================================================================================================
====================================================================================================

epoch:437
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0225, 5.0185, 5.0222],
        [5.0225, 5.7739, 6.0687],
        [5.0225, 5.2051, 5.0959],
        [5.0225, 5.2366, 5.1415]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:437, step:0 
model_pd.l_p.mean(): 0.05776436999440193 
model_pd.l_d.mean(): -19.321144104003906 
model_pd.lagr.mean(): -19.26338005065918 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4862], device='cuda:0')), ('power', tensor([-20.1860], device='cuda:0'))])
epoch£º437	 i:0 	 global-step:8740	 l-p:0.05776436999440193
epoch£º437	 i:1 	 global-step:8741	 l-p:0.40948227047920227
epoch£º437	 i:2 	 global-step:8742	 l-p:0.17480015754699707
epoch£º437	 i:3 	 global-step:8743	 l-p:0.08076825737953186
epoch£º437	 i:4 	 global-step:8744	 l-p:0.05793530493974686
epoch£º437	 i:5 	 global-step:8745	 l-p:0.10692424327135086
epoch£º437	 i:6 	 global-step:8746	 l-p:0.13807544112205505
epoch£º437	 i:7 	 global-step:8747	 l-p:0.12605515122413635
epoch£º437	 i:8 	 global-step:8748	 l-p:0.1674644500017166
epoch£º437	 i:9 	 global-step:8749	 l-p:0.11030619591474533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:438
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2510, 5.1936, 5.1404],
        [5.2510, 5.3159, 5.1805],
        [5.2510, 5.2268, 5.2430],
        [5.2510, 5.2510, 5.2510]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:438, step:0 
model_pd.l_p.mean(): 0.12328799813985825 
model_pd.l_d.mean(): -19.226806640625 
model_pd.lagr.mean(): -19.103519439697266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4199], device='cuda:0')), ('power', tensor([-20.0212], device='cuda:0'))])
epoch£º438	 i:0 	 global-step:8760	 l-p:0.12328799813985825
epoch£º438	 i:1 	 global-step:8761	 l-p:0.13101066648960114
epoch£º438	 i:2 	 global-step:8762	 l-p:0.1806824505329132
epoch£º438	 i:3 	 global-step:8763	 l-p:0.1052442416548729
epoch£º438	 i:4 	 global-step:8764	 l-p:0.08012986183166504
epoch£º438	 i:5 	 global-step:8765	 l-p:0.1320609450340271
epoch£º438	 i:6 	 global-step:8766	 l-p:0.17669326066970825
epoch£º438	 i:7 	 global-step:8767	 l-p:0.11763490736484528
epoch£º438	 i:8 	 global-step:8768	 l-p:0.13985083997249603
epoch£º438	 i:9 	 global-step:8769	 l-p:0.11785764247179031
====================================================================================================
====================================================================================================
====================================================================================================

epoch:439
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2415, 5.2114, 5.1101],
        [5.2415, 5.2320, 5.1146],
        [5.2415, 5.2358, 5.1162],
        [5.2415, 5.2413, 5.2415]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:439, step:0 
model_pd.l_p.mean(): 0.08725054562091827 
model_pd.l_d.mean(): -20.590747833251953 
model_pd.lagr.mean(): -20.503498077392578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3574], device='cuda:0')), ('power', tensor([-21.3459], device='cuda:0'))])
epoch£º439	 i:0 	 global-step:8780	 l-p:0.08725054562091827
epoch£º439	 i:1 	 global-step:8781	 l-p:0.11779295653104782
epoch£º439	 i:2 	 global-step:8782	 l-p:0.20968221127986908
epoch£º439	 i:3 	 global-step:8783	 l-p:0.11674612760543823
epoch£º439	 i:4 	 global-step:8784	 l-p:0.12870192527770996
epoch£º439	 i:5 	 global-step:8785	 l-p:0.6954043507575989
epoch£º439	 i:6 	 global-step:8786	 l-p:0.1326821744441986
epoch£º439	 i:7 	 global-step:8787	 l-p:0.18683229386806488
epoch£º439	 i:8 	 global-step:8788	 l-p:0.14335201680660248
epoch£º439	 i:9 	 global-step:8789	 l-p:0.1245976984500885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:440
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1169, 5.0912, 5.1086],
        [5.1169, 5.0962, 5.1114],
        [5.1169, 5.0474, 5.0315],
        [5.1169, 5.2889, 5.1741]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:440, step:0 
model_pd.l_p.mean(): 0.0650712177157402 
model_pd.l_d.mean(): -19.224390029907227 
model_pd.lagr.mean(): -19.159318923950195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4744], device='cuda:0')), ('power', tensor([-20.0752], device='cuda:0'))])
epoch£º440	 i:0 	 global-step:8800	 l-p:0.0650712177157402
epoch£º440	 i:1 	 global-step:8801	 l-p:0.12653596699237823
epoch£º440	 i:2 	 global-step:8802	 l-p:0.10984520614147186
epoch£º440	 i:3 	 global-step:8803	 l-p:0.15730519592761993
epoch£º440	 i:4 	 global-step:8804	 l-p:0.1423012763261795
epoch£º440	 i:5 	 global-step:8805	 l-p:0.1335781216621399
epoch£º440	 i:6 	 global-step:8806	 l-p:-0.6845234036445618
epoch£º440	 i:7 	 global-step:8807	 l-p:0.09154663234949112
epoch£º440	 i:8 	 global-step:8808	 l-p:0.14736057817935944
epoch£º440	 i:9 	 global-step:8809	 l-p:0.14410310983657837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:441
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0500, 5.0420, 5.0490],
        [5.0500, 5.2066, 5.0862],
        [5.0500, 5.0492, 5.0500],
        [5.0500, 4.9876, 4.9041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:441, step:0 
model_pd.l_p.mean(): 0.05050104856491089 
model_pd.l_d.mean(): -19.012205123901367 
model_pd.lagr.mean(): -18.96170425415039 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5416], device='cuda:0')), ('power', tensor([-19.9287], device='cuda:0'))])
epoch£º441	 i:0 	 global-step:8820	 l-p:0.05050104856491089
epoch£º441	 i:1 	 global-step:8821	 l-p:0.13876813650131226
epoch£º441	 i:2 	 global-step:8822	 l-p:-13.39433479309082
epoch£º441	 i:3 	 global-step:8823	 l-p:0.1487034559249878
epoch£º441	 i:4 	 global-step:8824	 l-p:0.010581216774880886
epoch£º441	 i:5 	 global-step:8825	 l-p:0.13289287686347961
epoch£º441	 i:6 	 global-step:8826	 l-p:0.12576860189437866
epoch£º441	 i:7 	 global-step:8827	 l-p:0.16224302351474762
epoch£º441	 i:8 	 global-step:8828	 l-p:0.11406728625297546
epoch£º441	 i:9 	 global-step:8829	 l-p:0.12164994329214096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:442
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1840, 5.1825, 5.1840],
        [5.1840, 5.6069, 5.6287],
        [5.1840, 5.2286, 5.0903],
        [5.1840, 5.1646, 5.1790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:442, step:0 
model_pd.l_p.mean(): 0.12304306030273438 
model_pd.l_d.mean(): -20.116968154907227 
model_pd.lagr.mean(): -19.993925094604492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4117], device='cuda:0')), ('power', tensor([-20.9195], device='cuda:0'))])
epoch£º442	 i:0 	 global-step:8840	 l-p:0.12304306030273438
epoch£º442	 i:1 	 global-step:8841	 l-p:0.129855215549469
epoch£º442	 i:2 	 global-step:8842	 l-p:0.3309853672981262
epoch£º442	 i:3 	 global-step:8843	 l-p:0.14729554951190948
epoch£º442	 i:4 	 global-step:8844	 l-p:0.14802154898643494
epoch£º442	 i:5 	 global-step:8845	 l-p:-0.5970994234085083
epoch£º442	 i:6 	 global-step:8846	 l-p:0.13419654965400696
epoch£º442	 i:7 	 global-step:8847	 l-p:0.13724714517593384
epoch£º442	 i:8 	 global-step:8848	 l-p:0.11662345379590988
epoch£º442	 i:9 	 global-step:8849	 l-p:-13.609960556030273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:443
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1724, 5.1363, 5.1559],
        [5.1724, 5.1716, 5.1723],
        [5.1724, 5.1714, 5.1723],
        [5.1724, 5.1256, 5.1437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:443, step:0 
model_pd.l_p.mean(): 0.11275053769350052 
model_pd.l_d.mean(): -19.888092041015625 
model_pd.lagr.mean(): -19.775341033935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4221], device='cuda:0')), ('power', tensor([-20.6972], device='cuda:0'))])
epoch£º443	 i:0 	 global-step:8860	 l-p:0.11275053769350052
epoch£º443	 i:1 	 global-step:8861	 l-p:0.13292033970355988
epoch£º443	 i:2 	 global-step:8862	 l-p:0.12808465957641602
epoch£º443	 i:3 	 global-step:8863	 l-p:-1.006504774093628
epoch£º443	 i:4 	 global-step:8864	 l-p:0.15837304294109344
epoch£º443	 i:5 	 global-step:8865	 l-p:0.13725334405899048
epoch£º443	 i:6 	 global-step:8866	 l-p:0.11641662567853928
epoch£º443	 i:7 	 global-step:8867	 l-p:0.030724186450242996
epoch£º443	 i:8 	 global-step:8868	 l-p:0.12762318551540375
epoch£º443	 i:9 	 global-step:8869	 l-p:0.5681366920471191
====================================================================================================
====================================================================================================
====================================================================================================

epoch:444
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9670e-01, 3.9336e-01,
         1.0000e+00, 3.1152e-01, 1.0000e+00, 7.9195e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0716, 5.2163, 5.0909],
        [5.0716, 5.0233, 5.0433],
        [5.0716, 4.9971, 4.9549],
        [5.0716, 5.1594, 5.0194]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:444, step:0 
model_pd.l_p.mean(): -3.8697750568389893 
model_pd.l_d.mean(): -18.82508087158203 
model_pd.lagr.mean(): -22.694856643676758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5343], device='cuda:0')), ('power', tensor([-19.7305], device='cuda:0'))])
epoch£º444	 i:0 	 global-step:8880	 l-p:-3.8697750568389893
epoch£º444	 i:1 	 global-step:8881	 l-p:0.14172570407390594
epoch£º444	 i:2 	 global-step:8882	 l-p:0.1337735503911972
epoch£º444	 i:3 	 global-step:8883	 l-p:0.12716303765773773
epoch£º444	 i:4 	 global-step:8884	 l-p:1.0574723482131958
epoch£º444	 i:5 	 global-step:8885	 l-p:0.1283939927816391
epoch£º444	 i:6 	 global-step:8886	 l-p:0.14749127626419067
epoch£º444	 i:7 	 global-step:8887	 l-p:0.15756821632385254
epoch£º444	 i:8 	 global-step:8888	 l-p:0.1663164347410202
epoch£º444	 i:9 	 global-step:8889	 l-p:-0.13648666441440582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:445
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9736, 4.9732, 4.9736],
        [4.9736, 4.8981, 4.8244],
        [4.9736, 4.9629, 4.9720],
        [4.9736, 4.9007, 4.9037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:445, step:0 
model_pd.l_p.mean(): -0.17427635192871094 
model_pd.l_d.mean(): -20.47718048095703 
model_pd.lagr.mean(): -20.651456832885742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4640], device='cuda:0')), ('power', tensor([-21.3407], device='cuda:0'))])
epoch£º445	 i:0 	 global-step:8900	 l-p:-0.17427635192871094
epoch£º445	 i:1 	 global-step:8901	 l-p:0.14077569544315338
epoch£º445	 i:2 	 global-step:8902	 l-p:0.10294618457555771
epoch£º445	 i:3 	 global-step:8903	 l-p:0.013820881955325603
epoch£º445	 i:4 	 global-step:8904	 l-p:0.13630345463752747
epoch£º445	 i:5 	 global-step:8905	 l-p:0.16720375418663025
epoch£º445	 i:6 	 global-step:8906	 l-p:0.10928358137607574
epoch£º445	 i:7 	 global-step:8907	 l-p:0.11673539876937866
epoch£º445	 i:8 	 global-step:8908	 l-p:0.1331198513507843
epoch£º445	 i:9 	 global-step:8909	 l-p:0.1287384182214737
====================================================================================================
====================================================================================================
====================================================================================================

epoch:446
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2822,  0.1851,  1.0000,  0.1214,
          1.0000,  0.6559, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3559,  0.2522,  1.0000,  0.1787,
          1.0000,  0.7086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8102,  0.7554,  1.0000,  0.7042,
          1.0000,  0.9323, 31.6228]], device='cuda:0')
 pt:tensor([[5.0000, 4.9260, 4.8528],
        [5.0000, 5.4528, 5.5064],
        [5.0000, 4.9569, 4.8372],
        [5.0000, 5.4788, 5.5515]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:446, step:0 
model_pd.l_p.mean(): 0.11259458214044571 
model_pd.l_d.mean(): -20.044021606445312 
model_pd.lagr.mean(): -19.931427001953125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4999], device='cuda:0')), ('power', tensor([-20.9366], device='cuda:0'))])
epoch£º446	 i:0 	 global-step:8920	 l-p:0.11259458214044571
epoch£º446	 i:1 	 global-step:8921	 l-p:0.14106562733650208
epoch£º446	 i:2 	 global-step:8922	 l-p:0.13134776055812836
epoch£º446	 i:3 	 global-step:8923	 l-p:0.10038431733846664
epoch£º446	 i:4 	 global-step:8924	 l-p:0.14123278856277466
epoch£º446	 i:5 	 global-step:8925	 l-p:0.1185932382941246
epoch£º446	 i:6 	 global-step:8926	 l-p:0.14205104112625122
epoch£º446	 i:7 	 global-step:8927	 l-p:0.1487545520067215
epoch£º446	 i:8 	 global-step:8928	 l-p:0.04856989160180092
epoch£º446	 i:9 	 global-step:8929	 l-p:0.07715384662151337
====================================================================================================
====================================================================================================
====================================================================================================

epoch:447
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9700, 4.9649, 4.9696],
        [4.9700, 4.9304, 4.9527],
        [4.9700, 4.9010, 4.8082],
        [4.9700, 4.9033, 4.8064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:447, step:0 
model_pd.l_p.mean(): 0.10359561443328857 
model_pd.l_d.mean(): -20.230823516845703 
model_pd.lagr.mean(): -20.127227783203125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4915], device='cuda:0')), ('power', tensor([-21.1181], device='cuda:0'))])
epoch£º447	 i:0 	 global-step:8940	 l-p:0.10359561443328857
epoch£º447	 i:1 	 global-step:8941	 l-p:0.1443396508693695
epoch£º447	 i:2 	 global-step:8942	 l-p:0.1497480422258377
epoch£º447	 i:3 	 global-step:8943	 l-p:0.13667811453342438
epoch£º447	 i:4 	 global-step:8944	 l-p:0.13387100398540497
epoch£º447	 i:5 	 global-step:8945	 l-p:-0.021418103948235512
epoch£º447	 i:6 	 global-step:8946	 l-p:0.14734968543052673
epoch£º447	 i:7 	 global-step:8947	 l-p:0.09811075031757355
epoch£º447	 i:8 	 global-step:8948	 l-p:0.09778448194265366
epoch£º447	 i:9 	 global-step:8949	 l-p:0.11854114383459091
====================================================================================================
====================================================================================================
====================================================================================================

epoch:448
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9816, 5.5498, 5.6924],
        [4.9816, 5.3667, 5.3730],
        [4.9816, 4.9521, 4.9718],
        [4.9816, 4.9815, 4.9816]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:448, step:0 
model_pd.l_p.mean(): 0.13646206259727478 
model_pd.l_d.mean(): -19.66501235961914 
model_pd.lagr.mean(): -19.52855110168457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5322], device='cuda:0')), ('power', tensor([-20.5839], device='cuda:0'))])
epoch£º448	 i:0 	 global-step:8960	 l-p:0.13646206259727478
epoch£º448	 i:1 	 global-step:8961	 l-p:0.09913893789052963
epoch£º448	 i:2 	 global-step:8962	 l-p:0.1474510282278061
epoch£º448	 i:3 	 global-step:8963	 l-p:0.0060000941157341
epoch£º448	 i:4 	 global-step:8964	 l-p:0.0961776077747345
epoch£º448	 i:5 	 global-step:8965	 l-p:0.08664407581090927
epoch£º448	 i:6 	 global-step:8966	 l-p:0.13636431097984314
epoch£º448	 i:7 	 global-step:8967	 l-p:0.1330367624759674
epoch£º448	 i:8 	 global-step:8968	 l-p:0.21827027201652527
epoch£º448	 i:9 	 global-step:8969	 l-p:0.13722734153270721
====================================================================================================
====================================================================================================
====================================================================================================

epoch:449
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9356, 4.9334, 4.9354],
        [4.9356, 4.8517, 4.8369],
        [4.9356, 4.8944, 4.9174],
        [4.9356, 4.8855, 4.9085]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:449, step:0 
model_pd.l_p.mean(): 0.11259913444519043 
model_pd.l_d.mean(): -20.60762596130371 
model_pd.lagr.mean(): -20.495027542114258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4636], device='cuda:0')), ('power', tensor([-21.4732], device='cuda:0'))])
epoch£º449	 i:0 	 global-step:8980	 l-p:0.11259913444519043
epoch£º449	 i:1 	 global-step:8981	 l-p:0.1085512712597847
epoch£º449	 i:2 	 global-step:8982	 l-p:0.16965964436531067
epoch£º449	 i:3 	 global-step:8983	 l-p:0.17257274687290192
epoch£º449	 i:4 	 global-step:8984	 l-p:0.15904055535793304
epoch£º449	 i:5 	 global-step:8985	 l-p:0.06245919689536095
epoch£º449	 i:6 	 global-step:8986	 l-p:0.14096306264400482
epoch£º449	 i:7 	 global-step:8987	 l-p:0.12226184457540512
epoch£º449	 i:8 	 global-step:8988	 l-p:0.1769225299358368
epoch£º449	 i:9 	 global-step:8989	 l-p:0.11037459969520569
====================================================================================================
====================================================================================================
====================================================================================================

epoch:450
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9710, 4.9706, 4.9710],
        [4.9710, 4.9709, 4.9710],
        [4.9710, 4.9678, 4.9708],
        [4.9710, 5.2153, 5.1340]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:450, step:0 
model_pd.l_p.mean(): 0.08949302136898041 
model_pd.l_d.mean(): -20.518417358398438 
model_pd.lagr.mean(): -20.428924560546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4651], device='cuda:0')), ('power', tensor([-21.3838], device='cuda:0'))])
epoch£º450	 i:0 	 global-step:9000	 l-p:0.08949302136898041
epoch£º450	 i:1 	 global-step:9001	 l-p:0.15657800436019897
epoch£º450	 i:2 	 global-step:9002	 l-p:0.11998473107814789
epoch£º450	 i:3 	 global-step:9003	 l-p:0.12756185233592987
epoch£º450	 i:4 	 global-step:9004	 l-p:0.07719490677118301
epoch£º450	 i:5 	 global-step:9005	 l-p:-0.300682008266449
epoch£º450	 i:6 	 global-step:9006	 l-p:0.1758505254983902
epoch£º450	 i:7 	 global-step:9007	 l-p:0.14862747490406036
epoch£º450	 i:8 	 global-step:9008	 l-p:0.09445890039205551
epoch£º450	 i:9 	 global-step:9009	 l-p:0.12090703099966049
====================================================================================================
====================================================================================================
====================================================================================================

epoch:451
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0184, 4.9952, 5.0122],
        [5.0184, 4.9644, 4.9853],
        [5.0184, 5.0183, 5.0184],
        [5.0184, 5.0181, 5.0184]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:451, step:0 
model_pd.l_p.mean(): 0.13416199386119843 
model_pd.l_d.mean(): -18.626604080200195 
model_pd.lagr.mean(): -18.492441177368164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5841], device='cuda:0')), ('power', tensor([-19.5799], device='cuda:0'))])
epoch£º451	 i:0 	 global-step:9020	 l-p:0.13416199386119843
epoch£º451	 i:1 	 global-step:9021	 l-p:0.10621827840805054
epoch£º451	 i:2 	 global-step:9022	 l-p:0.14076034724712372
epoch£º451	 i:3 	 global-step:9023	 l-p:0.15612904727458954
epoch£º451	 i:4 	 global-step:9024	 l-p:0.07203003019094467
epoch£º451	 i:5 	 global-step:9025	 l-p:0.11916327476501465
epoch£º451	 i:6 	 global-step:9026	 l-p:2.7550048828125
epoch£º451	 i:7 	 global-step:9027	 l-p:0.14835044741630554
epoch£º451	 i:8 	 global-step:9028	 l-p:0.13447746634483337
epoch£º451	 i:9 	 global-step:9029	 l-p:0.04153016582131386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:452
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0306, 4.9659, 4.9807],
        [5.0306, 5.0292, 5.0305],
        [5.0306, 5.0278, 5.0304],
        [5.0306, 5.0306, 5.0306]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:452, step:0 
model_pd.l_p.mean(): 0.2258678823709488 
model_pd.l_d.mean(): -19.935420989990234 
model_pd.lagr.mean(): -19.709552764892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5016], device='cuda:0')), ('power', tensor([-20.8277], device='cuda:0'))])
epoch£º452	 i:0 	 global-step:9040	 l-p:0.2258678823709488
epoch£º452	 i:1 	 global-step:9041	 l-p:15.085460662841797
epoch£º452	 i:2 	 global-step:9042	 l-p:0.10824446380138397
epoch£º452	 i:3 	 global-step:9043	 l-p:0.12578588724136353
epoch£º452	 i:4 	 global-step:9044	 l-p:0.12694653868675232
epoch£º452	 i:5 	 global-step:9045	 l-p:0.06968894600868225
epoch£º452	 i:6 	 global-step:9046	 l-p:0.12031856179237366
epoch£º452	 i:7 	 global-step:9047	 l-p:0.006814074236899614
epoch£º452	 i:8 	 global-step:9048	 l-p:0.13361701369285583
epoch£º452	 i:9 	 global-step:9049	 l-p:0.13579024374485016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:453
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0760, 5.0745, 5.0759],
        [5.0760, 5.0760, 5.0760],
        [5.0760, 5.0738, 5.0758],
        [5.0760, 5.0378, 5.0596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:453, step:0 
model_pd.l_p.mean(): 0.23287537693977356 
model_pd.l_d.mean(): -20.527393341064453 
model_pd.lagr.mean(): -20.294517517089844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4283], device='cuda:0')), ('power', tensor([-21.3549], device='cuda:0'))])
epoch£º453	 i:0 	 global-step:9060	 l-p:0.23287537693977356
epoch£º453	 i:1 	 global-step:9061	 l-p:0.1423768848180771
epoch£º453	 i:2 	 global-step:9062	 l-p:0.06286077201366425
epoch£º453	 i:3 	 global-step:9063	 l-p:0.07528002560138702
epoch£º453	 i:4 	 global-step:9064	 l-p:0.12637819349765778
epoch£º453	 i:5 	 global-step:9065	 l-p:0.12941265106201172
epoch£º453	 i:6 	 global-step:9066	 l-p:0.2534227967262268
epoch£º453	 i:7 	 global-step:9067	 l-p:0.07288260012865067
epoch£º453	 i:8 	 global-step:9068	 l-p:0.11001182347536087
epoch£º453	 i:9 	 global-step:9069	 l-p:0.1390993446111679
====================================================================================================
====================================================================================================
====================================================================================================

epoch:454
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1331, 5.0625, 5.0631],
        [5.1331, 5.2675, 5.1353],
        [5.1331, 5.1303, 5.1330],
        [5.1331, 5.1211, 5.1312]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:454, step:0 
model_pd.l_p.mean(): 0.1425628960132599 
model_pd.l_d.mean(): -20.23760223388672 
model_pd.lagr.mean(): -20.09503936767578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4133], device='cuda:0')), ('power', tensor([-21.0441], device='cuda:0'))])
epoch£º454	 i:0 	 global-step:9080	 l-p:0.1425628960132599
epoch£º454	 i:1 	 global-step:9081	 l-p:-0.15244893729686737
epoch£º454	 i:2 	 global-step:9082	 l-p:0.1695646196603775
epoch£º454	 i:3 	 global-step:9083	 l-p:0.14671671390533447
epoch£º454	 i:4 	 global-step:9084	 l-p:0.1566265970468521
epoch£º454	 i:5 	 global-step:9085	 l-p:0.12256284803152084
epoch£º454	 i:6 	 global-step:9086	 l-p:0.11641988158226013
epoch£º454	 i:7 	 global-step:9087	 l-p:0.09890662878751755
epoch£º454	 i:8 	 global-step:9088	 l-p:-0.01689663901925087
epoch£º454	 i:9 	 global-step:9089	 l-p:0.13324622809886932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:455
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0269, 4.9490, 4.8722],
        [5.0269, 5.0264, 5.0269],
        [5.0269, 5.0267, 5.0269],
        [5.0269, 5.0269, 5.0269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:455, step:0 
model_pd.l_p.mean(): 0.13024650514125824 
model_pd.l_d.mean(): -20.05367660522461 
model_pd.lagr.mean(): -19.923429489135742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4911], device='cuda:0')), ('power', tensor([-20.9373], device='cuda:0'))])
epoch£º455	 i:0 	 global-step:9100	 l-p:0.13024650514125824
epoch£º455	 i:1 	 global-step:9101	 l-p:0.12081380933523178
epoch£º455	 i:2 	 global-step:9102	 l-p:0.1519831418991089
epoch£º455	 i:3 	 global-step:9103	 l-p:0.12642712891101837
epoch£º455	 i:4 	 global-step:9104	 l-p:0.09836892038583755
epoch£º455	 i:5 	 global-step:9105	 l-p:0.11465154588222504
epoch£º455	 i:6 	 global-step:9106	 l-p:0.1283702254295349
epoch£º455	 i:7 	 global-step:9107	 l-p:0.28525081276893616
epoch£º455	 i:8 	 global-step:9108	 l-p:0.1721264272928238
epoch£º455	 i:9 	 global-step:9109	 l-p:0.1300046592950821
====================================================================================================
====================================================================================================
====================================================================================================

epoch:456
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8439, 4.8439, 4.8439],
        [4.8439, 4.7727, 4.7917],
        [4.8439, 4.8433, 4.8439],
        [4.8439, 4.8409, 4.8437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:456, step:0 
model_pd.l_p.mean(): 0.11575017869472504 
model_pd.l_d.mean(): -20.286760330200195 
model_pd.lagr.mean(): -20.171010971069336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5099], device='cuda:0')), ('power', tensor([-21.1942], device='cuda:0'))])
epoch£º456	 i:0 	 global-step:9120	 l-p:0.11575017869472504
epoch£º456	 i:1 	 global-step:9121	 l-p:0.16519767045974731
epoch£º456	 i:2 	 global-step:9122	 l-p:0.15432967245578766
epoch£º456	 i:3 	 global-step:9123	 l-p:0.909758448600769
epoch£º456	 i:4 	 global-step:9124	 l-p:0.13585735857486725
epoch£º456	 i:5 	 global-step:9125	 l-p:0.18559421598911285
epoch£º456	 i:6 	 global-step:9126	 l-p:0.1437620222568512
epoch£º456	 i:7 	 global-step:9127	 l-p:0.2232569456100464
epoch£º456	 i:8 	 global-step:9128	 l-p:0.15119680762290955
epoch£º456	 i:9 	 global-step:9129	 l-p:0.09870816767215729
====================================================================================================
====================================================================================================
====================================================================================================

epoch:457
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9111, 5.0733, 4.9511],
        [4.9111, 4.8362, 4.8493],
        [4.9111, 4.9065, 4.7523],
        [4.9111, 4.9111, 4.9111]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:457, step:0 
model_pd.l_p.mean(): 0.20454218983650208 
model_pd.l_d.mean(): -20.721052169799805 
model_pd.lagr.mean(): -20.516510009765625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4566], device='cuda:0')), ('power', tensor([-21.5814], device='cuda:0'))])
epoch£º457	 i:0 	 global-step:9140	 l-p:0.20454218983650208
epoch£º457	 i:1 	 global-step:9141	 l-p:0.08690500259399414
epoch£º457	 i:2 	 global-step:9142	 l-p:0.14637281000614166
epoch£º457	 i:3 	 global-step:9143	 l-p:0.1404775083065033
epoch£º457	 i:4 	 global-step:9144	 l-p:0.10740009695291519
epoch£º457	 i:5 	 global-step:9145	 l-p:0.1542004942893982
epoch£º457	 i:6 	 global-step:9146	 l-p:0.12498166412115097
epoch£º457	 i:7 	 global-step:9147	 l-p:0.11767545342445374
epoch£º457	 i:8 	 global-step:9148	 l-p:0.037783585488796234
epoch£º457	 i:9 	 global-step:9149	 l-p:0.12627920508384705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:458
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0748, 5.0747, 5.0748],
        [5.0748, 5.0318, 5.0545],
        [5.0748, 5.0748, 5.0748],
        [5.0748, 5.0747, 5.0748]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:458, step:0 
model_pd.l_p.mean(): 0.146454319357872 
model_pd.l_d.mean(): -20.560699462890625 
model_pd.lagr.mean(): -20.41424560546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4246], device='cuda:0')), ('power', tensor([-21.3849], device='cuda:0'))])
epoch£º458	 i:0 	 global-step:9160	 l-p:0.146454319357872
epoch£º458	 i:1 	 global-step:9161	 l-p:0.11840184032917023
epoch£º458	 i:2 	 global-step:9162	 l-p:0.13226674497127533
epoch£º458	 i:3 	 global-step:9163	 l-p:0.18616801500320435
epoch£º458	 i:4 	 global-step:9164	 l-p:0.11473540961742401
epoch£º458	 i:5 	 global-step:9165	 l-p:0.12571293115615845
epoch£º458	 i:6 	 global-step:9166	 l-p:-0.018703937530517578
epoch£º458	 i:7 	 global-step:9167	 l-p:0.1322842538356781
epoch£º458	 i:8 	 global-step:9168	 l-p:0.12246057391166687
epoch£º458	 i:9 	 global-step:9169	 l-p:0.22859440743923187
====================================================================================================
====================================================================================================
====================================================================================================

epoch:459
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1197, 5.1197, 5.1197],
        [5.1197, 5.2485, 5.1132],
        [5.1197, 5.8667, 6.1446],
        [5.1197, 5.1196, 5.1197]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:459, step:0 
model_pd.l_p.mean(): 0.13903763890266418 
model_pd.l_d.mean(): -20.060977935791016 
model_pd.lagr.mean(): -19.921939849853516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4200], device='cuda:0')), ('power', tensor([-20.8711], device='cuda:0'))])
epoch£º459	 i:0 	 global-step:9180	 l-p:0.13903763890266418
epoch£º459	 i:1 	 global-step:9181	 l-p:0.1433819979429245
epoch£º459	 i:2 	 global-step:9182	 l-p:0.11543864756822586
epoch£º459	 i:3 	 global-step:9183	 l-p:0.2451661378145218
epoch£º459	 i:4 	 global-step:9184	 l-p:0.07288776338100433
epoch£º459	 i:5 	 global-step:9185	 l-p:0.05717194825410843
epoch£º459	 i:6 	 global-step:9186	 l-p:0.13587355613708496
epoch£º459	 i:7 	 global-step:9187	 l-p:0.11300094425678253
epoch£º459	 i:8 	 global-step:9188	 l-p:0.15123610198497772
epoch£º459	 i:9 	 global-step:9189	 l-p:0.03870273008942604
====================================================================================================
====================================================================================================
====================================================================================================

epoch:460
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0459, 5.0362, 5.0446],
        [5.0459, 4.9662, 4.8908],
        [5.0459, 5.7084, 5.9202],
        [5.0459, 5.0707, 4.9186]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:460, step:0 
model_pd.l_p.mean(): 0.12303326278924942 
model_pd.l_d.mean(): -18.639196395874023 
model_pd.lagr.mean(): -18.516162872314453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5676], device='cuda:0')), ('power', tensor([-19.5756], device='cuda:0'))])
epoch£º460	 i:0 	 global-step:9200	 l-p:0.12303326278924942
epoch£º460	 i:1 	 global-step:9201	 l-p:0.13043902814388275
epoch£º460	 i:2 	 global-step:9202	 l-p:0.12587213516235352
epoch£º460	 i:3 	 global-step:9203	 l-p:0.24520805478096008
epoch£º460	 i:4 	 global-step:9204	 l-p:0.10842327028512955
epoch£º460	 i:5 	 global-step:9205	 l-p:0.14521348476409912
epoch£º460	 i:6 	 global-step:9206	 l-p:0.14386187493801117
epoch£º460	 i:7 	 global-step:9207	 l-p:0.13789041340351105
epoch£º460	 i:8 	 global-step:9208	 l-p:0.10108925402164459
epoch£º460	 i:9 	 global-step:9209	 l-p:0.11020852625370026
====================================================================================================
====================================================================================================
====================================================================================================

epoch:461
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9502, 4.9481, 4.9501],
        [4.9502, 4.8597, 4.7931],
        [4.9502, 4.9025, 4.9270],
        [4.9502, 5.0251, 4.8735]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:461, step:0 
model_pd.l_p.mean(): 0.16831782460212708 
model_pd.l_d.mean(): -18.435457229614258 
model_pd.lagr.mean(): -18.267139434814453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5806], device='cuda:0')), ('power', tensor([-19.3815], device='cuda:0'))])
epoch£º461	 i:0 	 global-step:9220	 l-p:0.16831782460212708
epoch£º461	 i:1 	 global-step:9221	 l-p:0.03865678235888481
epoch£º461	 i:2 	 global-step:9222	 l-p:0.11160372197628021
epoch£º461	 i:3 	 global-step:9223	 l-p:0.15940475463867188
epoch£º461	 i:4 	 global-step:9224	 l-p:0.13873416185379028
epoch£º461	 i:5 	 global-step:9225	 l-p:0.12963545322418213
epoch£º461	 i:6 	 global-step:9226	 l-p:0.15941420197486877
epoch£º461	 i:7 	 global-step:9227	 l-p:0.07773470878601074
epoch£º461	 i:8 	 global-step:9228	 l-p:0.09747819602489471
epoch£º461	 i:9 	 global-step:9229	 l-p:0.11667022109031677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:462
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0164, 4.9691, 4.8385],
        [5.0164, 4.9437, 4.8423],
        [5.0164, 4.9743, 4.9978],
        [5.0164, 4.9748, 4.9983]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:462, step:0 
model_pd.l_p.mean(): 0.09196583181619644 
model_pd.l_d.mean(): -20.30520248413086 
model_pd.lagr.mean(): -20.21323585510254 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4716], device='cuda:0')), ('power', tensor([-21.1733], device='cuda:0'))])
epoch£º462	 i:0 	 global-step:9240	 l-p:0.09196583181619644
epoch£º462	 i:1 	 global-step:9241	 l-p:0.2893100380897522
epoch£º462	 i:2 	 global-step:9242	 l-p:0.14557980000972748
epoch£º462	 i:3 	 global-step:9243	 l-p:0.09645506739616394
epoch£º462	 i:4 	 global-step:9244	 l-p:-0.2721311151981354
epoch£º462	 i:5 	 global-step:9245	 l-p:0.11929427087306976
epoch£º462	 i:6 	 global-step:9246	 l-p:0.15058362483978271
epoch£º462	 i:7 	 global-step:9247	 l-p:0.051708120852708817
epoch£º462	 i:8 	 global-step:9248	 l-p:0.09040989726781845
epoch£º462	 i:9 	 global-step:9249	 l-p:0.13993756473064423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:463
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0828, 5.0812, 5.0827],
        [5.0828, 5.0717, 5.0811],
        [5.0828, 5.0729, 5.0814],
        [5.0828, 5.0826, 5.0828]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:463, step:0 
model_pd.l_p.mean(): 0.17735634744167328 
model_pd.l_d.mean(): -18.08547592163086 
model_pd.lagr.mean(): -17.908119201660156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5948], device='cuda:0')), ('power', tensor([-19.0397], device='cuda:0'))])
epoch£º463	 i:0 	 global-step:9260	 l-p:0.17735634744167328
epoch£º463	 i:1 	 global-step:9261	 l-p:0.5452617406845093
epoch£º463	 i:2 	 global-step:9262	 l-p:0.14415910840034485
epoch£º463	 i:3 	 global-step:9263	 l-p:0.023726973682641983
epoch£º463	 i:4 	 global-step:9264	 l-p:0.1259506791830063
epoch£º463	 i:5 	 global-step:9265	 l-p:0.11757966876029968
epoch£º463	 i:6 	 global-step:9266	 l-p:0.12261205166578293
epoch£º463	 i:7 	 global-step:9267	 l-p:0.12926363945007324
epoch£º463	 i:8 	 global-step:9268	 l-p:0.05679820850491524
epoch£º463	 i:9 	 global-step:9269	 l-p:0.11752457171678543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:464
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1228, 5.1191, 5.1225],
        [5.1228, 5.2049, 5.0568],
        [5.1228, 5.3198, 5.2085],
        [5.1228, 5.1228, 5.1228]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:464, step:0 
model_pd.l_p.mean(): -0.039282653480768204 
model_pd.l_d.mean(): -20.418386459350586 
model_pd.lagr.mean(): -20.45766830444336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4235], device='cuda:0')), ('power', tensor([-21.2388], device='cuda:0'))])
epoch£º464	 i:0 	 global-step:9280	 l-p:-0.039282653480768204
epoch£º464	 i:1 	 global-step:9281	 l-p:0.1774589866399765
epoch£º464	 i:2 	 global-step:9282	 l-p:0.05358012393116951
epoch£º464	 i:3 	 global-step:9283	 l-p:0.1319223791360855
epoch£º464	 i:4 	 global-step:9284	 l-p:0.11153852194547653
epoch£º464	 i:5 	 global-step:9285	 l-p:0.12090404331684113
epoch£º464	 i:6 	 global-step:9286	 l-p:0.22158530354499817
epoch£º464	 i:7 	 global-step:9287	 l-p:0.14280685782432556
epoch£º464	 i:8 	 global-step:9288	 l-p:0.17070356011390686
epoch£º464	 i:9 	 global-step:9289	 l-p:0.13191913068294525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:465
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1164, 5.1164, 5.1164],
        [5.1164, 5.1149, 5.1164],
        [5.1164, 5.0958, 5.1115],
        [5.1164, 5.0332, 4.9888]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:465, step:0 
model_pd.l_p.mean(): 0.12822122871875763 
model_pd.l_d.mean(): -19.963485717773438 
model_pd.lagr.mean(): -19.835264205932617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4851], device='cuda:0')), ('power', tensor([-20.8392], device='cuda:0'))])
epoch£º465	 i:0 	 global-step:9300	 l-p:0.12822122871875763
epoch£º465	 i:1 	 global-step:9301	 l-p:0.25198468565940857
epoch£º465	 i:2 	 global-step:9302	 l-p:0.12264122068881989
epoch£º465	 i:3 	 global-step:9303	 l-p:0.1325642168521881
epoch£º465	 i:4 	 global-step:9304	 l-p:0.11956290900707245
epoch£º465	 i:5 	 global-step:9305	 l-p:0.12817899882793427
epoch£º465	 i:6 	 global-step:9306	 l-p:0.13396236300468445
epoch£º465	 i:7 	 global-step:9307	 l-p:0.1556507796049118
epoch£º465	 i:8 	 global-step:9308	 l-p:0.14165008068084717
epoch£º465	 i:9 	 global-step:9309	 l-p:0.09447459876537323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:466
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0161, 4.9301, 4.9202],
        [5.0161, 4.9253, 4.8825],
        [5.0161, 4.9999, 5.0130],
        [5.0161, 5.0048, 4.8537]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:466, step:0 
model_pd.l_p.mean(): 0.13730376958847046 
model_pd.l_d.mean(): -18.829853057861328 
model_pd.lagr.mean(): -18.692548751831055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5442], device='cuda:0')), ('power', tensor([-19.7456], device='cuda:0'))])
epoch£º466	 i:0 	 global-step:9320	 l-p:0.13730376958847046
epoch£º466	 i:1 	 global-step:9321	 l-p:0.12493148446083069
epoch£º466	 i:2 	 global-step:9322	 l-p:0.13656912744045258
epoch£º466	 i:3 	 global-step:9323	 l-p:0.12929068505764008
epoch£º466	 i:4 	 global-step:9324	 l-p:0.5330639481544495
epoch£º466	 i:5 	 global-step:9325	 l-p:0.04145665466785431
epoch£º466	 i:6 	 global-step:9326	 l-p:0.11259746551513672
epoch£º466	 i:7 	 global-step:9327	 l-p:0.0762137696146965
epoch£º466	 i:8 	 global-step:9328	 l-p:0.12453249841928482
epoch£º466	 i:9 	 global-step:9329	 l-p:0.11047933250665665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:467
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0638, 5.0637, 5.0638],
        [5.0638, 5.1894, 5.0502],
        [5.0638, 5.0397, 5.0574],
        [5.0638, 4.9762, 4.9478]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:467, step:0 
model_pd.l_p.mean(): 0.12484296411275864 
model_pd.l_d.mean(): -20.166099548339844 
model_pd.lagr.mean(): -20.041255950927734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4645], device='cuda:0')), ('power', tensor([-21.0243], device='cuda:0'))])
epoch£º467	 i:0 	 global-step:9340	 l-p:0.12484296411275864
epoch£º467	 i:1 	 global-step:9341	 l-p:0.1827094852924347
epoch£º467	 i:2 	 global-step:9342	 l-p:0.28428909182548523
epoch£º467	 i:3 	 global-step:9343	 l-p:0.12057742476463318
epoch£º467	 i:4 	 global-step:9344	 l-p:0.14298924803733826
epoch£º467	 i:5 	 global-step:9345	 l-p:0.49319326877593994
epoch£º467	 i:6 	 global-step:9346	 l-p:0.046790894120931625
epoch£º467	 i:7 	 global-step:9347	 l-p:0.044869255274534225
epoch£º467	 i:8 	 global-step:9348	 l-p:0.11636929959058762
epoch£º467	 i:9 	 global-step:9349	 l-p:0.11352375894784927
====================================================================================================
====================================================================================================
====================================================================================================

epoch:468
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1929, 5.1589, 5.1799],
        [5.1929, 5.3508, 5.2223],
        [5.1929, 5.1391, 5.1596],
        [5.1929, 5.8690, 6.0790]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:468, step:0 
model_pd.l_p.mean(): 0.38913169503211975 
model_pd.l_d.mean(): -20.3561954498291 
model_pd.lagr.mean(): -19.967063903808594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4155], device='cuda:0')), ('power', tensor([-21.1672], device='cuda:0'))])
epoch£º468	 i:0 	 global-step:9360	 l-p:0.38913169503211975
epoch£º468	 i:1 	 global-step:9361	 l-p:0.10627862066030502
epoch£º468	 i:2 	 global-step:9362	 l-p:0.12625907361507416
epoch£º468	 i:3 	 global-step:9363	 l-p:0.20456324517726898
epoch£º468	 i:4 	 global-step:9364	 l-p:0.1449894905090332
epoch£º468	 i:5 	 global-step:9365	 l-p:0.1437535583972931
epoch£º468	 i:6 	 global-step:9366	 l-p:0.12943792343139648
epoch£º468	 i:7 	 global-step:9367	 l-p:0.11124115437269211
epoch£º468	 i:8 	 global-step:9368	 l-p:-0.15811944007873535
epoch£º468	 i:9 	 global-step:9369	 l-p:0.11321454495191574
====================================================================================================
====================================================================================================
====================================================================================================

epoch:469
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1462, 5.1026, 5.1255],
        [5.1462, 5.1462, 5.1462],
        [5.1462, 5.1432, 5.1460],
        [5.1462, 5.1462, 5.1462]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:469, step:0 
model_pd.l_p.mean(): 0.09368109703063965 
model_pd.l_d.mean(): -20.31185531616211 
model_pd.lagr.mean(): -20.21817398071289 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4109], device='cuda:0')), ('power', tensor([-21.1172], device='cuda:0'))])
epoch£º469	 i:0 	 global-step:9380	 l-p:0.09368109703063965
epoch£º469	 i:1 	 global-step:9381	 l-p:0.08608616888523102
epoch£º469	 i:2 	 global-step:9382	 l-p:0.1291331946849823
epoch£º469	 i:3 	 global-step:9383	 l-p:0.13539837300777435
epoch£º469	 i:4 	 global-step:9384	 l-p:-0.0014862680109217763
epoch£º469	 i:5 	 global-step:9385	 l-p:0.8173947930335999
epoch£º469	 i:6 	 global-step:9386	 l-p:0.18865813314914703
epoch£º469	 i:7 	 global-step:9387	 l-p:0.1388571709394455
epoch£º469	 i:8 	 global-step:9388	 l-p:0.1079028844833374
epoch£º469	 i:9 	 global-step:9389	 l-p:0.1170315071940422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:470
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0429, 5.0426, 5.0428],
        [5.0429, 4.9553, 4.9412],
        [5.0429, 4.9868, 5.0102],
        [5.0429, 5.0427, 5.0428]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:470, step:0 
model_pd.l_p.mean(): 0.12230890244245529 
model_pd.l_d.mean(): -20.429719924926758 
model_pd.lagr.mean(): -20.307411193847656 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4388], device='cuda:0')), ('power', tensor([-21.2662], device='cuda:0'))])
epoch£º470	 i:0 	 global-step:9400	 l-p:0.12230890244245529
epoch£º470	 i:1 	 global-step:9401	 l-p:0.10474647581577301
epoch£º470	 i:2 	 global-step:9402	 l-p:0.1336723268032074
epoch£º470	 i:3 	 global-step:9403	 l-p:0.06787717342376709
epoch£º470	 i:4 	 global-step:9404	 l-p:0.15315541625022888
epoch£º470	 i:5 	 global-step:9405	 l-p:0.09425816684961319
epoch£º470	 i:6 	 global-step:9406	 l-p:0.16599449515342712
epoch£º470	 i:7 	 global-step:9407	 l-p:0.15203118324279785
epoch£º470	 i:8 	 global-step:9408	 l-p:0.09169267117977142
epoch£º470	 i:9 	 global-step:9409	 l-p:-0.3859333097934723
====================================================================================================
====================================================================================================
====================================================================================================

epoch:471
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0147, 5.0147, 5.0147],
        [5.0147, 5.0147, 5.0147],
        [5.0147, 5.0147, 5.0147],
        [5.0147, 5.0147, 5.0147]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:471, step:0 
model_pd.l_p.mean(): 0.593589186668396 
model_pd.l_d.mean(): -19.907297134399414 
model_pd.lagr.mean(): -19.31370735168457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5131], device='cuda:0')), ('power', tensor([-20.8110], device='cuda:0'))])
epoch£º471	 i:0 	 global-step:9420	 l-p:0.593589186668396
epoch£º471	 i:1 	 global-step:9421	 l-p:0.14299067854881287
epoch£º471	 i:2 	 global-step:9422	 l-p:-0.485066682100296
epoch£º471	 i:3 	 global-step:9423	 l-p:0.13928481936454773
epoch£º471	 i:4 	 global-step:9424	 l-p:-0.05966978892683983
epoch£º471	 i:5 	 global-step:9425	 l-p:0.13871945440769196
epoch£º471	 i:6 	 global-step:9426	 l-p:0.12710025906562805
epoch£º471	 i:7 	 global-step:9427	 l-p:0.13325946033000946
epoch£º471	 i:8 	 global-step:9428	 l-p:0.07010848075151443
epoch£º471	 i:9 	 global-step:9429	 l-p:0.13759157061576843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:472
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0779, 5.0068, 4.9012],
        [5.0779, 5.0565, 5.0727],
        [5.0779, 5.4175, 5.3840],
        [5.0779, 5.0779, 5.0779]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:472, step:0 
model_pd.l_p.mean(): 0.12855039536952972 
model_pd.l_d.mean(): -20.456459045410156 
model_pd.lagr.mean(): -20.327909469604492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4312], device='cuda:0')), ('power', tensor([-21.2856], device='cuda:0'))])
epoch£º472	 i:0 	 global-step:9440	 l-p:0.12855039536952972
epoch£º472	 i:1 	 global-step:9441	 l-p:0.34008410573005676
epoch£º472	 i:2 	 global-step:9442	 l-p:0.12705157697200775
epoch£º472	 i:3 	 global-step:9443	 l-p:0.13727450370788574
epoch£º472	 i:4 	 global-step:9444	 l-p:0.14335809648036957
epoch£º472	 i:5 	 global-step:9445	 l-p:0.12302667647600174
epoch£º472	 i:6 	 global-step:9446	 l-p:0.08444691449403763
epoch£º472	 i:7 	 global-step:9447	 l-p:0.11429688334465027
epoch£º472	 i:8 	 global-step:9448	 l-p:0.1965065896511078
epoch£º472	 i:9 	 global-step:9449	 l-p:0.12597686052322388
====================================================================================================
====================================================================================================
====================================================================================================

epoch:473
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9282, 4.8278, 4.7766],
        [4.9282, 4.9281, 4.9282],
        [4.9282, 4.9281, 4.9282],
        [4.9282, 4.9045, 4.9223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:473, step:0 
model_pd.l_p.mean(): 0.1251060962677002 
model_pd.l_d.mean(): -19.523305892944336 
model_pd.lagr.mean(): -19.3981990814209 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5991], device='cuda:0')), ('power', tensor([-20.5089], device='cuda:0'))])
epoch£º473	 i:0 	 global-step:9460	 l-p:0.1251060962677002
epoch£º473	 i:1 	 global-step:9461	 l-p:0.12683038413524628
epoch£º473	 i:2 	 global-step:9462	 l-p:0.18689404428005219
epoch£º473	 i:3 	 global-step:9463	 l-p:0.11728352308273315
epoch£º473	 i:4 	 global-step:9464	 l-p:0.13380120694637299
epoch£º473	 i:5 	 global-step:9465	 l-p:0.1488252878189087
epoch£º473	 i:6 	 global-step:9466	 l-p:0.17445050179958344
epoch£º473	 i:7 	 global-step:9467	 l-p:0.14809846878051758
epoch£º473	 i:8 	 global-step:9468	 l-p:0.11637739092111588
epoch£º473	 i:9 	 global-step:9469	 l-p:0.15662261843681335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:474
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9084, 4.9062, 4.9083],
        [4.9084, 4.9084, 4.9084],
        [4.9084, 4.9084, 4.9084],
        [4.9084, 4.8426, 4.8668]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:474, step:0 
model_pd.l_p.mean(): 0.12713439762592316 
model_pd.l_d.mean(): -20.112201690673828 
model_pd.lagr.mean(): -19.98506736755371 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5261], device='cuda:0')), ('power', tensor([-21.0332], device='cuda:0'))])
epoch£º474	 i:0 	 global-step:9480	 l-p:0.12713439762592316
epoch£º474	 i:1 	 global-step:9481	 l-p:0.15019984543323517
epoch£º474	 i:2 	 global-step:9482	 l-p:0.1520760953426361
epoch£º474	 i:3 	 global-step:9483	 l-p:0.16518668830394745
epoch£º474	 i:4 	 global-step:9484	 l-p:0.14922188222408295
epoch£º474	 i:5 	 global-step:9485	 l-p:0.12170462310314178
epoch£º474	 i:6 	 global-step:9486	 l-p:0.12848138809204102
epoch£º474	 i:7 	 global-step:9487	 l-p:0.17286555469036102
epoch£º474	 i:8 	 global-step:9488	 l-p:0.1108873039484024
epoch£º474	 i:9 	 global-step:9489	 l-p:0.015388836152851582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:475
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0045, 4.9274, 4.9409],
        [5.0045, 4.9125, 4.8986],
        [5.0045, 5.0045, 5.0045],
        [5.0045, 5.0045, 5.0045]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:475, step:0 
model_pd.l_p.mean(): 0.14188054203987122 
model_pd.l_d.mean(): -19.889490127563477 
model_pd.lagr.mean(): -19.747610092163086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5238], device='cuda:0')), ('power', tensor([-20.8039], device='cuda:0'))])
epoch£º475	 i:0 	 global-step:9500	 l-p:0.14188054203987122
epoch£º475	 i:1 	 global-step:9501	 l-p:0.15104889869689941
epoch£º475	 i:2 	 global-step:9502	 l-p:0.1366923749446869
epoch£º475	 i:3 	 global-step:9503	 l-p:0.09225688874721527
epoch£º475	 i:4 	 global-step:9504	 l-p:0.10439927875995636
epoch£º475	 i:5 	 global-step:9505	 l-p:0.18171584606170654
epoch£º475	 i:6 	 global-step:9506	 l-p:0.1492842584848404
epoch£º475	 i:7 	 global-step:9507	 l-p:0.19187785685062408
epoch£º475	 i:8 	 global-step:9508	 l-p:0.28786501288414
epoch£º475	 i:9 	 global-step:9509	 l-p:0.0984797552227974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:476
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1611, 5.5399, 5.5269],
        [5.1611, 5.1611, 5.1611],
        [5.1611, 5.4716, 5.4159],
        [5.1611, 5.1102, 5.1334]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:476, step:0 
model_pd.l_p.mean(): 0.11992625892162323 
model_pd.l_d.mean(): -20.294437408447266 
model_pd.lagr.mean(): -20.174510955810547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4232], device='cuda:0')), ('power', tensor([-21.1122], device='cuda:0'))])
epoch£º476	 i:0 	 global-step:9520	 l-p:0.11992625892162323
epoch£º476	 i:1 	 global-step:9521	 l-p:0.12833242118358612
epoch£º476	 i:2 	 global-step:9522	 l-p:0.21053220331668854
epoch£º476	 i:3 	 global-step:9523	 l-p:0.13367655873298645
epoch£º476	 i:4 	 global-step:9524	 l-p:0.1477770209312439
epoch£º476	 i:5 	 global-step:9525	 l-p:0.13701145350933075
epoch£º476	 i:6 	 global-step:9526	 l-p:-0.17631080746650696
epoch£º476	 i:7 	 global-step:9527	 l-p:0.12221106141805649
epoch£º476	 i:8 	 global-step:9528	 l-p:0.06835121661424637
epoch£º476	 i:9 	 global-step:9529	 l-p:0.12424922734498978
====================================================================================================
====================================================================================================
====================================================================================================

epoch:477
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1220, 5.1210, 5.1219],
        [5.1220, 5.2191, 5.0693],
        [5.1220, 5.0768, 5.1009],
        [5.1220, 5.0329, 4.9819]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:477, step:0 
model_pd.l_p.mean(): 0.12896029651165009 
model_pd.l_d.mean(): -19.183799743652344 
model_pd.lagr.mean(): -19.054840087890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5023], device='cuda:0')), ('power', tensor([-20.0628], device='cuda:0'))])
epoch£º477	 i:0 	 global-step:9540	 l-p:0.12896029651165009
epoch£º477	 i:1 	 global-step:9541	 l-p:0.1347246766090393
epoch£º477	 i:2 	 global-step:9542	 l-p:0.11035756021738052
epoch£º477	 i:3 	 global-step:9543	 l-p:0.13521309196949005
epoch£º477	 i:4 	 global-step:9544	 l-p:0.11183992028236389
epoch£º477	 i:5 	 global-step:9545	 l-p:0.038521669805049896
epoch£º477	 i:6 	 global-step:9546	 l-p:-0.035092972218990326
epoch£º477	 i:7 	 global-step:9547	 l-p:0.1320326030254364
epoch£º477	 i:8 	 global-step:9548	 l-p:0.1602475494146347
epoch£º477	 i:9 	 global-step:9549	 l-p:0.16240978240966797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:478
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9457, 4.9457, 4.9457],
        [4.9457, 5.3453, 5.3578],
        [4.9457, 4.9457, 4.9457],
        [4.9457, 4.8623, 4.7442]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:478, step:0 
model_pd.l_p.mean(): 0.09415269643068314 
model_pd.l_d.mean(): -18.607210159301758 
model_pd.lagr.mean(): -18.513057708740234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5951], device='cuda:0')), ('power', tensor([-19.5715], device='cuda:0'))])
epoch£º478	 i:0 	 global-step:9560	 l-p:0.09415269643068314
epoch£º478	 i:1 	 global-step:9561	 l-p:0.16659456491470337
epoch£º478	 i:2 	 global-step:9562	 l-p:0.11590465903282166
epoch£º478	 i:3 	 global-step:9563	 l-p:0.190337672829628
epoch£º478	 i:4 	 global-step:9564	 l-p:0.13471871614456177
epoch£º478	 i:5 	 global-step:9565	 l-p:0.13995592296123505
epoch£º478	 i:6 	 global-step:9566	 l-p:0.13484257459640503
epoch£º478	 i:7 	 global-step:9567	 l-p:0.16086846590042114
epoch£º478	 i:8 	 global-step:9568	 l-p:0.1290849894285202
epoch£º478	 i:9 	 global-step:9569	 l-p:0.15389381349086761
====================================================================================================
====================================================================================================
====================================================================================================

epoch:479
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9040, 5.2805, 5.2792],
        [4.9040, 4.8865, 4.9007],
        [4.9040, 4.8146, 4.6968],
        [4.9040, 4.8292, 4.8510]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:479, step:0 
model_pd.l_p.mean(): 0.12355689704418182 
model_pd.l_d.mean(): -19.73545265197754 
model_pd.lagr.mean(): -19.611896514892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5541], device='cuda:0')), ('power', tensor([-20.6784], device='cuda:0'))])
epoch£º479	 i:0 	 global-step:9580	 l-p:0.12355689704418182
epoch£º479	 i:1 	 global-step:9581	 l-p:0.1226554661989212
epoch£º479	 i:2 	 global-step:9582	 l-p:0.1546904295682907
epoch£º479	 i:3 	 global-step:9583	 l-p:0.10101800411939621
epoch£º479	 i:4 	 global-step:9584	 l-p:0.15535277128219604
epoch£º479	 i:5 	 global-step:9585	 l-p:0.18855981528759003
epoch£º479	 i:6 	 global-step:9586	 l-p:0.1684488207101822
epoch£º479	 i:7 	 global-step:9587	 l-p:0.19748157262802124
epoch£º479	 i:8 	 global-step:9588	 l-p:-0.221986323595047
epoch£º479	 i:9 	 global-step:9589	 l-p:0.21645472943782806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:480
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8774, 4.8774, 4.8774],
        [4.8774, 4.7774, 4.7704],
        [4.8774, 4.8774, 4.8774],
        [4.8774, 4.8116, 4.8379]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:480, step:0 
model_pd.l_p.mean(): 0.12576988339424133 
model_pd.l_d.mean(): -19.19596290588379 
model_pd.lagr.mean(): -19.070192337036133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5427], device='cuda:0')), ('power', tensor([-20.1170], device='cuda:0'))])
epoch£º480	 i:0 	 global-step:9600	 l-p:0.12576988339424133
epoch£º480	 i:1 	 global-step:9601	 l-p:0.13490773737430573
epoch£º480	 i:2 	 global-step:9602	 l-p:0.16711227595806122
epoch£º480	 i:3 	 global-step:9603	 l-p:0.14906801283359528
epoch£º480	 i:4 	 global-step:9604	 l-p:0.07682839035987854
epoch£º480	 i:5 	 global-step:9605	 l-p:0.17215172946453094
epoch£º480	 i:6 	 global-step:9606	 l-p:0.15154390037059784
epoch£º480	 i:7 	 global-step:9607	 l-p:0.14426974952220917
epoch£º480	 i:8 	 global-step:9608	 l-p:0.10888651758432388
epoch£º480	 i:9 	 global-step:9609	 l-p:0.0990031510591507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:481
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0373, 5.0014, 5.0246],
        [5.0373, 5.0373, 5.0373],
        [5.0373, 5.0329, 5.0369],
        [5.0373, 5.0372, 5.0373]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:481, step:0 
model_pd.l_p.mean(): 0.13674859702587128 
model_pd.l_d.mean(): -20.660802841186523 
model_pd.lagr.mean(): -20.5240535736084 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4281], device='cuda:0')), ('power', tensor([-21.4905], device='cuda:0'))])
epoch£º481	 i:0 	 global-step:9620	 l-p:0.13674859702587128
epoch£º481	 i:1 	 global-step:9621	 l-p:0.13711829483509064
epoch£º481	 i:2 	 global-step:9622	 l-p:0.1418200433254242
epoch£º481	 i:3 	 global-step:9623	 l-p:0.1637454628944397
epoch£º481	 i:4 	 global-step:9624	 l-p:-0.020217742770910263
epoch£º481	 i:5 	 global-step:9625	 l-p:0.14206410944461823
epoch£º481	 i:6 	 global-step:9626	 l-p:0.14443503320217133
epoch£º481	 i:7 	 global-step:9627	 l-p:0.1362173855304718
epoch£º481	 i:8 	 global-step:9628	 l-p:0.07354666292667389
epoch£º481	 i:9 	 global-step:9629	 l-p:0.1406150758266449
====================================================================================================
====================================================================================================
====================================================================================================

epoch:482
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9704, 4.9468, 4.9647],
        [4.9704, 4.9191, 4.9458],
        [4.9704, 4.8752, 4.7791],
        [4.9704, 5.0281, 4.8651]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:482, step:0 
model_pd.l_p.mean(): 0.08374693989753723 
model_pd.l_d.mean(): -20.744298934936523 
model_pd.lagr.mean(): -20.660551071166992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4302], device='cuda:0')), ('power', tensor([-21.5778], device='cuda:0'))])
epoch£º482	 i:0 	 global-step:9640	 l-p:0.08374693989753723
epoch£º482	 i:1 	 global-step:9641	 l-p:0.11348507553339005
epoch£º482	 i:2 	 global-step:9642	 l-p:0.09880973398685455
epoch£º482	 i:3 	 global-step:9643	 l-p:0.08778399974107742
epoch£º482	 i:4 	 global-step:9644	 l-p:0.17681021988391876
epoch£º482	 i:5 	 global-step:9645	 l-p:0.1168566644191742
epoch£º482	 i:6 	 global-step:9646	 l-p:0.1392594277858734
epoch£º482	 i:7 	 global-step:9647	 l-p:0.12808012962341309
epoch£º482	 i:8 	 global-step:9648	 l-p:0.12151452898979187
epoch£º482	 i:9 	 global-step:9649	 l-p:0.08694994449615479
====================================================================================================
====================================================================================================
====================================================================================================

epoch:483
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0964, 5.0963, 5.0964],
        [5.0964, 5.1780, 5.0219],
        [5.0964, 5.0959, 5.0963],
        [5.0964, 5.0964, 5.0964]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:483, step:0 
model_pd.l_p.mean(): -0.2815130650997162 
model_pd.l_d.mean(): -18.5760498046875 
model_pd.lagr.mean(): -18.857563018798828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5545], device='cuda:0')), ('power', tensor([-19.4978], device='cuda:0'))])
epoch£º483	 i:0 	 global-step:9660	 l-p:-0.2815130650997162
epoch£º483	 i:1 	 global-step:9661	 l-p:0.1953994184732437
epoch£º483	 i:2 	 global-step:9662	 l-p:0.14862540364265442
epoch£º483	 i:3 	 global-step:9663	 l-p:0.11972621828317642
epoch£º483	 i:4 	 global-step:9664	 l-p:0.13472558557987213
epoch£º483	 i:5 	 global-step:9665	 l-p:-0.027982236817479134
epoch£º483	 i:6 	 global-step:9666	 l-p:0.12072768807411194
epoch£º483	 i:7 	 global-step:9667	 l-p:0.1076185405254364
epoch£º483	 i:8 	 global-step:9668	 l-p:0.14828957617282867
epoch£º483	 i:9 	 global-step:9669	 l-p:0.10778472572565079
====================================================================================================
====================================================================================================
====================================================================================================

epoch:484
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3005,  0.2013,  1.0000,  0.1348,
          1.0000,  0.6698, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228]], device='cuda:0')
 pt:tensor([[5.1093, 5.3299, 5.2247],
        [5.1093, 5.0242, 4.9331],
        [5.1093, 5.0643, 4.9229],
        [5.1093, 5.3554, 5.2632]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:484, step:0 
model_pd.l_p.mean(): 0.12819765508174896 
model_pd.l_d.mean(): -19.479108810424805 
model_pd.lagr.mean(): -19.35091209411621 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5406], device='cuda:0')), ('power', tensor([-20.4032], device='cuda:0'))])
epoch£º484	 i:0 	 global-step:9680	 l-p:0.12819765508174896
epoch£º484	 i:1 	 global-step:9681	 l-p:0.13842640817165375
epoch£º484	 i:2 	 global-step:9682	 l-p:0.19997525215148926
epoch£º484	 i:3 	 global-step:9683	 l-p:0.022163791581988335
epoch£º484	 i:4 	 global-step:9684	 l-p:0.06725215166807175
epoch£º484	 i:5 	 global-step:9685	 l-p:0.6706061959266663
epoch£º484	 i:6 	 global-step:9686	 l-p:0.13159619271755219
epoch£º484	 i:7 	 global-step:9687	 l-p:0.12746098637580872
epoch£º484	 i:8 	 global-step:9688	 l-p:0.14681506156921387
epoch£º484	 i:9 	 global-step:9689	 l-p:0.1237649917602539
====================================================================================================
====================================================================================================
====================================================================================================

epoch:485
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0516, 5.0186, 5.0408],
        [5.0516, 4.9705, 4.9816],
        [5.0516, 5.0516, 5.0516],
        [5.0516, 5.0516, 5.0516]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:485, step:0 
model_pd.l_p.mean(): 0.1293131560087204 
model_pd.l_d.mean(): -20.94562339782715 
model_pd.lagr.mean(): -20.81631088256836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3666], device='cuda:0')), ('power', tensor([-21.7170], device='cuda:0'))])
epoch£º485	 i:0 	 global-step:9700	 l-p:0.1293131560087204
epoch£º485	 i:1 	 global-step:9701	 l-p:0.07935099303722382
epoch£º485	 i:2 	 global-step:9702	 l-p:0.11876700818538666
epoch£º485	 i:3 	 global-step:9703	 l-p:0.12615717947483063
epoch£º485	 i:4 	 global-step:9704	 l-p:0.026459040120244026
epoch£º485	 i:5 	 global-step:9705	 l-p:0.12607280910015106
epoch£º485	 i:6 	 global-step:9706	 l-p:0.1604289561510086
epoch£º485	 i:7 	 global-step:9707	 l-p:0.2177945226430893
epoch£º485	 i:8 	 global-step:9708	 l-p:0.15548430383205414
epoch£º485	 i:9 	 global-step:9709	 l-p:0.2685331106185913
====================================================================================================
====================================================================================================
====================================================================================================

epoch:486
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8942, 4.8312, 4.6786],
        [4.8942, 4.7857, 4.7539],
        [4.8942, 5.0440, 4.9088],
        [4.8942, 4.8322, 4.8596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:486, step:0 
model_pd.l_p.mean(): 0.15564702451229095 
model_pd.l_d.mean(): -19.87472915649414 
model_pd.lagr.mean(): -19.71908187866211 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5614], device='cuda:0')), ('power', tensor([-20.8279], device='cuda:0'))])
epoch£º486	 i:0 	 global-step:9720	 l-p:0.15564702451229095
epoch£º486	 i:1 	 global-step:9721	 l-p:0.12154441326856613
epoch£º486	 i:2 	 global-step:9722	 l-p:0.1420350968837738
epoch£º486	 i:3 	 global-step:9723	 l-p:0.20256221294403076
epoch£º486	 i:4 	 global-step:9724	 l-p:0.12687624990940094
epoch£º486	 i:5 	 global-step:9725	 l-p:0.12202505022287369
epoch£º486	 i:6 	 global-step:9726	 l-p:0.16495555639266968
epoch£º486	 i:7 	 global-step:9727	 l-p:0.2120935320854187
epoch£º486	 i:8 	 global-step:9728	 l-p:0.09549657255411148
epoch£º486	 i:9 	 global-step:9729	 l-p:0.1309942901134491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:487
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9410, 4.8351, 4.8018],
        [4.9410, 4.9408, 4.9410],
        [4.9410, 4.9410, 4.9410],
        [4.9410, 4.9546, 4.7850]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:487, step:0 
model_pd.l_p.mean(): 0.1153164952993393 
model_pd.l_d.mean(): -19.99798583984375 
model_pd.lagr.mean(): -19.88266944885254 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5146], device='cuda:0')), ('power', tensor([-20.9050], device='cuda:0'))])
epoch£º487	 i:0 	 global-step:9740	 l-p:0.1153164952993393
epoch£º487	 i:1 	 global-step:9741	 l-p:0.17238222062587738
epoch£º487	 i:2 	 global-step:9742	 l-p:0.11058396846055984
epoch£º487	 i:3 	 global-step:9743	 l-p:-0.06603395938873291
epoch£º487	 i:4 	 global-step:9744	 l-p:0.15962260961532593
epoch£º487	 i:5 	 global-step:9745	 l-p:0.10484365373849869
epoch£º487	 i:6 	 global-step:9746	 l-p:0.13574284315109253
epoch£º487	 i:7 	 global-step:9747	 l-p:0.1407010555267334
epoch£º487	 i:8 	 global-step:9748	 l-p:0.12829871475696564
epoch£º487	 i:9 	 global-step:9749	 l-p:0.12735970318317413
====================================================================================================
====================================================================================================
====================================================================================================

epoch:488
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0945, 5.0933, 5.0945],
        [5.0945, 5.0325, 4.9004],
        [5.0945, 5.0721, 5.0892],
        [5.0945, 5.4543, 5.4298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:488, step:0 
model_pd.l_p.mean(): 0.13150820136070251 
model_pd.l_d.mean(): -20.223623275756836 
model_pd.lagr.mean(): -20.09211540222168 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4620], device='cuda:0')), ('power', tensor([-21.0803], device='cuda:0'))])
epoch£º488	 i:0 	 global-step:9760	 l-p:0.13150820136070251
epoch£º488	 i:1 	 global-step:9761	 l-p:-0.001515779411420226
epoch£º488	 i:2 	 global-step:9762	 l-p:0.14995387196540833
epoch£º488	 i:3 	 global-step:9763	 l-p:0.10399919003248215
epoch£º488	 i:4 	 global-step:9764	 l-p:0.1461634635925293
epoch£º488	 i:5 	 global-step:9765	 l-p:0.2486187070608139
epoch£º488	 i:6 	 global-step:9766	 l-p:-0.05598270148038864
epoch£º488	 i:7 	 global-step:9767	 l-p:0.08929953724145889
epoch£º488	 i:8 	 global-step:9768	 l-p:0.10846803337335587
epoch£º488	 i:9 	 global-step:9769	 l-p:0.15109393000602722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:489
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0466, 5.0131, 5.0357],
        [5.0466, 5.0462, 5.0466],
        [5.0466, 5.0101, 5.0337],
        [5.0466, 5.0466, 5.0466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:489, step:0 
model_pd.l_p.mean(): 0.10642799735069275 
model_pd.l_d.mean(): -20.55620002746582 
model_pd.lagr.mean(): -20.449771881103516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4222], device='cuda:0')), ('power', tensor([-21.3779], device='cuda:0'))])
epoch£º489	 i:0 	 global-step:9780	 l-p:0.10642799735069275
epoch£º489	 i:1 	 global-step:9781	 l-p:0.11881799250841141
epoch£º489	 i:2 	 global-step:9782	 l-p:-0.6047577857971191
epoch£º489	 i:3 	 global-step:9783	 l-p:0.10780999064445496
epoch£º489	 i:4 	 global-step:9784	 l-p:0.15972007811069489
epoch£º489	 i:5 	 global-step:9785	 l-p:0.15633022785186768
epoch£º489	 i:6 	 global-step:9786	 l-p:0.13432933390140533
epoch£º489	 i:7 	 global-step:9787	 l-p:0.13618260622024536
epoch£º489	 i:8 	 global-step:9788	 l-p:-0.17744798958301544
epoch£º489	 i:9 	 global-step:9789	 l-p:0.12545321881771088
====================================================================================================
====================================================================================================
====================================================================================================

epoch:490
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0785, 5.0785, 5.0785],
        [5.0785, 5.0780, 5.0785],
        [5.0785, 5.0777, 5.0785],
        [5.0785, 5.0785, 5.0785]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:490, step:0 
model_pd.l_p.mean(): 0.1345367729663849 
model_pd.l_d.mean(): -20.6527099609375 
model_pd.lagr.mean(): -20.518173217773438 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4072], device='cuda:0')), ('power', tensor([-21.4606], device='cuda:0'))])
epoch£º490	 i:0 	 global-step:9800	 l-p:0.1345367729663849
epoch£º490	 i:1 	 global-step:9801	 l-p:0.10817626863718033
epoch£º490	 i:2 	 global-step:9802	 l-p:0.14847427606582642
epoch£º490	 i:3 	 global-step:9803	 l-p:0.09540750086307526
epoch£º490	 i:4 	 global-step:9804	 l-p:-0.2504826784133911
epoch£º490	 i:5 	 global-step:9805	 l-p:0.06752371788024902
epoch£º490	 i:6 	 global-step:9806	 l-p:0.0913420170545578
epoch£º490	 i:7 	 global-step:9807	 l-p:0.10257653892040253
epoch£º490	 i:8 	 global-step:9808	 l-p:0.17612357437610626
epoch£º490	 i:9 	 global-step:9809	 l-p:0.12407752871513367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:491
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1750, 5.6157, 5.6412],
        [5.1750, 5.0832, 5.0581],
        [5.1750, 5.1372, 5.1606],
        [5.1750, 5.3690, 5.2487]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:491, step:0 
model_pd.l_p.mean(): 0.21609191596508026 
model_pd.l_d.mean(): -19.258453369140625 
model_pd.lagr.mean(): -19.042362213134766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4647], device='cuda:0')), ('power', tensor([-20.0999], device='cuda:0'))])
epoch£º491	 i:0 	 global-step:9820	 l-p:0.21609191596508026
epoch£º491	 i:1 	 global-step:9821	 l-p:0.12222758680582047
epoch£º491	 i:2 	 global-step:9822	 l-p:0.12100014090538025
epoch£º491	 i:3 	 global-step:9823	 l-p:0.12730008363723755
epoch£º491	 i:4 	 global-step:9824	 l-p:0.18069270253181458
epoch£º491	 i:5 	 global-step:9825	 l-p:0.17722854018211365
epoch£º491	 i:6 	 global-step:9826	 l-p:0.12469734996557236
epoch£º491	 i:7 	 global-step:9827	 l-p:0.10011124610900879
epoch£º491	 i:8 	 global-step:9828	 l-p:0.11325681209564209
epoch£º491	 i:9 	 global-step:9829	 l-p:0.11488927900791168
====================================================================================================
====================================================================================================
====================================================================================================

epoch:492
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3286, 5.3286, 5.3286],
        [5.3286, 5.2828, 5.1583],
        [5.3286, 5.3214, 5.3278],
        [5.3286, 5.3233, 5.3282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:492, step:0 
model_pd.l_p.mean(): 0.16322506964206696 
model_pd.l_d.mean(): -20.43674659729004 
model_pd.lagr.mean(): -20.273521423339844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3727], device='cuda:0')), ('power', tensor([-21.2050], device='cuda:0'))])
epoch£º492	 i:0 	 global-step:9840	 l-p:0.16322506964206696
epoch£º492	 i:1 	 global-step:9841	 l-p:0.12006951123476028
epoch£º492	 i:2 	 global-step:9842	 l-p:0.06399921327829361
epoch£º492	 i:3 	 global-step:9843	 l-p:0.15625424683094025
epoch£º492	 i:4 	 global-step:9844	 l-p:0.12019693106412888
epoch£º492	 i:5 	 global-step:9845	 l-p:0.1423637717962265
epoch£º492	 i:6 	 global-step:9846	 l-p:0.13079224526882172
epoch£º492	 i:7 	 global-step:9847	 l-p:0.11908869445323944
epoch£º492	 i:8 	 global-step:9848	 l-p:0.1333945244550705
epoch£º492	 i:9 	 global-step:9849	 l-p:0.13262414932250977
====================================================================================================
====================================================================================================
====================================================================================================

epoch:493
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2460, 6.0184, 6.3006],
        [5.2460, 5.2457, 5.2460],
        [5.2460, 5.1598, 5.1460],
        [5.2460, 5.8422, 5.9791]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:493, step:0 
model_pd.l_p.mean(): 0.09515262395143509 
model_pd.l_d.mean(): -17.471786499023438 
model_pd.lagr.mean(): -17.37663459777832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5956], device='cuda:0')), ('power', tensor([-18.4154], device='cuda:0'))])
epoch£º493	 i:0 	 global-step:9860	 l-p:0.09515262395143509
epoch£º493	 i:1 	 global-step:9861	 l-p:0.11177948117256165
epoch£º493	 i:2 	 global-step:9862	 l-p:0.12949231266975403
epoch£º493	 i:3 	 global-step:9863	 l-p:0.14512762427330017
epoch£º493	 i:4 	 global-step:9864	 l-p:0.12768717110157013
epoch£º493	 i:5 	 global-step:9865	 l-p:-0.3242468237876892
epoch£º493	 i:6 	 global-step:9866	 l-p:0.11434286087751389
epoch£º493	 i:7 	 global-step:9867	 l-p:0.13303132355213165
epoch£º493	 i:8 	 global-step:9868	 l-p:0.4632144570350647
epoch£º493	 i:9 	 global-step:9869	 l-p:0.1719628870487213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:494
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1086, 5.0357, 4.9103],
        [5.1086, 5.7580, 5.9469],
        [5.1086, 5.0266, 5.0378],
        [5.1086, 5.0908, 5.1051]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:494, step:0 
model_pd.l_p.mean(): 0.11747513711452484 
model_pd.l_d.mean(): -18.08064842224121 
model_pd.lagr.mean(): -17.963172912597656 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5986], device='cuda:0')), ('power', tensor([-19.0387], device='cuda:0'))])
epoch£º494	 i:0 	 global-step:9880	 l-p:0.11747513711452484
epoch£º494	 i:1 	 global-step:9881	 l-p:0.14685025811195374
epoch£º494	 i:2 	 global-step:9882	 l-p:-0.8652485609054565
epoch£º494	 i:3 	 global-step:9883	 l-p:0.07767353951931
epoch£º494	 i:4 	 global-step:9884	 l-p:0.1396014541387558
epoch£º494	 i:5 	 global-step:9885	 l-p:0.18280673027038574
epoch£º494	 i:6 	 global-step:9886	 l-p:0.056174080818891525
epoch£º494	 i:7 	 global-step:9887	 l-p:0.13596758246421814
epoch£º494	 i:8 	 global-step:9888	 l-p:0.024172615259885788
epoch£º494	 i:9 	 global-step:9889	 l-p:0.14109660685062408
====================================================================================================
====================================================================================================
====================================================================================================

epoch:495
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1803, 5.0918, 5.0856],
        [5.1803, 5.1798, 5.1803],
        [5.1803, 5.5157, 5.4704],
        [5.1803, 5.1691, 5.1787]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:495, step:0 
model_pd.l_p.mean(): 0.12763234972953796 
model_pd.l_d.mean(): -20.255298614501953 
model_pd.lagr.mean(): -20.127666473388672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4196], device='cuda:0')), ('power', tensor([-21.0687], device='cuda:0'))])
epoch£º495	 i:0 	 global-step:9900	 l-p:0.12763234972953796
epoch£º495	 i:1 	 global-step:9901	 l-p:0.12413529306650162
epoch£º495	 i:2 	 global-step:9902	 l-p:0.13307799398899078
epoch£º495	 i:3 	 global-step:9903	 l-p:0.12379272282123566
epoch£º495	 i:4 	 global-step:9904	 l-p:0.26558977365493774
epoch£º495	 i:5 	 global-step:9905	 l-p:0.08342918008565903
epoch£º495	 i:6 	 global-step:9906	 l-p:0.2022581398487091
epoch£º495	 i:7 	 global-step:9907	 l-p:0.12104684859514236
epoch£º495	 i:8 	 global-step:9908	 l-p:0.13679613173007965
epoch£º495	 i:9 	 global-step:9909	 l-p:0.109275683760643
====================================================================================================
====================================================================================================
====================================================================================================

epoch:496
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0906, 5.0906, 5.0906],
        [5.0906, 5.0290, 5.0546],
        [5.0906, 5.0906, 5.0906],
        [5.0906, 5.4008, 5.3433]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:496, step:0 
model_pd.l_p.mean(): 0.13400273025035858 
model_pd.l_d.mean(): -20.41704559326172 
model_pd.lagr.mean(): -20.283042907714844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4334], device='cuda:0')), ('power', tensor([-21.2477], device='cuda:0'))])
epoch£º496	 i:0 	 global-step:9920	 l-p:0.13400273025035858
epoch£º496	 i:1 	 global-step:9921	 l-p:0.1242574006319046
epoch£º496	 i:2 	 global-step:9922	 l-p:0.09117972105741501
epoch£º496	 i:3 	 global-step:9923	 l-p:0.5878370404243469
epoch£º496	 i:4 	 global-step:9924	 l-p:0.07643340528011322
epoch£º496	 i:5 	 global-step:9925	 l-p:0.12285478413105011
epoch£º496	 i:6 	 global-step:9926	 l-p:0.13165457546710968
epoch£º496	 i:7 	 global-step:9927	 l-p:0.09466985613107681
epoch£º496	 i:8 	 global-step:9928	 l-p:0.1983242630958557
epoch£º496	 i:9 	 global-step:9929	 l-p:0.13147786259651184
====================================================================================================
====================================================================================================
====================================================================================================

epoch:497
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9852, 4.8863, 4.7773],
        [4.9852, 4.9852, 4.9852],
        [4.9852, 4.9852, 4.9852],
        [4.9852, 4.9835, 4.9851]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:497, step:0 
model_pd.l_p.mean(): 0.14873048663139343 
model_pd.l_d.mean(): -20.16493797302246 
model_pd.lagr.mean(): -20.016206741333008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5058], device='cuda:0')), ('power', tensor([-21.0658], device='cuda:0'))])
epoch£º497	 i:0 	 global-step:9940	 l-p:0.14873048663139343
epoch£º497	 i:1 	 global-step:9941	 l-p:0.07646140456199646
epoch£º497	 i:2 	 global-step:9942	 l-p:0.11695369333028793
epoch£º497	 i:3 	 global-step:9943	 l-p:0.11687339842319489
epoch£º497	 i:4 	 global-step:9944	 l-p:0.13096395134925842
epoch£º497	 i:5 	 global-step:9945	 l-p:0.17750360071659088
epoch£º497	 i:6 	 global-step:9946	 l-p:0.15091197192668915
epoch£º497	 i:7 	 global-step:9947	 l-p:0.14884032309055328
epoch£º497	 i:8 	 global-step:9948	 l-p:0.11068187654018402
epoch£º497	 i:9 	 global-step:9949	 l-p:0.9411529302597046
====================================================================================================
====================================================================================================
====================================================================================================

epoch:498
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8725, 4.8235, 4.8521],
        [4.8725, 5.2151, 5.1888],
        [4.8725, 4.9791, 4.8229],
        [4.8725, 4.8645, 4.8717]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:498, step:0 
model_pd.l_p.mean(): 0.20889994502067566 
model_pd.l_d.mean(): -20.2501277923584 
model_pd.lagr.mean(): -20.041227340698242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5230], device='cuda:0')), ('power', tensor([-21.1705], device='cuda:0'))])
epoch£º498	 i:0 	 global-step:9960	 l-p:0.20889994502067566
epoch£º498	 i:1 	 global-step:9961	 l-p:0.17335744202136993
epoch£º498	 i:2 	 global-step:9962	 l-p:0.12575119733810425
epoch£º498	 i:3 	 global-step:9963	 l-p:0.1821700483560562
epoch£º498	 i:4 	 global-step:9964	 l-p:0.20302730798721313
epoch£º498	 i:5 	 global-step:9965	 l-p:0.13838675618171692
epoch£º498	 i:6 	 global-step:9966	 l-p:0.17265084385871887
epoch£º498	 i:7 	 global-step:9967	 l-p:0.12958025932312012
epoch£º498	 i:8 	 global-step:9968	 l-p:0.09116151928901672
epoch£º498	 i:9 	 global-step:9969	 l-p:0.11023843288421631
====================================================================================================
====================================================================================================
====================================================================================================

epoch:499
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0357, 5.0349, 5.0357],
        [5.0357, 5.3956, 5.3717],
        [5.0357, 5.0357, 5.0357],
        [5.0357, 5.0357, 5.0357]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:499, step:0 
model_pd.l_p.mean(): 0.1577787697315216 
model_pd.l_d.mean(): -20.82790756225586 
model_pd.lagr.mean(): -20.670127868652344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3921], device='cuda:0')), ('power', tensor([-21.6235], device='cuda:0'))])
epoch£º499	 i:0 	 global-step:9980	 l-p:0.1577787697315216
epoch£º499	 i:1 	 global-step:9981	 l-p:0.10540340095758438
epoch£º499	 i:2 	 global-step:9982	 l-p:0.07475005835294724
epoch£º499	 i:3 	 global-step:9983	 l-p:0.14011994004249573
epoch£º499	 i:4 	 global-step:9984	 l-p:0.23694026470184326
epoch£º499	 i:5 	 global-step:9985	 l-p:0.06067353114485741
epoch£º499	 i:6 	 global-step:9986	 l-p:0.11981508880853653
epoch£º499	 i:7 	 global-step:9987	 l-p:0.04247354716062546
epoch£º499	 i:8 	 global-step:9988	 l-p:0.1270165741443634
epoch£º499	 i:9 	 global-step:9989	 l-p:0.12071114033460617
====================================================================================================
====================================================================================================
====================================================================================================

epoch:500
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2051, 5.1301, 5.1459],
        [5.2051, 5.1256, 5.0182],
        [5.2051, 5.1611, 5.1862],
        [5.2051, 5.3209, 5.1691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:500, step:0 
model_pd.l_p.mean(): -0.455270379781723 
model_pd.l_d.mean(): -20.60497283935547 
model_pd.lagr.mean(): -21.060243606567383 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3855], device='cuda:0')), ('power', tensor([-21.3895], device='cuda:0'))])
epoch£º500	 i:0 	 global-step:10000	 l-p:-0.455270379781723
epoch£º500	 i:1 	 global-step:10001	 l-p:0.12231750041246414
epoch£º500	 i:2 	 global-step:10002	 l-p:0.37019696831703186
epoch£º500	 i:3 	 global-step:10003	 l-p:0.11194940656423569
epoch£º500	 i:4 	 global-step:10004	 l-p:0.11126992851495743
epoch£º500	 i:5 	 global-step:10005	 l-p:0.11773908138275146
epoch£º500	 i:6 	 global-step:10006	 l-p:0.11367563158273697
epoch£º500	 i:7 	 global-step:10007	 l-p:0.13292621076107025
epoch£º500	 i:8 	 global-step:10008	 l-p:0.3919239640235901
epoch£º500	 i:9 	 global-step:10009	 l-p:0.12798891961574554
====================================================================================================
====================================================================================================
====================================================================================================

epoch:501
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1839, 5.0955, 5.0953],
        [5.1839, 5.1055, 4.9909],
        [5.1839, 5.1221, 4.9874],
        [5.1839, 5.1577, 5.1770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:501, step:0 
model_pd.l_p.mean(): -0.0364140085875988 
model_pd.l_d.mean(): -19.79290199279785 
model_pd.lagr.mean(): -19.829315185546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4663], device='cuda:0')), ('power', tensor([-20.6460], device='cuda:0'))])
epoch£º501	 i:0 	 global-step:10020	 l-p:-0.0364140085875988
epoch£º501	 i:1 	 global-step:10021	 l-p:0.146888867020607
epoch£º501	 i:2 	 global-step:10022	 l-p:0.07109137624502182
epoch£º501	 i:3 	 global-step:10023	 l-p:0.10621028393507004
epoch£º501	 i:4 	 global-step:10024	 l-p:0.13236473500728607
epoch£º501	 i:5 	 global-step:10025	 l-p:0.09344606101512909
epoch£º501	 i:6 	 global-step:10026	 l-p:0.11203347891569138
epoch£º501	 i:7 	 global-step:10027	 l-p:0.1293613463640213
epoch£º501	 i:8 	 global-step:10028	 l-p:-0.029419174417853355
epoch£º501	 i:9 	 global-step:10029	 l-p:0.12951874732971191
====================================================================================================
====================================================================================================
====================================================================================================

epoch:502
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0696, 4.9638, 4.8943],
        [5.0696, 5.0696, 5.0696],
        [5.0696, 5.4257, 5.3969],
        [5.0696, 5.0693, 5.0696]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:502, step:0 
model_pd.l_p.mean(): 0.1376265436410904 
model_pd.l_d.mean(): -19.437833786010742 
model_pd.lagr.mean(): -19.300207138061523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5094], device='cuda:0')), ('power', tensor([-20.3289], device='cuda:0'))])
epoch£º502	 i:0 	 global-step:10040	 l-p:0.1376265436410904
epoch£º502	 i:1 	 global-step:10041	 l-p:-0.07225324213504791
epoch£º502	 i:2 	 global-step:10042	 l-p:0.1677705943584442
epoch£º502	 i:3 	 global-step:10043	 l-p:0.14859843254089355
epoch£º502	 i:4 	 global-step:10044	 l-p:0.12312059104442596
epoch£º502	 i:5 	 global-step:10045	 l-p:0.15563806891441345
epoch£º502	 i:6 	 global-step:10046	 l-p:0.048099011182785034
epoch£º502	 i:7 	 global-step:10047	 l-p:0.1363399624824524
epoch£º502	 i:8 	 global-step:10048	 l-p:0.13516460359096527
epoch£º502	 i:9 	 global-step:10049	 l-p:0.12062696367502213
====================================================================================================
====================================================================================================
====================================================================================================

epoch:503
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9590, 4.9590, 4.9590],
        [4.9590, 5.2635, 5.2065],
        [4.9590, 4.9590, 4.9590],
        [4.9590, 4.9344, 4.9533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:503, step:0 
model_pd.l_p.mean(): 0.1590513288974762 
model_pd.l_d.mean(): -19.6605167388916 
model_pd.lagr.mean(): -19.50146484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5113], device='cuda:0')), ('power', tensor([-20.5577], device='cuda:0'))])
epoch£º503	 i:0 	 global-step:10060	 l-p:0.1590513288974762
epoch£º503	 i:1 	 global-step:10061	 l-p:0.1177358627319336
epoch£º503	 i:2 	 global-step:10062	 l-p:0.12041987478733063
epoch£º503	 i:3 	 global-step:10063	 l-p:0.1366930603981018
epoch£º503	 i:4 	 global-step:10064	 l-p:0.09851784259080887
epoch£º503	 i:5 	 global-step:10065	 l-p:0.20334939658641815
epoch£º503	 i:6 	 global-step:10066	 l-p:0.20417465269565582
epoch£º503	 i:7 	 global-step:10067	 l-p:0.33900186419487
epoch£º503	 i:8 	 global-step:10068	 l-p:0.13138437271118164
epoch£º503	 i:9 	 global-step:10069	 l-p:0.09473796933889389
====================================================================================================
====================================================================================================
====================================================================================================

epoch:504
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9073, 4.8956, 4.9057],
        [4.9073, 4.7964, 4.6818],
        [4.9073, 4.8757, 4.8984],
        [4.9073, 4.8060, 4.8130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:504, step:0 
model_pd.l_p.mean(): 0.10046455264091492 
model_pd.l_d.mean(): -20.287771224975586 
model_pd.lagr.mean(): -20.187307357788086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5192], device='cuda:0')), ('power', tensor([-21.2048], device='cuda:0'))])
epoch£º504	 i:0 	 global-step:10080	 l-p:0.10046455264091492
epoch£º504	 i:1 	 global-step:10081	 l-p:0.11527543514966965
epoch£º504	 i:2 	 global-step:10082	 l-p:0.18861770629882812
epoch£º504	 i:3 	 global-step:10083	 l-p:0.30868712067604065
epoch£º504	 i:4 	 global-step:10084	 l-p:0.16360434889793396
epoch£º504	 i:5 	 global-step:10085	 l-p:0.1277768760919571
epoch£º504	 i:6 	 global-step:10086	 l-p:0.13204032182693481
epoch£º504	 i:7 	 global-step:10087	 l-p:0.148955836892128
epoch£º504	 i:8 	 global-step:10088	 l-p:0.16176722943782806
epoch£º504	 i:9 	 global-step:10089	 l-p:0.1560170203447342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:505
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9671, 4.8575, 4.8426],
        [4.9671, 4.9155, 4.9443],
        [4.9671, 4.9625, 4.9668],
        [4.9671, 4.8974, 4.9253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:505, step:0 
model_pd.l_p.mean(): 0.18837806582450867 
model_pd.l_d.mean(): -20.476970672607422 
model_pd.lagr.mean(): -20.288593292236328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4700], device='cuda:0')), ('power', tensor([-21.3467], device='cuda:0'))])
epoch£º505	 i:0 	 global-step:10100	 l-p:0.18837806582450867
epoch£º505	 i:1 	 global-step:10101	 l-p:0.15414565801620483
epoch£º505	 i:2 	 global-step:10102	 l-p:0.10601464658975601
epoch£º505	 i:3 	 global-step:10103	 l-p:0.14815878868103027
epoch£º505	 i:4 	 global-step:10104	 l-p:0.07458378374576569
epoch£º505	 i:5 	 global-step:10105	 l-p:0.13167916238307953
epoch£º505	 i:6 	 global-step:10106	 l-p:12.766263961791992
epoch£º505	 i:7 	 global-step:10107	 l-p:0.14065761864185333
epoch£º505	 i:8 	 global-step:10108	 l-p:0.11068617552518845
epoch£º505	 i:9 	 global-step:10109	 l-p:0.07146505266427994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:506
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1433, 5.1323, 5.1418],
        [5.1433, 5.8287, 6.0420],
        [5.1433, 5.6802, 5.7751],
        [5.1433, 5.0691, 4.9360]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:506, step:0 
model_pd.l_p.mean(): 0.06709633767604828 
model_pd.l_d.mean(): -19.555265426635742 
model_pd.lagr.mean(): -19.488168716430664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5043], device='cuda:0')), ('power', tensor([-20.4433], device='cuda:0'))])
epoch£º506	 i:0 	 global-step:10120	 l-p:0.06709633767604828
epoch£º506	 i:1 	 global-step:10121	 l-p:0.04633466526865959
epoch£º506	 i:2 	 global-step:10122	 l-p:0.17126697301864624
epoch£º506	 i:3 	 global-step:10123	 l-p:0.10785213112831116
epoch£º506	 i:4 	 global-step:10124	 l-p:0.20492571592330933
epoch£º506	 i:5 	 global-step:10125	 l-p:0.11136747151613235
epoch£º506	 i:6 	 global-step:10126	 l-p:0.13960963487625122
epoch£º506	 i:7 	 global-step:10127	 l-p:0.10312755405902863
epoch£º506	 i:8 	 global-step:10128	 l-p:0.11551761627197266
epoch£º506	 i:9 	 global-step:10129	 l-p:0.12872444093227386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:507
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2812, 5.2383, 5.2631],
        [5.2812, 5.1866, 5.1398],
        [5.2812, 5.1936, 5.1077],
        [5.2812, 5.9475, 6.1351]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:507, step:0 
model_pd.l_p.mean(): 0.1350482702255249 
model_pd.l_d.mean(): -20.177167892456055 
model_pd.lagr.mean(): -20.0421199798584 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4159], device='cuda:0')), ('power', tensor([-20.9852], device='cuda:0'))])
epoch£º507	 i:0 	 global-step:10140	 l-p:0.1350482702255249
epoch£º507	 i:1 	 global-step:10141	 l-p:0.12179630994796753
epoch£º507	 i:2 	 global-step:10142	 l-p:0.1187596321105957
epoch£º507	 i:3 	 global-step:10143	 l-p:0.16391701996326447
epoch£º507	 i:4 	 global-step:10144	 l-p:0.1199306771159172
epoch£º507	 i:5 	 global-step:10145	 l-p:0.049292538315057755
epoch£º507	 i:6 	 global-step:10146	 l-p:0.04017329216003418
epoch£º507	 i:7 	 global-step:10147	 l-p:0.1708817183971405
epoch£º507	 i:8 	 global-step:10148	 l-p:-0.05312395840883255
epoch£º507	 i:9 	 global-step:10149	 l-p:0.12055152654647827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:508
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1947, 5.1713, 5.1891],
        [5.1947, 5.1924, 5.1946],
        [5.1947, 5.1941, 5.1947],
        [5.1947, 5.2063, 5.0371]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:508, step:0 
model_pd.l_p.mean(): 0.09646598249673843 
model_pd.l_d.mean(): -19.30970001220703 
model_pd.lagr.mean(): -19.213233947753906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4901], device='cuda:0')), ('power', tensor([-20.1783], device='cuda:0'))])
epoch£º508	 i:0 	 global-step:10160	 l-p:0.09646598249673843
epoch£º508	 i:1 	 global-step:10161	 l-p:0.1318119615316391
epoch£º508	 i:2 	 global-step:10162	 l-p:0.19010013341903687
epoch£º508	 i:3 	 global-step:10163	 l-p:-0.17407698929309845
epoch£º508	 i:4 	 global-step:10164	 l-p:-0.056427616626024246
epoch£º508	 i:5 	 global-step:10165	 l-p:0.1418108493089676
epoch£º508	 i:6 	 global-step:10166	 l-p:-0.20731036365032196
epoch£º508	 i:7 	 global-step:10167	 l-p:0.10944734513759613
epoch£º508	 i:8 	 global-step:10168	 l-p:0.14528968930244446
epoch£º508	 i:9 	 global-step:10169	 l-p:0.13621771335601807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:509
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2176, 5.4610, 5.3591],
        [5.2176, 5.2171, 5.2176],
        [5.2176, 5.9493, 6.1952],
        [5.2176, 5.1785, 5.2030]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:509, step:0 
model_pd.l_p.mean(): 0.31549498438835144 
model_pd.l_d.mean(): -19.72450065612793 
model_pd.lagr.mean(): -19.409006118774414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4488], device='cuda:0')), ('power', tensor([-20.5582], device='cuda:0'))])
epoch£º509	 i:0 	 global-step:10180	 l-p:0.31549498438835144
epoch£º509	 i:1 	 global-step:10181	 l-p:0.3715864419937134
epoch£º509	 i:2 	 global-step:10182	 l-p:0.11856785416603088
epoch£º509	 i:3 	 global-step:10183	 l-p:0.13873103260993958
epoch£º509	 i:4 	 global-step:10184	 l-p:0.09567102044820786
epoch£º509	 i:5 	 global-step:10185	 l-p:0.12387349456548691
epoch£º509	 i:6 	 global-step:10186	 l-p:0.11508139967918396
epoch£º509	 i:7 	 global-step:10187	 l-p:0.029940443113446236
epoch£º509	 i:8 	 global-step:10188	 l-p:0.10862018167972565
epoch£º509	 i:9 	 global-step:10189	 l-p:0.14224325120449066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:510
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1383, 5.1368, 5.1382],
        [5.1383, 5.2647, 5.1123],
        [5.1383, 5.3015, 5.1629],
        [5.1383, 5.1465, 4.9739]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:510, step:0 
model_pd.l_p.mean(): 0.11697477847337723 
model_pd.l_d.mean(): -18.554580688476562 
model_pd.lagr.mean(): -18.437606811523438 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4725], device='cuda:0')), ('power', tensor([-19.3909], device='cuda:0'))])
epoch£º510	 i:0 	 global-step:10200	 l-p:0.11697477847337723
epoch£º510	 i:1 	 global-step:10201	 l-p:0.14711914956569672
epoch£º510	 i:2 	 global-step:10202	 l-p:0.1311964988708496
epoch£º510	 i:3 	 global-step:10203	 l-p:0.03355345502495766
epoch£º510	 i:4 	 global-step:10204	 l-p:0.12733085453510284
epoch£º510	 i:5 	 global-step:10205	 l-p:0.13584469258785248
epoch£º510	 i:6 	 global-step:10206	 l-p:0.14693062007427216
epoch£º510	 i:7 	 global-step:10207	 l-p:0.13113225996494293
epoch£º510	 i:8 	 global-step:10208	 l-p:0.1594473272562027
epoch£º510	 i:9 	 global-step:10209	 l-p:0.14549608528614044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:511
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9541, 4.8794, 4.9071],
        [4.9541, 4.9988, 4.8212],
        [4.9541, 4.9191, 4.9435],
        [4.9541, 4.9541, 4.9541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:511, step:0 
model_pd.l_p.mean(): 0.17241179943084717 
model_pd.l_d.mean(): -19.93472671508789 
model_pd.lagr.mean(): -19.76231575012207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5435], device='cuda:0')), ('power', tensor([-20.8704], device='cuda:0'))])
epoch£º511	 i:0 	 global-step:10220	 l-p:0.17241179943084717
epoch£º511	 i:1 	 global-step:10221	 l-p:0.1381761133670807
epoch£º511	 i:2 	 global-step:10222	 l-p:0.12515395879745483
epoch£º511	 i:3 	 global-step:10223	 l-p:0.16665582358837128
epoch£º511	 i:4 	 global-step:10224	 l-p:0.1335073560476303
epoch£º511	 i:5 	 global-step:10225	 l-p:0.17315301299095154
epoch£º511	 i:6 	 global-step:10226	 l-p:0.12684586644172668
epoch£º511	 i:7 	 global-step:10227	 l-p:0.15643368661403656
epoch£º511	 i:8 	 global-step:10228	 l-p:0.05814683437347412
epoch£º511	 i:9 	 global-step:10229	 l-p:0.14536620676517487
====================================================================================================
====================================================================================================
====================================================================================================

epoch:512
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9677, 4.9558, 4.9662],
        [4.9677, 4.9250, 4.9522],
        [4.9677, 4.9970, 4.8171],
        [4.9677, 4.8655, 4.8711]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:512, step:0 
model_pd.l_p.mean(): 0.15959516167640686 
model_pd.l_d.mean(): -20.486459732055664 
model_pd.lagr.mean(): -20.32686424255371 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4564], device='cuda:0')), ('power', tensor([-21.3422], device='cuda:0'))])
epoch£º512	 i:0 	 global-step:10240	 l-p:0.15959516167640686
epoch£º512	 i:1 	 global-step:10241	 l-p:0.10015422850847244
epoch£º512	 i:2 	 global-step:10242	 l-p:0.19502030313014984
epoch£º512	 i:3 	 global-step:10243	 l-p:0.1395532637834549
epoch£º512	 i:4 	 global-step:10244	 l-p:0.10637928545475006
epoch£º512	 i:5 	 global-step:10245	 l-p:0.1066352054476738
epoch£º512	 i:6 	 global-step:10246	 l-p:0.1290365755558014
epoch£º512	 i:7 	 global-step:10247	 l-p:0.14346690475940704
epoch£º512	 i:8 	 global-step:10248	 l-p:0.1565553992986679
epoch£º512	 i:9 	 global-step:10249	 l-p:0.2127162516117096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:513
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9316, 4.8105, 4.7760],
        [4.9316, 5.2384, 5.1820],
        [4.9316, 4.8166, 4.8029],
        [4.9316, 4.9294, 4.9315]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:513, step:0 
model_pd.l_p.mean(): 0.15052783489227295 
model_pd.l_d.mean(): -20.32325553894043 
model_pd.lagr.mean(): -20.172727584838867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5052], device='cuda:0')), ('power', tensor([-21.2265], device='cuda:0'))])
epoch£º513	 i:0 	 global-step:10260	 l-p:0.15052783489227295
epoch£º513	 i:1 	 global-step:10261	 l-p:0.16621807217597961
epoch£º513	 i:2 	 global-step:10262	 l-p:0.13869830965995789
epoch£º513	 i:3 	 global-step:10263	 l-p:0.1500721275806427
epoch£º513	 i:4 	 global-step:10264	 l-p:0.18867355585098267
epoch£º513	 i:5 	 global-step:10265	 l-p:0.13301119208335876
epoch£º513	 i:6 	 global-step:10266	 l-p:0.12614378333091736
epoch£º513	 i:7 	 global-step:10267	 l-p:0.12811465561389923
epoch£º513	 i:8 	 global-step:10268	 l-p:0.07051984965801239
epoch£º513	 i:9 	 global-step:10269	 l-p:0.09916509687900543
====================================================================================================
====================================================================================================
====================================================================================================

epoch:514
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0459, 5.5835, 5.6837],
        [5.0459, 5.3637, 5.3088],
        [5.0459, 4.9428, 4.9411],
        [5.0459, 5.0457, 5.0459]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:514, step:0 
model_pd.l_p.mean(): 0.14640405774116516 
model_pd.l_d.mean(): -20.290199279785156 
model_pd.lagr.mean(): -20.143795013427734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4581], device='cuda:0')), ('power', tensor([-21.1441], device='cuda:0'))])
epoch£º514	 i:0 	 global-step:10280	 l-p:0.14640405774116516
epoch£º514	 i:1 	 global-step:10281	 l-p:0.1467144936323166
epoch£º514	 i:2 	 global-step:10282	 l-p:0.16178692877292633
epoch£º514	 i:3 	 global-step:10283	 l-p:0.6140908002853394
epoch£º514	 i:4 	 global-step:10284	 l-p:0.08202031254768372
epoch£º514	 i:5 	 global-step:10285	 l-p:0.13201163709163666
epoch£º514	 i:6 	 global-step:10286	 l-p:0.11545461416244507
epoch£º514	 i:7 	 global-step:10287	 l-p:0.1409192532300949
epoch£º514	 i:8 	 global-step:10288	 l-p:0.12382333725690842
epoch£º514	 i:9 	 global-step:10289	 l-p:0.1341673582792282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:515
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4296e-01, 3.3766e-01,
         1.0000e+00, 2.5740e-01, 1.0000e+00, 7.6229e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0346, 5.3165, 5.2397],
        [5.0346, 5.2670, 5.1619],
        [5.0346, 4.9815, 5.0106],
        [5.0346, 4.9982, 4.8236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:515, step:0 
model_pd.l_p.mean(): 0.031243912875652313 
model_pd.l_d.mean(): -18.912067413330078 
model_pd.lagr.mean(): -18.880823135375977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5612], device='cuda:0')), ('power', tensor([-19.8470], device='cuda:0'))])
epoch£º515	 i:0 	 global-step:10300	 l-p:0.031243912875652313
epoch£º515	 i:1 	 global-step:10301	 l-p:0.10502978414297104
epoch£º515	 i:2 	 global-step:10302	 l-p:0.11629577726125717
epoch£º515	 i:3 	 global-step:10303	 l-p:0.15913720428943634
epoch£º515	 i:4 	 global-step:10304	 l-p:0.13361163437366486
epoch£º515	 i:5 	 global-step:10305	 l-p:0.13451112806797028
epoch£º515	 i:6 	 global-step:10306	 l-p:0.10179930180311203
epoch£º515	 i:7 	 global-step:10307	 l-p:0.29698294401168823
epoch£º515	 i:8 	 global-step:10308	 l-p:0.2741682231426239
epoch£º515	 i:9 	 global-step:10309	 l-p:0.15344440937042236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:516
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9036, 4.8938, 4.9025],
        [4.9036, 4.7873, 4.7787],
        [4.9036, 5.1453, 5.0497],
        [4.9036, 4.8856, 4.9004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:516, step:0 
model_pd.l_p.mean(): 0.40353095531463623 
model_pd.l_d.mean(): -19.852304458618164 
model_pd.lagr.mean(): -19.448774337768555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5643], device='cuda:0')), ('power', tensor([-20.8080], device='cuda:0'))])
epoch£º516	 i:0 	 global-step:10320	 l-p:0.40353095531463623
epoch£º516	 i:1 	 global-step:10321	 l-p:0.20355528593063354
epoch£º516	 i:2 	 global-step:10322	 l-p:0.14012032747268677
epoch£º516	 i:3 	 global-step:10323	 l-p:0.13006561994552612
epoch£º516	 i:4 	 global-step:10324	 l-p:0.09681030362844467
epoch£º516	 i:5 	 global-step:10325	 l-p:0.17408154904842377
epoch£º516	 i:6 	 global-step:10326	 l-p:0.10528376698493958
epoch£º516	 i:7 	 global-step:10327	 l-p:0.15065699815750122
epoch£º516	 i:8 	 global-step:10328	 l-p:0.13170386850833893
epoch£º516	 i:9 	 global-step:10329	 l-p:0.14036986231803894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:517
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9699, 4.9623, 4.9692],
        [4.9699, 4.8584, 4.7370],
        [4.9699, 4.8512, 4.7499],
        [4.9699, 4.9699, 4.9699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:517, step:0 
model_pd.l_p.mean(): 0.09695892035961151 
model_pd.l_d.mean(): -19.893627166748047 
model_pd.lagr.mean(): -19.796669006347656 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4986], device='cuda:0')), ('power', tensor([-20.7821], device='cuda:0'))])
epoch£º517	 i:0 	 global-step:10340	 l-p:0.09695892035961151
epoch£º517	 i:1 	 global-step:10341	 l-p:0.1302875131368637
epoch£º517	 i:2 	 global-step:10342	 l-p:0.17338202893733978
epoch£º517	 i:3 	 global-step:10343	 l-p:0.14516755938529968
epoch£º517	 i:4 	 global-step:10344	 l-p:0.11847298592329025
epoch£º517	 i:5 	 global-step:10345	 l-p:0.12862378358840942
epoch£º517	 i:6 	 global-step:10346	 l-p:0.09982660412788391
epoch£º517	 i:7 	 global-step:10347	 l-p:-1.4519903659820557
epoch£º517	 i:8 	 global-step:10348	 l-p:0.08051005005836487
epoch£º517	 i:9 	 global-step:10349	 l-p:0.13835465908050537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:518
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6054,  0.5121,  1.0000,  0.4332,
          1.0000,  0.8459, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4016,  0.2963,  1.0000,  0.2186,
          1.0000,  0.7378, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1448,  0.0760,  1.0000,  0.0399,
          1.0000,  0.5251, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228]], device='cuda:0')
 pt:tensor([[5.1608, 5.3202, 5.1774],
        [5.1608, 5.1081, 4.9484],
        [5.1608, 5.0826, 5.1046],
        [5.1608, 5.0516, 5.0077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:518, step:0 
model_pd.l_p.mean(): 0.11515957862138748 
model_pd.l_d.mean(): -20.13790512084961 
model_pd.lagr.mean(): -20.02274513244629 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4597], device='cuda:0')), ('power', tensor([-20.9906], device='cuda:0'))])
epoch£º518	 i:0 	 global-step:10360	 l-p:0.11515957862138748
epoch£º518	 i:1 	 global-step:10361	 l-p:0.040594056248664856
epoch£º518	 i:2 	 global-step:10362	 l-p:0.03076215647161007
epoch£º518	 i:3 	 global-step:10363	 l-p:0.12188339233398438
epoch£º518	 i:4 	 global-step:10364	 l-p:0.19363021850585938
epoch£º518	 i:5 	 global-step:10365	 l-p:0.1358828842639923
epoch£º518	 i:6 	 global-step:10366	 l-p:0.09242460131645203
epoch£º518	 i:7 	 global-step:10367	 l-p:0.1252603828907013
epoch£º518	 i:8 	 global-step:10368	 l-p:0.12060149013996124
epoch£º518	 i:9 	 global-step:10369	 l-p:0.2586863934993744
====================================================================================================
====================================================================================================
====================================================================================================

epoch:519
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2313, 5.2285, 5.2311],
        [5.2313, 5.9028, 6.0953],
        [5.2313, 5.2283, 5.2311],
        [5.2313, 5.1666, 5.1927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:519, step:0 
model_pd.l_p.mean(): 0.12696032226085663 
model_pd.l_d.mean(): -20.25492286682129 
model_pd.lagr.mean(): -20.127962112426758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4073], device='cuda:0')), ('power', tensor([-21.0556], device='cuda:0'))])
epoch£º519	 i:0 	 global-step:10380	 l-p:0.12696032226085663
epoch£º519	 i:1 	 global-step:10381	 l-p:0.1767003834247589
epoch£º519	 i:2 	 global-step:10382	 l-p:0.11138007789850235
epoch£º519	 i:3 	 global-step:10383	 l-p:0.15061765909194946
epoch£º519	 i:4 	 global-step:10384	 l-p:0.0006829166086390615
epoch£º519	 i:5 	 global-step:10385	 l-p:-0.05542554333806038
epoch£º519	 i:6 	 global-step:10386	 l-p:0.12001994997262955
epoch£º519	 i:7 	 global-step:10387	 l-p:0.08464040607213974
epoch£º519	 i:8 	 global-step:10388	 l-p:0.11362360417842865
epoch£º519	 i:9 	 global-step:10389	 l-p:0.11703448742628098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:520
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1619, 5.1619, 5.1619],
        [5.1619, 5.6870, 5.7688],
        [5.1619, 5.1619, 5.1619],
        [5.1619, 5.1547, 5.1612]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:520, step:0 
model_pd.l_p.mean(): 0.13434498012065887 
model_pd.l_d.mean(): -19.414220809936523 
model_pd.lagr.mean(): -19.279876708984375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4454], device='cuda:0')), ('power', tensor([-20.2386], device='cuda:0'))])
epoch£º520	 i:0 	 global-step:10400	 l-p:0.13434498012065887
epoch£º520	 i:1 	 global-step:10401	 l-p:0.07287661731243134
epoch£º520	 i:2 	 global-step:10402	 l-p:0.12131831794977188
epoch£º520	 i:3 	 global-step:10403	 l-p:-0.09252019971609116
epoch£º520	 i:4 	 global-step:10404	 l-p:0.16069673001766205
epoch£º520	 i:5 	 global-step:10405	 l-p:0.21889770030975342
epoch£º520	 i:6 	 global-step:10406	 l-p:0.13279643654823303
epoch£º520	 i:7 	 global-step:10407	 l-p:0.12931953370571136
epoch£º520	 i:8 	 global-step:10408	 l-p:0.13989505171775818
epoch£º520	 i:9 	 global-step:10409	 l-p:0.16684894263744354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:521
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0802, 4.9640, 4.8927],
        [5.0802, 5.0525, 5.0732],
        [5.0802, 5.6616, 5.7918],
        [5.0802, 4.9658, 4.9337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:521, step:0 
model_pd.l_p.mean(): 0.13488902151584625 
model_pd.l_d.mean(): -20.566375732421875 
model_pd.lagr.mean(): -20.431486129760742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4174], device='cuda:0')), ('power', tensor([-21.3833], device='cuda:0'))])
epoch£º521	 i:0 	 global-step:10420	 l-p:0.13488902151584625
epoch£º521	 i:1 	 global-step:10421	 l-p:0.12534335255622864
epoch£º521	 i:2 	 global-step:10422	 l-p:-0.04531996697187424
epoch£º521	 i:3 	 global-step:10423	 l-p:0.15412010252475739
epoch£º521	 i:4 	 global-step:10424	 l-p:0.11484355479478836
epoch£º521	 i:5 	 global-step:10425	 l-p:0.15448470413684845
epoch£º521	 i:6 	 global-step:10426	 l-p:0.14785490930080414
epoch£º521	 i:7 	 global-step:10427	 l-p:0.14644202589988708
epoch£º521	 i:8 	 global-step:10428	 l-p:0.11760053038597107
epoch£º521	 i:9 	 global-step:10429	 l-p:0.2671818137168884
====================================================================================================
====================================================================================================
====================================================================================================

epoch:522
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9182, 4.9144, 4.9180],
        [4.9182, 4.9182, 4.9182],
        [4.9182, 4.9094, 4.9173],
        [4.9182, 4.9083, 4.9171]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:522, step:0 
model_pd.l_p.mean(): 0.1167648658156395 
model_pd.l_d.mean(): -20.590885162353516 
model_pd.lagr.mean(): -20.47412109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4798], device='cuda:0')), ('power', tensor([-21.4729], device='cuda:0'))])
epoch£º522	 i:0 	 global-step:10440	 l-p:0.1167648658156395
epoch£º522	 i:1 	 global-step:10441	 l-p:0.20849111676216125
epoch£º522	 i:2 	 global-step:10442	 l-p:0.2573491334915161
epoch£º522	 i:3 	 global-step:10443	 l-p:0.14989051222801208
epoch£º522	 i:4 	 global-step:10444	 l-p:0.18144263327121735
epoch£º522	 i:5 	 global-step:10445	 l-p:0.1676424741744995
epoch£º522	 i:6 	 global-step:10446	 l-p:0.07630160450935364
epoch£º522	 i:7 	 global-step:10447	 l-p:0.10951300710439682
epoch£º522	 i:8 	 global-step:10448	 l-p:0.1324991136789322
epoch£º522	 i:9 	 global-step:10449	 l-p:0.11032529920339584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:523
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0495, 5.0314, 5.0463],
        [5.0495, 5.0495, 5.0495],
        [5.0495, 5.6177, 5.7390],
        [5.0495, 5.0494, 5.0495]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:523, step:0 
model_pd.l_p.mean(): 0.10401926934719086 
model_pd.l_d.mean(): -18.14482307434082 
model_pd.lagr.mean(): -18.040803909301758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6136], device='cuda:0')), ('power', tensor([-19.1197], device='cuda:0'))])
epoch£º523	 i:0 	 global-step:10460	 l-p:0.10401926934719086
epoch£º523	 i:1 	 global-step:10461	 l-p:0.0917939841747284
epoch£º523	 i:2 	 global-step:10462	 l-p:0.4146590232849121
epoch£º523	 i:3 	 global-step:10463	 l-p:0.14829270541667938
epoch£º523	 i:4 	 global-step:10464	 l-p:0.1344839632511139
epoch£º523	 i:5 	 global-step:10465	 l-p:-0.026990404352545738
epoch£º523	 i:6 	 global-step:10466	 l-p:0.16022907197475433
epoch£º523	 i:7 	 global-step:10467	 l-p:0.14768756926059723
epoch£º523	 i:8 	 global-step:10468	 l-p:0.14166153967380524
epoch£º523	 i:9 	 global-step:10469	 l-p:0.13884757459163666
====================================================================================================
====================================================================================================
====================================================================================================

epoch:524
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0137, 5.0025, 5.0123],
        [5.0137, 5.2773, 5.1882],
        [5.0137, 5.0134, 5.0137],
        [5.0137, 5.0031, 5.0124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:524, step:0 
model_pd.l_p.mean(): 0.19218526780605316 
model_pd.l_d.mean(): -20.16177749633789 
model_pd.lagr.mean(): -19.969593048095703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4824], device='cuda:0')), ('power', tensor([-21.0384], device='cuda:0'))])
epoch£º524	 i:0 	 global-step:10480	 l-p:0.19218526780605316
epoch£º524	 i:1 	 global-step:10481	 l-p:0.1272401213645935
epoch£º524	 i:2 	 global-step:10482	 l-p:0.02487759105861187
epoch£º524	 i:3 	 global-step:10483	 l-p:0.11004921048879623
epoch£º524	 i:4 	 global-step:10484	 l-p:0.13437211513519287
epoch£º524	 i:5 	 global-step:10485	 l-p:0.13238368928432465
epoch£º524	 i:6 	 global-step:10486	 l-p:0.1327986866235733
epoch£º524	 i:7 	 global-step:10487	 l-p:0.12455969303846359
epoch£º524	 i:8 	 global-step:10488	 l-p:0.1503288596868515
epoch£º524	 i:9 	 global-step:10489	 l-p:0.11384321004152298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:525
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9913, 4.9889, 4.9912],
        [4.9913, 4.9913, 4.9913],
        [4.9913, 4.9881, 4.9911],
        [4.9913, 4.8654, 4.8169]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:525, step:0 
model_pd.l_p.mean(): 0.16480636596679688 
model_pd.l_d.mean(): -19.570547103881836 
model_pd.lagr.mean(): -19.40574073791504 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5178], device='cuda:0')), ('power', tensor([-20.4728], device='cuda:0'))])
epoch£º525	 i:0 	 global-step:10500	 l-p:0.16480636596679688
epoch£º525	 i:1 	 global-step:10501	 l-p:0.08089213818311691
epoch£º525	 i:2 	 global-step:10502	 l-p:0.12351377308368683
epoch£º525	 i:3 	 global-step:10503	 l-p:0.12675826251506805
epoch£º525	 i:4 	 global-step:10504	 l-p:0.12352500855922699
epoch£º525	 i:5 	 global-step:10505	 l-p:0.12467868626117706
epoch£º525	 i:6 	 global-step:10506	 l-p:0.18748126924037933
epoch£º525	 i:7 	 global-step:10507	 l-p:0.17309729754924774
epoch£º525	 i:8 	 global-step:10508	 l-p:0.13536421954631805
epoch£º525	 i:9 	 global-step:10509	 l-p:0.10603347420692444
====================================================================================================
====================================================================================================
====================================================================================================

epoch:526
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0015, 4.9903, 5.0001],
        [5.0015, 4.8861, 4.8776],
        [5.0015, 5.0616, 4.8817],
        [5.0015, 5.1334, 4.9770]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:526, step:0 
model_pd.l_p.mean(): 0.1522396206855774 
model_pd.l_d.mean(): -20.497325897216797 
model_pd.lagr.mean(): -20.3450870513916 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4707], device='cuda:0')), ('power', tensor([-21.3682], device='cuda:0'))])
epoch£º526	 i:0 	 global-step:10520	 l-p:0.1522396206855774
epoch£º526	 i:1 	 global-step:10521	 l-p:0.1444004476070404
epoch£º526	 i:2 	 global-step:10522	 l-p:0.06057208776473999
epoch£º526	 i:3 	 global-step:10523	 l-p:0.12598951160907745
epoch£º526	 i:4 	 global-step:10524	 l-p:0.12373525649309158
epoch£º526	 i:5 	 global-step:10525	 l-p:0.1370694935321808
epoch£º526	 i:6 	 global-step:10526	 l-p:0.18601295351982117
epoch£º526	 i:7 	 global-step:10527	 l-p:0.1376737356185913
epoch£º526	 i:8 	 global-step:10528	 l-p:0.39128199219703674
epoch£º526	 i:9 	 global-step:10529	 l-p:0.1337174028158188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:527
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1769, 5.1769, 5.1769],
        [5.1769, 5.0754, 4.9610],
        [5.1769, 5.1769, 5.1769],
        [5.1769, 5.1762, 5.1769]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:527, step:0 
model_pd.l_p.mean(): 0.12700389325618744 
model_pd.l_d.mean(): -20.3385066986084 
model_pd.lagr.mean(): -20.211502075195312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4134], device='cuda:0')), ('power', tensor([-21.1470], device='cuda:0'))])
epoch£º527	 i:0 	 global-step:10540	 l-p:0.12700389325618744
epoch£º527	 i:1 	 global-step:10541	 l-p:0.13349196314811707
epoch£º527	 i:2 	 global-step:10542	 l-p:0.15145310759544373
epoch£º527	 i:3 	 global-step:10543	 l-p:0.13967056572437286
epoch£º527	 i:4 	 global-step:10544	 l-p:-0.6567865014076233
epoch£º527	 i:5 	 global-step:10545	 l-p:0.1322329044342041
epoch£º527	 i:6 	 global-step:10546	 l-p:0.1347024142742157
epoch£º527	 i:7 	 global-step:10547	 l-p:-0.290572851896286
epoch£º527	 i:8 	 global-step:10548	 l-p:0.1272018700838089
epoch£º527	 i:9 	 global-step:10549	 l-p:0.1400386244058609
====================================================================================================
====================================================================================================
====================================================================================================

epoch:528
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0406, 4.9954, 5.0238],
        [5.0406, 5.0340, 5.0400],
        [5.0406, 5.3407, 5.2714],
        [5.0406, 5.3409, 5.2717]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:528, step:0 
model_pd.l_p.mean(): 0.05131912976503372 
model_pd.l_d.mean(): -19.628843307495117 
model_pd.lagr.mean(): -19.577524185180664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5261], device='cuda:0')), ('power', tensor([-20.5408], device='cuda:0'))])
epoch£º528	 i:0 	 global-step:10560	 l-p:0.05131912976503372
epoch£º528	 i:1 	 global-step:10561	 l-p:0.12280982732772827
epoch£º528	 i:2 	 global-step:10562	 l-p:0.13189499080181122
epoch£º528	 i:3 	 global-step:10563	 l-p:0.1608799695968628
epoch£º528	 i:4 	 global-step:10564	 l-p:0.12483550608158112
epoch£º528	 i:5 	 global-step:10565	 l-p:0.13144215941429138
epoch£º528	 i:6 	 global-step:10566	 l-p:0.16422335803508759
epoch£º528	 i:7 	 global-step:10567	 l-p:0.15718282759189606
epoch£º528	 i:8 	 global-step:10568	 l-p:0.13534151017665863
epoch£º528	 i:9 	 global-step:10569	 l-p:0.16117660701274872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:529
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3405,  0.2378,  1.0000,  0.1660,
          1.0000,  0.6983, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4541,  0.3490,  1.0000,  0.2683,
          1.0000,  0.7686, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6844,  0.6031,  1.0000,  0.5315,
          1.0000,  0.8812, 31.6228]], device='cuda:0')
 pt:tensor([[4.9503, 4.8286, 4.7007],
        [4.9503, 4.8953, 4.7085],
        [4.9503, 4.8714, 4.9019],
        [4.9503, 5.1488, 5.0237]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:529, step:0 
model_pd.l_p.mean(): 0.10894060134887695 
model_pd.l_d.mean(): -20.531158447265625 
model_pd.lagr.mean(): -20.422218322753906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4810], device='cuda:0')), ('power', tensor([-21.4132], device='cuda:0'))])
epoch£º529	 i:0 	 global-step:10580	 l-p:0.10894060134887695
epoch£º529	 i:1 	 global-step:10581	 l-p:0.10096122324466705
epoch£º529	 i:2 	 global-step:10582	 l-p:0.1444486677646637
epoch£º529	 i:3 	 global-step:10583	 l-p:0.1420564353466034
epoch£º529	 i:4 	 global-step:10584	 l-p:0.202448770403862
epoch£º529	 i:5 	 global-step:10585	 l-p:0.07980884611606598
epoch£º529	 i:6 	 global-step:10586	 l-p:0.15865972638130188
epoch£º529	 i:7 	 global-step:10587	 l-p:0.15628407895565033
epoch£º529	 i:8 	 global-step:10588	 l-p:0.1560499370098114
epoch£º529	 i:9 	 global-step:10589	 l-p:0.14221009612083435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:530
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0123, 5.0123, 5.0123],
        [5.0123, 4.8834, 4.8125],
        [5.0123, 5.0123, 5.0123],
        [5.0123, 5.1174, 4.9492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:530, step:0 
model_pd.l_p.mean(): 0.1351993978023529 
model_pd.l_d.mean(): -20.174476623535156 
model_pd.lagr.mean(): -20.039278030395508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4934], device='cuda:0')), ('power', tensor([-21.0627], device='cuda:0'))])
epoch£º530	 i:0 	 global-step:10600	 l-p:0.1351993978023529
epoch£º530	 i:1 	 global-step:10601	 l-p:0.12727801501750946
epoch£º530	 i:2 	 global-step:10602	 l-p:0.12385208159685135
epoch£º530	 i:3 	 global-step:10603	 l-p:0.0943431481719017
epoch£º530	 i:4 	 global-step:10604	 l-p:-0.18392053246498108
epoch£º530	 i:5 	 global-step:10605	 l-p:0.13334505259990692
epoch£º530	 i:6 	 global-step:10606	 l-p:0.1647016555070877
epoch£º530	 i:7 	 global-step:10607	 l-p:0.10793962329626083
epoch£º530	 i:8 	 global-step:10608	 l-p:0.1340644359588623
epoch£º530	 i:9 	 global-step:10609	 l-p:0.06382234394550323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:531
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1064, 5.1013, 5.1060],
        [5.1064, 5.1064, 5.1064],
        [5.1064, 5.1064, 5.1064],
        [5.1064, 5.1064, 5.1064]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:531, step:0 
model_pd.l_p.mean(): 0.14249113202095032 
model_pd.l_d.mean(): -19.267446517944336 
model_pd.lagr.mean(): -19.124956130981445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4743], device='cuda:0')), ('power', tensor([-20.1190], device='cuda:0'))])
epoch£º531	 i:0 	 global-step:10620	 l-p:0.14249113202095032
epoch£º531	 i:1 	 global-step:10621	 l-p:0.12641356885433197
epoch£º531	 i:2 	 global-step:10622	 l-p:0.14386001229286194
epoch£º531	 i:3 	 global-step:10623	 l-p:0.13510523736476898
epoch£º531	 i:4 	 global-step:10624	 l-p:0.07787831872701645
epoch£º531	 i:5 	 global-step:10625	 l-p:0.19736681878566742
epoch£º531	 i:6 	 global-step:10626	 l-p:-12.769981384277344
epoch£º531	 i:7 	 global-step:10627	 l-p:0.13416047394275665
epoch£º531	 i:8 	 global-step:10628	 l-p:0.13559313118457794
epoch£º531	 i:9 	 global-step:10629	 l-p:0.1330973356962204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:532
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1730, 5.1851, 5.0008],
        [5.1730, 5.1271, 5.1552],
        [5.1730, 5.2540, 5.0794],
        [5.1730, 5.1702, 5.1729]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:532, step:0 
model_pd.l_p.mean(): 0.050974003970623016 
model_pd.l_d.mean(): -19.265844345092773 
model_pd.lagr.mean(): -19.21487045288086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4745], device='cuda:0')), ('power', tensor([-20.1176], device='cuda:0'))])
epoch£º532	 i:0 	 global-step:10640	 l-p:0.050974003970623016
epoch£º532	 i:1 	 global-step:10641	 l-p:0.1311292201280594
epoch£º532	 i:2 	 global-step:10642	 l-p:0.21405628323554993
epoch£º532	 i:3 	 global-step:10643	 l-p:0.12510384619235992
epoch£º532	 i:4 	 global-step:10644	 l-p:0.00034655569470487535
epoch£º532	 i:5 	 global-step:10645	 l-p:0.15423144400119781
epoch£º532	 i:6 	 global-step:10646	 l-p:0.11755933612585068
epoch£º532	 i:7 	 global-step:10647	 l-p:0.11489435285329819
epoch£º532	 i:8 	 global-step:10648	 l-p:0.10787726938724518
epoch£º532	 i:9 	 global-step:10649	 l-p:0.29058894515037537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:533
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2546, 5.5716, 5.5031],
        [5.2546, 5.2515, 5.2544],
        [5.2546, 5.2307, 5.2492],
        [5.2546, 5.1425, 5.0980]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:533, step:0 
model_pd.l_p.mean(): 0.12368007749319077 
model_pd.l_d.mean(): -19.561988830566406 
model_pd.lagr.mean(): -19.438308715820312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4285], device='cuda:0')), ('power', tensor([-20.3716], device='cuda:0'))])
epoch£º533	 i:0 	 global-step:10660	 l-p:0.12368007749319077
epoch£º533	 i:1 	 global-step:10661	 l-p:0.11584095656871796
epoch£º533	 i:2 	 global-step:10662	 l-p:0.11634082347154617
epoch£º533	 i:3 	 global-step:10663	 l-p:0.14307473599910736
epoch£º533	 i:4 	 global-step:10664	 l-p:-0.009185737930238247
epoch£º533	 i:5 	 global-step:10665	 l-p:0.13696306943893433
epoch£º533	 i:6 	 global-step:10666	 l-p:0.09996214509010315
epoch£º533	 i:7 	 global-step:10667	 l-p:0.09876033663749695
epoch£º533	 i:8 	 global-step:10668	 l-p:0.13902312517166138
epoch£º533	 i:9 	 global-step:10669	 l-p:-0.22255000472068787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:534
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1366, 5.0172, 4.9833],
        [5.1366, 5.0579, 4.8970],
        [5.1366, 5.1285, 5.1358],
        [5.1366, 5.0158, 4.9358]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:534, step:0 
model_pd.l_p.mean(): 0.08736497908830643 
model_pd.l_d.mean(): -19.804729461669922 
model_pd.lagr.mean(): -19.717365264892578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5026], device='cuda:0')), ('power', tensor([-20.6956], device='cuda:0'))])
epoch£º534	 i:0 	 global-step:10680	 l-p:0.08736497908830643
epoch£º534	 i:1 	 global-step:10681	 l-p:0.13832193613052368
epoch£º534	 i:2 	 global-step:10682	 l-p:0.12656471133232117
epoch£º534	 i:3 	 global-step:10683	 l-p:0.12863390147686005
epoch£º534	 i:4 	 global-step:10684	 l-p:0.11257955431938171
epoch£º534	 i:5 	 global-step:10685	 l-p:-0.36437106132507324
epoch£º534	 i:6 	 global-step:10686	 l-p:0.20692652463912964
epoch£º534	 i:7 	 global-step:10687	 l-p:0.11708083748817444
epoch£º534	 i:8 	 global-step:10688	 l-p:0.14855535328388214
epoch£º534	 i:9 	 global-step:10689	 l-p:0.1575334221124649
====================================================================================================
====================================================================================================
====================================================================================================

epoch:535
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0463, 5.0461, 5.0463],
        [5.0463, 5.0254, 5.0423],
        [5.0463, 5.0463, 5.0463],
        [5.0463, 5.0012, 5.0300]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:535, step:0 
model_pd.l_p.mean(): -0.12088271975517273 
model_pd.l_d.mean(): -20.169565200805664 
model_pd.lagr.mean(): -20.290447235107422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4629], device='cuda:0')), ('power', tensor([-21.0262], device='cuda:0'))])
epoch£º535	 i:0 	 global-step:10700	 l-p:-0.12088271975517273
epoch£º535	 i:1 	 global-step:10701	 l-p:0.14360947906970978
epoch£º535	 i:2 	 global-step:10702	 l-p:0.15428170561790466
epoch£º535	 i:3 	 global-step:10703	 l-p:0.16126641631126404
epoch£º535	 i:4 	 global-step:10704	 l-p:0.12376950681209564
epoch£º535	 i:5 	 global-step:10705	 l-p:0.15034008026123047
epoch£º535	 i:6 	 global-step:10706	 l-p:0.1281883269548416
epoch£º535	 i:7 	 global-step:10707	 l-p:0.1117371916770935
epoch£º535	 i:8 	 global-step:10708	 l-p:0.10087785124778748
epoch£º535	 i:9 	 global-step:10709	 l-p:0.13787594437599182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:536
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9896, 4.9785, 4.9883],
        [4.9896, 4.9842, 4.9892],
        [4.9896, 4.9795, 4.9885],
        [4.9896, 5.0808, 4.9051]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:536, step:0 
model_pd.l_p.mean(): 0.15775109827518463 
model_pd.l_d.mean(): -19.418201446533203 
model_pd.lagr.mean(): -19.26045036315918 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5339], device='cuda:0')), ('power', tensor([-20.3343], device='cuda:0'))])
epoch£º536	 i:0 	 global-step:10720	 l-p:0.15775109827518463
epoch£º536	 i:1 	 global-step:10721	 l-p:0.07961371541023254
epoch£º536	 i:2 	 global-step:10722	 l-p:0.05070000886917114
epoch£º536	 i:3 	 global-step:10723	 l-p:0.10630643367767334
epoch£º536	 i:4 	 global-step:10724	 l-p:0.11219791322946548
epoch£º536	 i:5 	 global-step:10725	 l-p:0.10818183422088623
epoch£º536	 i:6 	 global-step:10726	 l-p:0.13937631249427795
epoch£º536	 i:7 	 global-step:10727	 l-p:0.15387848019599915
epoch£º536	 i:8 	 global-step:10728	 l-p:0.13042454421520233
epoch£º536	 i:9 	 global-step:10729	 l-p:0.10729418694972992
====================================================================================================
====================================================================================================
====================================================================================================

epoch:537
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1229, 5.1229, 5.1229],
        [5.1229, 4.9982, 4.9442],
        [5.1229, 4.9993, 4.9558],
        [5.1229, 5.1229, 5.1229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:537, step:0 
model_pd.l_p.mean(): 0.10175447165966034 
model_pd.l_d.mean(): -20.665870666503906 
model_pd.lagr.mean(): -20.564115524291992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3860], device='cuda:0')), ('power', tensor([-21.4521], device='cuda:0'))])
epoch£º537	 i:0 	 global-step:10740	 l-p:0.10175447165966034
epoch£º537	 i:1 	 global-step:10741	 l-p:0.12425008416175842
epoch£º537	 i:2 	 global-step:10742	 l-p:0.1944570243358612
epoch£º537	 i:3 	 global-step:10743	 l-p:0.06923840939998627
epoch£º537	 i:4 	 global-step:10744	 l-p:0.13407188653945923
epoch£º537	 i:5 	 global-step:10745	 l-p:0.04848940297961235
epoch£º537	 i:6 	 global-step:10746	 l-p:0.12460371106863022
epoch£º537	 i:7 	 global-step:10747	 l-p:0.10195575654506683
epoch£º537	 i:8 	 global-step:10748	 l-p:0.12859630584716797
epoch£º537	 i:9 	 global-step:10749	 l-p:0.16270336508750916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:538
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2243, 5.1921, 5.2151],
        [5.2243, 5.2237, 5.2242],
        [5.2243, 5.2242, 5.2243],
        [5.2243, 5.1731, 5.2023]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:538, step:0 
model_pd.l_p.mean(): 0.12872157990932465 
model_pd.l_d.mean(): -20.45366668701172 
model_pd.lagr.mean(): -20.3249454498291 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3939], device='cuda:0')), ('power', tensor([-21.2441], device='cuda:0'))])
epoch£º538	 i:0 	 global-step:10760	 l-p:0.12872157990932465
epoch£º538	 i:1 	 global-step:10761	 l-p:-0.02444780059158802
epoch£º538	 i:2 	 global-step:10762	 l-p:0.1331019550561905
epoch£º538	 i:3 	 global-step:10763	 l-p:0.11123987287282944
epoch£º538	 i:4 	 global-step:10764	 l-p:-0.040994711220264435
epoch£º538	 i:5 	 global-step:10765	 l-p:0.16245588660240173
epoch£º538	 i:6 	 global-step:10766	 l-p:0.13014011085033417
epoch£º538	 i:7 	 global-step:10767	 l-p:0.2197149246931076
epoch£º538	 i:8 	 global-step:10768	 l-p:0.1327199786901474
epoch£º538	 i:9 	 global-step:10769	 l-p:-0.04408563673496246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:539
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2306, 5.1484, 5.1723],
        [5.2306, 5.2306, 5.2306],
        [5.2306, 5.2178, 5.2288],
        [5.2306, 5.2305, 5.2306]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:539, step:0 
model_pd.l_p.mean(): 0.11340701580047607 
model_pd.l_d.mean(): -20.3194522857666 
model_pd.lagr.mean(): -20.206045150756836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4158], device='cuda:0')), ('power', tensor([-21.1301], device='cuda:0'))])
epoch£º539	 i:0 	 global-step:10780	 l-p:0.11340701580047607
epoch£º539	 i:1 	 global-step:10781	 l-p:0.45834997296333313
epoch£º539	 i:2 	 global-step:10782	 l-p:0.12728428840637207
epoch£º539	 i:3 	 global-step:10783	 l-p:0.14904862642288208
epoch£º539	 i:4 	 global-step:10784	 l-p:0.12310288846492767
epoch£º539	 i:5 	 global-step:10785	 l-p:0.12375171482563019
epoch£º539	 i:6 	 global-step:10786	 l-p:0.11583173274993896
epoch£º539	 i:7 	 global-step:10787	 l-p:0.11775893718004227
epoch£º539	 i:8 	 global-step:10788	 l-p:0.18861956894397736
epoch£º539	 i:9 	 global-step:10789	 l-p:0.04644970968365669
====================================================================================================
====================================================================================================
====================================================================================================

epoch:540
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1700, 5.0727, 4.9294],
        [5.1700, 5.1700, 5.1700],
        [5.1700, 5.0645, 5.0689],
        [5.1700, 5.3001, 5.1381]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:540, step:0 
model_pd.l_p.mean(): 0.16367225348949432 
model_pd.l_d.mean(): -19.14153289794922 
model_pd.lagr.mean(): -18.977861404418945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5447], device='cuda:0')), ('power', tensor([-20.0636], device='cuda:0'))])
epoch£º540	 i:0 	 global-step:10800	 l-p:0.16367225348949432
epoch£º540	 i:1 	 global-step:10801	 l-p:0.21527209877967834
epoch£º540	 i:2 	 global-step:10802	 l-p:0.13532359898090363
epoch£º540	 i:3 	 global-step:10803	 l-p:0.1251152604818344
epoch£º540	 i:4 	 global-step:10804	 l-p:0.06984134763479233
epoch£º540	 i:5 	 global-step:10805	 l-p:0.12123064696788788
epoch£º540	 i:6 	 global-step:10806	 l-p:0.12623146176338196
epoch£º540	 i:7 	 global-step:10807	 l-p:0.13404594361782074
epoch£º540	 i:8 	 global-step:10808	 l-p:0.13679616153240204
epoch£º540	 i:9 	 global-step:10809	 l-p:0.7378883361816406
====================================================================================================
====================================================================================================
====================================================================================================

epoch:541
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0994, 5.0624, 5.0881],
        [5.0994, 5.0699, 5.0919],
        [5.0994, 5.5828, 5.6321],
        [5.0994, 5.0994, 5.0994]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:541, step:0 
model_pd.l_p.mean(): 0.11554381996393204 
model_pd.l_d.mean(): -19.072683334350586 
model_pd.lagr.mean(): -18.95713996887207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4912], device='cuda:0')), ('power', tensor([-19.9381], device='cuda:0'))])
epoch£º541	 i:0 	 global-step:10820	 l-p:0.11554381996393204
epoch£º541	 i:1 	 global-step:10821	 l-p:0.13924558460712433
epoch£º541	 i:2 	 global-step:10822	 l-p:-0.10274473577737808
epoch£º541	 i:3 	 global-step:10823	 l-p:0.09879442304372787
epoch£º541	 i:4 	 global-step:10824	 l-p:0.1518293023109436
epoch£º541	 i:5 	 global-step:10825	 l-p:0.139551043510437
epoch£º541	 i:6 	 global-step:10826	 l-p:0.12172337621450424
epoch£º541	 i:7 	 global-step:10827	 l-p:0.10914558172225952
epoch£º541	 i:8 	 global-step:10828	 l-p:0.12450201064348221
epoch£º541	 i:9 	 global-step:10829	 l-p:0.15959954261779785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:542
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9372, 4.9372, 4.9372],
        [4.9372, 4.8003, 4.6790],
        [4.9372, 5.5439, 5.6971],
        [4.9372, 4.8249, 4.6589]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:542, step:0 
model_pd.l_p.mean(): 0.16124311089515686 
model_pd.l_d.mean(): -18.437711715698242 
model_pd.lagr.mean(): -18.27646827697754 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5911], device='cuda:0')), ('power', tensor([-19.3947], device='cuda:0'))])
epoch£º542	 i:0 	 global-step:10840	 l-p:0.16124311089515686
epoch£º542	 i:1 	 global-step:10841	 l-p:0.18204806745052338
epoch£º542	 i:2 	 global-step:10842	 l-p:0.13503257930278778
epoch£º542	 i:3 	 global-step:10843	 l-p:0.3061699867248535
epoch£º542	 i:4 	 global-step:10844	 l-p:0.09790438413619995
epoch£º542	 i:5 	 global-step:10845	 l-p:0.1404913067817688
epoch£º542	 i:6 	 global-step:10846	 l-p:0.13647577166557312
epoch£º542	 i:7 	 global-step:10847	 l-p:0.16919562220573425
epoch£º542	 i:8 	 global-step:10848	 l-p:0.150824636220932
epoch£º542	 i:9 	 global-step:10849	 l-p:0.13146519660949707
====================================================================================================
====================================================================================================
====================================================================================================

epoch:543
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9628, 4.8484, 4.6878],
        [4.9628, 5.5176, 5.6283],
        [4.9628, 4.8834, 4.9162],
        [4.9628, 4.9614, 4.9627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:543, step:0 
model_pd.l_p.mean(): 0.13537730276584625 
model_pd.l_d.mean(): -19.932958602905273 
model_pd.lagr.mean(): -19.79758071899414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5132], device='cuda:0')), ('power', tensor([-20.8372], device='cuda:0'))])
epoch£º543	 i:0 	 global-step:10860	 l-p:0.13537730276584625
epoch£º543	 i:1 	 global-step:10861	 l-p:0.08687309920787811
epoch£º543	 i:2 	 global-step:10862	 l-p:0.1632051020860672
epoch£º543	 i:3 	 global-step:10863	 l-p:0.13653472065925598
epoch£º543	 i:4 	 global-step:10864	 l-p:0.12847279012203217
epoch£º543	 i:5 	 global-step:10865	 l-p:0.12213725596666336
epoch£º543	 i:6 	 global-step:10866	 l-p:0.15786340832710266
epoch£º543	 i:7 	 global-step:10867	 l-p:0.2208442986011505
epoch£º543	 i:8 	 global-step:10868	 l-p:0.2528097927570343
epoch£º543	 i:9 	 global-step:10869	 l-p:0.20186583697795868
====================================================================================================
====================================================================================================
====================================================================================================

epoch:544
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9721, 4.9709, 4.9720],
        [4.9721, 4.8796, 4.6991],
        [4.9721, 4.9721, 4.9721],
        [4.9721, 5.3096, 5.2632]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:544, step:0 
model_pd.l_p.mean(): 0.11207428574562073 
model_pd.l_d.mean(): -20.433517456054688 
model_pd.lagr.mean(): -20.321443557739258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4627], device='cuda:0')), ('power', tensor([-21.2948], device='cuda:0'))])
epoch£º544	 i:0 	 global-step:10880	 l-p:0.11207428574562073
epoch£º544	 i:1 	 global-step:10881	 l-p:0.13667719066143036
epoch£º544	 i:2 	 global-step:10882	 l-p:0.10571058094501495
epoch£º544	 i:3 	 global-step:10883	 l-p:0.14874818921089172
epoch£º544	 i:4 	 global-step:10884	 l-p:0.1393536478281021
epoch£º544	 i:5 	 global-step:10885	 l-p:0.1400064378976822
epoch£º544	 i:6 	 global-step:10886	 l-p:0.09541536122560501
epoch£º544	 i:7 	 global-step:10887	 l-p:0.12025216966867447
epoch£º544	 i:8 	 global-step:10888	 l-p:0.1456858068704605
epoch£º544	 i:9 	 global-step:10889	 l-p:0.130819171667099
====================================================================================================
====================================================================================================
====================================================================================================

epoch:545
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0663, 5.0544, 5.0648],
        [5.0663, 5.0200, 5.0495],
        [5.0663, 4.9446, 4.9355],
        [5.0663, 4.9469, 4.9423]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:545, step:0 
model_pd.l_p.mean(): 0.09860144555568695 
model_pd.l_d.mean(): -20.591777801513672 
model_pd.lagr.mean(): -20.493175506591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4345], device='cuda:0')), ('power', tensor([-21.4268], device='cuda:0'))])
epoch£º545	 i:0 	 global-step:10900	 l-p:0.09860144555568695
epoch£º545	 i:1 	 global-step:10901	 l-p:0.14175325632095337
epoch£º545	 i:2 	 global-step:10902	 l-p:0.13157245516777039
epoch£º545	 i:3 	 global-step:10903	 l-p:0.11448323726654053
epoch£º545	 i:4 	 global-step:10904	 l-p:0.06933259218931198
epoch£º545	 i:5 	 global-step:10905	 l-p:0.4428885579109192
epoch£º545	 i:6 	 global-step:10906	 l-p:0.1487511545419693
epoch£º545	 i:7 	 global-step:10907	 l-p:0.09517966210842133
epoch£º545	 i:8 	 global-step:10908	 l-p:0.1489836424589157
epoch£º545	 i:9 	 global-step:10909	 l-p:0.11315564811229706
====================================================================================================
====================================================================================================
====================================================================================================

epoch:546
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2045, 5.1783, 5.1984],
        [5.2045, 5.1794, 5.1989],
        [5.2045, 5.2044, 5.2045],
        [5.2045, 5.0900, 4.9757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:546, step:0 
model_pd.l_p.mean(): 0.1257198452949524 
model_pd.l_d.mean(): -19.645124435424805 
model_pd.lagr.mean(): -19.519405364990234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3964], device='cuda:0')), ('power', tensor([-20.4230], device='cuda:0'))])
epoch£º546	 i:0 	 global-step:10920	 l-p:0.1257198452949524
epoch£º546	 i:1 	 global-step:10921	 l-p:0.13247302174568176
epoch£º546	 i:2 	 global-step:10922	 l-p:0.12695170938968658
epoch£º546	 i:3 	 global-step:10923	 l-p:0.068161241710186
epoch£º546	 i:4 	 global-step:10924	 l-p:0.0306102242320776
epoch£º546	 i:5 	 global-step:10925	 l-p:0.1558077186346054
epoch£º546	 i:6 	 global-step:10926	 l-p:0.1369926929473877
epoch£º546	 i:7 	 global-step:10927	 l-p:0.14085620641708374
epoch£º546	 i:8 	 global-step:10928	 l-p:0.11897622048854828
epoch£º546	 i:9 	 global-step:10929	 l-p:0.1296176314353943
====================================================================================================
====================================================================================================
====================================================================================================

epoch:547
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1832, 5.0881, 5.1075],
        [5.1832, 5.7096, 5.7845],
        [5.1832, 5.1822, 5.1831],
        [5.1832, 5.1832, 5.1832]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:547, step:0 
model_pd.l_p.mean(): 0.11979349702596664 
model_pd.l_d.mean(): -20.32850456237793 
model_pd.lagr.mean(): -20.208711624145508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4239], device='cuda:0')), ('power', tensor([-21.1477], device='cuda:0'))])
epoch£º547	 i:0 	 global-step:10940	 l-p:0.11979349702596664
epoch£º547	 i:1 	 global-step:10941	 l-p:0.15045584738254547
epoch£º547	 i:2 	 global-step:10942	 l-p:0.11391061544418335
epoch£º547	 i:3 	 global-step:10943	 l-p:0.09199997037649155
epoch£º547	 i:4 	 global-step:10944	 l-p:0.17048080265522003
epoch£º547	 i:5 	 global-step:10945	 l-p:0.15086714923381805
epoch£º547	 i:6 	 global-step:10946	 l-p:0.13780759274959564
epoch£º547	 i:7 	 global-step:10947	 l-p:0.03577718138694763
epoch£º547	 i:8 	 global-step:10948	 l-p:0.13479486107826233
epoch£º547	 i:9 	 global-step:10949	 l-p:0.11412469297647476
====================================================================================================
====================================================================================================
====================================================================================================

epoch:548
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4479e-01, 7.6032e-02,
         1.0000e+00, 3.9925e-02, 1.0000e+00, 5.2511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0856, 5.0853, 5.0856],
        [5.0856, 4.9959, 5.0231],
        [5.0856, 5.0538, 5.0772],
        [5.0856, 5.0393, 5.0688]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:548, step:0 
model_pd.l_p.mean(): 0.12365784496068954 
model_pd.l_d.mean(): -19.279077529907227 
model_pd.lagr.mean(): -19.155420303344727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5163], device='cuda:0')), ('power', tensor([-20.1743], device='cuda:0'))])
epoch£º548	 i:0 	 global-step:10960	 l-p:0.12365784496068954
epoch£º548	 i:1 	 global-step:10961	 l-p:0.12740713357925415
epoch£º548	 i:2 	 global-step:10962	 l-p:0.15225398540496826
epoch£º548	 i:3 	 global-step:10963	 l-p:0.12492915987968445
epoch£º548	 i:4 	 global-step:10964	 l-p:0.1800680309534073
epoch£º548	 i:5 	 global-step:10965	 l-p:0.1285538226366043
epoch£º548	 i:6 	 global-step:10966	 l-p:0.13502250611782074
epoch£º548	 i:7 	 global-step:10967	 l-p:0.100017249584198
epoch£º548	 i:8 	 global-step:10968	 l-p:0.12099310010671616
epoch£º548	 i:9 	 global-step:10969	 l-p:0.1007518544793129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:549
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9931, 5.3151, 5.2564],
        [4.9931, 4.9878, 4.9927],
        [4.9931, 4.9849, 4.9923],
        [4.9931, 4.8550, 4.8172]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:549, step:0 
model_pd.l_p.mean(): 0.11437949538230896 
model_pd.l_d.mean(): -20.42268180847168 
model_pd.lagr.mean(): -20.30830192565918 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4575], device='cuda:0')), ('power', tensor([-21.2784], device='cuda:0'))])
epoch£º549	 i:0 	 global-step:10980	 l-p:0.11437949538230896
epoch£º549	 i:1 	 global-step:10981	 l-p:0.14864063262939453
epoch£º549	 i:2 	 global-step:10982	 l-p:0.15914307534694672
epoch£º549	 i:3 	 global-step:10983	 l-p:0.1383386254310608
epoch£º549	 i:4 	 global-step:10984	 l-p:0.1027645468711853
epoch£º549	 i:5 	 global-step:10985	 l-p:0.116991326212883
epoch£º549	 i:6 	 global-step:10986	 l-p:0.17090405523777008
epoch£º549	 i:7 	 global-step:10987	 l-p:0.14652711153030396
epoch£º549	 i:8 	 global-step:10988	 l-p:0.14510975778102875
epoch£º549	 i:9 	 global-step:10989	 l-p:0.1436476856470108
====================================================================================================
====================================================================================================
====================================================================================================

epoch:550
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9511, 4.9164, 4.9417],
        [4.9511, 4.8103, 4.6861],
        [4.9511, 4.8043, 4.7344],
        [4.9511, 4.8944, 4.9277]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:550, step:0 
model_pd.l_p.mean(): 0.14426428079605103 
model_pd.l_d.mean(): -18.393512725830078 
model_pd.lagr.mean(): -18.249248504638672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5987], device='cuda:0')), ('power', tensor([-19.3576], device='cuda:0'))])
epoch£º550	 i:0 	 global-step:11000	 l-p:0.14426428079605103
epoch£º550	 i:1 	 global-step:11001	 l-p:0.11075809597969055
epoch£º550	 i:2 	 global-step:11002	 l-p:0.24961648881435394
epoch£º550	 i:3 	 global-step:11003	 l-p:0.11198486387729645
epoch£º550	 i:4 	 global-step:11004	 l-p:0.12127021700143814
epoch£º550	 i:5 	 global-step:11005	 l-p:0.13784758746623993
epoch£º550	 i:6 	 global-step:11006	 l-p:0.16915881633758545
epoch£º550	 i:7 	 global-step:11007	 l-p:0.13417774438858032
epoch£º550	 i:8 	 global-step:11008	 l-p:0.13732287287712097
epoch£º550	 i:9 	 global-step:11009	 l-p:0.12040138244628906
====================================================================================================
====================================================================================================
====================================================================================================

epoch:551
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0078, 4.8950, 4.9090],
        [5.0078, 5.0896, 4.9054],
        [5.0078, 5.5628, 5.6687],
        [5.0078, 5.0078, 5.0078]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:551, step:0 
model_pd.l_p.mean(): 0.13616041839122772 
model_pd.l_d.mean(): -18.040658950805664 
model_pd.lagr.mean(): -17.904499053955078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5963], device='cuda:0')), ('power', tensor([-18.9956], device='cuda:0'))])
epoch£º551	 i:0 	 global-step:11020	 l-p:0.13616041839122772
epoch£º551	 i:1 	 global-step:11021	 l-p:0.14694827795028687
epoch£º551	 i:2 	 global-step:11022	 l-p:0.11103776842355728
epoch£º551	 i:3 	 global-step:11023	 l-p:0.1668224334716797
epoch£º551	 i:4 	 global-step:11024	 l-p:0.12540993094444275
epoch£º551	 i:5 	 global-step:11025	 l-p:0.15888045728206635
epoch£º551	 i:6 	 global-step:11026	 l-p:0.14054226875305176
epoch£º551	 i:7 	 global-step:11027	 l-p:0.12678653001785278
epoch£º551	 i:8 	 global-step:11028	 l-p:0.11340770125389099
epoch£º551	 i:9 	 global-step:11029	 l-p:0.13590043783187866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:552
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9869, 4.8493, 4.7212],
        [4.9869, 4.9868, 4.9869],
        [4.9869, 4.9869, 4.9869],
        [4.9869, 4.9869, 4.9869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:552, step:0 
model_pd.l_p.mean(): 0.11007653921842575 
model_pd.l_d.mean(): -19.79231834411621 
model_pd.lagr.mean(): -19.682241439819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5224], device='cuda:0')), ('power', tensor([-20.7035], device='cuda:0'))])
epoch£º552	 i:0 	 global-step:11040	 l-p:0.11007653921842575
epoch£º552	 i:1 	 global-step:11041	 l-p:0.19005709886550903
epoch£º552	 i:2 	 global-step:11042	 l-p:0.16614003479480743
epoch£º552	 i:3 	 global-step:11043	 l-p:0.1464000642299652
epoch£º552	 i:4 	 global-step:11044	 l-p:0.13639913499355316
epoch£º552	 i:5 	 global-step:11045	 l-p:0.1175757348537445
epoch£º552	 i:6 	 global-step:11046	 l-p:0.125754714012146
epoch£º552	 i:7 	 global-step:11047	 l-p:0.11949542909860611
epoch£º552	 i:8 	 global-step:11048	 l-p:0.16620494425296783
epoch£º552	 i:9 	 global-step:11049	 l-p:0.18592949211597443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:553
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9556, 4.9556, 4.9556],
        [4.9556, 4.8149, 4.6828],
        [4.9556, 5.4524, 5.5165],
        [4.9556, 4.9094, 4.9399]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:553, step:0 
model_pd.l_p.mean(): 0.23094525933265686 
model_pd.l_d.mean(): -20.109209060668945 
model_pd.lagr.mean(): -19.878263473510742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5377], device='cuda:0')), ('power', tensor([-21.0421], device='cuda:0'))])
epoch£º553	 i:0 	 global-step:11060	 l-p:0.23094525933265686
epoch£º553	 i:1 	 global-step:11061	 l-p:0.11724715679883957
epoch£º553	 i:2 	 global-step:11062	 l-p:0.11184976994991302
epoch£º553	 i:3 	 global-step:11063	 l-p:0.1761433333158493
epoch£º553	 i:4 	 global-step:11064	 l-p:0.14122700691223145
epoch£º553	 i:5 	 global-step:11065	 l-p:0.08467650413513184
epoch£º553	 i:6 	 global-step:11066	 l-p:0.1344907432794571
epoch£º553	 i:7 	 global-step:11067	 l-p:0.13500501215457916
epoch£º553	 i:8 	 global-step:11068	 l-p:0.10820387303829193
epoch£º553	 i:9 	 global-step:11069	 l-p:0.18418321013450623
====================================================================================================
====================================================================================================
====================================================================================================

epoch:554
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0451, 5.0233, 5.0410],
        [5.0451, 5.0442, 5.0451],
        [5.0451, 5.0182, 5.0391],
        [5.0451, 4.9811, 4.7853]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:554, step:0 
model_pd.l_p.mean(): 0.08011061698198318 
model_pd.l_d.mean(): -19.866666793823242 
model_pd.lagr.mean(): -19.786556243896484 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4604], device='cuda:0')), ('power', tensor([-20.7150], device='cuda:0'))])
epoch£º554	 i:0 	 global-step:11080	 l-p:0.08011061698198318
epoch£º554	 i:1 	 global-step:11081	 l-p:0.12933391332626343
epoch£º554	 i:2 	 global-step:11082	 l-p:0.09155118465423584
epoch£º554	 i:3 	 global-step:11083	 l-p:0.10729680955410004
epoch£º554	 i:4 	 global-step:11084	 l-p:0.137238010764122
epoch£º554	 i:5 	 global-step:11085	 l-p:0.223200723528862
epoch£º554	 i:6 	 global-step:11086	 l-p:0.12105138599872589
epoch£º554	 i:7 	 global-step:11087	 l-p:0.11866271495819092
epoch£º554	 i:8 	 global-step:11088	 l-p:0.12202039361000061
epoch£º554	 i:9 	 global-step:11089	 l-p:0.13505123555660248
====================================================================================================
====================================================================================================
====================================================================================================

epoch:555
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1677, 5.1677, 5.1677],
        [5.1677, 5.0922, 5.1240],
        [5.1677, 5.1548, 5.1660],
        [5.1677, 5.0776, 5.1042]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:555, step:0 
model_pd.l_p.mean(): 0.1371891051530838 
model_pd.l_d.mean(): -20.254344940185547 
model_pd.lagr.mean(): -20.117155075073242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4266], device='cuda:0')), ('power', tensor([-21.0749], device='cuda:0'))])
epoch£º555	 i:0 	 global-step:11100	 l-p:0.1371891051530838
epoch£º555	 i:1 	 global-step:11101	 l-p:0.08155594766139984
epoch£º555	 i:2 	 global-step:11102	 l-p:0.10636118054389954
epoch£º555	 i:3 	 global-step:11103	 l-p:0.1639517992734909
epoch£º555	 i:4 	 global-step:11104	 l-p:0.1453864723443985
epoch£º555	 i:5 	 global-step:11105	 l-p:-0.1194678544998169
epoch£º555	 i:6 	 global-step:11106	 l-p:0.14011456072330475
epoch£º555	 i:7 	 global-step:11107	 l-p:0.10878438502550125
epoch£º555	 i:8 	 global-step:11108	 l-p:0.11109265685081482
epoch£º555	 i:9 	 global-step:11109	 l-p:0.1273886263370514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:556
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3042, 5.1853, 5.0890],
        [5.3042, 5.3042, 5.3042],
        [5.3042, 5.3800, 5.1966],
        [5.3042, 5.3024, 5.3041]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:556, step:0 
model_pd.l_p.mean(): 0.1562698781490326 
model_pd.l_d.mean(): -20.389015197753906 
model_pd.lagr.mean(): -20.232746124267578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3905], device='cuda:0')), ('power', tensor([-21.1748], device='cuda:0'))])
epoch£º556	 i:0 	 global-step:11120	 l-p:0.1562698781490326
epoch£º556	 i:1 	 global-step:11121	 l-p:0.19454017281532288
epoch£º556	 i:2 	 global-step:11122	 l-p:0.1767328530550003
epoch£º556	 i:3 	 global-step:11123	 l-p:0.11747773736715317
epoch£º556	 i:4 	 global-step:11124	 l-p:0.1331034153699875
epoch£º556	 i:5 	 global-step:11125	 l-p:0.14286330342292786
epoch£º556	 i:6 	 global-step:11126	 l-p:0.4948207437992096
epoch£º556	 i:7 	 global-step:11127	 l-p:0.1239919438958168
epoch£º556	 i:8 	 global-step:11128	 l-p:0.11263711005449295
epoch£º556	 i:9 	 global-step:11129	 l-p:0.13227514922618866
====================================================================================================
====================================================================================================
====================================================================================================

epoch:557
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2013, 5.1910, 5.2001],
        [5.2013, 5.1108, 4.9432],
        [5.2013, 5.1426, 5.1748],
        [5.2013, 5.2652, 5.0765]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:557, step:0 
model_pd.l_p.mean(): 0.06396567821502686 
model_pd.l_d.mean(): -19.793773651123047 
model_pd.lagr.mean(): -19.729808807373047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4874], device='cuda:0')), ('power', tensor([-20.6687], device='cuda:0'))])
epoch£º557	 i:0 	 global-step:11140	 l-p:0.06396567821502686
epoch£º557	 i:1 	 global-step:11141	 l-p:0.12761087715625763
epoch£º557	 i:2 	 global-step:11142	 l-p:-0.9063601493835449
epoch£º557	 i:3 	 global-step:11143	 l-p:0.12794291973114014
epoch£º557	 i:4 	 global-step:11144	 l-p:0.12670469284057617
epoch£º557	 i:5 	 global-step:11145	 l-p:0.13245545327663422
epoch£º557	 i:6 	 global-step:11146	 l-p:0.3411751985549927
epoch£º557	 i:7 	 global-step:11147	 l-p:0.13629548251628876
epoch£º557	 i:8 	 global-step:11148	 l-p:0.11638368666172028
epoch£º557	 i:9 	 global-step:11149	 l-p:0.120612733066082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:558
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0688, 4.9256, 4.8439],
        [5.0688, 5.0700, 4.8663],
        [5.0688, 5.0391, 5.0617],
        [5.0688, 5.0618, 5.0682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:558, step:0 
model_pd.l_p.mean(): 0.08358091115951538 
model_pd.l_d.mean(): -19.75863265991211 
model_pd.lagr.mean(): -19.675052642822266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4803], device='cuda:0')), ('power', tensor([-20.6256], device='cuda:0'))])
epoch£º558	 i:0 	 global-step:11160	 l-p:0.08358091115951538
epoch£º558	 i:1 	 global-step:11161	 l-p:0.04580424726009369
epoch£º558	 i:2 	 global-step:11162	 l-p:0.12646381556987762
epoch£º558	 i:3 	 global-step:11163	 l-p:0.13369882106781006
epoch£º558	 i:4 	 global-step:11164	 l-p:0.11510328203439713
epoch£º558	 i:5 	 global-step:11165	 l-p:0.13794805109500885
epoch£º558	 i:6 	 global-step:11166	 l-p:0.1249861940741539
epoch£º558	 i:7 	 global-step:11167	 l-p:0.16151383519172668
epoch£º558	 i:8 	 global-step:11168	 l-p:0.14581505954265594
epoch£º558	 i:9 	 global-step:11169	 l-p:0.1533832550048828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:559
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0044, 4.9254, 4.7258],
        [5.0044, 4.9949, 5.0034],
        [5.0044, 5.0043, 5.0044],
        [5.0044, 5.0036, 5.0043]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:559, step:0 
model_pd.l_p.mean(): 0.14452588558197021 
model_pd.l_d.mean(): -19.73452377319336 
model_pd.lagr.mean(): -19.589998245239258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5641], device='cuda:0')), ('power', tensor([-20.6878], device='cuda:0'))])
epoch£º559	 i:0 	 global-step:11180	 l-p:0.14452588558197021
epoch£º559	 i:1 	 global-step:11181	 l-p:0.15110042691230774
epoch£º559	 i:2 	 global-step:11182	 l-p:0.10686581581830978
epoch£º559	 i:3 	 global-step:11183	 l-p:0.16292405128479004
epoch£º559	 i:4 	 global-step:11184	 l-p:0.17437084019184113
epoch£º559	 i:5 	 global-step:11185	 l-p:0.14121375977993011
epoch£º559	 i:6 	 global-step:11186	 l-p:0.16487710177898407
epoch£º559	 i:7 	 global-step:11187	 l-p:0.1536353975534439
epoch£º559	 i:8 	 global-step:11188	 l-p:0.04985523223876953
epoch£º559	 i:9 	 global-step:11189	 l-p:0.11882674694061279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:560
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9770,  0.9695,  1.0000,  0.9620,
          1.0000,  0.9923, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9814,  0.9752,  1.0000,  0.9691,
          1.0000,  0.9938, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228]], device='cuda:0')
 pt:tensor([[5.0883, 5.7179, 5.8740],
        [5.0883, 4.9578, 4.8173],
        [5.0883, 5.7246, 5.8860],
        [5.0883, 5.1133, 4.9115]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:560, step:0 
model_pd.l_p.mean(): 0.11795243620872498 
model_pd.l_d.mean(): -18.558191299438477 
model_pd.lagr.mean(): -18.44023895263672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5439], device='cuda:0')), ('power', tensor([-19.4685], device='cuda:0'))])
epoch£º560	 i:0 	 global-step:11200	 l-p:0.11795243620872498
epoch£º560	 i:1 	 global-step:11201	 l-p:0.24079810082912445
epoch£º560	 i:2 	 global-step:11202	 l-p:0.07828397303819656
epoch£º560	 i:3 	 global-step:11203	 l-p:0.5845664143562317
epoch£º560	 i:4 	 global-step:11204	 l-p:0.16189265251159668
epoch£º560	 i:5 	 global-step:11205	 l-p:0.11139168590307236
epoch£º560	 i:6 	 global-step:11206	 l-p:0.13141889870166779
epoch£º560	 i:7 	 global-step:11207	 l-p:0.18121522665023804
epoch£º560	 i:8 	 global-step:11208	 l-p:0.14224879443645477
epoch£º560	 i:9 	 global-step:11209	 l-p:0.11826138943433762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:561
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3620, 5.3620, 5.3620],
        [5.3620, 5.2535, 5.1289],
        [5.3620, 5.3530, 5.3610],
        [5.3620, 5.4567, 5.2765]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:561, step:0 
model_pd.l_p.mean(): 0.15498162806034088 
model_pd.l_d.mean(): -19.51214599609375 
model_pd.lagr.mean(): -19.35716438293457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4014], device='cuda:0')), ('power', tensor([-20.2928], device='cuda:0'))])
epoch£º561	 i:0 	 global-step:11220	 l-p:0.15498162806034088
epoch£º561	 i:1 	 global-step:11221	 l-p:0.12978436052799225
epoch£º561	 i:2 	 global-step:11222	 l-p:0.13465498387813568
epoch£º561	 i:3 	 global-step:11223	 l-p:0.11227817088365555
epoch£º561	 i:4 	 global-step:11224	 l-p:0.12118756026029587
epoch£º561	 i:5 	 global-step:11225	 l-p:0.1166234090924263
epoch£º561	 i:6 	 global-step:11226	 l-p:0.06035750359296799
epoch£º561	 i:7 	 global-step:11227	 l-p:0.1266944855451584
epoch£º561	 i:8 	 global-step:11228	 l-p:0.14206261932849884
epoch£º561	 i:9 	 global-step:11229	 l-p:0.13873864710330963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:562
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3432, 5.3060, 5.3318],
        [5.3432, 5.3427, 5.3432],
        [5.3432, 5.3431, 5.3432],
        [5.3432, 5.2952, 5.1112]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:562, step:0 
model_pd.l_p.mean(): 0.1261458545923233 
model_pd.l_d.mean(): -19.737167358398438 
model_pd.lagr.mean(): -19.611021041870117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3913], device='cuda:0')), ('power', tensor([-20.5115], device='cuda:0'))])
epoch£º562	 i:0 	 global-step:11240	 l-p:0.1261458545923233
epoch£º562	 i:1 	 global-step:11241	 l-p:0.12193313241004944
epoch£º562	 i:2 	 global-step:11242	 l-p:0.1359032243490219
epoch£º562	 i:3 	 global-step:11243	 l-p:0.41071006655693054
epoch£º562	 i:4 	 global-step:11244	 l-p:0.10705073922872543
epoch£º562	 i:5 	 global-step:11245	 l-p:0.13080081343650818
epoch£º562	 i:6 	 global-step:11246	 l-p:0.1377059519290924
epoch£º562	 i:7 	 global-step:11247	 l-p:0.13447429239749908
epoch£º562	 i:8 	 global-step:11248	 l-p:0.018154066056013107
epoch£º562	 i:9 	 global-step:11249	 l-p:0.08552340418100357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:563
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0410, 5.0336, 5.0403],
        [5.0410, 4.9380, 4.9649],
        [5.0410, 4.9680, 5.0037],
        [5.0410, 5.0070, 5.0321]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:563, step:0 
model_pd.l_p.mean(): 0.06283807754516602 
model_pd.l_d.mean(): -20.42877960205078 
model_pd.lagr.mean(): -20.365942001342773 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4649], device='cuda:0')), ('power', tensor([-21.2923], device='cuda:0'))])
epoch£º563	 i:0 	 global-step:11260	 l-p:0.06283807754516602
epoch£º563	 i:1 	 global-step:11261	 l-p:0.1478082537651062
epoch£º563	 i:2 	 global-step:11262	 l-p:0.125986248254776
epoch£º563	 i:3 	 global-step:11263	 l-p:0.17893840372562408
epoch£º563	 i:4 	 global-step:11264	 l-p:0.09905726462602615
epoch£º563	 i:5 	 global-step:11265	 l-p:0.1583932340145111
epoch£º563	 i:6 	 global-step:11266	 l-p:0.14102160930633545
epoch£º563	 i:7 	 global-step:11267	 l-p:0.15015283226966858
epoch£º563	 i:8 	 global-step:11268	 l-p:0.11469531804323196
epoch£º563	 i:9 	 global-step:11269	 l-p:0.12898193299770355
====================================================================================================
====================================================================================================
====================================================================================================

epoch:564
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4000,  0.2948,  1.0000,  0.2172,
          1.0000,  0.7368, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1715,  0.0953,  1.0000,  0.0530,
          1.0000,  0.5556, 31.6228]], device='cuda:0')
 pt:tensor([[5.0183, 4.9010, 4.7225],
        [5.0183, 5.0253, 4.8163],
        [5.0183, 4.8659, 4.7843],
        [5.0183, 4.9023, 4.9204]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:564, step:0 
model_pd.l_p.mean(): 0.07755205780267715 
model_pd.l_d.mean(): -19.542434692382812 
model_pd.lagr.mean(): -19.464881896972656 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4898], device='cuda:0')), ('power', tensor([-20.4152], device='cuda:0'))])
epoch£º564	 i:0 	 global-step:11280	 l-p:0.07755205780267715
epoch£º564	 i:1 	 global-step:11281	 l-p:0.22741219401359558
epoch£º564	 i:2 	 global-step:11282	 l-p:0.1347801685333252
epoch£º564	 i:3 	 global-step:11283	 l-p:0.14173050224781036
epoch£º564	 i:4 	 global-step:11284	 l-p:0.19522100687026978
epoch£º564	 i:5 	 global-step:11285	 l-p:0.10730040073394775
epoch£º564	 i:6 	 global-step:11286	 l-p:0.13114283978939056
epoch£º564	 i:7 	 global-step:11287	 l-p:0.12823335826396942
epoch£º564	 i:8 	 global-step:11288	 l-p:0.13240540027618408
epoch£º564	 i:9 	 global-step:11289	 l-p:0.12099038809537888
====================================================================================================
====================================================================================================
====================================================================================================

epoch:565
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8974, 4.8921, 4.8970],
        [4.8974, 4.7892, 4.8190],
        [4.8974, 5.3847, 5.4407],
        [4.8974, 5.3898, 5.4497]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:565, step:0 
model_pd.l_p.mean(): 0.033702705055475235 
model_pd.l_d.mean(): -20.010623931884766 
model_pd.lagr.mean(): -19.97692108154297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5476], device='cuda:0')), ('power', tensor([-20.9520], device='cuda:0'))])
epoch£º565	 i:0 	 global-step:11300	 l-p:0.033702705055475235
epoch£º565	 i:1 	 global-step:11301	 l-p:0.21943067014217377
epoch£º565	 i:2 	 global-step:11302	 l-p:-0.07580877840518951
epoch£º565	 i:3 	 global-step:11303	 l-p:0.1566496193408966
epoch£º565	 i:4 	 global-step:11304	 l-p:0.4011169970035553
epoch£º565	 i:5 	 global-step:11305	 l-p:0.20673900842666626
epoch£º565	 i:6 	 global-step:11306	 l-p:0.16536936163902283
epoch£º565	 i:7 	 global-step:11307	 l-p:0.18182815611362457
epoch£º565	 i:8 	 global-step:11308	 l-p:0.097190260887146
epoch£º565	 i:9 	 global-step:11309	 l-p:0.10497016459703445
====================================================================================================
====================================================================================================
====================================================================================================

epoch:566
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0317, 5.0181, 5.0299],
        [5.0317, 5.0300, 5.0316],
        [5.0317, 4.9315, 4.9611],
        [5.0317, 4.9809, 5.0132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:566, step:0 
model_pd.l_p.mean(): 0.06883666664361954 
model_pd.l_d.mean(): -20.192901611328125 
model_pd.lagr.mean(): -20.124065399169922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4839], device='cuda:0')), ('power', tensor([-21.0716], device='cuda:0'))])
epoch£º566	 i:0 	 global-step:11320	 l-p:0.06883666664361954
epoch£º566	 i:1 	 global-step:11321	 l-p:0.11903686076402664
epoch£º566	 i:2 	 global-step:11322	 l-p:0.10460361838340759
epoch£º566	 i:3 	 global-step:11323	 l-p:0.16041408479213715
epoch£º566	 i:4 	 global-step:11324	 l-p:0.16259638965129852
epoch£º566	 i:5 	 global-step:11325	 l-p:0.1162947565317154
epoch£º566	 i:6 	 global-step:11326	 l-p:0.1946256458759308
epoch£º566	 i:7 	 global-step:11327	 l-p:-0.04167194291949272
epoch£º566	 i:8 	 global-step:11328	 l-p:0.08083736896514893
epoch£º566	 i:9 	 global-step:11329	 l-p:0.12767061591148376
====================================================================================================
====================================================================================================
====================================================================================================

epoch:567
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228]], device='cuda:0')
 pt:tensor([[5.2282, 5.8506, 5.9905],
        [5.2282, 5.7825, 5.8699],
        [5.2282, 5.1128, 5.1188],
        [5.2282, 5.5908, 5.5426]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:567, step:0 
model_pd.l_p.mean(): 0.13682128489017487 
model_pd.l_d.mean(): -18.37995719909668 
model_pd.lagr.mean(): -18.243135452270508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5390], device='cuda:0')), ('power', tensor([-19.2819], device='cuda:0'))])
epoch£º567	 i:0 	 global-step:11340	 l-p:0.13682128489017487
epoch£º567	 i:1 	 global-step:11341	 l-p:0.1316581517457962
epoch£º567	 i:2 	 global-step:11342	 l-p:0.14715585112571716
epoch£º567	 i:3 	 global-step:11343	 l-p:0.1269218772649765
epoch£º567	 i:4 	 global-step:11344	 l-p:0.6358543038368225
epoch£º567	 i:5 	 global-step:11345	 l-p:0.12320657819509506
epoch£º567	 i:6 	 global-step:11346	 l-p:0.32093745470046997
epoch£º567	 i:7 	 global-step:11347	 l-p:0.13483211398124695
epoch£º567	 i:8 	 global-step:11348	 l-p:0.12877172231674194
epoch£º567	 i:9 	 global-step:11349	 l-p:-0.17921553552150726
====================================================================================================
====================================================================================================
====================================================================================================

epoch:568
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2559, 5.1358, 4.9996],
        [5.2559, 5.2469, 5.2550],
        [5.2559, 5.4722, 5.3378],
        [5.2559, 5.1452, 4.9933]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:568, step:0 
model_pd.l_p.mean(): -0.10417894273996353 
model_pd.l_d.mean(): -19.878049850463867 
model_pd.lagr.mean(): -19.982229232788086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4558], device='cuda:0')), ('power', tensor([-20.7218], device='cuda:0'))])
epoch£º568	 i:0 	 global-step:11360	 l-p:-0.10417894273996353
epoch£º568	 i:1 	 global-step:11361	 l-p:0.1425822228193283
epoch£º568	 i:2 	 global-step:11362	 l-p:0.11108264327049255
epoch£º568	 i:3 	 global-step:11363	 l-p:0.1118219643831253
epoch£º568	 i:4 	 global-step:11364	 l-p:0.19844749569892883
epoch£º568	 i:5 	 global-step:11365	 l-p:0.11663579940795898
epoch£º568	 i:6 	 global-step:11366	 l-p:0.11484935134649277
epoch£º568	 i:7 	 global-step:11367	 l-p:0.10209645330905914
epoch£º568	 i:8 	 global-step:11368	 l-p:0.21751768887043
epoch£º568	 i:9 	 global-step:11369	 l-p:0.06697443127632141
====================================================================================================
====================================================================================================
====================================================================================================

epoch:569
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1819, 5.1819, 5.1819],
        [5.1819, 5.1501, 5.1738],
        [5.1819, 5.7675, 5.8810],
        [5.1819, 5.1819, 5.1819]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:569, step:0 
model_pd.l_p.mean(): 0.12099986523389816 
model_pd.l_d.mean(): -19.40523338317871 
model_pd.lagr.mean(): -19.28423309326172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4741], device='cuda:0')), ('power', tensor([-20.2592], device='cuda:0'))])
epoch£º569	 i:0 	 global-step:11380	 l-p:0.12099986523389816
epoch£º569	 i:1 	 global-step:11381	 l-p:0.1581842601299286
epoch£º569	 i:2 	 global-step:11382	 l-p:0.09000308066606522
epoch£º569	 i:3 	 global-step:11383	 l-p:0.12145020812749863
epoch£º569	 i:4 	 global-step:11384	 l-p:0.16588382422924042
epoch£º569	 i:5 	 global-step:11385	 l-p:0.12645286321640015
epoch£º569	 i:6 	 global-step:11386	 l-p:0.12810209393501282
epoch£º569	 i:7 	 global-step:11387	 l-p:0.1442393809556961
epoch£º569	 i:8 	 global-step:11388	 l-p:0.16374275088310242
epoch£º569	 i:9 	 global-step:11389	 l-p:0.12269395589828491
====================================================================================================
====================================================================================================
====================================================================================================

epoch:570
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1811, 5.1708, 5.1799],
        [5.1811, 5.1811, 5.1811],
        [5.1811, 5.0462, 4.9246],
        [5.1811, 5.4889, 5.4075]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:570, step:0 
model_pd.l_p.mean(): 0.12838619947433472 
model_pd.l_d.mean(): -19.912120819091797 
model_pd.lagr.mean(): -19.783735275268555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4464], device='cuda:0')), ('power', tensor([-20.7468], device='cuda:0'))])
epoch£º570	 i:0 	 global-step:11400	 l-p:0.12838619947433472
epoch£º570	 i:1 	 global-step:11401	 l-p:0.13414859771728516
epoch£º570	 i:2 	 global-step:11402	 l-p:0.18164274096488953
epoch£º570	 i:3 	 global-step:11403	 l-p:-0.24035388231277466
epoch£º570	 i:4 	 global-step:11404	 l-p:0.13019989430904388
epoch£º570	 i:5 	 global-step:11405	 l-p:0.1521228849887848
epoch£º570	 i:6 	 global-step:11406	 l-p:0.136723592877388
epoch£º570	 i:7 	 global-step:11407	 l-p:0.13432574272155762
epoch£º570	 i:8 	 global-step:11408	 l-p:0.10045114904642105
epoch£º570	 i:9 	 global-step:11409	 l-p:0.17522567510604858
====================================================================================================
====================================================================================================
====================================================================================================

epoch:571
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0099, 4.8567, 4.8086],
        [5.0099, 5.0099, 5.0099],
        [5.0099, 4.9787, 5.0025],
        [5.0099, 5.0044, 5.0096]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:571, step:0 
model_pd.l_p.mean(): 0.1920997053384781 
model_pd.l_d.mean(): -19.222156524658203 
model_pd.lagr.mean(): -19.03005599975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5170], device='cuda:0')), ('power', tensor([-20.1171], device='cuda:0'))])
epoch£º571	 i:0 	 global-step:11420	 l-p:0.1920997053384781
epoch£º571	 i:1 	 global-step:11421	 l-p:0.07991240918636322
epoch£º571	 i:2 	 global-step:11422	 l-p:0.1175195500254631
epoch£º571	 i:3 	 global-step:11423	 l-p:0.12815095484256744
epoch£º571	 i:4 	 global-step:11424	 l-p:0.11183752119541168
epoch£º571	 i:5 	 global-step:11425	 l-p:0.12830750644207
epoch£º571	 i:6 	 global-step:11426	 l-p:0.17211560904979706
epoch£º571	 i:7 	 global-step:11427	 l-p:0.11020740121603012
epoch£º571	 i:8 	 global-step:11428	 l-p:0.1670820564031601
epoch£º571	 i:9 	 global-step:11429	 l-p:0.12357940524816513
====================================================================================================
====================================================================================================
====================================================================================================

epoch:572
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0255, 5.0245, 5.0255],
        [5.0255, 5.6191, 5.7469],
        [5.0255, 5.5237, 5.5789],
        [5.0255, 5.0126, 5.0239]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:572, step:0 
model_pd.l_p.mean(): 0.12931090593338013 
model_pd.l_d.mean(): -19.427518844604492 
model_pd.lagr.mean(): -19.298208236694336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5356], device='cuda:0')), ('power', tensor([-20.3455], device='cuda:0'))])
epoch£º572	 i:0 	 global-step:11440	 l-p:0.12931090593338013
epoch£º572	 i:1 	 global-step:11441	 l-p:0.10991951078176498
epoch£º572	 i:2 	 global-step:11442	 l-p:0.11801290512084961
epoch£º572	 i:3 	 global-step:11443	 l-p:0.15282918512821198
epoch£º572	 i:4 	 global-step:11444	 l-p:0.15826840698719025
epoch£º572	 i:5 	 global-step:11445	 l-p:0.13550543785095215
epoch£º572	 i:6 	 global-step:11446	 l-p:0.35321733355522156
epoch£º572	 i:7 	 global-step:11447	 l-p:0.1934187114238739
epoch£º572	 i:8 	 global-step:11448	 l-p:0.1679193079471588
epoch£º572	 i:9 	 global-step:11449	 l-p:0.18027743697166443
====================================================================================================
====================================================================================================
====================================================================================================

epoch:573
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9984, 4.9401, 4.9752],
        [4.9984, 4.8540, 4.8402],
        [4.9984, 4.9179, 4.9552],
        [4.9984, 4.9981, 4.9984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:573, step:0 
model_pd.l_p.mean(): 0.16814269125461578 
model_pd.l_d.mean(): -19.457368850708008 
model_pd.lagr.mean(): -19.289226531982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4923], device='cuda:0')), ('power', tensor([-20.3311], device='cuda:0'))])
epoch£º573	 i:0 	 global-step:11460	 l-p:0.16814269125461578
epoch£º573	 i:1 	 global-step:11461	 l-p:0.15467597544193268
epoch£º573	 i:2 	 global-step:11462	 l-p:0.0955933928489685
epoch£º573	 i:3 	 global-step:11463	 l-p:0.08925214409828186
epoch£º573	 i:4 	 global-step:11464	 l-p:0.1037636548280716
epoch£º573	 i:5 	 global-step:11465	 l-p:-0.6442526578903198
epoch£º573	 i:6 	 global-step:11466	 l-p:0.1990991234779358
epoch£º573	 i:7 	 global-step:11467	 l-p:0.12560461461544037
epoch£º573	 i:8 	 global-step:11468	 l-p:0.15116840600967407
epoch£º573	 i:9 	 global-step:11469	 l-p:0.10130859166383743
====================================================================================================
====================================================================================================
====================================================================================================

epoch:574
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2217, 5.0833, 4.9741],
        [5.2217, 5.3020, 5.1099],
        [5.2217, 5.2217, 5.2217],
        [5.2217, 5.2178, 5.2215]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:574, step:0 
model_pd.l_p.mean(): 0.12717461585998535 
model_pd.l_d.mean(): -20.134206771850586 
model_pd.lagr.mean(): -20.00703239440918 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4198], device='cuda:0')), ('power', tensor([-20.9455], device='cuda:0'))])
epoch£º574	 i:0 	 global-step:11480	 l-p:0.12717461585998535
epoch£º574	 i:1 	 global-step:11481	 l-p:0.12831689417362213
epoch£º574	 i:2 	 global-step:11482	 l-p:0.1692524552345276
epoch£º574	 i:3 	 global-step:11483	 l-p:0.060996878892183304
epoch£º574	 i:4 	 global-step:11484	 l-p:0.33582961559295654
epoch£º574	 i:5 	 global-step:11485	 l-p:0.13926760852336884
epoch£º574	 i:6 	 global-step:11486	 l-p:0.07302284240722656
epoch£º574	 i:7 	 global-step:11487	 l-p:0.04709376394748688
epoch£º574	 i:8 	 global-step:11488	 l-p:0.13596808910369873
epoch£º574	 i:9 	 global-step:11489	 l-p:0.10935220122337341
====================================================================================================
====================================================================================================
====================================================================================================

epoch:575
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2451, 5.2334, 5.2437],
        [5.2451, 5.1180, 5.1087],
        [5.2451, 5.2451, 5.2451],
        [5.2451, 5.2319, 5.2434]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:575, step:0 
model_pd.l_p.mean(): -0.3078293204307556 
model_pd.l_d.mean(): -20.148984909057617 
model_pd.lagr.mean(): -20.45681381225586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4486], device='cuda:0')), ('power', tensor([-20.9904], device='cuda:0'))])
epoch£º575	 i:0 	 global-step:11500	 l-p:-0.3078293204307556
epoch£º575	 i:1 	 global-step:11501	 l-p:-0.09233127534389496
epoch£º575	 i:2 	 global-step:11502	 l-p:-0.2101380079984665
epoch£º575	 i:3 	 global-step:11503	 l-p:0.10655535757541656
epoch£º575	 i:4 	 global-step:11504	 l-p:0.1382032334804535
epoch£º575	 i:5 	 global-step:11505	 l-p:0.12104381620883942
epoch£º575	 i:6 	 global-step:11506	 l-p:0.11862180382013321
epoch£º575	 i:7 	 global-step:11507	 l-p:0.13152159750461578
epoch£º575	 i:8 	 global-step:11508	 l-p:0.16490501165390015
epoch£º575	 i:9 	 global-step:11509	 l-p:0.1316457837820053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:576
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2741, 5.2738, 5.2741],
        [5.2741, 5.2707, 5.2739],
        [5.2741, 5.8182, 5.8930],
        [5.2741, 5.2304, 5.0299]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:576, step:0 
model_pd.l_p.mean(): 0.2888495624065399 
model_pd.l_d.mean(): -20.098896026611328 
model_pd.lagr.mean(): -19.810047149658203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4281], device='cuda:0')), ('power', tensor([-20.9181], device='cuda:0'))])
epoch£º576	 i:0 	 global-step:11520	 l-p:0.2888495624065399
epoch£º576	 i:1 	 global-step:11521	 l-p:0.1289202719926834
epoch£º576	 i:2 	 global-step:11522	 l-p:0.1255425363779068
epoch£º576	 i:3 	 global-step:11523	 l-p:-0.07627665996551514
epoch£º576	 i:4 	 global-step:11524	 l-p:0.11544905602931976
epoch£º576	 i:5 	 global-step:11525	 l-p:0.1200418472290039
epoch£º576	 i:6 	 global-step:11526	 l-p:0.038557007908821106
epoch£º576	 i:7 	 global-step:11527	 l-p:0.1213889792561531
epoch£º576	 i:8 	 global-step:11528	 l-p:0.19404006004333496
epoch£º576	 i:9 	 global-step:11529	 l-p:0.13467605412006378
====================================================================================================
====================================================================================================
====================================================================================================

epoch:577
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2374, 5.1419, 5.1700],
        [5.2374, 5.2374, 5.2374],
        [5.2374, 5.3422, 5.1574],
        [5.2374, 5.2374, 5.2374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:577, step:0 
model_pd.l_p.mean(): -0.19869452714920044 
model_pd.l_d.mean(): -20.4703369140625 
model_pd.lagr.mean(): -20.669031143188477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4009], device='cuda:0')), ('power', tensor([-21.2683], device='cuda:0'))])
epoch£º577	 i:0 	 global-step:11540	 l-p:-0.19869452714920044
epoch£º577	 i:1 	 global-step:11541	 l-p:0.14157994091510773
epoch£º577	 i:2 	 global-step:11542	 l-p:0.11747834831476212
epoch£º577	 i:3 	 global-step:11543	 l-p:0.1097356453537941
epoch£º577	 i:4 	 global-step:11544	 l-p:0.18198448419570923
epoch£º577	 i:5 	 global-step:11545	 l-p:0.12963180243968964
epoch£º577	 i:6 	 global-step:11546	 l-p:0.06887120753526688
epoch£º577	 i:7 	 global-step:11547	 l-p:0.11953154951334
epoch£º577	 i:8 	 global-step:11548	 l-p:0.1376098245382309
epoch£º577	 i:9 	 global-step:11549	 l-p:0.19158832728862762
====================================================================================================
====================================================================================================
====================================================================================================

epoch:578
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2191, 5.2191, 5.2191],
        [5.2191, 5.2963, 5.1021],
        [5.2191, 5.0802, 5.0461],
        [5.2191, 5.0927, 4.9437]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:578, step:0 
model_pd.l_p.mean(): 0.23146411776542664 
model_pd.l_d.mean(): -20.326425552368164 
model_pd.lagr.mean(): -20.094961166381836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4108], device='cuda:0')), ('power', tensor([-21.1320], device='cuda:0'))])
epoch£º578	 i:0 	 global-step:11560	 l-p:0.23146411776542664
epoch£º578	 i:1 	 global-step:11561	 l-p:0.05603119730949402
epoch£º578	 i:2 	 global-step:11562	 l-p:0.12182559072971344
epoch£º578	 i:3 	 global-step:11563	 l-p:0.1020730584859848
epoch£º578	 i:4 	 global-step:11564	 l-p:0.17479351162910461
epoch£º578	 i:5 	 global-step:11565	 l-p:0.19224074482917786
epoch£º578	 i:6 	 global-step:11566	 l-p:0.11831654608249664
epoch£º578	 i:7 	 global-step:11567	 l-p:0.11557643860578537
epoch£º578	 i:8 	 global-step:11568	 l-p:0.12384496629238129
epoch£º578	 i:9 	 global-step:11569	 l-p:0.13996082544326782
====================================================================================================
====================================================================================================
====================================================================================================

epoch:579
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.3702, 6.0331, 6.1936],
        [5.3702, 5.3010, 5.3340],
        [5.3702, 5.2549, 5.1167],
        [5.3702, 5.3193, 5.3504]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:579, step:0 
model_pd.l_p.mean(): 0.12087380141019821 
model_pd.l_d.mean(): -20.195608139038086 
model_pd.lagr.mean(): -20.07473373413086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3802], device='cuda:0')), ('power', tensor([-20.9670], device='cuda:0'))])
epoch£º579	 i:0 	 global-step:11580	 l-p:0.12087380141019821
epoch£º579	 i:1 	 global-step:11581	 l-p:0.15594680607318878
epoch£º579	 i:2 	 global-step:11582	 l-p:0.10961702466011047
epoch£º579	 i:3 	 global-step:11583	 l-p:0.14524807035923004
epoch£º579	 i:4 	 global-step:11584	 l-p:0.18894270062446594
epoch£º579	 i:5 	 global-step:11585	 l-p:0.12921294569969177
epoch£º579	 i:6 	 global-step:11586	 l-p:0.14439761638641357
epoch£º579	 i:7 	 global-step:11587	 l-p:0.09973019361495972
epoch£º579	 i:8 	 global-step:11588	 l-p:0.13174720108509064
epoch£º579	 i:9 	 global-step:11589	 l-p:0.15636713802814484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:580
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2471, 5.2272, 5.2436],
        [5.2471, 5.2415, 5.2467],
        [5.2471, 5.2059, 5.0007],
        [5.2471, 5.2217, 5.2418]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:580, step:0 
model_pd.l_p.mean(): 0.017343200743198395 
model_pd.l_d.mean(): -20.72299575805664 
model_pd.lagr.mean(): -20.705652236938477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3540], device='cuda:0')), ('power', tensor([-21.4771], device='cuda:0'))])
epoch£º580	 i:0 	 global-step:11600	 l-p:0.017343200743198395
epoch£º580	 i:1 	 global-step:11601	 l-p:0.13190650939941406
epoch£º580	 i:2 	 global-step:11602	 l-p:0.12474585324525833
epoch£º580	 i:3 	 global-step:11603	 l-p:0.08237139880657196
epoch£º580	 i:4 	 global-step:11604	 l-p:0.36769941449165344
epoch£º580	 i:5 	 global-step:11605	 l-p:0.15214607119560242
epoch£º580	 i:6 	 global-step:11606	 l-p:0.1527153104543686
epoch£º580	 i:7 	 global-step:11607	 l-p:0.26919323205947876
epoch£º580	 i:8 	 global-step:11608	 l-p:0.12336280196905136
epoch£º580	 i:9 	 global-step:11609	 l-p:0.11994414776563644
====================================================================================================
====================================================================================================
====================================================================================================

epoch:581
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1104, 5.1104, 5.1104],
        [5.1104, 5.3147, 5.1731],
        [5.1104, 4.9614, 4.8328],
        [5.1104, 5.0152, 5.0482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:581, step:0 
model_pd.l_p.mean(): 0.15726029872894287 
model_pd.l_d.mean(): -20.111244201660156 
model_pd.lagr.mean(): -19.953983306884766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4811], device='cuda:0')), ('power', tensor([-20.9856], device='cuda:0'))])
epoch£º581	 i:0 	 global-step:11620	 l-p:0.15726029872894287
epoch£º581	 i:1 	 global-step:11621	 l-p:0.09713164716959
epoch£º581	 i:2 	 global-step:11622	 l-p:0.12438987195491791
epoch£º581	 i:3 	 global-step:11623	 l-p:0.13303638994693756
epoch£º581	 i:4 	 global-step:11624	 l-p:0.13596659898757935
epoch£º581	 i:5 	 global-step:11625	 l-p:0.1106012687087059
epoch£º581	 i:6 	 global-step:11626	 l-p:0.10100097209215164
epoch£º581	 i:7 	 global-step:11627	 l-p:0.21276026964187622
epoch£º581	 i:8 	 global-step:11628	 l-p:0.15402500331401825
epoch£º581	 i:9 	 global-step:11629	 l-p:0.14844995737075806
====================================================================================================
====================================================================================================
====================================================================================================

epoch:582
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9795, 4.9274, 4.9612],
        [4.9795, 5.4549, 5.4932],
        [4.9795, 4.9791, 4.9795],
        [4.9795, 4.9684, 4.9783]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:582, step:0 
model_pd.l_p.mean(): 0.16788047552108765 
model_pd.l_d.mean(): -18.35658836364746 
model_pd.lagr.mean(): -18.18870735168457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5869], device='cuda:0')), ('power', tensor([-19.3077], device='cuda:0'))])
epoch£º582	 i:0 	 global-step:11640	 l-p:0.16788047552108765
epoch£º582	 i:1 	 global-step:11641	 l-p:0.17169342935085297
epoch£º582	 i:2 	 global-step:11642	 l-p:0.12690110504627228
epoch£º582	 i:3 	 global-step:11643	 l-p:0.14433185756206512
epoch£º582	 i:4 	 global-step:11644	 l-p:0.16333824396133423
epoch£º582	 i:5 	 global-step:11645	 l-p:0.18961505591869354
epoch£º582	 i:6 	 global-step:11646	 l-p:0.11336097121238708
epoch£º582	 i:7 	 global-step:11647	 l-p:0.12379714101552963
epoch£º582	 i:8 	 global-step:11648	 l-p:0.12186185270547867
epoch£º582	 i:9 	 global-step:11649	 l-p:0.13955582678318024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:583
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0302, 4.9954, 5.0213],
        [5.0302, 5.1225, 4.9291],
        [5.0302, 5.1442, 4.9595],
        [5.0302, 4.8831, 4.7209]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:583, step:0 
model_pd.l_p.mean(): 0.13691508769989014 
model_pd.l_d.mean(): -20.29928970336914 
model_pd.lagr.mean(): -20.16237449645996 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4940], device='cuda:0')), ('power', tensor([-21.1905], device='cuda:0'))])
epoch£º583	 i:0 	 global-step:11660	 l-p:0.13691508769989014
epoch£º583	 i:1 	 global-step:11661	 l-p:0.14623567461967468
epoch£º583	 i:2 	 global-step:11662	 l-p:0.1322263926267624
epoch£º583	 i:3 	 global-step:11663	 l-p:0.10835525393486023
epoch£º583	 i:4 	 global-step:11664	 l-p:0.20007701218128204
epoch£º583	 i:5 	 global-step:11665	 l-p:0.1725630909204483
epoch£º583	 i:6 	 global-step:11666	 l-p:0.08942132443189621
epoch£º583	 i:7 	 global-step:11667	 l-p:0.12766429781913757
epoch£º583	 i:8 	 global-step:11668	 l-p:0.12507709860801697
epoch£º583	 i:9 	 global-step:11669	 l-p:0.17594440281391144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:584
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0251, 5.0039, 5.0214],
        [5.0251, 5.0251, 5.0251],
        [5.0251, 4.9693, 5.0041],
        [5.0251, 4.9584, 4.9957]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:584, step:0 
model_pd.l_p.mean(): 0.21179908514022827 
model_pd.l_d.mean(): -20.527759552001953 
model_pd.lagr.mean(): -20.315959930419922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4588], device='cuda:0')), ('power', tensor([-21.3868], device='cuda:0'))])
epoch£º584	 i:0 	 global-step:11680	 l-p:0.21179908514022827
epoch£º584	 i:1 	 global-step:11681	 l-p:0.11688576638698578
epoch£º584	 i:2 	 global-step:11682	 l-p:0.1399717628955841
epoch£º584	 i:3 	 global-step:11683	 l-p:0.1349935382604599
epoch£º584	 i:4 	 global-step:11684	 l-p:0.1140325590968132
epoch£º584	 i:5 	 global-step:11685	 l-p:-0.04005122184753418
epoch£º584	 i:6 	 global-step:11686	 l-p:0.13934943079948425
epoch£º584	 i:7 	 global-step:11687	 l-p:0.16740445792675018
epoch£º584	 i:8 	 global-step:11688	 l-p:0.07640577107667923
epoch£º584	 i:9 	 global-step:11689	 l-p:0.10655105113983154
====================================================================================================
====================================================================================================
====================================================================================================

epoch:585
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2236, 5.1511, 5.1868],
        [5.2236, 5.2164, 5.2230],
        [5.2236, 5.2221, 5.2236],
        [5.2236, 5.2141, 5.2226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:585, step:0 
model_pd.l_p.mean(): 0.22359268367290497 
model_pd.l_d.mean(): -20.26081085205078 
model_pd.lagr.mean(): -20.03721809387207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4051], device='cuda:0')), ('power', tensor([-21.0592], device='cuda:0'))])
epoch£º585	 i:0 	 global-step:11700	 l-p:0.22359268367290497
epoch£º585	 i:1 	 global-step:11701	 l-p:0.12016109377145767
epoch£º585	 i:2 	 global-step:11702	 l-p:0.18213409185409546
epoch£º585	 i:3 	 global-step:11703	 l-p:-0.03831825032830238
epoch£º585	 i:4 	 global-step:11704	 l-p:0.10561367869377136
epoch£º585	 i:5 	 global-step:11705	 l-p:0.11754102259874344
epoch£º585	 i:6 	 global-step:11706	 l-p:0.13125818967819214
epoch£º585	 i:7 	 global-step:11707	 l-p:0.11187032610177994
epoch£º585	 i:8 	 global-step:11708	 l-p:0.12708860635757446
epoch£º585	 i:9 	 global-step:11709	 l-p:0.40805792808532715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:586
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2796, 5.2790, 5.2796],
        [5.2796, 5.8529, 5.9470],
        [5.2796, 5.1553, 5.1567],
        [5.2796, 5.2488, 5.2721]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:586, step:0 
model_pd.l_p.mean(): 0.12776997685432434 
model_pd.l_d.mean(): -20.02627182006836 
model_pd.lagr.mean(): -19.898502349853516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4226], device='cuda:0')), ('power', tensor([-20.8384], device='cuda:0'))])
epoch£º586	 i:0 	 global-step:11720	 l-p:0.12776997685432434
epoch£º586	 i:1 	 global-step:11721	 l-p:0.1370667666196823
epoch£º586	 i:2 	 global-step:11722	 l-p:0.012134654447436333
epoch£º586	 i:3 	 global-step:11723	 l-p:0.15105457603931427
epoch£º586	 i:4 	 global-step:11724	 l-p:0.05532306432723999
epoch£º586	 i:5 	 global-step:11725	 l-p:0.11075906455516815
epoch£º586	 i:6 	 global-step:11726	 l-p:-0.8677676320075989
epoch£º586	 i:7 	 global-step:11727	 l-p:0.13307982683181763
epoch£º586	 i:8 	 global-step:11728	 l-p:0.11485704034566879
epoch£º586	 i:9 	 global-step:11729	 l-p:0.1401267796754837
====================================================================================================
====================================================================================================
====================================================================================================

epoch:587
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1233, 4.9714, 4.8400],
        [5.1233, 5.2006, 5.0012],
        [5.1233, 5.1394, 4.9247],
        [5.1233, 5.0694, 5.1033]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:587, step:0 
model_pd.l_p.mean(): 0.12372983992099762 
model_pd.l_d.mean(): -20.013517379760742 
model_pd.lagr.mean(): -19.889787673950195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4868], device='cuda:0')), ('power', tensor([-20.8919], device='cuda:0'))])
epoch£º587	 i:0 	 global-step:11740	 l-p:0.12372983992099762
epoch£º587	 i:1 	 global-step:11741	 l-p:0.05933966860175133
epoch£º587	 i:2 	 global-step:11742	 l-p:-0.045520465821027756
epoch£º587	 i:3 	 global-step:11743	 l-p:0.12974582612514496
epoch£º587	 i:4 	 global-step:11744	 l-p:0.1333896368741989
epoch£º587	 i:5 	 global-step:11745	 l-p:0.19935855269432068
epoch£º587	 i:6 	 global-step:11746	 l-p:0.17254231870174408
epoch£º587	 i:7 	 global-step:11747	 l-p:0.18065574765205383
epoch£º587	 i:8 	 global-step:11748	 l-p:0.12745152413845062
epoch£º587	 i:9 	 global-step:11749	 l-p:0.4915999472141266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:588
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9546, 4.9546, 4.9546],
        [4.9546, 4.8673, 4.9067],
        [4.9546, 4.7845, 4.7252],
        [4.9546, 4.8797, 4.9191]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:588, step:0 
model_pd.l_p.mean(): 0.13739721477031708 
model_pd.l_d.mean(): -20.33449363708496 
model_pd.lagr.mean(): -20.19709587097168 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5029], device='cuda:0')), ('power', tensor([-21.2356], device='cuda:0'))])
epoch£º588	 i:0 	 global-step:11760	 l-p:0.13739721477031708
epoch£º588	 i:1 	 global-step:11761	 l-p:0.12137658894062042
epoch£º588	 i:2 	 global-step:11762	 l-p:0.11988988518714905
epoch£º588	 i:3 	 global-step:11763	 l-p:0.1248559057712555
epoch£º588	 i:4 	 global-step:11764	 l-p:0.0800647884607315
epoch£º588	 i:5 	 global-step:11765	 l-p:0.21013052761554718
epoch£º588	 i:6 	 global-step:11766	 l-p:0.06103642284870148
epoch£º588	 i:7 	 global-step:11767	 l-p:0.1370883584022522
epoch£º588	 i:8 	 global-step:11768	 l-p:0.15607447922229767
epoch£º588	 i:9 	 global-step:11769	 l-p:0.1058252677321434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:589
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.8445, 4.8293, 4.8425],
        [4.8445, 4.8436, 4.8444],
        [4.8445, 4.7994, 4.8311],
        [4.8445, 4.8444, 4.8445]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:589, step:0 
model_pd.l_p.mean(): 0.11501249670982361 
model_pd.l_d.mean(): -20.110471725463867 
model_pd.lagr.mean(): -19.995458602905273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5645], device='cuda:0')), ('power', tensor([-21.0712], device='cuda:0'))])
epoch£º589	 i:0 	 global-step:11780	 l-p:0.11501249670982361
epoch£º589	 i:1 	 global-step:11781	 l-p:0.13470901548862457
epoch£º589	 i:2 	 global-step:11782	 l-p:0.1887068748474121
epoch£º589	 i:3 	 global-step:11783	 l-p:0.1215784028172493
epoch£º589	 i:4 	 global-step:11784	 l-p:0.06856059283018112
epoch£º589	 i:5 	 global-step:11785	 l-p:0.24215197563171387
epoch£º589	 i:6 	 global-step:11786	 l-p:0.16925430297851562
epoch£º589	 i:7 	 global-step:11787	 l-p:0.14246021211147308
epoch£º589	 i:8 	 global-step:11788	 l-p:0.1535605937242508
epoch£º589	 i:9 	 global-step:11789	 l-p:0.13225582242012024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:590
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0535, 5.0192, 5.0449],
        [5.0535, 5.0133, 4.7894],
        [5.0535, 4.9968, 5.0321],
        [5.0535, 5.0435, 5.0525]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:590, step:0 
model_pd.l_p.mean(): 0.16230140626430511 
model_pd.l_d.mean(): -20.19975471496582 
model_pd.lagr.mean(): -20.037452697753906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4858], device='cuda:0')), ('power', tensor([-21.0807], device='cuda:0'))])
epoch£º590	 i:0 	 global-step:11800	 l-p:0.16230140626430511
epoch£º590	 i:1 	 global-step:11801	 l-p:0.13678933680057526
epoch£º590	 i:2 	 global-step:11802	 l-p:0.1270829439163208
epoch£º590	 i:3 	 global-step:11803	 l-p:0.1348206251859665
epoch£º590	 i:4 	 global-step:11804	 l-p:0.14265307784080505
epoch£º590	 i:5 	 global-step:11805	 l-p:0.13351494073867798
epoch£º590	 i:6 	 global-step:11806	 l-p:0.12861700356006622
epoch£º590	 i:7 	 global-step:11807	 l-p:0.08868677914142609
epoch£º590	 i:8 	 global-step:11808	 l-p:0.017780212685465813
epoch£º590	 i:9 	 global-step:11809	 l-p:0.13974325358867645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:591
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0571, 5.1305, 4.9275],
        [5.0571, 4.8946, 4.7661],
        [5.0571, 5.0570, 5.0571],
        [5.0571, 5.0567, 5.0571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:591, step:0 
model_pd.l_p.mean(): 0.12487781047821045 
model_pd.l_d.mean(): -18.689653396606445 
model_pd.lagr.mean(): -18.564775466918945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5318], device='cuda:0')), ('power', tensor([-19.5900], device='cuda:0'))])
epoch£º591	 i:0 	 global-step:11820	 l-p:0.12487781047821045
epoch£º591	 i:1 	 global-step:11821	 l-p:0.17455336451530457
epoch£º591	 i:2 	 global-step:11822	 l-p:0.12059742957353592
epoch£º591	 i:3 	 global-step:11823	 l-p:0.12158578634262085
epoch£º591	 i:4 	 global-step:11824	 l-p:0.06534873694181442
epoch£º591	 i:5 	 global-step:11825	 l-p:0.11230456084012985
epoch£º591	 i:6 	 global-step:11826	 l-p:0.10190664976835251
epoch£º591	 i:7 	 global-step:11827	 l-p:0.17656119167804718
epoch£º591	 i:8 	 global-step:11828	 l-p:0.09341946989297867
epoch£º591	 i:9 	 global-step:11829	 l-p:0.13247178494930267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:592
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9731,  0.9643,  1.0000,  0.9556,
          1.0000,  0.9910, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2614,  0.1671,  1.0000,  0.1069,
          1.0000,  0.6394, 31.6228]], device='cuda:0')
 pt:tensor([[5.1211, 4.9673, 4.9329],
        [5.1211, 4.9673, 4.9326],
        [5.1211, 5.7236, 5.8471],
        [5.1211, 4.9613, 4.8998]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:592, step:0 
model_pd.l_p.mean(): 0.12654560804367065 
model_pd.l_d.mean(): -19.9986572265625 
model_pd.lagr.mean(): -19.872112274169922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4444], device='cuda:0')), ('power', tensor([-20.8329], device='cuda:0'))])
epoch£º592	 i:0 	 global-step:11840	 l-p:0.12654560804367065
epoch£º592	 i:1 	 global-step:11841	 l-p:0.07492069900035858
epoch£º592	 i:2 	 global-step:11842	 l-p:1.9362030029296875
epoch£º592	 i:3 	 global-step:11843	 l-p:0.1403324156999588
epoch£º592	 i:4 	 global-step:11844	 l-p:0.004712607711553574
epoch£º592	 i:5 	 global-step:11845	 l-p:0.12445741146802902
epoch£º592	 i:6 	 global-step:11846	 l-p:0.1384589523077011
epoch£º592	 i:7 	 global-step:11847	 l-p:0.13046906888484955
epoch£º592	 i:8 	 global-step:11848	 l-p:0.17194896936416626
epoch£º592	 i:9 	 global-step:11849	 l-p:0.12141694873571396
====================================================================================================
====================================================================================================
====================================================================================================

epoch:593
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0795, 5.0087, 5.0469],
        [5.0795, 4.9712, 5.0028],
        [5.0795, 5.0794, 5.0795],
        [5.0795, 5.0795, 5.0795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:593, step:0 
model_pd.l_p.mean(): 0.021553214639425278 
model_pd.l_d.mean(): -20.60871696472168 
model_pd.lagr.mean(): -20.5871639251709 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4301], device='cuda:0')), ('power', tensor([-21.4395], device='cuda:0'))])
epoch£º593	 i:0 	 global-step:11860	 l-p:0.021553214639425278
epoch£º593	 i:1 	 global-step:11861	 l-p:0.1079075038433075
epoch£º593	 i:2 	 global-step:11862	 l-p:0.10958882421255112
epoch£º593	 i:3 	 global-step:11863	 l-p:0.18839022517204285
epoch£º593	 i:4 	 global-step:11864	 l-p:0.14138072729110718
epoch£º593	 i:5 	 global-step:11865	 l-p:0.13555362820625305
epoch£º593	 i:6 	 global-step:11866	 l-p:0.13103409111499786
epoch£º593	 i:7 	 global-step:11867	 l-p:0.18417972326278687
epoch£º593	 i:8 	 global-step:11868	 l-p:0.15395087003707886
epoch£º593	 i:9 	 global-step:11869	 l-p:0.10288362205028534
====================================================================================================
====================================================================================================
====================================================================================================

epoch:594
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0609, 4.9594, 4.9948],
        [5.0609, 5.0609, 5.0609],
        [5.0609, 4.9998, 5.0366],
        [5.0609, 5.3053, 5.1830]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:594, step:0 
model_pd.l_p.mean(): 0.10851670801639557 
model_pd.l_d.mean(): -20.172828674316406 
model_pd.lagr.mean(): -20.064311981201172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5055], device='cuda:0')), ('power', tensor([-21.0736], device='cuda:0'))])
epoch£º594	 i:0 	 global-step:11880	 l-p:0.10851670801639557
epoch£º594	 i:1 	 global-step:11881	 l-p:0.13076132535934448
epoch£º594	 i:2 	 global-step:11882	 l-p:0.11534728854894638
epoch£º594	 i:3 	 global-step:11883	 l-p:0.1323106288909912
epoch£º594	 i:4 	 global-step:11884	 l-p:0.1173354908823967
epoch£º594	 i:5 	 global-step:11885	 l-p:0.02942282147705555
epoch£º594	 i:6 	 global-step:11886	 l-p:0.13824142515659332
epoch£º594	 i:7 	 global-step:11887	 l-p:0.17986899614334106
epoch£º594	 i:8 	 global-step:11888	 l-p:0.12239988148212433
epoch£º594	 i:9 	 global-step:11889	 l-p:0.1386709213256836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:595
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0702, 5.3204, 5.2009],
        [5.0702, 4.9109, 4.8772],
        [5.0702, 5.0701, 5.0702],
        [5.0702, 5.0685, 5.0701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:595, step:0 
model_pd.l_p.mean(): 0.1447397917509079 
model_pd.l_d.mean(): -20.45488929748535 
model_pd.lagr.mean(): -20.310150146484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4465], device='cuda:0')), ('power', tensor([-21.2999], device='cuda:0'))])
epoch£º595	 i:0 	 global-step:11900	 l-p:0.1447397917509079
epoch£º595	 i:1 	 global-step:11901	 l-p:0.12301775813102722
epoch£º595	 i:2 	 global-step:11902	 l-p:0.16149646043777466
epoch£º595	 i:3 	 global-step:11903	 l-p:0.18974147737026215
epoch£º595	 i:4 	 global-step:11904	 l-p:0.12315680086612701
epoch£º595	 i:5 	 global-step:11905	 l-p:0.1273881494998932
epoch£º595	 i:6 	 global-step:11906	 l-p:0.1252804696559906
epoch£º595	 i:7 	 global-step:11907	 l-p:0.11757034808397293
epoch£º595	 i:8 	 global-step:11908	 l-p:0.12067113816738129
epoch£º595	 i:9 	 global-step:11909	 l-p:0.11789966374635696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:596
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9839, 4.9463, 4.9742],
        [4.9839, 4.9831, 4.9839],
        [4.9839, 4.8722, 4.6513],
        [4.9839, 4.9839, 4.9839]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:596, step:0 
model_pd.l_p.mean(): 0.11433091759681702 
model_pd.l_d.mean(): -20.412906646728516 
model_pd.lagr.mean(): -20.29857635498047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4711], device='cuda:0')), ('power', tensor([-21.2826], device='cuda:0'))])
epoch£º596	 i:0 	 global-step:11920	 l-p:0.11433091759681702
epoch£º596	 i:1 	 global-step:11921	 l-p:0.14626075327396393
epoch£º596	 i:2 	 global-step:11922	 l-p:0.15558841824531555
epoch£º596	 i:3 	 global-step:11923	 l-p:-1.0532244443893433
epoch£º596	 i:4 	 global-step:11924	 l-p:0.12439112365245819
epoch£º596	 i:5 	 global-step:11925	 l-p:-0.4511723518371582
epoch£º596	 i:6 	 global-step:11926	 l-p:0.443297803401947
epoch£º596	 i:7 	 global-step:11927	 l-p:0.6903318762779236
epoch£º596	 i:8 	 global-step:11928	 l-p:0.07807380706071854
epoch£º596	 i:9 	 global-step:11929	 l-p:0.13152694702148438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:597
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9465, 4.7921, 4.7900],
        [4.9465, 4.9214, 4.9418],
        [4.9465, 4.9465, 4.9465],
        [4.9465, 4.9449, 4.9465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:597, step:0 
model_pd.l_p.mean(): 0.14803019165992737 
model_pd.l_d.mean(): -20.318187713623047 
model_pd.lagr.mean(): -20.17015838623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4916], device='cuda:0')), ('power', tensor([-21.2073], device='cuda:0'))])
epoch£º597	 i:0 	 global-step:11940	 l-p:0.14803019165992737
epoch£º597	 i:1 	 global-step:11941	 l-p:0.14338795840740204
epoch£º597	 i:2 	 global-step:11942	 l-p:0.13174863159656525
epoch£º597	 i:3 	 global-step:11943	 l-p:0.013978276401758194
epoch£º597	 i:4 	 global-step:11944	 l-p:0.2387063205242157
epoch£º597	 i:5 	 global-step:11945	 l-p:1.3563611507415771
epoch£º597	 i:6 	 global-step:11946	 l-p:0.08978725969791412
epoch£º597	 i:7 	 global-step:11947	 l-p:0.17956571280956268
epoch£º597	 i:8 	 global-step:11948	 l-p:0.13382762670516968
epoch£º597	 i:9 	 global-step:11949	 l-p:0.15009401738643646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:598
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0228, 5.0220, 5.0228],
        [5.0228, 5.1402, 4.9511],
        [5.0228, 4.8917, 4.9122],
        [5.0228, 5.4393, 5.4281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:598, step:0 
model_pd.l_p.mean(): 0.11597516387701035 
model_pd.l_d.mean(): -19.144062042236328 
model_pd.lagr.mean(): -19.028087615966797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5405], device='cuda:0')), ('power', tensor([-20.0618], device='cuda:0'))])
epoch£º598	 i:0 	 global-step:11960	 l-p:0.11597516387701035
epoch£º598	 i:1 	 global-step:11961	 l-p:0.12267555296421051
epoch£º598	 i:2 	 global-step:11962	 l-p:0.11038487404584885
epoch£º598	 i:3 	 global-step:11963	 l-p:0.13170039653778076
epoch£º598	 i:4 	 global-step:11964	 l-p:0.11860157549381256
epoch£º598	 i:5 	 global-step:11965	 l-p:0.1256221979856491
epoch£º598	 i:6 	 global-step:11966	 l-p:0.14668937027454376
epoch£º598	 i:7 	 global-step:11967	 l-p:0.13910745084285736
epoch£º598	 i:8 	 global-step:11968	 l-p:0.15566468238830566
epoch£º598	 i:9 	 global-step:11969	 l-p:0.13149243593215942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:599
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0866, 5.0866, 5.0866],
        [5.0866, 4.9518, 4.9654],
        [5.0866, 5.3355, 5.2132],
        [5.0866, 5.0828, 5.0864]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:599, step:0 
model_pd.l_p.mean(): 0.13990981876850128 
model_pd.l_d.mean(): -19.681896209716797 
model_pd.lagr.mean(): -19.5419864654541 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5480], device='cuda:0')), ('power', tensor([-20.6175], device='cuda:0'))])
epoch£º599	 i:0 	 global-step:11980	 l-p:0.13990981876850128
epoch£º599	 i:1 	 global-step:11981	 l-p:0.1579378843307495
epoch£º599	 i:2 	 global-step:11982	 l-p:0.12200679630041122
epoch£º599	 i:3 	 global-step:11983	 l-p:0.1725360006093979
epoch£º599	 i:4 	 global-step:11984	 l-p:0.09485582262277603
epoch£º599	 i:5 	 global-step:11985	 l-p:0.0782926008105278
epoch£º599	 i:6 	 global-step:11986	 l-p:0.13580693304538727
epoch£º599	 i:7 	 global-step:11987	 l-p:0.13388602435588837
epoch£º599	 i:8 	 global-step:11988	 l-p:0.09532947093248367
epoch£º599	 i:9 	 global-step:11989	 l-p:0.12265504896640778
====================================================================================================
====================================================================================================
====================================================================================================

epoch:600
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0538, 5.1065, 4.8911],
        [5.0538, 5.0538, 5.0538],
        [5.0538, 5.0538, 5.0538],
        [5.0538, 5.0538, 5.0538]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:600, step:0 
model_pd.l_p.mean(): 0.13150213658809662 
model_pd.l_d.mean(): -19.481985092163086 
model_pd.lagr.mean(): -19.350482940673828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4773], device='cuda:0')), ('power', tensor([-20.3406], device='cuda:0'))])
epoch£º600	 i:0 	 global-step:12000	 l-p:0.13150213658809662
epoch£º600	 i:1 	 global-step:12001	 l-p:0.14150825142860413
epoch£º600	 i:2 	 global-step:12002	 l-p:0.16930854320526123
epoch£º600	 i:3 	 global-step:12003	 l-p:0.08570435643196106
epoch£º600	 i:4 	 global-step:12004	 l-p:0.10178247839212418
epoch£º600	 i:5 	 global-step:12005	 l-p:0.13075050711631775
epoch£º600	 i:6 	 global-step:12006	 l-p:0.21899008750915527
epoch£º600	 i:7 	 global-step:12007	 l-p:0.1176513284444809
epoch£º600	 i:8 	 global-step:12008	 l-p:0.12751877307891846
epoch£º600	 i:9 	 global-step:12009	 l-p:0.12941746413707733
====================================================================================================
====================================================================================================
====================================================================================================

epoch:601
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0511, 5.0171, 4.7844],
        [5.0511, 4.8805, 4.7373],
        [5.0511, 5.0307, 5.0478],
        [5.0511, 5.0504, 5.0511]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:601, step:0 
model_pd.l_p.mean(): 0.09800232946872711 
model_pd.l_d.mean(): -20.488731384277344 
model_pd.lagr.mean(): -20.390729904174805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4350], device='cuda:0')), ('power', tensor([-21.3224], device='cuda:0'))])
epoch£º601	 i:0 	 global-step:12020	 l-p:0.09800232946872711
epoch£º601	 i:1 	 global-step:12021	 l-p:0.12571893632411957
epoch£º601	 i:2 	 global-step:12022	 l-p:0.17831231653690338
epoch£º601	 i:3 	 global-step:12023	 l-p:0.15035809576511383
epoch£º601	 i:4 	 global-step:12024	 l-p:0.13009577989578247
epoch£º601	 i:5 	 global-step:12025	 l-p:0.10629644244909286
epoch£º601	 i:6 	 global-step:12026	 l-p:0.1926647573709488
epoch£º601	 i:7 	 global-step:12027	 l-p:0.10672444105148315
epoch£º601	 i:8 	 global-step:12028	 l-p:0.10156222432851791
epoch£º601	 i:9 	 global-step:12029	 l-p:0.12802179157733917
====================================================================================================
====================================================================================================
====================================================================================================

epoch:602
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3701e-05, 1.0886e-06,
         1.0000e+00, 3.5161e-08, 1.0000e+00, 3.2301e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0530, 5.0530, 5.0530],
        [5.0530, 5.0845, 4.8626],
        [5.0530, 5.0289, 5.0486],
        [5.0530, 5.0407, 4.8099]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:602, step:0 
model_pd.l_p.mean(): 0.13181203603744507 
model_pd.l_d.mean(): -20.246524810791016 
model_pd.lagr.mean(): -20.114713668823242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4796], device='cuda:0')), ('power', tensor([-21.1218], device='cuda:0'))])
epoch£º602	 i:0 	 global-step:12040	 l-p:0.13181203603744507
epoch£º602	 i:1 	 global-step:12041	 l-p:0.06810542941093445
epoch£º602	 i:2 	 global-step:12042	 l-p:0.11809226870536804
epoch£º602	 i:3 	 global-step:12043	 l-p:0.11203940957784653
epoch£º602	 i:4 	 global-step:12044	 l-p:0.10815178602933884
epoch£º602	 i:5 	 global-step:12045	 l-p:0.1948876529932022
epoch£º602	 i:6 	 global-step:12046	 l-p:0.12507155537605286
epoch£º602	 i:7 	 global-step:12047	 l-p:0.09117317199707031
epoch£º602	 i:8 	 global-step:12048	 l-p:0.1490652710199356
epoch£º602	 i:9 	 global-step:12049	 l-p:0.16299472749233246
====================================================================================================
====================================================================================================
====================================================================================================

epoch:603
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0916, 4.9725, 5.0011],
        [5.0916, 5.0807, 5.0904],
        [5.0916, 5.0915, 5.0916],
        [5.0916, 4.9801, 5.0130]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:603, step:0 
model_pd.l_p.mean(): 0.09282180666923523 
model_pd.l_d.mean(): -20.60010528564453 
model_pd.lagr.mean(): -20.50728416442871 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4169], device='cuda:0')), ('power', tensor([-21.4171], device='cuda:0'))])
epoch£º603	 i:0 	 global-step:12060	 l-p:0.09282180666923523
epoch£º603	 i:1 	 global-step:12061	 l-p:0.17124205827713013
epoch£º603	 i:2 	 global-step:12062	 l-p:0.1336379200220108
epoch£º603	 i:3 	 global-step:12063	 l-p:0.12821730971336365
epoch£º603	 i:4 	 global-step:12064	 l-p:0.09624822437763214
epoch£º603	 i:5 	 global-step:12065	 l-p:0.13006208837032318
epoch£º603	 i:6 	 global-step:12066	 l-p:0.14084623754024506
epoch£º603	 i:7 	 global-step:12067	 l-p:-0.10962503403425217
epoch£º603	 i:8 	 global-step:12068	 l-p:0.11496152728796005
epoch£º603	 i:9 	 global-step:12069	 l-p:0.12200004607439041
====================================================================================================
====================================================================================================
====================================================================================================

epoch:604
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1192, 5.4078, 5.3069],
        [5.1192, 5.1192, 5.1192],
        [5.1192, 5.0548, 5.0926],
        [5.1192, 4.9533, 4.9029]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:604, step:0 
model_pd.l_p.mean(): -2.063732385635376 
model_pd.l_d.mean(): -20.370494842529297 
model_pd.lagr.mean(): -22.434226989746094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4412], device='cuda:0')), ('power', tensor([-21.2084], device='cuda:0'))])
epoch£º604	 i:0 	 global-step:12080	 l-p:-2.063732385635376
epoch£º604	 i:1 	 global-step:12081	 l-p:0.15495800971984863
epoch£º604	 i:2 	 global-step:12082	 l-p:0.11143621802330017
epoch£º604	 i:3 	 global-step:12083	 l-p:0.09062109142541885
epoch£º604	 i:4 	 global-step:12084	 l-p:0.12878036499023438
epoch£º604	 i:5 	 global-step:12085	 l-p:0.1124994158744812
epoch£º604	 i:6 	 global-step:12086	 l-p:-0.38088107109069824
epoch£º604	 i:7 	 global-step:12087	 l-p:0.11083313077688217
epoch£º604	 i:8 	 global-step:12088	 l-p:0.14074598252773285
epoch£º604	 i:9 	 global-step:12089	 l-p:0.11973714828491211
====================================================================================================
====================================================================================================
====================================================================================================

epoch:605
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1699, 5.1418, 4.9143],
        [5.1699, 5.0652, 5.0991],
        [5.1699, 5.1163, 5.1508],
        [5.1699, 5.1262, 5.1568]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:605, step:0 
model_pd.l_p.mean(): 0.11079713702201843 
model_pd.l_d.mean(): -20.266756057739258 
model_pd.lagr.mean(): -20.15595817565918 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4310], device='cuda:0')), ('power', tensor([-21.0922], device='cuda:0'))])
epoch£º605	 i:0 	 global-step:12100	 l-p:0.11079713702201843
epoch£º605	 i:1 	 global-step:12101	 l-p:0.12086553126573563
epoch£º605	 i:2 	 global-step:12102	 l-p:0.13754434883594513
epoch£º605	 i:3 	 global-step:12103	 l-p:0.1274612993001938
epoch£º605	 i:4 	 global-step:12104	 l-p:0.06926074624061584
epoch£º605	 i:5 	 global-step:12105	 l-p:0.1427278369665146
epoch£º605	 i:6 	 global-step:12106	 l-p:0.11231954395771027
epoch£º605	 i:7 	 global-step:12107	 l-p:0.5059575438499451
epoch£º605	 i:8 	 global-step:12108	 l-p:0.12561428546905518
epoch£º605	 i:9 	 global-step:12109	 l-p:0.020882634446024895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:606
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1150, 5.1150, 5.1150],
        [5.1150, 5.3720, 5.2520],
        [5.1150, 4.9484, 4.8989],
        [5.1150, 5.1146, 5.1150]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:606, step:0 
model_pd.l_p.mean(): 0.11312169581651688 
model_pd.l_d.mean(): -20.530004501342773 
model_pd.lagr.mean(): -20.41688346862793 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4246], device='cuda:0')), ('power', tensor([-21.3536], device='cuda:0'))])
epoch£º606	 i:0 	 global-step:12120	 l-p:0.11312169581651688
epoch£º606	 i:1 	 global-step:12121	 l-p:0.14028385281562805
epoch£º606	 i:2 	 global-step:12122	 l-p:0.12887971103191376
epoch£º606	 i:3 	 global-step:12123	 l-p:0.09151813387870789
epoch£º606	 i:4 	 global-step:12124	 l-p:0.1427265703678131
epoch£º606	 i:5 	 global-step:12125	 l-p:0.07539371401071548
epoch£º606	 i:6 	 global-step:12126	 l-p:0.1434655636548996
epoch£º606	 i:7 	 global-step:12127	 l-p:0.14512796700000763
epoch£º606	 i:8 	 global-step:12128	 l-p:0.1348276138305664
epoch£º606	 i:9 	 global-step:12129	 l-p:0.08163817971944809
====================================================================================================
====================================================================================================
====================================================================================================

epoch:607
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0607, 5.0514, 5.0599],
        [5.0607, 4.8850, 4.8115],
        [5.0607, 5.0594, 5.0607],
        [5.0607, 4.8948, 4.8621]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:607, step:0 
model_pd.l_p.mean(): 0.1473398506641388 
model_pd.l_d.mean(): -20.321001052856445 
model_pd.lagr.mean(): -20.173660278320312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4696], device='cuda:0')), ('power', tensor([-21.1874], device='cuda:0'))])
epoch£º607	 i:0 	 global-step:12140	 l-p:0.1473398506641388
epoch£º607	 i:1 	 global-step:12141	 l-p:0.13036617636680603
epoch£º607	 i:2 	 global-step:12142	 l-p:0.10996038466691971
epoch£º607	 i:3 	 global-step:12143	 l-p:0.14910659193992615
epoch£º607	 i:4 	 global-step:12144	 l-p:0.16341674327850342
epoch£º607	 i:5 	 global-step:12145	 l-p:0.11419031769037247
epoch£º607	 i:6 	 global-step:12146	 l-p:0.0955839529633522
epoch£º607	 i:7 	 global-step:12147	 l-p:0.12473887205123901
epoch£º607	 i:8 	 global-step:12148	 l-p:0.13706310093402863
epoch£º607	 i:9 	 global-step:12149	 l-p:0.1782231479883194
====================================================================================================
====================================================================================================
====================================================================================================

epoch:608
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0298, 5.0284, 5.0298],
        [5.0298, 5.0242, 5.0295],
        [5.0298, 4.9666, 5.0049],
        [5.0298, 5.2370, 5.0907]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:608, step:0 
model_pd.l_p.mean(): 0.18190418183803558 
model_pd.l_d.mean(): -19.433305740356445 
model_pd.lagr.mean(): -19.251401901245117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5542], device='cuda:0')), ('power', tensor([-20.3707], device='cuda:0'))])
epoch£º608	 i:0 	 global-step:12160	 l-p:0.18190418183803558
epoch£º608	 i:1 	 global-step:12161	 l-p:0.14536471664905548
epoch£º608	 i:2 	 global-step:12162	 l-p:0.14179283380508423
epoch£º608	 i:3 	 global-step:12163	 l-p:0.18065983057022095
epoch£º608	 i:4 	 global-step:12164	 l-p:0.07196696847677231
epoch£º608	 i:5 	 global-step:12165	 l-p:0.11850030720233917
epoch£º608	 i:6 	 global-step:12166	 l-p:0.1349148154258728
epoch£º608	 i:7 	 global-step:12167	 l-p:0.11680430918931961
epoch£º608	 i:8 	 global-step:12168	 l-p:0.15502658486366272
epoch£º608	 i:9 	 global-step:12169	 l-p:0.1331397444009781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:609
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0457, 5.0457, 5.0457],
        [5.0457, 5.0282, 5.0432],
        [5.0457, 5.2567, 5.1118],
        [5.0457, 4.9345, 4.9699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:609, step:0 
model_pd.l_p.mean(): 0.14303043484687805 
model_pd.l_d.mean(): -19.09967613220215 
model_pd.lagr.mean(): -18.956645965576172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5637], device='cuda:0')), ('power', tensor([-20.0407], device='cuda:0'))])
epoch£º609	 i:0 	 global-step:12180	 l-p:0.14303043484687805
epoch£º609	 i:1 	 global-step:12181	 l-p:0.13666394352912903
epoch£º609	 i:2 	 global-step:12182	 l-p:0.15650546550750732
epoch£º609	 i:3 	 global-step:12183	 l-p:0.13233442604541779
epoch£º609	 i:4 	 global-step:12184	 l-p:0.11511741578578949
epoch£º609	 i:5 	 global-step:12185	 l-p:0.14268669486045837
epoch£º609	 i:6 	 global-step:12186	 l-p:0.16916772723197937
epoch£º609	 i:7 	 global-step:12187	 l-p:0.13713516294956207
epoch£º609	 i:8 	 global-step:12188	 l-p:0.10108985751867294
epoch£º609	 i:9 	 global-step:12189	 l-p:0.10667543858289719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:610
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0456, 4.9342, 4.9697],
        [5.0456, 5.0452, 5.0455],
        [5.0456, 5.0455, 5.0456],
        [5.0456, 5.0114, 5.0374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:610, step:0 
model_pd.l_p.mean(): 0.08113358169794083 
model_pd.l_d.mean(): -20.25150489807129 
model_pd.lagr.mean(): -20.170372009277344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4734], device='cuda:0')), ('power', tensor([-21.1205], device='cuda:0'))])
epoch£º610	 i:0 	 global-step:12200	 l-p:0.08113358169794083
epoch£º610	 i:1 	 global-step:12201	 l-p:0.14317427575588226
epoch£º610	 i:2 	 global-step:12202	 l-p:0.1284700334072113
epoch£º610	 i:3 	 global-step:12203	 l-p:0.12469787150621414
epoch£º610	 i:4 	 global-step:12204	 l-p:0.12854960560798645
epoch£º610	 i:5 	 global-step:12205	 l-p:0.12931279838085175
epoch£º610	 i:6 	 global-step:12206	 l-p:0.044980768114328384
epoch£º610	 i:7 	 global-step:12207	 l-p:0.1870487481355667
epoch£º610	 i:8 	 global-step:12208	 l-p:0.1540784388780594
epoch£º610	 i:9 	 global-step:12209	 l-p:0.14434729516506195
====================================================================================================
====================================================================================================
====================================================================================================

epoch:611
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0975, 4.9261, 4.8702],
        [5.0975, 5.3877, 5.2874],
        [5.0975, 5.6192, 5.6771],
        [5.0975, 5.0964, 5.0974]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:611, step:0 
model_pd.l_p.mean(): 0.0966765359044075 
model_pd.l_d.mean(): -20.428171157836914 
model_pd.lagr.mean(): -20.33149528503418 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4184], device='cuda:0')), ('power', tensor([-21.2436], device='cuda:0'))])
epoch£º611	 i:0 	 global-step:12220	 l-p:0.0966765359044075
epoch£º611	 i:1 	 global-step:12221	 l-p:0.11932104825973511
epoch£º611	 i:2 	 global-step:12222	 l-p:0.08645978569984436
epoch£º611	 i:3 	 global-step:12223	 l-p:0.15971355140209198
epoch£º611	 i:4 	 global-step:12224	 l-p:0.1395426094532013
epoch£º611	 i:5 	 global-step:12225	 l-p:0.11145558953285217
epoch£º611	 i:6 	 global-step:12226	 l-p:0.1274634301662445
epoch£º611	 i:7 	 global-step:12227	 l-p:0.12887616455554962
epoch£º611	 i:8 	 global-step:12228	 l-p:-0.26996099948883057
epoch£º611	 i:9 	 global-step:12229	 l-p:0.1483488380908966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:612
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1155, 5.0721, 4.8387],
        [5.1155, 5.1154, 5.1155],
        [5.1155, 4.9531, 4.9244],
        [5.1155, 5.1146, 5.1154]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:612, step:0 
model_pd.l_p.mean(): 0.14370884001255035 
model_pd.l_d.mean(): -19.348922729492188 
model_pd.lagr.mean(): -19.20521354675293 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4853], device='cuda:0')), ('power', tensor([-20.2134], device='cuda:0'))])
epoch£º612	 i:0 	 global-step:12240	 l-p:0.14370884001255035
epoch£º612	 i:1 	 global-step:12241	 l-p:0.12938666343688965
epoch£º612	 i:2 	 global-step:12242	 l-p:0.09711658954620361
epoch£º612	 i:3 	 global-step:12243	 l-p:0.12965570390224457
epoch£º612	 i:4 	 global-step:12244	 l-p:0.10832003504037857
epoch£º612	 i:5 	 global-step:12245	 l-p:0.13359685242176056
epoch£º612	 i:6 	 global-step:12246	 l-p:0.1434859186410904
epoch£º612	 i:7 	 global-step:12247	 l-p:0.14526425302028656
epoch£º612	 i:8 	 global-step:12248	 l-p:0.18989822268486023
epoch£º612	 i:9 	 global-step:12249	 l-p:0.05921734496951103
====================================================================================================
====================================================================================================
====================================================================================================

epoch:613
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0592, 5.0591, 5.0592],
        [5.0592, 5.0348, 5.0547],
        [5.0592, 5.0560, 5.0591],
        [5.0592, 5.0025, 5.0389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:613, step:0 
model_pd.l_p.mean(): 0.20149356126785278 
model_pd.l_d.mean(): -20.423450469970703 
model_pd.lagr.mean(): -20.221956253051758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4582], device='cuda:0')), ('power', tensor([-21.2799], device='cuda:0'))])
epoch£º613	 i:0 	 global-step:12260	 l-p:0.20149356126785278
epoch£º613	 i:1 	 global-step:12261	 l-p:0.16407932341098785
epoch£º613	 i:2 	 global-step:12262	 l-p:0.11997663974761963
epoch£º613	 i:3 	 global-step:12263	 l-p:0.046640701591968536
epoch£º613	 i:4 	 global-step:12264	 l-p:0.11409498006105423
epoch£º613	 i:5 	 global-step:12265	 l-p:0.12233136594295502
epoch£º613	 i:6 	 global-step:12266	 l-p:0.14103567600250244
epoch£º613	 i:7 	 global-step:12267	 l-p:0.09702502936124802
epoch£º613	 i:8 	 global-step:12268	 l-p:0.10751780867576599
epoch£º613	 i:9 	 global-step:12269	 l-p:0.13230563700199127
====================================================================================================
====================================================================================================
====================================================================================================

epoch:614
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0869, 4.9853, 4.7583],
        [5.0869, 4.9088, 4.8167],
        [5.0869, 5.0835, 5.0867],
        [5.0869, 5.0852, 5.0868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:614, step:0 
model_pd.l_p.mean(): 0.13713736832141876 
model_pd.l_d.mean(): -20.281455993652344 
model_pd.lagr.mean(): -20.144319534301758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4701], device='cuda:0')), ('power', tensor([-21.1476], device='cuda:0'))])
epoch£º614	 i:0 	 global-step:12280	 l-p:0.13713736832141876
epoch£º614	 i:1 	 global-step:12281	 l-p:0.14917795360088348
epoch£º614	 i:2 	 global-step:12282	 l-p:0.03603916987776756
epoch£º614	 i:3 	 global-step:12283	 l-p:0.12793715298175812
epoch£º614	 i:4 	 global-step:12284	 l-p:0.12904411554336548
epoch£º614	 i:5 	 global-step:12285	 l-p:0.1404017060995102
epoch£º614	 i:6 	 global-step:12286	 l-p:0.1818615347146988
epoch£º614	 i:7 	 global-step:12287	 l-p:0.11115109920501709
epoch£º614	 i:8 	 global-step:12288	 l-p:0.1534261703491211
epoch£º614	 i:9 	 global-step:12289	 l-p:0.12180931121110916
====================================================================================================
====================================================================================================
====================================================================================================

epoch:615
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0552, 4.8737, 4.7620],
        [5.0552, 4.8742, 4.7893],
        [5.0552, 5.0957, 4.8727],
        [5.0552, 5.0552, 5.0552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:615, step:0 
model_pd.l_p.mean(): 0.13364292681217194 
model_pd.l_d.mean(): -18.989328384399414 
model_pd.lagr.mean(): -18.85568618774414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5202], device='cuda:0')), ('power', tensor([-19.8832], device='cuda:0'))])
epoch£º615	 i:0 	 global-step:12300	 l-p:0.13364292681217194
epoch£º615	 i:1 	 global-step:12301	 l-p:0.09816817939281464
epoch£º615	 i:2 	 global-step:12302	 l-p:0.13883939385414124
epoch£º615	 i:3 	 global-step:12303	 l-p:0.10494182258844376
epoch£º615	 i:4 	 global-step:12304	 l-p:0.13161781430244446
epoch£º615	 i:5 	 global-step:12305	 l-p:0.13045966625213623
epoch£º615	 i:6 	 global-step:12306	 l-p:0.14810673892498016
epoch£º615	 i:7 	 global-step:12307	 l-p:0.17618519067764282
epoch£º615	 i:8 	 global-step:12308	 l-p:0.12805908918380737
epoch£º615	 i:9 	 global-step:12309	 l-p:0.11761258542537689
====================================================================================================
====================================================================================================
====================================================================================================

epoch:616
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0574, 5.0554, 5.0574],
        [5.0574, 5.0246, 5.0499],
        [5.0574, 5.2122, 5.0357],
        [5.0574, 4.9114, 4.7074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:616, step:0 
model_pd.l_p.mean(): 0.1332835704088211 
model_pd.l_d.mean(): -18.592382431030273 
model_pd.lagr.mean(): -18.45909881591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6016], device='cuda:0')), ('power', tensor([-19.5631], device='cuda:0'))])
epoch£º616	 i:0 	 global-step:12320	 l-p:0.1332835704088211
epoch£º616	 i:1 	 global-step:12321	 l-p:0.15654699504375458
epoch£º616	 i:2 	 global-step:12322	 l-p:0.15682412683963776
epoch£º616	 i:3 	 global-step:12323	 l-p:0.14712677896022797
epoch£º616	 i:4 	 global-step:12324	 l-p:0.07323796302080154
epoch£º616	 i:5 	 global-step:12325	 l-p:0.10706129670143127
epoch£º616	 i:6 	 global-step:12326	 l-p:0.1320970058441162
epoch£º616	 i:7 	 global-step:12327	 l-p:0.12428567558526993
epoch£º616	 i:8 	 global-step:12328	 l-p:0.13822625577449799
epoch£º616	 i:9 	 global-step:12329	 l-p:0.13015127182006836
====================================================================================================
====================================================================================================
====================================================================================================

epoch:617
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0545, 4.9947, 5.0322],
        [5.0545, 4.9120, 4.9263],
        [5.0545, 5.5882, 5.6558],
        [5.0545, 4.9957, 5.0330]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:617, step:0 
model_pd.l_p.mean(): 0.14619989693164825 
model_pd.l_d.mean(): -20.097095489501953 
model_pd.lagr.mean(): -19.950895309448242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4799], device='cuda:0')), ('power', tensor([-20.9700], device='cuda:0'))])
epoch£º617	 i:0 	 global-step:12340	 l-p:0.14619989693164825
epoch£º617	 i:1 	 global-step:12341	 l-p:0.1125122457742691
epoch£º617	 i:2 	 global-step:12342	 l-p:0.16936162114143372
epoch£º617	 i:3 	 global-step:12343	 l-p:0.1170257180929184
epoch£º617	 i:4 	 global-step:12344	 l-p:0.13022051751613617
epoch£º617	 i:5 	 global-step:12345	 l-p:0.1533733308315277
epoch£º617	 i:6 	 global-step:12346	 l-p:0.12834586203098297
epoch£º617	 i:7 	 global-step:12347	 l-p:0.21337789297103882
epoch£º617	 i:8 	 global-step:12348	 l-p:0.11985936760902405
epoch£º617	 i:9 	 global-step:12349	 l-p:0.1460520476102829
====================================================================================================
====================================================================================================
====================================================================================================

epoch:618
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0236, 4.8374, 4.7336],
        [5.0236, 4.9618, 5.0002],
        [5.0236, 5.5222, 5.5652],
        [5.0236, 4.8524, 4.8240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:618, step:0 
model_pd.l_p.mean(): 0.12933120131492615 
model_pd.l_d.mean(): -20.140932083129883 
model_pd.lagr.mean(): -20.011600494384766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4895], device='cuda:0')), ('power', tensor([-21.0246], device='cuda:0'))])
epoch£º618	 i:0 	 global-step:12360	 l-p:0.12933120131492615
epoch£º618	 i:1 	 global-step:12361	 l-p:0.1716671586036682
epoch£º618	 i:2 	 global-step:12362	 l-p:0.10436457395553589
epoch£º618	 i:3 	 global-step:12363	 l-p:0.2756367325782776
epoch£º618	 i:4 	 global-step:12364	 l-p:0.1217246726155281
epoch£º618	 i:5 	 global-step:12365	 l-p:0.16134698688983917
epoch£º618	 i:6 	 global-step:12366	 l-p:0.13718672096729279
epoch£º618	 i:7 	 global-step:12367	 l-p:0.15994670987129211
epoch£º618	 i:8 	 global-step:12368	 l-p:0.08885293453931808
epoch£º618	 i:9 	 global-step:12369	 l-p:0.15676139295101166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:619
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0157, 4.9561, 4.9939],
        [5.0157, 4.9000, 4.9361],
        [5.0157, 4.8959, 4.9301],
        [5.0157, 5.0952, 4.8845]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:619, step:0 
model_pd.l_p.mean(): 0.17409121990203857 
model_pd.l_d.mean(): -20.200653076171875 
model_pd.lagr.mean(): -20.026561737060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5106], device='cuda:0')), ('power', tensor([-21.1073], device='cuda:0'))])
epoch£º619	 i:0 	 global-step:12380	 l-p:0.17409121990203857
epoch£º619	 i:1 	 global-step:12381	 l-p:0.15229780972003937
epoch£º619	 i:2 	 global-step:12382	 l-p:0.12052609771490097
epoch£º619	 i:3 	 global-step:12383	 l-p:0.09011697769165039
epoch£º619	 i:4 	 global-step:12384	 l-p:0.17869530618190765
epoch£º619	 i:5 	 global-step:12385	 l-p:0.1085052490234375
epoch£º619	 i:6 	 global-step:12386	 l-p:0.1446709930896759
epoch£º619	 i:7 	 global-step:12387	 l-p:0.11601589620113373
epoch£º619	 i:8 	 global-step:12388	 l-p:0.12582752108573914
epoch£º619	 i:9 	 global-step:12389	 l-p:0.17096595466136932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:620
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0563, 5.0335, 5.0523],
        [5.0563, 4.9156, 4.9327],
        [5.0563, 5.0563, 5.0563],
        [5.0563, 5.0189, 4.7795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:620, step:0 
model_pd.l_p.mean(): 0.1316131353378296 
model_pd.l_d.mean(): -20.707412719726562 
model_pd.lagr.mean(): -20.5757999420166 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4341], device='cuda:0')), ('power', tensor([-21.5442], device='cuda:0'))])
epoch£º620	 i:0 	 global-step:12400	 l-p:0.1316131353378296
epoch£º620	 i:1 	 global-step:12401	 l-p:0.183160662651062
epoch£º620	 i:2 	 global-step:12402	 l-p:0.12635640799999237
epoch£º620	 i:3 	 global-step:12403	 l-p:0.10617025941610336
epoch£º620	 i:4 	 global-step:12404	 l-p:0.12353482097387314
epoch£º620	 i:5 	 global-step:12405	 l-p:0.0981050655245781
epoch£º620	 i:6 	 global-step:12406	 l-p:0.1525149941444397
epoch£º620	 i:7 	 global-step:12407	 l-p:0.1378486007452011
epoch£º620	 i:8 	 global-step:12408	 l-p:0.14071650803089142
epoch£º620	 i:9 	 global-step:12409	 l-p:0.12567968666553497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:621
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0619, 5.1094, 4.8872],
        [5.0619, 5.0610, 5.0618],
        [5.0619, 5.0619, 5.0619],
        [5.0619, 5.0618, 5.0619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:621, step:0 
model_pd.l_p.mean(): 0.14938387274742126 
model_pd.l_d.mean(): -20.710779190063477 
model_pd.lagr.mean(): -20.5613956451416 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4117], device='cuda:0')), ('power', tensor([-21.5244], device='cuda:0'))])
epoch£º621	 i:0 	 global-step:12420	 l-p:0.14938387274742126
epoch£º621	 i:1 	 global-step:12421	 l-p:0.12904761731624603
epoch£º621	 i:2 	 global-step:12422	 l-p:0.12708373367786407
epoch£º621	 i:3 	 global-step:12423	 l-p:0.1847013235092163
epoch£º621	 i:4 	 global-step:12424	 l-p:0.183924600481987
epoch£º621	 i:5 	 global-step:12425	 l-p:0.0754055380821228
epoch£º621	 i:6 	 global-step:12426	 l-p:0.11230000853538513
epoch£º621	 i:7 	 global-step:12427	 l-p:0.12670089304447174
epoch£º621	 i:8 	 global-step:12428	 l-p:0.14642857015132904
epoch£º621	 i:9 	 global-step:12429	 l-p:0.037957463413476944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:622
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0849, 5.0849, 5.0849],
        [5.0849, 4.9102, 4.8605],
        [5.0849, 4.9787, 4.7494],
        [5.0849, 4.9332, 4.9350]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:622, step:0 
model_pd.l_p.mean(): 0.15995675325393677 
model_pd.l_d.mean(): -20.603904724121094 
model_pd.lagr.mean(): -20.44394874572754 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4276], device='cuda:0')), ('power', tensor([-21.4321], device='cuda:0'))])
epoch£º622	 i:0 	 global-step:12440	 l-p:0.15995675325393677
epoch£º622	 i:1 	 global-step:12441	 l-p:0.13205979764461517
epoch£º622	 i:2 	 global-step:12442	 l-p:0.12656991183757782
epoch£º622	 i:3 	 global-step:12443	 l-p:0.1026974469423294
epoch£º622	 i:4 	 global-step:12444	 l-p:0.12122812867164612
epoch£º622	 i:5 	 global-step:12445	 l-p:0.07852364331483841
epoch£º622	 i:6 	 global-step:12446	 l-p:0.16457843780517578
epoch£º622	 i:7 	 global-step:12447	 l-p:0.14709711074829102
epoch£º622	 i:8 	 global-step:12448	 l-p:0.12470085173845291
epoch£º622	 i:9 	 global-step:12449	 l-p:0.12078675627708435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:623
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4639e-01, 7.7152e-02,
         1.0000e+00, 4.0662e-02, 1.0000e+00, 5.2703e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0524, 5.0185, 5.0446],
        [5.0524, 4.9390, 4.9753],
        [5.0524, 5.0460, 5.0520],
        [5.0524, 5.0523, 5.0524]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:623, step:0 
model_pd.l_p.mean(): 0.15254071354866028 
model_pd.l_d.mean(): -20.14116859436035 
model_pd.lagr.mean(): -19.988628387451172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4429], device='cuda:0')), ('power', tensor([-20.9765], device='cuda:0'))])
epoch£º623	 i:0 	 global-step:12460	 l-p:0.15254071354866028
epoch£º623	 i:1 	 global-step:12461	 l-p:0.12747067213058472
epoch£º623	 i:2 	 global-step:12462	 l-p:0.1704641729593277
epoch£º623	 i:3 	 global-step:12463	 l-p:0.21510711312294006
epoch£º623	 i:4 	 global-step:12464	 l-p:0.11874261498451233
epoch£º623	 i:5 	 global-step:12465	 l-p:0.1355501264333725
epoch£º623	 i:6 	 global-step:12466	 l-p:0.12809355556964874
epoch£º623	 i:7 	 global-step:12467	 l-p:0.12283923476934433
epoch£º623	 i:8 	 global-step:12468	 l-p:0.1217903420329094
epoch£º623	 i:9 	 global-step:12469	 l-p:0.11993930488824844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:624
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0325, 5.0241, 5.0318],
        [5.0325, 5.3888, 5.3310],
        [5.0325, 5.0308, 5.0325],
        [5.0325, 5.0325, 5.0325]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:624, step:0 
model_pd.l_p.mean(): 0.18249978125095367 
model_pd.l_d.mean(): -20.341724395751953 
model_pd.lagr.mean(): -20.159225463867188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4738], device='cuda:0')), ('power', tensor([-21.2129], device='cuda:0'))])
epoch£º624	 i:0 	 global-step:12480	 l-p:0.18249978125095367
epoch£º624	 i:1 	 global-step:12481	 l-p:0.129569873213768
epoch£º624	 i:2 	 global-step:12482	 l-p:0.15093155205249786
epoch£º624	 i:3 	 global-step:12483	 l-p:0.0926319882273674
epoch£º624	 i:4 	 global-step:12484	 l-p:0.18356318771839142
epoch£º624	 i:5 	 global-step:12485	 l-p:0.1445545107126236
epoch£º624	 i:6 	 global-step:12486	 l-p:0.14647670090198517
epoch£º624	 i:7 	 global-step:12487	 l-p:0.09635652601718903
epoch£º624	 i:8 	 global-step:12488	 l-p:0.11063500493764877
epoch£º624	 i:9 	 global-step:12489	 l-p:0.1575748473405838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:625
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4754e-01, 2.4435e-01,
         1.0000e+00, 1.7180e-01, 1.0000e+00, 7.0308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0537, 5.0537, 5.0537],
        [5.0537, 5.0413, 5.0523],
        [5.0537, 5.0525, 5.0536],
        [5.0537, 4.8741, 4.7213]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:625, step:0 
model_pd.l_p.mean(): 0.1532856673002243 
model_pd.l_d.mean(): -20.16935920715332 
model_pd.lagr.mean(): -20.01607322692871 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4880], device='cuda:0')), ('power', tensor([-21.0519], device='cuda:0'))])
epoch£º625	 i:0 	 global-step:12500	 l-p:0.1532856673002243
epoch£º625	 i:1 	 global-step:12501	 l-p:0.09799562394618988
epoch£º625	 i:2 	 global-step:12502	 l-p:0.09385283291339874
epoch£º625	 i:3 	 global-step:12503	 l-p:0.13619790971279144
epoch£º625	 i:4 	 global-step:12504	 l-p:0.1492605209350586
epoch£º625	 i:5 	 global-step:12505	 l-p:0.15545567870140076
epoch£º625	 i:6 	 global-step:12506	 l-p:0.11187194287776947
epoch£º625	 i:7 	 global-step:12507	 l-p:0.12847161293029785
epoch£º625	 i:8 	 global-step:12508	 l-p:0.12917460501194
epoch£º625	 i:9 	 global-step:12509	 l-p:0.1329544484615326
====================================================================================================
====================================================================================================
====================================================================================================

epoch:626
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0852, 5.0435, 5.0737],
        [5.0852, 4.9081, 4.8548],
        [5.0852, 5.0852, 5.0852],
        [5.0852, 5.4770, 5.4405]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:626, step:0 
model_pd.l_p.mean(): 0.056422628462314606 
model_pd.l_d.mean(): -20.67224884033203 
model_pd.lagr.mean(): -20.615825653076172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4136], device='cuda:0')), ('power', tensor([-21.4872], device='cuda:0'))])
epoch£º626	 i:0 	 global-step:12520	 l-p:0.056422628462314606
epoch£º626	 i:1 	 global-step:12521	 l-p:0.15768960118293762
epoch£º626	 i:2 	 global-step:12522	 l-p:0.12383949011564255
epoch£º626	 i:3 	 global-step:12523	 l-p:0.13612698018550873
epoch£º626	 i:4 	 global-step:12524	 l-p:0.10564414411783218
epoch£º626	 i:5 	 global-step:12525	 l-p:0.11249048262834549
epoch£º626	 i:6 	 global-step:12526	 l-p:0.10065288096666336
epoch£º626	 i:7 	 global-step:12527	 l-p:0.15693174302577972
epoch£º626	 i:8 	 global-step:12528	 l-p:0.11886661499738693
epoch£º626	 i:9 	 global-step:12529	 l-p:0.13028375804424286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:627
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1171, 5.0224, 5.0629],
        [5.1171, 4.9448, 4.7885],
        [5.1171, 5.0296, 5.0708],
        [5.1171, 5.0856, 5.1101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:627, step:0 
model_pd.l_p.mean(): 0.10761136561632156 
model_pd.l_d.mean(): -20.583444595336914 
model_pd.lagr.mean(): -20.475833892822266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4075], device='cuda:0')), ('power', tensor([-21.3904], device='cuda:0'))])
epoch£º627	 i:0 	 global-step:12540	 l-p:0.10761136561632156
epoch£º627	 i:1 	 global-step:12541	 l-p:0.09679918736219406
epoch£º627	 i:2 	 global-step:12542	 l-p:0.1530286818742752
epoch£º627	 i:3 	 global-step:12543	 l-p:0.11520511656999588
epoch£º627	 i:4 	 global-step:12544	 l-p:0.12769003212451935
epoch£º627	 i:5 	 global-step:12545	 l-p:0.1250772625207901
epoch£º627	 i:6 	 global-step:12546	 l-p:0.15166112780570984
epoch£º627	 i:7 	 global-step:12547	 l-p:0.033789198845624924
epoch£º627	 i:8 	 global-step:12548	 l-p:0.15661020576953888
epoch£º627	 i:9 	 global-step:12549	 l-p:0.15030887722969055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:628
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0697, 4.8843, 4.7896],
        [5.0697, 5.0694, 5.0697],
        [5.0697, 5.0688, 5.0697],
        [5.0697, 5.0047, 5.0439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:628, step:0 
model_pd.l_p.mean(): 0.0731547623872757 
model_pd.l_d.mean(): -20.619699478149414 
model_pd.lagr.mean(): -20.546545028686523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4251], device='cuda:0')), ('power', tensor([-21.4455], device='cuda:0'))])
epoch£º628	 i:0 	 global-step:12560	 l-p:0.0731547623872757
epoch£º628	 i:1 	 global-step:12561	 l-p:0.14633944630622864
epoch£º628	 i:2 	 global-step:12562	 l-p:0.17654548585414886
epoch£º628	 i:3 	 global-step:12563	 l-p:0.17066852748394012
epoch£º628	 i:4 	 global-step:12564	 l-p:0.10218694806098938
epoch£º628	 i:5 	 global-step:12565	 l-p:0.14134302735328674
epoch£º628	 i:6 	 global-step:12566	 l-p:0.13006429374217987
epoch£º628	 i:7 	 global-step:12567	 l-p:0.12339639663696289
epoch£º628	 i:8 	 global-step:12568	 l-p:0.16077382862567902
epoch£º628	 i:9 	 global-step:12569	 l-p:0.11052541434764862
====================================================================================================
====================================================================================================
====================================================================================================

epoch:629
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0396, 5.0388, 5.0396],
        [5.0396, 4.8589, 4.8104],
        [5.0396, 5.0351, 5.0394],
        [5.0396, 5.2350, 5.0777]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:629, step:0 
model_pd.l_p.mean(): 0.14613565802574158 
model_pd.l_d.mean(): -20.88764762878418 
model_pd.lagr.mean(): -20.741512298583984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3959], device='cuda:0')), ('power', tensor([-21.6882], device='cuda:0'))])
epoch£º629	 i:0 	 global-step:12580	 l-p:0.14613565802574158
epoch£º629	 i:1 	 global-step:12581	 l-p:0.1343589425086975
epoch£º629	 i:2 	 global-step:12582	 l-p:0.1316763013601303
epoch£º629	 i:3 	 global-step:12583	 l-p:0.12834309041500092
epoch£º629	 i:4 	 global-step:12584	 l-p:0.19905978441238403
epoch£º629	 i:5 	 global-step:12585	 l-p:0.09660118073225021
epoch£º629	 i:6 	 global-step:12586	 l-p:0.1104046106338501
epoch£º629	 i:7 	 global-step:12587	 l-p:0.12085789442062378
epoch£º629	 i:8 	 global-step:12588	 l-p:0.22842083871364594
epoch£º629	 i:9 	 global-step:12589	 l-p:0.16216784715652466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:630
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0240, 5.0239, 5.0240],
        [5.0240, 4.8417, 4.7937],
        [5.0240, 4.8326, 4.7199],
        [5.0240, 5.0240, 5.0240]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:630, step:0 
model_pd.l_p.mean(): 0.16459761559963226 
model_pd.l_d.mean(): -19.90862464904785 
model_pd.lagr.mean(): -19.74402618408203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4992], device='cuda:0')), ('power', tensor([-20.7979], device='cuda:0'))])
epoch£º630	 i:0 	 global-step:12600	 l-p:0.16459761559963226
epoch£º630	 i:1 	 global-step:12601	 l-p:0.16922351717948914
epoch£º630	 i:2 	 global-step:12602	 l-p:0.21345193684101105
epoch£º630	 i:3 	 global-step:12603	 l-p:0.1076306700706482
epoch£º630	 i:4 	 global-step:12604	 l-p:0.11371413618326187
epoch£º630	 i:5 	 global-step:12605	 l-p:0.13930834829807281
epoch£º630	 i:6 	 global-step:12606	 l-p:0.1014585867524147
epoch£º630	 i:7 	 global-step:12607	 l-p:0.12123902887105942
epoch£º630	 i:8 	 global-step:12608	 l-p:0.13823947310447693
epoch£º630	 i:9 	 global-step:12609	 l-p:0.1214711144566536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:631
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0602, 5.0437, 5.0580],
        [5.0602, 4.9939, 4.7500],
        [5.0602, 5.0602, 5.0602],
        [5.0602, 4.9341, 4.9654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:631, step:0 
model_pd.l_p.mean(): 0.11755160987377167 
model_pd.l_d.mean(): -20.823226928710938 
model_pd.lagr.mean(): -20.70567512512207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3953], device='cuda:0')), ('power', tensor([-21.6220], device='cuda:0'))])
epoch£º631	 i:0 	 global-step:12620	 l-p:0.11755160987377167
epoch£º631	 i:1 	 global-step:12621	 l-p:0.11040879040956497
epoch£º631	 i:2 	 global-step:12622	 l-p:0.14612096548080444
epoch£º631	 i:3 	 global-step:12623	 l-p:0.15350939333438873
epoch£º631	 i:4 	 global-step:12624	 l-p:0.20180392265319824
epoch£º631	 i:5 	 global-step:12625	 l-p:0.1756943017244339
epoch£º631	 i:6 	 global-step:12626	 l-p:0.0909474641084671
epoch£º631	 i:7 	 global-step:12627	 l-p:0.12485041469335556
epoch£º631	 i:8 	 global-step:12628	 l-p:0.12923261523246765
epoch£º631	 i:9 	 global-step:12629	 l-p:0.12460099160671234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:632
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0550, 5.0006, 5.0367],
        [5.0550, 5.0381, 5.0527],
        [5.0550, 5.0498, 5.0546],
        [5.0550, 5.0509, 5.0547]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:632, step:0 
model_pd.l_p.mean(): 0.12186553329229355 
model_pd.l_d.mean(): -19.34954833984375 
model_pd.lagr.mean(): -19.22768211364746 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5005], device='cuda:0')), ('power', tensor([-20.2298], device='cuda:0'))])
epoch£º632	 i:0 	 global-step:12640	 l-p:0.12186553329229355
epoch£º632	 i:1 	 global-step:12641	 l-p:0.1530599147081375
epoch£º632	 i:2 	 global-step:12642	 l-p:0.13050679862499237
epoch£º632	 i:3 	 global-step:12643	 l-p:0.14734111726284027
epoch£º632	 i:4 	 global-step:12644	 l-p:0.11758308857679367
epoch£º632	 i:5 	 global-step:12645	 l-p:0.13447219133377075
epoch£º632	 i:6 	 global-step:12646	 l-p:0.12432369589805603
epoch£º632	 i:7 	 global-step:12647	 l-p:0.16289447247982025
epoch£º632	 i:8 	 global-step:12648	 l-p:0.11250323802232742
epoch£º632	 i:9 	 global-step:12649	 l-p:0.19141514599323273
====================================================================================================
====================================================================================================
====================================================================================================

epoch:633
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0265, 5.0122, 5.0248],
        [5.0265, 4.8360, 4.6992],
        [5.0265, 4.9157, 4.9554],
        [5.0265, 5.5661, 5.6363]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:633, step:0 
model_pd.l_p.mean(): 0.13534437119960785 
model_pd.l_d.mean(): -19.711681365966797 
model_pd.lagr.mean(): -19.576337814331055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5093], device='cuda:0')), ('power', tensor([-20.6078], device='cuda:0'))])
epoch£º633	 i:0 	 global-step:12660	 l-p:0.13534437119960785
epoch£º633	 i:1 	 global-step:12661	 l-p:0.15490110218524933
epoch£º633	 i:2 	 global-step:12662	 l-p:0.12254207581281662
epoch£º633	 i:3 	 global-step:12663	 l-p:0.1592656373977661
epoch£º633	 i:4 	 global-step:12664	 l-p:0.12535397708415985
epoch£º633	 i:5 	 global-step:12665	 l-p:0.19074071943759918
epoch£º633	 i:6 	 global-step:12666	 l-p:0.15049049258232117
epoch£º633	 i:7 	 global-step:12667	 l-p:0.18363478779792786
epoch£º633	 i:8 	 global-step:12668	 l-p:0.14028429985046387
epoch£º633	 i:9 	 global-step:12669	 l-p:0.09534585475921631
====================================================================================================
====================================================================================================
====================================================================================================

epoch:634
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0519, 5.0519, 5.0519],
        [5.0519, 5.0481, 5.0517],
        [5.0519, 4.9502, 4.9916],
        [5.0519, 4.9599, 5.0024]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:634, step:0 
model_pd.l_p.mean(): 0.1244211196899414 
model_pd.l_d.mean(): -19.91728401184082 
model_pd.lagr.mean(): -19.792861938476562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5009], device='cuda:0')), ('power', tensor([-20.8085], device='cuda:0'))])
epoch£º634	 i:0 	 global-step:12680	 l-p:0.1244211196899414
epoch£º634	 i:1 	 global-step:12681	 l-p:0.18655623495578766
epoch£º634	 i:2 	 global-step:12682	 l-p:0.16688770055770874
epoch£º634	 i:3 	 global-step:12683	 l-p:0.16424492001533508
epoch£º634	 i:4 	 global-step:12684	 l-p:0.044201549142599106
epoch£º634	 i:5 	 global-step:12685	 l-p:0.14535148441791534
epoch£º634	 i:6 	 global-step:12686	 l-p:0.12371272593736649
epoch£º634	 i:7 	 global-step:12687	 l-p:0.06353028863668442
epoch£º634	 i:8 	 global-step:12688	 l-p:0.10915844887495041
epoch£º634	 i:9 	 global-step:12689	 l-p:0.13907478749752045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:635
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1248, 4.9813, 4.9954],
        [5.1248, 4.9907, 5.0140],
        [5.1248, 5.1239, 5.1248],
        [5.1248, 4.9977, 4.7760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:635, step:0 
model_pd.l_p.mean(): 0.14122532308101654 
model_pd.l_d.mean(): -20.449777603149414 
model_pd.lagr.mean(): -20.308551788330078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4449], device='cuda:0')), ('power', tensor([-21.2929], device='cuda:0'))])
epoch£º635	 i:0 	 global-step:12700	 l-p:0.14122532308101654
epoch£º635	 i:1 	 global-step:12701	 l-p:0.1314004808664322
epoch£º635	 i:2 	 global-step:12702	 l-p:0.10982529073953629
epoch£º635	 i:3 	 global-step:12703	 l-p:0.13896751403808594
epoch£º635	 i:4 	 global-step:12704	 l-p:0.03975846245884895
epoch£º635	 i:5 	 global-step:12705	 l-p:0.0946388766169548
epoch£º635	 i:6 	 global-step:12706	 l-p:0.13778822124004364
epoch£º635	 i:7 	 global-step:12707	 l-p:0.12303786724805832
epoch£º635	 i:8 	 global-step:12708	 l-p:0.10704699158668518
epoch£º635	 i:9 	 global-step:12709	 l-p:0.3005058169364929
====================================================================================================
====================================================================================================
====================================================================================================

epoch:636
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1605, 5.1016, 5.1389],
        [5.1605, 5.1432, 5.1581],
        [5.1605, 5.1123, 4.8730],
        [5.1605, 5.1604, 5.1605]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:636, step:0 
model_pd.l_p.mean(): 0.11412680149078369 
model_pd.l_d.mean(): -20.330562591552734 
model_pd.lagr.mean(): -20.2164363861084 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4168], device='cuda:0')), ('power', tensor([-21.1424], device='cuda:0'))])
epoch£º636	 i:0 	 global-step:12720	 l-p:0.11412680149078369
epoch£º636	 i:1 	 global-step:12721	 l-p:0.1207137480378151
epoch£º636	 i:2 	 global-step:12722	 l-p:0.10047193616628647
epoch£º636	 i:3 	 global-step:12723	 l-p:0.12059998512268066
epoch£º636	 i:4 	 global-step:12724	 l-p:0.029290875419974327
epoch£º636	 i:5 	 global-step:12725	 l-p:0.06282858550548553
epoch£º636	 i:6 	 global-step:12726	 l-p:0.13213764131069183
epoch£º636	 i:7 	 global-step:12727	 l-p:-0.5205538868904114
epoch£º636	 i:8 	 global-step:12728	 l-p:0.17518018186092377
epoch£º636	 i:9 	 global-step:12729	 l-p:0.15681883692741394
====================================================================================================
====================================================================================================
====================================================================================================

epoch:637
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1335, 4.9639, 4.9338],
        [5.1335, 5.7058, 5.7954],
        [5.1335, 5.0080, 5.0375],
        [5.1335, 5.1169, 5.1312]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:637, step:0 
model_pd.l_p.mean(): 0.15276989340782166 
model_pd.l_d.mean(): -19.311534881591797 
model_pd.lagr.mean(): -19.15876579284668 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4807], device='cuda:0')), ('power', tensor([-20.1705], device='cuda:0'))])
epoch£º637	 i:0 	 global-step:12740	 l-p:0.15276989340782166
epoch£º637	 i:1 	 global-step:12741	 l-p:0.12476925551891327
epoch£º637	 i:2 	 global-step:12742	 l-p:0.09555977582931519
epoch£º637	 i:3 	 global-step:12743	 l-p:0.14194749295711517
epoch£º637	 i:4 	 global-step:12744	 l-p:0.1170785129070282
epoch£º637	 i:5 	 global-step:12745	 l-p:0.12401538342237473
epoch£º637	 i:6 	 global-step:12746	 l-p:-0.060395728796720505
epoch£º637	 i:7 	 global-step:12747	 l-p:0.11776580661535263
epoch£º637	 i:8 	 global-step:12748	 l-p:0.1665552854537964
epoch£º637	 i:9 	 global-step:12749	 l-p:0.0885024219751358
====================================================================================================
====================================================================================================
====================================================================================================

epoch:638
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1039, 5.1036, 5.1039],
        [5.1039, 5.1039, 5.1039],
        [5.1039, 5.3125, 5.1595],
        [5.1039, 5.5169, 5.4920]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:638, step:0 
model_pd.l_p.mean(): 0.13819009065628052 
model_pd.l_d.mean(): -20.25737762451172 
model_pd.lagr.mean(): -20.11918830871582 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4994], device='cuda:0')), ('power', tensor([-21.1535], device='cuda:0'))])
epoch£º638	 i:0 	 global-step:12760	 l-p:0.13819009065628052
epoch£º638	 i:1 	 global-step:12761	 l-p:0.08522231131792068
epoch£º638	 i:2 	 global-step:12762	 l-p:0.033032339066267014
epoch£º638	 i:3 	 global-step:12763	 l-p:0.1221872866153717
epoch£º638	 i:4 	 global-step:12764	 l-p:0.1408187597990036
epoch£º638	 i:5 	 global-step:12765	 l-p:0.10138009488582611
epoch£º638	 i:6 	 global-step:12766	 l-p:0.15415388345718384
epoch£º638	 i:7 	 global-step:12767	 l-p:0.1460770070552826
epoch£º638	 i:8 	 global-step:12768	 l-p:0.14122407138347626
epoch£º638	 i:9 	 global-step:12769	 l-p:0.1302623152732849
====================================================================================================
====================================================================================================
====================================================================================================

epoch:639
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0925, 5.5366, 5.5337],
        [5.0925, 5.0887, 5.0923],
        [5.0925, 4.9095, 4.8477],
        [5.0925, 4.9165, 4.8795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:639, step:0 
model_pd.l_p.mean(): 0.1038501039147377 
model_pd.l_d.mean(): -18.201711654663086 
model_pd.lagr.mean(): -18.097862243652344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6318], device='cuda:0')), ('power', tensor([-19.1964], device='cuda:0'))])
epoch£º639	 i:0 	 global-step:12780	 l-p:0.1038501039147377
epoch£º639	 i:1 	 global-step:12781	 l-p:0.1838405728340149
epoch£º639	 i:2 	 global-step:12782	 l-p:0.12819671630859375
epoch£º639	 i:3 	 global-step:12783	 l-p:0.08810092508792877
epoch£º639	 i:4 	 global-step:12784	 l-p:0.1394854485988617
epoch£º639	 i:5 	 global-step:12785	 l-p:0.07044858485460281
epoch£º639	 i:6 	 global-step:12786	 l-p:0.1404285728931427
epoch£º639	 i:7 	 global-step:12787	 l-p:0.14046132564544678
epoch£º639	 i:8 	 global-step:12788	 l-p:-0.5322138667106628
epoch£º639	 i:9 	 global-step:12789	 l-p:0.13662485778331757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:640
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1368, 4.9596, 4.9091],
        [5.1368, 4.9555, 4.8189],
        [5.1368, 5.1368, 5.1368],
        [5.1368, 5.1192, 5.1343]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:640, step:0 
model_pd.l_p.mean(): 0.09858500957489014 
model_pd.l_d.mean(): -19.884536743164062 
model_pd.lagr.mean(): -19.785951614379883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5209], device='cuda:0')), ('power', tensor([-20.7959], device='cuda:0'))])
epoch£º640	 i:0 	 global-step:12800	 l-p:0.09858500957489014
epoch£º640	 i:1 	 global-step:12801	 l-p:0.14117993414402008
epoch£º640	 i:2 	 global-step:12802	 l-p:-0.22391945123672485
epoch£º640	 i:3 	 global-step:12803	 l-p:0.09181184321641922
epoch£º640	 i:4 	 global-step:12804	 l-p:0.1699938029050827
epoch£º640	 i:5 	 global-step:12805	 l-p:0.11829361319541931
epoch£º640	 i:6 	 global-step:12806	 l-p:0.1276358962059021
epoch£º640	 i:7 	 global-step:12807	 l-p:0.12122472375631332
epoch£º640	 i:8 	 global-step:12808	 l-p:0.13053163886070251
epoch£º640	 i:9 	 global-step:12809	 l-p:0.13961419463157654
====================================================================================================
====================================================================================================
====================================================================================================

epoch:641
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1215, 5.1201, 5.1214],
        [5.1215, 5.0824, 4.8394],
        [5.1215, 5.1215, 5.1215],
        [5.1215, 5.0616, 5.0995]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:641, step:0 
model_pd.l_p.mean(): 0.027712926268577576 
model_pd.l_d.mean(): -20.476747512817383 
model_pd.lagr.mean(): -20.449033737182617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4488], device='cuda:0')), ('power', tensor([-21.3244], device='cuda:0'))])
epoch£º641	 i:0 	 global-step:12820	 l-p:0.027712926268577576
epoch£º641	 i:1 	 global-step:12821	 l-p:0.0924888402223587
epoch£º641	 i:2 	 global-step:12822	 l-p:0.10692255944013596
epoch£º641	 i:3 	 global-step:12823	 l-p:0.11071959882974625
epoch£º641	 i:4 	 global-step:12824	 l-p:0.15273135900497437
epoch£º641	 i:5 	 global-step:12825	 l-p:0.18079671263694763
epoch£º641	 i:6 	 global-step:12826	 l-p:0.1344955563545227
epoch£º641	 i:7 	 global-step:12827	 l-p:0.01695322059094906
epoch£º641	 i:8 	 global-step:12828	 l-p:0.13407450914382935
epoch£º641	 i:9 	 global-step:12829	 l-p:0.13571080565452576
====================================================================================================
====================================================================================================
====================================================================================================

epoch:642
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1070, 5.1068, 5.1070],
        [5.1070, 4.9309, 4.8937],
        [5.1070, 5.1070, 5.1070],
        [5.1070, 5.5772, 5.5916]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:642, step:0 
model_pd.l_p.mean(): -0.0014019751688465476 
model_pd.l_d.mean(): -19.04306411743164 
model_pd.lagr.mean(): -19.044466018676758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5657], device='cuda:0')), ('power', tensor([-19.9851], device='cuda:0'))])
epoch£º642	 i:0 	 global-step:12840	 l-p:-0.0014019751688465476
epoch£º642	 i:1 	 global-step:12841	 l-p:0.15711797773838043
epoch£º642	 i:2 	 global-step:12842	 l-p:0.18298178911209106
epoch£º642	 i:3 	 global-step:12843	 l-p:0.13014914095401764
epoch£º642	 i:4 	 global-step:12844	 l-p:0.09416566789150238
epoch£º642	 i:5 	 global-step:12845	 l-p:0.14371420443058014
epoch£º642	 i:6 	 global-step:12846	 l-p:0.12846826016902924
epoch£º642	 i:7 	 global-step:12847	 l-p:0.11365228146314621
epoch£º642	 i:8 	 global-step:12848	 l-p:0.0940636619925499
epoch£º642	 i:9 	 global-step:12849	 l-p:0.1795114427804947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:643
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0518, 4.9850, 5.0254],
        [5.0518, 5.0518, 5.0518],
        [5.0518, 4.9216, 4.6876],
        [5.0518, 5.0518, 5.0518]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:643, step:0 
model_pd.l_p.mean(): 0.09883120656013489 
model_pd.l_d.mean(): -18.889719009399414 
model_pd.lagr.mean(): -18.7908878326416 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5594], device='cuda:0')), ('power', tensor([-19.8224], device='cuda:0'))])
epoch£º643	 i:0 	 global-step:12860	 l-p:0.09883120656013489
epoch£º643	 i:1 	 global-step:12861	 l-p:0.12178035825490952
epoch£º643	 i:2 	 global-step:12862	 l-p:0.1319582164287567
epoch£º643	 i:3 	 global-step:12863	 l-p:0.10452650487422943
epoch£º643	 i:4 	 global-step:12864	 l-p:0.2250640094280243
epoch£º643	 i:5 	 global-step:12865	 l-p:0.1021108627319336
epoch£º643	 i:6 	 global-step:12866	 l-p:0.17923255264759064
epoch£º643	 i:7 	 global-step:12867	 l-p:0.1220509335398674
epoch£º643	 i:8 	 global-step:12868	 l-p:0.13544970750808716
epoch£º643	 i:9 	 global-step:12869	 l-p:0.15368019044399261
====================================================================================================
====================================================================================================
====================================================================================================

epoch:644
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4607,  0.3558,  1.0000,  0.2748,
          1.0000,  0.7724, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1271,  0.0639,  1.0000,  0.0321,
          1.0000,  0.5028, 31.6228]], device='cuda:0')
 pt:tensor([[5.0678, 4.8760, 4.7827],
        [5.0678, 4.9397, 4.7060],
        [5.0678, 4.8869, 4.8465],
        [5.0678, 4.9692, 5.0117]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:644, step:0 
model_pd.l_p.mean(): 0.14843623340129852 
model_pd.l_d.mean(): -20.11914825439453 
model_pd.lagr.mean(): -19.970712661743164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4984], device='cuda:0')), ('power', tensor([-21.0115], device='cuda:0'))])
epoch£º644	 i:0 	 global-step:12880	 l-p:0.14843623340129852
epoch£º644	 i:1 	 global-step:12881	 l-p:0.1806899607181549
epoch£º644	 i:2 	 global-step:12882	 l-p:0.1463845819234848
epoch£º644	 i:3 	 global-step:12883	 l-p:0.12813563644886017
epoch£º644	 i:4 	 global-step:12884	 l-p:0.16119182109832764
epoch£º644	 i:5 	 global-step:12885	 l-p:0.14958429336547852
epoch£º644	 i:6 	 global-step:12886	 l-p:0.13367529213428497
epoch£º644	 i:7 	 global-step:12887	 l-p:0.05483675003051758
epoch£º644	 i:8 	 global-step:12888	 l-p:0.07154921442270279
epoch£º644	 i:9 	 global-step:12889	 l-p:0.0715993344783783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:645
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1119, 5.0542, 5.0916],
        [5.1119, 4.9452, 4.9314],
        [5.1119, 5.0853, 4.8413],
        [5.1119, 5.1087, 5.1118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:645, step:0 
model_pd.l_p.mean(): 0.14595524966716766 
model_pd.l_d.mean(): -19.904897689819336 
model_pd.lagr.mean(): -19.758941650390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5004], device='cuda:0')), ('power', tensor([-20.7954], device='cuda:0'))])
epoch£º645	 i:0 	 global-step:12900	 l-p:0.14595524966716766
epoch£º645	 i:1 	 global-step:12901	 l-p:-0.010928821749985218
epoch£º645	 i:2 	 global-step:12902	 l-p:0.11876259744167328
epoch£º645	 i:3 	 global-step:12903	 l-p:0.11280976980924606
epoch£º645	 i:4 	 global-step:12904	 l-p:0.12264156341552734
epoch£º645	 i:5 	 global-step:12905	 l-p:0.03903267905116081
epoch£º645	 i:6 	 global-step:12906	 l-p:0.1430250108242035
epoch£º645	 i:7 	 global-step:12907	 l-p:0.08681169897317886
epoch£º645	 i:8 	 global-step:12908	 l-p:0.10201705992221832
epoch£º645	 i:9 	 global-step:12909	 l-p:0.12592248618602753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:646
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9387,  0.9192,  1.0000,  0.9000,
          1.0000,  0.9792, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228]], device='cuda:0')
 pt:tensor([[5.1842, 5.1109, 4.8708],
        [5.1842, 5.6744, 5.6988],
        [5.1842, 5.7146, 5.7684],
        [5.1842, 5.0147, 4.9842]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:646, step:0 
model_pd.l_p.mean(): 0.10310766845941544 
model_pd.l_d.mean(): -19.2421875 
model_pd.lagr.mean(): -19.139080047607422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5256], device='cuda:0')), ('power', tensor([-20.1464], device='cuda:0'))])
epoch£º646	 i:0 	 global-step:12920	 l-p:0.10310766845941544
epoch£º646	 i:1 	 global-step:12921	 l-p:0.1593012660741806
epoch£º646	 i:2 	 global-step:12922	 l-p:0.12475129216909409
epoch£º646	 i:3 	 global-step:12923	 l-p:0.1358000487089157
epoch£º646	 i:4 	 global-step:12924	 l-p:-0.6923955678939819
epoch£º646	 i:5 	 global-step:12925	 l-p:0.04989223554730415
epoch£º646	 i:6 	 global-step:12926	 l-p:0.12066518515348434
epoch£º646	 i:7 	 global-step:12927	 l-p:0.12399408221244812
epoch£º646	 i:8 	 global-step:12928	 l-p:0.13433662056922913
epoch£º646	 i:9 	 global-step:12929	 l-p:0.2239307165145874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:647
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1622, 5.1622, 5.1622],
        [5.1622, 5.1619, 5.1622],
        [5.1622, 5.0428, 4.8142],
        [5.1622, 5.1618, 5.1622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:647, step:0 
model_pd.l_p.mean(): 0.13682326674461365 
model_pd.l_d.mean(): -19.90982437133789 
model_pd.lagr.mean(): -19.773000717163086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5031], device='cuda:0')), ('power', tensor([-20.8032], device='cuda:0'))])
epoch£º647	 i:0 	 global-step:12940	 l-p:0.13682326674461365
epoch£º647	 i:1 	 global-step:12941	 l-p:0.13121818006038666
epoch£º647	 i:2 	 global-step:12942	 l-p:0.12407100200653076
epoch£º647	 i:3 	 global-step:12943	 l-p:0.12074117362499237
epoch£º647	 i:4 	 global-step:12944	 l-p:0.1106952354311943
epoch£º647	 i:5 	 global-step:12945	 l-p:0.12891888618469238
epoch£º647	 i:6 	 global-step:12946	 l-p:-0.0018615865847095847
epoch£º647	 i:7 	 global-step:12947	 l-p:0.09973815083503723
epoch£º647	 i:8 	 global-step:12948	 l-p:0.14772285521030426
epoch£º647	 i:9 	 global-step:12949	 l-p:0.09268251061439514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:648
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0843, 5.0553, 4.8088],
        [5.0843, 5.0843, 5.0843],
        [5.0843, 5.0466, 5.0750],
        [5.0843, 5.0797, 5.0840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:648, step:0 
model_pd.l_p.mean(): 0.1187722310423851 
model_pd.l_d.mean(): -20.621671676635742 
model_pd.lagr.mean(): -20.502899169921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4374], device='cuda:0')), ('power', tensor([-21.4603], device='cuda:0'))])
epoch£º648	 i:0 	 global-step:12960	 l-p:0.1187722310423851
epoch£º648	 i:1 	 global-step:12961	 l-p:0.1275041550397873
epoch£º648	 i:2 	 global-step:12962	 l-p:0.17363126575946808
epoch£º648	 i:3 	 global-step:12963	 l-p:0.14339536428451538
epoch£º648	 i:4 	 global-step:12964	 l-p:0.18089890480041504
epoch£º648	 i:5 	 global-step:12965	 l-p:0.14567746222019196
epoch£º648	 i:6 	 global-step:12966	 l-p:0.13188384473323822
epoch£º648	 i:7 	 global-step:12967	 l-p:0.09923151135444641
epoch£º648	 i:8 	 global-step:12968	 l-p:0.1203780546784401
epoch£º648	 i:9 	 global-step:12969	 l-p:0.09582029283046722
====================================================================================================
====================================================================================================
====================================================================================================

epoch:649
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0743, 5.0372, 5.0653],
        [5.0743, 5.0714, 5.0742],
        [5.0743, 5.0720, 5.0743],
        [5.0743, 4.8923, 4.8519]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:649, step:0 
model_pd.l_p.mean(): 0.12256433069705963 
model_pd.l_d.mean(): -19.927785873413086 
model_pd.lagr.mean(): -19.805221557617188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4748], device='cuda:0')), ('power', tensor([-20.7922], device='cuda:0'))])
epoch£º649	 i:0 	 global-step:12980	 l-p:0.12256433069705963
epoch£º649	 i:1 	 global-step:12981	 l-p:0.12158086150884628
epoch£º649	 i:2 	 global-step:12982	 l-p:0.12950649857521057
epoch£º649	 i:3 	 global-step:12983	 l-p:0.15068887174129486
epoch£º649	 i:4 	 global-step:12984	 l-p:0.18270377814769745
epoch£º649	 i:5 	 global-step:12985	 l-p:0.13316643238067627
epoch£º649	 i:6 	 global-step:12986	 l-p:0.13246460258960724
epoch£º649	 i:7 	 global-step:12987	 l-p:0.15217198431491852
epoch£º649	 i:8 	 global-step:12988	 l-p:0.06018725410103798
epoch£º649	 i:9 	 global-step:12989	 l-p:0.21235564351081848
====================================================================================================
====================================================================================================
====================================================================================================

epoch:650
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0441, 4.9498, 4.9936],
        [5.0441, 4.9959, 5.0298],
        [5.0441, 4.8784, 4.6667],
        [5.0441, 4.8938, 4.9109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:650, step:0 
model_pd.l_p.mean(): 0.1028168722987175 
model_pd.l_d.mean(): -20.5644588470459 
model_pd.lagr.mean(): -20.461641311645508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4515], device='cuda:0')), ('power', tensor([-21.4166], device='cuda:0'))])
epoch£º650	 i:0 	 global-step:13000	 l-p:0.1028168722987175
epoch£º650	 i:1 	 global-step:13001	 l-p:0.11169042438268661
epoch£º650	 i:2 	 global-step:13002	 l-p:0.16533702611923218
epoch£º650	 i:3 	 global-step:13003	 l-p:0.06590919941663742
epoch£º650	 i:4 	 global-step:13004	 l-p:0.13944284617900848
epoch£º650	 i:5 	 global-step:13005	 l-p:0.1533880978822708
epoch£º650	 i:6 	 global-step:13006	 l-p:0.15379731357097626
epoch£º650	 i:7 	 global-step:13007	 l-p:0.15453441441059113
epoch£º650	 i:8 	 global-step:13008	 l-p:0.1494317352771759
epoch£º650	 i:9 	 global-step:13009	 l-p:0.1567728966474533
====================================================================================================
====================================================================================================
====================================================================================================

epoch:651
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0672, 5.0672, 5.0672],
        [5.0672, 5.4985, 5.4854],
        [5.0672, 5.0672, 5.0672],
        [5.0672, 4.8962, 4.8834]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:651, step:0 
model_pd.l_p.mean(): 0.16086171567440033 
model_pd.l_d.mean(): -20.143800735473633 
model_pd.lagr.mean(): -19.982938766479492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4982], device='cuda:0')), ('power', tensor([-21.0364], device='cuda:0'))])
epoch£º651	 i:0 	 global-step:13020	 l-p:0.16086171567440033
epoch£º651	 i:1 	 global-step:13021	 l-p:0.12008873373270035
epoch£º651	 i:2 	 global-step:13022	 l-p:0.08746258169412613
epoch£º651	 i:3 	 global-step:13023	 l-p:0.11882581561803818
epoch£º651	 i:4 	 global-step:13024	 l-p:0.12884493172168732
epoch£º651	 i:5 	 global-step:13025	 l-p:0.13692297041416168
epoch£º651	 i:6 	 global-step:13026	 l-p:0.16157609224319458
epoch£º651	 i:7 	 global-step:13027	 l-p:0.18384814262390137
epoch£º651	 i:8 	 global-step:13028	 l-p:0.23973746597766876
epoch£º651	 i:9 	 global-step:13029	 l-p:0.09964946657419205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:652
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0388, 4.9262, 4.9673],
        [5.0388, 5.0271, 5.0376],
        [5.0388, 5.0388, 5.0388],
        [5.0388, 5.0375, 5.0388]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:652, step:0 
model_pd.l_p.mean(): 0.10601653158664703 
model_pd.l_d.mean(): -20.40087890625 
model_pd.lagr.mean(): -20.294862747192383 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-21.2497], device='cuda:0'))])
epoch£º652	 i:0 	 global-step:13040	 l-p:0.10601653158664703
epoch£º652	 i:1 	 global-step:13041	 l-p:0.1593390852212906
epoch£º652	 i:2 	 global-step:13042	 l-p:0.12634800374507904
epoch£º652	 i:3 	 global-step:13043	 l-p:0.15623262524604797
epoch£º652	 i:4 	 global-step:13044	 l-p:0.08180730789899826
epoch£º652	 i:5 	 global-step:13045	 l-p:0.1799490749835968
epoch£º652	 i:6 	 global-step:13046	 l-p:0.152130126953125
epoch£º652	 i:7 	 global-step:13047	 l-p:0.1572248339653015
epoch£º652	 i:8 	 global-step:13048	 l-p:0.12556099891662598
epoch£º652	 i:9 	 global-step:13049	 l-p:0.10245410352945328
====================================================================================================
====================================================================================================
====================================================================================================

epoch:653
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1218, 5.2476, 5.0492],
        [5.1218, 5.0226, 4.7793],
        [5.1218, 5.2555, 5.0607],
        [5.1218, 5.0552, 5.0955]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:653, step:0 
model_pd.l_p.mean(): 0.16575004160404205 
model_pd.l_d.mean(): -20.02153778076172 
model_pd.lagr.mean(): -19.85578727722168 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4763], device='cuda:0')), ('power', tensor([-20.8893], device='cuda:0'))])
epoch£º653	 i:0 	 global-step:13060	 l-p:0.16575004160404205
epoch£º653	 i:1 	 global-step:13061	 l-p:0.11932383477687836
epoch£º653	 i:2 	 global-step:13062	 l-p:0.1469036191701889
epoch£º653	 i:3 	 global-step:13063	 l-p:0.12508653104305267
epoch£º653	 i:4 	 global-step:13064	 l-p:0.1404201239347458
epoch£º653	 i:5 	 global-step:13065	 l-p:-0.012133479118347168
epoch£º653	 i:6 	 global-step:13066	 l-p:0.08373918384313583
epoch£º653	 i:7 	 global-step:13067	 l-p:0.1283866912126541
epoch£º653	 i:8 	 global-step:13068	 l-p:0.11558734625577927
epoch£º653	 i:9 	 global-step:13069	 l-p:0.06195563077926636
====================================================================================================
====================================================================================================
====================================================================================================

epoch:654
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1432, 5.2528, 5.0469],
        [5.1432, 4.9867, 4.9905],
        [5.1432, 5.1432, 5.1432],
        [5.1432, 4.9692, 4.9414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:654, step:0 
model_pd.l_p.mean(): 0.12254320085048676 
model_pd.l_d.mean(): -19.505870819091797 
model_pd.lagr.mean(): -19.38332748413086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4881], device='cuda:0')), ('power', tensor([-20.3761], device='cuda:0'))])
epoch£º654	 i:0 	 global-step:13080	 l-p:0.12254320085048676
epoch£º654	 i:1 	 global-step:13081	 l-p:0.03919009119272232
epoch£º654	 i:2 	 global-step:13082	 l-p:0.14111219346523285
epoch£º654	 i:3 	 global-step:13083	 l-p:0.08459140360355377
epoch£º654	 i:4 	 global-step:13084	 l-p:0.1369897723197937
epoch£º654	 i:5 	 global-step:13085	 l-p:0.06502856314182281
epoch£º654	 i:6 	 global-step:13086	 l-p:0.1278127133846283
epoch£º654	 i:7 	 global-step:13087	 l-p:0.11029060930013657
epoch£º654	 i:8 	 global-step:13088	 l-p:0.24761854112148285
epoch£º654	 i:9 	 global-step:13089	 l-p:0.13533104956150055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:655
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4043,  0.2990,  1.0000,  0.2211,
          1.0000,  0.7394, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1592,  0.0863,  1.0000,  0.0468,
          1.0000,  0.5420, 31.6228]], device='cuda:0')
 pt:tensor([[5.1714, 4.9871, 4.8469],
        [5.1714, 5.0152, 4.8118],
        [5.1714, 4.9942, 4.9531],
        [5.1714, 5.0446, 5.0757]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:655, step:0 
model_pd.l_p.mean(): 0.11286396533250809 
model_pd.l_d.mean(): -20.378999710083008 
model_pd.lagr.mean(): -20.266136169433594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4291], device='cuda:0')), ('power', tensor([-21.2045], device='cuda:0'))])
epoch£º655	 i:0 	 global-step:13100	 l-p:0.11286396533250809
epoch£º655	 i:1 	 global-step:13101	 l-p:0.056188009679317474
epoch£º655	 i:2 	 global-step:13102	 l-p:0.1414104849100113
epoch£º655	 i:3 	 global-step:13103	 l-p:0.17330777645111084
epoch£º655	 i:4 	 global-step:13104	 l-p:0.025031136348843575
epoch£º655	 i:5 	 global-step:13105	 l-p:0.1287311464548111
epoch£º655	 i:6 	 global-step:13106	 l-p:0.12962578237056732
epoch£º655	 i:7 	 global-step:13107	 l-p:0.10466024279594421
epoch£º655	 i:8 	 global-step:13108	 l-p:-0.8652136921882629
epoch£º655	 i:9 	 global-step:13109	 l-p:0.12272610515356064
====================================================================================================
====================================================================================================
====================================================================================================

epoch:656
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1328, 5.1123, 5.1296],
        [5.1328, 5.1002, 5.1256],
        [5.1328, 5.1180, 5.1309],
        [5.1328, 5.0237, 4.7829]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:656, step:0 
model_pd.l_p.mean(): 0.11760219186544418 
model_pd.l_d.mean(): -20.431488037109375 
model_pd.lagr.mean(): -20.313886642456055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4298], device='cuda:0')), ('power', tensor([-21.2587], device='cuda:0'))])
epoch£º656	 i:0 	 global-step:13120	 l-p:0.11760219186544418
epoch£º656	 i:1 	 global-step:13121	 l-p:0.12958474457263947
epoch£º656	 i:2 	 global-step:13122	 l-p:0.028193911537528038
epoch£º656	 i:3 	 global-step:13123	 l-p:0.13436537981033325
epoch£º656	 i:4 	 global-step:13124	 l-p:0.13837462663650513
epoch£º656	 i:5 	 global-step:13125	 l-p:0.21453799307346344
epoch£º656	 i:6 	 global-step:13126	 l-p:0.14451976120471954
epoch£º656	 i:7 	 global-step:13127	 l-p:0.05449238047003746
epoch£º656	 i:8 	 global-step:13128	 l-p:0.136500284075737
epoch£º656	 i:9 	 global-step:13129	 l-p:0.11942078173160553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:657
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2318,  0.1424,  1.0000,  0.0875,
          1.0000,  0.6143, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3156,  0.2149,  1.0000,  0.1463,
          1.0000,  0.6809, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5787,  0.4823,  1.0000,  0.4019,
          1.0000,  0.8333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4980,  0.3947,  1.0000,  0.3128,
          1.0000,  0.7926, 31.6228]], device='cuda:0')
 pt:tensor([[5.0945, 4.9158, 4.8871],
        [5.0945, 4.8995, 4.7759],
        [5.0945, 5.0782, 4.8311],
        [5.0945, 4.9950, 4.7476]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:657, step:0 
model_pd.l_p.mean(): 0.14059866964817047 
model_pd.l_d.mean(): -20.274282455444336 
model_pd.lagr.mean(): -20.133684158325195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4771], device='cuda:0')), ('power', tensor([-21.1476], device='cuda:0'))])
epoch£º657	 i:0 	 global-step:13140	 l-p:0.14059866964817047
epoch£º657	 i:1 	 global-step:13141	 l-p:0.08450675010681152
epoch£º657	 i:2 	 global-step:13142	 l-p:0.16895553469657898
epoch£º657	 i:3 	 global-step:13143	 l-p:0.14069485664367676
epoch£º657	 i:4 	 global-step:13144	 l-p:0.11608203500509262
epoch£º657	 i:5 	 global-step:13145	 l-p:0.10084181278944016
epoch£º657	 i:6 	 global-step:13146	 l-p:0.13650968670845032
epoch£º657	 i:7 	 global-step:13147	 l-p:0.12451490014791489
epoch£º657	 i:8 	 global-step:13148	 l-p:0.1430666297674179
epoch£º657	 i:9 	 global-step:13149	 l-p:0.005153932608664036
====================================================================================================
====================================================================================================
====================================================================================================

epoch:658
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1212, 5.1211, 5.1212],
        [5.1212, 4.9319, 4.7828],
        [5.1212, 4.9348, 4.8784],
        [5.1212, 4.9308, 4.8553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:658, step:0 
model_pd.l_p.mean(): 0.07654545456171036 
model_pd.l_d.mean(): -20.063940048217773 
model_pd.lagr.mean(): -19.987394332885742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4819], device='cuda:0')), ('power', tensor([-20.9382], device='cuda:0'))])
epoch£º658	 i:0 	 global-step:13160	 l-p:0.07654545456171036
epoch£º658	 i:1 	 global-step:13161	 l-p:0.14958691596984863
epoch£º658	 i:2 	 global-step:13162	 l-p:0.16250163316726685
epoch£º658	 i:3 	 global-step:13163	 l-p:0.10388225317001343
epoch£º658	 i:4 	 global-step:13164	 l-p:0.13828596472740173
epoch£º658	 i:5 	 global-step:13165	 l-p:0.10819285362958908
epoch£º658	 i:6 	 global-step:13166	 l-p:0.0012041091686114669
epoch£º658	 i:7 	 global-step:13167	 l-p:0.12768910825252533
epoch£º658	 i:8 	 global-step:13168	 l-p:0.14900581538677216
epoch£º658	 i:9 	 global-step:13169	 l-p:0.1243094727396965
====================================================================================================
====================================================================================================
====================================================================================================

epoch:659
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1225, 5.0950, 4.8469],
        [5.1225, 5.1222, 5.1225],
        [5.1225, 4.9292, 4.8129],
        [5.1225, 5.1216, 5.1225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:659, step:0 
model_pd.l_p.mean(): 0.12202689796686172 
model_pd.l_d.mean(): -20.319250106811523 
model_pd.lagr.mean(): -20.197223663330078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4220], device='cuda:0')), ('power', tensor([-21.1363], device='cuda:0'))])
epoch£º659	 i:0 	 global-step:13180	 l-p:0.12202689796686172
epoch£º659	 i:1 	 global-step:13181	 l-p:0.1100403219461441
epoch£º659	 i:2 	 global-step:13182	 l-p:0.16214890778064728
epoch£º659	 i:3 	 global-step:13183	 l-p:0.13305047154426575
epoch£º659	 i:4 	 global-step:13184	 l-p:0.11791185289621353
epoch£º659	 i:5 	 global-step:13185	 l-p:0.13183094561100006
epoch£º659	 i:6 	 global-step:13186	 l-p:0.10902013629674911
epoch£º659	 i:7 	 global-step:13187	 l-p:0.13097284734249115
epoch£º659	 i:8 	 global-step:13188	 l-p:0.1988499015569687
epoch£º659	 i:9 	 global-step:13189	 l-p:0.13258323073387146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:660
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0416, 5.2171, 5.0433],
        [5.0416, 4.9217, 4.9614],
        [5.0416, 5.0159, 4.7640],
        [5.0416, 5.0415, 5.0416]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:660, step:0 
model_pd.l_p.mean(): 0.14532369375228882 
model_pd.l_d.mean(): -19.527530670166016 
model_pd.lagr.mean(): -19.3822078704834 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5665], device='cuda:0')), ('power', tensor([-20.4794], device='cuda:0'))])
epoch£º660	 i:0 	 global-step:13200	 l-p:0.14532369375228882
epoch£º660	 i:1 	 global-step:13201	 l-p:0.16150961816310883
epoch£º660	 i:2 	 global-step:13202	 l-p:0.13782642781734467
epoch£º660	 i:3 	 global-step:13203	 l-p:0.1270560473203659
epoch£º660	 i:4 	 global-step:13204	 l-p:0.1708453744649887
epoch£º660	 i:5 	 global-step:13205	 l-p:0.13214083015918732
epoch£º660	 i:6 	 global-step:13206	 l-p:0.11585988104343414
epoch£º660	 i:7 	 global-step:13207	 l-p:0.1243470162153244
epoch£º660	 i:8 	 global-step:13208	 l-p:0.12487640976905823
epoch£º660	 i:9 	 global-step:13209	 l-p:0.21359889209270477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:661
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0487, 5.0477, 5.0486],
        [5.0487, 4.9468, 4.9908],
        [5.0487, 5.0217, 5.0436],
        [5.0487, 4.9011, 4.9238]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:661, step:0 
model_pd.l_p.mean(): 0.13800173997879028 
model_pd.l_d.mean(): -19.814794540405273 
model_pd.lagr.mean(): -19.67679214477539 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4904], device='cuda:0')), ('power', tensor([-20.6932], device='cuda:0'))])
epoch£º661	 i:0 	 global-step:13220	 l-p:0.13800173997879028
epoch£º661	 i:1 	 global-step:13221	 l-p:0.1328112632036209
epoch£º661	 i:2 	 global-step:13222	 l-p:0.15531399846076965
epoch£º661	 i:3 	 global-step:13223	 l-p:0.19450919330120087
epoch£º661	 i:4 	 global-step:13224	 l-p:0.11442293971776962
epoch£º661	 i:5 	 global-step:13225	 l-p:0.11042094230651855
epoch£º661	 i:6 	 global-step:13226	 l-p:0.13504642248153687
epoch£º661	 i:7 	 global-step:13227	 l-p:0.1712239384651184
epoch£º661	 i:8 	 global-step:13228	 l-p:0.10835178196430206
epoch£º661	 i:9 	 global-step:13229	 l-p:0.18574820458889008
====================================================================================================
====================================================================================================
====================================================================================================

epoch:662
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0542, 5.0542, 5.0542],
        [5.0542, 5.0542, 5.0542],
        [5.0542, 5.0321, 5.0506],
        [5.0542, 4.8790, 4.8669]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:662, step:0 
model_pd.l_p.mean(): 0.13585041463375092 
model_pd.l_d.mean(): -20.74018096923828 
model_pd.lagr.mean(): -20.60433006286621 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4059], device='cuda:0')), ('power', tensor([-21.5484], device='cuda:0'))])
epoch£º662	 i:0 	 global-step:13240	 l-p:0.13585041463375092
epoch£º662	 i:1 	 global-step:13241	 l-p:0.10230285674333572
epoch£º662	 i:2 	 global-step:13242	 l-p:0.14896313846111298
epoch£º662	 i:3 	 global-step:13243	 l-p:0.09899872541427612
epoch£º662	 i:4 	 global-step:13244	 l-p:0.10615922510623932
epoch£º662	 i:5 	 global-step:13245	 l-p:0.12789799273014069
epoch£º662	 i:6 	 global-step:13246	 l-p:0.1547061651945114
epoch£º662	 i:7 	 global-step:13247	 l-p:0.15144981443881989
epoch£º662	 i:8 	 global-step:13248	 l-p:0.14381565153598785
epoch£º662	 i:9 	 global-step:13249	 l-p:0.14599864184856415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:663
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0941, 5.2139, 5.0108],
        [5.0941, 5.0336, 5.0725],
        [5.0941, 4.8963, 4.7716],
        [5.0941, 5.0812, 5.0927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:663, step:0 
model_pd.l_p.mean(): 0.12800028920173645 
model_pd.l_d.mean(): -20.42528533935547 
model_pd.lagr.mean(): -20.297285079956055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4284], device='cuda:0')), ('power', tensor([-21.2509], device='cuda:0'))])
epoch£º663	 i:0 	 global-step:13260	 l-p:0.12800028920173645
epoch£º663	 i:1 	 global-step:13261	 l-p:0.11560876667499542
epoch£º663	 i:2 	 global-step:13262	 l-p:0.08305761218070984
epoch£º663	 i:3 	 global-step:13263	 l-p:0.10840392857789993
epoch£º663	 i:4 	 global-step:13264	 l-p:0.1358686089515686
epoch£º663	 i:5 	 global-step:13265	 l-p:0.19910447299480438
epoch£º663	 i:6 	 global-step:13266	 l-p:0.171669140458107
epoch£º663	 i:7 	 global-step:13267	 l-p:0.10226032137870789
epoch£º663	 i:8 	 global-step:13268	 l-p:0.10507150739431381
epoch£º663	 i:9 	 global-step:13269	 l-p:0.12462258338928223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:664
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0974, 5.0973, 5.0974],
        [5.0974, 4.9025, 4.7527],
        [5.0974, 5.0956, 5.0973],
        [5.0974, 4.9314, 4.9305]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:664, step:0 
model_pd.l_p.mean(): 0.1329377144575119 
model_pd.l_d.mean(): -20.221921920776367 
model_pd.lagr.mean(): -20.0889835357666 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4722], device='cuda:0')), ('power', tensor([-21.0891], device='cuda:0'))])
epoch£º664	 i:0 	 global-step:13280	 l-p:0.1329377144575119
epoch£º664	 i:1 	 global-step:13281	 l-p:0.160023495554924
epoch£º664	 i:2 	 global-step:13282	 l-p:0.12565839290618896
epoch£º664	 i:3 	 global-step:13283	 l-p:0.11467795073986053
epoch£º664	 i:4 	 global-step:13284	 l-p:0.129313662648201
epoch£º664	 i:5 	 global-step:13285	 l-p:0.1275404840707779
epoch£º664	 i:6 	 global-step:13286	 l-p:0.16571617126464844
epoch£º664	 i:7 	 global-step:13287	 l-p:0.12626953423023224
epoch£º664	 i:8 	 global-step:13288	 l-p:0.16980238258838654
epoch£º664	 i:9 	 global-step:13289	 l-p:0.05359521135687828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:665
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0681, 5.0658, 5.0681],
        [5.0681, 5.0657, 5.0681],
        [5.0681, 4.8680, 4.7269],
        [5.0681, 4.9938, 5.0367]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:665, step:0 
model_pd.l_p.mean(): 0.09500541538000107 
model_pd.l_d.mean(): -20.245223999023438 
model_pd.lagr.mean(): -20.150218963623047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4948], device='cuda:0')), ('power', tensor([-21.1363], device='cuda:0'))])
epoch£º665	 i:0 	 global-step:13300	 l-p:0.09500541538000107
epoch£º665	 i:1 	 global-step:13301	 l-p:0.13746510446071625
epoch£º665	 i:2 	 global-step:13302	 l-p:0.17708078026771545
epoch£º665	 i:3 	 global-step:13303	 l-p:0.12686799466609955
epoch£º665	 i:4 	 global-step:13304	 l-p:0.12357136607170105
epoch£º665	 i:5 	 global-step:13305	 l-p:0.13569898903369904
epoch£º665	 i:6 	 global-step:13306	 l-p:0.13606898486614227
epoch£º665	 i:7 	 global-step:13307	 l-p:0.17274746298789978
epoch£º665	 i:8 	 global-step:13308	 l-p:0.09993227571249008
epoch£º665	 i:9 	 global-step:13309	 l-p:0.14204218983650208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:666
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0634, 5.0634, 5.0634],
        [5.0634, 4.9754, 4.7192],
        [5.0634, 4.9917, 4.7347],
        [5.0634, 4.9011, 4.9094]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:666, step:0 
model_pd.l_p.mean(): 0.12425603717565536 
model_pd.l_d.mean(): -18.728708267211914 
model_pd.lagr.mean(): -18.60445213317871 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5529], device='cuda:0')), ('power', tensor([-19.6515], device='cuda:0'))])
epoch£º666	 i:0 	 global-step:13320	 l-p:0.12425603717565536
epoch£º666	 i:1 	 global-step:13321	 l-p:0.09223995357751846
epoch£º666	 i:2 	 global-step:13322	 l-p:0.13228419423103333
epoch£º666	 i:3 	 global-step:13323	 l-p:0.15525050461292267
epoch£º666	 i:4 	 global-step:13324	 l-p:0.10700417309999466
epoch£º666	 i:5 	 global-step:13325	 l-p:0.16175466775894165
epoch£º666	 i:6 	 global-step:13326	 l-p:0.16432972252368927
epoch£º666	 i:7 	 global-step:13327	 l-p:0.13897071778774261
epoch£º666	 i:8 	 global-step:13328	 l-p:0.1435791552066803
epoch£º666	 i:9 	 global-step:13329	 l-p:0.20871856808662415
====================================================================================================
====================================================================================================
====================================================================================================

epoch:667
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9445,  0.9267,  1.0000,  0.9092,
          1.0000,  0.9811, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2653,  0.1705,  1.0000,  0.1095,
          1.0000,  0.6426, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3475,  0.2444,  1.0000,  0.1718,
          1.0000,  0.7031, 31.6228]], device='cuda:0')
 pt:tensor([[5.0484, 5.5244, 5.5405],
        [5.0484, 4.8499, 4.7840],
        [5.0484, 5.2864, 5.1460],
        [5.0484, 4.8474, 4.6889]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:667, step:0 
model_pd.l_p.mean(): 0.13175898790359497 
model_pd.l_d.mean(): -20.114267349243164 
model_pd.lagr.mean(): -19.982507705688477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5137], device='cuda:0')), ('power', tensor([-21.0224], device='cuda:0'))])
epoch£º667	 i:0 	 global-step:13340	 l-p:0.13175898790359497
epoch£º667	 i:1 	 global-step:13341	 l-p:0.11534903943538666
epoch£º667	 i:2 	 global-step:13342	 l-p:0.1417766809463501
epoch£º667	 i:3 	 global-step:13343	 l-p:0.1806202083826065
epoch£º667	 i:4 	 global-step:13344	 l-p:0.16608983278274536
epoch£º667	 i:5 	 global-step:13345	 l-p:0.18122653663158417
epoch£º667	 i:6 	 global-step:13346	 l-p:0.11933041363954544
epoch£º667	 i:7 	 global-step:13347	 l-p:0.11189168691635132
epoch£º667	 i:8 	 global-step:13348	 l-p:0.13906043767929077
epoch£º667	 i:9 	 global-step:13349	 l-p:0.12720881402492523
====================================================================================================
====================================================================================================
====================================================================================================

epoch:668
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0581, 5.0283, 5.0521],
        [5.0581, 5.0581, 5.0581],
        [5.0581, 4.8711, 4.8399],
        [5.0581, 4.9475, 4.6931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:668, step:0 
model_pd.l_p.mean(): 0.11742425709962845 
model_pd.l_d.mean(): -19.813175201416016 
model_pd.lagr.mean(): -19.695751190185547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5203], device='cuda:0')), ('power', tensor([-20.7225], device='cuda:0'))])
epoch£º668	 i:0 	 global-step:13360	 l-p:0.11742425709962845
epoch£º668	 i:1 	 global-step:13361	 l-p:0.1420695036649704
epoch£º668	 i:2 	 global-step:13362	 l-p:0.13456624746322632
epoch£º668	 i:3 	 global-step:13363	 l-p:0.1592516303062439
epoch£º668	 i:4 	 global-step:13364	 l-p:0.12308970093727112
epoch£º668	 i:5 	 global-step:13365	 l-p:0.16674621403217316
epoch£º668	 i:6 	 global-step:13366	 l-p:0.11956438422203064
epoch£º668	 i:7 	 global-step:13367	 l-p:0.1698964238166809
epoch£º668	 i:8 	 global-step:13368	 l-p:0.11303499341011047
epoch£º668	 i:9 	 global-step:13369	 l-p:0.2585868537425995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:669
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0347, 4.8775, 4.8951],
        [5.0347, 5.0346, 5.0347],
        [5.0347, 5.0347, 5.0347],
        [5.0347, 4.8341, 4.6617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:669, step:0 
model_pd.l_p.mean(): 0.13702504336833954 
model_pd.l_d.mean(): -19.94609260559082 
model_pd.lagr.mean(): -19.809066772460938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4369], device='cuda:0')), ('power', tensor([-20.7716], device='cuda:0'))])
epoch£º669	 i:0 	 global-step:13380	 l-p:0.13702504336833954
epoch£º669	 i:1 	 global-step:13381	 l-p:0.16601960361003876
epoch£º669	 i:2 	 global-step:13382	 l-p:0.13346609473228455
epoch£º669	 i:3 	 global-step:13383	 l-p:0.15772275626659393
epoch£º669	 i:4 	 global-step:13384	 l-p:0.1687304675579071
epoch£º669	 i:5 	 global-step:13385	 l-p:0.1072276309132576
epoch£º669	 i:6 	 global-step:13386	 l-p:0.15242937207221985
epoch£º669	 i:7 	 global-step:13387	 l-p:0.17842021584510803
epoch£º669	 i:8 	 global-step:13388	 l-p:0.17783227562904358
epoch£º669	 i:9 	 global-step:13389	 l-p:0.11030452698469162
====================================================================================================
====================================================================================================
====================================================================================================

epoch:670
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0576, 5.0576, 5.0576],
        [5.0576, 4.9866, 5.0292],
        [5.0576, 5.0561, 5.0576],
        [5.0576, 5.0576, 5.0576]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:670, step:0 
model_pd.l_p.mean(): 0.22788655757904053 
model_pd.l_d.mean(): -20.4530086517334 
model_pd.lagr.mean(): -20.225122451782227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4794], device='cuda:0')), ('power', tensor([-21.3320], device='cuda:0'))])
epoch£º670	 i:0 	 global-step:13400	 l-p:0.22788655757904053
epoch£º670	 i:1 	 global-step:13401	 l-p:0.11660375446081161
epoch£º670	 i:2 	 global-step:13402	 l-p:0.1375531107187271
epoch£º670	 i:3 	 global-step:13403	 l-p:0.1314331740140915
epoch£º670	 i:4 	 global-step:13404	 l-p:0.14362503588199615
epoch£º670	 i:5 	 global-step:13405	 l-p:0.09820754081010818
epoch£º670	 i:6 	 global-step:13406	 l-p:0.14678658545017242
epoch£º670	 i:7 	 global-step:13407	 l-p:0.12362996488809586
epoch£º670	 i:8 	 global-step:13408	 l-p:0.1344747394323349
epoch£º670	 i:9 	 global-step:13409	 l-p:0.15114472806453705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:671
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0499, 5.0391, 5.0489],
        [5.0499, 5.0463, 5.0498],
        [5.0499, 4.8546, 4.8055],
        [5.0499, 5.0462, 5.0498]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:671, step:0 
model_pd.l_p.mean(): 0.1413249671459198 
model_pd.l_d.mean(): -20.574207305908203 
model_pd.lagr.mean(): -20.43288230895996 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4317], device='cuda:0')), ('power', tensor([-21.4061], device='cuda:0'))])
epoch£º671	 i:0 	 global-step:13420	 l-p:0.1413249671459198
epoch£º671	 i:1 	 global-step:13421	 l-p:0.15653292834758759
epoch£º671	 i:2 	 global-step:13422	 l-p:0.1146102100610733
epoch£º671	 i:3 	 global-step:13423	 l-p:0.1596830189228058
epoch£º671	 i:4 	 global-step:13424	 l-p:0.2022925168275833
epoch£º671	 i:5 	 global-step:13425	 l-p:0.18804341554641724
epoch£º671	 i:6 	 global-step:13426	 l-p:0.09343007951974869
epoch£º671	 i:7 	 global-step:13427	 l-p:0.12221593409776688
epoch£º671	 i:8 	 global-step:13428	 l-p:0.17407949268817902
epoch£º671	 i:9 	 global-step:13429	 l-p:0.14839580655097961
====================================================================================================
====================================================================================================
====================================================================================================

epoch:672
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0431, 5.0218, 5.0398],
        [5.0431, 4.8999, 4.6534],
        [5.0431, 5.0431, 5.0432],
        [5.0431, 4.8554, 4.8281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:672, step:0 
model_pd.l_p.mean(): 0.2111273854970932 
model_pd.l_d.mean(): -19.966224670410156 
model_pd.lagr.mean(): -19.755096435546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5112], device='cuda:0')), ('power', tensor([-20.8690], device='cuda:0'))])
epoch£º672	 i:0 	 global-step:13440	 l-p:0.2111273854970932
epoch£º672	 i:1 	 global-step:13441	 l-p:0.13489839434623718
epoch£º672	 i:2 	 global-step:13442	 l-p:0.15298935770988464
epoch£º672	 i:3 	 global-step:13443	 l-p:0.14787407219409943
epoch£º672	 i:4 	 global-step:13444	 l-p:0.10348907113075256
epoch£º672	 i:5 	 global-step:13445	 l-p:0.11368092149496078
epoch£º672	 i:6 	 global-step:13446	 l-p:0.13692741096019745
epoch£º672	 i:7 	 global-step:13447	 l-p:0.13561290502548218
epoch£º672	 i:8 	 global-step:13448	 l-p:0.11884921789169312
epoch£º672	 i:9 	 global-step:13449	 l-p:0.21107450127601624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:673
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0426, 5.0424, 5.0426],
        [5.0426, 5.0426, 5.0426],
        [5.0426, 5.0285, 4.7741],
        [5.0426, 5.0321, 5.0416]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:673, step:0 
model_pd.l_p.mean(): 0.13296686112880707 
model_pd.l_d.mean(): -20.0838565826416 
model_pd.lagr.mean(): -19.950889587402344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4634], device='cuda:0')), ('power', tensor([-20.9393], device='cuda:0'))])
epoch£º673	 i:0 	 global-step:13460	 l-p:0.13296686112880707
epoch£º673	 i:1 	 global-step:13461	 l-p:0.1582368016242981
epoch£º673	 i:2 	 global-step:13462	 l-p:0.18845424056053162
epoch£º673	 i:3 	 global-step:13463	 l-p:0.2758459448814392
epoch£º673	 i:4 	 global-step:13464	 l-p:0.13158518075942993
epoch£º673	 i:5 	 global-step:13465	 l-p:0.1588420867919922
epoch£º673	 i:6 	 global-step:13466	 l-p:0.18909230828285217
epoch£º673	 i:7 	 global-step:13467	 l-p:0.09632803499698639
epoch£º673	 i:8 	 global-step:13468	 l-p:0.1753990203142166
epoch£º673	 i:9 	 global-step:13469	 l-p:0.09943532943725586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:674
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0165, 4.9032, 4.9476],
        [5.0165, 4.9178, 4.9640],
        [5.0165, 4.8663, 4.8927],
        [5.0165, 4.8052, 4.6866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:674, step:0 
model_pd.l_p.mean(): 0.1817525029182434 
model_pd.l_d.mean(): -19.408313751220703 
model_pd.lagr.mean(): -19.226560592651367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5643], device='cuda:0')), ('power', tensor([-20.3557], device='cuda:0'))])
epoch£º674	 i:0 	 global-step:13480	 l-p:0.1817525029182434
epoch£º674	 i:1 	 global-step:13481	 l-p:0.13407956063747406
epoch£º674	 i:2 	 global-step:13482	 l-p:0.11566844582557678
epoch£º674	 i:3 	 global-step:13483	 l-p:0.13805648684501648
epoch£º674	 i:4 	 global-step:13484	 l-p:0.08330624550580978
epoch£º674	 i:5 	 global-step:13485	 l-p:0.1553901880979538
epoch£º674	 i:6 	 global-step:13486	 l-p:0.2966776490211487
epoch£º674	 i:7 	 global-step:13487	 l-p:0.18939852714538574
epoch£º674	 i:8 	 global-step:13488	 l-p:0.1440877914428711
epoch£º674	 i:9 	 global-step:13489	 l-p:0.14305604994297028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:675
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0673, 4.8987, 4.9027],
        [5.0673, 4.8631, 4.7769],
        [5.0673, 4.9718, 5.0174],
        [5.0673, 4.9943, 5.0374]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:675, step:0 
model_pd.l_p.mean(): 0.10359295457601547 
model_pd.l_d.mean(): -20.011980056762695 
model_pd.lagr.mean(): -19.90838623046875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4748], device='cuda:0')), ('power', tensor([-20.8780], device='cuda:0'))])
epoch£º675	 i:0 	 global-step:13500	 l-p:0.10359295457601547
epoch£º675	 i:1 	 global-step:13501	 l-p:0.1400538980960846
epoch£º675	 i:2 	 global-step:13502	 l-p:0.15122678875923157
epoch£º675	 i:3 	 global-step:13503	 l-p:0.1342884749174118
epoch£º675	 i:4 	 global-step:13504	 l-p:0.16165903210639954
epoch£º675	 i:5 	 global-step:13505	 l-p:0.15003132820129395
epoch£º675	 i:6 	 global-step:13506	 l-p:0.12050439417362213
epoch£º675	 i:7 	 global-step:13507	 l-p:0.12145672738552094
epoch£º675	 i:8 	 global-step:13508	 l-p:0.15493974089622498
epoch£º675	 i:9 	 global-step:13509	 l-p:0.0003926610806956887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:676
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1271, 5.1271, 5.1271],
        [5.1271, 4.9369, 4.7563],
        [5.1271, 5.1151, 5.1258],
        [5.1271, 5.1231, 5.1269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:676, step:0 
model_pd.l_p.mean(): 0.09862879663705826 
model_pd.l_d.mean(): -20.292016983032227 
model_pd.lagr.mean(): -20.193387985229492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4409], device='cuda:0')), ('power', tensor([-21.1281], device='cuda:0'))])
epoch£º676	 i:0 	 global-step:13520	 l-p:0.09862879663705826
epoch£º676	 i:1 	 global-step:13521	 l-p:0.1464015245437622
epoch£º676	 i:2 	 global-step:13522	 l-p:0.1751164346933365
epoch£º676	 i:3 	 global-step:13523	 l-p:0.12419011443853378
epoch£º676	 i:4 	 global-step:13524	 l-p:0.14252552390098572
epoch£º676	 i:5 	 global-step:13525	 l-p:0.057778358459472656
epoch£º676	 i:6 	 global-step:13526	 l-p:0.17973411083221436
epoch£º676	 i:7 	 global-step:13527	 l-p:0.12475161999464035
epoch£º676	 i:8 	 global-step:13528	 l-p:0.12004278600215912
epoch£º676	 i:9 	 global-step:13529	 l-p:0.1257268488407135
====================================================================================================
====================================================================================================
====================================================================================================

epoch:677
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4142e-01, 1.5033e-01,
         1.0000e+00, 9.3606e-02, 1.0000e+00, 6.2267e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0704, 5.0704, 5.0704],
        [5.0704, 4.9708, 5.0164],
        [5.0704, 4.8781, 4.8397],
        [5.0704, 5.0539, 5.0682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:677, step:0 
model_pd.l_p.mean(): 0.12780103087425232 
model_pd.l_d.mean(): -20.426549911499023 
model_pd.lagr.mean(): -20.298748016357422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4549], device='cuda:0')), ('power', tensor([-21.2797], device='cuda:0'))])
epoch£º677	 i:0 	 global-step:13540	 l-p:0.12780103087425232
epoch£º677	 i:1 	 global-step:13541	 l-p:0.14656203985214233
epoch£º677	 i:2 	 global-step:13542	 l-p:0.11393695324659348
epoch£º677	 i:3 	 global-step:13543	 l-p:0.14232629537582397
epoch£º677	 i:4 	 global-step:13544	 l-p:0.18201754987239838
epoch£º677	 i:5 	 global-step:13545	 l-p:0.1306544691324234
epoch£º677	 i:6 	 global-step:13546	 l-p:0.4139603078365326
epoch£º677	 i:7 	 global-step:13547	 l-p:-0.2795106768608093
epoch£º677	 i:8 	 global-step:13548	 l-p:0.16073818504810333
epoch£º677	 i:9 	 global-step:13549	 l-p:0.11233425885438919
====================================================================================================
====================================================================================================
====================================================================================================

epoch:678
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9898, 4.9579, 4.9833],
        [4.9898, 4.9882, 4.9898],
        [4.9898, 4.9618, 4.9846],
        [4.9898, 4.9496, 4.9800]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:678, step:0 
model_pd.l_p.mean(): -0.04027091711759567 
model_pd.l_d.mean(): -19.766386032104492 
model_pd.lagr.mean(): -19.806657791137695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5693], device='cuda:0')), ('power', tensor([-20.7256], device='cuda:0'))])
epoch£º678	 i:0 	 global-step:13560	 l-p:-0.04027091711759567
epoch£º678	 i:1 	 global-step:13561	 l-p:0.18132545053958893
epoch£º678	 i:2 	 global-step:13562	 l-p:0.13051724433898926
epoch£º678	 i:3 	 global-step:13563	 l-p:0.25936928391456604
epoch£º678	 i:4 	 global-step:13564	 l-p:0.12972193956375122
epoch£º678	 i:5 	 global-step:13565	 l-p:0.135172501206398
epoch£º678	 i:6 	 global-step:13566	 l-p:0.17470458149909973
epoch£º678	 i:7 	 global-step:13567	 l-p:0.14824612438678741
epoch£º678	 i:8 	 global-step:13568	 l-p:0.14480045437812805
epoch£º678	 i:9 	 global-step:13569	 l-p:0.22525133192539215
====================================================================================================
====================================================================================================
====================================================================================================

epoch:679
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9958, 4.9957, 4.9958],
        [4.9958, 4.9466, 4.6820],
        [4.9958, 4.9688, 4.9909],
        [4.9958, 4.9452, 4.9809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:679, step:0 
model_pd.l_p.mean(): 0.1879890412092209 
model_pd.l_d.mean(): -20.282785415649414 
model_pd.lagr.mean(): -20.094797134399414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4918], device='cuda:0')), ('power', tensor([-21.1714], device='cuda:0'))])
epoch£º679	 i:0 	 global-step:13580	 l-p:0.1879890412092209
epoch£º679	 i:1 	 global-step:13581	 l-p:0.1656416803598404
epoch£º679	 i:2 	 global-step:13582	 l-p:0.17420050501823425
epoch£º679	 i:3 	 global-step:13583	 l-p:7.636582374572754
epoch£º679	 i:4 	 global-step:13584	 l-p:0.2346101701259613
epoch£º679	 i:5 	 global-step:13585	 l-p:0.11954431980848312
epoch£º679	 i:6 	 global-step:13586	 l-p:0.1658141314983368
epoch£º679	 i:7 	 global-step:13587	 l-p:0.11952275782823563
epoch£º679	 i:8 	 global-step:13588	 l-p:0.12315376102924347
epoch£º679	 i:9 	 global-step:13589	 l-p:0.11458472907543182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:680
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0339, 5.0271, 5.0335],
        [5.0339, 5.0315, 5.0338],
        [5.0339, 4.9961, 5.0250],
        [5.0339, 5.2181, 5.0450]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:680, step:0 
model_pd.l_p.mean(): 0.2113286852836609 
model_pd.l_d.mean(): -20.78432846069336 
model_pd.lagr.mean(): -20.572999954223633 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4259], device='cuda:0')), ('power', tensor([-21.6141], device='cuda:0'))])
epoch£º680	 i:0 	 global-step:13600	 l-p:0.2113286852836609
epoch£º680	 i:1 	 global-step:13601	 l-p:0.1417371779680252
epoch£º680	 i:2 	 global-step:13602	 l-p:0.15428853034973145
epoch£º680	 i:3 	 global-step:13603	 l-p:0.1674746721982956
epoch£º680	 i:4 	 global-step:13604	 l-p:0.20975945889949799
epoch£º680	 i:5 	 global-step:13605	 l-p:0.15317991375923157
epoch£º680	 i:6 	 global-step:13606	 l-p:0.13614799082279205
epoch£º680	 i:7 	 global-step:13607	 l-p:0.11516045033931732
epoch£º680	 i:8 	 global-step:13608	 l-p:0.12352103739976883
epoch£º680	 i:9 	 global-step:13609	 l-p:0.1175280287861824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:681
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0934, 4.9140, 4.9013],
        [5.0934, 5.4770, 5.4245],
        [5.0934, 5.0925, 5.0934],
        [5.0934, 5.0924, 5.0934]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:681, step:0 
model_pd.l_p.mean(): 0.10520296543836594 
model_pd.l_d.mean(): -20.213924407958984 
model_pd.lagr.mean(): -20.108720779418945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4629], device='cuda:0')), ('power', tensor([-21.0714], device='cuda:0'))])
epoch£º681	 i:0 	 global-step:13620	 l-p:0.10520296543836594
epoch£º681	 i:1 	 global-step:13621	 l-p:0.13279986381530762
epoch£º681	 i:2 	 global-step:13622	 l-p:0.1719251275062561
epoch£º681	 i:3 	 global-step:13623	 l-p:0.1457233726978302
epoch£º681	 i:4 	 global-step:13624	 l-p:0.1469164341688156
epoch£º681	 i:5 	 global-step:13625	 l-p:0.08468465507030487
epoch£º681	 i:6 	 global-step:13626	 l-p:0.050158824771642685
epoch£º681	 i:7 	 global-step:13627	 l-p:0.14570876955986023
epoch£º681	 i:8 	 global-step:13628	 l-p:0.15816375613212585
epoch£º681	 i:9 	 global-step:13629	 l-p:0.11039762198925018
====================================================================================================
====================================================================================================
====================================================================================================

epoch:682
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0905, 4.9948, 5.0405],
        [5.0905, 4.9670, 5.0073],
        [5.0905, 5.0881, 5.0904],
        [5.0905, 5.0754, 5.0887]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:682, step:0 
model_pd.l_p.mean(): 0.14069142937660217 
model_pd.l_d.mean(): -20.3770694732666 
model_pd.lagr.mean(): -20.236377716064453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4599], device='cuda:0')), ('power', tensor([-21.2344], device='cuda:0'))])
epoch£º682	 i:0 	 global-step:13640	 l-p:0.14069142937660217
epoch£º682	 i:1 	 global-step:13641	 l-p:0.1264539211988449
epoch£º682	 i:2 	 global-step:13642	 l-p:0.12362072616815567
epoch£º682	 i:3 	 global-step:13643	 l-p:0.19404220581054688
epoch£º682	 i:4 	 global-step:13644	 l-p:0.17192456126213074
epoch£º682	 i:5 	 global-step:13645	 l-p:0.12878550589084625
epoch£º682	 i:6 	 global-step:13646	 l-p:0.13634265959262848
epoch£º682	 i:7 	 global-step:13647	 l-p:0.1405821442604065
epoch£º682	 i:8 	 global-step:13648	 l-p:0.11199117451906204
epoch£º682	 i:9 	 global-step:13649	 l-p:0.7395876049995422
====================================================================================================
====================================================================================================
====================================================================================================

epoch:683
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9813, 4.8606, 4.9048],
        [4.9813, 4.9447, 4.9730],
        [4.9813, 4.9813, 4.9813],
        [4.9813, 4.8268, 4.8534]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:683, step:0 
model_pd.l_p.mean(): 0.1463780552148819 
model_pd.l_d.mean(): -20.70047378540039 
model_pd.lagr.mean(): -20.554096221923828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4612], device='cuda:0')), ('power', tensor([-21.5652], device='cuda:0'))])
epoch£º683	 i:0 	 global-step:13660	 l-p:0.1463780552148819
epoch£º683	 i:1 	 global-step:13661	 l-p:-0.25036782026290894
epoch£º683	 i:2 	 global-step:13662	 l-p:0.11291813850402832
epoch£º683	 i:3 	 global-step:13663	 l-p:0.1362413465976715
epoch£º683	 i:4 	 global-step:13664	 l-p:4.012043476104736
epoch£º683	 i:5 	 global-step:13665	 l-p:0.12512992322444916
epoch£º683	 i:6 	 global-step:13666	 l-p:0.08070772141218185
epoch£º683	 i:7 	 global-step:13667	 l-p:0.1634717732667923
epoch£º683	 i:8 	 global-step:13668	 l-p:0.21240581572055817
epoch£º683	 i:9 	 global-step:13669	 l-p:0.11515253037214279
====================================================================================================
====================================================================================================
====================================================================================================

epoch:684
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9782, 4.8125, 4.5640],
        [4.9782, 4.9782, 4.9782],
        [4.9782, 5.1961, 5.0431],
        [4.9782, 4.9759, 4.9781]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:684, step:0 
model_pd.l_p.mean(): 0.12202955037355423 
model_pd.l_d.mean(): -20.201791763305664 
model_pd.lagr.mean(): -20.079761505126953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5043], device='cuda:0')), ('power', tensor([-21.1019], device='cuda:0'))])
epoch£º684	 i:0 	 global-step:13680	 l-p:0.12202955037355423
epoch£º684	 i:1 	 global-step:13681	 l-p:0.12456056475639343
epoch£º684	 i:2 	 global-step:13682	 l-p:0.130921870470047
epoch£º684	 i:3 	 global-step:13683	 l-p:0.22366221249103546
epoch£º684	 i:4 	 global-step:13684	 l-p:0.12337733805179596
epoch£º684	 i:5 	 global-step:13685	 l-p:0.32699650526046753
epoch£º684	 i:6 	 global-step:13686	 l-p:0.44401630759239197
epoch£º684	 i:7 	 global-step:13687	 l-p:-4.0795722007751465
epoch£º684	 i:8 	 global-step:13688	 l-p:0.13114897906780243
epoch£º684	 i:9 	 global-step:13689	 l-p:-0.16226240992546082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:685
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0038, 4.9241, 4.9695],
        [5.0038, 5.3514, 5.2780],
        [5.0038, 4.7874, 4.6613],
        [5.0038, 5.0038, 5.0038]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:685, step:0 
model_pd.l_p.mean(): 0.2000684291124344 
model_pd.l_d.mean(): -20.468355178833008 
model_pd.lagr.mean(): -20.268287658691406 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4838], device='cuda:0')), ('power', tensor([-21.3522], device='cuda:0'))])
epoch£º685	 i:0 	 global-step:13700	 l-p:0.2000684291124344
epoch£º685	 i:1 	 global-step:13701	 l-p:0.1761595904827118
epoch£º685	 i:2 	 global-step:13702	 l-p:0.1179489716887474
epoch£º685	 i:3 	 global-step:13703	 l-p:0.2823526859283447
epoch£º685	 i:4 	 global-step:13704	 l-p:0.12978723645210266
epoch£º685	 i:5 	 global-step:13705	 l-p:0.11567995697259903
epoch£º685	 i:6 	 global-step:13706	 l-p:0.07707113772630692
epoch£º685	 i:7 	 global-step:13707	 l-p:0.16487222909927368
epoch£º685	 i:8 	 global-step:13708	 l-p:0.10043658316135406
epoch£º685	 i:9 	 global-step:13709	 l-p:0.12623439729213715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:686
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1213, 5.1210, 5.1213],
        [5.1213, 5.1207, 5.1213],
        [5.1213, 5.0905, 5.1150],
        [5.1213, 5.3574, 5.2107]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:686, step:0 
model_pd.l_p.mean(): 0.0592547208070755 
model_pd.l_d.mean(): -18.88934326171875 
model_pd.lagr.mean(): -18.830087661743164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5811], device='cuda:0')), ('power', tensor([-19.8444], device='cuda:0'))])
epoch£º686	 i:0 	 global-step:13720	 l-p:0.0592547208070755
epoch£º686	 i:1 	 global-step:13721	 l-p:0.13579343259334564
epoch£º686	 i:2 	 global-step:13722	 l-p:0.08848171681165695
epoch£º686	 i:3 	 global-step:13723	 l-p:0.14578565955162048
epoch£º686	 i:4 	 global-step:13724	 l-p:0.10860466957092285
epoch£º686	 i:5 	 global-step:13725	 l-p:0.10482850670814514
epoch£º686	 i:6 	 global-step:13726	 l-p:0.09299666434526443
epoch£º686	 i:7 	 global-step:13727	 l-p:0.13254414498806
epoch£º686	 i:8 	 global-step:13728	 l-p:0.13370482623577118
epoch£º686	 i:9 	 global-step:13729	 l-p:0.1387409120798111
====================================================================================================
====================================================================================================
====================================================================================================

epoch:687
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4057,  0.3004,  1.0000,  0.2224,
          1.0000,  0.7403, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6689,  0.5850,  1.0000,  0.5116,
          1.0000,  0.8745, 31.6228]], device='cuda:0')
 pt:tensor([[5.1567, 5.3643, 5.2002],
        [5.1567, 4.9825, 4.7710],
        [5.1567, 4.9677, 4.9273],
        [5.1567, 5.2486, 5.0279]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:687, step:0 
model_pd.l_p.mean(): 0.052081771194934845 
model_pd.l_d.mean(): -19.591840744018555 
model_pd.lagr.mean(): -19.539758682250977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5426], device='cuda:0')), ('power', tensor([-20.5202], device='cuda:0'))])
epoch£º687	 i:0 	 global-step:13740	 l-p:0.052081771194934845
epoch£º687	 i:1 	 global-step:13741	 l-p:0.1696888655424118
epoch£º687	 i:2 	 global-step:13742	 l-p:0.13594822585582733
epoch£º687	 i:3 	 global-step:13743	 l-p:0.130211740732193
epoch£º687	 i:4 	 global-step:13744	 l-p:0.14804750680923462
epoch£º687	 i:5 	 global-step:13745	 l-p:0.10488446801900864
epoch£º687	 i:6 	 global-step:13746	 l-p:0.08473356068134308
epoch£º687	 i:7 	 global-step:13747	 l-p:0.13666152954101562
epoch£º687	 i:8 	 global-step:13748	 l-p:0.1403987556695938
epoch£º687	 i:9 	 global-step:13749	 l-p:0.011564671993255615
====================================================================================================
====================================================================================================
====================================================================================================

epoch:688
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1225, 5.1225, 5.1225],
        [5.1225, 4.9244, 4.7499],
        [5.1225, 4.9657, 4.9833],
        [5.1225, 5.1224, 5.1225]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:688, step:0 
model_pd.l_p.mean(): 0.04438702389597893 
model_pd.l_d.mean(): -19.318233489990234 
model_pd.lagr.mean(): -19.273845672607422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5251], device='cuda:0')), ('power', tensor([-20.2234], device='cuda:0'))])
epoch£º688	 i:0 	 global-step:13760	 l-p:0.04438702389597893
epoch£º688	 i:1 	 global-step:13761	 l-p:0.13419024646282196
epoch£º688	 i:2 	 global-step:13762	 l-p:0.15025639533996582
epoch£º688	 i:3 	 global-step:13763	 l-p:0.17741534113883972
epoch£º688	 i:4 	 global-step:13764	 l-p:0.15109391510486603
epoch£º688	 i:5 	 global-step:13765	 l-p:0.12153270095586777
epoch£º688	 i:6 	 global-step:13766	 l-p:0.04571831598877907
epoch£º688	 i:7 	 global-step:13767	 l-p:0.13426531851291656
epoch£º688	 i:8 	 global-step:13768	 l-p:0.1223769336938858
epoch£º688	 i:9 	 global-step:13769	 l-p:0.1320251077413559
====================================================================================================
====================================================================================================
====================================================================================================

epoch:689
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0997, 5.0341, 5.0755],
        [5.0997, 5.0926, 5.0992],
        [5.0997, 5.0997, 5.0997],
        [5.0997, 5.0997, 5.0997]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:689, step:0 
model_pd.l_p.mean(): 0.1785019189119339 
model_pd.l_d.mean(): -20.509065628051758 
model_pd.lagr.mean(): -20.330564498901367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4440], device='cuda:0')), ('power', tensor([-21.3525], device='cuda:0'))])
epoch£º689	 i:0 	 global-step:13780	 l-p:0.1785019189119339
epoch£º689	 i:1 	 global-step:13781	 l-p:0.13678093254566193
epoch£º689	 i:2 	 global-step:13782	 l-p:0.12131816893815994
epoch£º689	 i:3 	 global-step:13783	 l-p:0.12771612405776978
epoch£º689	 i:4 	 global-step:13784	 l-p:0.12224055081605911
epoch£º689	 i:5 	 global-step:13785	 l-p:0.1215786561369896
epoch£º689	 i:6 	 global-step:13786	 l-p:0.19737079739570618
epoch£º689	 i:7 	 global-step:13787	 l-p:0.09528055787086487
epoch£º689	 i:8 	 global-step:13788	 l-p:0.276000440120697
epoch£º689	 i:9 	 global-step:13789	 l-p:0.12684428691864014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:690
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0284, 5.0284, 5.0284],
        [5.0284, 5.4360, 5.4009],
        [5.0284, 5.0279, 5.0284],
        [5.0284, 5.0284, 5.0284]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:690, step:0 
model_pd.l_p.mean(): 0.1394675225019455 
model_pd.l_d.mean(): -20.451326370239258 
model_pd.lagr.mean(): -20.311859130859375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4758], device='cuda:0')), ('power', tensor([-21.3266], device='cuda:0'))])
epoch£º690	 i:0 	 global-step:13800	 l-p:0.1394675225019455
epoch£º690	 i:1 	 global-step:13801	 l-p:0.0804327204823494
epoch£º690	 i:2 	 global-step:13802	 l-p:0.12493497878313065
epoch£º690	 i:3 	 global-step:13803	 l-p:0.4664021134376526
epoch£º690	 i:4 	 global-step:13804	 l-p:0.13246408104896545
epoch£º690	 i:5 	 global-step:13805	 l-p:0.19679173827171326
epoch£º690	 i:6 	 global-step:13806	 l-p:0.2307761311531067
epoch£º690	 i:7 	 global-step:13807	 l-p:0.17394591867923737
epoch£º690	 i:8 	 global-step:13808	 l-p:0.1365911066532135
epoch£º690	 i:9 	 global-step:13809	 l-p:0.13357768952846527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:691
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0424, 5.0423, 5.0424],
        [5.0424, 4.9362, 4.9827],
        [5.0424, 4.9357, 4.6694],
        [5.0424, 5.0401, 5.0423]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:691, step:0 
model_pd.l_p.mean(): 0.11142965406179428 
model_pd.l_d.mean(): -19.426204681396484 
model_pd.lagr.mean(): -19.314775466918945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5206], device='cuda:0')), ('power', tensor([-20.3287], device='cuda:0'))])
epoch£º691	 i:0 	 global-step:13820	 l-p:0.11142965406179428
epoch£º691	 i:1 	 global-step:13821	 l-p:0.09836572408676147
epoch£º691	 i:2 	 global-step:13822	 l-p:0.1707492619752884
epoch£º691	 i:3 	 global-step:13823	 l-p:0.1699834167957306
epoch£º691	 i:4 	 global-step:13824	 l-p:0.19095908105373383
epoch£º691	 i:5 	 global-step:13825	 l-p:0.10006609559059143
epoch£º691	 i:6 	 global-step:13826	 l-p:0.20650127530097961
epoch£º691	 i:7 	 global-step:13827	 l-p:0.11076512187719345
epoch£º691	 i:8 	 global-step:13828	 l-p:0.11658985167741776
epoch£º691	 i:9 	 global-step:13829	 l-p:0.11819308996200562
====================================================================================================
====================================================================================================
====================================================================================================

epoch:692
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1066, 5.1063, 5.1066],
        [5.1066, 5.1060, 5.1065],
        [5.1066, 5.0433, 5.0839],
        [5.1066, 5.1066, 5.1066]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:692, step:0 
model_pd.l_p.mean(): 0.07558655738830566 
model_pd.l_d.mean(): -20.677486419677734 
model_pd.lagr.mean(): -20.601900100708008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4044], device='cuda:0')), ('power', tensor([-21.4830], device='cuda:0'))])
epoch£º692	 i:0 	 global-step:13840	 l-p:0.07558655738830566
epoch£º692	 i:1 	 global-step:13841	 l-p:0.12363927811384201
epoch£º692	 i:2 	 global-step:13842	 l-p:0.1559646725654602
epoch£º692	 i:3 	 global-step:13843	 l-p:0.14123335480690002
epoch£º692	 i:4 	 global-step:13844	 l-p:0.10398422926664352
epoch£º692	 i:5 	 global-step:13845	 l-p:0.11814447492361069
epoch£º692	 i:6 	 global-step:13846	 l-p:0.1437971293926239
epoch£º692	 i:7 	 global-step:13847	 l-p:0.12262464314699173
epoch£º692	 i:8 	 global-step:13848	 l-p:0.1879403442144394
epoch£º692	 i:9 	 global-step:13849	 l-p:0.10911859571933746
====================================================================================================
====================================================================================================
====================================================================================================

epoch:693
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0775, 5.0737, 5.0773],
        [5.0775, 5.1878, 4.9740],
        [5.0775, 5.4440, 5.3787],
        [5.0775, 5.4847, 5.4468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:693, step:0 
model_pd.l_p.mean(): 0.08974643796682358 
model_pd.l_d.mean(): -19.400646209716797 
model_pd.lagr.mean(): -19.31089973449707 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5772], device='cuda:0')), ('power', tensor([-20.3612], device='cuda:0'))])
epoch£º693	 i:0 	 global-step:13860	 l-p:0.08974643796682358
epoch£º693	 i:1 	 global-step:13861	 l-p:0.07097101956605911
epoch£º693	 i:2 	 global-step:13862	 l-p:0.15317603945732117
epoch£º693	 i:3 	 global-step:13863	 l-p:0.1547943651676178
epoch£º693	 i:4 	 global-step:13864	 l-p:0.16376245021820068
epoch£º693	 i:5 	 global-step:13865	 l-p:0.13033993542194366
epoch£º693	 i:6 	 global-step:13866	 l-p:0.11769851297140121
epoch£º693	 i:7 	 global-step:13867	 l-p:0.18898196518421173
epoch£º693	 i:8 	 global-step:13868	 l-p:0.17136628925800323
epoch£º693	 i:9 	 global-step:13869	 l-p:0.11001048982143402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:694
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0869, 5.0229, 5.0640],
        [5.0869, 4.8792, 4.7107],
        [5.0869, 5.0868, 5.0869],
        [5.0869, 5.0869, 5.0869]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:694, step:0 
model_pd.l_p.mean(): 0.09414897114038467 
model_pd.l_d.mean(): -19.041166305541992 
model_pd.lagr.mean(): -18.947017669677734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5481], device='cuda:0')), ('power', tensor([-19.9649], device='cuda:0'))])
epoch£º694	 i:0 	 global-step:13880	 l-p:0.09414897114038467
epoch£º694	 i:1 	 global-step:13881	 l-p:0.16526709496974945
epoch£º694	 i:2 	 global-step:13882	 l-p:0.127205029129982
epoch£º694	 i:3 	 global-step:13883	 l-p:0.126299187541008
epoch£º694	 i:4 	 global-step:13884	 l-p:0.08751092106103897
epoch£º694	 i:5 	 global-step:13885	 l-p:0.16524623334407806
epoch£º694	 i:6 	 global-step:13886	 l-p:0.13962344825267792
epoch£º694	 i:7 	 global-step:13887	 l-p:0.13519403338432312
epoch£º694	 i:8 	 global-step:13888	 l-p:0.12921284139156342
epoch£º694	 i:9 	 global-step:13889	 l-p:0.12521754205226898
====================================================================================================
====================================================================================================
====================================================================================================

epoch:695
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1108, 4.9031, 4.8156],
        [5.1108, 4.9601, 4.7132],
        [5.1108, 4.9688, 4.7173],
        [5.1108, 5.1105, 5.1108]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:695, step:0 
model_pd.l_p.mean(): 0.12428867071866989 
model_pd.l_d.mean(): -19.38934898376465 
model_pd.lagr.mean(): -19.265060424804688 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4887], device='cuda:0')), ('power', tensor([-20.2580], device='cuda:0'))])
epoch£º695	 i:0 	 global-step:13900	 l-p:0.12428867071866989
epoch£º695	 i:1 	 global-step:13901	 l-p:0.1664970964193344
epoch£º695	 i:2 	 global-step:13902	 l-p:0.0771915391087532
epoch£º695	 i:3 	 global-step:13903	 l-p:0.13174274563789368
epoch£º695	 i:4 	 global-step:13904	 l-p:0.1614932417869568
epoch£º695	 i:5 	 global-step:13905	 l-p:0.12087985873222351
epoch£º695	 i:6 	 global-step:13906	 l-p:0.13705086708068848
epoch£º695	 i:7 	 global-step:13907	 l-p:0.09937801212072372
epoch£º695	 i:8 	 global-step:13908	 l-p:0.13187025487422943
epoch£º695	 i:9 	 global-step:13909	 l-p:0.13495458662509918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:696
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0860, 5.0771, 5.0852],
        [5.0860, 4.9306, 4.6826],
        [5.0860, 5.0414, 5.0741],
        [5.0860, 5.4322, 5.3528]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:696, step:0 
model_pd.l_p.mean(): 0.19813762605190277 
model_pd.l_d.mean(): -20.643672943115234 
model_pd.lagr.mean(): -20.44553565979004 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4295], device='cuda:0')), ('power', tensor([-21.4745], device='cuda:0'))])
epoch£º696	 i:0 	 global-step:13920	 l-p:0.19813762605190277
epoch£º696	 i:1 	 global-step:13921	 l-p:0.11965052038431168
epoch£º696	 i:2 	 global-step:13922	 l-p:0.16780444979667664
epoch£º696	 i:3 	 global-step:13923	 l-p:0.1200660765171051
epoch£º696	 i:4 	 global-step:13924	 l-p:0.1435779482126236
epoch£º696	 i:5 	 global-step:13925	 l-p:0.21112382411956787
epoch£º696	 i:6 	 global-step:13926	 l-p:0.1378815472126007
epoch£º696	 i:7 	 global-step:13927	 l-p:0.11917074024677277
epoch£º696	 i:8 	 global-step:13928	 l-p:0.13816875219345093
epoch£º696	 i:9 	 global-step:13929	 l-p:0.1210278868675232
====================================================================================================
====================================================================================================
====================================================================================================

epoch:697
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0031, 5.0031, 5.0031],
        [5.0031, 5.0031, 5.0031],
        [5.0031, 5.0767, 4.8462],
        [5.0031, 5.0022, 5.0031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:697, step:0 
model_pd.l_p.mean(): 0.13369892537593842 
model_pd.l_d.mean(): -19.963281631469727 
model_pd.lagr.mean(): -19.82958221435547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4724], device='cuda:0')), ('power', tensor([-20.8259], device='cuda:0'))])
epoch£º697	 i:0 	 global-step:13940	 l-p:0.13369892537593842
epoch£º697	 i:1 	 global-step:13941	 l-p:0.17021295428276062
epoch£º697	 i:2 	 global-step:13942	 l-p:-0.03533542901277542
epoch£º697	 i:3 	 global-step:13943	 l-p:0.1352626532316208
epoch£º697	 i:4 	 global-step:13944	 l-p:0.6845673322677612
epoch£º697	 i:5 	 global-step:13945	 l-p:0.15002880990505219
epoch£º697	 i:6 	 global-step:13946	 l-p:0.19587856531143188
epoch£º697	 i:7 	 global-step:13947	 l-p:0.17391033470630646
epoch£º697	 i:8 	 global-step:13948	 l-p:0.14374585449695587
epoch£º697	 i:9 	 global-step:13949	 l-p:0.20105904340744019
====================================================================================================
====================================================================================================
====================================================================================================

epoch:698
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0214, 4.9846, 5.0131],
        [5.0214, 5.0189, 5.0213],
        [5.0214, 4.8334, 4.8233],
        [5.0214, 4.8010, 4.6808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:698, step:0 
model_pd.l_p.mean(): 0.14673028886318207 
model_pd.l_d.mean(): -20.425338745117188 
model_pd.lagr.mean(): -20.278608322143555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4660], device='cuda:0')), ('power', tensor([-21.2899], device='cuda:0'))])
epoch£º698	 i:0 	 global-step:13960	 l-p:0.14673028886318207
epoch£º698	 i:1 	 global-step:13961	 l-p:0.11956090480089188
epoch£º698	 i:2 	 global-step:13962	 l-p:0.22841358184814453
epoch£º698	 i:3 	 global-step:13963	 l-p:0.15140044689178467
epoch£º698	 i:4 	 global-step:13964	 l-p:0.13267111778259277
epoch£º698	 i:5 	 global-step:13965	 l-p:0.17555077373981476
epoch£º698	 i:6 	 global-step:13966	 l-p:0.30958959460258484
epoch£º698	 i:7 	 global-step:13967	 l-p:0.1671883910894394
epoch£º698	 i:8 	 global-step:13968	 l-p:0.09498702734708786
epoch£º698	 i:9 	 global-step:13969	 l-p:0.11968301236629486
====================================================================================================
====================================================================================================
====================================================================================================

epoch:699
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0620, 5.0550, 5.0615],
        [5.0620, 4.9980, 5.0394],
        [5.0620, 4.8456, 4.7030],
        [5.0620, 5.0620, 5.0620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:699, step:0 
model_pd.l_p.mean(): 0.13889652490615845 
model_pd.l_d.mean(): -20.27853012084961 
model_pd.lagr.mean(): -20.139633178710938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4817], device='cuda:0')), ('power', tensor([-21.1567], device='cuda:0'))])
epoch£º699	 i:0 	 global-step:13980	 l-p:0.13889652490615845
epoch£º699	 i:1 	 global-step:13981	 l-p:0.12328444421291351
epoch£º699	 i:2 	 global-step:13982	 l-p:0.14829452335834503
epoch£º699	 i:3 	 global-step:13983	 l-p:0.1320108026266098
epoch£º699	 i:4 	 global-step:13984	 l-p:0.17541205883026123
epoch£º699	 i:5 	 global-step:13985	 l-p:0.06086192652583122
epoch£º699	 i:6 	 global-step:13986	 l-p:0.09920792281627655
epoch£º699	 i:7 	 global-step:13987	 l-p:0.1754026710987091
epoch£º699	 i:8 	 global-step:13988	 l-p:0.14497269690036774
epoch£º699	 i:9 	 global-step:13989	 l-p:0.13365183770656586
====================================================================================================
====================================================================================================
====================================================================================================

epoch:700
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1318, 5.1269, 5.1316],
        [5.1318, 4.9258, 4.8489],
        [5.1318, 4.9411, 4.9126],
        [5.1318, 5.0668, 5.1082]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:700, step:0 
model_pd.l_p.mean(): 0.10980697721242905 
model_pd.l_d.mean(): -20.272417068481445 
model_pd.lagr.mean(): -20.16261100769043 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4288], device='cuda:0')), ('power', tensor([-21.0957], device='cuda:0'))])
epoch£º700	 i:0 	 global-step:14000	 l-p:0.10980697721242905
epoch£º700	 i:1 	 global-step:14001	 l-p:0.11958249658346176
epoch£º700	 i:2 	 global-step:14002	 l-p:0.07948438823223114
epoch£º700	 i:3 	 global-step:14003	 l-p:0.12370140105485916
epoch£º700	 i:4 	 global-step:14004	 l-p:0.11915401369333267
epoch£º700	 i:5 	 global-step:14005	 l-p:0.1554364114999771
epoch£º700	 i:6 	 global-step:14006	 l-p:0.04421662539243698
epoch£º700	 i:7 	 global-step:14007	 l-p:0.12655435502529144
epoch£º700	 i:8 	 global-step:14008	 l-p:0.11334273219108582
epoch£º700	 i:9 	 global-step:14009	 l-p:0.1523289978504181
====================================================================================================
====================================================================================================
====================================================================================================

epoch:701
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1522, 5.1449, 5.1516],
        [5.1522, 5.0731, 5.1179],
        [5.1522, 5.1522, 5.1522],
        [5.1522, 4.9874, 4.9992]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:701, step:0 
model_pd.l_p.mean(): 0.11924461275339127 
model_pd.l_d.mean(): -19.958383560180664 
model_pd.lagr.mean(): -19.83913803100586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4784], device='cuda:0')), ('power', tensor([-20.8271], device='cuda:0'))])
epoch£º701	 i:0 	 global-step:14020	 l-p:0.11924461275339127
epoch£º701	 i:1 	 global-step:14021	 l-p:0.14218568801879883
epoch£º701	 i:2 	 global-step:14022	 l-p:0.1300954967737198
epoch£º701	 i:3 	 global-step:14023	 l-p:0.15304389595985413
epoch£º701	 i:4 	 global-step:14024	 l-p:0.08106610178947449
epoch£º701	 i:5 	 global-step:14025	 l-p:0.029330424964427948
epoch£º701	 i:6 	 global-step:14026	 l-p:0.13598604500293732
epoch£º701	 i:7 	 global-step:14027	 l-p:0.05802394822239876
epoch£º701	 i:8 	 global-step:14028	 l-p:0.12680238485336304
epoch£º701	 i:9 	 global-step:14029	 l-p:0.14732718467712402
====================================================================================================
====================================================================================================
====================================================================================================

epoch:702
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1490, 5.0857, 5.1265],
        [5.1490, 5.3274, 5.1447],
        [5.1490, 5.0877, 5.1277],
        [5.1490, 5.1490, 5.1490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:702, step:0 
model_pd.l_p.mean(): 0.12255193293094635 
model_pd.l_d.mean(): -20.124433517456055 
model_pd.lagr.mean(): -20.001880645751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4689], device='cuda:0')), ('power', tensor([-20.9864], device='cuda:0'))])
epoch£º702	 i:0 	 global-step:14040	 l-p:0.12255193293094635
epoch£º702	 i:1 	 global-step:14041	 l-p:0.17007911205291748
epoch£º702	 i:2 	 global-step:14042	 l-p:-0.10652174800634384
epoch£º702	 i:3 	 global-step:14043	 l-p:0.08691579848527908
epoch£º702	 i:4 	 global-step:14044	 l-p:0.1298193484544754
epoch£º702	 i:5 	 global-step:14045	 l-p:0.13612805306911469
epoch£º702	 i:6 	 global-step:14046	 l-p:0.09183526784181595
epoch£º702	 i:7 	 global-step:14047	 l-p:0.15515083074569702
epoch£º702	 i:8 	 global-step:14048	 l-p:0.13179461658000946
epoch£º702	 i:9 	 global-step:14049	 l-p:0.1197085902094841
====================================================================================================
====================================================================================================
====================================================================================================

epoch:703
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5132e-02, 3.7428e-03,
         1.0000e+00, 9.2577e-04, 1.0000e+00, 2.4734e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1554, 5.1525, 5.1552],
        [5.1554, 5.4138, 5.2757],
        [5.1554, 5.1430, 5.1541],
        [5.1554, 5.4390, 5.3161]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:703, step:0 
model_pd.l_p.mean(): 0.12059011310338974 
model_pd.l_d.mean(): -20.275001525878906 
model_pd.lagr.mean(): -20.15441131591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4693], device='cuda:0')), ('power', tensor([-21.1402], device='cuda:0'))])
epoch£º703	 i:0 	 global-step:14060	 l-p:0.12059011310338974
epoch£º703	 i:1 	 global-step:14061	 l-p:0.11073838919401169
epoch£º703	 i:2 	 global-step:14062	 l-p:0.07056643068790436
epoch£º703	 i:3 	 global-step:14063	 l-p:0.1322258561849594
epoch£º703	 i:4 	 global-step:14064	 l-p:0.1673445850610733
epoch£º703	 i:5 	 global-step:14065	 l-p:0.1634196788072586
epoch£º703	 i:6 	 global-step:14066	 l-p:0.10996569693088531
epoch£º703	 i:7 	 global-step:14067	 l-p:0.10055366903543472
epoch£º703	 i:8 	 global-step:14068	 l-p:0.02585912123322487
epoch£º703	 i:9 	 global-step:14069	 l-p:0.14891454577445984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:704
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1262, 5.1262, 5.1262],
        [5.1262, 5.0814, 5.1143],
        [5.1262, 5.0453, 5.0909],
        [5.1262, 4.9532, 4.7182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:704, step:0 
model_pd.l_p.mean(): 0.1310884952545166 
model_pd.l_d.mean(): -20.889041900634766 
model_pd.lagr.mean(): -20.757953643798828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3669], device='cuda:0')), ('power', tensor([-21.6596], device='cuda:0'))])
epoch£º704	 i:0 	 global-step:14080	 l-p:0.1310884952545166
epoch£º704	 i:1 	 global-step:14081	 l-p:0.08680370450019836
epoch£º704	 i:2 	 global-step:14082	 l-p:0.16233043372631073
epoch£º704	 i:3 	 global-step:14083	 l-p:0.08033592253923416
epoch£º704	 i:4 	 global-step:14084	 l-p:0.1493348777294159
epoch£º704	 i:5 	 global-step:14085	 l-p:0.13058696687221527
epoch£º704	 i:6 	 global-step:14086	 l-p:0.10637858510017395
epoch£º704	 i:7 	 global-step:14087	 l-p:0.14192280173301697
epoch£º704	 i:8 	 global-step:14088	 l-p:0.11123977601528168
epoch£º704	 i:9 	 global-step:14089	 l-p:0.1367737501859665
====================================================================================================
====================================================================================================
====================================================================================================

epoch:705
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1490, 5.1490, 5.1491],
        [5.1490, 5.1446, 5.1488],
        [5.1490, 4.9884, 4.7457],
        [5.1490, 5.2466, 5.0236]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:705, step:0 
model_pd.l_p.mean(): -0.050972774624824524 
model_pd.l_d.mean(): -19.985050201416016 
model_pd.lagr.mean(): -20.036022186279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5047], device='cuda:0')), ('power', tensor([-20.8815], device='cuda:0'))])
epoch£º705	 i:0 	 global-step:14100	 l-p:-0.050972774624824524
epoch£º705	 i:1 	 global-step:14101	 l-p:0.15294483304023743
epoch£º705	 i:2 	 global-step:14102	 l-p:0.1402324140071869
epoch£º705	 i:3 	 global-step:14103	 l-p:0.1506747007369995
epoch£º705	 i:4 	 global-step:14104	 l-p:0.1149292141199112
epoch£º705	 i:5 	 global-step:14105	 l-p:0.11379037797451019
epoch£º705	 i:6 	 global-step:14106	 l-p:0.12967415153980255
epoch£º705	 i:7 	 global-step:14107	 l-p:0.14101283252239227
epoch£º705	 i:8 	 global-step:14108	 l-p:0.08630872517824173
epoch£º705	 i:9 	 global-step:14109	 l-p:0.12348592281341553
====================================================================================================
====================================================================================================
====================================================================================================

epoch:706
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1175, 5.0420, 5.0867],
        [5.1175, 5.2978, 5.1156],
        [5.1175, 5.0808, 5.1092],
        [5.1175, 4.9018, 4.7703]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:706, step:0 
model_pd.l_p.mean(): 0.15659891068935394 
model_pd.l_d.mean(): -20.350955963134766 
model_pd.lagr.mean(): -20.19435691833496 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4660], device='cuda:0')), ('power', tensor([-21.2141], device='cuda:0'))])
epoch£º706	 i:0 	 global-step:14120	 l-p:0.15659891068935394
epoch£º706	 i:1 	 global-step:14121	 l-p:0.12526154518127441
epoch£º706	 i:2 	 global-step:14122	 l-p:0.1460476517677307
epoch£º706	 i:3 	 global-step:14123	 l-p:0.10736271739006042
epoch£º706	 i:4 	 global-step:14124	 l-p:0.10475976765155792
epoch£º706	 i:5 	 global-step:14125	 l-p:0.20817214250564575
epoch£º706	 i:6 	 global-step:14126	 l-p:0.12337961047887802
epoch£º706	 i:7 	 global-step:14127	 l-p:0.29953569173812866
epoch£º706	 i:8 	 global-step:14128	 l-p:0.21430309116840363
epoch£º706	 i:9 	 global-step:14129	 l-p:0.1370847374200821
====================================================================================================
====================================================================================================
====================================================================================================

epoch:707
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0241, 5.0238, 5.0241],
        [5.0241, 5.0241, 5.0241],
        [5.0241, 5.0241, 5.0241],
        [5.0241, 4.9144, 4.9627]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:707, step:0 
model_pd.l_p.mean(): 0.6935250759124756 
model_pd.l_d.mean(): -20.751914978027344 
model_pd.lagr.mean(): -20.05838966369629 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4366], device='cuda:0')), ('power', tensor([-21.5922], device='cuda:0'))])
epoch£º707	 i:0 	 global-step:14140	 l-p:0.6935250759124756
epoch£º707	 i:1 	 global-step:14141	 l-p:0.11236371099948883
epoch£º707	 i:2 	 global-step:14142	 l-p:0.21669963002204895
epoch£º707	 i:3 	 global-step:14143	 l-p:0.1327563226222992
epoch£º707	 i:4 	 global-step:14144	 l-p:0.12768737971782684
epoch£º707	 i:5 	 global-step:14145	 l-p:0.249420166015625
epoch£º707	 i:6 	 global-step:14146	 l-p:0.18259526789188385
epoch£º707	 i:7 	 global-step:14147	 l-p:0.14464505016803741
epoch£º707	 i:8 	 global-step:14148	 l-p:0.12458022683858871
epoch£º707	 i:9 	 global-step:14149	 l-p:0.11358799040317535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:708
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0491, 4.9746, 5.0196],
        [5.0491, 4.8769, 4.8912],
        [5.0491, 4.9942, 4.7212],
        [5.0491, 4.9737, 5.0190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:708, step:0 
model_pd.l_p.mean(): 0.13489685952663422 
model_pd.l_d.mean(): -20.214536666870117 
model_pd.lagr.mean(): -20.079639434814453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4948], device='cuda:0')), ('power', tensor([-21.1050], device='cuda:0'))])
epoch£º708	 i:0 	 global-step:14160	 l-p:0.13489685952663422
epoch£º708	 i:1 	 global-step:14161	 l-p:0.11234597861766815
epoch£º708	 i:2 	 global-step:14162	 l-p:0.16332319378852844
epoch£º708	 i:3 	 global-step:14163	 l-p:0.16886289417743683
epoch£º708	 i:4 	 global-step:14164	 l-p:0.13770239055156708
epoch£º708	 i:5 	 global-step:14165	 l-p:0.06414852291345596
epoch£º708	 i:6 	 global-step:14166	 l-p:0.28723904490470886
epoch£º708	 i:7 	 global-step:14167	 l-p:0.1870425045490265
epoch£º708	 i:8 	 global-step:14168	 l-p:0.1451995074748993
epoch£º708	 i:9 	 global-step:14169	 l-p:0.1705416887998581
====================================================================================================
====================================================================================================
====================================================================================================

epoch:709
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0897, 4.8801, 4.8215],
        [5.0897, 5.0516, 5.0809],
        [5.0897, 5.5512, 5.5468],
        [5.0897, 4.9119, 4.6697]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:709, step:0 
model_pd.l_p.mean(): 0.10457742214202881 
model_pd.l_d.mean(): -20.026018142700195 
model_pd.lagr.mean(): -19.92144012451172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4764], device='cuda:0')), ('power', tensor([-20.8939], device='cuda:0'))])
epoch£º709	 i:0 	 global-step:14180	 l-p:0.10457742214202881
epoch£º709	 i:1 	 global-step:14181	 l-p:0.14985404908657074
epoch£º709	 i:2 	 global-step:14182	 l-p:0.0662221908569336
epoch£º709	 i:3 	 global-step:14183	 l-p:0.09590102732181549
epoch£º709	 i:4 	 global-step:14184	 l-p:0.155015766620636
epoch£º709	 i:5 	 global-step:14185	 l-p:0.1151060089468956
epoch£º709	 i:6 	 global-step:14186	 l-p:0.13700519502162933
epoch£º709	 i:7 	 global-step:14187	 l-p:0.14555031061172485
epoch£º709	 i:8 	 global-step:14188	 l-p:0.12415088713169098
epoch£º709	 i:9 	 global-step:14189	 l-p:0.1573801040649414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:710
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1367, 5.1195, 5.1345],
        [5.1367, 4.9318, 4.8777],
        [5.1367, 4.9635, 4.9709],
        [5.1367, 5.1092, 5.1317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:710, step:0 
model_pd.l_p.mean(): 0.12143949419260025 
model_pd.l_d.mean(): -20.33965492248535 
model_pd.lagr.mean(): -20.218215942382812 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4287], device='cuda:0')), ('power', tensor([-21.1641], device='cuda:0'))])
epoch£º710	 i:0 	 global-step:14200	 l-p:0.12143949419260025
epoch£º710	 i:1 	 global-step:14201	 l-p:0.12582644820213318
epoch£º710	 i:2 	 global-step:14202	 l-p:0.12567239999771118
epoch£º710	 i:3 	 global-step:14203	 l-p:0.06633072346448898
epoch£º710	 i:4 	 global-step:14204	 l-p:0.1544487476348877
epoch£º710	 i:5 	 global-step:14205	 l-p:0.09654273837804794
epoch£º710	 i:6 	 global-step:14206	 l-p:0.21243511140346527
epoch£º710	 i:7 	 global-step:14207	 l-p:0.1446448564529419
epoch£º710	 i:8 	 global-step:14208	 l-p:0.1554640531539917
epoch£º710	 i:9 	 global-step:14209	 l-p:0.13540975749492645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:711
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0866, 5.0756, 5.0856],
        [5.0866, 5.1476, 4.9075],
        [5.0866, 5.0737, 4.8090],
        [5.0866, 5.0866, 5.0866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:711, step:0 
model_pd.l_p.mean(): 0.14953561127185822 
model_pd.l_d.mean(): -19.594228744506836 
model_pd.lagr.mean(): -19.444692611694336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5415], device='cuda:0')), ('power', tensor([-20.5214], device='cuda:0'))])
epoch£º711	 i:0 	 global-step:14220	 l-p:0.14953561127185822
epoch£º711	 i:1 	 global-step:14221	 l-p:0.18760621547698975
epoch£º711	 i:2 	 global-step:14222	 l-p:0.10115715116262436
epoch£º711	 i:3 	 global-step:14223	 l-p:0.1577741503715515
epoch£º711	 i:4 	 global-step:14224	 l-p:0.14715181291103363
epoch£º711	 i:5 	 global-step:14225	 l-p:0.15310943126678467
epoch£º711	 i:6 	 global-step:14226	 l-p:0.025171291083097458
epoch£º711	 i:7 	 global-step:14227	 l-p:0.12181304395198822
epoch£º711	 i:8 	 global-step:14228	 l-p:0.08492876589298248
epoch£º711	 i:9 	 global-step:14229	 l-p:0.11845937371253967
====================================================================================================
====================================================================================================
====================================================================================================

epoch:712
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1721, 5.1192, 4.8510],
        [5.1721, 5.2699, 5.0451],
        [5.1721, 5.0990, 5.1430],
        [5.1721, 5.4033, 5.2466]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:712, step:0 
model_pd.l_p.mean(): -0.6355788111686707 
model_pd.l_d.mean(): -20.050098419189453 
model_pd.lagr.mean(): -20.68567657470703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5081], device='cuda:0')), ('power', tensor([-20.9513], device='cuda:0'))])
epoch£º712	 i:0 	 global-step:14240	 l-p:-0.6355788111686707
epoch£º712	 i:1 	 global-step:14241	 l-p:0.11662590503692627
epoch£º712	 i:2 	 global-step:14242	 l-p:0.13236825168132782
epoch£º712	 i:3 	 global-step:14243	 l-p:0.13864968717098236
epoch£º712	 i:4 	 global-step:14244	 l-p:0.1162949725985527
epoch£º712	 i:5 	 global-step:14245	 l-p:0.1234222799539566
epoch£º712	 i:6 	 global-step:14246	 l-p:0.13800273835659027
epoch£º712	 i:7 	 global-step:14247	 l-p:0.08362539112567902
epoch£º712	 i:8 	 global-step:14248	 l-p:0.07065334916114807
epoch£º712	 i:9 	 global-step:14249	 l-p:0.12388423830270767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:713
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1608, 5.1538, 5.1603],
        [5.1608, 4.9469, 4.8000],
        [5.1608, 5.1608, 5.1608],
        [5.1608, 4.9812, 4.7501]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:713, step:0 
model_pd.l_p.mean(): 0.0860988050699234 
model_pd.l_d.mean(): -20.71921730041504 
model_pd.lagr.mean(): -20.63311767578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3837], device='cuda:0')), ('power', tensor([-21.5041], device='cuda:0'))])
epoch£º713	 i:0 	 global-step:14260	 l-p:0.0860988050699234
epoch£º713	 i:1 	 global-step:14261	 l-p:0.03818073868751526
epoch£º713	 i:2 	 global-step:14262	 l-p:0.13545018434524536
epoch£º713	 i:3 	 global-step:14263	 l-p:0.13205455243587494
epoch£º713	 i:4 	 global-step:14264	 l-p:0.15629243850708008
epoch£º713	 i:5 	 global-step:14265	 l-p:0.1412721425294876
epoch£º713	 i:6 	 global-step:14266	 l-p:0.1203736886382103
epoch£º713	 i:7 	 global-step:14267	 l-p:0.12351283431053162
epoch£º713	 i:8 	 global-step:14268	 l-p:0.06750403344631195
epoch£º713	 i:9 	 global-step:14269	 l-p:0.18350298702716827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:714
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1050, 5.1050, 5.1050],
        [5.1050, 5.0367, 4.7630],
        [5.1050, 4.9313, 4.6844],
        [5.1050, 5.1008, 5.1048]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:714, step:0 
model_pd.l_p.mean(): 0.15023177862167358 
model_pd.l_d.mean(): -20.439931869506836 
model_pd.lagr.mean(): -20.28969955444336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4493], device='cuda:0')), ('power', tensor([-21.2875], device='cuda:0'))])
epoch£º714	 i:0 	 global-step:14280	 l-p:0.15023177862167358
epoch£º714	 i:1 	 global-step:14281	 l-p:0.1699654906988144
epoch£º714	 i:2 	 global-step:14282	 l-p:0.13803577423095703
epoch£º714	 i:3 	 global-step:14283	 l-p:0.14212597906589508
epoch£º714	 i:4 	 global-step:14284	 l-p:0.19185993075370789
epoch£º714	 i:5 	 global-step:14285	 l-p:0.1938338279724121
epoch£º714	 i:6 	 global-step:14286	 l-p:0.10829275101423264
epoch£º714	 i:7 	 global-step:14287	 l-p:0.09854098409414291
epoch£º714	 i:8 	 global-step:14288	 l-p:0.12503038346767426
epoch£º714	 i:9 	 global-step:14289	 l-p:0.09676793962717056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:715
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0573, 5.0452, 5.0561],
        [5.0573, 5.4210, 5.3497],
        [5.0573, 4.8311, 4.6885],
        [5.0573, 5.0573, 5.0573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:715, step:0 
model_pd.l_p.mean(): 0.13500580191612244 
model_pd.l_d.mean(): -20.22793960571289 
model_pd.lagr.mean(): -20.092933654785156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4833], device='cuda:0')), ('power', tensor([-21.1067], device='cuda:0'))])
epoch£º715	 i:0 	 global-step:14300	 l-p:0.13500580191612244
epoch£º715	 i:1 	 global-step:14301	 l-p:0.1938989907503128
epoch£º715	 i:2 	 global-step:14302	 l-p:0.11948313564062119
epoch£º715	 i:3 	 global-step:14303	 l-p:0.1574430912733078
epoch£º715	 i:4 	 global-step:14304	 l-p:0.1824064403772354
epoch£º715	 i:5 	 global-step:14305	 l-p:0.17828154563903809
epoch£º715	 i:6 	 global-step:14306	 l-p:0.13005833327770233
epoch£º715	 i:7 	 global-step:14307	 l-p:0.23657755553722382
epoch£º715	 i:8 	 global-step:14308	 l-p:0.12744775414466858
epoch£º715	 i:9 	 global-step:14309	 l-p:0.11193893104791641
====================================================================================================
====================================================================================================
====================================================================================================

epoch:716
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0648, 4.9446, 4.6691],
        [5.0648, 5.0622, 5.0647],
        [5.0648, 5.0648, 5.0648],
        [5.0648, 5.4112, 5.3281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:716, step:0 
model_pd.l_p.mean(): 0.17495372891426086 
model_pd.l_d.mean(): -20.191205978393555 
model_pd.lagr.mean(): -20.016252517700195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4989], device='cuda:0')), ('power', tensor([-21.0854], device='cuda:0'))])
epoch£º716	 i:0 	 global-step:14320	 l-p:0.17495372891426086
epoch£º716	 i:1 	 global-step:14321	 l-p:0.13677798211574554
epoch£º716	 i:2 	 global-step:14322	 l-p:0.14119748771190643
epoch£º716	 i:3 	 global-step:14323	 l-p:0.12977394461631775
epoch£º716	 i:4 	 global-step:14324	 l-p:0.20446085929870605
epoch£º716	 i:5 	 global-step:14325	 l-p:0.10992531478404999
epoch£º716	 i:6 	 global-step:14326	 l-p:0.1304934173822403
epoch£º716	 i:7 	 global-step:14327	 l-p:0.16231819987297058
epoch£º716	 i:8 	 global-step:14328	 l-p:0.0770394578576088
epoch£º716	 i:9 	 global-step:14329	 l-p:0.12989966571331024
====================================================================================================
====================================================================================================
====================================================================================================

epoch:717
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0922, 4.9906, 5.0391],
        [5.0922, 4.9629, 5.0065],
        [5.0922, 5.0869, 5.0919],
        [5.0922, 5.2084, 4.9916]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:717, step:0 
model_pd.l_p.mean(): 0.14742319285869598 
model_pd.l_d.mean(): -20.69513702392578 
model_pd.lagr.mean(): -20.547714233398438 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4152], device='cuda:0')), ('power', tensor([-21.5122], device='cuda:0'))])
epoch£º717	 i:0 	 global-step:14340	 l-p:0.14742319285869598
epoch£º717	 i:1 	 global-step:14341	 l-p:0.10868312418460846
epoch£º717	 i:2 	 global-step:14342	 l-p:0.20042675733566284
epoch£º717	 i:3 	 global-step:14343	 l-p:0.10433061420917511
epoch£º717	 i:4 	 global-step:14344	 l-p:0.12926295399665833
epoch£º717	 i:5 	 global-step:14345	 l-p:0.07195582985877991
epoch£º717	 i:6 	 global-step:14346	 l-p:0.16138547658920288
epoch£º717	 i:7 	 global-step:14347	 l-p:0.19934071600437164
epoch£º717	 i:8 	 global-step:14348	 l-p:0.2673419415950775
epoch£º717	 i:9 	 global-step:14349	 l-p:0.12138835340738297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:718
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9137,  0.8867,  1.0000,  0.8604,
          1.0000,  0.9704, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228]], device='cuda:0')
 pt:tensor([[5.0612, 4.8574, 4.8272],
        [5.0612, 4.8932, 4.9142],
        [5.0612, 5.4606, 5.4127],
        [5.0612, 4.9212, 4.9616]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:718, step:0 
model_pd.l_p.mean(): 0.1310628056526184 
model_pd.l_d.mean(): -20.71015739440918 
model_pd.lagr.mean(): -20.57909393310547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4166], device='cuda:0')), ('power', tensor([-21.5289], device='cuda:0'))])
epoch£º718	 i:0 	 global-step:14360	 l-p:0.1310628056526184
epoch£º718	 i:1 	 global-step:14361	 l-p:0.10483547300100327
epoch£º718	 i:2 	 global-step:14362	 l-p:0.24510501325130463
epoch£º718	 i:3 	 global-step:14363	 l-p:0.15666094422340393
epoch£º718	 i:4 	 global-step:14364	 l-p:0.17958129942417145
epoch£º718	 i:5 	 global-step:14365	 l-p:0.1070689857006073
epoch£º718	 i:6 	 global-step:14366	 l-p:0.12374217063188553
epoch£º718	 i:7 	 global-step:14367	 l-p:0.10911209136247635
epoch£º718	 i:8 	 global-step:14368	 l-p:0.25526151061058044
epoch£º718	 i:9 	 global-step:14369	 l-p:0.11907756328582764
====================================================================================================
====================================================================================================
====================================================================================================

epoch:719
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0685, 4.9182, 4.9527],
        [5.0685, 4.9713, 4.6931],
        [5.0685, 5.1432, 4.9074],
        [5.0685, 5.0459, 5.0650]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:719, step:0 
model_pd.l_p.mean(): 0.1268332302570343 
model_pd.l_d.mean(): -18.516061782836914 
model_pd.lagr.mean(): -18.38922882080078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6019], device='cuda:0')), ('power', tensor([-19.4857], device='cuda:0'))])
epoch£º719	 i:0 	 global-step:14380	 l-p:0.1268332302570343
epoch£º719	 i:1 	 global-step:14381	 l-p:0.18516132235527039
epoch£º719	 i:2 	 global-step:14382	 l-p:0.13016442954540253
epoch£º719	 i:3 	 global-step:14383	 l-p:0.10333621501922607
epoch£º719	 i:4 	 global-step:14384	 l-p:0.13976921141147614
epoch£º719	 i:5 	 global-step:14385	 l-p:0.1698116511106491
epoch£º719	 i:6 	 global-step:14386	 l-p:0.13564248383045197
epoch£º719	 i:7 	 global-step:14387	 l-p:0.10021025687456131
epoch£º719	 i:8 	 global-step:14388	 l-p:0.15346167981624603
epoch£º719	 i:9 	 global-step:14389	 l-p:0.14211778342723846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:720
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1224, 5.0526, 5.0962],
        [5.1224, 4.9221, 4.8904],
        [5.1224, 4.9443, 4.7005],
        [5.1224, 4.9031, 4.8032]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:720, step:0 
model_pd.l_p.mean(): 0.10967735201120377 
model_pd.l_d.mean(): -20.191593170166016 
model_pd.lagr.mean(): -20.0819149017334 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4595], device='cuda:0')), ('power', tensor([-21.0450], device='cuda:0'))])
epoch£º720	 i:0 	 global-step:14400	 l-p:0.10967735201120377
epoch£º720	 i:1 	 global-step:14401	 l-p:0.12187564373016357
epoch£º720	 i:2 	 global-step:14402	 l-p:0.038638681173324585
epoch£º720	 i:3 	 global-step:14403	 l-p:0.13312341272830963
epoch£º720	 i:4 	 global-step:14404	 l-p:0.12217843532562256
epoch£º720	 i:5 	 global-step:14405	 l-p:0.13696391880512238
epoch£º720	 i:6 	 global-step:14406	 l-p:0.1409214287996292
epoch£º720	 i:7 	 global-step:14407	 l-p:0.180266335606575
epoch£º720	 i:8 	 global-step:14408	 l-p:0.1435735523700714
epoch£º720	 i:9 	 global-step:14409	 l-p:0.11399297416210175
====================================================================================================
====================================================================================================
====================================================================================================

epoch:721
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9132,  0.8860,  1.0000,  0.8596,
          1.0000,  0.9702, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3078,  0.2078,  1.0000,  0.1403,
          1.0000,  0.6752, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228]], device='cuda:0')
 pt:tensor([[5.1265, 4.9882, 4.7212],
        [5.1265, 5.5464, 5.5096],
        [5.1265, 4.9053, 4.7853],
        [5.1265, 4.9094, 4.7370]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:721, step:0 
model_pd.l_p.mean(): 0.11616336554288864 
model_pd.l_d.mean(): -19.451007843017578 
model_pd.lagr.mean(): -19.3348445892334 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5051], device='cuda:0')), ('power', tensor([-20.3379], device='cuda:0'))])
epoch£º721	 i:0 	 global-step:14420	 l-p:0.11616336554288864
epoch£º721	 i:1 	 global-step:14421	 l-p:0.08025108277797699
epoch£º721	 i:2 	 global-step:14422	 l-p:0.1421220898628235
epoch£º721	 i:3 	 global-step:14423	 l-p:0.147918701171875
epoch£º721	 i:4 	 global-step:14424	 l-p:0.10746312141418457
epoch£º721	 i:5 	 global-step:14425	 l-p:0.13134941458702087
epoch£º721	 i:6 	 global-step:14426	 l-p:0.15861502289772034
epoch£º721	 i:7 	 global-step:14427	 l-p:0.1233343854546547
epoch£º721	 i:8 	 global-step:14428	 l-p:0.13716374337673187
epoch£º721	 i:9 	 global-step:14429	 l-p:0.18455304205417633
====================================================================================================
====================================================================================================
====================================================================================================

epoch:722
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0753, 5.0753, 5.0753],
        [5.0753, 5.0727, 5.0752],
        [5.0753, 5.0753, 5.0753],
        [5.0753, 5.0278, 5.0625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:722, step:0 
model_pd.l_p.mean(): 0.10214778780937195 
model_pd.l_d.mean(): -20.068449020385742 
model_pd.lagr.mean(): -19.96630096435547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5022], device='cuda:0')), ('power', tensor([-20.9638], device='cuda:0'))])
epoch£º722	 i:0 	 global-step:14440	 l-p:0.10214778780937195
epoch£º722	 i:1 	 global-step:14441	 l-p:0.14079847931861877
epoch£º722	 i:2 	 global-step:14442	 l-p:0.16585782170295715
epoch£º722	 i:3 	 global-step:14443	 l-p:0.13175946474075317
epoch£º722	 i:4 	 global-step:14444	 l-p:0.13754932582378387
epoch£º722	 i:5 	 global-step:14445	 l-p:0.2762635052204132
epoch£º722	 i:6 	 global-step:14446	 l-p:0.12146304547786713
epoch£º722	 i:7 	 global-step:14447	 l-p:0.13348731398582458
epoch£º722	 i:8 	 global-step:14448	 l-p:0.13557420670986176
epoch£º722	 i:9 	 global-step:14449	 l-p:1.1764129400253296
====================================================================================================
====================================================================================================
====================================================================================================

epoch:723
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0226, 4.9570, 4.9998],
        [5.0226, 4.9204, 4.6375],
        [5.0226, 5.0225, 5.0226],
        [5.0226, 5.0226, 5.0226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:723, step:0 
model_pd.l_p.mean(): 0.11727745831012726 
model_pd.l_d.mean(): -20.37186622619629 
model_pd.lagr.mean(): -20.254589080810547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4882], device='cuda:0')), ('power', tensor([-21.2584], device='cuda:0'))])
epoch£º723	 i:0 	 global-step:14460	 l-p:0.11727745831012726
epoch£º723	 i:1 	 global-step:14461	 l-p:0.19722795486450195
epoch£º723	 i:2 	 global-step:14462	 l-p:0.21952712535858154
epoch£º723	 i:3 	 global-step:14463	 l-p:0.1294669359922409
epoch£º723	 i:4 	 global-step:14464	 l-p:0.39940401911735535
epoch£º723	 i:5 	 global-step:14465	 l-p:0.19783316552639008
epoch£º723	 i:6 	 global-step:14466	 l-p:0.10518892109394073
epoch£º723	 i:7 	 global-step:14467	 l-p:0.12713877856731415
epoch£º723	 i:8 	 global-step:14468	 l-p:0.47142574191093445
epoch£º723	 i:9 	 global-step:14469	 l-p:0.13010378181934357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:724
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0484, 5.0456, 5.0483],
        [5.0484, 4.9840, 4.7046],
        [5.0484, 5.0364, 5.0472],
        [5.0484, 4.8308, 4.7728]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:724, step:0 
model_pd.l_p.mean(): 0.11905137449502945 
model_pd.l_d.mean(): -19.666549682617188 
model_pd.lagr.mean(): -19.54749870300293 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5118], device='cuda:0')), ('power', tensor([-20.5644], device='cuda:0'))])
epoch£º724	 i:0 	 global-step:14480	 l-p:0.11905137449502945
epoch£º724	 i:1 	 global-step:14481	 l-p:0.10833627730607986
epoch£º724	 i:2 	 global-step:14482	 l-p:0.20858748257160187
epoch£º724	 i:3 	 global-step:14483	 l-p:0.1469740867614746
epoch£º724	 i:4 	 global-step:14484	 l-p:0.121034637093544
epoch£º724	 i:5 	 global-step:14485	 l-p:0.1457999348640442
epoch£º724	 i:6 	 global-step:14486	 l-p:0.13115254044532776
epoch£º724	 i:7 	 global-step:14487	 l-p:0.14197027683258057
epoch£º724	 i:8 	 global-step:14488	 l-p:0.21387556195259094
epoch£º724	 i:9 	 global-step:14489	 l-p:0.10378392785787582
====================================================================================================
====================================================================================================
====================================================================================================

epoch:725
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0956, 5.0913, 5.0954],
        [5.0956, 4.9499, 4.6804],
        [5.0956, 4.9124, 4.6652],
        [5.0956, 4.9649, 5.0088]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:725, step:0 
model_pd.l_p.mean(): 0.13080312311649323 
model_pd.l_d.mean(): -20.406044006347656 
model_pd.lagr.mean(): -20.275239944458008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4290], device='cuda:0')), ('power', tensor([-21.2319], device='cuda:0'))])
epoch£º725	 i:0 	 global-step:14500	 l-p:0.13080312311649323
epoch£º725	 i:1 	 global-step:14501	 l-p:0.12495708465576172
epoch£º725	 i:2 	 global-step:14502	 l-p:0.13000088930130005
epoch£º725	 i:3 	 global-step:14503	 l-p:0.15769532322883606
epoch£º725	 i:4 	 global-step:14504	 l-p:0.12974272668361664
epoch£º725	 i:5 	 global-step:14505	 l-p:0.26353195309638977
epoch£º725	 i:6 	 global-step:14506	 l-p:0.19175292551517487
epoch£º725	 i:7 	 global-step:14507	 l-p:0.10280132293701172
epoch£º725	 i:8 	 global-step:14508	 l-p:0.1124684140086174
epoch£º725	 i:9 	 global-step:14509	 l-p:0.1256953328847885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:726
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1394,  0.0723,  1.0000,  0.0375,
          1.0000,  0.5185, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5998,  0.5059,  1.0000,  0.4266,
          1.0000,  0.8434, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7857,  0.7250,  1.0000,  0.6690,
          1.0000,  0.9228, 31.6228]], device='cuda:0')
 pt:tensor([[5.0488, 4.9234, 4.9703],
        [5.0488, 4.8176, 4.6847],
        [5.0488, 5.0017, 4.7243],
        [5.0488, 5.2481, 5.0741]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:726, step:0 
model_pd.l_p.mean(): 0.17035147547721863 
model_pd.l_d.mean(): -19.071247100830078 
model_pd.lagr.mean(): -18.900896072387695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5699], device='cuda:0')), ('power', tensor([-20.0181], device='cuda:0'))])
epoch£º726	 i:0 	 global-step:14520	 l-p:0.17035147547721863
epoch£º726	 i:1 	 global-step:14521	 l-p:0.1810840517282486
epoch£º726	 i:2 	 global-step:14522	 l-p:0.190852090716362
epoch£º726	 i:3 	 global-step:14523	 l-p:0.18161238729953766
epoch£º726	 i:4 	 global-step:14524	 l-p:0.14605100452899933
epoch£º726	 i:5 	 global-step:14525	 l-p:0.13046368956565857
epoch£º726	 i:6 	 global-step:14526	 l-p:0.20669203996658325
epoch£º726	 i:7 	 global-step:14527	 l-p:0.06449908018112183
epoch£º726	 i:8 	 global-step:14528	 l-p:0.150669127702713
epoch£º726	 i:9 	 global-step:14529	 l-p:0.12213084101676941
====================================================================================================
====================================================================================================
====================================================================================================

epoch:727
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0973, 5.0973, 5.0973],
        [5.0973, 5.2442, 5.0408],
        [5.0973, 5.5080, 5.4650],
        [5.0973, 5.0973, 5.0973]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:727, step:0 
model_pd.l_p.mean(): 0.15766771137714386 
model_pd.l_d.mean(): -20.36435890197754 
model_pd.lagr.mean(): -20.20669174194336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4704], device='cuda:0')), ('power', tensor([-21.2324], device='cuda:0'))])
epoch£º727	 i:0 	 global-step:14540	 l-p:0.15766771137714386
epoch£º727	 i:1 	 global-step:14541	 l-p:0.12320247292518616
epoch£º727	 i:2 	 global-step:14542	 l-p:0.13744451105594635
epoch£º727	 i:3 	 global-step:14543	 l-p:0.1141313686966896
epoch£º727	 i:4 	 global-step:14544	 l-p:0.13491305708885193
epoch£º727	 i:5 	 global-step:14545	 l-p:0.20300912857055664
epoch£º727	 i:6 	 global-step:14546	 l-p:0.12552417814731598
epoch£º727	 i:7 	 global-step:14547	 l-p:0.16007108986377716
epoch£º727	 i:8 	 global-step:14548	 l-p:0.1499122828245163
epoch£º727	 i:9 	 global-step:14549	 l-p:0.0708700641989708
====================================================================================================
====================================================================================================
====================================================================================================

epoch:728
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1202, 5.1067, 5.1188],
        [5.1202, 5.1195, 5.1202],
        [5.1202, 4.9378, 4.6920],
        [5.1202, 5.5691, 5.5514]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:728, step:0 
model_pd.l_p.mean(): 0.12938310205936432 
model_pd.l_d.mean(): -20.305334091186523 
model_pd.lagr.mean(): -20.17595100402832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4597], device='cuda:0')), ('power', tensor([-21.1611], device='cuda:0'))])
epoch£º728	 i:0 	 global-step:14560	 l-p:0.12938310205936432
epoch£º728	 i:1 	 global-step:14561	 l-p:0.10107452422380447
epoch£º728	 i:2 	 global-step:14562	 l-p:0.12586170434951782
epoch£º728	 i:3 	 global-step:14563	 l-p:0.14532171189785004
epoch£º728	 i:4 	 global-step:14564	 l-p:0.13764537870883942
epoch£º728	 i:5 	 global-step:14565	 l-p:0.13245856761932373
epoch£º728	 i:6 	 global-step:14566	 l-p:-0.6337206959724426
epoch£º728	 i:7 	 global-step:14567	 l-p:0.13703759014606476
epoch£º728	 i:8 	 global-step:14568	 l-p:0.11325172334909439
epoch£º728	 i:9 	 global-step:14569	 l-p:0.10231159627437592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:729
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1848, 5.0488, 5.0884],
        [5.1848, 5.1116, 5.1560],
        [5.1848, 5.0208, 5.0406],
        [5.1848, 5.1848, 5.1848]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:729, step:0 
model_pd.l_p.mean(): 3.191291332244873 
model_pd.l_d.mean(): -19.154743194580078 
model_pd.lagr.mean(): -15.963451385498047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5662], device='cuda:0')), ('power', tensor([-20.0994], device='cuda:0'))])
epoch£º729	 i:0 	 global-step:14580	 l-p:3.191291332244873
epoch£º729	 i:1 	 global-step:14581	 l-p:0.11317026615142822
epoch£º729	 i:2 	 global-step:14582	 l-p:0.16044582426548004
epoch£º729	 i:3 	 global-step:14583	 l-p:0.1274920105934143
epoch£º729	 i:4 	 global-step:14584	 l-p:0.14081069827079773
epoch£º729	 i:5 	 global-step:14585	 l-p:0.11440758407115936
epoch£º729	 i:6 	 global-step:14586	 l-p:0.12498900294303894
epoch£º729	 i:7 	 global-step:14587	 l-p:0.03558138385415077
epoch£º729	 i:8 	 global-step:14588	 l-p:0.13559037446975708
epoch£º729	 i:9 	 global-step:14589	 l-p:0.12081334739923477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:730
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5584,  0.4599,  1.0000,  0.3787,
          1.0000,  0.8235, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7771,  0.7145,  1.0000,  0.6569,
          1.0000,  0.9194, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4602,  0.3553,  1.0000,  0.2743,
          1.0000,  0.7721, 31.6228]], device='cuda:0')
 pt:tensor([[5.1177, 5.0373, 4.7587],
        [5.1177, 5.3228, 5.1499],
        [5.1177, 4.9385, 4.6883],
        [5.1177, 4.9476, 4.6906]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:730, step:0 
model_pd.l_p.mean(): 0.05202556401491165 
model_pd.l_d.mean(): -19.144207000732422 
model_pd.lagr.mean(): -19.092182159423828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5543], device='cuda:0')), ('power', tensor([-20.0763], device='cuda:0'))])
epoch£º730	 i:0 	 global-step:14600	 l-p:0.05202556401491165
epoch£º730	 i:1 	 global-step:14601	 l-p:0.13983701169490814
epoch£º730	 i:2 	 global-step:14602	 l-p:0.18014705181121826
epoch£º730	 i:3 	 global-step:14603	 l-p:0.1269492208957672
epoch£º730	 i:4 	 global-step:14604	 l-p:0.14679564535617828
epoch£º730	 i:5 	 global-step:14605	 l-p:0.13494308292865753
epoch£º730	 i:6 	 global-step:14606	 l-p:0.13863611221313477
epoch£º730	 i:7 	 global-step:14607	 l-p:0.14008688926696777
epoch£º730	 i:8 	 global-step:14608	 l-p:0.3689480423927307
epoch£º730	 i:9 	 global-step:14609	 l-p:0.09314480423927307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:731
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0284, 5.3291, 5.2156],
        [5.0284, 4.8436, 4.8525],
        [5.0284, 4.7933, 4.6495],
        [5.0284, 5.0174, 5.0274]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:731, step:0 
model_pd.l_p.mean(): 0.12006548792123795 
model_pd.l_d.mean(): -20.11652183532715 
model_pd.lagr.mean(): -19.996456146240234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5202], device='cuda:0')), ('power', tensor([-21.0314], device='cuda:0'))])
epoch£º731	 i:0 	 global-step:14620	 l-p:0.12006548792123795
epoch£º731	 i:1 	 global-step:14621	 l-p:0.13067781925201416
epoch£º731	 i:2 	 global-step:14622	 l-p:0.2147979736328125
epoch£º731	 i:3 	 global-step:14623	 l-p:0.3322904109954834
epoch£º731	 i:4 	 global-step:14624	 l-p:0.13888266682624817
epoch£º731	 i:5 	 global-step:14625	 l-p:0.3189008831977844
epoch£º731	 i:6 	 global-step:14626	 l-p:0.10089203715324402
epoch£º731	 i:7 	 global-step:14627	 l-p:-0.11180149763822556
epoch£º731	 i:8 	 global-step:14628	 l-p:0.05562747269868851
epoch£º731	 i:9 	 global-step:14629	 l-p:0.12149926275014877
====================================================================================================
====================================================================================================
====================================================================================================

epoch:732
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9819, 4.8981, 4.6109],
        [4.9819, 4.9794, 4.9818],
        [4.9819, 4.9819, 4.9819],
        [4.9819, 4.9781, 4.9817]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:732, step:0 
model_pd.l_p.mean(): -1.1787679195404053 
model_pd.l_d.mean(): -19.38084602355957 
model_pd.lagr.mean(): -20.559614181518555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5591], device='cuda:0')), ('power', tensor([-20.3223], device='cuda:0'))])
epoch£º732	 i:0 	 global-step:14640	 l-p:-1.1787679195404053
epoch£º732	 i:1 	 global-step:14641	 l-p:0.42738935351371765
epoch£º732	 i:2 	 global-step:14642	 l-p:0.1263473629951477
epoch£º732	 i:3 	 global-step:14643	 l-p:0.13737846910953522
epoch£º732	 i:4 	 global-step:14644	 l-p:0.7323383688926697
epoch£º732	 i:5 	 global-step:14645	 l-p:0.1205858588218689
epoch£º732	 i:6 	 global-step:14646	 l-p:0.19771361351013184
epoch£º732	 i:7 	 global-step:14647	 l-p:0.0966772809624672
epoch£º732	 i:8 	 global-step:14648	 l-p:0.1384357064962387
epoch£º732	 i:9 	 global-step:14649	 l-p:0.2678620219230652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:733
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0602, 5.0602, 5.0602],
        [5.0602, 5.0481, 5.0590],
        [5.0602, 4.9814, 5.0282],
        [5.0602, 5.0271, 5.0535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:733, step:0 
model_pd.l_p.mean(): 0.1330575942993164 
model_pd.l_d.mean(): -19.69640350341797 
model_pd.lagr.mean(): -19.56334686279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5161], device='cuda:0')), ('power', tensor([-20.5992], device='cuda:0'))])
epoch£º733	 i:0 	 global-step:14660	 l-p:0.1330575942993164
epoch£º733	 i:1 	 global-step:14661	 l-p:0.10786473751068115
epoch£º733	 i:2 	 global-step:14662	 l-p:0.22471021115779877
epoch£º733	 i:3 	 global-step:14663	 l-p:0.17223216593265533
epoch£º733	 i:4 	 global-step:14664	 l-p:0.07033657282590866
epoch£º733	 i:5 	 global-step:14665	 l-p:0.14967221021652222
epoch£º733	 i:6 	 global-step:14666	 l-p:0.16473108530044556
epoch£º733	 i:7 	 global-step:14667	 l-p:0.10657535493373871
epoch£º733	 i:8 	 global-step:14668	 l-p:0.1642388552427292
epoch£º733	 i:9 	 global-step:14669	 l-p:0.10572686791419983
====================================================================================================
====================================================================================================
====================================================================================================

epoch:734
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1332, 5.1319, 5.1331],
        [5.1332, 5.2522, 5.0335],
        [5.1332, 5.4711, 5.3777],
        [5.1332, 4.9307, 4.7087]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:734, step:0 
model_pd.l_p.mean(): 0.15534985065460205 
model_pd.l_d.mean(): -20.273738861083984 
model_pd.lagr.mean(): -20.118389129638672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4549], device='cuda:0')), ('power', tensor([-21.1240], device='cuda:0'))])
epoch£º734	 i:0 	 global-step:14680	 l-p:0.15534985065460205
epoch£º734	 i:1 	 global-step:14681	 l-p:0.11051620543003082
epoch£º734	 i:2 	 global-step:14682	 l-p:0.10161765664815903
epoch£º734	 i:3 	 global-step:14683	 l-p:0.1609470546245575
epoch£º734	 i:4 	 global-step:14684	 l-p:0.10929669439792633
epoch£º734	 i:5 	 global-step:14685	 l-p:0.13913755118846893
epoch£º734	 i:6 	 global-step:14686	 l-p:0.06896854937076569
epoch£º734	 i:7 	 global-step:14687	 l-p:0.10557639598846436
epoch£º734	 i:8 	 global-step:14688	 l-p:0.13103075325489044
epoch£º734	 i:9 	 global-step:14689	 l-p:0.22493672370910645
====================================================================================================
====================================================================================================
====================================================================================================

epoch:735
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0993, 4.8729, 4.7726],
        [5.0993, 5.0157, 5.0634],
        [5.0993, 5.0072, 4.7259],
        [5.0993, 4.9767, 4.6978]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:735, step:0 
model_pd.l_p.mean(): 0.14501143991947174 
model_pd.l_d.mean(): -19.862079620361328 
model_pd.lagr.mean(): -19.71706771850586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4758], device='cuda:0')), ('power', tensor([-20.7263], device='cuda:0'))])
epoch£º735	 i:0 	 global-step:14700	 l-p:0.14501143991947174
epoch£º735	 i:1 	 global-step:14701	 l-p:0.09627087414264679
epoch£º735	 i:2 	 global-step:14702	 l-p:0.1048484593629837
epoch£º735	 i:3 	 global-step:14703	 l-p:0.1308276653289795
epoch£º735	 i:4 	 global-step:14704	 l-p:0.23381522297859192
epoch£º735	 i:5 	 global-step:14705	 l-p:0.13341782987117767
epoch£º735	 i:6 	 global-step:14706	 l-p:0.12051747739315033
epoch£º735	 i:7 	 global-step:14707	 l-p:0.15923157334327698
epoch£º735	 i:8 	 global-step:14708	 l-p:0.14952416718006134
epoch£º735	 i:9 	 global-step:14709	 l-p:0.2443356215953827
====================================================================================================
====================================================================================================
====================================================================================================

epoch:736
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0730, 5.3994, 5.3001],
        [5.0730, 5.0059, 5.0493],
        [5.0730, 4.8410, 4.6877],
        [5.0730, 4.8406, 4.6962]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:736, step:0 
model_pd.l_p.mean(): 0.16569125652313232 
model_pd.l_d.mean(): -20.285547256469727 
model_pd.lagr.mean(): -20.119855880737305 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4908], device='cuda:0')), ('power', tensor([-21.1732], device='cuda:0'))])
epoch£º736	 i:0 	 global-step:14720	 l-p:0.16569125652313232
epoch£º736	 i:1 	 global-step:14721	 l-p:0.10947024822235107
epoch£º736	 i:2 	 global-step:14722	 l-p:0.1901458352804184
epoch£º736	 i:3 	 global-step:14723	 l-p:0.1389831006526947
epoch£º736	 i:4 	 global-step:14724	 l-p:0.09470517933368683
epoch£º736	 i:5 	 global-step:14725	 l-p:0.12950609624385834
epoch£º736	 i:6 	 global-step:14726	 l-p:0.1373887062072754
epoch£º736	 i:7 	 global-step:14727	 l-p:0.12880957126617432
epoch£º736	 i:8 	 global-step:14728	 l-p:0.0927305594086647
epoch£º736	 i:9 	 global-step:14729	 l-p:0.16820915043354034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:737
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1200, 5.0866, 5.1132],
        [5.1200, 5.5584, 5.5319],
        [5.1200, 5.0148, 4.7344],
        [5.1200, 5.4903, 5.4177]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:737, step:0 
model_pd.l_p.mean(): 0.12258943170309067 
model_pd.l_d.mean(): -20.519243240356445 
model_pd.lagr.mean(): -20.39665412902832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4159], device='cuda:0')), ('power', tensor([-21.3337], device='cuda:0'))])
epoch£º737	 i:0 	 global-step:14740	 l-p:0.12258943170309067
epoch£º737	 i:1 	 global-step:14741	 l-p:0.18866030871868134
epoch£º737	 i:2 	 global-step:14742	 l-p:0.06733359396457672
epoch£º737	 i:3 	 global-step:14743	 l-p:0.10741034895181656
epoch£º737	 i:4 	 global-step:14744	 l-p:0.16490107774734497
epoch£º737	 i:5 	 global-step:14745	 l-p:0.12621231377124786
epoch£º737	 i:6 	 global-step:14746	 l-p:0.13445991277694702
epoch£º737	 i:7 	 global-step:14747	 l-p:0.13022635877132416
epoch£º737	 i:8 	 global-step:14748	 l-p:0.1406555473804474
epoch£º737	 i:9 	 global-step:14749	 l-p:0.11909240484237671
====================================================================================================
====================================================================================================
====================================================================================================

epoch:738
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1155, 5.1031, 5.1142],
        [5.1155, 5.1129, 5.1154],
        [5.1155, 5.1147, 5.1154],
        [5.1155, 5.1155, 5.1155]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:738, step:0 
model_pd.l_p.mean(): 0.10310979932546616 
model_pd.l_d.mean(): -20.11029815673828 
model_pd.lagr.mean(): -20.00718879699707 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4856], device='cuda:0')), ('power', tensor([-20.9893], device='cuda:0'))])
epoch£º738	 i:0 	 global-step:14760	 l-p:0.10310979932546616
epoch£º738	 i:1 	 global-step:14761	 l-p:0.13634000718593597
epoch£º738	 i:2 	 global-step:14762	 l-p:0.09109854698181152
epoch£º738	 i:3 	 global-step:14763	 l-p:0.14539074897766113
epoch£º738	 i:4 	 global-step:14764	 l-p:0.1299806535243988
epoch£º738	 i:5 	 global-step:14765	 l-p:0.1471107453107834
epoch£º738	 i:6 	 global-step:14766	 l-p:0.1464305967092514
epoch£º738	 i:7 	 global-step:14767	 l-p:0.15386679768562317
epoch£º738	 i:8 	 global-step:14768	 l-p:0.15248852968215942
epoch£º738	 i:9 	 global-step:14769	 l-p:0.17230460047721863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:739
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0909, 5.0909, 5.0909],
        [5.0909, 5.0909, 5.0909],
        [5.0909, 5.0216, 4.7389],
        [5.0909, 4.9567, 5.0015]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:739, step:0 
model_pd.l_p.mean(): 0.15384934842586517 
model_pd.l_d.mean(): -19.566320419311523 
model_pd.lagr.mean(): -19.412471771240234 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5312], device='cuda:0')), ('power', tensor([-20.4823], device='cuda:0'))])
epoch£º739	 i:0 	 global-step:14780	 l-p:0.15384934842586517
epoch£º739	 i:1 	 global-step:14781	 l-p:0.13061408698558807
epoch£º739	 i:2 	 global-step:14782	 l-p:0.1796008199453354
epoch£º739	 i:3 	 global-step:14783	 l-p:0.15646426379680634
epoch£º739	 i:4 	 global-step:14784	 l-p:0.1257525235414505
epoch£º739	 i:5 	 global-step:14785	 l-p:0.17367535829544067
epoch£º739	 i:6 	 global-step:14786	 l-p:0.08539795875549316
epoch£º739	 i:7 	 global-step:14787	 l-p:0.130941703915596
epoch£º739	 i:8 	 global-step:14788	 l-p:0.05548161268234253
epoch£º739	 i:9 	 global-step:14789	 l-p:0.08986923098564148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:740
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1563, 5.1563, 5.1563],
        [5.1563, 5.1450, 5.1552],
        [5.1563, 4.9445, 4.8941],
        [5.1563, 4.9522, 4.7300]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:740, step:0 
model_pd.l_p.mean(): 0.12884005904197693 
model_pd.l_d.mean(): -19.06415557861328 
model_pd.lagr.mean(): -18.93531608581543 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5230], device='cuda:0')), ('power', tensor([-19.9624], device='cuda:0'))])
epoch£º740	 i:0 	 global-step:14800	 l-p:0.12884005904197693
epoch£º740	 i:1 	 global-step:14801	 l-p:0.0308695025742054
epoch£º740	 i:2 	 global-step:14802	 l-p:0.09970364719629288
epoch£º740	 i:3 	 global-step:14803	 l-p:0.056214749813079834
epoch£º740	 i:4 	 global-step:14804	 l-p:0.13386033475399017
epoch£º740	 i:5 	 global-step:14805	 l-p:0.12608502805233002
epoch£º740	 i:6 	 global-step:14806	 l-p:0.14228279888629913
epoch£º740	 i:7 	 global-step:14807	 l-p:0.14243315160274506
epoch£º740	 i:8 	 global-step:14808	 l-p:0.1317579448223114
epoch£º740	 i:9 	 global-step:14809	 l-p:0.15075808763504028
====================================================================================================
====================================================================================================
====================================================================================================

epoch:741
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1426, 4.9767, 4.7135],
        [5.1426, 5.5572, 5.5126],
        [5.1426, 4.9848, 4.7173],
        [5.1426, 5.1426, 5.1426]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:741, step:0 
model_pd.l_p.mean(): 0.11591209471225739 
model_pd.l_d.mean(): -20.17236328125 
model_pd.lagr.mean(): -20.05645179748535 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4797], device='cuda:0')), ('power', tensor([-21.0465], device='cuda:0'))])
epoch£º741	 i:0 	 global-step:14820	 l-p:0.11591209471225739
epoch£º741	 i:1 	 global-step:14821	 l-p:0.10416219383478165
epoch£º741	 i:2 	 global-step:14822	 l-p:0.12721814215183258
epoch£º741	 i:3 	 global-step:14823	 l-p:0.1571510136127472
epoch£º741	 i:4 	 global-step:14824	 l-p:0.07345007359981537
epoch£º741	 i:5 	 global-step:14825	 l-p:0.11528172343969345
epoch£º741	 i:6 	 global-step:14826	 l-p:0.1478338986635208
epoch£º741	 i:7 	 global-step:14827	 l-p:0.14863692224025726
epoch£º741	 i:8 	 global-step:14828	 l-p:0.16620942950248718
epoch£º741	 i:9 	 global-step:14829	 l-p:0.1490578055381775
====================================================================================================
====================================================================================================
====================================================================================================

epoch:742
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0825, 5.4789, 5.4239],
        [5.0825, 4.9544, 5.0017],
        [5.0825, 5.0784, 5.0823],
        [5.0825, 4.8529, 4.7628]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:742, step:0 
model_pd.l_p.mean(): 0.1384609043598175 
model_pd.l_d.mean(): -19.69013214111328 
model_pd.lagr.mean(): -19.551671981811523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5528], device='cuda:0')), ('power', tensor([-20.6308], device='cuda:0'))])
epoch£º742	 i:0 	 global-step:14840	 l-p:0.1384609043598175
epoch£º742	 i:1 	 global-step:14841	 l-p:0.12035151571035385
epoch£º742	 i:2 	 global-step:14842	 l-p:0.11576109379529953
epoch£º742	 i:3 	 global-step:14843	 l-p:0.13425356149673462
epoch£º742	 i:4 	 global-step:14844	 l-p:0.24836380779743195
epoch£º742	 i:5 	 global-step:14845	 l-p:0.14699751138687134
epoch£º742	 i:6 	 global-step:14846	 l-p:0.218205526471138
epoch£º742	 i:7 	 global-step:14847	 l-p:0.12312272936105728
epoch£º742	 i:8 	 global-step:14848	 l-p:0.07315873354673386
epoch£º742	 i:9 	 global-step:14849	 l-p:0.1253756880760193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:743
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1086, 5.1085, 5.1086],
        [5.1086, 5.0433, 5.0861],
        [5.1086, 5.0645, 4.7849],
        [5.1086, 4.9294, 4.6683]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:743, step:0 
model_pd.l_p.mean(): 0.060150016099214554 
model_pd.l_d.mean(): -19.727066040039062 
model_pd.lagr.mean(): -19.666915893554688 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5216], device='cuda:0')), ('power', tensor([-20.6361], device='cuda:0'))])
epoch£º743	 i:0 	 global-step:14860	 l-p:0.060150016099214554
epoch£º743	 i:1 	 global-step:14861	 l-p:0.12918990850448608
epoch£º743	 i:2 	 global-step:14862	 l-p:0.13597041368484497
epoch£º743	 i:3 	 global-step:14863	 l-p:0.13547372817993164
epoch£º743	 i:4 	 global-step:14864	 l-p:0.12610331177711487
epoch£º743	 i:5 	 global-step:14865	 l-p:0.12593165040016174
epoch£º743	 i:6 	 global-step:14866	 l-p:0.16144686937332153
epoch£º743	 i:7 	 global-step:14867	 l-p:0.14043134450912476
epoch£º743	 i:8 	 global-step:14868	 l-p:0.12316017597913742
epoch£º743	 i:9 	 global-step:14869	 l-p:0.15715615451335907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:744
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1196, 5.0343, 5.0826],
        [5.1196, 4.9838, 5.0279],
        [5.1196, 5.0358, 5.0838],
        [5.1196, 5.0811, 5.1109]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:744, step:0 
model_pd.l_p.mean(): 0.13200686872005463 
model_pd.l_d.mean(): -20.7474365234375 
model_pd.lagr.mean(): -20.615428924560547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3888], device='cuda:0')), ('power', tensor([-21.5381], device='cuda:0'))])
epoch£º744	 i:0 	 global-step:14880	 l-p:0.13200686872005463
epoch£º744	 i:1 	 global-step:14881	 l-p:0.1298278570175171
epoch£º744	 i:2 	 global-step:14882	 l-p:0.1080789566040039
epoch£º744	 i:3 	 global-step:14883	 l-p:0.10899306833744049
epoch£º744	 i:4 	 global-step:14884	 l-p:0.20156389474868774
epoch£º744	 i:5 	 global-step:14885	 l-p:0.08963268995285034
epoch£º744	 i:6 	 global-step:14886	 l-p:0.14957012236118317
epoch£º744	 i:7 	 global-step:14887	 l-p:0.13276632130146027
epoch£º744	 i:8 	 global-step:14888	 l-p:0.9933305978775024
epoch£º744	 i:9 	 global-step:14889	 l-p:0.2568883001804352
====================================================================================================
====================================================================================================
====================================================================================================

epoch:745
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0352, 5.0015, 5.0284],
        [5.0352, 5.0352, 5.0352],
        [5.0352, 5.4376, 5.3876],
        [5.0352, 4.8748, 4.5931]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:745, step:0 
model_pd.l_p.mean(): 0.2412562072277069 
model_pd.l_d.mean(): -19.53651237487793 
model_pd.lagr.mean(): -19.295255661010742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5507], device='cuda:0')), ('power', tensor([-20.4722], device='cuda:0'))])
epoch£º745	 i:0 	 global-step:14900	 l-p:0.2412562072277069
epoch£º745	 i:1 	 global-step:14901	 l-p:0.13665986061096191
epoch£º745	 i:2 	 global-step:14902	 l-p:0.2529774308204651
epoch£º745	 i:3 	 global-step:14903	 l-p:0.1083647683262825
epoch£º745	 i:4 	 global-step:14904	 l-p:0.13335393369197845
epoch£º745	 i:5 	 global-step:14905	 l-p:0.12062256783246994
epoch£º745	 i:6 	 global-step:14906	 l-p:0.26974207162857056
epoch£º745	 i:7 	 global-step:14907	 l-p:0.1521272361278534
epoch£º745	 i:8 	 global-step:14908	 l-p:0.1794702559709549
epoch£º745	 i:9 	 global-step:14909	 l-p:0.11797665059566498
====================================================================================================
====================================================================================================
====================================================================================================

epoch:746
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0839, 5.0772, 5.0835],
        [5.0839, 5.0741, 5.0831],
        [5.0839, 5.1234, 4.8672],
        [5.0839, 5.0839, 5.0839]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:746, step:0 
model_pd.l_p.mean(): 0.14576755464076996 
model_pd.l_d.mean(): -19.108034133911133 
model_pd.lagr.mean(): -18.96226692199707 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5527], device='cuda:0')), ('power', tensor([-20.0378], device='cuda:0'))])
epoch£º746	 i:0 	 global-step:14920	 l-p:0.14576755464076996
epoch£º746	 i:1 	 global-step:14921	 l-p:0.20109225809574127
epoch£º746	 i:2 	 global-step:14922	 l-p:0.12732204794883728
epoch£º746	 i:3 	 global-step:14923	 l-p:0.13419248163700104
epoch£º746	 i:4 	 global-step:14924	 l-p:0.12726429104804993
epoch£º746	 i:5 	 global-step:14925	 l-p:0.12833993136882782
epoch£º746	 i:6 	 global-step:14926	 l-p:0.1369396299123764
epoch£º746	 i:7 	 global-step:14927	 l-p:0.10811050236225128
epoch£º746	 i:8 	 global-step:14928	 l-p:0.11069934815168381
epoch£º746	 i:9 	 global-step:14929	 l-p:0.12139277905225754
====================================================================================================
====================================================================================================
====================================================================================================

epoch:747
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1372, 5.0835, 5.1215],
        [5.1372, 5.0987, 5.1285],
        [5.1372, 5.1397, 4.8712],
        [5.1372, 5.1604, 4.8986]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:747, step:0 
model_pd.l_p.mean(): 0.18258696794509888 
model_pd.l_d.mean(): -19.778837203979492 
model_pd.lagr.mean(): -19.596250534057617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5010], device='cuda:0')), ('power', tensor([-20.6676], device='cuda:0'))])
epoch£º747	 i:0 	 global-step:14940	 l-p:0.18258696794509888
epoch£º747	 i:1 	 global-step:14941	 l-p:0.16445955634117126
epoch£º747	 i:2 	 global-step:14942	 l-p:0.13645856082439423
epoch£º747	 i:3 	 global-step:14943	 l-p:0.10532119870185852
epoch£º747	 i:4 	 global-step:14944	 l-p:0.1328246146440506
epoch£º747	 i:5 	 global-step:14945	 l-p:0.10124145448207855
epoch£º747	 i:6 	 global-step:14946	 l-p:0.1541358083486557
epoch£º747	 i:7 	 global-step:14947	 l-p:0.08498600125312805
epoch£º747	 i:8 	 global-step:14948	 l-p:0.15554311871528625
epoch£º747	 i:9 	 global-step:14949	 l-p:0.14350533485412598
====================================================================================================
====================================================================================================
====================================================================================================

epoch:748
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0772, 4.8814, 4.8792],
        [5.0772, 5.4678, 5.4080],
        [5.0772, 4.9656, 5.0163],
        [5.0772, 4.8665, 4.8396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:748, step:0 
model_pd.l_p.mean(): 0.16555532813072205 
model_pd.l_d.mean(): -20.573686599731445 
model_pd.lagr.mean(): -20.408130645751953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4403], device='cuda:0')), ('power', tensor([-21.4144], device='cuda:0'))])
epoch£º748	 i:0 	 global-step:14960	 l-p:0.16555532813072205
epoch£º748	 i:1 	 global-step:14961	 l-p:0.15250279009342194
epoch£º748	 i:2 	 global-step:14962	 l-p:0.1804458349943161
epoch£º748	 i:3 	 global-step:14963	 l-p:0.13061195611953735
epoch£º748	 i:4 	 global-step:14964	 l-p:0.09142518043518066
epoch£º748	 i:5 	 global-step:14965	 l-p:0.2433183491230011
epoch£º748	 i:6 	 global-step:14966	 l-p:0.10157974064350128
epoch£º748	 i:7 	 global-step:14967	 l-p:0.1645982265472412
epoch£º748	 i:8 	 global-step:14968	 l-p:0.12480722367763519
epoch£º748	 i:9 	 global-step:14969	 l-p:0.12302321940660477
====================================================================================================
====================================================================================================
====================================================================================================

epoch:749
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0909, 5.4461, 5.3620],
        [5.0909, 5.0676, 5.0873],
        [5.0909, 5.0635, 5.0862],
        [5.0909, 4.8640, 4.6591]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:749, step:0 
model_pd.l_p.mean(): 0.12994618713855743 
model_pd.l_d.mean(): -19.930030822753906 
model_pd.lagr.mean(): -19.800085067749023 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5231], device='cuda:0')), ('power', tensor([-20.8445], device='cuda:0'))])
epoch£º749	 i:0 	 global-step:14980	 l-p:0.12994618713855743
epoch£º749	 i:1 	 global-step:14981	 l-p:0.12908460199832916
epoch£º749	 i:2 	 global-step:14982	 l-p:0.12980042397975922
epoch£º749	 i:3 	 global-step:14983	 l-p:0.13324418663978577
epoch£º749	 i:4 	 global-step:14984	 l-p:0.10400693863630295
epoch£º749	 i:5 	 global-step:14985	 l-p:0.14392390847206116
epoch£º749	 i:6 	 global-step:14986	 l-p:0.22916775941848755
epoch£º749	 i:7 	 global-step:14987	 l-p:0.18319854140281677
epoch£º749	 i:8 	 global-step:14988	 l-p:0.1413245052099228
epoch£º749	 i:9 	 global-step:14989	 l-p:0.12076608091592789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:750
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1026, 5.0069, 4.7194],
        [5.1026, 5.4191, 5.3096],
        [5.1026, 5.0982, 5.1024],
        [5.1026, 5.0971, 5.1023]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:750, step:0 
model_pd.l_p.mean(): 0.11872085928916931 
model_pd.l_d.mean(): -20.245563507080078 
model_pd.lagr.mean(): -20.126842498779297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4715], device='cuda:0')), ('power', tensor([-21.1125], device='cuda:0'))])
epoch£º750	 i:0 	 global-step:15000	 l-p:0.11872085928916931
epoch£º750	 i:1 	 global-step:15001	 l-p:0.1857222318649292
epoch£º750	 i:2 	 global-step:15002	 l-p:0.18696250021457672
epoch£º750	 i:3 	 global-step:15003	 l-p:0.1681564301252365
epoch£º750	 i:4 	 global-step:15004	 l-p:0.029109403491020203
epoch£º750	 i:5 	 global-step:15005	 l-p:0.13461963832378387
epoch£º750	 i:6 	 global-step:15006	 l-p:0.12352780997753143
epoch£º750	 i:7 	 global-step:15007	 l-p:0.12120754271745682
epoch£º750	 i:8 	 global-step:15008	 l-p:0.1212659627199173
epoch£º750	 i:9 	 global-step:15009	 l-p:0.13139216601848602
====================================================================================================
====================================================================================================
====================================================================================================

epoch:751
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8453e-01, 1.0505e-01,
         1.0000e+00, 5.9809e-02, 1.0000e+00, 5.6932e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1373, 5.0238, 5.0736],
        [5.1373, 5.0709, 4.7869],
        [5.1373, 5.1373, 5.1373],
        [5.1373, 4.9651, 4.9866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:751, step:0 
model_pd.l_p.mean(): 0.13937106728553772 
model_pd.l_d.mean(): -20.505680084228516 
model_pd.lagr.mean(): -20.366308212280273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4508], device='cuda:0')), ('power', tensor([-21.3560], device='cuda:0'))])
epoch£º751	 i:0 	 global-step:15020	 l-p:0.13937106728553772
epoch£º751	 i:1 	 global-step:15021	 l-p:0.12595891952514648
epoch£º751	 i:2 	 global-step:15022	 l-p:0.14941319823265076
epoch£º751	 i:3 	 global-step:15023	 l-p:0.16648659110069275
epoch£º751	 i:4 	 global-step:15024	 l-p:0.07250190526247025
epoch£º751	 i:5 	 global-step:15025	 l-p:0.1168706864118576
epoch£º751	 i:6 	 global-step:15026	 l-p:0.14447592198848724
epoch£º751	 i:7 	 global-step:15027	 l-p:0.13112987577915192
epoch£º751	 i:8 	 global-step:15028	 l-p:0.1503487527370453
epoch£º751	 i:9 	 global-step:15029	 l-p:0.0991901084780693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:752
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1011, 5.1011, 5.1011],
        [5.1011, 5.0598, 5.0914],
        [5.1011, 4.9719, 5.0196],
        [5.1011, 5.0967, 5.1009]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:752, step:0 
model_pd.l_p.mean(): 0.11272791028022766 
model_pd.l_d.mean(): -19.309778213500977 
model_pd.lagr.mean(): -19.197050094604492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5446], device='cuda:0')), ('power', tensor([-20.2349], device='cuda:0'))])
epoch£º752	 i:0 	 global-step:15040	 l-p:0.11272791028022766
epoch£º752	 i:1 	 global-step:15041	 l-p:0.12767153978347778
epoch£º752	 i:2 	 global-step:15042	 l-p:0.09536775201559067
epoch£º752	 i:3 	 global-step:15043	 l-p:0.1385413557291031
epoch£º752	 i:4 	 global-step:15044	 l-p:0.18380235135555267
epoch£º752	 i:5 	 global-step:15045	 l-p:0.18890979886054993
epoch£º752	 i:6 	 global-step:15046	 l-p:0.13393007218837738
epoch£º752	 i:7 	 global-step:15047	 l-p:0.13839110732078552
epoch£º752	 i:8 	 global-step:15048	 l-p:0.1300269067287445
epoch£º752	 i:9 	 global-step:15049	 l-p:0.122858427464962
====================================================================================================
====================================================================================================
====================================================================================================

epoch:753
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1180, 5.1160, 5.1179],
        [5.1180, 5.1179, 5.1180],
        [5.1180, 4.9832, 5.0290],
        [5.1180, 4.8973, 4.8435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:753, step:0 
model_pd.l_p.mean(): 0.13637994229793549 
model_pd.l_d.mean(): -20.430578231811523 
model_pd.lagr.mean(): -20.294198989868164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4632], device='cuda:0')), ('power', tensor([-21.2923], device='cuda:0'))])
epoch£º753	 i:0 	 global-step:15060	 l-p:0.13637994229793549
epoch£º753	 i:1 	 global-step:15061	 l-p:0.14364442229270935
epoch£º753	 i:2 	 global-step:15062	 l-p:0.1344628930091858
epoch£º753	 i:3 	 global-step:15063	 l-p:0.17092733085155487
epoch£º753	 i:4 	 global-step:15064	 l-p:0.16031882166862488
epoch£º753	 i:5 	 global-step:15065	 l-p:0.07199271023273468
epoch£º753	 i:6 	 global-step:15066	 l-p:0.20101825892925262
epoch£º753	 i:7 	 global-step:15067	 l-p:0.15181133151054382
epoch£º753	 i:8 	 global-step:15068	 l-p:0.13673032820224762
epoch£º753	 i:9 	 global-step:15069	 l-p:0.12623052299022675
====================================================================================================
====================================================================================================
====================================================================================================

epoch:754
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1082, 5.1027, 5.1079],
        [5.1082, 5.1041, 5.1080],
        [5.1082, 5.2018, 4.9667],
        [5.1082, 5.1081, 5.1082]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:754, step:0 
model_pd.l_p.mean(): 0.11649482697248459 
model_pd.l_d.mean(): -20.347972869873047 
model_pd.lagr.mean(): -20.231477737426758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4518], device='cuda:0')), ('power', tensor([-21.1964], device='cuda:0'))])
epoch£º754	 i:0 	 global-step:15080	 l-p:0.11649482697248459
epoch£º754	 i:1 	 global-step:15081	 l-p:0.1001068726181984
epoch£º754	 i:2 	 global-step:15082	 l-p:0.1342235654592514
epoch£º754	 i:3 	 global-step:15083	 l-p:0.17469432950019836
epoch£º754	 i:4 	 global-step:15084	 l-p:0.17899055778980255
epoch£º754	 i:5 	 global-step:15085	 l-p:0.1216670498251915
epoch£º754	 i:6 	 global-step:15086	 l-p:0.126455619931221
epoch£º754	 i:7 	 global-step:15087	 l-p:0.14415031671524048
epoch£º754	 i:8 	 global-step:15088	 l-p:0.13855677843093872
epoch£º754	 i:9 	 global-step:15089	 l-p:0.10894452780485153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:755
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1103, 4.9454, 4.9757],
        [5.1103, 5.0679, 5.1002],
        [5.1103, 5.0072, 4.7181],
        [5.1103, 4.8809, 4.8031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:755, step:0 
model_pd.l_p.mean(): 0.10536989569664001 
model_pd.l_d.mean(): -19.950725555419922 
model_pd.lagr.mean(): -19.845355987548828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4930], device='cuda:0')), ('power', tensor([-20.8344], device='cuda:0'))])
epoch£º755	 i:0 	 global-step:15100	 l-p:0.10536989569664001
epoch£º755	 i:1 	 global-step:15101	 l-p:0.1287175416946411
epoch£º755	 i:2 	 global-step:15102	 l-p:0.14850187301635742
epoch£º755	 i:3 	 global-step:15103	 l-p:0.15235647559165955
epoch£º755	 i:4 	 global-step:15104	 l-p:0.1457778364419937
epoch£º755	 i:5 	 global-step:15105	 l-p:0.23679980635643005
epoch£º755	 i:6 	 global-step:15106	 l-p:0.11673679947853088
epoch£º755	 i:7 	 global-step:15107	 l-p:0.1373804658651352
epoch£º755	 i:8 	 global-step:15108	 l-p:0.2908021807670593
epoch£º755	 i:9 	 global-step:15109	 l-p:0.22056323289871216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:756
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0349, 4.8901, 4.5986],
        [5.0349, 5.0349, 5.0349],
        [5.0349, 4.7919, 4.6065],
        [5.0349, 4.7959, 4.7070]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:756, step:0 
model_pd.l_p.mean(): 0.11997558921575546 
model_pd.l_d.mean(): -20.48199462890625 
model_pd.lagr.mean(): -20.362018585205078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4569], device='cuda:0')), ('power', tensor([-21.3382], device='cuda:0'))])
epoch£º756	 i:0 	 global-step:15120	 l-p:0.11997558921575546
epoch£º756	 i:1 	 global-step:15121	 l-p:0.3648185729980469
epoch£º756	 i:2 	 global-step:15122	 l-p:-0.04644806683063507
epoch£º756	 i:3 	 global-step:15123	 l-p:0.16701608896255493
epoch£º756	 i:4 	 global-step:15124	 l-p:0.22150973975658417
epoch£º756	 i:5 	 global-step:15125	 l-p:0.16231948137283325
epoch£º756	 i:6 	 global-step:15126	 l-p:0.10839258134365082
epoch£º756	 i:7 	 global-step:15127	 l-p:0.13872534036636353
epoch£º756	 i:8 	 global-step:15128	 l-p:0.07588127255439758
epoch£º756	 i:9 	 global-step:15129	 l-p:0.15847520530223846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:757
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0752, 4.8700, 4.6155],
        [5.0752, 4.9639, 4.6719],
        [5.0752, 4.9384, 4.9852],
        [5.0752, 5.0752, 5.0752]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:757, step:0 
model_pd.l_p.mean(): 0.1465628445148468 
model_pd.l_d.mean(): -18.583099365234375 
model_pd.lagr.mean(): -18.43653678894043 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5635], device='cuda:0')), ('power', tensor([-19.5143], device='cuda:0'))])
epoch£º757	 i:0 	 global-step:15140	 l-p:0.1465628445148468
epoch£º757	 i:1 	 global-step:15141	 l-p:0.15806828439235687
epoch£º757	 i:2 	 global-step:15142	 l-p:0.15174689888954163
epoch£º757	 i:3 	 global-step:15143	 l-p:0.12107487767934799
epoch£º757	 i:4 	 global-step:15144	 l-p:0.16961900889873505
epoch£º757	 i:5 	 global-step:15145	 l-p:0.13243861496448517
epoch£º757	 i:6 	 global-step:15146	 l-p:0.12288393080234528
epoch£º757	 i:7 	 global-step:15147	 l-p:0.08839008957147598
epoch£º757	 i:8 	 global-step:15148	 l-p:0.12496723234653473
epoch£º757	 i:9 	 global-step:15149	 l-p:0.17780299484729767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:758
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1326, 4.9412, 4.9440],
        [5.1326, 4.9680, 4.9981],
        [5.1326, 5.0606, 5.1061],
        [5.1326, 4.8993, 4.7975]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:758, step:0 
model_pd.l_p.mean(): 0.07780031859874725 
model_pd.l_d.mean(): -20.605472564697266 
model_pd.lagr.mean(): -20.527671813964844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4155], device='cuda:0')), ('power', tensor([-21.4211], device='cuda:0'))])
epoch£º758	 i:0 	 global-step:15160	 l-p:0.07780031859874725
epoch£º758	 i:1 	 global-step:15161	 l-p:0.13624759018421173
epoch£º758	 i:2 	 global-step:15162	 l-p:0.15856626629829407
epoch£º758	 i:3 	 global-step:15163	 l-p:0.10829944163560867
epoch£º758	 i:4 	 global-step:15164	 l-p:0.17649978399276733
epoch£º758	 i:5 	 global-step:15165	 l-p:0.11043771356344223
epoch£º758	 i:6 	 global-step:15166	 l-p:0.11245924979448318
epoch£º758	 i:7 	 global-step:15167	 l-p:0.09176645427942276
epoch£º758	 i:8 	 global-step:15168	 l-p:0.07440676540136337
epoch£º758	 i:9 	 global-step:15169	 l-p:0.13475611805915833
====================================================================================================
====================================================================================================
====================================================================================================

epoch:759
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1766, 5.1766, 5.1766],
        [5.1766, 5.1766, 5.1766],
        [5.1766, 5.0689, 4.7834],
        [5.1766, 5.1766, 5.1766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:759, step:0 
model_pd.l_p.mean(): 0.06178763136267662 
model_pd.l_d.mean(): -19.6906681060791 
model_pd.lagr.mean(): -19.62887954711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4547], device='cuda:0')), ('power', tensor([-20.5298], device='cuda:0'))])
epoch£º759	 i:0 	 global-step:15180	 l-p:0.06178763136267662
epoch£º759	 i:1 	 global-step:15181	 l-p:0.15134988725185394
epoch£º759	 i:2 	 global-step:15182	 l-p:0.1271190643310547
epoch£º759	 i:3 	 global-step:15183	 l-p:0.1456991285085678
epoch£º759	 i:4 	 global-step:15184	 l-p:0.11947153508663177
epoch£º759	 i:5 	 global-step:15185	 l-p:0.15409530699253082
epoch£º759	 i:6 	 global-step:15186	 l-p:0.08010146766901016
epoch£º759	 i:7 	 global-step:15187	 l-p:0.12807288765907288
epoch£º759	 i:8 	 global-step:15188	 l-p:-0.0642123594880104
epoch£º759	 i:9 	 global-step:15189	 l-p:0.1151977926492691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:760
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1744, 5.1415, 5.1678],
        [5.1744, 4.9848, 4.9875],
        [5.1744, 4.9450, 4.7654],
        [5.1744, 5.1045, 5.1491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:760, step:0 
model_pd.l_p.mean(): 0.0077039143070578575 
model_pd.l_d.mean(): -19.120040893554688 
model_pd.lagr.mean(): -19.112337112426758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5446], device='cuda:0')), ('power', tensor([-20.0416], device='cuda:0'))])
epoch£º760	 i:0 	 global-step:15200	 l-p:0.0077039143070578575
epoch£º760	 i:1 	 global-step:15201	 l-p:0.12433625012636185
epoch£º760	 i:2 	 global-step:15202	 l-p:0.06298526376485825
epoch£º760	 i:3 	 global-step:15203	 l-p:0.14573784172534943
epoch£º760	 i:4 	 global-step:15204	 l-p:0.1337449997663498
epoch£º760	 i:5 	 global-step:15205	 l-p:0.12202996015548706
epoch£º760	 i:6 	 global-step:15206	 l-p:0.08968605846166611
epoch£º760	 i:7 	 global-step:15207	 l-p:0.1389438807964325
epoch£º760	 i:8 	 global-step:15208	 l-p:0.14373373985290527
epoch£º760	 i:9 	 global-step:15209	 l-p:0.1180262342095375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:761
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1301, 5.1133, 5.1280],
        [5.1301, 4.9324, 4.6767],
        [5.1301, 4.9171, 4.8864],
        [5.1301, 5.0802, 4.7946]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:761, step:0 
model_pd.l_p.mean(): 0.13431452214717865 
model_pd.l_d.mean(): -18.28759002685547 
model_pd.lagr.mean(): -18.153276443481445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6065], device='cuda:0')), ('power', tensor([-19.2577], device='cuda:0'))])
epoch£º761	 i:0 	 global-step:15220	 l-p:0.13431452214717865
epoch£º761	 i:1 	 global-step:15221	 l-p:0.09067799150943756
epoch£º761	 i:2 	 global-step:15222	 l-p:0.12495988607406616
epoch£º761	 i:3 	 global-step:15223	 l-p:0.13759532570838928
epoch£º761	 i:4 	 global-step:15224	 l-p:0.13379617035388947
epoch£º761	 i:5 	 global-step:15225	 l-p:0.5230668783187866
epoch£º761	 i:6 	 global-step:15226	 l-p:0.185546875
epoch£º761	 i:7 	 global-step:15227	 l-p:0.1529441624879837
epoch£º761	 i:8 	 global-step:15228	 l-p:0.11013959348201752
epoch£º761	 i:9 	 global-step:15229	 l-p:-1.7271978855133057
====================================================================================================
====================================================================================================
====================================================================================================

epoch:762
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0138, 4.8070, 4.8019],
        [5.0138, 4.7945, 4.7690],
        [5.0138, 4.7763, 4.5508],
        [5.0138, 5.0138, 5.0138]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:762, step:0 
model_pd.l_p.mean(): 0.13616755604743958 
model_pd.l_d.mean(): -20.561214447021484 
model_pd.lagr.mean(): -20.425046920776367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4684], device='cuda:0')), ('power', tensor([-21.4309], device='cuda:0'))])
epoch£º762	 i:0 	 global-step:15240	 l-p:0.13616755604743958
epoch£º762	 i:1 	 global-step:15241	 l-p:0.3591339886188507
epoch£º762	 i:2 	 global-step:15242	 l-p:0.13414201140403748
epoch£º762	 i:3 	 global-step:15243	 l-p:0.13210253417491913
epoch£º762	 i:4 	 global-step:15244	 l-p:0.134601891040802
epoch£º762	 i:5 	 global-step:15245	 l-p:0.1308995932340622
epoch£º762	 i:6 	 global-step:15246	 l-p:0.33213087916374207
epoch£º762	 i:7 	 global-step:15247	 l-p:-0.004215283319354057
epoch£º762	 i:8 	 global-step:15248	 l-p:0.4121290445327759
epoch£º762	 i:9 	 global-step:15249	 l-p:0.06900279223918915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:763
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9933, 4.7404, 4.6047],
        [4.9933, 4.9878, 4.9930],
        [4.9933, 4.9507, 4.9833],
        [4.9933, 5.1620, 4.9652]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:763, step:0 
model_pd.l_p.mean(): 0.15616610646247864 
model_pd.l_d.mean(): -19.31421661376953 
model_pd.lagr.mean(): -19.158050537109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5496], device='cuda:0')), ('power', tensor([-20.2446], device='cuda:0'))])
epoch£º763	 i:0 	 global-step:15260	 l-p:0.15616610646247864
epoch£º763	 i:1 	 global-step:15261	 l-p:-0.03274386376142502
epoch£º763	 i:2 	 global-step:15262	 l-p:0.10960988700389862
epoch£º763	 i:3 	 global-step:15263	 l-p:0.125474214553833
epoch£º763	 i:4 	 global-step:15264	 l-p:0.1484542042016983
epoch£º763	 i:5 	 global-step:15265	 l-p:0.12945494055747986
epoch£º763	 i:6 	 global-step:15266	 l-p:4.519635200500488
epoch£º763	 i:7 	 global-step:15267	 l-p:0.16356033086776733
epoch£º763	 i:8 	 global-step:15268	 l-p:0.14740435779094696
epoch£º763	 i:9 	 global-step:15269	 l-p:0.14712873101234436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:764
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0810, 5.0035, 5.0511],
        [5.0810, 5.0772, 5.0808],
        [5.0810, 4.8538, 4.6253],
        [5.0810, 4.8641, 4.8345]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:764, step:0 
model_pd.l_p.mean(): 0.1798543483018875 
model_pd.l_d.mean(): -20.641027450561523 
model_pd.lagr.mean(): -20.46117401123047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4420], device='cuda:0')), ('power', tensor([-21.4847], device='cuda:0'))])
epoch£º764	 i:0 	 global-step:15280	 l-p:0.1798543483018875
epoch£º764	 i:1 	 global-step:15281	 l-p:0.10200908780097961
epoch£º764	 i:2 	 global-step:15282	 l-p:0.14754202961921692
epoch£º764	 i:3 	 global-step:15283	 l-p:0.1753227412700653
epoch£º764	 i:4 	 global-step:15284	 l-p:0.12874911725521088
epoch£º764	 i:5 	 global-step:15285	 l-p:0.12839137017726898
epoch£º764	 i:6 	 global-step:15286	 l-p:0.13133473694324493
epoch£º764	 i:7 	 global-step:15287	 l-p:0.17352910339832306
epoch£º764	 i:8 	 global-step:15288	 l-p:0.06297542154788971
epoch£º764	 i:9 	 global-step:15289	 l-p:0.07849914580583572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:765
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1558, 5.0501, 5.1011],
        [5.1558, 5.1498, 5.1554],
        [5.1558, 5.1556, 5.1558],
        [5.1558, 5.0360, 5.0860]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:765, step:0 
model_pd.l_p.mean(): 0.1174701675772667 
model_pd.l_d.mean(): -18.43219566345215 
model_pd.lagr.mean(): -18.314725875854492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5713], device='cuda:0')), ('power', tensor([-19.3686], device='cuda:0'))])
epoch£º765	 i:0 	 global-step:15300	 l-p:0.1174701675772667
epoch£º765	 i:1 	 global-step:15301	 l-p:0.1759922206401825
epoch£º765	 i:2 	 global-step:15302	 l-p:0.0898272842168808
epoch£º765	 i:3 	 global-step:15303	 l-p:0.1191893145442009
epoch£º765	 i:4 	 global-step:15304	 l-p:0.15921859443187714
epoch£º765	 i:5 	 global-step:15305	 l-p:0.13078898191452026
epoch£º765	 i:6 	 global-step:15306	 l-p:0.42347148060798645
epoch£º765	 i:7 	 global-step:15307	 l-p:0.12881417572498322
epoch£º765	 i:8 	 global-step:15308	 l-p:0.07461939007043839
epoch£º765	 i:9 	 global-step:15309	 l-p:0.08728471398353577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:766
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2271, 5.5963, 5.5138],
        [5.2271, 4.9969, 4.8700],
        [5.2271, 5.1796, 5.2145],
        [5.2271, 5.3498, 5.1250]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:766, step:0 
model_pd.l_p.mean(): 0.12277521938085556 
model_pd.l_d.mean(): -20.353126525878906 
model_pd.lagr.mean(): -20.230350494384766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4283], device='cuda:0')), ('power', tensor([-21.1773], device='cuda:0'))])
epoch£º766	 i:0 	 global-step:15320	 l-p:0.12277521938085556
epoch£º766	 i:1 	 global-step:15321	 l-p:0.12438508868217468
epoch£º766	 i:2 	 global-step:15322	 l-p:0.14118029177188873
epoch£º766	 i:3 	 global-step:15323	 l-p:0.191736102104187
epoch£º766	 i:4 	 global-step:15324	 l-p:0.13652794063091278
epoch£º766	 i:5 	 global-step:15325	 l-p:0.12435107678174973
epoch£º766	 i:6 	 global-step:15326	 l-p:0.09589145332574844
epoch£º766	 i:7 	 global-step:15327	 l-p:0.1179903969168663
epoch£º766	 i:8 	 global-step:15328	 l-p:0.07720696926116943
epoch£º766	 i:9 	 global-step:15329	 l-p:0.128505140542984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:767
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2179, 5.1549, 5.1970],
        [5.2179, 5.2178, 5.2179],
        [5.2179, 5.2179, 5.2179],
        [5.2179, 5.4794, 5.3296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:767, step:0 
model_pd.l_p.mean(): 0.1168193444609642 
model_pd.l_d.mean(): -20.622623443603516 
model_pd.lagr.mean(): -20.50580406188965 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3886], device='cuda:0')), ('power', tensor([-21.4107], device='cuda:0'))])
epoch£º767	 i:0 	 global-step:15340	 l-p:0.1168193444609642
epoch£º767	 i:1 	 global-step:15341	 l-p:0.06598915159702301
epoch£º767	 i:2 	 global-step:15342	 l-p:0.11372926086187363
epoch£º767	 i:3 	 global-step:15343	 l-p:0.06074605509638786
epoch£º767	 i:4 	 global-step:15344	 l-p:0.13108067214488983
epoch£º767	 i:5 	 global-step:15345	 l-p:0.13561339676380157
epoch£º767	 i:6 	 global-step:15346	 l-p:0.17161914706230164
epoch£º767	 i:7 	 global-step:15347	 l-p:0.1500331610441208
epoch£º767	 i:8 	 global-step:15348	 l-p:0.11099442094564438
epoch£º767	 i:9 	 global-step:15349	 l-p:-0.035107020288705826
====================================================================================================
====================================================================================================
====================================================================================================

epoch:768
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1810, 4.9451, 4.8109],
        [5.1810, 5.6864, 5.6988],
        [5.1810, 5.1330, 5.1683],
        [5.1810, 5.1133, 4.8255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:768, step:0 
model_pd.l_p.mean(): 0.1505332589149475 
model_pd.l_d.mean(): -19.33440589904785 
model_pd.lagr.mean(): -19.18387222290039 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4882], device='cuda:0')), ('power', tensor([-20.2016], device='cuda:0'))])
epoch£º768	 i:0 	 global-step:15360	 l-p:0.1505332589149475
epoch£º768	 i:1 	 global-step:15361	 l-p:0.12333890795707703
epoch£º768	 i:2 	 global-step:15362	 l-p:0.12740623950958252
epoch£º768	 i:3 	 global-step:15363	 l-p:0.09055104851722717
epoch£º768	 i:4 	 global-step:15364	 l-p:0.1082574799656868
epoch£º768	 i:5 	 global-step:15365	 l-p:-0.14976583421230316
epoch£º768	 i:6 	 global-step:15366	 l-p:0.1319393366575241
epoch£º768	 i:7 	 global-step:15367	 l-p:0.17033599317073822
epoch£º768	 i:8 	 global-step:15368	 l-p:0.13443511724472046
epoch£º768	 i:9 	 global-step:15369	 l-p:0.11033780872821808
====================================================================================================
====================================================================================================
====================================================================================================

epoch:769
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1550, 4.9220, 4.7277],
        [5.1550, 5.1292, 4.8472],
        [5.1550, 5.1208, 4.8368],
        [5.1550, 5.1409, 5.1535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:769, step:0 
model_pd.l_p.mean(): 0.12540428340435028 
model_pd.l_d.mean(): -20.095380783081055 
model_pd.lagr.mean(): -19.9699764251709 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4709], device='cuda:0')), ('power', tensor([-20.9588], device='cuda:0'))])
epoch£º769	 i:0 	 global-step:15380	 l-p:0.12540428340435028
epoch£º769	 i:1 	 global-step:15381	 l-p:0.15454670786857605
epoch£º769	 i:2 	 global-step:15382	 l-p:0.09808705002069473
epoch£º769	 i:3 	 global-step:15383	 l-p:0.228768453001976
epoch£º769	 i:4 	 global-step:15384	 l-p:0.11320522427558899
epoch£º769	 i:5 	 global-step:15385	 l-p:0.12653793394565582
epoch£º769	 i:6 	 global-step:15386	 l-p:0.16591307520866394
epoch£º769	 i:7 	 global-step:15387	 l-p:0.1260695457458496
epoch£º769	 i:8 	 global-step:15388	 l-p:0.19806984066963196
epoch£º769	 i:9 	 global-step:15389	 l-p:0.13571105897426605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:770
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0425, 5.0424, 5.0425],
        [5.0425, 4.9668, 4.6702],
        [5.0425, 4.8014, 4.5834],
        [5.0425, 4.9242, 4.9768]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:770, step:0 
model_pd.l_p.mean(): -0.45638641715049744 
model_pd.l_d.mean(): -18.197101593017578 
model_pd.lagr.mean(): -18.653488159179688 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6307], device='cuda:0')), ('power', tensor([-19.1906], device='cuda:0'))])
epoch£º770	 i:0 	 global-step:15400	 l-p:-0.45638641715049744
epoch£º770	 i:1 	 global-step:15401	 l-p:0.15604372322559357
epoch£º770	 i:2 	 global-step:15402	 l-p:0.12330484390258789
epoch£º770	 i:3 	 global-step:15403	 l-p:4.923439025878906
epoch£º770	 i:4 	 global-step:15404	 l-p:0.21349379420280457
epoch£º770	 i:5 	 global-step:15405	 l-p:0.13166628777980804
epoch£º770	 i:6 	 global-step:15406	 l-p:0.1406215876340866
epoch£º770	 i:7 	 global-step:15407	 l-p:0.1575799286365509
epoch£º770	 i:8 	 global-step:15408	 l-p:0.10993286222219467
epoch£º770	 i:9 	 global-step:15409	 l-p:-0.47014299035072327
====================================================================================================
====================================================================================================
====================================================================================================

epoch:771
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[4.9972, 4.9972, 4.9972],
        [4.9972, 4.9532, 4.9867],
        [4.9972, 4.9925, 4.9970],
        [4.9972, 4.7415, 4.5620]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:771, step:0 
model_pd.l_p.mean(): 0.12931472063064575 
model_pd.l_d.mean(): -20.653255462646484 
model_pd.lagr.mean(): -20.523941040039062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4398], device='cuda:0')), ('power', tensor([-21.4950], device='cuda:0'))])
epoch£º771	 i:0 	 global-step:15420	 l-p:0.12931472063064575
epoch£º771	 i:1 	 global-step:15421	 l-p:0.11766465753316879
epoch£º771	 i:2 	 global-step:15422	 l-p:0.018029436469078064
epoch£º771	 i:3 	 global-step:15423	 l-p:0.1405775249004364
epoch£º771	 i:4 	 global-step:15424	 l-p:-0.19743354618549347
epoch£º771	 i:5 	 global-step:15425	 l-p:0.16863679885864258
epoch£º771	 i:6 	 global-step:15426	 l-p:0.14750351011753082
epoch£º771	 i:7 	 global-step:15427	 l-p:0.19722682237625122
epoch£º771	 i:8 	 global-step:15428	 l-p:0.45858675241470337
epoch£º771	 i:9 	 global-step:15429	 l-p:0.23623709380626678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:772
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0106, 4.9680, 5.0006],
        [5.0106, 4.9356, 4.9831],
        [5.0106, 5.2791, 5.1380],
        [5.0106, 4.9665, 5.0000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:772, step:0 
model_pd.l_p.mean(): 0.09372532367706299 
model_pd.l_d.mean(): -20.663944244384766 
model_pd.lagr.mean(): -20.570219039916992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4524], device='cuda:0')), ('power', tensor([-21.5189], device='cuda:0'))])
epoch£º772	 i:0 	 global-step:15440	 l-p:0.09372532367706299
epoch£º772	 i:1 	 global-step:15441	 l-p:0.15470241010189056
epoch£º772	 i:2 	 global-step:15442	 l-p:0.10196246951818466
epoch£º772	 i:3 	 global-step:15443	 l-p:0.1464480608701706
epoch£º772	 i:4 	 global-step:15444	 l-p:0.2127954065799713
epoch£º772	 i:5 	 global-step:15445	 l-p:0.11647011339664459
epoch£º772	 i:6 	 global-step:15446	 l-p:0.1159021332859993
epoch£º772	 i:7 	 global-step:15447	 l-p:0.18247267603874207
epoch£º772	 i:8 	 global-step:15448	 l-p:0.07969611883163452
epoch£º772	 i:9 	 global-step:15449	 l-p:0.1671813428401947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:773
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1300, 5.1300, 5.1300],
        [5.1300, 5.0921, 5.1218],
        [5.1300, 4.8954, 4.8168],
        [5.1300, 4.8983, 4.8298]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:773, step:0 
model_pd.l_p.mean(): 0.13517840206623077 
model_pd.l_d.mean(): -20.852590560913086 
model_pd.lagr.mean(): -20.7174129486084 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3836], device='cuda:0')), ('power', tensor([-21.6399], device='cuda:0'))])
epoch£º773	 i:0 	 global-step:15460	 l-p:0.13517840206623077
epoch£º773	 i:1 	 global-step:15461	 l-p:0.13240300118923187
epoch£º773	 i:2 	 global-step:15462	 l-p:0.09324207156896591
epoch£º773	 i:3 	 global-step:15463	 l-p:0.07821987569332123
epoch£º773	 i:4 	 global-step:15464	 l-p:0.14147008955478668
epoch£º773	 i:5 	 global-step:15465	 l-p:0.15995672345161438
epoch£º773	 i:6 	 global-step:15466	 l-p:-10.534785270690918
epoch£º773	 i:7 	 global-step:15467	 l-p:0.1320074498653412
epoch£º773	 i:8 	 global-step:15468	 l-p:0.11655072867870331
epoch£º773	 i:9 	 global-step:15469	 l-p:0.1082746684551239
====================================================================================================
====================================================================================================
====================================================================================================

epoch:774
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2349, 5.1364, 5.1868],
        [5.2349, 5.1948, 5.2257],
        [5.2349, 5.0019, 4.8745],
        [5.2349, 5.2156, 5.2323]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:774, step:0 
model_pd.l_p.mean(): 0.07965753227472305 
model_pd.l_d.mean(): -20.55448341369629 
model_pd.lagr.mean(): -20.47482681274414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4032], device='cuda:0')), ('power', tensor([-21.3565], device='cuda:0'))])
epoch£º774	 i:0 	 global-step:15480	 l-p:0.07965753227472305
epoch£º774	 i:1 	 global-step:15481	 l-p:0.11644818633794785
epoch£º774	 i:2 	 global-step:15482	 l-p:0.12723049521446228
epoch£º774	 i:3 	 global-step:15483	 l-p:0.196772500872612
epoch£º774	 i:4 	 global-step:15484	 l-p:0.1350545436143875
epoch£º774	 i:5 	 global-step:15485	 l-p:0.06390251964330673
epoch£º774	 i:6 	 global-step:15486	 l-p:0.13586142659187317
epoch£º774	 i:7 	 global-step:15487	 l-p:0.31979039311408997
epoch£º774	 i:8 	 global-step:15488	 l-p:0.11978321522474289
epoch£º774	 i:9 	 global-step:15489	 l-p:0.11656486243009567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:775
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2831, 5.0573, 4.9605],
        [5.2831, 5.0722, 5.0309],
        [5.2831, 5.2831, 5.2831],
        [5.2831, 5.4152, 5.1925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:775, step:0 
model_pd.l_p.mean(): 0.12368825823068619 
model_pd.l_d.mean(): -18.660114288330078 
model_pd.lagr.mean(): -18.536426544189453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5935], device='cuda:0')), ('power', tensor([-19.6238], device='cuda:0'))])
epoch£º775	 i:0 	 global-step:15500	 l-p:0.12368825823068619
epoch£º775	 i:1 	 global-step:15501	 l-p:0.14050701260566711
epoch£º775	 i:2 	 global-step:15502	 l-p:0.09110410511493683
epoch£º775	 i:3 	 global-step:15503	 l-p:0.24081306159496307
epoch£º775	 i:4 	 global-step:15504	 l-p:0.08256910741329193
epoch£º775	 i:5 	 global-step:15505	 l-p:0.11081672459840775
epoch£º775	 i:6 	 global-step:15506	 l-p:0.12922173738479614
epoch£º775	 i:7 	 global-step:15507	 l-p:0.13514955341815948
epoch£º775	 i:8 	 global-step:15508	 l-p:0.13460545241832733
epoch£º775	 i:9 	 global-step:15509	 l-p:-0.7242404222488403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:776
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1838,  0.1045,  1.0000,  0.0594,
          1.0000,  0.5685, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2420,  0.1508,  1.0000,  0.0940,
          1.0000,  0.6232, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228]], device='cuda:0')
 pt:tensor([[5.2906, 5.1199, 5.1408],
        [5.2906, 5.0746, 5.0182],
        [5.2906, 5.0800, 5.0386],
        [5.2906, 5.6761, 5.6004]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:776, step:0 
model_pd.l_p.mean(): 0.11881988495588303 
model_pd.l_d.mean(): -20.42544937133789 
model_pd.lagr.mean(): -20.306629180908203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4101], device='cuda:0')), ('power', tensor([-21.2322], device='cuda:0'))])
epoch£º776	 i:0 	 global-step:15520	 l-p:0.11881988495588303
epoch£º776	 i:1 	 global-step:15521	 l-p:0.11751805990934372
epoch£º776	 i:2 	 global-step:15522	 l-p:0.12982705235481262
epoch£º776	 i:3 	 global-step:15523	 l-p:0.12421076744794846
epoch£º776	 i:4 	 global-step:15524	 l-p:0.12006168067455292
epoch£º776	 i:5 	 global-step:15525	 l-p:0.09198840707540512
epoch£º776	 i:6 	 global-step:15526	 l-p:0.1249290183186531
epoch£º776	 i:7 	 global-step:15527	 l-p:0.40916550159454346
epoch£º776	 i:8 	 global-step:15528	 l-p:0.13348247110843658
epoch£º776	 i:9 	 global-step:15529	 l-p:0.1039055585861206
====================================================================================================
====================================================================================================
====================================================================================================

epoch:777
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2428, 5.0352, 5.0090],
        [5.2428, 5.1595, 5.2079],
        [5.2428, 5.0248, 4.9742],
        [5.2428, 5.0099, 4.8575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:777, step:0 
model_pd.l_p.mean(): 0.11392983049154282 
model_pd.l_d.mean(): -20.413780212402344 
model_pd.lagr.mean(): -20.299850463867188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4120], device='cuda:0')), ('power', tensor([-21.2222], device='cuda:0'))])
epoch£º777	 i:0 	 global-step:15540	 l-p:0.11392983049154282
epoch£º777	 i:1 	 global-step:15541	 l-p:0.1341896653175354
epoch£º777	 i:2 	 global-step:15542	 l-p:0.13020876049995422
epoch£º777	 i:3 	 global-step:15543	 l-p:0.14097808301448822
epoch£º777	 i:4 	 global-step:15544	 l-p:0.14714914560317993
epoch£º777	 i:5 	 global-step:15545	 l-p:0.03095284476876259
epoch£º777	 i:6 	 global-step:15546	 l-p:0.12059219181537628
epoch£º777	 i:7 	 global-step:15547	 l-p:0.16114726662635803
epoch£º777	 i:8 	 global-step:15548	 l-p:-0.06983780115842819
epoch£º777	 i:9 	 global-step:15549	 l-p:0.14282764494419098
====================================================================================================
====================================================================================================
====================================================================================================

epoch:778
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1758, 5.1749, 5.1758],
        [5.1758, 5.0975, 5.1452],
        [5.1758, 5.1588, 5.1738],
        [5.1758, 5.1690, 5.1754]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:778, step:0 
model_pd.l_p.mean(): 0.1267269253730774 
model_pd.l_d.mean(): -18.885013580322266 
model_pd.lagr.mean(): -18.75828742980957 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5889], device='cuda:0')), ('power', tensor([-19.8481], device='cuda:0'))])
epoch£º778	 i:0 	 global-step:15560	 l-p:0.1267269253730774
epoch£º778	 i:1 	 global-step:15561	 l-p:0.14060257375240326
epoch£º778	 i:2 	 global-step:15562	 l-p:0.07091358304023743
epoch£º778	 i:3 	 global-step:15563	 l-p:0.15452717244625092
epoch£º778	 i:4 	 global-step:15564	 l-p:0.07688687741756439
epoch£º778	 i:5 	 global-step:15565	 l-p:0.021675758063793182
epoch£º778	 i:6 	 global-step:15566	 l-p:0.15584838390350342
epoch£º778	 i:7 	 global-step:15567	 l-p:0.138147234916687
epoch£º778	 i:8 	 global-step:15568	 l-p:0.13209925591945648
epoch£º778	 i:9 	 global-step:15569	 l-p:0.14642806351184845
====================================================================================================
====================================================================================================
====================================================================================================

epoch:779
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1542, 4.9947, 5.0319],
        [5.1542, 4.9146, 4.7288],
        [5.1542, 5.1540, 5.1542],
        [5.1542, 5.0755, 5.1234]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:779, step:0 
model_pd.l_p.mean(): 0.10780969262123108 
model_pd.l_d.mean(): -20.32901382446289 
model_pd.lagr.mean(): -20.22120475769043 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4460], device='cuda:0')), ('power', tensor([-21.1710], device='cuda:0'))])
epoch£º779	 i:0 	 global-step:15580	 l-p:0.10780969262123108
epoch£º779	 i:1 	 global-step:15581	 l-p:0.16582143306732178
epoch£º779	 i:2 	 global-step:15582	 l-p:0.1326153725385666
epoch£º779	 i:3 	 global-step:15583	 l-p:0.12302111089229584
epoch£º779	 i:4 	 global-step:15584	 l-p:0.15686021745204926
epoch£º779	 i:5 	 global-step:15585	 l-p:0.07073904573917389
epoch£º779	 i:6 	 global-step:15586	 l-p:0.13476578891277313
epoch£º779	 i:7 	 global-step:15587	 l-p:0.15269896388053894
epoch£º779	 i:8 	 global-step:15588	 l-p:0.165737122297287
epoch£º779	 i:9 	 global-step:15589	 l-p:0.2278956025838852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:780
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0723, 5.0669, 5.0720],
        [5.0723, 5.0534, 5.0698],
        [5.0723, 5.2005, 4.9775],
        [5.0723, 5.1939, 4.9676]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:780, step:0 
model_pd.l_p.mean(): 0.21077755093574524 
model_pd.l_d.mean(): -18.507368087768555 
model_pd.lagr.mean(): -18.29659080505371 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5846], device='cuda:0')), ('power', tensor([-19.4589], device='cuda:0'))])
epoch£º780	 i:0 	 global-step:15600	 l-p:0.21077755093574524
epoch£º780	 i:1 	 global-step:15601	 l-p:0.09063396602869034
epoch£º780	 i:2 	 global-step:15602	 l-p:0.14507417380809784
epoch£º780	 i:3 	 global-step:15603	 l-p:0.12522104382514954
epoch£º780	 i:4 	 global-step:15604	 l-p:0.17543724179267883
epoch£º780	 i:5 	 global-step:15605	 l-p:0.11968303471803665
epoch£º780	 i:6 	 global-step:15606	 l-p:0.21598517894744873
epoch£º780	 i:7 	 global-step:15607	 l-p:0.1379195600748062
epoch£º780	 i:8 	 global-step:15608	 l-p:0.09481161832809448
epoch£º780	 i:9 	 global-step:15609	 l-p:0.13194693624973297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:781
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1493, 5.2972, 5.0830],
        [5.1493, 5.1493, 5.1493],
        [5.1493, 5.1432, 5.1490],
        [5.1493, 5.1466, 5.1492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:781, step:0 
model_pd.l_p.mean(): 0.17925666272640228 
model_pd.l_d.mean(): -20.35504722595215 
model_pd.lagr.mean(): -20.175790786743164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4606], device='cuda:0')), ('power', tensor([-21.2127], device='cuda:0'))])
epoch£º781	 i:0 	 global-step:15620	 l-p:0.17925666272640228
epoch£º781	 i:1 	 global-step:15621	 l-p:0.12819121778011322
epoch£º781	 i:2 	 global-step:15622	 l-p:0.12273808568716049
epoch£º781	 i:3 	 global-step:15623	 l-p:0.10096036642789841
epoch£º781	 i:4 	 global-step:15624	 l-p:0.13084174692630768
epoch£º781	 i:5 	 global-step:15625	 l-p:0.06608358770608902
epoch£º781	 i:6 	 global-step:15626	 l-p:0.11064086854457855
epoch£º781	 i:7 	 global-step:15627	 l-p:-0.01579931192100048
epoch£º781	 i:8 	 global-step:15628	 l-p:0.138765349984169
epoch£º781	 i:9 	 global-step:15629	 l-p:0.11975657939910889
====================================================================================================
====================================================================================================
====================================================================================================

epoch:782
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1880, 5.1697, 5.1856],
        [5.1880, 5.2440, 4.9869],
        [5.1880, 5.1237, 5.1667],
        [5.1880, 5.1198, 5.1643]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:782, step:0 
model_pd.l_p.mean(): 0.03986218199133873 
model_pd.l_d.mean(): -19.967876434326172 
model_pd.lagr.mean(): -19.928014755249023 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4234], device='cuda:0')), ('power', tensor([-20.7797], device='cuda:0'))])
epoch£º782	 i:0 	 global-step:15640	 l-p:0.03986218199133873
epoch£º782	 i:1 	 global-step:15641	 l-p:0.14568012952804565
epoch£º782	 i:2 	 global-step:15642	 l-p:0.12370326370000839
epoch£º782	 i:3 	 global-step:15643	 l-p:0.1319778561592102
epoch£º782	 i:4 	 global-step:15644	 l-p:0.1340850442647934
epoch£º782	 i:5 	 global-step:15645	 l-p:0.11812694370746613
epoch£º782	 i:6 	 global-step:15646	 l-p:0.08829542249441147
epoch£º782	 i:7 	 global-step:15647	 l-p:0.128450408577919
epoch£º782	 i:8 	 global-step:15648	 l-p:0.16257517039775848
epoch£º782	 i:9 	 global-step:15649	 l-p:0.17905612289905548
====================================================================================================
====================================================================================================
====================================================================================================

epoch:783
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1167, 5.0579, 4.7640],
        [5.1167, 4.8851, 4.8284],
        [5.1167, 5.0243, 4.7262],
        [5.1167, 5.0718, 5.1057]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:783, step:0 
model_pd.l_p.mean(): 0.16452474892139435 
model_pd.l_d.mean(): -20.514266967773438 
model_pd.lagr.mean(): -20.349742889404297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4232], device='cuda:0')), ('power', tensor([-21.3362], device='cuda:0'))])
epoch£º783	 i:0 	 global-step:15660	 l-p:0.16452474892139435
epoch£º783	 i:1 	 global-step:15661	 l-p:0.16775719821453094
epoch£º783	 i:2 	 global-step:15662	 l-p:0.12801656126976013
epoch£º783	 i:3 	 global-step:15663	 l-p:0.12126228213310242
epoch£º783	 i:4 	 global-step:15664	 l-p:0.1299961805343628
epoch£º783	 i:5 	 global-step:15665	 l-p:0.10276474058628082
epoch£º783	 i:6 	 global-step:15666	 l-p:0.16634704172611237
epoch£º783	 i:7 	 global-step:15667	 l-p:0.15030071139335632
epoch£º783	 i:8 	 global-step:15668	 l-p:0.09323129802942276
epoch£º783	 i:9 	 global-step:15669	 l-p:0.1862705945968628
====================================================================================================
====================================================================================================
====================================================================================================

epoch:784
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0974, 5.0313, 4.7350],
        [5.0974, 5.0974, 5.0974],
        [5.0974, 4.9937, 5.0465],
        [5.0974, 4.9856, 5.0386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:784, step:0 
model_pd.l_p.mean(): 0.1290731579065323 
model_pd.l_d.mean(): -20.071125030517578 
model_pd.lagr.mean(): -19.94205093383789 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5170], device='cuda:0')), ('power', tensor([-20.9819], device='cuda:0'))])
epoch£º784	 i:0 	 global-step:15680	 l-p:0.1290731579065323
epoch£º784	 i:1 	 global-step:15681	 l-p:0.13856928050518036
epoch£º784	 i:2 	 global-step:15682	 l-p:0.10731152445077896
epoch£º784	 i:3 	 global-step:15683	 l-p:0.1867550164461136
epoch£º784	 i:4 	 global-step:15684	 l-p:0.10025159269571304
epoch£º784	 i:5 	 global-step:15685	 l-p:0.13647814095020294
epoch£º784	 i:6 	 global-step:15686	 l-p:0.1719702184200287
epoch£º784	 i:7 	 global-step:15687	 l-p:0.1574060469865799
epoch£º784	 i:8 	 global-step:15688	 l-p:0.10754457116127014
epoch£º784	 i:9 	 global-step:15689	 l-p:0.1306827962398529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:785
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1350, 5.1350, 5.1350],
        [5.1350, 5.1305, 5.1347],
        [5.1350, 4.9182, 4.6673],
        [5.1350, 4.9637, 4.9945]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:785, step:0 
model_pd.l_p.mean(): 0.09642676264047623 
model_pd.l_d.mean(): -19.784381866455078 
model_pd.lagr.mean(): -19.687955856323242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4534], device='cuda:0')), ('power', tensor([-20.6239], device='cuda:0'))])
epoch£º785	 i:0 	 global-step:15700	 l-p:0.09642676264047623
epoch£º785	 i:1 	 global-step:15701	 l-p:0.1512024700641632
epoch£º785	 i:2 	 global-step:15702	 l-p:0.08475351333618164
epoch£º785	 i:3 	 global-step:15703	 l-p:0.11062303185462952
epoch£º785	 i:4 	 global-step:15704	 l-p:0.13257475197315216
epoch£º785	 i:5 	 global-step:15705	 l-p:0.2129247933626175
epoch£º785	 i:6 	 global-step:15706	 l-p:0.16047042608261108
epoch£º785	 i:7 	 global-step:15707	 l-p:0.1882447749376297
epoch£º785	 i:8 	 global-step:15708	 l-p:0.12289834767580032
epoch£º785	 i:9 	 global-step:15709	 l-p:0.17340409755706787
====================================================================================================
====================================================================================================
====================================================================================================

epoch:786
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1003, 5.1003, 5.1003],
        [5.1003, 5.4737, 5.3948],
        [5.1003, 5.0703, 4.7818],
        [5.1003, 5.0346, 5.0786]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:786, step:0 
model_pd.l_p.mean(): 0.13113071024417877 
model_pd.l_d.mean(): -20.569334030151367 
model_pd.lagr.mean(): -20.438203811645508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4298], device='cuda:0')), ('power', tensor([-21.3992], device='cuda:0'))])
epoch£º786	 i:0 	 global-step:15720	 l-p:0.13113071024417877
epoch£º786	 i:1 	 global-step:15721	 l-p:0.13982902467250824
epoch£º786	 i:2 	 global-step:15722	 l-p:0.1582566499710083
epoch£º786	 i:3 	 global-step:15723	 l-p:0.21038131415843964
epoch£º786	 i:4 	 global-step:15724	 l-p:0.12174739688634872
epoch£º786	 i:5 	 global-step:15725	 l-p:0.11502049118280411
epoch£º786	 i:6 	 global-step:15726	 l-p:0.12792356312274933
epoch£º786	 i:7 	 global-step:15727	 l-p:0.1581406593322754
epoch£º786	 i:8 	 global-step:15728	 l-p:0.11544027179479599
epoch£º786	 i:9 	 global-step:15729	 l-p:0.15299741923809052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:787
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1411, 5.0774, 4.7828],
        [5.1411, 5.0962, 5.1301],
        [5.1411, 5.1226, 5.1387],
        [5.1411, 5.0607, 5.1094]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:787, step:0 
model_pd.l_p.mean(): 0.11600048094987869 
model_pd.l_d.mean(): -19.265411376953125 
model_pd.lagr.mean(): -19.149410247802734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5619], device='cuda:0')), ('power', tensor([-20.2077], device='cuda:0'))])
epoch£º787	 i:0 	 global-step:15740	 l-p:0.11600048094987869
epoch£º787	 i:1 	 global-step:15741	 l-p:0.1126207709312439
epoch£º787	 i:2 	 global-step:15742	 l-p:0.14024406671524048
epoch£º787	 i:3 	 global-step:15743	 l-p:0.09985839575529099
epoch£º787	 i:4 	 global-step:15744	 l-p:0.09755297750234604
epoch£º787	 i:5 	 global-step:15745	 l-p:0.1412372887134552
epoch£º787	 i:6 	 global-step:15746	 l-p:0.11769217997789383
epoch£º787	 i:7 	 global-step:15747	 l-p:0.14678120613098145
epoch£º787	 i:8 	 global-step:15748	 l-p:0.15923556685447693
epoch£º787	 i:9 	 global-step:15749	 l-p:0.13432057201862335
====================================================================================================
====================================================================================================
====================================================================================================

epoch:788
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1345, 4.8930, 4.8020],
        [5.1345, 5.1345, 5.1345],
        [5.1345, 4.8876, 4.7631],
        [5.1345, 5.2144, 4.9664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:788, step:0 
model_pd.l_p.mean(): 0.13661785423755646 
model_pd.l_d.mean(): -20.54180145263672 
model_pd.lagr.mean(): -20.405183792114258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4172], device='cuda:0')), ('power', tensor([-21.3580], device='cuda:0'))])
epoch£º788	 i:0 	 global-step:15760	 l-p:0.13661785423755646
epoch£º788	 i:1 	 global-step:15761	 l-p:0.14613814651966095
epoch£º788	 i:2 	 global-step:15762	 l-p:0.11625958979129791
epoch£º788	 i:3 	 global-step:15763	 l-p:0.11373443156480789
epoch£º788	 i:4 	 global-step:15764	 l-p:0.20499637722969055
epoch£º788	 i:5 	 global-step:15765	 l-p:0.18092532455921173
epoch£º788	 i:6 	 global-step:15766	 l-p:0.1404142528772354
epoch£º788	 i:7 	 global-step:15767	 l-p:0.12590844929218292
epoch£º788	 i:8 	 global-step:15768	 l-p:0.15251761674880981
epoch£º788	 i:9 	 global-step:15769	 l-p:0.1390676647424698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:789
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0842, 5.0374, 5.0725],
        [5.0842, 4.8458, 4.7826],
        [5.0842, 5.0842, 5.0842],
        [5.0842, 5.0842, 5.0842]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:789, step:0 
model_pd.l_p.mean(): 0.12244947254657745 
model_pd.l_d.mean(): -20.26859474182129 
model_pd.lagr.mean(): -20.14614486694336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4709], device='cuda:0')), ('power', tensor([-21.1354], device='cuda:0'))])
epoch£º789	 i:0 	 global-step:15780	 l-p:0.12244947254657745
epoch£º789	 i:1 	 global-step:15781	 l-p:0.15584617853164673
epoch£º789	 i:2 	 global-step:15782	 l-p:0.3778931796550751
epoch£º789	 i:3 	 global-step:15783	 l-p:0.17325852811336517
epoch£º789	 i:4 	 global-step:15784	 l-p:0.21659237146377563
epoch£º789	 i:5 	 global-step:15785	 l-p:0.15239790081977844
epoch£º789	 i:6 	 global-step:15786	 l-p:0.1172657236456871
epoch£º789	 i:7 	 global-step:15787	 l-p:0.12129470705986023
epoch£º789	 i:8 	 global-step:15788	 l-p:0.1596471667289734
epoch£º789	 i:9 	 global-step:15789	 l-p:0.14465276896953583
====================================================================================================
====================================================================================================
====================================================================================================

epoch:790
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0978, 5.0233, 5.0706],
        [5.0978, 5.0978, 5.0978],
        [5.0978, 5.0399, 4.7436],
        [5.0978, 4.8602, 4.7968]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:790, step:0 
model_pd.l_p.mean(): 0.11993178725242615 
model_pd.l_d.mean(): -20.77399253845215 
model_pd.lagr.mean(): -20.65406036376953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4106], device='cuda:0')), ('power', tensor([-21.5877], device='cuda:0'))])
epoch£º790	 i:0 	 global-step:15800	 l-p:0.11993178725242615
epoch£º790	 i:1 	 global-step:15801	 l-p:0.2018498182296753
epoch£º790	 i:2 	 global-step:15802	 l-p:0.14783626794815063
epoch£º790	 i:3 	 global-step:15803	 l-p:0.14104677736759186
epoch£º790	 i:4 	 global-step:15804	 l-p:0.1439911425113678
epoch£º790	 i:5 	 global-step:15805	 l-p:0.128330260515213
epoch£º790	 i:6 	 global-step:15806	 l-p:0.16472464799880981
epoch£º790	 i:7 	 global-step:15807	 l-p:0.1255408674478531
epoch£º790	 i:8 	 global-step:15808	 l-p:0.14209558069705963
epoch£º790	 i:9 	 global-step:15809	 l-p:0.08501064777374268
====================================================================================================
====================================================================================================
====================================================================================================

epoch:791
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1411, 4.9165, 4.8777],
        [5.1411, 5.5748, 5.5345],
        [5.1411, 5.0321, 5.0849],
        [5.1411, 4.9459, 4.9548]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:791, step:0 
model_pd.l_p.mean(): 0.15936420857906342 
model_pd.l_d.mean(): -20.487380981445312 
model_pd.lagr.mean(): -20.32801628112793 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4104], device='cuda:0')), ('power', tensor([-21.2955], device='cuda:0'))])
epoch£º791	 i:0 	 global-step:15820	 l-p:0.15936420857906342
epoch£º791	 i:1 	 global-step:15821	 l-p:0.14920377731323242
epoch£º791	 i:2 	 global-step:15822	 l-p:0.06083061173558235
epoch£º791	 i:3 	 global-step:15823	 l-p:0.12198302149772644
epoch£º791	 i:4 	 global-step:15824	 l-p:0.09909067302942276
epoch£º791	 i:5 	 global-step:15825	 l-p:0.13339096307754517
epoch£º791	 i:6 	 global-step:15826	 l-p:0.09635475277900696
epoch£º791	 i:7 	 global-step:15827	 l-p:0.15029659867286682
epoch£º791	 i:8 	 global-step:15828	 l-p:0.1275382936000824
epoch£º791	 i:9 	 global-step:15829	 l-p:0.15515536069869995
====================================================================================================
====================================================================================================
====================================================================================================

epoch:792
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1427, 4.8991, 4.7018],
        [5.1427, 5.0110, 4.7127],
        [5.1427, 5.0733, 5.1186],
        [5.1427, 5.0024, 5.0505]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:792, step:0 
model_pd.l_p.mean(): 0.13324616849422455 
model_pd.l_d.mean(): -20.63102912902832 
model_pd.lagr.mean(): -20.497783660888672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4099], device='cuda:0')), ('power', tensor([-21.4413], device='cuda:0'))])
epoch£º792	 i:0 	 global-step:15840	 l-p:0.13324616849422455
epoch£º792	 i:1 	 global-step:15841	 l-p:0.07543600350618362
epoch£º792	 i:2 	 global-step:15842	 l-p:0.1431979537010193
epoch£º792	 i:3 	 global-step:15843	 l-p:0.14458438754081726
epoch£º792	 i:4 	 global-step:15844	 l-p:0.11987116187810898
epoch£º792	 i:5 	 global-step:15845	 l-p:0.1277729719877243
epoch£º792	 i:6 	 global-step:15846	 l-p:0.09870437532663345
epoch£º792	 i:7 	 global-step:15847	 l-p:0.3073973059654236
epoch£º792	 i:8 	 global-step:15848	 l-p:0.17534664273262024
epoch£º792	 i:9 	 global-step:15849	 l-p:0.12598004937171936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:793
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0414, 5.0414, 5.0414],
        [5.0414, 5.0386, 5.0413],
        [5.0414, 5.0414, 5.0414],
        [5.0414, 4.9935, 5.0293]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:793, step:0 
model_pd.l_p.mean(): 0.13373763859272003 
model_pd.l_d.mean(): -20.635696411132812 
model_pd.lagr.mean(): -20.5019588470459 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4217], device='cuda:0')), ('power', tensor([-21.4583], device='cuda:0'))])
epoch£º793	 i:0 	 global-step:15860	 l-p:0.13373763859272003
epoch£º793	 i:1 	 global-step:15861	 l-p:0.183542400598526
epoch£º793	 i:2 	 global-step:15862	 l-p:0.12066628783941269
epoch£º793	 i:3 	 global-step:15863	 l-p:0.12157123535871506
epoch£º793	 i:4 	 global-step:15864	 l-p:0.12391271442174911
epoch£º793	 i:5 	 global-step:15865	 l-p:0.16995282471179962
epoch£º793	 i:6 	 global-step:15866	 l-p:0.1373664289712906
epoch£º793	 i:7 	 global-step:15867	 l-p:-15.45500373840332
epoch£º793	 i:8 	 global-step:15868	 l-p:0.235549658536911
epoch£º793	 i:9 	 global-step:15869	 l-p:0.040040090680122375
====================================================================================================
====================================================================================================
====================================================================================================

epoch:794
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0172, 4.7542, 4.6044],
        [5.0172, 4.9950, 5.0141],
        [5.0172, 4.7564, 4.6311],
        [5.0172, 5.0172, 5.0172]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:794, step:0 
model_pd.l_p.mean(): 0.22959719598293304 
model_pd.l_d.mean(): -20.115795135498047 
model_pd.lagr.mean(): -19.886198043823242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5214], device='cuda:0')), ('power', tensor([-21.0320], device='cuda:0'))])
epoch£º794	 i:0 	 global-step:15880	 l-p:0.22959719598293304
epoch£º794	 i:1 	 global-step:15881	 l-p:0.11641975492238998
epoch£º794	 i:2 	 global-step:15882	 l-p:0.3449016213417053
epoch£º794	 i:3 	 global-step:15883	 l-p:0.13276132941246033
epoch£º794	 i:4 	 global-step:15884	 l-p:0.5370196104049683
epoch£º794	 i:5 	 global-step:15885	 l-p:-0.18135495483875275
epoch£º794	 i:6 	 global-step:15886	 l-p:0.11426780372858047
epoch£º794	 i:7 	 global-step:15887	 l-p:0.2139902263879776
epoch£º794	 i:8 	 global-step:15888	 l-p:0.11945094913244247
epoch£º794	 i:9 	 global-step:15889	 l-p:0.1910129189491272
====================================================================================================
====================================================================================================
====================================================================================================

epoch:795
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0788, 5.0788, 5.0788],
        [5.0788, 4.8641, 4.5922],
        [5.0788, 5.0358, 5.0687],
        [5.0788, 5.0378, 5.0695]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:795, step:0 
model_pd.l_p.mean(): 0.11417525261640549 
model_pd.l_d.mean(): -20.23480987548828 
model_pd.lagr.mean(): -20.120634078979492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4724], device='cuda:0')), ('power', tensor([-21.1025], device='cuda:0'))])
epoch£º795	 i:0 	 global-step:15900	 l-p:0.11417525261640549
epoch£º795	 i:1 	 global-step:15901	 l-p:0.17020337283611298
epoch£º795	 i:2 	 global-step:15902	 l-p:0.16891935467720032
epoch£º795	 i:3 	 global-step:15903	 l-p:0.14761196076869965
epoch£º795	 i:4 	 global-step:15904	 l-p:0.16180391609668732
epoch£º795	 i:5 	 global-step:15905	 l-p:0.11137708276510239
epoch£º795	 i:6 	 global-step:15906	 l-p:0.21011283993721008
epoch£º795	 i:7 	 global-step:15907	 l-p:0.12250600755214691
epoch£º795	 i:8 	 global-step:15908	 l-p:0.11480712890625
epoch£º795	 i:9 	 global-step:15909	 l-p:0.12251422554254532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:796
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1420, 5.0909, 5.1282],
        [5.1420, 5.0911, 4.7965],
        [5.1420, 5.0100, 5.0606],
        [5.1420, 5.1420, 5.1420]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:796, step:0 
model_pd.l_p.mean(): 0.140019953250885 
model_pd.l_d.mean(): -19.81044578552246 
model_pd.lagr.mean(): -19.670425415039062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4867], device='cuda:0')), ('power', tensor([-20.6850], device='cuda:0'))])
epoch£º796	 i:0 	 global-step:15920	 l-p:0.140019953250885
epoch£º796	 i:1 	 global-step:15921	 l-p:0.1232268288731575
epoch£º796	 i:2 	 global-step:15922	 l-p:0.09886682033538818
epoch£º796	 i:3 	 global-step:15923	 l-p:0.13734650611877441
epoch£º796	 i:4 	 global-step:15924	 l-p:0.1526821404695511
epoch£º796	 i:5 	 global-step:15925	 l-p:0.10209330916404724
epoch£º796	 i:6 	 global-step:15926	 l-p:0.11323560774326324
epoch£º796	 i:7 	 global-step:15927	 l-p:0.14001961052417755
epoch£º796	 i:8 	 global-step:15928	 l-p:0.13477209210395813
epoch£º796	 i:9 	 global-step:15929	 l-p:0.15912692248821259
====================================================================================================
====================================================================================================
====================================================================================================

epoch:797
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1528, 5.0632, 5.1143],
        [5.1528, 5.1364, 5.1509],
        [5.1528, 5.1522, 5.1528],
        [5.1528, 5.1528, 5.1528]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:797, step:0 
model_pd.l_p.mean(): 0.11635815352201462 
model_pd.l_d.mean(): -20.35426139831543 
model_pd.lagr.mean(): -20.237903594970703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4495], device='cuda:0')), ('power', tensor([-21.2005], device='cuda:0'))])
epoch£º797	 i:0 	 global-step:15940	 l-p:0.11635815352201462
epoch£º797	 i:1 	 global-step:15941	 l-p:0.11680611968040466
epoch£º797	 i:2 	 global-step:15942	 l-p:0.14361320436000824
epoch£º797	 i:3 	 global-step:15943	 l-p:0.15493793785572052
epoch£º797	 i:4 	 global-step:15944	 l-p:0.16701431572437286
epoch£º797	 i:5 	 global-step:15945	 l-p:0.119331493973732
epoch£º797	 i:6 	 global-step:15946	 l-p:0.13818195462226868
epoch£º797	 i:7 	 global-step:15947	 l-p:0.11232032626867294
epoch£º797	 i:8 	 global-step:15948	 l-p:0.16979384422302246
epoch£º797	 i:9 	 global-step:15949	 l-p:0.17061862349510193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:798
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0899, 5.1168, 4.8447],
        [5.0899, 5.0852, 5.0896],
        [5.0899, 4.9545, 5.0058],
        [5.0899, 5.3552, 5.2060]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:798, step:0 
model_pd.l_p.mean(): 0.1487581431865692 
model_pd.l_d.mean(): -19.74732208251953 
model_pd.lagr.mean(): -19.59856414794922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4764], device='cuda:0')), ('power', tensor([-20.6101], device='cuda:0'))])
epoch£º798	 i:0 	 global-step:15960	 l-p:0.1487581431865692
epoch£º798	 i:1 	 global-step:15961	 l-p:0.11369400471448898
epoch£º798	 i:2 	 global-step:15962	 l-p:0.20731419324874878
epoch£º798	 i:3 	 global-step:15963	 l-p:0.14231452345848083
epoch£º798	 i:4 	 global-step:15964	 l-p:-5.495704174041748
epoch£º798	 i:5 	 global-step:15965	 l-p:0.17679722607135773
epoch£º798	 i:6 	 global-step:15966	 l-p:0.11392465978860855
epoch£º798	 i:7 	 global-step:15967	 l-p:0.21065381169319153
epoch£º798	 i:8 	 global-step:15968	 l-p:0.20477795600891113
epoch£º798	 i:9 	 global-step:15969	 l-p:0.18962813913822174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:799
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0635, 5.0203, 5.0535],
        [5.0635, 5.0622, 5.0635],
        [5.0635, 5.0635, 5.0635],
        [5.0635, 5.0608, 5.0634]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:799, step:0 
model_pd.l_p.mean(): 0.1129186823964119 
model_pd.l_d.mean(): -20.84897232055664 
model_pd.lagr.mean(): -20.736053466796875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3994], device='cuda:0')), ('power', tensor([-21.6525], device='cuda:0'))])
epoch£º799	 i:0 	 global-step:15980	 l-p:0.1129186823964119
epoch£º799	 i:1 	 global-step:15981	 l-p:0.647231936454773
epoch£º799	 i:2 	 global-step:15982	 l-p:0.16317352652549744
epoch£º799	 i:3 	 global-step:15983	 l-p:0.11893042176961899
epoch£º799	 i:4 	 global-step:15984	 l-p:0.1283286213874817
epoch£º799	 i:5 	 global-step:15985	 l-p:0.10698296129703522
epoch£º799	 i:6 	 global-step:15986	 l-p:0.11953434348106384
epoch£º799	 i:7 	 global-step:15987	 l-p:0.12001349776983261
epoch£º799	 i:8 	 global-step:15988	 l-p:0.2960492670536041
epoch£º799	 i:9 	 global-step:15989	 l-p:0.13057628273963928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:800
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0758, 4.8519, 4.8291],
        [5.0758, 5.2340, 5.0235],
        [5.0758, 5.0679, 5.0753],
        [5.0758, 4.8171, 4.6633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:800, step:0 
model_pd.l_p.mean(): 0.11195816099643707 
model_pd.l_d.mean(): -18.39389419555664 
model_pd.lagr.mean(): -18.281936645507812 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5973], device='cuda:0')), ('power', tensor([-19.3565], device='cuda:0'))])
epoch£º800	 i:0 	 global-step:16000	 l-p:0.11195816099643707
epoch£º800	 i:1 	 global-step:16001	 l-p:0.11063104122877121
epoch£º800	 i:2 	 global-step:16002	 l-p:0.14986100792884827
epoch£º800	 i:3 	 global-step:16003	 l-p:0.24045713245868683
epoch£º800	 i:4 	 global-step:16004	 l-p:0.1521587073802948
epoch£º800	 i:5 	 global-step:16005	 l-p:0.12236807495355606
epoch£º800	 i:6 	 global-step:16006	 l-p:0.2694205343723297
epoch£º800	 i:7 	 global-step:16007	 l-p:0.13974277675151825
epoch£º800	 i:8 	 global-step:16008	 l-p:0.19038276374340057
epoch£º800	 i:9 	 global-step:16009	 l-p:0.1324283629655838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:801
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.7815,  0.7198,  1.0000,  0.6630,
          1.0000,  0.9211, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7501,  0.6816,  1.0000,  0.6193,
          1.0000,  0.9086, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2351,  0.1451,  1.0000,  0.0895,
          1.0000,  0.6172, 31.6228]], device='cuda:0')
 pt:tensor([[5.0877, 5.2492, 5.0401],
        [5.0877, 5.2038, 4.9713],
        [5.0877, 4.8707, 4.5994],
        [5.0877, 4.8612, 4.8321]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:801, step:0 
model_pd.l_p.mean(): 0.12957260012626648 
model_pd.l_d.mean(): -19.577220916748047 
model_pd.lagr.mean(): -19.447649002075195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5012], device='cuda:0')), ('power', tensor([-20.4624], device='cuda:0'))])
epoch£º801	 i:0 	 global-step:16020	 l-p:0.12957260012626648
epoch£º801	 i:1 	 global-step:16021	 l-p:0.13077116012573242
epoch£º801	 i:2 	 global-step:16022	 l-p:0.10173771530389786
epoch£º801	 i:3 	 global-step:16023	 l-p:0.15790191292762756
epoch£º801	 i:4 	 global-step:16024	 l-p:0.15263710916042328
epoch£º801	 i:5 	 global-step:16025	 l-p:0.16901056468486786
epoch£º801	 i:6 	 global-step:16026	 l-p:0.11918361485004425
epoch£º801	 i:7 	 global-step:16027	 l-p:0.13216379284858704
epoch£º801	 i:8 	 global-step:16028	 l-p:0.21271350979804993
epoch£º801	 i:9 	 global-step:16029	 l-p:0.18521104753017426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:802
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1054, 5.1018, 5.1052],
        [5.1054, 4.9845, 5.0381],
        [5.1054, 5.1054, 5.1054],
        [5.1054, 4.8524, 4.7369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:802, step:0 
model_pd.l_p.mean(): 0.18933036923408508 
model_pd.l_d.mean(): -18.9508113861084 
model_pd.lagr.mean(): -18.7614803314209 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5256], device='cuda:0')), ('power', tensor([-19.8495], device='cuda:0'))])
epoch£º802	 i:0 	 global-step:16040	 l-p:0.18933036923408508
epoch£º802	 i:1 	 global-step:16041	 l-p:0.15444692969322205
epoch£º802	 i:2 	 global-step:16042	 l-p:0.1813545972108841
epoch£º802	 i:3 	 global-step:16043	 l-p:0.12484428286552429
epoch£º802	 i:4 	 global-step:16044	 l-p:0.08465610444545746
epoch£º802	 i:5 	 global-step:16045	 l-p:0.1316772848367691
epoch£º802	 i:6 	 global-step:16046	 l-p:0.14335577189922333
epoch£º802	 i:7 	 global-step:16047	 l-p:0.11434941738843918
epoch£º802	 i:8 	 global-step:16048	 l-p:0.12829378247261047
epoch£º802	 i:9 	 global-step:16049	 l-p:0.13435755670070648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:803
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4241,  0.3187,  1.0000,  0.2394,
          1.0000,  0.7513, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3293,  0.2274,  1.0000,  0.1570,
          1.0000,  0.6906, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2741,  0.1781,  1.0000,  0.1157,
          1.0000,  0.6496, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3232,  0.2218,  1.0000,  0.1522,
          1.0000,  0.6862, 31.6228]], device='cuda:0')
 pt:tensor([[5.1367, 4.9106, 4.6599],
        [5.1367, 4.8840, 4.7311],
        [5.1367, 4.8938, 4.8132],
        [5.1367, 4.8841, 4.7389]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:803, step:0 
model_pd.l_p.mean(): 0.13839349150657654 
model_pd.l_d.mean(): -20.557722091674805 
model_pd.lagr.mean(): -20.419328689575195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4334], device='cuda:0')), ('power', tensor([-21.3911], device='cuda:0'))])
epoch£º803	 i:0 	 global-step:16060	 l-p:0.13839349150657654
epoch£º803	 i:1 	 global-step:16061	 l-p:0.12148326635360718
epoch£º803	 i:2 	 global-step:16062	 l-p:0.09620534628629684
epoch£º803	 i:3 	 global-step:16063	 l-p:0.14252540469169617
epoch£º803	 i:4 	 global-step:16064	 l-p:0.15719737112522125
epoch£º803	 i:5 	 global-step:16065	 l-p:0.15909023582935333
epoch£º803	 i:6 	 global-step:16066	 l-p:0.0997341051697731
epoch£º803	 i:7 	 global-step:16067	 l-p:0.15041780471801758
epoch£º803	 i:8 	 global-step:16068	 l-p:0.15969249606132507
epoch£º803	 i:9 	 global-step:16069	 l-p:0.1110793873667717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:804
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1465, 5.1439, 5.1464],
        [5.1465, 5.1114, 5.1395],
        [5.1465, 4.9007, 4.8046],
        [5.1465, 5.0171, 4.7156]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:804, step:0 
model_pd.l_p.mean(): 0.12521201372146606 
model_pd.l_d.mean(): -19.67293930053711 
model_pd.lagr.mean(): -19.547727584838867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5356], device='cuda:0')), ('power', tensor([-20.5956], device='cuda:0'))])
epoch£º804	 i:0 	 global-step:16080	 l-p:0.12521201372146606
epoch£º804	 i:1 	 global-step:16081	 l-p:0.15502886474132538
epoch£º804	 i:2 	 global-step:16082	 l-p:0.09988199174404144
epoch£º804	 i:3 	 global-step:16083	 l-p:0.13495215773582458
epoch£º804	 i:4 	 global-step:16084	 l-p:0.11869478225708008
epoch£º804	 i:5 	 global-step:16085	 l-p:0.11308187246322632
epoch£º804	 i:6 	 global-step:16086	 l-p:0.15501046180725098
epoch£º804	 i:7 	 global-step:16087	 l-p:0.12757575511932373
epoch£º804	 i:8 	 global-step:16088	 l-p:0.08594225347042084
epoch£º804	 i:9 	 global-step:16089	 l-p:0.1889045238494873
====================================================================================================
====================================================================================================
====================================================================================================

epoch:805
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6284e-01, 8.2143e-01,
         1.0000e+00, 7.8201e-01, 1.0000e+00, 9.5201e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1311, 5.1311, 5.1311],
        [5.1311, 5.1311, 5.1311],
        [5.1311, 5.4279, 5.2964],
        [5.1311, 5.0142, 5.0676]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:805, step:0 
model_pd.l_p.mean(): 0.12481395155191422 
model_pd.l_d.mean(): -19.311134338378906 
model_pd.lagr.mean(): -19.186321258544922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5284], device='cuda:0')), ('power', tensor([-20.2195], device='cuda:0'))])
epoch£º805	 i:0 	 global-step:16100	 l-p:0.12481395155191422
epoch£º805	 i:1 	 global-step:16101	 l-p:0.12768684327602386
epoch£º805	 i:2 	 global-step:16102	 l-p:0.16952843964099884
epoch£º805	 i:3 	 global-step:16103	 l-p:0.10431942343711853
epoch£º805	 i:4 	 global-step:16104	 l-p:0.1513073742389679
epoch£º805	 i:5 	 global-step:16105	 l-p:0.13876889646053314
epoch£º805	 i:6 	 global-step:16106	 l-p:0.1708471029996872
epoch£º805	 i:7 	 global-step:16107	 l-p:0.13213783502578735
epoch£º805	 i:8 	 global-step:16108	 l-p:0.12392181158065796
epoch£º805	 i:9 	 global-step:16109	 l-p:0.1334400624036789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:806
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1248, 5.1248, 5.1248],
        [5.1248, 5.1242, 5.1248],
        [5.1248, 5.1248, 5.1248],
        [5.1248, 5.1223, 5.1247]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:806, step:0 
model_pd.l_p.mean(): 0.07593537867069244 
model_pd.l_d.mean(): -20.205289840698242 
model_pd.lagr.mean(): -20.12935447692871 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4713], device='cuda:0')), ('power', tensor([-21.0713], device='cuda:0'))])
epoch£º806	 i:0 	 global-step:16120	 l-p:0.07593537867069244
epoch£º806	 i:1 	 global-step:16121	 l-p:0.17729133367538452
epoch£º806	 i:2 	 global-step:16122	 l-p:0.10770177841186523
epoch£º806	 i:3 	 global-step:16123	 l-p:0.1641339808702469
epoch£º806	 i:4 	 global-step:16124	 l-p:0.14371293783187866
epoch£º806	 i:5 	 global-step:16125	 l-p:0.13135002553462982
epoch£º806	 i:6 	 global-step:16126	 l-p:0.128078430891037
epoch£º806	 i:7 	 global-step:16127	 l-p:0.12538520991802216
epoch£º806	 i:8 	 global-step:16128	 l-p:0.18444328010082245
epoch£º806	 i:9 	 global-step:16129	 l-p:0.12961961328983307
====================================================================================================
====================================================================================================
====================================================================================================

epoch:807
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1350, 5.1344, 5.1350],
        [5.1350, 5.1345, 5.1350],
        [5.1350, 5.1337, 5.1350],
        [5.1350, 5.1350, 5.1350]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:807, step:0 
model_pd.l_p.mean(): 0.09483204036951065 
model_pd.l_d.mean(): -20.751081466674805 
model_pd.lagr.mean(): -20.65625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3969], device='cuda:0')), ('power', tensor([-21.5502], device='cuda:0'))])
epoch£º807	 i:0 	 global-step:16140	 l-p:0.09483204036951065
epoch£º807	 i:1 	 global-step:16141	 l-p:0.15448856353759766
epoch£º807	 i:2 	 global-step:16142	 l-p:0.14379383623600006
epoch£º807	 i:3 	 global-step:16143	 l-p:0.14987307786941528
epoch£º807	 i:4 	 global-step:16144	 l-p:0.1273222267627716
epoch£º807	 i:5 	 global-step:16145	 l-p:0.0773729532957077
epoch£º807	 i:6 	 global-step:16146	 l-p:0.12238922715187073
epoch£º807	 i:7 	 global-step:16147	 l-p:0.14866235852241516
epoch£º807	 i:8 	 global-step:16148	 l-p:0.12112145870923996
epoch£º807	 i:9 	 global-step:16149	 l-p:0.18491053581237793
====================================================================================================
====================================================================================================
====================================================================================================

epoch:808
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1418, 5.1363, 5.1415],
        [5.1418, 5.1286, 5.1405],
        [5.1418, 5.1418, 5.1418],
        [5.1418, 5.1376, 5.1416]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:808, step:0 
model_pd.l_p.mean(): 0.15356144309043884 
model_pd.l_d.mean(): -19.890281677246094 
model_pd.lagr.mean(): -19.73672103881836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4518], device='cuda:0')), ('power', tensor([-20.7301], device='cuda:0'))])
epoch£º808	 i:0 	 global-step:16160	 l-p:0.15356144309043884
epoch£º808	 i:1 	 global-step:16161	 l-p:0.12443605065345764
epoch£º808	 i:2 	 global-step:16162	 l-p:0.15199004113674164
epoch£º808	 i:3 	 global-step:16163	 l-p:0.1436011642217636
epoch£º808	 i:4 	 global-step:16164	 l-p:0.056958284229040146
epoch£º808	 i:5 	 global-step:16165	 l-p:0.16329321265220642
epoch£º808	 i:6 	 global-step:16166	 l-p:0.14394761621952057
epoch£º808	 i:7 	 global-step:16167	 l-p:0.1225501224398613
epoch£º808	 i:8 	 global-step:16168	 l-p:0.13741227984428406
epoch£º808	 i:9 	 global-step:16169	 l-p:0.10829678177833557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:809
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1510, 5.1510, 5.1510],
        [5.1510, 5.1505, 5.1510],
        [5.1510, 5.0906, 5.1325],
        [5.1510, 5.1509, 5.1510]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:809, step:0 
model_pd.l_p.mean(): 0.11540433764457703 
model_pd.l_d.mean(): -19.13822364807129 
model_pd.lagr.mean(): -19.02281951904297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5420], device='cuda:0')), ('power', tensor([-20.0575], device='cuda:0'))])
epoch£º809	 i:0 	 global-step:16180	 l-p:0.11540433764457703
epoch£º809	 i:1 	 global-step:16181	 l-p:0.10486151278018951
epoch£º809	 i:2 	 global-step:16182	 l-p:0.12697039544582367
epoch£º809	 i:3 	 global-step:16183	 l-p:0.1031496599316597
epoch£º809	 i:4 	 global-step:16184	 l-p:0.12104570120573044
epoch£º809	 i:5 	 global-step:16185	 l-p:0.12794288992881775
epoch£º809	 i:6 	 global-step:16186	 l-p:0.13917584717273712
epoch£º809	 i:7 	 global-step:16187	 l-p:0.15948441624641418
epoch£º809	 i:8 	 global-step:16188	 l-p:0.12399370223283768
epoch£º809	 i:9 	 global-step:16189	 l-p:0.13268667459487915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:810
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1711, 4.9819, 4.9992],
        [5.1711, 4.9273, 4.7232],
        [5.1711, 5.6676, 5.6678],
        [5.1711, 5.1691, 5.1711]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:810, step:0 
model_pd.l_p.mean(): 0.17279407382011414 
model_pd.l_d.mean(): -19.71430206298828 
model_pd.lagr.mean(): -19.541507720947266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4547], device='cuda:0')), ('power', tensor([-20.5539], device='cuda:0'))])
epoch£º810	 i:0 	 global-step:16200	 l-p:0.17279407382011414
epoch£º810	 i:1 	 global-step:16201	 l-p:0.15756496787071228
epoch£º810	 i:2 	 global-step:16202	 l-p:0.0887560024857521
epoch£º810	 i:3 	 global-step:16203	 l-p:0.1038290411233902
epoch£º810	 i:4 	 global-step:16204	 l-p:0.12129409611225128
epoch£º810	 i:5 	 global-step:16205	 l-p:0.07079792022705078
epoch£º810	 i:6 	 global-step:16206	 l-p:0.1346496343612671
epoch£º810	 i:7 	 global-step:16207	 l-p:0.11443688720464706
epoch£º810	 i:8 	 global-step:16208	 l-p:0.12291193008422852
epoch£º810	 i:9 	 global-step:16209	 l-p:0.1351516842842102
====================================================================================================
====================================================================================================
====================================================================================================

epoch:811
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2317e-01, 2.2177e-01,
         1.0000e+00, 1.5219e-01, 1.0000e+00, 6.8624e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1663, 4.9152, 4.7695],
        [5.1663, 5.0177, 4.7193],
        [5.1663, 5.0768, 5.1280],
        [5.1663, 5.0266, 5.0756]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:811, step:0 
model_pd.l_p.mean(): 0.05136285349726677 
model_pd.l_d.mean(): -19.398117065429688 
model_pd.lagr.mean(): -19.34675407409668 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5376], device='cuda:0')), ('power', tensor([-20.3176], device='cuda:0'))])
epoch£º811	 i:0 	 global-step:16220	 l-p:0.05136285349726677
epoch£º811	 i:1 	 global-step:16221	 l-p:0.15971289575099945
epoch£º811	 i:2 	 global-step:16222	 l-p:0.10452641546726227
epoch£º811	 i:3 	 global-step:16223	 l-p:0.12329146265983582
epoch£º811	 i:4 	 global-step:16224	 l-p:0.1601022183895111
epoch£º811	 i:5 	 global-step:16225	 l-p:0.15737301111221313
epoch£º811	 i:6 	 global-step:16226	 l-p:0.126674085855484
epoch£º811	 i:7 	 global-step:16227	 l-p:0.10864309221506119
epoch£º811	 i:8 	 global-step:16228	 l-p:0.0909876897931099
epoch£º811	 i:9 	 global-step:16229	 l-p:0.1332547664642334
====================================================================================================
====================================================================================================
====================================================================================================

epoch:812
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1753, 5.1691, 5.1749],
        [5.1753, 5.1535, 5.1722],
        [5.1753, 5.3662, 5.1706],
        [5.1753, 5.1497, 5.1712]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:812, step:0 
model_pd.l_p.mean(): 0.024212364107370377 
model_pd.l_d.mean(): -20.537601470947266 
model_pd.lagr.mean(): -20.513389587402344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4305], device='cuda:0')), ('power', tensor([-21.3675], device='cuda:0'))])
epoch£º812	 i:0 	 global-step:16240	 l-p:0.024212364107370377
epoch£º812	 i:1 	 global-step:16241	 l-p:0.11678729951381683
epoch£º812	 i:2 	 global-step:16242	 l-p:0.10443154722452164
epoch£º812	 i:3 	 global-step:16243	 l-p:0.13510923087596893
epoch£º812	 i:4 	 global-step:16244	 l-p:0.1540950983762741
epoch£º812	 i:5 	 global-step:16245	 l-p:0.14648863673210144
epoch£º812	 i:6 	 global-step:16246	 l-p:0.11682233214378357
epoch£º812	 i:7 	 global-step:16247	 l-p:0.1287628710269928
epoch£º812	 i:8 	 global-step:16248	 l-p:0.13291804492473602
epoch£º812	 i:9 	 global-step:16249	 l-p:0.12353929132223129
====================================================================================================
====================================================================================================
====================================================================================================

epoch:813
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1660, 5.0592, 5.1125],
        [5.1660, 4.9323, 4.8776],
        [5.1660, 4.9427, 4.9120],
        [5.1660, 5.1660, 5.1660]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:813, step:0 
model_pd.l_p.mean(): 0.14063628017902374 
model_pd.l_d.mean(): -19.653921127319336 
model_pd.lagr.mean(): -19.51328468322754 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4681], device='cuda:0')), ('power', tensor([-20.5062], device='cuda:0'))])
epoch£º813	 i:0 	 global-step:16260	 l-p:0.14063628017902374
epoch£º813	 i:1 	 global-step:16261	 l-p:0.09629664570093155
epoch£º813	 i:2 	 global-step:16262	 l-p:0.12569177150726318
epoch£º813	 i:3 	 global-step:16263	 l-p:0.12304019927978516
epoch£º813	 i:4 	 global-step:16264	 l-p:0.18669429421424866
epoch£º813	 i:5 	 global-step:16265	 l-p:0.10064170509576797
epoch£º813	 i:6 	 global-step:16266	 l-p:0.10624820739030838
epoch£º813	 i:7 	 global-step:16267	 l-p:0.14036498963832855
epoch£º813	 i:8 	 global-step:16268	 l-p:0.1701904982328415
epoch£º813	 i:9 	 global-step:16269	 l-p:0.11425068229436874
====================================================================================================
====================================================================================================
====================================================================================================

epoch:814
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1226, 5.1226, 5.1226],
        [5.1226, 5.1226, 5.1226],
        [5.1226, 5.0780, 5.1119],
        [5.1226, 5.1226, 5.1226]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:814, step:0 
model_pd.l_p.mean(): 0.1349143087863922 
model_pd.l_d.mean(): -20.554075241088867 
model_pd.lagr.mean(): -20.419160842895508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4079], device='cuda:0')), ('power', tensor([-21.3609], device='cuda:0'))])
epoch£º814	 i:0 	 global-step:16280	 l-p:0.1349143087863922
epoch£º814	 i:1 	 global-step:16281	 l-p:0.1993762105703354
epoch£º814	 i:2 	 global-step:16282	 l-p:0.12189529091119766
epoch£º814	 i:3 	 global-step:16283	 l-p:0.1075938269495964
epoch£º814	 i:4 	 global-step:16284	 l-p:0.1467435359954834
epoch£º814	 i:5 	 global-step:16285	 l-p:0.18541739881038666
epoch£º814	 i:6 	 global-step:16286	 l-p:0.13798651099205017
epoch£º814	 i:7 	 global-step:16287	 l-p:0.09627002477645874
epoch£º814	 i:8 	 global-step:16288	 l-p:0.13098010420799255
epoch£º814	 i:9 	 global-step:16289	 l-p:0.17104236781597137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:815
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0997, 5.0997, 5.0997],
        [5.0997, 5.2672, 5.0599],
        [5.0997, 5.0146, 5.0654],
        [5.0997, 4.9952, 5.0492]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:815, step:0 
model_pd.l_p.mean(): 0.11027571558952332 
model_pd.l_d.mean(): -18.896352767944336 
model_pd.lagr.mean(): -18.78607749938965 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5735], device='cuda:0')), ('power', tensor([-19.8437], device='cuda:0'))])
epoch£º815	 i:0 	 global-step:16300	 l-p:0.11027571558952332
epoch£º815	 i:1 	 global-step:16301	 l-p:0.09415765851736069
epoch£º815	 i:2 	 global-step:16302	 l-p:0.13989655673503876
epoch£º815	 i:3 	 global-step:16303	 l-p:0.2152795046567917
epoch£º815	 i:4 	 global-step:16304	 l-p:0.12125209718942642
epoch£º815	 i:5 	 global-step:16305	 l-p:0.128871351480484
epoch£º815	 i:6 	 global-step:16306	 l-p:0.18318220973014832
epoch£º815	 i:7 	 global-step:16307	 l-p:0.13635273277759552
epoch£º815	 i:8 	 global-step:16308	 l-p:0.1683601289987564
epoch£º815	 i:9 	 global-step:16309	 l-p:0.2226835936307907
====================================================================================================
====================================================================================================
====================================================================================================

epoch:816
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0948, 5.0120, 5.0623],
        [5.0948, 4.9495, 4.9990],
        [5.0948, 5.0866, 5.0942],
        [5.0948, 5.0499, 5.0840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:816, step:0 
model_pd.l_p.mean(): 0.15336349606513977 
model_pd.l_d.mean(): -20.7087459564209 
model_pd.lagr.mean(): -20.555381774902344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3994], device='cuda:0')), ('power', tensor([-21.5097], device='cuda:0'))])
epoch£º816	 i:0 	 global-step:16320	 l-p:0.15336349606513977
epoch£º816	 i:1 	 global-step:16321	 l-p:0.13183458149433136
epoch£º816	 i:2 	 global-step:16322	 l-p:0.130961611866951
epoch£º816	 i:3 	 global-step:16323	 l-p:0.07371871918439865
epoch£º816	 i:4 	 global-step:16324	 l-p:0.26809173822402954
epoch£º816	 i:5 	 global-step:16325	 l-p:0.12091316282749176
epoch£º816	 i:6 	 global-step:16326	 l-p:0.20874544978141785
epoch£º816	 i:7 	 global-step:16327	 l-p:0.2572746276855469
epoch£º816	 i:8 	 global-step:16328	 l-p:0.13048681616783142
epoch£º816	 i:9 	 global-step:16329	 l-p:0.12983398139476776
====================================================================================================
====================================================================================================
====================================================================================================

epoch:817
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0843, 5.2594, 5.0564],
        [5.0843, 5.0842, 5.0843],
        [5.0843, 5.0832, 5.0843],
        [5.0843, 4.8272, 4.6262]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:817, step:0 
model_pd.l_p.mean(): 0.09589778631925583 
model_pd.l_d.mean(): -18.484004974365234 
model_pd.lagr.mean(): -18.388107299804688 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6030], device='cuda:0')), ('power', tensor([-19.4541], device='cuda:0'))])
epoch£º817	 i:0 	 global-step:16340	 l-p:0.09589778631925583
epoch£º817	 i:1 	 global-step:16341	 l-p:0.15255461633205414
epoch£º817	 i:2 	 global-step:16342	 l-p:0.11346092075109482
epoch£º817	 i:3 	 global-step:16343	 l-p:0.17155173420906067
epoch£º817	 i:4 	 global-step:16344	 l-p:0.11691141128540039
epoch£º817	 i:5 	 global-step:16345	 l-p:0.2276884913444519
epoch£º817	 i:6 	 global-step:16346	 l-p:0.1318996399641037
epoch£º817	 i:7 	 global-step:16347	 l-p:0.23571254312992096
epoch£º817	 i:8 	 global-step:16348	 l-p:0.16844996809959412
epoch£º817	 i:9 	 global-step:16349	 l-p:0.11850994825363159
====================================================================================================
====================================================================================================
====================================================================================================

epoch:818
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1074, 5.3725, 5.2209],
        [5.1074, 5.2637, 5.0501],
        [5.1074, 5.1074, 5.1074],
        [5.1074, 5.0347, 5.0818]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:818, step:0 
model_pd.l_p.mean(): 0.10322673618793488 
model_pd.l_d.mean(): -19.107032775878906 
model_pd.lagr.mean(): -19.00380516052246 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5811], device='cuda:0')), ('power', tensor([-20.0661], device='cuda:0'))])
epoch£º818	 i:0 	 global-step:16360	 l-p:0.10322673618793488
epoch£º818	 i:1 	 global-step:16361	 l-p:0.14796143770217896
epoch£º818	 i:2 	 global-step:16362	 l-p:0.23640649020671844
epoch£º818	 i:3 	 global-step:16363	 l-p:0.1321076899766922
epoch£º818	 i:4 	 global-step:16364	 l-p:0.19017180800437927
epoch£º818	 i:5 	 global-step:16365	 l-p:0.10067413747310638
epoch£º818	 i:6 	 global-step:16366	 l-p:0.08232025057077408
epoch£º818	 i:7 	 global-step:16367	 l-p:0.12759679555892944
epoch£º818	 i:8 	 global-step:16368	 l-p:0.12537631392478943
epoch£º818	 i:9 	 global-step:16369	 l-p:0.16027015447616577
====================================================================================================
====================================================================================================
====================================================================================================

epoch:819
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2455e-01, 6.2201e-02,
         1.0000e+00, 3.1063e-02, 1.0000e+00, 4.9940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1351, 5.0174, 5.0713],
        [5.1351, 4.8919, 4.6648],
        [5.1351, 5.1303, 5.1349],
        [5.1351, 5.1903, 4.9275]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:819, step:0 
model_pd.l_p.mean(): 0.12304069846868515 
model_pd.l_d.mean(): -20.703083038330078 
model_pd.lagr.mean(): -20.580041885375977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4199], device='cuda:0')), ('power', tensor([-21.5251], device='cuda:0'))])
epoch£º819	 i:0 	 global-step:16380	 l-p:0.12304069846868515
epoch£º819	 i:1 	 global-step:16381	 l-p:0.20167924463748932
epoch£º819	 i:2 	 global-step:16382	 l-p:0.10581647604703903
epoch£º819	 i:3 	 global-step:16383	 l-p:0.11524985730648041
epoch£º819	 i:4 	 global-step:16384	 l-p:0.18374735116958618
epoch£º819	 i:5 	 global-step:16385	 l-p:0.12454262375831604
epoch£º819	 i:6 	 global-step:16386	 l-p:0.10887978971004486
epoch£º819	 i:7 	 global-step:16387	 l-p:0.16906732320785522
epoch£º819	 i:8 	 global-step:16388	 l-p:0.16451026499271393
epoch£º819	 i:9 	 global-step:16389	 l-p:0.07971934974193573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:820
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1175, 4.8872, 4.6308],
        [5.1175, 5.1162, 5.1175],
        [5.1175, 4.8611, 4.6776],
        [5.1175, 5.1176, 5.1176]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:820, step:0 
model_pd.l_p.mean(): 0.19182811677455902 
model_pd.l_d.mean(): -19.397489547729492 
model_pd.lagr.mean(): -19.20566177368164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4977], device='cuda:0')), ('power', tensor([-20.2757], device='cuda:0'))])
epoch£º820	 i:0 	 global-step:16400	 l-p:0.19182811677455902
epoch£º820	 i:1 	 global-step:16401	 l-p:0.11193637549877167
epoch£º820	 i:2 	 global-step:16402	 l-p:0.14611199498176575
epoch£º820	 i:3 	 global-step:16403	 l-p:0.12675483524799347
epoch£º820	 i:4 	 global-step:16404	 l-p:0.13287080824375153
epoch£º820	 i:5 	 global-step:16405	 l-p:0.0976540744304657
epoch£º820	 i:6 	 global-step:16406	 l-p:0.17454278469085693
epoch£º820	 i:7 	 global-step:16407	 l-p:0.1305021047592163
epoch£º820	 i:8 	 global-step:16408	 l-p:0.16711410880088806
epoch£º820	 i:9 	 global-step:16409	 l-p:0.09997719526290894
====================================================================================================
====================================================================================================
====================================================================================================

epoch:821
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1389, 4.9853, 5.0306],
        [5.1389, 5.0541, 5.1047],
        [5.1389, 4.9736, 5.0128],
        [5.1389, 4.8843, 4.7553]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:821, step:0 
model_pd.l_p.mean(): 0.1098814532160759 
model_pd.l_d.mean(): -20.800193786621094 
model_pd.lagr.mean(): -20.690311431884766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3898], device='cuda:0')), ('power', tensor([-21.5929], device='cuda:0'))])
epoch£º821	 i:0 	 global-step:16420	 l-p:0.1098814532160759
epoch£º821	 i:1 	 global-step:16421	 l-p:0.15852390229701996
epoch£º821	 i:2 	 global-step:16422	 l-p:0.11051937192678452
epoch£º821	 i:3 	 global-step:16423	 l-p:0.14271460473537445
epoch£º821	 i:4 	 global-step:16424	 l-p:0.15841636061668396
epoch£º821	 i:5 	 global-step:16425	 l-p:0.09601263701915741
epoch£º821	 i:6 	 global-step:16426	 l-p:0.17529448866844177
epoch£º821	 i:7 	 global-step:16427	 l-p:0.12256364524364471
epoch£º821	 i:8 	 global-step:16428	 l-p:0.12063619494438171
epoch£º821	 i:9 	 global-step:16429	 l-p:0.09548435360193253
====================================================================================================
====================================================================================================
====================================================================================================

epoch:822
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1658, 5.1583, 5.1653],
        [5.1658, 5.1042, 4.8046],
        [5.1658, 5.0524, 4.7483],
        [5.1658, 5.1655, 5.1658]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:822, step:0 
model_pd.l_p.mean(): 0.1323283314704895 
model_pd.l_d.mean(): -19.898473739624023 
model_pd.lagr.mean(): -19.766145706176758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4650], device='cuda:0')), ('power', tensor([-20.7522], device='cuda:0'))])
epoch£º822	 i:0 	 global-step:16440	 l-p:0.1323283314704895
epoch£º822	 i:1 	 global-step:16441	 l-p:0.07909469306468964
epoch£º822	 i:2 	 global-step:16442	 l-p:0.14739269018173218
epoch£º822	 i:3 	 global-step:16443	 l-p:0.08370621502399445
epoch£º822	 i:4 	 global-step:16444	 l-p:0.12047895044088364
epoch£º822	 i:5 	 global-step:16445	 l-p:0.12781047821044922
epoch£º822	 i:6 	 global-step:16446	 l-p:0.1140797957777977
epoch£º822	 i:7 	 global-step:16447	 l-p:0.09827179461717606
epoch£º822	 i:8 	 global-step:16448	 l-p:0.13472488522529602
epoch£º822	 i:9 	 global-step:16449	 l-p:0.1865745186805725
====================================================================================================
====================================================================================================
====================================================================================================

epoch:823
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1686, 5.0828, 5.1334],
        [5.1686, 5.1686, 5.1686],
        [5.1686, 4.9444, 4.9139],
        [5.1686, 5.4834, 5.3604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:823, step:0 
model_pd.l_p.mean(): 0.2049321085214615 
model_pd.l_d.mean(): -19.98770523071289 
model_pd.lagr.mean(): -19.782773971557617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5186], device='cuda:0')), ('power', tensor([-20.8986], device='cuda:0'))])
epoch£º823	 i:0 	 global-step:16460	 l-p:0.2049321085214615
epoch£º823	 i:1 	 global-step:16461	 l-p:0.14467377960681915
epoch£º823	 i:2 	 global-step:16462	 l-p:0.11828716099262238
epoch£º823	 i:3 	 global-step:16463	 l-p:0.16250427067279816
epoch£º823	 i:4 	 global-step:16464	 l-p:0.13065040111541748
epoch£º823	 i:5 	 global-step:16465	 l-p:0.09620731323957443
epoch£º823	 i:6 	 global-step:16466	 l-p:0.046265799552202225
epoch£º823	 i:7 	 global-step:16467	 l-p:0.1386178433895111
epoch£º823	 i:8 	 global-step:16468	 l-p:0.08841011673212051
epoch£º823	 i:9 	 global-step:16469	 l-p:0.11701442301273346
====================================================================================================
====================================================================================================
====================================================================================================

epoch:824
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1614, 4.9074, 4.7558],
        [5.1614, 5.1356, 5.1573],
        [5.1614, 5.0567, 5.1103],
        [5.1614, 5.1614, 5.1614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:824, step:0 
model_pd.l_p.mean(): 0.10936596989631653 
model_pd.l_d.mean(): -20.160245895385742 
model_pd.lagr.mean(): -20.050880432128906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4749], device='cuda:0')), ('power', tensor([-21.0291], device='cuda:0'))])
epoch£º824	 i:0 	 global-step:16480	 l-p:0.10936596989631653
epoch£º824	 i:1 	 global-step:16481	 l-p:0.10976649820804596
epoch£º824	 i:2 	 global-step:16482	 l-p:0.16063344478607178
epoch£º824	 i:3 	 global-step:16483	 l-p:0.12259132415056229
epoch£º824	 i:4 	 global-step:16484	 l-p:0.14036843180656433
epoch£º824	 i:5 	 global-step:16485	 l-p:0.05013386160135269
epoch£º824	 i:6 	 global-step:16486	 l-p:0.133785679936409
epoch£º824	 i:7 	 global-step:16487	 l-p:0.1448352336883545
epoch£º824	 i:8 	 global-step:16488	 l-p:0.13885186612606049
epoch£º824	 i:9 	 global-step:16489	 l-p:0.15064173936843872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:825
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1493, 5.5300, 5.4500],
        [5.1493, 5.1488, 5.1493],
        [5.1493, 5.1493, 5.1493],
        [5.1493, 5.1083, 5.1401]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:825, step:0 
model_pd.l_p.mean(): 0.0875537097454071 
model_pd.l_d.mean(): -19.656574249267578 
model_pd.lagr.mean(): -19.569021224975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5070], device='cuda:0')), ('power', tensor([-20.5492], device='cuda:0'))])
epoch£º825	 i:0 	 global-step:16500	 l-p:0.0875537097454071
epoch£º825	 i:1 	 global-step:16501	 l-p:0.11819053441286087
epoch£º825	 i:2 	 global-step:16502	 l-p:0.13352015614509583
epoch£º825	 i:3 	 global-step:16503	 l-p:0.12877123057842255
epoch£º825	 i:4 	 global-step:16504	 l-p:0.1592431515455246
epoch£º825	 i:5 	 global-step:16505	 l-p:0.12330281734466553
epoch£º825	 i:6 	 global-step:16506	 l-p:0.1266852766275406
epoch£º825	 i:7 	 global-step:16507	 l-p:0.15339353680610657
epoch£º825	 i:8 	 global-step:16508	 l-p:0.1562974452972412
epoch£º825	 i:9 	 global-step:16509	 l-p:0.14336779713630676
====================================================================================================
====================================================================================================
====================================================================================================

epoch:826
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1364, 5.1327, 5.1362],
        [5.1364, 5.1264, 5.1355],
        [5.1364, 5.1336, 5.1363],
        [5.1364, 4.8870, 4.7952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:826, step:0 
model_pd.l_p.mean(): 0.1262156218290329 
model_pd.l_d.mean(): -20.520727157592773 
model_pd.lagr.mean(): -20.394512176513672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4220], device='cuda:0')), ('power', tensor([-21.3415], device='cuda:0'))])
epoch£º826	 i:0 	 global-step:16520	 l-p:0.1262156218290329
epoch£º826	 i:1 	 global-step:16521	 l-p:0.1726800501346588
epoch£º826	 i:2 	 global-step:16522	 l-p:0.1374320238828659
epoch£º826	 i:3 	 global-step:16523	 l-p:0.08158302307128906
epoch£º826	 i:4 	 global-step:16524	 l-p:0.101454958319664
epoch£º826	 i:5 	 global-step:16525	 l-p:0.12502732872962952
epoch£º826	 i:6 	 global-step:16526	 l-p:0.13852526247501373
epoch£º826	 i:7 	 global-step:16527	 l-p:0.14625082910060883
epoch£º826	 i:8 	 global-step:16528	 l-p:0.14212344586849213
epoch£º826	 i:9 	 global-step:16529	 l-p:0.1417333334684372
====================================================================================================
====================================================================================================
====================================================================================================

epoch:827
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1551, 5.1543, 5.1551],
        [5.1551, 5.1140, 5.1459],
        [5.1551, 5.1551, 5.1551],
        [5.1551, 5.0034, 5.0493]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:827, step:0 
model_pd.l_p.mean(): 0.12474801391363144 
model_pd.l_d.mean(): -20.209808349609375 
model_pd.lagr.mean(): -20.085060119628906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4472], device='cuda:0')), ('power', tensor([-21.0508], device='cuda:0'))])
epoch£º827	 i:0 	 global-step:16540	 l-p:0.12474801391363144
epoch£º827	 i:1 	 global-step:16541	 l-p:0.136741504073143
epoch£º827	 i:2 	 global-step:16542	 l-p:0.11341151595115662
epoch£º827	 i:3 	 global-step:16543	 l-p:0.09180442243814468
epoch£º827	 i:4 	 global-step:16544	 l-p:0.16367122530937195
epoch£º827	 i:5 	 global-step:16545	 l-p:0.1790764182806015
epoch£º827	 i:6 	 global-step:16546	 l-p:0.07941162586212158
epoch£º827	 i:7 	 global-step:16547	 l-p:0.13501639664173126
epoch£º827	 i:8 	 global-step:16548	 l-p:0.12790937721729279
epoch£º827	 i:9 	 global-step:16549	 l-p:0.11540819704532623
====================================================================================================
====================================================================================================
====================================================================================================

epoch:828
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1698, 5.0563, 5.1101],
        [5.1698, 4.9755, 4.9899],
        [5.1698, 5.1244, 5.1587],
        [5.1698, 5.1305, 5.1612]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:828, step:0 
model_pd.l_p.mean(): 0.11812322586774826 
model_pd.l_d.mean(): -20.297679901123047 
model_pd.lagr.mean(): -20.179555892944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4344], device='cuda:0')), ('power', tensor([-21.1272], device='cuda:0'))])
epoch£º828	 i:0 	 global-step:16560	 l-p:0.11812322586774826
epoch£º828	 i:1 	 global-step:16561	 l-p:0.0614047572016716
epoch£º828	 i:2 	 global-step:16562	 l-p:0.0644344687461853
epoch£º828	 i:3 	 global-step:16563	 l-p:0.12619680166244507
epoch£º828	 i:4 	 global-step:16564	 l-p:0.15031351149082184
epoch£º828	 i:5 	 global-step:16565	 l-p:0.16908782720565796
epoch£º828	 i:6 	 global-step:16566	 l-p:0.14330050349235535
epoch£º828	 i:7 	 global-step:16567	 l-p:0.13746002316474915
epoch£º828	 i:8 	 global-step:16568	 l-p:0.12369798123836517
epoch£º828	 i:9 	 global-step:16569	 l-p:0.12005650997161865
====================================================================================================
====================================================================================================
====================================================================================================

epoch:829
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1785, 5.1029, 5.1507],
        [5.1785, 5.0927, 4.7897],
        [5.1785, 5.1784, 5.1785],
        [5.1785, 5.1779, 5.1785]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:829, step:0 
model_pd.l_p.mean(): 0.0728076621890068 
model_pd.l_d.mean(): -19.446487426757812 
model_pd.lagr.mean(): -19.373680114746094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4875], device='cuda:0')), ('power', tensor([-20.3150], device='cuda:0'))])
epoch£º829	 i:0 	 global-step:16580	 l-p:0.0728076621890068
epoch£º829	 i:1 	 global-step:16581	 l-p:0.10733085125684738
epoch£º829	 i:2 	 global-step:16582	 l-p:0.13286761939525604
epoch£º829	 i:3 	 global-step:16583	 l-p:0.05158614367246628
epoch£º829	 i:4 	 global-step:16584	 l-p:0.14698763191699982
epoch£º829	 i:5 	 global-step:16585	 l-p:0.10858757048845291
epoch£º829	 i:6 	 global-step:16586	 l-p:0.19289997220039368
epoch£º829	 i:7 	 global-step:16587	 l-p:0.16003261506557465
epoch£º829	 i:8 	 global-step:16588	 l-p:0.12989148497581482
epoch£º829	 i:9 	 global-step:16589	 l-p:0.08938755095005035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:830
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1715, 5.1709, 5.1715],
        [5.1715, 4.9483, 4.9212],
        [5.1715, 4.9182, 4.7815],
        [5.1715, 5.1715, 5.1715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:830, step:0 
model_pd.l_p.mean(): 0.14325600862503052 
model_pd.l_d.mean(): -20.457244873046875 
model_pd.lagr.mean(): -20.313989639282227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4279], device='cuda:0')), ('power', tensor([-21.2830], device='cuda:0'))])
epoch£º830	 i:0 	 global-step:16600	 l-p:0.14325600862503052
epoch£º830	 i:1 	 global-step:16601	 l-p:0.10501081496477127
epoch£º830	 i:2 	 global-step:16602	 l-p:0.13562801480293274
epoch£º830	 i:3 	 global-step:16603	 l-p:0.08318155258893967
epoch£º830	 i:4 	 global-step:16604	 l-p:0.12851077318191528
epoch£º830	 i:5 	 global-step:16605	 l-p:0.14705872535705566
epoch£º830	 i:6 	 global-step:16606	 l-p:0.1659736931324005
epoch£º830	 i:7 	 global-step:16607	 l-p:0.07793086767196655
epoch£º830	 i:8 	 global-step:16608	 l-p:0.20254482328891754
epoch£º830	 i:9 	 global-step:16609	 l-p:0.11669059842824936
====================================================================================================
====================================================================================================
====================================================================================================

epoch:831
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1337, 5.1337, 5.1337],
        [5.1337, 5.1310, 5.1336],
        [5.1337, 5.1190, 5.1321],
        [5.1337, 5.1337, 5.1337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:831, step:0 
model_pd.l_p.mean(): 0.13453452289104462 
model_pd.l_d.mean(): -19.885013580322266 
model_pd.lagr.mean(): -19.750478744506836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4696], device='cuda:0')), ('power', tensor([-20.7432], device='cuda:0'))])
epoch£º831	 i:0 	 global-step:16620	 l-p:0.13453452289104462
epoch£º831	 i:1 	 global-step:16621	 l-p:0.0979967787861824
epoch£º831	 i:2 	 global-step:16622	 l-p:0.13734254240989685
epoch£º831	 i:3 	 global-step:16623	 l-p:0.164183109998703
epoch£º831	 i:4 	 global-step:16624	 l-p:0.08715829998254776
epoch£º831	 i:5 	 global-step:16625	 l-p:0.15184304118156433
epoch£º831	 i:6 	 global-step:16626	 l-p:0.16169074177742004
epoch£º831	 i:7 	 global-step:16627	 l-p:0.14981898665428162
epoch£º831	 i:8 	 global-step:16628	 l-p:0.14297376573085785
epoch£º831	 i:9 	 global-step:16629	 l-p:0.15396980941295624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:832
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1212, 5.1170, 5.1210],
        [5.1212, 4.8928, 4.8635],
        [5.1212, 4.9613, 4.6569],
        [5.1212, 4.8927, 4.8632]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:832, step:0 
model_pd.l_p.mean(): 0.10099870711565018 
model_pd.l_d.mean(): -20.09871482849121 
model_pd.lagr.mean(): -19.997716903686523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4715], device='cuda:0')), ('power', tensor([-20.9629], device='cuda:0'))])
epoch£º832	 i:0 	 global-step:16640	 l-p:0.10099870711565018
epoch£º832	 i:1 	 global-step:16641	 l-p:0.14672407507896423
epoch£º832	 i:2 	 global-step:16642	 l-p:0.09342644363641739
epoch£º832	 i:3 	 global-step:16643	 l-p:0.08439666777849197
epoch£º832	 i:4 	 global-step:16644	 l-p:0.13090668618679047
epoch£º832	 i:5 	 global-step:16645	 l-p:0.1707676500082016
epoch£º832	 i:6 	 global-step:16646	 l-p:0.2706962823867798
epoch£º832	 i:7 	 global-step:16647	 l-p:0.1592356562614441
epoch£º832	 i:8 	 global-step:16648	 l-p:0.1460660696029663
epoch£º832	 i:9 	 global-step:16649	 l-p:0.1806468516588211
====================================================================================================
====================================================================================================
====================================================================================================

epoch:833
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1061, 5.1061, 5.1061],
        [5.1061, 5.0990, 5.1057],
        [5.1061, 4.9936, 5.0484],
        [5.1061, 5.1023, 5.1059]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:833, step:0 
model_pd.l_p.mean(): 0.13403622806072235 
model_pd.l_d.mean(): -19.64002227783203 
model_pd.lagr.mean(): -19.505985260009766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5324], device='cuda:0')), ('power', tensor([-20.5587], device='cuda:0'))])
epoch£º833	 i:0 	 global-step:16660	 l-p:0.13403622806072235
epoch£º833	 i:1 	 global-step:16661	 l-p:0.1480972319841385
epoch£º833	 i:2 	 global-step:16662	 l-p:0.13521529734134674
epoch£º833	 i:3 	 global-step:16663	 l-p:0.13824768364429474
epoch£º833	 i:4 	 global-step:16664	 l-p:0.10167919844388962
epoch£º833	 i:5 	 global-step:16665	 l-p:0.11865215748548508
epoch£º833	 i:6 	 global-step:16666	 l-p:0.11117567121982574
epoch£º833	 i:7 	 global-step:16667	 l-p:0.07843087613582611
epoch£º833	 i:8 	 global-step:16668	 l-p:0.2279035896062851
epoch£º833	 i:9 	 global-step:16669	 l-p:0.34121599793434143
====================================================================================================
====================================================================================================
====================================================================================================

epoch:834
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0959, 5.0959, 5.0959],
        [5.0959, 5.4915, 5.4223],
        [5.0959, 5.0959, 5.0959],
        [5.0959, 4.9399, 4.9863]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:834, step:0 
model_pd.l_p.mean(): 0.15764804184436798 
model_pd.l_d.mean(): -19.58579444885254 
model_pd.lagr.mean(): -19.428146362304688 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5807], device='cuda:0')), ('power', tensor([-20.5535], device='cuda:0'))])
epoch£º834	 i:0 	 global-step:16680	 l-p:0.15764804184436798
epoch£º834	 i:1 	 global-step:16681	 l-p:0.12473416328430176
epoch£º834	 i:2 	 global-step:16682	 l-p:0.1189071461558342
epoch£º834	 i:3 	 global-step:16683	 l-p:0.12877558171749115
epoch£º834	 i:4 	 global-step:16684	 l-p:0.30416354537010193
epoch£º834	 i:5 	 global-step:16685	 l-p:0.10497302561998367
epoch£º834	 i:6 	 global-step:16686	 l-p:0.2515624463558197
epoch£º834	 i:7 	 global-step:16687	 l-p:0.11072631925344467
epoch£º834	 i:8 	 global-step:16688	 l-p:0.09904149919748306
epoch£º834	 i:9 	 global-step:16689	 l-p:0.1445794254541397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:835
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0985, 5.0983, 5.0985],
        [5.0985, 4.9758, 5.0305],
        [5.0985, 4.8382, 4.6454],
        [5.0985, 4.9010, 4.6074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:835, step:0 
model_pd.l_p.mean(): 0.08016004413366318 
model_pd.l_d.mean(): -19.717819213867188 
model_pd.lagr.mean(): -19.637659072875977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4997], device='cuda:0')), ('power', tensor([-20.6041], device='cuda:0'))])
epoch£º835	 i:0 	 global-step:16700	 l-p:0.08016004413366318
epoch£º835	 i:1 	 global-step:16701	 l-p:0.14953213930130005
epoch£º835	 i:2 	 global-step:16702	 l-p:0.11942481994628906
epoch£º835	 i:3 	 global-step:16703	 l-p:0.21934133768081665
epoch£º835	 i:4 	 global-step:16704	 l-p:0.23261012136936188
epoch£º835	 i:5 	 global-step:16705	 l-p:0.13301444053649902
epoch£º835	 i:6 	 global-step:16706	 l-p:0.10042870044708252
epoch£º835	 i:7 	 global-step:16707	 l-p:0.17900584638118744
epoch£º835	 i:8 	 global-step:16708	 l-p:0.13907963037490845
epoch£º835	 i:9 	 global-step:16709	 l-p:0.13216423988342285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:836
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1126, 5.0996, 5.1113],
        [5.1126, 5.1203, 4.8371],
        [5.1126, 5.1116, 5.1126],
        [5.1126, 5.0063, 5.0608]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:836, step:0 
model_pd.l_p.mean(): 0.1484864503145218 
model_pd.l_d.mean(): -20.412174224853516 
model_pd.lagr.mean(): -20.263687133789062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4650], device='cuda:0')), ('power', tensor([-21.2755], device='cuda:0'))])
epoch£º836	 i:0 	 global-step:16720	 l-p:0.1484864503145218
epoch£º836	 i:1 	 global-step:16721	 l-p:0.1366894692182541
epoch£º836	 i:2 	 global-step:16722	 l-p:0.11871596425771713
epoch£º836	 i:3 	 global-step:16723	 l-p:0.10308574140071869
epoch£º836	 i:4 	 global-step:16724	 l-p:0.16921192407608032
epoch£º836	 i:5 	 global-step:16725	 l-p:0.1323947161436081
epoch£º836	 i:6 	 global-step:16726	 l-p:0.12647004425525665
epoch£º836	 i:7 	 global-step:16727	 l-p:0.14340268075466156
epoch£º836	 i:8 	 global-step:16728	 l-p:0.13343358039855957
epoch£º836	 i:9 	 global-step:16729	 l-p:0.1845957487821579
====================================================================================================
====================================================================================================
====================================================================================================

epoch:837
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1284, 5.0571, 5.1038],
        [5.1284, 5.1282, 5.1284],
        [5.1284, 5.0282, 4.7198],
        [5.1284, 4.9088, 4.8947]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:837, step:0 
model_pd.l_p.mean(): 0.1866212785243988 
model_pd.l_d.mean(): -20.731224060058594 
model_pd.lagr.mean(): -20.54460334777832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4216], device='cuda:0')), ('power', tensor([-21.5555], device='cuda:0'))])
epoch£º837	 i:0 	 global-step:16740	 l-p:0.1866212785243988
epoch£º837	 i:1 	 global-step:16741	 l-p:0.13192854821681976
epoch£º837	 i:2 	 global-step:16742	 l-p:0.12337015569210052
epoch£º837	 i:3 	 global-step:16743	 l-p:0.12673230469226837
epoch£º837	 i:4 	 global-step:16744	 l-p:0.1192387193441391
epoch£º837	 i:5 	 global-step:16745	 l-p:0.1316523551940918
epoch£º837	 i:6 	 global-step:16746	 l-p:0.18342415988445282
epoch£º837	 i:7 	 global-step:16747	 l-p:0.08288063108921051
epoch£º837	 i:8 	 global-step:16748	 l-p:0.15066029131412506
epoch£º837	 i:9 	 global-step:16749	 l-p:0.1255866140127182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:838
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1364, 5.1364, 5.1364],
        [5.1364, 5.5053, 5.4169],
        [5.1364, 4.9838, 5.0305],
        [5.1364, 5.0142, 5.0684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:838, step:0 
model_pd.l_p.mean(): 0.1048458069562912 
model_pd.l_d.mean(): -19.6124324798584 
model_pd.lagr.mean(): -19.507587432861328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5850], device='cuda:0')), ('power', tensor([-20.5851], device='cuda:0'))])
epoch£º838	 i:0 	 global-step:16760	 l-p:0.1048458069562912
epoch£º838	 i:1 	 global-step:16761	 l-p:0.15217691659927368
epoch£º838	 i:2 	 global-step:16762	 l-p:0.12282494455575943
epoch£º838	 i:3 	 global-step:16763	 l-p:0.12662243843078613
epoch£º838	 i:4 	 global-step:16764	 l-p:0.11410284042358398
epoch£º838	 i:5 	 global-step:16765	 l-p:0.14961712062358856
epoch£º838	 i:6 	 global-step:16766	 l-p:0.11957933008670807
epoch£º838	 i:7 	 global-step:16767	 l-p:0.1829552799463272
epoch£º838	 i:8 	 global-step:16768	 l-p:0.1302480697631836
epoch£º838	 i:9 	 global-step:16769	 l-p:0.1180030032992363
====================================================================================================
====================================================================================================
====================================================================================================

epoch:839
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1559, 5.1557, 5.1559],
        [5.1559, 5.1557, 5.1559],
        [5.1559, 5.1559, 5.1559],
        [5.1559, 5.1532, 5.1558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:839, step:0 
model_pd.l_p.mean(): 0.1219010129570961 
model_pd.l_d.mean(): -20.165285110473633 
model_pd.lagr.mean(): -20.043384552001953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4795], device='cuda:0')), ('power', tensor([-21.0389], device='cuda:0'))])
epoch£º839	 i:0 	 global-step:16780	 l-p:0.1219010129570961
epoch£º839	 i:1 	 global-step:16781	 l-p:0.17018134891986847
epoch£º839	 i:2 	 global-step:16782	 l-p:0.13331185281276703
epoch£º839	 i:3 	 global-step:16783	 l-p:0.1826256960630417
epoch£º839	 i:4 	 global-step:16784	 l-p:0.1675981879234314
epoch£º839	 i:5 	 global-step:16785	 l-p:0.1001281887292862
epoch£º839	 i:6 	 global-step:16786	 l-p:0.07738874107599258
epoch£º839	 i:7 	 global-step:16787	 l-p:0.11748622357845306
epoch£º839	 i:8 	 global-step:16788	 l-p:0.11963115632534027
epoch£º839	 i:9 	 global-step:16789	 l-p:0.11677104979753494
====================================================================================================
====================================================================================================
====================================================================================================

epoch:840
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1479, 5.1479, 5.1479],
        [5.1479, 5.0193, 4.7116],
        [5.1479, 5.0228, 5.0766],
        [5.1479, 5.1479, 5.1479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:840, step:0 
model_pd.l_p.mean(): 0.10890107601881027 
model_pd.l_d.mean(): -19.147579193115234 
model_pd.lagr.mean(): -19.038677215576172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5486], device='cuda:0')), ('power', tensor([-20.0739], device='cuda:0'))])
epoch£º840	 i:0 	 global-step:16800	 l-p:0.10890107601881027
epoch£º840	 i:1 	 global-step:16801	 l-p:0.10163574665784836
epoch£º840	 i:2 	 global-step:16802	 l-p:0.09112020581960678
epoch£º840	 i:3 	 global-step:16803	 l-p:0.11969537287950516
epoch£º840	 i:4 	 global-step:16804	 l-p:0.17766277492046356
epoch£º840	 i:5 	 global-step:16805	 l-p:0.1620475798845291
epoch£º840	 i:6 	 global-step:16806	 l-p:0.1280863732099533
epoch£º840	 i:7 	 global-step:16807	 l-p:0.1588079035282135
epoch£º840	 i:8 	 global-step:16808	 l-p:0.09912795573472977
epoch£º840	 i:9 	 global-step:16809	 l-p:0.14042262732982635
====================================================================================================
====================================================================================================
====================================================================================================

epoch:841
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1547, 4.9119, 4.6788],
        [5.1547, 5.1489, 5.1544],
        [5.1547, 5.1526, 5.1547],
        [5.1547, 5.0304, 4.7229]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:841, step:0 
model_pd.l_p.mean(): 0.07978125661611557 
model_pd.l_d.mean(): -20.35017204284668 
model_pd.lagr.mean(): -20.2703914642334 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4642], device='cuda:0')), ('power', tensor([-21.2115], device='cuda:0'))])
epoch£º841	 i:0 	 global-step:16820	 l-p:0.07978125661611557
epoch£º841	 i:1 	 global-step:16821	 l-p:0.09921044856309891
epoch£º841	 i:2 	 global-step:16822	 l-p:0.12310176342725754
epoch£º841	 i:3 	 global-step:16823	 l-p:0.18719100952148438
epoch£º841	 i:4 	 global-step:16824	 l-p:0.12704519927501678
epoch£º841	 i:5 	 global-step:16825	 l-p:0.11574486643075943
epoch£º841	 i:6 	 global-step:16826	 l-p:0.15706844627857208
epoch£º841	 i:7 	 global-step:16827	 l-p:0.10746016353368759
epoch£º841	 i:8 	 global-step:16828	 l-p:0.1303323656320572
epoch£º841	 i:9 	 global-step:16829	 l-p:0.16419023275375366
====================================================================================================
====================================================================================================
====================================================================================================

epoch:842
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1455, 5.0613, 5.1119],
        [5.1455, 4.9407, 4.6570],
        [5.1455, 5.3203, 5.1142],
        [5.1455, 5.1455, 5.1455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:842, step:0 
model_pd.l_p.mean(): 0.1318647712469101 
model_pd.l_d.mean(): -20.738473892211914 
model_pd.lagr.mean(): -20.606609344482422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4001], device='cuda:0')), ('power', tensor([-21.5407], device='cuda:0'))])
epoch£º842	 i:0 	 global-step:16840	 l-p:0.1318647712469101
epoch£º842	 i:1 	 global-step:16841	 l-p:0.07020214200019836
epoch£º842	 i:2 	 global-step:16842	 l-p:0.1745937466621399
epoch£º842	 i:3 	 global-step:16843	 l-p:0.12351085245609283
epoch£º842	 i:4 	 global-step:16844	 l-p:0.13663753867149353
epoch£º842	 i:5 	 global-step:16845	 l-p:0.11274099349975586
epoch£º842	 i:6 	 global-step:16846	 l-p:0.12836235761642456
epoch£º842	 i:7 	 global-step:16847	 l-p:0.10802355408668518
epoch£º842	 i:8 	 global-step:16848	 l-p:0.22720889747142792
epoch£º842	 i:9 	 global-step:16849	 l-p:0.11931304633617401
====================================================================================================
====================================================================================================
====================================================================================================

epoch:843
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1395, 4.9994, 4.6915],
        [5.1395, 5.1395, 5.1395],
        [5.1395, 5.1242, 5.1378],
        [5.1395, 5.3060, 5.0954]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:843, step:0 
model_pd.l_p.mean(): 0.1081821545958519 
model_pd.l_d.mean(): -19.80934715270996 
model_pd.lagr.mean(): -19.70116424560547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4887], device='cuda:0')), ('power', tensor([-20.6859], device='cuda:0'))])
epoch£º843	 i:0 	 global-step:16860	 l-p:0.1081821545958519
epoch£º843	 i:1 	 global-step:16861	 l-p:0.11779571324586868
epoch£º843	 i:2 	 global-step:16862	 l-p:0.10183065384626389
epoch£º843	 i:3 	 global-step:16863	 l-p:0.14325757324695587
epoch£º843	 i:4 	 global-step:16864	 l-p:0.19811619818210602
epoch£º843	 i:5 	 global-step:16865	 l-p:0.08688335120677948
epoch£º843	 i:6 	 global-step:16866	 l-p:0.14940686523914337
epoch£º843	 i:7 	 global-step:16867	 l-p:0.11012426018714905
epoch£º843	 i:8 	 global-step:16868	 l-p:0.1433485746383667
epoch£º843	 i:9 	 global-step:16869	 l-p:0.17590083181858063
====================================================================================================
====================================================================================================
====================================================================================================

epoch:844
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1474, 4.9895, 4.6850],
        [5.1474, 5.1473, 5.1474],
        [5.1474, 4.9103, 4.8613],
        [5.1474, 5.0104, 5.0622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:844, step:0 
model_pd.l_p.mean(): 0.12145974487066269 
model_pd.l_d.mean(): -20.5072021484375 
model_pd.lagr.mean(): -20.3857421875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4308], device='cuda:0')), ('power', tensor([-21.3369], device='cuda:0'))])
epoch£º844	 i:0 	 global-step:16880	 l-p:0.12145974487066269
epoch£º844	 i:1 	 global-step:16881	 l-p:0.1291157305240631
epoch£º844	 i:2 	 global-step:16882	 l-p:0.20298537611961365
epoch£º844	 i:3 	 global-step:16883	 l-p:0.050861649215221405
epoch£º844	 i:4 	 global-step:16884	 l-p:0.14231260120868683
epoch£º844	 i:5 	 global-step:16885	 l-p:0.13745814561843872
epoch£º844	 i:6 	 global-step:16886	 l-p:0.1391783505678177
epoch£º844	 i:7 	 global-step:16887	 l-p:0.15058983862400055
epoch£º844	 i:8 	 global-step:16888	 l-p:0.11994095891714096
epoch£º844	 i:9 	 global-step:16889	 l-p:0.1299162209033966
====================================================================================================
====================================================================================================
====================================================================================================

epoch:845
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1443, 5.1443, 5.1443],
        [5.1443, 5.1266, 5.1422],
        [5.1443, 4.9650, 4.9964],
        [5.1443, 4.8902, 4.7856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:845, step:0 
model_pd.l_p.mean(): 0.1352034956216812 
model_pd.l_d.mean(): -20.279560089111328 
model_pd.lagr.mean(): -20.14435577392578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4633], device='cuda:0')), ('power', tensor([-21.1386], device='cuda:0'))])
epoch£º845	 i:0 	 global-step:16900	 l-p:0.1352034956216812
epoch£º845	 i:1 	 global-step:16901	 l-p:0.1232040673494339
epoch£º845	 i:2 	 global-step:16902	 l-p:0.12752166390419006
epoch£º845	 i:3 	 global-step:16903	 l-p:0.1724906712770462
epoch£º845	 i:4 	 global-step:16904	 l-p:0.13118501007556915
epoch£º845	 i:5 	 global-step:16905	 l-p:0.12068451941013336
epoch£º845	 i:6 	 global-step:16906	 l-p:0.10895052552223206
epoch£º845	 i:7 	 global-step:16907	 l-p:0.1338929384946823
epoch£º845	 i:8 	 global-step:16908	 l-p:0.12329962104558945
epoch£º845	 i:9 	 global-step:16909	 l-p:0.15061818063259125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:846
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1421, 5.0354, 5.0898],
        [5.1421, 5.1023, 5.1334],
        [5.1421, 5.0673, 5.1152],
        [5.1421, 5.1421, 5.1421]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:846, step:0 
model_pd.l_p.mean(): 0.1336522102355957 
model_pd.l_d.mean(): -19.376785278320312 
model_pd.lagr.mean(): -19.243133544921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5166], device='cuda:0')), ('power', tensor([-20.2742], device='cuda:0'))])
epoch£º846	 i:0 	 global-step:16920	 l-p:0.1336522102355957
epoch£º846	 i:1 	 global-step:16921	 l-p:0.16761720180511475
epoch£º846	 i:2 	 global-step:16922	 l-p:0.19363167881965637
epoch£º846	 i:3 	 global-step:16923	 l-p:0.0908491238951683
epoch£º846	 i:4 	 global-step:16924	 l-p:0.1249450072646141
epoch£º846	 i:5 	 global-step:16925	 l-p:0.06604167819023132
epoch£º846	 i:6 	 global-step:16926	 l-p:0.11776997894048691
epoch£º846	 i:7 	 global-step:16927	 l-p:0.1383371204137802
epoch£º846	 i:8 	 global-step:16928	 l-p:0.14343133568763733
epoch£º846	 i:9 	 global-step:16929	 l-p:0.14268377423286438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:847
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1565, 5.1079, 5.1441],
        [5.1565, 5.1565, 5.1565],
        [5.1565, 5.1044, 5.1425],
        [5.1565, 5.0477, 5.1022]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:847, step:0 
model_pd.l_p.mean(): 0.12264352291822433 
model_pd.l_d.mean(): -20.577091217041016 
model_pd.lagr.mean(): -20.45444679260254 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4096], device='cuda:0')), ('power', tensor([-21.3861], device='cuda:0'))])
epoch£º847	 i:0 	 global-step:16940	 l-p:0.12264352291822433
epoch£º847	 i:1 	 global-step:16941	 l-p:0.1517181098461151
epoch£º847	 i:2 	 global-step:16942	 l-p:0.14457906782627106
epoch£º847	 i:3 	 global-step:16943	 l-p:0.08917322754859924
epoch£º847	 i:4 	 global-step:16944	 l-p:0.1475571095943451
epoch£º847	 i:5 	 global-step:16945	 l-p:0.17122110724449158
epoch£º847	 i:6 	 global-step:16946	 l-p:0.06069938465952873
epoch£º847	 i:7 	 global-step:16947	 l-p:0.1483357846736908
epoch£º847	 i:8 	 global-step:16948	 l-p:0.1632949858903885
epoch£º847	 i:9 	 global-step:16949	 l-p:0.11738799512386322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:848
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1521, 5.0789, 5.1262],
        [5.1521, 4.9473, 4.9539],
        [5.1521, 5.1516, 5.1521],
        [5.1521, 4.9349, 4.6605]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:848, step:0 
model_pd.l_p.mean(): 0.16980178654193878 
model_pd.l_d.mean(): -20.63910675048828 
model_pd.lagr.mean(): -20.46930503845215 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4052], device='cuda:0')), ('power', tensor([-21.4448], device='cuda:0'))])
epoch£º848	 i:0 	 global-step:16960	 l-p:0.16980178654193878
epoch£º848	 i:1 	 global-step:16961	 l-p:0.056189727038145065
epoch£º848	 i:2 	 global-step:16962	 l-p:0.13621750473976135
epoch£º848	 i:3 	 global-step:16963	 l-p:0.15014880895614624
epoch£º848	 i:4 	 global-step:16964	 l-p:0.15743543207645416
epoch£º848	 i:5 	 global-step:16965	 l-p:0.1368858516216278
epoch£º848	 i:6 	 global-step:16966	 l-p:0.13089685142040253
epoch£º848	 i:7 	 global-step:16967	 l-p:0.1537974327802658
epoch£º848	 i:8 	 global-step:16968	 l-p:0.07823482155799866
epoch£º848	 i:9 	 global-step:16969	 l-p:0.09405136853456497
====================================================================================================
====================================================================================================
====================================================================================================

epoch:849
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1728, 4.9366, 4.8868],
        [5.1728, 5.1419, 4.8462],
        [5.1728, 4.9488, 4.9248],
        [5.1728, 5.1245, 5.1606]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:849, step:0 
model_pd.l_p.mean(): 0.12358951568603516 
model_pd.l_d.mean(): -18.805217742919922 
model_pd.lagr.mean(): -18.681629180908203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5495], device='cuda:0')), ('power', tensor([-19.7260], device='cuda:0'))])
epoch£º849	 i:0 	 global-step:16980	 l-p:0.12358951568603516
epoch£º849	 i:1 	 global-step:16981	 l-p:0.13636931777000427
epoch£º849	 i:2 	 global-step:16982	 l-p:0.12902513146400452
epoch£º849	 i:3 	 global-step:16983	 l-p:0.11659888923168182
epoch£º849	 i:4 	 global-step:16984	 l-p:0.08185643702745438
epoch£º849	 i:5 	 global-step:16985	 l-p:0.16185890138149261
epoch£º849	 i:6 	 global-step:16986	 l-p:0.13802683353424072
epoch£º849	 i:7 	 global-step:16987	 l-p:0.07117666304111481
epoch£º849	 i:8 	 global-step:16988	 l-p:0.12419821321964264
epoch£º849	 i:9 	 global-step:16989	 l-p:0.15896302461624146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:850
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8903,  0.8564,  1.0000,  0.8239,
          1.0000,  0.9620, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3539,  0.2504,  1.0000,  0.1771,
          1.0000,  0.7074, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1548,  0.0831,  1.0000,  0.0446,
          1.0000,  0.5369, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9009,  0.8700,  1.0000,  0.8403,
          1.0000,  0.9658, 31.6228]], device='cuda:0')
 pt:tensor([[5.1670, 5.5068, 5.3976],
        [5.1670, 4.9106, 4.7255],
        [5.1670, 5.0121, 5.0578],
        [5.1670, 5.5235, 5.4251]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:850, step:0 
model_pd.l_p.mean(): 0.11146606504917145 
model_pd.l_d.mean(): -20.161819458007812 
model_pd.lagr.mean(): -20.05035400390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4735], device='cuda:0')), ('power', tensor([-21.0292], device='cuda:0'))])
epoch£º850	 i:0 	 global-step:17000	 l-p:0.11146606504917145
epoch£º850	 i:1 	 global-step:17001	 l-p:0.1411019265651703
epoch£º850	 i:2 	 global-step:17002	 l-p:0.11516653746366501
epoch£º850	 i:3 	 global-step:17003	 l-p:0.13950000703334808
epoch£º850	 i:4 	 global-step:17004	 l-p:0.09951367229223251
epoch£º850	 i:5 	 global-step:17005	 l-p:0.17535558342933655
epoch£º850	 i:6 	 global-step:17006	 l-p:0.11354061216115952
epoch£º850	 i:7 	 global-step:17007	 l-p:0.2008216679096222
epoch£º850	 i:8 	 global-step:17008	 l-p:0.10306211560964584
epoch£º850	 i:9 	 global-step:17009	 l-p:0.11979090422391891
====================================================================================================
====================================================================================================
====================================================================================================

epoch:851
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1440, 5.1440, 5.1440],
        [5.1440, 5.1438, 5.1441],
        [5.1440, 5.0936, 5.1309],
        [5.1440, 4.9530, 4.6577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:851, step:0 
model_pd.l_p.mean(): 0.16021829843521118 
model_pd.l_d.mean(): -19.52048110961914 
model_pd.lagr.mean(): -19.360261917114258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5580], device='cuda:0')), ('power', tensor([-20.4634], device='cuda:0'))])
epoch£º851	 i:0 	 global-step:17020	 l-p:0.16021829843521118
epoch£º851	 i:1 	 global-step:17021	 l-p:0.10945514589548111
epoch£º851	 i:2 	 global-step:17022	 l-p:0.18791839480400085
epoch£º851	 i:3 	 global-step:17023	 l-p:0.11544141173362732
epoch£º851	 i:4 	 global-step:17024	 l-p:0.11702568829059601
epoch£º851	 i:5 	 global-step:17025	 l-p:0.09954261779785156
epoch£º851	 i:6 	 global-step:17026	 l-p:0.18520240485668182
epoch£º851	 i:7 	 global-step:17027	 l-p:0.0812917947769165
epoch£º851	 i:8 	 global-step:17028	 l-p:0.1330323964357376
epoch£º851	 i:9 	 global-step:17029	 l-p:0.11868740618228912
====================================================================================================
====================================================================================================
====================================================================================================

epoch:852
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6515e-03, 1.9520e-04,
         1.0000e+00, 2.3073e-05, 1.0000e+00, 1.1820e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1508, 5.6220, 5.6014],
        [5.1508, 4.8912, 4.7532],
        [5.1508, 4.8981, 4.8044],
        [5.1508, 5.1507, 5.1508]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:852, step:0 
model_pd.l_p.mean(): 0.09917280077934265 
model_pd.l_d.mean(): -19.568368911743164 
model_pd.lagr.mean(): -19.469196319580078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5106], device='cuda:0')), ('power', tensor([-20.4631], device='cuda:0'))])
epoch£º852	 i:0 	 global-step:17040	 l-p:0.09917280077934265
epoch£º852	 i:1 	 global-step:17041	 l-p:0.09611158818006516
epoch£º852	 i:2 	 global-step:17042	 l-p:0.1490640789270401
epoch£º852	 i:3 	 global-step:17043	 l-p:0.14548130333423615
epoch£º852	 i:4 	 global-step:17044	 l-p:0.11285415291786194
epoch£º852	 i:5 	 global-step:17045	 l-p:0.12275367230176926
epoch£º852	 i:6 	 global-step:17046	 l-p:0.16263000667095184
epoch£º852	 i:7 	 global-step:17047	 l-p:0.13873763382434845
epoch£º852	 i:8 	 global-step:17048	 l-p:0.15377506613731384
epoch£º852	 i:9 	 global-step:17049	 l-p:0.13812705874443054
====================================================================================================
====================================================================================================
====================================================================================================

epoch:853
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8114e-01, 5.9931e-01,
         1.0000e+00, 5.2730e-01, 1.0000e+00, 8.7986e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1332, 5.1520, 4.8712],
        [5.1332, 5.1218, 5.1322],
        [5.1332, 4.9699, 5.0131],
        [5.1332, 4.9939, 5.0460]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:853, step:0 
model_pd.l_p.mean(): 0.17479126155376434 
model_pd.l_d.mean(): -19.39858627319336 
model_pd.lagr.mean(): -19.22379493713379 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5196], device='cuda:0')), ('power', tensor([-20.2995], device='cuda:0'))])
epoch£º853	 i:0 	 global-step:17060	 l-p:0.17479126155376434
epoch£º853	 i:1 	 global-step:17061	 l-p:0.12438853085041046
epoch£º853	 i:2 	 global-step:17062	 l-p:0.12498769164085388
epoch£º853	 i:3 	 global-step:17063	 l-p:0.1506626158952713
epoch£º853	 i:4 	 global-step:17064	 l-p:0.16696438193321228
epoch£º853	 i:5 	 global-step:17065	 l-p:0.13834556937217712
epoch£º853	 i:6 	 global-step:17066	 l-p:0.17507164180278778
epoch£º853	 i:7 	 global-step:17067	 l-p:0.08332651108503342
epoch£º853	 i:8 	 global-step:17068	 l-p:0.15756042301654816
epoch£º853	 i:9 	 global-step:17069	 l-p:0.11196587979793549
====================================================================================================
====================================================================================================
====================================================================================================

epoch:854
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1247, 4.9147, 4.6271],
        [5.1247, 5.1246, 5.1247],
        [5.1247, 5.4458, 5.3255],
        [5.1247, 5.1247, 5.1247]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:854, step:0 
model_pd.l_p.mean(): 0.13687913119792938 
model_pd.l_d.mean(): -19.93340301513672 
model_pd.lagr.mean(): -19.796524047851562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5325], device='cuda:0')), ('power', tensor([-20.8576], device='cuda:0'))])
epoch£º854	 i:0 	 global-step:17080	 l-p:0.13687913119792938
epoch£º854	 i:1 	 global-step:17081	 l-p:0.24253031611442566
epoch£º854	 i:2 	 global-step:17082	 l-p:0.1106315478682518
epoch£º854	 i:3 	 global-step:17083	 l-p:0.17013761401176453
epoch£º854	 i:4 	 global-step:17084	 l-p:0.12622104585170746
epoch£º854	 i:5 	 global-step:17085	 l-p:0.09583452343940735
epoch£º854	 i:6 	 global-step:17086	 l-p:0.11763334274291992
epoch£º854	 i:7 	 global-step:17087	 l-p:0.14153407514095306
epoch£º854	 i:8 	 global-step:17088	 l-p:0.12676432728767395
epoch£º854	 i:9 	 global-step:17089	 l-p:0.1342313587665558
====================================================================================================
====================================================================================================
====================================================================================================

epoch:855
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1237, 5.0171, 4.7048],
        [5.1237, 5.2400, 5.0023],
        [5.1237, 5.1180, 5.1234],
        [5.1237, 4.8828, 4.8342]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:855, step:0 
model_pd.l_p.mean(): 0.15818940103054047 
model_pd.l_d.mean(): -20.482425689697266 
model_pd.lagr.mean(): -20.324235916137695 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4488], device='cuda:0')), ('power', tensor([-21.3302], device='cuda:0'))])
epoch£º855	 i:0 	 global-step:17100	 l-p:0.15818940103054047
epoch£º855	 i:1 	 global-step:17101	 l-p:0.1325838416814804
epoch£º855	 i:2 	 global-step:17102	 l-p:0.12016675621271133
epoch£º855	 i:3 	 global-step:17103	 l-p:0.11723874509334564
epoch£º855	 i:4 	 global-step:17104	 l-p:0.16657088696956635
epoch£º855	 i:5 	 global-step:17105	 l-p:0.16095027327537537
epoch£º855	 i:6 	 global-step:17106	 l-p:0.14235633611679077
epoch£º855	 i:7 	 global-step:17107	 l-p:0.2685079872608185
epoch£º855	 i:8 	 global-step:17108	 l-p:0.15696300566196442
epoch£º855	 i:9 	 global-step:17109	 l-p:0.1259087771177292
====================================================================================================
====================================================================================================
====================================================================================================

epoch:856
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0952, 4.8901, 4.9025],
        [5.0952, 5.0888, 5.0948],
        [5.0952, 4.8852, 4.5922],
        [5.0952, 5.0952, 5.0952]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:856, step:0 
model_pd.l_p.mean(): 0.1877869814634323 
model_pd.l_d.mean(): -19.67205238342285 
model_pd.lagr.mean(): -19.48426628112793 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5276], device='cuda:0')), ('power', tensor([-20.5864], device='cuda:0'))])
epoch£º856	 i:0 	 global-step:17120	 l-p:0.1877869814634323
epoch£º856	 i:1 	 global-step:17121	 l-p:0.13420139253139496
epoch£º856	 i:2 	 global-step:17122	 l-p:0.16837047040462494
epoch£º856	 i:3 	 global-step:17123	 l-p:0.11586064845323563
epoch£º856	 i:4 	 global-step:17124	 l-p:0.2867251932621002
epoch£º856	 i:5 	 global-step:17125	 l-p:0.20551437139511108
epoch£º856	 i:6 	 global-step:17126	 l-p:0.1523113250732422
epoch£º856	 i:7 	 global-step:17127	 l-p:0.12057008594274521
epoch£º856	 i:8 	 global-step:17128	 l-p:0.11355261504650116
epoch£º856	 i:9 	 global-step:17129	 l-p:0.1099826917052269
====================================================================================================
====================================================================================================
====================================================================================================

epoch:857
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9244e-02, 1.3336e-02,
         1.0000e+00, 4.5320e-03, 1.0000e+00, 3.3983e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1086, 4.9983, 4.6845],
        [5.1086, 4.8995, 4.9062],
        [5.1086, 5.0893, 5.1062],
        [5.1086, 5.0355, 4.7265]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:857, step:0 
model_pd.l_p.mean(): 0.11212120950222015 
model_pd.l_d.mean(): -19.13703727722168 
model_pd.lagr.mean(): -19.02491569519043 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6028], device='cuda:0')), ('power', tensor([-20.1192], device='cuda:0'))])
epoch£º857	 i:0 	 global-step:17140	 l-p:0.11212120950222015
epoch£º857	 i:1 	 global-step:17141	 l-p:0.15979623794555664
epoch£º857	 i:2 	 global-step:17142	 l-p:0.1773047000169754
epoch£º857	 i:3 	 global-step:17143	 l-p:0.2586745023727417
epoch£º857	 i:4 	 global-step:17144	 l-p:0.0989052951335907
epoch£º857	 i:5 	 global-step:17145	 l-p:0.12082342058420181
epoch£º857	 i:6 	 global-step:17146	 l-p:0.12680748105049133
epoch£º857	 i:7 	 global-step:17147	 l-p:0.1649308204650879
epoch£º857	 i:8 	 global-step:17148	 l-p:0.13031859695911407
epoch£º857	 i:9 	 global-step:17149	 l-p:0.10622432827949524
====================================================================================================
====================================================================================================
====================================================================================================

epoch:858
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1385, 5.1380, 5.1385],
        [5.1385, 4.9698, 5.0102],
        [5.1385, 5.1384, 5.1385],
        [5.1385, 5.1308, 5.1379]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:858, step:0 
model_pd.l_p.mean(): 0.17387720942497253 
model_pd.l_d.mean(): -20.499282836914062 
model_pd.lagr.mean(): -20.32540512084961 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4236], device='cuda:0')), ('power', tensor([-21.3213], device='cuda:0'))])
epoch£º858	 i:0 	 global-step:17160	 l-p:0.17387720942497253
epoch£º858	 i:1 	 global-step:17161	 l-p:0.14980517327785492
epoch£º858	 i:2 	 global-step:17162	 l-p:0.12931853532791138
epoch£º858	 i:3 	 global-step:17163	 l-p:0.1394490748643875
epoch£º858	 i:4 	 global-step:17164	 l-p:0.07196467369794846
epoch£º858	 i:5 	 global-step:17165	 l-p:0.15826931595802307
epoch£º858	 i:6 	 global-step:17166	 l-p:0.12111116200685501
epoch£º858	 i:7 	 global-step:17167	 l-p:0.10693887621164322
epoch£º858	 i:8 	 global-step:17168	 l-p:0.16950072348117828
epoch£º858	 i:9 	 global-step:17169	 l-p:0.10321700572967529
====================================================================================================
====================================================================================================
====================================================================================================

epoch:859
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1533, 4.8936, 4.7056],
        [5.1533, 5.1532, 5.1533],
        [5.1533, 5.5576, 5.4903],
        [5.1533, 5.1021, 5.1398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:859, step:0 
model_pd.l_p.mean(): 0.12036614865064621 
model_pd.l_d.mean(): -20.392301559448242 
model_pd.lagr.mean(): -20.271934509277344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4337], device='cuda:0')), ('power', tensor([-21.2228], device='cuda:0'))])
epoch£º859	 i:0 	 global-step:17180	 l-p:0.12036614865064621
epoch£º859	 i:1 	 global-step:17181	 l-p:0.09943612664937973
epoch£º859	 i:2 	 global-step:17182	 l-p:0.1474970430135727
epoch£º859	 i:3 	 global-step:17183	 l-p:0.11401207000017166
epoch£º859	 i:4 	 global-step:17184	 l-p:0.10209502279758453
epoch£º859	 i:5 	 global-step:17185	 l-p:0.15541039407253265
epoch£º859	 i:6 	 global-step:17186	 l-p:0.13831639289855957
epoch£º859	 i:7 	 global-step:17187	 l-p:0.20616047084331512
epoch£º859	 i:8 	 global-step:17188	 l-p:0.14285539090633392
epoch£º859	 i:9 	 global-step:17189	 l-p:0.15808989107608795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:860
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1194, 5.0724, 5.1079],
        [5.1194, 5.1110, 5.1188],
        [5.1194, 4.9619, 5.0087],
        [5.1194, 5.1044, 5.1178]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:860, step:0 
model_pd.l_p.mean(): 0.11043254286050797 
model_pd.l_d.mean(): -20.34259033203125 
model_pd.lagr.mean(): -20.232158660888672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4576], device='cuda:0')), ('power', tensor([-21.1970], device='cuda:0'))])
epoch£º860	 i:0 	 global-step:17200	 l-p:0.11043254286050797
epoch£º860	 i:1 	 global-step:17201	 l-p:0.15484267473220825
epoch£º860	 i:2 	 global-step:17202	 l-p:0.1493532359600067
epoch£º860	 i:3 	 global-step:17203	 l-p:0.21243542432785034
epoch£º860	 i:4 	 global-step:17204	 l-p:0.1457616537809372
epoch£º860	 i:5 	 global-step:17205	 l-p:0.09830888360738754
epoch£º860	 i:6 	 global-step:17206	 l-p:0.13037370145320892
epoch£º860	 i:7 	 global-step:17207	 l-p:0.13131563365459442
epoch£º860	 i:8 	 global-step:17208	 l-p:0.1756371706724167
epoch£º860	 i:9 	 global-step:17209	 l-p:0.16258631646633148
====================================================================================================
====================================================================================================
====================================================================================================

epoch:861
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1137, 4.8537, 4.7491],
        [5.1137, 5.2214, 4.9791],
        [5.1137, 5.0238, 5.0764],
        [5.1137, 5.1013, 5.1125]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:861, step:0 
model_pd.l_p.mean(): 0.16984111070632935 
model_pd.l_d.mean(): -20.474777221679688 
model_pd.lagr.mean(): -20.304935455322266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4490], device='cuda:0')), ('power', tensor([-21.3227], device='cuda:0'))])
epoch£º861	 i:0 	 global-step:17220	 l-p:0.16984111070632935
epoch£º861	 i:1 	 global-step:17221	 l-p:0.13210466504096985
epoch£º861	 i:2 	 global-step:17222	 l-p:0.12272801995277405
epoch£º861	 i:3 	 global-step:17223	 l-p:0.17299839854240417
epoch£º861	 i:4 	 global-step:17224	 l-p:0.13531449437141418
epoch£º861	 i:5 	 global-step:17225	 l-p:0.1532318890094757
epoch£º861	 i:6 	 global-step:17226	 l-p:0.12338103353977203
epoch£º861	 i:7 	 global-step:17227	 l-p:0.11193840950727463
epoch£º861	 i:8 	 global-step:17228	 l-p:0.12689514458179474
epoch£º861	 i:9 	 global-step:17229	 l-p:0.1830325722694397
====================================================================================================
====================================================================================================
====================================================================================================

epoch:862
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1261, 4.8824, 4.6305],
        [5.1261, 4.9267, 4.9435],
        [5.1261, 5.1181, 5.1256],
        [5.1261, 5.1261, 5.1261]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:862, step:0 
model_pd.l_p.mean(): 0.13138072192668915 
model_pd.l_d.mean(): -20.249027252197266 
model_pd.lagr.mean(): -20.117647171020508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4524], device='cuda:0')), ('power', tensor([-21.0962], device='cuda:0'))])
epoch£º862	 i:0 	 global-step:17240	 l-p:0.13138072192668915
epoch£º862	 i:1 	 global-step:17241	 l-p:0.13838467001914978
epoch£º862	 i:2 	 global-step:17242	 l-p:0.1285093128681183
epoch£º862	 i:3 	 global-step:17243	 l-p:0.11456432193517685
epoch£º862	 i:4 	 global-step:17244	 l-p:0.18445272743701935
epoch£º862	 i:5 	 global-step:17245	 l-p:0.18850727379322052
epoch£º862	 i:6 	 global-step:17246	 l-p:0.13831210136413574
epoch£º862	 i:7 	 global-step:17247	 l-p:0.13717412948608398
epoch£º862	 i:8 	 global-step:17248	 l-p:0.16412490606307983
epoch£º862	 i:9 	 global-step:17249	 l-p:0.1258777529001236
====================================================================================================
====================================================================================================
====================================================================================================

epoch:863
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1230, 5.0828, 5.1143],
        [5.1230, 5.1021, 5.1202],
        [5.1230, 5.0402, 5.0910],
        [5.1230, 4.8839, 4.6234]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:863, step:0 
model_pd.l_p.mean(): 0.16108931601047516 
model_pd.l_d.mean(): -19.428251266479492 
model_pd.lagr.mean(): -19.267162322998047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5021], device='cuda:0')), ('power', tensor([-20.3116], device='cuda:0'))])
epoch£º863	 i:0 	 global-step:17260	 l-p:0.16108931601047516
epoch£º863	 i:1 	 global-step:17261	 l-p:0.17055535316467285
epoch£º863	 i:2 	 global-step:17262	 l-p:0.13079392910003662
epoch£º863	 i:3 	 global-step:17263	 l-p:0.12076421827077866
epoch£º863	 i:4 	 global-step:17264	 l-p:0.09137187898159027
epoch£º863	 i:5 	 global-step:17265	 l-p:0.12254943698644638
epoch£º863	 i:6 	 global-step:17266	 l-p:0.11502161622047424
epoch£º863	 i:7 	 global-step:17267	 l-p:0.1997501701116562
epoch£º863	 i:8 	 global-step:17268	 l-p:0.15499693155288696
epoch£º863	 i:9 	 global-step:17269	 l-p:0.11594848334789276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:864
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1387, 5.0352, 4.7229],
        [5.1387, 5.1387, 5.1387],
        [5.1387, 5.1387, 5.1387],
        [5.1387, 5.5311, 5.4560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:864, step:0 
model_pd.l_p.mean(): 0.1657949537038803 
model_pd.l_d.mean(): -19.348173141479492 
model_pd.lagr.mean(): -19.1823787689209 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4854], device='cuda:0')), ('power', tensor([-20.2127], device='cuda:0'))])
epoch£º864	 i:0 	 global-step:17280	 l-p:0.1657949537038803
epoch£º864	 i:1 	 global-step:17281	 l-p:0.12372869998216629
epoch£º864	 i:2 	 global-step:17282	 l-p:0.10873274505138397
epoch£º864	 i:3 	 global-step:17283	 l-p:0.18226318061351776
epoch£º864	 i:4 	 global-step:17284	 l-p:0.13794727623462677
epoch£º864	 i:5 	 global-step:17285	 l-p:0.0656619518995285
epoch£º864	 i:6 	 global-step:17286	 l-p:0.1256735920906067
epoch£º864	 i:7 	 global-step:17287	 l-p:0.12361413985490799
epoch£º864	 i:8 	 global-step:17288	 l-p:0.14319075644016266
epoch£º864	 i:9 	 global-step:17289	 l-p:0.14374570548534393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:865
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1539, 5.1539, 5.1539],
        [5.1539, 5.1529, 5.1539],
        [5.1539, 5.1539, 5.1539],
        [5.1539, 5.1539, 5.1540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:865, step:0 
model_pd.l_p.mean(): 0.1277121901512146 
model_pd.l_d.mean(): -20.226627349853516 
model_pd.lagr.mean(): -20.098915100097656 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4384], device='cuda:0')), ('power', tensor([-21.0589], device='cuda:0'))])
epoch£º865	 i:0 	 global-step:17300	 l-p:0.1277121901512146
epoch£º865	 i:1 	 global-step:17301	 l-p:0.18637114763259888
epoch£º865	 i:2 	 global-step:17302	 l-p:0.15327809751033783
epoch£º865	 i:3 	 global-step:17303	 l-p:0.07636348158121109
epoch£º865	 i:4 	 global-step:17304	 l-p:0.17343194782733917
epoch£º865	 i:5 	 global-step:17305	 l-p:0.09296008199453354
epoch£º865	 i:6 	 global-step:17306	 l-p:0.11266440898180008
epoch£º865	 i:7 	 global-step:17307	 l-p:0.09509683400392532
epoch£º865	 i:8 	 global-step:17308	 l-p:0.13054800033569336
epoch£º865	 i:9 	 global-step:17309	 l-p:0.14154626429080963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:866
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1662, 5.3277, 5.1119],
        [5.1662, 5.6399, 5.6193],
        [5.1662, 5.1044, 5.1473],
        [5.1662, 5.1635, 5.1662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:866, step:0 
model_pd.l_p.mean(): 0.0784960612654686 
model_pd.l_d.mean(): -19.511890411376953 
model_pd.lagr.mean(): -19.433393478393555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5006], device='cuda:0')), ('power', tensor([-20.3952], device='cuda:0'))])
epoch£º866	 i:0 	 global-step:17320	 l-p:0.0784960612654686
epoch£º866	 i:1 	 global-step:17321	 l-p:0.16439147293567657
epoch£º866	 i:2 	 global-step:17322	 l-p:0.12312459945678711
epoch£º866	 i:3 	 global-step:17323	 l-p:0.14338794350624084
epoch£º866	 i:4 	 global-step:17324	 l-p:0.10984626412391663
epoch£º866	 i:5 	 global-step:17325	 l-p:0.16469764709472656
epoch£º866	 i:6 	 global-step:17326	 l-p:0.10994001477956772
epoch£º866	 i:7 	 global-step:17327	 l-p:0.12371203303337097
epoch£º866	 i:8 	 global-step:17328	 l-p:0.12075323611497879
epoch£º866	 i:9 	 global-step:17329	 l-p:0.11360404640436172
====================================================================================================
====================================================================================================
====================================================================================================

epoch:867
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1666, 5.1666, 5.1666],
        [5.1666, 4.9917, 5.0276],
        [5.1666, 5.2733, 5.0294],
        [5.1666, 5.1631, 5.1664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:867, step:0 
model_pd.l_p.mean(): 0.13345913589000702 
model_pd.l_d.mean(): -19.33458137512207 
model_pd.lagr.mean(): -19.201122283935547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5301], device='cuda:0')), ('power', tensor([-20.2451], device='cuda:0'))])
epoch£º867	 i:0 	 global-step:17340	 l-p:0.13345913589000702
epoch£º867	 i:1 	 global-step:17341	 l-p:0.1088753491640091
epoch£º867	 i:2 	 global-step:17342	 l-p:0.2111383080482483
epoch£º867	 i:3 	 global-step:17343	 l-p:0.1271142214536667
epoch£º867	 i:4 	 global-step:17344	 l-p:0.10996049642562866
epoch£º867	 i:5 	 global-step:17345	 l-p:0.10927869379520416
epoch£º867	 i:6 	 global-step:17346	 l-p:0.12237658351659775
epoch£º867	 i:7 	 global-step:17347	 l-p:0.16121989488601685
epoch£º867	 i:8 	 global-step:17348	 l-p:0.09946411103010178
epoch£º867	 i:9 	 global-step:17349	 l-p:0.1375672072172165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:868
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.3311,  0.2291,  1.0000,  0.1585,
          1.0000,  0.6918, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7108,  0.6343,  1.0000,  0.5661,
          1.0000,  0.8924, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2047,  0.1207,  1.0000,  0.0711,
          1.0000,  0.5894, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228]], device='cuda:0')
 pt:tensor([[5.1248, 4.8587, 4.7007],
        [5.1248, 5.1782, 4.9101],
        [5.1248, 4.9154, 4.9220],
        [5.1248, 4.8625, 4.6585]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:868, step:0 
model_pd.l_p.mean(): 0.1670890897512436 
model_pd.l_d.mean(): -20.44117546081543 
model_pd.lagr.mean(): -20.274085998535156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4618], device='cuda:0')), ('power', tensor([-21.3017], device='cuda:0'))])
epoch£º868	 i:0 	 global-step:17360	 l-p:0.1670890897512436
epoch£º868	 i:1 	 global-step:17361	 l-p:0.11909598857164383
epoch£º868	 i:2 	 global-step:17362	 l-p:0.19802001118659973
epoch£º868	 i:3 	 global-step:17363	 l-p:0.16401386260986328
epoch£º868	 i:4 	 global-step:17364	 l-p:0.19941183924674988
epoch£º868	 i:5 	 global-step:17365	 l-p:0.10677392780780792
epoch£º868	 i:6 	 global-step:17366	 l-p:0.1072721853852272
epoch£º868	 i:7 	 global-step:17367	 l-p:0.17522287368774414
epoch£º868	 i:8 	 global-step:17368	 l-p:0.12965917587280273
epoch£º868	 i:9 	 global-step:17369	 l-p:0.09762825816869736
====================================================================================================
====================================================================================================
====================================================================================================

epoch:869
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1039, 5.4884, 5.4085],
        [5.1039, 5.1002, 5.1038],
        [5.1039, 4.9111, 4.6072],
        [5.1039, 4.9471, 4.9953]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:869, step:0 
model_pd.l_p.mean(): 0.13668258488178253 
model_pd.l_d.mean(): -19.384124755859375 
model_pd.lagr.mean(): -19.2474422454834 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5719], device='cuda:0')), ('power', tensor([-20.3389], device='cuda:0'))])
epoch£º869	 i:0 	 global-step:17380	 l-p:0.13668258488178253
epoch£º869	 i:1 	 global-step:17381	 l-p:0.09102246910333633
epoch£º869	 i:2 	 global-step:17382	 l-p:0.08698742836713791
epoch£º869	 i:3 	 global-step:17383	 l-p:0.17975276708602905
epoch£º869	 i:4 	 global-step:17384	 l-p:0.1263827681541443
epoch£º869	 i:5 	 global-step:17385	 l-p:0.2685832977294922
epoch£º869	 i:6 	 global-step:17386	 l-p:0.187961608171463
epoch£º869	 i:7 	 global-step:17387	 l-p:0.44943949580192566
epoch£º869	 i:8 	 global-step:17388	 l-p:0.13554181158542633
epoch£º869	 i:9 	 global-step:17389	 l-p:0.16210633516311646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:870
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3185,  0.2175,  1.0000,  0.1485,
          1.0000,  0.6829, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2005,  0.1173,  1.0000,  0.0687,
          1.0000,  0.5853, 31.6228]], device='cuda:0')
 pt:tensor([[5.0874, 4.8753, 4.8821],
        [5.0874, 4.8178, 4.6764],
        [5.0874, 4.8297, 4.7488],
        [5.0874, 4.8796, 4.8914]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:870, step:0 
model_pd.l_p.mean(): 0.12840791046619415 
model_pd.l_d.mean(): -20.824968338012695 
model_pd.lagr.mean(): -20.69655990600586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4030], device='cuda:0')), ('power', tensor([-21.6318], device='cuda:0'))])
epoch£º870	 i:0 	 global-step:17400	 l-p:0.12840791046619415
epoch£º870	 i:1 	 global-step:17401	 l-p:0.20141196250915527
epoch£º870	 i:2 	 global-step:17402	 l-p:0.10819469392299652
epoch£º870	 i:3 	 global-step:17403	 l-p:0.3413955569267273
epoch£º870	 i:4 	 global-step:17404	 l-p:0.12005775421857834
epoch£º870	 i:5 	 global-step:17405	 l-p:0.17268840968608856
epoch£º870	 i:6 	 global-step:17406	 l-p:0.14728166162967682
epoch£º870	 i:7 	 global-step:17407	 l-p:0.18168190121650696
epoch£º870	 i:8 	 global-step:17408	 l-p:0.12091716378927231
epoch£º870	 i:9 	 global-step:17409	 l-p:0.2032749354839325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:871
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0922, 4.8293, 4.7274],
        [5.0922, 5.0744, 5.0901],
        [5.0922, 4.9442, 4.6275],
        [5.0922, 4.8566, 4.8281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:871, step:0 
model_pd.l_p.mean(): 0.16990920901298523 
model_pd.l_d.mean(): -20.102977752685547 
model_pd.lagr.mean(): -19.933069229125977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5156], device='cuda:0')), ('power', tensor([-21.0129], device='cuda:0'))])
epoch£º871	 i:0 	 global-step:17420	 l-p:0.16990920901298523
epoch£º871	 i:1 	 global-step:17421	 l-p:0.16734671592712402
epoch£º871	 i:2 	 global-step:17422	 l-p:0.1621362715959549
epoch£º871	 i:3 	 global-step:17423	 l-p:0.1338491439819336
epoch£º871	 i:4 	 global-step:17424	 l-p:0.2088891714811325
epoch£º871	 i:5 	 global-step:17425	 l-p:0.1266126036643982
epoch£º871	 i:6 	 global-step:17426	 l-p:0.12537965178489685
epoch£º871	 i:7 	 global-step:17427	 l-p:0.1757882684469223
epoch£º871	 i:8 	 global-step:17428	 l-p:0.1050611212849617
epoch£º871	 i:9 	 global-step:17429	 l-p:0.14132142066955566
====================================================================================================
====================================================================================================
====================================================================================================

epoch:872
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1241, 5.0819, 5.1146],
        [5.1241, 5.0512, 4.7410],
        [5.1241, 5.1241, 5.1241],
        [5.1241, 5.0916, 5.1181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:872, step:0 
model_pd.l_p.mean(): 0.20620973408222198 
model_pd.l_d.mean(): -20.279752731323242 
model_pd.lagr.mean(): -20.073543548583984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4807], device='cuda:0')), ('power', tensor([-21.1569], device='cuda:0'))])
epoch£º872	 i:0 	 global-step:17440	 l-p:0.20620973408222198
epoch£º872	 i:1 	 global-step:17441	 l-p:0.13144813477993011
epoch£º872	 i:2 	 global-step:17442	 l-p:0.11356007307767868
epoch£º872	 i:3 	 global-step:17443	 l-p:0.10435435175895691
epoch£º872	 i:4 	 global-step:17444	 l-p:0.14418454468250275
epoch£º872	 i:5 	 global-step:17445	 l-p:0.13382278382778168
epoch£º872	 i:6 	 global-step:17446	 l-p:0.17112067341804504
epoch£º872	 i:7 	 global-step:17447	 l-p:0.08879872411489487
epoch£º872	 i:8 	 global-step:17448	 l-p:0.17082275450229645
epoch£º872	 i:9 	 global-step:17449	 l-p:0.12300019711256027
====================================================================================================
====================================================================================================
====================================================================================================

epoch:873
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1419, 5.0126, 5.0673],
        [5.1419, 5.1342, 5.1414],
        [5.1419, 4.8851, 4.7912],
        [5.1419, 4.9091, 4.8793]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:873, step:0 
model_pd.l_p.mean(): 0.10986468940973282 
model_pd.l_d.mean(): -19.271387100219727 
model_pd.lagr.mean(): -19.161521911621094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5352], device='cuda:0')), ('power', tensor([-20.1861], device='cuda:0'))])
epoch£º873	 i:0 	 global-step:17460	 l-p:0.10986468940973282
epoch£º873	 i:1 	 global-step:17461	 l-p:0.14203637838363647
epoch£º873	 i:2 	 global-step:17462	 l-p:0.16469909250736237
epoch£º873	 i:3 	 global-step:17463	 l-p:0.09694943577051163
epoch£º873	 i:4 	 global-step:17464	 l-p:0.14428655803203583
epoch£º873	 i:5 	 global-step:17465	 l-p:0.16998633742332458
epoch£º873	 i:6 	 global-step:17466	 l-p:0.15631914138793945
epoch£º873	 i:7 	 global-step:17467	 l-p:0.1678103357553482
epoch£º873	 i:8 	 global-step:17468	 l-p:0.15038129687309265
epoch£º873	 i:9 	 global-step:17469	 l-p:0.11982812732458115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:874
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1172, 5.1172, 5.1172],
        [5.1172, 5.1168, 5.1172],
        [5.1172, 5.1172, 5.1172],
        [5.1172, 5.2844, 5.0718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:874, step:0 
model_pd.l_p.mean(): 0.118637815117836 
model_pd.l_d.mean(): -20.52292251586914 
model_pd.lagr.mean(): -20.404285430908203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4234], device='cuda:0')), ('power', tensor([-21.3453], device='cuda:0'))])
epoch£º874	 i:0 	 global-step:17480	 l-p:0.118637815117836
epoch£º874	 i:1 	 global-step:17481	 l-p:0.14910809695720673
epoch£º874	 i:2 	 global-step:17482	 l-p:0.13756175339221954
epoch£º874	 i:3 	 global-step:17483	 l-p:0.16585911810398102
epoch£º874	 i:4 	 global-step:17484	 l-p:0.1752903014421463
epoch£º874	 i:5 	 global-step:17485	 l-p:0.12471800297498703
epoch£º874	 i:6 	 global-step:17486	 l-p:0.19544528424739838
epoch£º874	 i:7 	 global-step:17487	 l-p:0.13810712099075317
epoch£º874	 i:8 	 global-step:17488	 l-p:0.11299993097782135
epoch£º874	 i:9 	 global-step:17489	 l-p:0.135777086019516
====================================================================================================
====================================================================================================
====================================================================================================

epoch:875
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9796e-01, 3.9469e-01,
         1.0000e+00, 3.1284e-01, 1.0000e+00, 7.9262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1271, 5.0828, 5.1168],
        [5.1271, 4.9637, 5.0084],
        [5.1271, 5.1271, 5.1271],
        [5.1271, 4.9340, 4.6318]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:875, step:0 
model_pd.l_p.mean(): 0.10094770044088364 
model_pd.l_d.mean(): -20.533872604370117 
model_pd.lagr.mean(): -20.432924270629883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4156], device='cuda:0')), ('power', tensor([-21.3483], device='cuda:0'))])
epoch£º875	 i:0 	 global-step:17500	 l-p:0.10094770044088364
epoch£º875	 i:1 	 global-step:17501	 l-p:0.15592369437217712
epoch£º875	 i:2 	 global-step:17502	 l-p:0.19821994006633759
epoch£º875	 i:3 	 global-step:17503	 l-p:0.19640155136585236
epoch£º875	 i:4 	 global-step:17504	 l-p:0.11344890296459198
epoch£º875	 i:5 	 global-step:17505	 l-p:0.11137906461954117
epoch£º875	 i:6 	 global-step:17506	 l-p:0.1375967413187027
epoch£º875	 i:7 	 global-step:17507	 l-p:0.11009938269853592
epoch£º875	 i:8 	 global-step:17508	 l-p:0.08953770250082016
epoch£º875	 i:9 	 global-step:17509	 l-p:0.1995454579591751
====================================================================================================
====================================================================================================
====================================================================================================

epoch:876
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1354, 4.9800, 4.6673],
        [5.1354, 5.5890, 5.5544],
        [5.1354, 5.5425, 5.4763],
        [5.1354, 5.1354, 5.1354]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:876, step:0 
model_pd.l_p.mean(): 0.19853758811950684 
model_pd.l_d.mean(): -19.34221649169922 
model_pd.lagr.mean(): -19.143678665161133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5349], device='cuda:0')), ('power', tensor([-20.2579], device='cuda:0'))])
epoch£º876	 i:0 	 global-step:17520	 l-p:0.19853758811950684
epoch£º876	 i:1 	 global-step:17521	 l-p:0.1024327427148819
epoch£º876	 i:2 	 global-step:17522	 l-p:0.10750602185726166
epoch£º876	 i:3 	 global-step:17523	 l-p:0.13141632080078125
epoch£º876	 i:4 	 global-step:17524	 l-p:0.12296111136674881
epoch£º876	 i:5 	 global-step:17525	 l-p:0.10698606073856354
epoch£º876	 i:6 	 global-step:17526	 l-p:0.13134051859378815
epoch£º876	 i:7 	 global-step:17527	 l-p:0.1591523289680481
epoch£º876	 i:8 	 global-step:17528	 l-p:0.12380777299404144
epoch£º876	 i:9 	 global-step:17529	 l-p:0.17365601658821106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:877
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1359, 5.1343, 5.1358],
        [5.1359, 5.1359, 5.1359],
        [5.1359, 4.8887, 4.6387],
        [5.1359, 5.5891, 5.5542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:877, step:0 
model_pd.l_p.mean(): 0.11032146215438843 
model_pd.l_d.mean(): -19.032209396362305 
model_pd.lagr.mean(): -18.92188835144043 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5107], device='cuda:0')), ('power', tensor([-19.9170], device='cuda:0'))])
epoch£º877	 i:0 	 global-step:17540	 l-p:0.11032146215438843
epoch£º877	 i:1 	 global-step:17541	 l-p:0.14857205748558044
epoch£º877	 i:2 	 global-step:17542	 l-p:0.15364398062229156
epoch£º877	 i:3 	 global-step:17543	 l-p:0.11281520873308182
epoch£º877	 i:4 	 global-step:17544	 l-p:0.11577749252319336
epoch£º877	 i:5 	 global-step:17545	 l-p:0.13992126286029816
epoch£º877	 i:6 	 global-step:17546	 l-p:0.16301244497299194
epoch£º877	 i:7 	 global-step:17547	 l-p:0.1304164081811905
epoch£º877	 i:8 	 global-step:17548	 l-p:0.17678584158420563
epoch£º877	 i:9 	 global-step:17549	 l-p:0.15267133712768555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:878
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1347, 5.1346, 5.1347],
        [5.1347, 5.4539, 5.3302],
        [5.1347, 5.4615, 5.3426],
        [5.1347, 5.1346, 5.1347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:878, step:0 
model_pd.l_p.mean(): 0.17114382982254028 
model_pd.l_d.mean(): -20.500015258789062 
model_pd.lagr.mean(): -20.32887077331543 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4281], device='cuda:0')), ('power', tensor([-21.3268], device='cuda:0'))])
epoch£º878	 i:0 	 global-step:17560	 l-p:0.17114382982254028
epoch£º878	 i:1 	 global-step:17561	 l-p:0.19818969070911407
epoch£º878	 i:2 	 global-step:17562	 l-p:0.09739433228969574
epoch£º878	 i:3 	 global-step:17563	 l-p:0.14307674765586853
epoch£º878	 i:4 	 global-step:17564	 l-p:0.11900576949119568
epoch£º878	 i:5 	 global-step:17565	 l-p:0.13463231921195984
epoch£º878	 i:6 	 global-step:17566	 l-p:0.12964148819446564
epoch£º878	 i:7 	 global-step:17567	 l-p:0.10152718424797058
epoch£º878	 i:8 	 global-step:17568	 l-p:0.1396375596523285
epoch£º878	 i:9 	 global-step:17569	 l-p:0.1228860467672348
====================================================================================================
====================================================================================================
====================================================================================================

epoch:879
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1434, 5.1432, 5.1434],
        [5.1434, 5.0575, 5.1092],
        [5.1434, 4.9309, 4.6409],
        [5.1434, 5.1104, 5.1372]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:879, step:0 
model_pd.l_p.mean(): 0.14697131514549255 
model_pd.l_d.mean(): -20.433183670043945 
model_pd.lagr.mean(): -20.286212921142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4378], device='cuda:0')), ('power', tensor([-21.2687], device='cuda:0'))])
epoch£º879	 i:0 	 global-step:17580	 l-p:0.14697131514549255
epoch£º879	 i:1 	 global-step:17581	 l-p:0.09941695630550385
epoch£º879	 i:2 	 global-step:17582	 l-p:0.13030365109443665
epoch£º879	 i:3 	 global-step:17583	 l-p:0.19559185206890106
epoch£º879	 i:4 	 global-step:17584	 l-p:0.06325210630893707
epoch£º879	 i:5 	 global-step:17585	 l-p:0.13993330299854279
epoch£º879	 i:6 	 global-step:17586	 l-p:0.11094050109386444
epoch£º879	 i:7 	 global-step:17587	 l-p:0.13277952373027802
epoch£º879	 i:8 	 global-step:17588	 l-p:0.212208092212677
epoch£º879	 i:9 	 global-step:17589	 l-p:0.13928702473640442
====================================================================================================
====================================================================================================
====================================================================================================

epoch:880
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1307, 4.8647, 4.6717],
        [5.1307, 5.0381, 5.0915],
        [5.1307, 4.8808, 4.8169],
        [5.1307, 5.1305, 5.1307]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:880, step:0 
model_pd.l_p.mean(): 0.16617871820926666 
model_pd.l_d.mean(): -20.01971435546875 
model_pd.lagr.mean(): -19.853534698486328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5244], device='cuda:0')), ('power', tensor([-20.9372], device='cuda:0'))])
epoch£º880	 i:0 	 global-step:17600	 l-p:0.16617871820926666
epoch£º880	 i:1 	 global-step:17601	 l-p:0.12475257366895676
epoch£º880	 i:2 	 global-step:17602	 l-p:0.15024222433567047
epoch£º880	 i:3 	 global-step:17603	 l-p:0.11677627265453339
epoch£º880	 i:4 	 global-step:17604	 l-p:0.17422617971897125
epoch£º880	 i:5 	 global-step:17605	 l-p:0.21057042479515076
epoch£º880	 i:6 	 global-step:17606	 l-p:0.13280899822711945
epoch£º880	 i:7 	 global-step:17607	 l-p:0.1357000321149826
epoch£º880	 i:8 	 global-step:17608	 l-p:0.10084476321935654
epoch£º880	 i:9 	 global-step:17609	 l-p:0.1440022885799408
====================================================================================================
====================================================================================================
====================================================================================================

epoch:881
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1184, 5.1184, 5.1184],
        [5.1184, 5.0098, 5.0656],
        [5.1184, 4.8675, 4.8037],
        [5.1184, 5.4127, 5.2735]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:881, step:0 
model_pd.l_p.mean(): 0.16937312483787537 
model_pd.l_d.mean(): -19.18488883972168 
model_pd.lagr.mean(): -19.01551628112793 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5055], device='cuda:0')), ('power', tensor([-20.0672], device='cuda:0'))])
epoch£º881	 i:0 	 global-step:17620	 l-p:0.16937312483787537
epoch£º881	 i:1 	 global-step:17621	 l-p:0.24081897735595703
epoch£º881	 i:2 	 global-step:17622	 l-p:0.13801951706409454
epoch£º881	 i:3 	 global-step:17623	 l-p:0.12646332383155823
epoch£º881	 i:4 	 global-step:17624	 l-p:0.11093786358833313
epoch£º881	 i:5 	 global-step:17625	 l-p:0.14775967597961426
epoch£º881	 i:6 	 global-step:17626	 l-p:0.12583915889263153
epoch£º881	 i:7 	 global-step:17627	 l-p:0.12905822694301605
epoch£º881	 i:8 	 global-step:17628	 l-p:0.13116152584552765
epoch£º881	 i:9 	 global-step:17629	 l-p:0.10229162126779556
====================================================================================================
====================================================================================================
====================================================================================================

epoch:882
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1282, 4.8839, 4.8353],
        [5.1282, 4.8692, 4.6413],
        [5.1282, 5.1282, 5.1282],
        [5.1282, 5.1242, 5.1280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:882, step:0 
model_pd.l_p.mean(): 0.09092559665441513 
model_pd.l_d.mean(): -20.263004302978516 
model_pd.lagr.mean(): -20.17207908630371 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4800], device='cuda:0')), ('power', tensor([-21.1391], device='cuda:0'))])
epoch£º882	 i:0 	 global-step:17640	 l-p:0.09092559665441513
epoch£º882	 i:1 	 global-step:17641	 l-p:0.15019391477108002
epoch£º882	 i:2 	 global-step:17642	 l-p:0.18355792760849
epoch£º882	 i:3 	 global-step:17643	 l-p:0.08966413140296936
epoch£º882	 i:4 	 global-step:17644	 l-p:0.26067981123924255
epoch£º882	 i:5 	 global-step:17645	 l-p:0.14932332932949066
epoch£º882	 i:6 	 global-step:17646	 l-p:0.14585690200328827
epoch£º882	 i:7 	 global-step:17647	 l-p:0.1418614387512207
epoch£º882	 i:8 	 global-step:17648	 l-p:0.12039244174957275
epoch£º882	 i:9 	 global-step:17649	 l-p:0.1240854263305664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:883
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1087, 5.1059, 5.1086],
        [5.1087, 5.1082, 5.1087],
        [5.1087, 5.0958, 5.1075],
        [5.1087, 4.9438, 4.9892]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:883, step:0 
model_pd.l_p.mean(): 0.13021962344646454 
model_pd.l_d.mean(): -20.5089054107666 
model_pd.lagr.mean(): -20.378684997558594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4461], device='cuda:0')), ('power', tensor([-21.3545], device='cuda:0'))])
epoch£º883	 i:0 	 global-step:17660	 l-p:0.13021962344646454
epoch£º883	 i:1 	 global-step:17661	 l-p:0.14092695713043213
epoch£º883	 i:2 	 global-step:17662	 l-p:0.13718822598457336
epoch£º883	 i:3 	 global-step:17663	 l-p:0.2614079713821411
epoch£º883	 i:4 	 global-step:17664	 l-p:0.15843738615512848
epoch£º883	 i:5 	 global-step:17665	 l-p:0.16100269556045532
epoch£º883	 i:6 	 global-step:17666	 l-p:0.19051486253738403
epoch£º883	 i:7 	 global-step:17667	 l-p:0.10273066908121109
epoch£º883	 i:8 	 global-step:17668	 l-p:0.1116148829460144
epoch£º883	 i:9 	 global-step:17669	 l-p:0.13680411875247955
====================================================================================================
====================================================================================================
====================================================================================================

epoch:884
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1241, 5.1241, 5.1241],
        [5.1241, 4.8582, 4.6542],
        [5.1241, 5.1241, 5.1241],
        [5.1241, 4.9191, 4.6200]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:884, step:0 
model_pd.l_p.mean(): 0.11001256853342056 
model_pd.l_d.mean(): -19.801355361938477 
model_pd.lagr.mean(): -19.691343307495117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4827], device='cuda:0')), ('power', tensor([-20.6716], device='cuda:0'))])
epoch£º884	 i:0 	 global-step:17680	 l-p:0.11001256853342056
epoch£º884	 i:1 	 global-step:17681	 l-p:0.09429468959569931
epoch£º884	 i:2 	 global-step:17682	 l-p:0.15503157675266266
epoch£º884	 i:3 	 global-step:17683	 l-p:0.1282835602760315
epoch£º884	 i:4 	 global-step:17684	 l-p:0.14506196975708008
epoch£º884	 i:5 	 global-step:17685	 l-p:0.0855378806591034
epoch£º884	 i:6 	 global-step:17686	 l-p:0.15206100046634674
epoch£º884	 i:7 	 global-step:17687	 l-p:0.15580840408802032
epoch£º884	 i:8 	 global-step:17688	 l-p:0.20173490047454834
epoch£º884	 i:9 	 global-step:17689	 l-p:0.13431572914123535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:885
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1564, 4.9900, 4.6796],
        [5.1564, 5.1564, 5.1564],
        [5.1564, 4.9026, 4.8223],
        [5.1564, 5.1773, 4.8942]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:885, step:0 
model_pd.l_p.mean(): 0.13806556165218353 
model_pd.l_d.mean(): -20.215396881103516 
model_pd.lagr.mean(): -20.07733154296875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4754], device='cuda:0')), ('power', tensor([-21.0858], device='cuda:0'))])
epoch£º885	 i:0 	 global-step:17700	 l-p:0.13806556165218353
epoch£º885	 i:1 	 global-step:17701	 l-p:0.19467194378376007
epoch£º885	 i:2 	 global-step:17702	 l-p:0.07874331623315811
epoch£º885	 i:3 	 global-step:17703	 l-p:0.13701976835727692
epoch£º885	 i:4 	 global-step:17704	 l-p:0.13561701774597168
epoch£º885	 i:5 	 global-step:17705	 l-p:0.12001199275255203
epoch£º885	 i:6 	 global-step:17706	 l-p:0.12030865252017975
epoch£º885	 i:7 	 global-step:17707	 l-p:0.11813061684370041
epoch£º885	 i:8 	 global-step:17708	 l-p:0.12349440902471542
epoch£º885	 i:9 	 global-step:17709	 l-p:0.13977615535259247
====================================================================================================
====================================================================================================
====================================================================================================

epoch:886
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1484, 5.1466, 5.1483],
        [5.1484, 4.9916, 5.0396],
        [5.1484, 5.1406, 5.1478],
        [5.1484, 5.0542, 5.1079]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:886, step:0 
model_pd.l_p.mean(): 0.14738181233406067 
model_pd.l_d.mean(): -19.227468490600586 
model_pd.lagr.mean(): -19.08008575439453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5705], device='cuda:0')), ('power', tensor([-20.1779], device='cuda:0'))])
epoch£º886	 i:0 	 global-step:17720	 l-p:0.14738181233406067
epoch£º886	 i:1 	 global-step:17721	 l-p:0.10034895688295364
epoch£º886	 i:2 	 global-step:17722	 l-p:0.17615966498851776
epoch£º886	 i:3 	 global-step:17723	 l-p:0.14713424444198608
epoch£º886	 i:4 	 global-step:17724	 l-p:0.11762319505214691
epoch£º886	 i:5 	 global-step:17725	 l-p:0.23434962332248688
epoch£º886	 i:6 	 global-step:17726	 l-p:0.15852634608745575
epoch£º886	 i:7 	 global-step:17727	 l-p:0.09749062359333038
epoch£º886	 i:8 	 global-step:17728	 l-p:0.14396946132183075
epoch£º886	 i:9 	 global-step:17729	 l-p:0.08283121138811111
====================================================================================================
====================================================================================================
====================================================================================================

epoch:887
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1215, 5.0987, 5.1182],
        [5.1215, 4.8538, 4.7260],
        [5.1215, 5.0034, 5.0598],
        [5.1215, 5.0271, 5.0811]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:887, step:0 
model_pd.l_p.mean(): 0.12056384980678558 
model_pd.l_d.mean(): -20.821178436279297 
model_pd.lagr.mean(): -20.70061492919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3885], device='cuda:0')), ('power', tensor([-21.6129], device='cuda:0'))])
epoch£º887	 i:0 	 global-step:17740	 l-p:0.12056384980678558
epoch£º887	 i:1 	 global-step:17741	 l-p:0.11644311994314194
epoch£º887	 i:2 	 global-step:17742	 l-p:0.1498197466135025
epoch£º887	 i:3 	 global-step:17743	 l-p:0.12017890810966492
epoch£º887	 i:4 	 global-step:17744	 l-p:0.2242337018251419
epoch£º887	 i:5 	 global-step:17745	 l-p:0.12156842648983002
epoch£º887	 i:6 	 global-step:17746	 l-p:0.2126648724079132
epoch£º887	 i:7 	 global-step:17747	 l-p:0.12095517665147781
epoch£º887	 i:8 	 global-step:17748	 l-p:0.21196249127388
epoch£º887	 i:9 	 global-step:17749	 l-p:0.08513989299535751
====================================================================================================
====================================================================================================
====================================================================================================

epoch:888
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2540,  0.1609,  1.0000,  0.1019,
          1.0000,  0.6333, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2503,  0.1578,  1.0000,  0.0994,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[5.1193, 5.4130, 5.2728],
        [5.1193, 4.8712, 4.8174],
        [5.1193, 5.3902, 5.2360],
        [5.1193, 4.8733, 4.8244]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:888, step:0 
model_pd.l_p.mean(): 0.07588935643434525 
model_pd.l_d.mean(): -19.29164695739746 
model_pd.lagr.mean(): -19.215757369995117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5854], device='cuda:0')), ('power', tensor([-20.2587], device='cuda:0'))])
epoch£º888	 i:0 	 global-step:17760	 l-p:0.07588935643434525
epoch£º888	 i:1 	 global-step:17761	 l-p:0.2514418065547943
epoch£º888	 i:2 	 global-step:17762	 l-p:0.1343974471092224
epoch£º888	 i:3 	 global-step:17763	 l-p:0.11609826982021332
epoch£º888	 i:4 	 global-step:17764	 l-p:0.174216166138649
epoch£º888	 i:5 	 global-step:17765	 l-p:0.1230180412530899
epoch£º888	 i:6 	 global-step:17766	 l-p:0.10612905025482178
epoch£º888	 i:7 	 global-step:17767	 l-p:0.11411470919847488
epoch£º888	 i:8 	 global-step:17768	 l-p:0.1859653741121292
epoch£º888	 i:9 	 global-step:17769	 l-p:0.19250449538230896
====================================================================================================
====================================================================================================
====================================================================================================

epoch:889
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1161, 5.1161, 5.1161],
        [5.1161, 5.1161, 5.1161],
        [5.1161, 4.9444, 4.9864],
        [5.1161, 5.0530, 5.0969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:889, step:0 
model_pd.l_p.mean(): 0.16312792897224426 
model_pd.l_d.mean(): -20.734933853149414 
model_pd.lagr.mean(): -20.571805953979492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4231], device='cuda:0')), ('power', tensor([-21.5609], device='cuda:0'))])
epoch£º889	 i:0 	 global-step:17780	 l-p:0.16312792897224426
epoch£º889	 i:1 	 global-step:17781	 l-p:0.17359551787376404
epoch£º889	 i:2 	 global-step:17782	 l-p:0.12962664663791656
epoch£º889	 i:3 	 global-step:17783	 l-p:0.1432448774576187
epoch£º889	 i:4 	 global-step:17784	 l-p:0.11763793975114822
epoch£º889	 i:5 	 global-step:17785	 l-p:0.1480431854724884
epoch£º889	 i:6 	 global-step:17786	 l-p:0.12433536350727081
epoch£º889	 i:7 	 global-step:17787	 l-p:0.16182608902454376
epoch£º889	 i:8 	 global-step:17788	 l-p:0.10555160045623779
epoch£º889	 i:9 	 global-step:17789	 l-p:0.16131186485290527
====================================================================================================
====================================================================================================
====================================================================================================

epoch:890
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1434, 5.1434, 5.1434],
        [5.1434, 5.1434, 5.1434],
        [5.1434, 4.9318, 4.9381],
        [5.1434, 4.9236, 4.9190]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:890, step:0 
model_pd.l_p.mean(): 0.11423841118812561 
model_pd.l_d.mean(): -20.29830551147461 
model_pd.lagr.mean(): -20.184066772460938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4655], device='cuda:0')), ('power', tensor([-21.1600], device='cuda:0'))])
epoch£º890	 i:0 	 global-step:17800	 l-p:0.11423841118812561
epoch£º890	 i:1 	 global-step:17801	 l-p:0.1268877238035202
epoch£º890	 i:2 	 global-step:17802	 l-p:0.09280268102884293
epoch£º890	 i:3 	 global-step:17803	 l-p:0.12807397544384003
epoch£º890	 i:4 	 global-step:17804	 l-p:0.19143809378147125
epoch£º890	 i:5 	 global-step:17805	 l-p:0.13782213628292084
epoch£º890	 i:6 	 global-step:17806	 l-p:0.13851094245910645
epoch£º890	 i:7 	 global-step:17807	 l-p:0.14376835525035858
epoch£º890	 i:8 	 global-step:17808	 l-p:0.10733714699745178
epoch£º890	 i:9 	 global-step:17809	 l-p:0.12761066854000092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:891
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1713, 5.1502, 5.1684],
        [5.1713, 5.2724, 5.0235],
        [5.1713, 5.1120, 5.1540],
        [5.1713, 5.1689, 5.1712]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:891, step:0 
model_pd.l_p.mean(): 0.12653613090515137 
model_pd.l_d.mean(): -18.967924118041992 
model_pd.lagr.mean(): -18.841388702392578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5495], device='cuda:0')), ('power', tensor([-19.8917], device='cuda:0'))])
epoch£º891	 i:0 	 global-step:17820	 l-p:0.12653613090515137
epoch£º891	 i:1 	 global-step:17821	 l-p:0.1056637391448021
epoch£º891	 i:2 	 global-step:17822	 l-p:0.13566292822360992
epoch£º891	 i:3 	 global-step:17823	 l-p:0.1569584310054779
epoch£º891	 i:4 	 global-step:17824	 l-p:0.10535742342472076
epoch£º891	 i:5 	 global-step:17825	 l-p:0.11913039535284042
epoch£º891	 i:6 	 global-step:17826	 l-p:0.10293641686439514
epoch£º891	 i:7 	 global-step:17827	 l-p:0.14386551082134247
epoch£º891	 i:8 	 global-step:17828	 l-p:0.15876643359661102
epoch£º891	 i:9 	 global-step:17829	 l-p:0.10493960976600647
====================================================================================================
====================================================================================================
====================================================================================================

epoch:892
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1752, 4.9370, 4.8973],
        [5.1752, 5.1752, 5.1752],
        [5.1752, 5.0448, 4.7296],
        [5.1752, 5.1707, 5.1749]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:892, step:0 
model_pd.l_p.mean(): 0.13426333665847778 
model_pd.l_d.mean(): -19.84249496459961 
model_pd.lagr.mean(): -19.70823097229004 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5250], device='cuda:0')), ('power', tensor([-20.7573], device='cuda:0'))])
epoch£º892	 i:0 	 global-step:17840	 l-p:0.13426333665847778
epoch£º892	 i:1 	 global-step:17841	 l-p:0.13727246224880219
epoch£º892	 i:2 	 global-step:17842	 l-p:0.17884664237499237
epoch£º892	 i:3 	 global-step:17843	 l-p:0.041755497455596924
epoch£º892	 i:4 	 global-step:17844	 l-p:0.16265705227851868
epoch£º892	 i:5 	 global-step:17845	 l-p:0.09385742247104645
epoch£º892	 i:6 	 global-step:17846	 l-p:0.09907901287078857
epoch£º892	 i:7 	 global-step:17847	 l-p:0.11638299375772476
epoch£º892	 i:8 	 global-step:17848	 l-p:0.14536407589912415
epoch£º892	 i:9 	 global-step:17849	 l-p:0.14114461839199066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:893
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3208e-01, 9.1048e-01,
         1.0000e+00, 8.8938e-01, 1.0000e+00, 9.7683e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1626, 5.2282, 4.9626],
        [5.1626, 5.5555, 5.4772],
        [5.1626, 5.1492, 5.1613],
        [5.1626, 4.9568, 4.9688]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:893, step:0 
model_pd.l_p.mean(): 0.07196126133203506 
model_pd.l_d.mean(): -19.240962982177734 
model_pd.lagr.mean(): -19.169002532958984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5870], device='cuda:0')), ('power', tensor([-20.2087], device='cuda:0'))])
epoch£º893	 i:0 	 global-step:17860	 l-p:0.07196126133203506
epoch£º893	 i:1 	 global-step:17861	 l-p:0.17030452191829681
epoch£º893	 i:2 	 global-step:17862	 l-p:0.1364147663116455
epoch£º893	 i:3 	 global-step:17863	 l-p:0.12424960732460022
epoch£º893	 i:4 	 global-step:17864	 l-p:0.20784370601177216
epoch£º893	 i:5 	 global-step:17865	 l-p:0.12003675103187561
epoch£º893	 i:6 	 global-step:17866	 l-p:0.1165805235505104
epoch£º893	 i:7 	 global-step:17867	 l-p:0.15808548033237457
epoch£º893	 i:8 	 global-step:17868	 l-p:0.1108371913433075
epoch£º893	 i:9 	 global-step:17869	 l-p:0.13477328419685364
====================================================================================================
====================================================================================================
====================================================================================================

epoch:894
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1282, 5.1276, 5.1281],
        [5.1282, 5.1222, 5.1278],
        [5.1282, 4.9092, 4.6164],
        [5.1282, 4.8580, 4.7154]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:894, step:0 
model_pd.l_p.mean(): 0.12510016560554504 
model_pd.l_d.mean(): -19.895057678222656 
model_pd.lagr.mean(): -19.769956588745117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4812], device='cuda:0')), ('power', tensor([-20.7654], device='cuda:0'))])
epoch£º894	 i:0 	 global-step:17880	 l-p:0.12510016560554504
epoch£º894	 i:1 	 global-step:17881	 l-p:0.07559863477945328
epoch£º894	 i:2 	 global-step:17882	 l-p:0.1325887143611908
epoch£º894	 i:3 	 global-step:17883	 l-p:0.124350905418396
epoch£º894	 i:4 	 global-step:17884	 l-p:0.09540881961584091
epoch£º894	 i:5 	 global-step:17885	 l-p:0.12420331686735153
epoch£º894	 i:6 	 global-step:17886	 l-p:0.11035889387130737
epoch£º894	 i:7 	 global-step:17887	 l-p:0.24717657268047333
epoch£º894	 i:8 	 global-step:17888	 l-p:0.4142581522464752
epoch£º894	 i:9 	 global-step:17889	 l-p:0.2460440844297409
====================================================================================================
====================================================================================================
====================================================================================================

epoch:895
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0903, 5.0903, 5.0903],
        [5.0903, 5.0773, 5.0891],
        [5.0903, 5.3722, 5.2245],
        [5.0903, 4.8161, 4.6737]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:895, step:0 
model_pd.l_p.mean(): 0.13162264227867126 
model_pd.l_d.mean(): -18.789175033569336 
model_pd.lagr.mean(): -18.65755271911621 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6034], device='cuda:0')), ('power', tensor([-19.7655], device='cuda:0'))])
epoch£º895	 i:0 	 global-step:17900	 l-p:0.13162264227867126
epoch£º895	 i:1 	 global-step:17901	 l-p:0.18575623631477356
epoch£º895	 i:2 	 global-step:17902	 l-p:0.164951890707016
epoch£º895	 i:3 	 global-step:17903	 l-p:0.16709384322166443
epoch£º895	 i:4 	 global-step:17904	 l-p:0.14021341502666473
epoch£º895	 i:5 	 global-step:17905	 l-p:0.11948144435882568
epoch£º895	 i:6 	 global-step:17906	 l-p:0.137104794383049
epoch£º895	 i:7 	 global-step:17907	 l-p:0.1931307166814804
epoch£º895	 i:8 	 global-step:17908	 l-p:0.2693629860877991
epoch£º895	 i:9 	 global-step:17909	 l-p:0.09602786600589752
====================================================================================================
====================================================================================================
====================================================================================================

epoch:896
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1115, 5.4910, 5.4051],
        [5.1115, 5.0613, 5.0988],
        [5.1115, 5.1115, 5.1115],
        [5.1115, 4.8409, 4.7117]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:896, step:0 
model_pd.l_p.mean(): 0.13536296784877777 
model_pd.l_d.mean(): -20.109451293945312 
model_pd.lagr.mean(): -19.974088668823242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4893], device='cuda:0')), ('power', tensor([-20.9923], device='cuda:0'))])
epoch£º896	 i:0 	 global-step:17920	 l-p:0.13536296784877777
epoch£º896	 i:1 	 global-step:17921	 l-p:0.19883868098258972
epoch£º896	 i:2 	 global-step:17922	 l-p:0.1172950267791748
epoch£º896	 i:3 	 global-step:17923	 l-p:0.1918899565935135
epoch£º896	 i:4 	 global-step:17924	 l-p:0.176498681306839
epoch£º896	 i:5 	 global-step:17925	 l-p:0.13041280210018158
epoch£º896	 i:6 	 global-step:17926	 l-p:0.12073596566915512
epoch£º896	 i:7 	 global-step:17927	 l-p:0.15349972248077393
epoch£º896	 i:8 	 global-step:17928	 l-p:0.11368808150291443
epoch£º896	 i:9 	 global-step:17929	 l-p:0.13227815926074982
====================================================================================================
====================================================================================================
====================================================================================================

epoch:897
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4833e-02, 2.6045e-02,
         1.0000e+00, 1.0463e-02, 1.0000e+00, 4.0173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1423, 5.0976, 5.1319],
        [5.1423, 5.1378, 5.1421],
        [5.1423, 5.0961, 5.1313],
        [5.1423, 4.8945, 4.8401]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:897, step:0 
model_pd.l_p.mean(): 0.1411065012216568 
model_pd.l_d.mean(): -20.39162826538086 
model_pd.lagr.mean(): -20.25052261352539 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4274], device='cuda:0')), ('power', tensor([-21.2156], device='cuda:0'))])
epoch£º897	 i:0 	 global-step:17940	 l-p:0.1411065012216568
epoch£º897	 i:1 	 global-step:17941	 l-p:0.07843174785375595
epoch£º897	 i:2 	 global-step:17942	 l-p:0.11888869851827621
epoch£º897	 i:3 	 global-step:17943	 l-p:0.13932397961616516
epoch£º897	 i:4 	 global-step:17944	 l-p:0.1606680303812027
epoch£º897	 i:5 	 global-step:17945	 l-p:0.14085857570171356
epoch£º897	 i:6 	 global-step:17946	 l-p:0.14858505129814148
epoch£º897	 i:7 	 global-step:17947	 l-p:0.07457908242940903
epoch£º897	 i:8 	 global-step:17948	 l-p:0.2224031239748001
epoch£º897	 i:9 	 global-step:17949	 l-p:0.144271120429039
====================================================================================================
====================================================================================================
====================================================================================================

epoch:898
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1475, 4.9022, 4.8529],
        [5.1475, 5.1469, 5.1475],
        [5.1475, 5.1029, 5.1372],
        [5.1475, 5.6057, 5.5719]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:898, step:0 
model_pd.l_p.mean(): 0.15748688578605652 
model_pd.l_d.mean(): -19.504901885986328 
model_pd.lagr.mean(): -19.347415924072266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5146], device='cuda:0')), ('power', tensor([-20.4026], device='cuda:0'))])
epoch£º898	 i:0 	 global-step:17960	 l-p:0.15748688578605652
epoch£º898	 i:1 	 global-step:17961	 l-p:0.16282598674297333
epoch£º898	 i:2 	 global-step:17962	 l-p:0.15621724724769592
epoch£º898	 i:3 	 global-step:17963	 l-p:0.14233118295669556
epoch£º898	 i:4 	 global-step:17964	 l-p:0.06733179092407227
epoch£º898	 i:5 	 global-step:17965	 l-p:0.08711475133895874
epoch£º898	 i:6 	 global-step:17966	 l-p:0.10650674253702164
epoch£º898	 i:7 	 global-step:17967	 l-p:0.14460419118404388
epoch£º898	 i:8 	 global-step:17968	 l-p:0.14489682018756866
epoch£º898	 i:9 	 global-step:17969	 l-p:0.1082368940114975
====================================================================================================
====================================================================================================
====================================================================================================

epoch:899
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1930, 5.0923, 4.7781],
        [5.1930, 4.9407, 4.7079],
        [5.1930, 4.9510, 4.9012],
        [5.1930, 5.1919, 5.1930]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:899, step:0 
model_pd.l_p.mean(): 0.15401986241340637 
model_pd.l_d.mean(): -19.942350387573242 
model_pd.lagr.mean(): -19.788330078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4468], device='cuda:0')), ('power', tensor([-20.7780], device='cuda:0'))])
epoch£º899	 i:0 	 global-step:17980	 l-p:0.15401986241340637
epoch£º899	 i:1 	 global-step:17981	 l-p:0.07301806658506393
epoch£º899	 i:2 	 global-step:17982	 l-p:0.09346882253885269
epoch£º899	 i:3 	 global-step:17983	 l-p:0.11960769444704056
epoch£º899	 i:4 	 global-step:17984	 l-p:0.13009822368621826
epoch£º899	 i:5 	 global-step:17985	 l-p:0.18309837579727173
epoch£º899	 i:6 	 global-step:17986	 l-p:0.14921177923679352
epoch£º899	 i:7 	 global-step:17987	 l-p:0.0195098128169775
epoch£º899	 i:8 	 global-step:17988	 l-p:0.12449350953102112
epoch£º899	 i:9 	 global-step:17989	 l-p:0.12285615503787994
====================================================================================================
====================================================================================================
====================================================================================================

epoch:900
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0940e-01, 5.2322e-02,
         1.0000e+00, 2.5024e-02, 1.0000e+00, 4.7827e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1959, 4.9330, 4.8011],
        [5.1959, 5.0874, 5.1428],
        [5.1959, 5.1959, 5.1959],
        [5.1959, 5.0943, 5.1490]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:900, step:0 
model_pd.l_p.mean(): 0.1458078771829605 
model_pd.l_d.mean(): -20.480310440063477 
model_pd.lagr.mean(): -20.334503173828125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4164], device='cuda:0')), ('power', tensor([-21.2945], device='cuda:0'))])
epoch£º900	 i:0 	 global-step:18000	 l-p:0.1458078771829605
epoch£º900	 i:1 	 global-step:18001	 l-p:0.0443575456738472
epoch£º900	 i:2 	 global-step:18002	 l-p:0.1505628228187561
epoch£º900	 i:3 	 global-step:18003	 l-p:0.1554090678691864
epoch£º900	 i:4 	 global-step:18004	 l-p:0.12617087364196777
epoch£º900	 i:5 	 global-step:18005	 l-p:0.13169364631175995
epoch£º900	 i:6 	 global-step:18006	 l-p:0.09975622594356537
epoch£º900	 i:7 	 global-step:18007	 l-p:0.11979226768016815
epoch£º900	 i:8 	 global-step:18008	 l-p:0.12560953199863434
epoch£º900	 i:9 	 global-step:18009	 l-p:0.11656481772661209
====================================================================================================
====================================================================================================
====================================================================================================

epoch:901
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1585, 5.1160, 5.1490],
        [5.1585, 5.1542, 5.1583],
        [5.1585, 5.2732, 5.0304],
        [5.1585, 4.9267, 4.9035]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:901, step:0 
model_pd.l_p.mean(): 0.18080826103687286 
model_pd.l_d.mean(): -20.29515266418457 
model_pd.lagr.mean(): -20.114343643188477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4736], device='cuda:0')), ('power', tensor([-21.1652], device='cuda:0'))])
epoch£º901	 i:0 	 global-step:18020	 l-p:0.18080826103687286
epoch£º901	 i:1 	 global-step:18021	 l-p:0.14138364791870117
epoch£º901	 i:2 	 global-step:18022	 l-p:0.12290769070386887
epoch£º901	 i:3 	 global-step:18023	 l-p:0.09438535571098328
epoch£º901	 i:4 	 global-step:18024	 l-p:0.12611150741577148
epoch£º901	 i:5 	 global-step:18025	 l-p:0.1490379422903061
epoch£º901	 i:6 	 global-step:18026	 l-p:0.18233688175678253
epoch£º901	 i:7 	 global-step:18027	 l-p:0.10178037732839584
epoch£º901	 i:8 	 global-step:18028	 l-p:0.2142406553030014
epoch£º901	 i:9 	 global-step:18029	 l-p:0.1183677464723587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:902
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1197, 5.1166, 5.1196],
        [5.1197, 4.8818, 4.8530],
        [5.1197, 4.9767, 4.6564],
        [5.1197, 5.0461, 5.0944]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:902, step:0 
model_pd.l_p.mean(): 0.13283272087574005 
model_pd.l_d.mean(): -19.42667579650879 
model_pd.lagr.mean(): -19.293842315673828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4899], device='cuda:0')), ('power', tensor([-20.2974], device='cuda:0'))])
epoch£º902	 i:0 	 global-step:18040	 l-p:0.13283272087574005
epoch£º902	 i:1 	 global-step:18041	 l-p:0.16287639737129211
epoch£º902	 i:2 	 global-step:18042	 l-p:0.1955397129058838
epoch£º902	 i:3 	 global-step:18043	 l-p:0.1929597407579422
epoch£º902	 i:4 	 global-step:18044	 l-p:0.11149489879608154
epoch£º902	 i:5 	 global-step:18045	 l-p:0.17420370876789093
epoch£º902	 i:6 	 global-step:18046	 l-p:0.11597055196762085
epoch£º902	 i:7 	 global-step:18047	 l-p:0.10794030874967575
epoch£º902	 i:8 	 global-step:18048	 l-p:0.13768048584461212
epoch£º902	 i:9 	 global-step:18049	 l-p:0.11033736914396286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:903
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1402, 5.0925, 5.1286],
        [5.1402, 5.4376, 5.2980],
        [5.1402, 4.9550, 4.9882],
        [5.1402, 4.9930, 5.0454]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:903, step:0 
model_pd.l_p.mean(): 0.14386306703090668 
model_pd.l_d.mean(): -20.10909652709961 
model_pd.lagr.mean(): -19.965232849121094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5121], device='cuda:0')), ('power', tensor([-21.0155], device='cuda:0'))])
epoch£º903	 i:0 	 global-step:18060	 l-p:0.14386306703090668
epoch£º903	 i:1 	 global-step:18061	 l-p:0.13843226432800293
epoch£º903	 i:2 	 global-step:18062	 l-p:0.14920125901699066
epoch£º903	 i:3 	 global-step:18063	 l-p:0.1336870789527893
epoch£º903	 i:4 	 global-step:18064	 l-p:0.09939483553171158
epoch£º903	 i:5 	 global-step:18065	 l-p:0.1554955393075943
epoch£º903	 i:6 	 global-step:18066	 l-p:0.13242383301258087
epoch£º903	 i:7 	 global-step:18067	 l-p:0.12723039090633392
epoch£º903	 i:8 	 global-step:18068	 l-p:0.1264893114566803
epoch£º903	 i:9 	 global-step:18069	 l-p:0.14331533014774323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:904
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1504, 5.1249, 5.1465],
        [5.1504, 4.9685, 5.0038],
        [5.1504, 5.1504, 5.1504],
        [5.1504, 5.1461, 5.1502]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:904, step:0 
model_pd.l_p.mean(): 0.11680648475885391 
model_pd.l_d.mean(): -19.206838607788086 
model_pd.lagr.mean(): -19.09003257751465 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5352], device='cuda:0')), ('power', tensor([-20.1203], device='cuda:0'))])
epoch£º904	 i:0 	 global-step:18080	 l-p:0.11680648475885391
epoch£º904	 i:1 	 global-step:18081	 l-p:0.1524750292301178
epoch£º904	 i:2 	 global-step:18082	 l-p:0.15199412405490875
epoch£º904	 i:3 	 global-step:18083	 l-p:0.14585235714912415
epoch£º904	 i:4 	 global-step:18084	 l-p:0.12696364521980286
epoch£º904	 i:5 	 global-step:18085	 l-p:0.19090721011161804
epoch£º904	 i:6 	 global-step:18086	 l-p:0.1225099191069603
epoch£º904	 i:7 	 global-step:18087	 l-p:0.19229282438755035
epoch£º904	 i:8 	 global-step:18088	 l-p:0.07994566857814789
epoch£º904	 i:9 	 global-step:18089	 l-p:0.12802235782146454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:905
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1216, 5.1208, 5.1216],
        [5.1216, 5.0113, 5.0678],
        [5.1216, 4.8514, 4.6461],
        [5.1216, 4.8674, 4.8031]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:905, step:0 
model_pd.l_p.mean(): 0.13839483261108398 
model_pd.l_d.mean(): -20.415027618408203 
model_pd.lagr.mean(): -20.27663230895996 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4212], device='cuda:0')), ('power', tensor([-21.2330], device='cuda:0'))])
epoch£º905	 i:0 	 global-step:18100	 l-p:0.13839483261108398
epoch£º905	 i:1 	 global-step:18101	 l-p:0.12438367307186127
epoch£º905	 i:2 	 global-step:18102	 l-p:0.132046177983284
epoch£º905	 i:3 	 global-step:18103	 l-p:0.12339963763952255
epoch£º905	 i:4 	 global-step:18104	 l-p:0.09157727658748627
epoch£º905	 i:5 	 global-step:18105	 l-p:0.20008140802383423
epoch£º905	 i:6 	 global-step:18106	 l-p:0.19921337068080902
epoch£º905	 i:7 	 global-step:18107	 l-p:0.24404004216194153
epoch£º905	 i:8 	 global-step:18108	 l-p:0.13665485382080078
epoch£º905	 i:9 	 global-step:18109	 l-p:0.17764468491077423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:906
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.4003e-01, 6.6937e-01,
         1.0000e+00, 6.0546e-01, 1.0000e+00, 9.0452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1141, 4.8777, 4.5956],
        [5.1141, 4.8890, 4.5963],
        [5.1141, 5.1962, 4.9377],
        [5.1141, 5.1138, 5.1141]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:906, step:0 
model_pd.l_p.mean(): 0.12613874673843384 
model_pd.l_d.mean(): -20.078128814697266 
model_pd.lagr.mean(): -19.951990127563477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4777], device='cuda:0')), ('power', tensor([-20.9483], device='cuda:0'))])
epoch£º906	 i:0 	 global-step:18120	 l-p:0.12613874673843384
epoch£º906	 i:1 	 global-step:18121	 l-p:0.09933149069547653
epoch£º906	 i:2 	 global-step:18122	 l-p:0.1325208842754364
epoch£º906	 i:3 	 global-step:18123	 l-p:0.23707346618175507
epoch£º906	 i:4 	 global-step:18124	 l-p:0.14749982953071594
epoch£º906	 i:5 	 global-step:18125	 l-p:0.18820659816265106
epoch£º906	 i:6 	 global-step:18126	 l-p:0.15034930408000946
epoch£º906	 i:7 	 global-step:18127	 l-p:0.12153627723455429
epoch£º906	 i:8 	 global-step:18128	 l-p:0.11666497588157654
epoch£º906	 i:9 	 global-step:18129	 l-p:0.15293367207050323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:907
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4293e-01, 3.3763e-01,
         1.0000e+00, 2.5737e-01, 1.0000e+00, 7.6228e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1399, 5.1125, 5.1355],
        [5.1399, 5.0139, 5.0702],
        [5.1399, 5.0609, 5.1110],
        [5.1399, 4.9021, 4.6269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:907, step:0 
model_pd.l_p.mean(): 0.21892251074314117 
model_pd.l_d.mean(): -19.46932029724121 
model_pd.lagr.mean(): -19.250398635864258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4780], device='cuda:0')), ('power', tensor([-20.3284], device='cuda:0'))])
epoch£º907	 i:0 	 global-step:18140	 l-p:0.21892251074314117
epoch£º907	 i:1 	 global-step:18141	 l-p:0.12845686078071594
epoch£º907	 i:2 	 global-step:18142	 l-p:0.05850455164909363
epoch£º907	 i:3 	 global-step:18143	 l-p:0.1588374674320221
epoch£º907	 i:4 	 global-step:18144	 l-p:0.1337338536977768
epoch£º907	 i:5 	 global-step:18145	 l-p:0.11300945281982422
epoch£º907	 i:6 	 global-step:18146	 l-p:0.1339181661605835
epoch£º907	 i:7 	 global-step:18147	 l-p:0.11803952604532242
epoch£º907	 i:8 	 global-step:18148	 l-p:0.17781098186969757
epoch£º907	 i:9 	 global-step:18149	 l-p:0.1251118779182434
====================================================================================================
====================================================================================================
====================================================================================================

epoch:908
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8628,  0.8214,  1.0000,  0.7820,
          1.0000,  0.9520, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1810,  0.1024,  1.0000,  0.0579,
          1.0000,  0.5657, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4788,  0.3746,  1.0000,  0.2931,
          1.0000,  0.7823, 31.6228]], device='cuda:0')
 pt:tensor([[5.1508, 5.4278, 5.2752],
        [5.1508, 4.9615, 4.9914],
        [5.1508, 4.9481, 4.6459],
        [5.1508, 4.9391, 4.6422]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:908, step:0 
model_pd.l_p.mean(): 0.11673030257225037 
model_pd.l_d.mean(): -19.187881469726562 
model_pd.lagr.mean(): -19.071151733398438 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5869], device='cuda:0')), ('power', tensor([-20.1545], device='cuda:0'))])
epoch£º908	 i:0 	 global-step:18160	 l-p:0.11673030257225037
epoch£º908	 i:1 	 global-step:18161	 l-p:0.13519255816936493
epoch£º908	 i:2 	 global-step:18162	 l-p:0.11811359971761703
epoch£º908	 i:3 	 global-step:18163	 l-p:0.12880805134773254
epoch£º908	 i:4 	 global-step:18164	 l-p:0.09341765940189362
epoch£º908	 i:5 	 global-step:18165	 l-p:0.13471032679080963
epoch£º908	 i:6 	 global-step:18166	 l-p:0.184233158826828
epoch£º908	 i:7 	 global-step:18167	 l-p:0.12991462647914886
epoch£º908	 i:8 	 global-step:18168	 l-p:0.13556018471717834
epoch£º908	 i:9 	 global-step:18169	 l-p:0.15013471245765686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:909
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2609e-02, 1.0418e-02,
         1.0000e+00, 3.3284e-03, 1.0000e+00, 3.1948e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1608, 5.1468, 5.1594],
        [5.1608, 5.1608, 5.1608],
        [5.1608, 5.0497, 4.7318],
        [5.1608, 5.1608, 5.1608]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:909, step:0 
model_pd.l_p.mean(): 0.13743284344673157 
model_pd.l_d.mean(): -20.245609283447266 
model_pd.lagr.mean(): -20.108177185058594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4460], device='cuda:0')), ('power', tensor([-21.0861], device='cuda:0'))])
epoch£º909	 i:0 	 global-step:18180	 l-p:0.13743284344673157
epoch£º909	 i:1 	 global-step:18181	 l-p:0.13251113891601562
epoch£º909	 i:2 	 global-step:18182	 l-p:0.11805608123540878
epoch£º909	 i:3 	 global-step:18183	 l-p:0.10685161501169205
epoch£º909	 i:4 	 global-step:18184	 l-p:0.13176846504211426
epoch£º909	 i:5 	 global-step:18185	 l-p:0.12234096974134445
epoch£º909	 i:6 	 global-step:18186	 l-p:0.21637065708637238
epoch£º909	 i:7 	 global-step:18187	 l-p:0.09362772107124329
epoch£º909	 i:8 	 global-step:18188	 l-p:0.10054299980401993
epoch£º909	 i:9 	 global-step:18189	 l-p:0.1498338133096695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:910
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1506, 5.1506, 5.1506],
        [5.1506, 5.1502, 5.1506],
        [5.1506, 5.1506, 5.1506],
        [5.1506, 4.9210, 4.6383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:910, step:0 
model_pd.l_p.mean(): 0.178883895277977 
model_pd.l_d.mean(): -19.1675968170166 
model_pd.lagr.mean(): -18.988712310791016 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5366], device='cuda:0')), ('power', tensor([-20.0818], device='cuda:0'))])
epoch£º910	 i:0 	 global-step:18200	 l-p:0.178883895277977
epoch£º910	 i:1 	 global-step:18201	 l-p:0.1531611829996109
epoch£º910	 i:2 	 global-step:18202	 l-p:0.0603015311062336
epoch£º910	 i:3 	 global-step:18203	 l-p:0.1500895619392395
epoch£º910	 i:4 	 global-step:18204	 l-p:0.16785266995429993
epoch£º910	 i:5 	 global-step:18205	 l-p:0.1245163232088089
epoch£º910	 i:6 	 global-step:18206	 l-p:0.11370754987001419
epoch£º910	 i:7 	 global-step:18207	 l-p:0.18705923855304718
epoch£º910	 i:8 	 global-step:18208	 l-p:0.1048259437084198
epoch£º910	 i:9 	 global-step:18209	 l-p:0.14314976334571838
====================================================================================================
====================================================================================================
====================================================================================================

epoch:911
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1255, 4.9237, 4.9442],
        [5.1255, 5.1255, 5.1255],
        [5.1255, 4.9808, 4.6600],
        [5.1255, 4.8965, 4.8835]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:911, step:0 
model_pd.l_p.mean(): 0.15858237445354462 
model_pd.l_d.mean(): -20.575769424438477 
model_pd.lagr.mean(): -20.417186737060547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4357], device='cuda:0')), ('power', tensor([-21.4117], device='cuda:0'))])
epoch£º911	 i:0 	 global-step:18220	 l-p:0.15858237445354462
epoch£º911	 i:1 	 global-step:18221	 l-p:0.2649988532066345
epoch£º911	 i:2 	 global-step:18222	 l-p:0.13352669775485992
epoch£º911	 i:3 	 global-step:18223	 l-p:0.1831839680671692
epoch£º911	 i:4 	 global-step:18224	 l-p:0.13287724554538727
epoch£º911	 i:5 	 global-step:18225	 l-p:0.15136435627937317
epoch£º911	 i:6 	 global-step:18226	 l-p:0.09066135436296463
epoch£º911	 i:7 	 global-step:18227	 l-p:0.13116618990898132
epoch£º911	 i:8 	 global-step:18228	 l-p:0.11299734562635422
epoch£º911	 i:9 	 global-step:18229	 l-p:0.09468863904476166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:912
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1226, 5.2271, 4.9789],
        [5.1226, 5.0469, 5.0960],
        [5.1226, 4.8604, 4.6210],
        [5.1226, 5.1174, 5.1223]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:912, step:0 
model_pd.l_p.mean(): 0.155303955078125 
model_pd.l_d.mean(): -19.26475715637207 
model_pd.lagr.mean(): -19.109453201293945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5860], device='cuda:0')), ('power', tensor([-20.2319], device='cuda:0'))])
epoch£º912	 i:0 	 global-step:18240	 l-p:0.155303955078125
epoch£º912	 i:1 	 global-step:18241	 l-p:0.1125178337097168
epoch£º912	 i:2 	 global-step:18242	 l-p:0.1704087257385254
epoch£º912	 i:3 	 global-step:18243	 l-p:0.17301824688911438
epoch£º912	 i:4 	 global-step:18244	 l-p:0.10738793760538101
epoch£º912	 i:5 	 global-step:18245	 l-p:0.16495877504348755
epoch£º912	 i:6 	 global-step:18246	 l-p:0.1394355744123459
epoch£º912	 i:7 	 global-step:18247	 l-p:0.13819007575511932
epoch£º912	 i:8 	 global-step:18248	 l-p:0.16219128668308258
epoch£º912	 i:9 	 global-step:18249	 l-p:0.12765318155288696
====================================================================================================
====================================================================================================
====================================================================================================

epoch:913
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5884e-03, 1.8533e-04,
         1.0000e+00, 2.1624e-05, 1.0000e+00, 1.1668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1265, 5.1264, 5.1265],
        [5.1265, 4.8647, 4.6252],
        [5.1265, 5.1260, 5.1265],
        [5.1265, 4.9096, 4.6112]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:913, step:0 
model_pd.l_p.mean(): 0.11003964394330978 
model_pd.l_d.mean(): -19.989137649536133 
model_pd.lagr.mean(): -19.879098892211914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5124], device='cuda:0')), ('power', tensor([-20.8936], device='cuda:0'))])
epoch£º913	 i:0 	 global-step:18260	 l-p:0.11003964394330978
epoch£º913	 i:1 	 global-step:18261	 l-p:0.11499275267124176
epoch£º913	 i:2 	 global-step:18262	 l-p:0.126736581325531
epoch£º913	 i:3 	 global-step:18263	 l-p:0.14130030572414398
epoch£º913	 i:4 	 global-step:18264	 l-p:0.14087457954883575
epoch£º913	 i:5 	 global-step:18265	 l-p:0.18155023455619812
epoch£º913	 i:6 	 global-step:18266	 l-p:0.17738276720046997
epoch£º913	 i:7 	 global-step:18267	 l-p:0.1850329041481018
epoch£º913	 i:8 	 global-step:18268	 l-p:0.23266200721263885
epoch£º913	 i:9 	 global-step:18269	 l-p:0.12270964682102203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:914
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8317e-01, 1.8595e-01,
         1.0000e+00, 1.2211e-01, 1.0000e+00, 6.5667e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1205, 4.8557, 4.7615],
        [5.1205, 5.1204, 5.1205],
        [5.1205, 5.0701, 5.1077],
        [5.1205, 4.8461, 4.6862]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:914, step:0 
model_pd.l_p.mean(): 0.1282576471567154 
model_pd.l_d.mean(): -19.797250747680664 
model_pd.lagr.mean(): -19.66899299621582 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4985], device='cuda:0')), ('power', tensor([-20.6838], device='cuda:0'))])
epoch£º914	 i:0 	 global-step:18280	 l-p:0.1282576471567154
epoch£º914	 i:1 	 global-step:18281	 l-p:0.10425620526075363
epoch£º914	 i:2 	 global-step:18282	 l-p:0.09286055713891983
epoch£º914	 i:3 	 global-step:18283	 l-p:0.16300548613071442
epoch£º914	 i:4 	 global-step:18284	 l-p:0.1404198557138443
epoch£º914	 i:5 	 global-step:18285	 l-p:0.16269603371620178
epoch£º914	 i:6 	 global-step:18286	 l-p:0.19129544496536255
epoch£º914	 i:7 	 global-step:18287	 l-p:0.12404841184616089
epoch£º914	 i:8 	 global-step:18288	 l-p:0.10419907420873642
epoch£º914	 i:9 	 global-step:18289	 l-p:0.1640235036611557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:915
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1667, 5.0920, 5.1406],
        [5.1667, 5.1646, 5.1667],
        [5.1667, 5.1667, 5.1667],
        [5.1667, 5.1622, 5.1665]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:915, step:0 
model_pd.l_p.mean(): 0.11166202276945114 
model_pd.l_d.mean(): -20.491931915283203 
model_pd.lagr.mean(): -20.38027000427246 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4452], device='cuda:0')), ('power', tensor([-21.3363], device='cuda:0'))])
epoch£º915	 i:0 	 global-step:18300	 l-p:0.11166202276945114
epoch£º915	 i:1 	 global-step:18301	 l-p:0.12341323494911194
epoch£º915	 i:2 	 global-step:18302	 l-p:0.09103575348854065
epoch£º915	 i:3 	 global-step:18303	 l-p:0.134047269821167
epoch£º915	 i:4 	 global-step:18304	 l-p:0.18750084936618805
epoch£º915	 i:5 	 global-step:18305	 l-p:0.10947006940841675
epoch£º915	 i:6 	 global-step:18306	 l-p:0.18550726771354675
epoch£º915	 i:7 	 global-step:18307	 l-p:0.10535728186368942
epoch£º915	 i:8 	 global-step:18308	 l-p:0.11584464460611343
epoch£º915	 i:9 	 global-step:18309	 l-p:0.12241262197494507
====================================================================================================
====================================================================================================
====================================================================================================

epoch:916
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8137e-01, 9.7524e-01,
         1.0000e+00, 9.6914e-01, 1.0000e+00, 9.9375e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1781, 5.6512, 5.6258],
        [5.1781, 5.1781, 5.1781],
        [5.1781, 5.1526, 5.1742],
        [5.1781, 4.9473, 4.6693]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:916, step:0 
model_pd.l_p.mean(): 0.13427549600601196 
model_pd.l_d.mean(): -19.778078079223633 
model_pd.lagr.mean(): -19.643802642822266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4441], device='cuda:0')), ('power', tensor([-20.6079], device='cuda:0'))])
epoch£º916	 i:0 	 global-step:18320	 l-p:0.13427549600601196
epoch£º916	 i:1 	 global-step:18321	 l-p:0.10019689053297043
epoch£º916	 i:2 	 global-step:18322	 l-p:0.1924632042646408
epoch£º916	 i:3 	 global-step:18323	 l-p:0.1018507331609726
epoch£º916	 i:4 	 global-step:18324	 l-p:0.12257754802703857
epoch£º916	 i:5 	 global-step:18325	 l-p:0.10288514941930771
epoch£º916	 i:6 	 global-step:18326	 l-p:0.12198140472173691
epoch£º916	 i:7 	 global-step:18327	 l-p:0.15068720281124115
epoch£º916	 i:8 	 global-step:18328	 l-p:0.1278662085533142
epoch£º916	 i:9 	 global-step:18329	 l-p:0.1241970956325531
====================================================================================================
====================================================================================================
====================================================================================================

epoch:917
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1612, 5.1184, 5.1516],
        [5.1612, 5.1612, 5.1612],
        [5.1612, 5.4357, 5.2806],
        [5.1612, 5.0918, 5.1383]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:917, step:0 
model_pd.l_p.mean(): 0.12821125984191895 
model_pd.l_d.mean(): -20.749162673950195 
model_pd.lagr.mean(): -20.62095069885254 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3842], device='cuda:0')), ('power', tensor([-21.5351], device='cuda:0'))])
epoch£º917	 i:0 	 global-step:18340	 l-p:0.12821125984191895
epoch£º917	 i:1 	 global-step:18341	 l-p:0.128961443901062
epoch£º917	 i:2 	 global-step:18342	 l-p:0.1228795200586319
epoch£º917	 i:3 	 global-step:18343	 l-p:0.1460026204586029
epoch£º917	 i:4 	 global-step:18344	 l-p:0.15568333864212036
epoch£º917	 i:5 	 global-step:18345	 l-p:0.12155315279960632
epoch£º917	 i:6 	 global-step:18346	 l-p:0.16353824734687805
epoch£º917	 i:7 	 global-step:18347	 l-p:0.18955688178539276
epoch£º917	 i:8 	 global-step:18348	 l-p:0.14386728405952454
epoch£º917	 i:9 	 global-step:18349	 l-p:0.08832534402608871
====================================================================================================
====================================================================================================
====================================================================================================

epoch:918
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1333, 5.1333, 5.1333],
        [5.1333, 5.1194, 5.1319],
        [5.1333, 5.1333, 5.1333],
        [5.1333, 4.9827, 4.6617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:918, step:0 
model_pd.l_p.mean(): 0.15856918692588806 
model_pd.l_d.mean(): -18.227962493896484 
model_pd.lagr.mean(): -18.069393157958984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6130], device='cuda:0')), ('power', tensor([-19.2038], device='cuda:0'))])
epoch£º918	 i:0 	 global-step:18360	 l-p:0.15856918692588806
epoch£º918	 i:1 	 global-step:18361	 l-p:0.15371403098106384
epoch£º918	 i:2 	 global-step:18362	 l-p:0.07562950253486633
epoch£º918	 i:3 	 global-step:18363	 l-p:0.1553550660610199
epoch£º918	 i:4 	 global-step:18364	 l-p:0.16426192224025726
epoch£º918	 i:5 	 global-step:18365	 l-p:0.1323762685060501
epoch£º918	 i:6 	 global-step:18366	 l-p:0.1319584995508194
epoch£º918	 i:7 	 global-step:18367	 l-p:0.19409380853176117
epoch£º918	 i:8 	 global-step:18368	 l-p:0.10865465551614761
epoch£º918	 i:9 	 global-step:18369	 l-p:0.12104291468858719
====================================================================================================
====================================================================================================
====================================================================================================

epoch:919
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9321,  0.9105,  1.0000,  0.8894,
          1.0000,  0.9768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6345,  0.5452,  1.0000,  0.4685,
          1.0000,  0.8593, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1946,  0.1128,  1.0000,  0.0654,
          1.0000,  0.5795, 31.6228]], device='cuda:0')
 pt:tensor([[5.1408, 5.5208, 5.4329],
        [5.1408, 5.0850, 4.7736],
        [5.1408, 5.1952, 4.9229],
        [5.1408, 4.9363, 4.9541]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:919, step:0 
model_pd.l_p.mean(): 0.1403963565826416 
model_pd.l_d.mean(): -19.308462142944336 
model_pd.lagr.mean(): -19.168066024780273 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5134], device='cuda:0')), ('power', tensor([-20.2012], device='cuda:0'))])
epoch£º919	 i:0 	 global-step:18380	 l-p:0.1403963565826416
epoch£º919	 i:1 	 global-step:18381	 l-p:0.14041921496391296
epoch£º919	 i:2 	 global-step:18382	 l-p:0.16346131265163422
epoch£º919	 i:3 	 global-step:18383	 l-p:0.11352481693029404
epoch£º919	 i:4 	 global-step:18384	 l-p:0.1765272170305252
epoch£º919	 i:5 	 global-step:18385	 l-p:0.14827071130275726
epoch£º919	 i:6 	 global-step:18386	 l-p:0.1299583911895752
epoch£º919	 i:7 	 global-step:18387	 l-p:0.13770517706871033
epoch£º919	 i:8 	 global-step:18388	 l-p:0.11913098394870758
epoch£º919	 i:9 	 global-step:18389	 l-p:0.1315489560365677
====================================================================================================
====================================================================================================
====================================================================================================

epoch:920
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5035e-01, 1.5778e-01,
         1.0000e+00, 9.9442e-02, 1.0000e+00, 6.3025e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1375, 4.9210, 4.6222],
        [5.1375, 5.1375, 5.1375],
        [5.1375, 4.9280, 4.9410],
        [5.1375, 4.8889, 4.8398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:920, step:0 
model_pd.l_p.mean(): 0.15060651302337646 
model_pd.l_d.mean(): -19.720104217529297 
model_pd.lagr.mean(): -19.56949806213379 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5095], device='cuda:0')), ('power', tensor([-20.6166], device='cuda:0'))])
epoch£º920	 i:0 	 global-step:18400	 l-p:0.15060651302337646
epoch£º920	 i:1 	 global-step:18401	 l-p:0.11228755861520767
epoch£º920	 i:2 	 global-step:18402	 l-p:0.14590337872505188
epoch£º920	 i:3 	 global-step:18403	 l-p:0.11550373584032059
epoch£º920	 i:4 	 global-step:18404	 l-p:0.1182938665151596
epoch£º920	 i:5 	 global-step:18405	 l-p:0.17010390758514404
epoch£º920	 i:6 	 global-step:18406	 l-p:0.1455499231815338
epoch£º920	 i:7 	 global-step:18407	 l-p:0.10934289544820786
epoch£º920	 i:8 	 global-step:18408	 l-p:0.20948117971420288
epoch£º920	 i:9 	 global-step:18409	 l-p:0.15599414706230164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:921
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7906e-01, 4.8264e-01,
         1.0000e+00, 4.0229e-01, 1.0000e+00, 8.3350e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1198, 5.1198, 5.1198],
        [5.1198, 4.9936, 4.6704],
        [5.1198, 5.0435, 5.0929],
        [5.1198, 5.1198, 5.1198]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:921, step:0 
model_pd.l_p.mean(): 0.16264168918132782 
model_pd.l_d.mean(): -19.143041610717773 
model_pd.lagr.mean(): -18.98040008544922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5756], device='cuda:0')), ('power', tensor([-20.0971], device='cuda:0'))])
epoch£º921	 i:0 	 global-step:18420	 l-p:0.16264168918132782
epoch£º921	 i:1 	 global-step:18421	 l-p:0.13725437223911285
epoch£º921	 i:2 	 global-step:18422	 l-p:0.15448519587516785
epoch£º921	 i:3 	 global-step:18423	 l-p:0.16365684568881989
epoch£º921	 i:4 	 global-step:18424	 l-p:0.10832054913043976
epoch£º921	 i:5 	 global-step:18425	 l-p:0.1094353049993515
epoch£º921	 i:6 	 global-step:18426	 l-p:0.12736700475215912
epoch£º921	 i:7 	 global-step:18427	 l-p:0.14504584670066833
epoch£º921	 i:8 	 global-step:18428	 l-p:0.16504952311515808
epoch£º921	 i:9 	 global-step:18429	 l-p:0.15410758554935455
====================================================================================================
====================================================================================================
====================================================================================================

epoch:922
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5014e-01, 6.8159e-01,
         1.0000e+00, 6.1931e-01, 1.0000e+00, 9.0862e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1498, 5.1498, 5.1498],
        [5.1498, 4.9592, 4.9894],
        [5.1498, 5.4463, 5.3046],
        [5.1498, 5.2530, 5.0030]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:922, step:0 
model_pd.l_p.mean(): 0.1743653416633606 
model_pd.l_d.mean(): -20.366153717041016 
model_pd.lagr.mean(): -20.191787719726562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4597], device='cuda:0')), ('power', tensor([-21.2231], device='cuda:0'))])
epoch£º922	 i:0 	 global-step:18440	 l-p:0.1743653416633606
epoch£º922	 i:1 	 global-step:18441	 l-p:0.11564312875270844
epoch£º922	 i:2 	 global-step:18442	 l-p:0.13607729971408844
epoch£º922	 i:3 	 global-step:18443	 l-p:0.12153547257184982
epoch£º922	 i:4 	 global-step:18444	 l-p:0.1162932962179184
epoch£º922	 i:5 	 global-step:18445	 l-p:0.10057968646287918
epoch£º922	 i:6 	 global-step:18446	 l-p:0.096433125436306
epoch£º922	 i:7 	 global-step:18447	 l-p:0.17362342774868011
epoch£º922	 i:8 	 global-step:18448	 l-p:0.20535743236541748
epoch£º922	 i:9 	 global-step:18449	 l-p:0.1012253388762474
====================================================================================================
====================================================================================================
====================================================================================================

epoch:923
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1580, 4.9309, 4.6431],
        [5.1580, 4.9236, 4.9004],
        [5.1580, 5.0079, 5.0599],
        [5.1580, 5.1555, 5.1579]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:923, step:0 
model_pd.l_p.mean(): 0.12279059737920761 
model_pd.l_d.mean(): -20.32309913635254 
model_pd.lagr.mean(): -20.200307846069336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4432], device='cuda:0')), ('power', tensor([-21.1622], device='cuda:0'))])
epoch£º923	 i:0 	 global-step:18460	 l-p:0.12279059737920761
epoch£º923	 i:1 	 global-step:18461	 l-p:0.18799346685409546
epoch£º923	 i:2 	 global-step:18462	 l-p:0.1337200254201889
epoch£º923	 i:3 	 global-step:18463	 l-p:0.11493684351444244
epoch£º923	 i:4 	 global-step:18464	 l-p:0.13024471700191498
epoch£º923	 i:5 	 global-step:18465	 l-p:0.12657015025615692
epoch£º923	 i:6 	 global-step:18466	 l-p:0.10760276764631271
epoch£º923	 i:7 	 global-step:18467	 l-p:0.16112159192562103
epoch£º923	 i:8 	 global-step:18468	 l-p:0.14338862895965576
epoch£º923	 i:9 	 global-step:18469	 l-p:0.10159923136234283
====================================================================================================
====================================================================================================
====================================================================================================

epoch:924
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1445, 4.9388, 4.9559],
        [5.1445, 4.9226, 4.6277],
        [5.1445, 5.0844, 5.1270],
        [5.1445, 4.9930, 5.0449]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:924, step:0 
model_pd.l_p.mean(): 0.11312832683324814 
model_pd.l_d.mean(): -20.094993591308594 
model_pd.lagr.mean(): -19.98186492919922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4959], device='cuda:0')), ('power', tensor([-20.9844], device='cuda:0'))])
epoch£º924	 i:0 	 global-step:18480	 l-p:0.11312832683324814
epoch£º924	 i:1 	 global-step:18481	 l-p:0.19400733709335327
epoch£º924	 i:2 	 global-step:18482	 l-p:0.14544114470481873
epoch£º924	 i:3 	 global-step:18483	 l-p:0.13590747117996216
epoch£º924	 i:4 	 global-step:18484	 l-p:0.12589359283447266
epoch£º924	 i:5 	 global-step:18485	 l-p:0.12600858509540558
epoch£º924	 i:6 	 global-step:18486	 l-p:0.23279441893100739
epoch£º924	 i:7 	 global-step:18487	 l-p:0.11267241835594177
epoch£º924	 i:8 	 global-step:18488	 l-p:0.09747474640607834
epoch£º924	 i:9 	 global-step:18489	 l-p:0.11687856167554855
====================================================================================================
====================================================================================================
====================================================================================================

epoch:925
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1130, 5.1129, 5.1130],
        [5.1130, 5.0729, 4.7645],
        [5.1130, 4.9416, 4.6206],
        [5.1130, 4.8780, 4.5883]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:925, step:0 
model_pd.l_p.mean(): 0.16338634490966797 
model_pd.l_d.mean(): -20.30066680908203 
model_pd.lagr.mean(): -20.137279510498047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4616], device='cuda:0')), ('power', tensor([-21.1584], device='cuda:0'))])
epoch£º925	 i:0 	 global-step:18500	 l-p:0.16338634490966797
epoch£º925	 i:1 	 global-step:18501	 l-p:0.10079643130302429
epoch£º925	 i:2 	 global-step:18502	 l-p:0.12685570120811462
epoch£º925	 i:3 	 global-step:18503	 l-p:0.23374846577644348
epoch£º925	 i:4 	 global-step:18504	 l-p:0.11625823378562927
epoch£º925	 i:5 	 global-step:18505	 l-p:0.19745619595050812
epoch£º925	 i:6 	 global-step:18506	 l-p:0.18363578617572784
epoch£º925	 i:7 	 global-step:18507	 l-p:0.1862427145242691
epoch£º925	 i:8 	 global-step:18508	 l-p:0.13258010149002075
epoch£º925	 i:9 	 global-step:18509	 l-p:-0.38326549530029297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:926
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0781, 5.0781, 5.0781],
        [5.0781, 4.8000, 4.6703],
        [5.0781, 4.7982, 4.6540],
        [5.0781, 4.9687, 4.6436]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:926, step:0 
model_pd.l_p.mean(): 0.13493619859218597 
model_pd.l_d.mean(): -19.792646408081055 
model_pd.lagr.mean(): -19.657711029052734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4701], device='cuda:0')), ('power', tensor([-20.6496], device='cuda:0'))])
epoch£º926	 i:0 	 global-step:18520	 l-p:0.13493619859218597
epoch£º926	 i:1 	 global-step:18521	 l-p:-0.3244640529155731
epoch£º926	 i:2 	 global-step:18522	 l-p:0.1205221563577652
epoch£º926	 i:3 	 global-step:18523	 l-p:0.13522391021251678
epoch£º926	 i:4 	 global-step:18524	 l-p:0.11316939443349838
epoch£º926	 i:5 	 global-step:18525	 l-p:0.3889453411102295
epoch£º926	 i:6 	 global-step:18526	 l-p:0.25589504837989807
epoch£º926	 i:7 	 global-step:18527	 l-p:0.1148483082652092
epoch£º926	 i:8 	 global-step:18528	 l-p:0.32643407583236694
epoch£º926	 i:9 	 global-step:18529	 l-p:0.1413409411907196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:927
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0680, 5.0674, 5.0680],
        [5.0680, 4.9916, 5.0414],
        [5.0680, 5.0639, 5.0678],
        [5.0680, 4.9792, 5.0329]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:927, step:0 
model_pd.l_p.mean(): 0.14252953231334686 
model_pd.l_d.mean(): -18.478271484375 
model_pd.lagr.mean(): -18.33574104309082 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5635], device='cuda:0')), ('power', tensor([-19.4074], device='cuda:0'))])
epoch£º927	 i:0 	 global-step:18540	 l-p:0.14252953231334686
epoch£º927	 i:1 	 global-step:18541	 l-p:0.27113428711891174
epoch£º927	 i:2 	 global-step:18542	 l-p:0.13199636340141296
epoch£º927	 i:3 	 global-step:18543	 l-p:0.1392764002084732
epoch£º927	 i:4 	 global-step:18544	 l-p:0.412702739238739
epoch£º927	 i:5 	 global-step:18545	 l-p:0.12227106839418411
epoch£º927	 i:6 	 global-step:18546	 l-p:0.13404889404773712
epoch£º927	 i:7 	 global-step:18547	 l-p:-0.02727511338889599
epoch£º927	 i:8 	 global-step:18548	 l-p:0.12722572684288025
epoch£º927	 i:9 	 global-step:18549	 l-p:0.20260681211948395
====================================================================================================
====================================================================================================
====================================================================================================

epoch:928
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0740, 5.0740, 5.0740],
        [5.0740, 4.9633, 5.0209],
        [5.0740, 5.0740, 5.0740],
        [5.0740, 5.0722, 5.0739]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:928, step:0 
model_pd.l_p.mean(): 0.11434558779001236 
model_pd.l_d.mean(): -20.272912979125977 
model_pd.lagr.mean(): -20.158567428588867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4597], device='cuda:0')), ('power', tensor([-21.1282], device='cuda:0'))])
epoch£º928	 i:0 	 global-step:18560	 l-p:0.11434558779001236
epoch£º928	 i:1 	 global-step:18561	 l-p:0.12854968011379242
epoch£º928	 i:2 	 global-step:18562	 l-p:0.16467319428920746
epoch£º928	 i:3 	 global-step:18563	 l-p:0.14572136104106903
epoch£º928	 i:4 	 global-step:18564	 l-p:0.2446756660938263
epoch£º928	 i:5 	 global-step:18565	 l-p:0.13404232263565063
epoch£º928	 i:6 	 global-step:18566	 l-p:0.29083478450775146
epoch£º928	 i:7 	 global-step:18567	 l-p:0.12246140837669373
epoch£º928	 i:8 	 global-step:18568	 l-p:0.19067855179309845
epoch£º928	 i:9 	 global-step:18569	 l-p:-0.5501620173454285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:929
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0832, 5.0832, 5.0832],
        [5.0832, 5.0337, 5.0710],
        [5.0832, 4.8018, 4.6291],
        [5.0832, 5.0832, 5.0832]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:929, step:0 
model_pd.l_p.mean(): 0.09845990687608719 
model_pd.l_d.mean(): -20.485692977905273 
model_pd.lagr.mean(): -20.38723373413086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4502], device='cuda:0')), ('power', tensor([-21.3351], device='cuda:0'))])
epoch£º929	 i:0 	 global-step:18580	 l-p:0.09845990687608719
epoch£º929	 i:1 	 global-step:18581	 l-p:-20.391136169433594
epoch£º929	 i:2 	 global-step:18582	 l-p:0.2286796122789383
epoch£º929	 i:3 	 global-step:18583	 l-p:0.13621993362903595
epoch£º929	 i:4 	 global-step:18584	 l-p:0.09877461940050125
epoch£º929	 i:5 	 global-step:18585	 l-p:0.13903814554214478
epoch£º929	 i:6 	 global-step:18586	 l-p:0.15907178819179535
epoch£º929	 i:7 	 global-step:18587	 l-p:0.13447555899620056
epoch£º929	 i:8 	 global-step:18588	 l-p:0.19056560099124908
epoch£º929	 i:9 	 global-step:18589	 l-p:0.13601751625537872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:930
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0994, 5.0911, 5.0988],
        [5.0994, 4.9448, 4.6194],
        [5.0994, 5.0792, 5.0968],
        [5.0994, 5.0982, 5.0994]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:930, step:0 
model_pd.l_p.mean(): 0.10688862204551697 
model_pd.l_d.mean(): -20.399667739868164 
model_pd.lagr.mean(): -20.29277992248535 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4640], device='cuda:0')), ('power', tensor([-21.2617], device='cuda:0'))])
epoch£º930	 i:0 	 global-step:18600	 l-p:0.10688862204551697
epoch£º930	 i:1 	 global-step:18601	 l-p:0.13924819231033325
epoch£º930	 i:2 	 global-step:18602	 l-p:0.17631246149539948
epoch£º930	 i:3 	 global-step:18603	 l-p:0.43514809012413025
epoch£º930	 i:4 	 global-step:18604	 l-p:0.21023677289485931
epoch£º930	 i:5 	 global-step:18605	 l-p:0.13710477948188782
epoch£º930	 i:6 	 global-step:18606	 l-p:0.1777273565530777
epoch£º930	 i:7 	 global-step:18607	 l-p:0.23655258119106293
epoch£º930	 i:8 	 global-step:18608	 l-p:0.09428516775369644
epoch£º930	 i:9 	 global-step:18609	 l-p:0.08359745144844055
====================================================================================================
====================================================================================================
====================================================================================================

epoch:931
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1563e-01, 2.1490e-01,
         1.0000e+00, 1.4632e-01, 1.0000e+00, 6.8086e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1143, 5.1121, 5.1142],
        [5.1143, 4.8377, 4.6980],
        [5.1143, 5.1143, 5.1143],
        [5.1143, 5.0027, 5.0599]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:931, step:0 
model_pd.l_p.mean(): 0.15832291543483734 
model_pd.l_d.mean(): -20.13131332397461 
model_pd.lagr.mean(): -19.972990036010742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4736], device='cuda:0')), ('power', tensor([-20.9983], device='cuda:0'))])
epoch£º931	 i:0 	 global-step:18620	 l-p:0.15832291543483734
epoch£º931	 i:1 	 global-step:18621	 l-p:0.13335779309272766
epoch£º931	 i:2 	 global-step:18622	 l-p:0.15456053614616394
epoch£º931	 i:3 	 global-step:18623	 l-p:0.18911994993686676
epoch£º931	 i:4 	 global-step:18624	 l-p:0.1200210377573967
epoch£º931	 i:5 	 global-step:18625	 l-p:0.13115835189819336
epoch£º931	 i:6 	 global-step:18626	 l-p:0.10764691978693008
epoch£º931	 i:7 	 global-step:18627	 l-p:0.14216256141662598
epoch£º931	 i:8 	 global-step:18628	 l-p:0.19415922462940216
epoch£º931	 i:9 	 global-step:18629	 l-p:0.11744056642055511
====================================================================================================
====================================================================================================
====================================================================================================

epoch:932
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1429, 4.9845, 4.6629],
        [5.1429, 5.0578, 5.1100],
        [5.1429, 4.9170, 4.9107],
        [5.1429, 5.1290, 5.1415]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:932, step:0 
model_pd.l_p.mean(): 0.18085117638111115 
model_pd.l_d.mean(): -18.89003562927246 
model_pd.lagr.mean(): -18.709184646606445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5669], device='cuda:0')), ('power', tensor([-19.8304], device='cuda:0'))])
epoch£º932	 i:0 	 global-step:18640	 l-p:0.18085117638111115
epoch£º932	 i:1 	 global-step:18641	 l-p:0.15678207576274872
epoch£º932	 i:2 	 global-step:18642	 l-p:0.12084213644266129
epoch£º932	 i:3 	 global-step:18643	 l-p:0.1928306221961975
epoch£º932	 i:4 	 global-step:18644	 l-p:0.08827799558639526
epoch£º932	 i:5 	 global-step:18645	 l-p:0.10334079712629318
epoch£º932	 i:6 	 global-step:18646	 l-p:0.12287836521863937
epoch£º932	 i:7 	 global-step:18647	 l-p:0.07933622598648071
epoch£º932	 i:8 	 global-step:18648	 l-p:0.1307026892900467
epoch£º932	 i:9 	 global-step:18649	 l-p:0.19162452220916748
====================================================================================================
====================================================================================================
====================================================================================================

epoch:933
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1524, 5.0087, 4.6864],
        [5.1524, 4.9028, 4.8528],
        [5.1524, 4.8786, 4.6924],
        [5.1524, 5.1524, 5.1524]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:933, step:0 
model_pd.l_p.mean(): 0.1269104778766632 
model_pd.l_d.mean(): -20.44313621520996 
model_pd.lagr.mean(): -20.316225051879883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4512], device='cuda:0')), ('power', tensor([-21.2927], device='cuda:0'))])
epoch£º933	 i:0 	 global-step:18660	 l-p:0.1269104778766632
epoch£º933	 i:1 	 global-step:18661	 l-p:0.08597562462091446
epoch£º933	 i:2 	 global-step:18662	 l-p:0.14998671412467957
epoch£º933	 i:3 	 global-step:18663	 l-p:0.12485314905643463
epoch£º933	 i:4 	 global-step:18664	 l-p:0.15032421052455902
epoch£º933	 i:5 	 global-step:18665	 l-p:0.12067792564630508
epoch£º933	 i:6 	 global-step:18666	 l-p:0.19149155914783478
epoch£º933	 i:7 	 global-step:18667	 l-p:0.17141273617744446
epoch£º933	 i:8 	 global-step:18668	 l-p:0.10355854779481888
epoch£º933	 i:9 	 global-step:18669	 l-p:0.15360122919082642
====================================================================================================
====================================================================================================
====================================================================================================

epoch:934
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1431, 5.1431, 5.1431],
        [5.1431, 5.1431, 5.1431],
        [5.1431, 4.9870, 5.0379],
        [5.1431, 5.0259, 5.0832]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:934, step:0 
model_pd.l_p.mean(): 0.18764476478099823 
model_pd.l_d.mean(): -19.893770217895508 
model_pd.lagr.mean(): -19.706125259399414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4907], device='cuda:0')), ('power', tensor([-20.7740], device='cuda:0'))])
epoch£º934	 i:0 	 global-step:18680	 l-p:0.18764476478099823
epoch£º934	 i:1 	 global-step:18681	 l-p:0.13110966980457306
epoch£º934	 i:2 	 global-step:18682	 l-p:0.09525304287672043
epoch£º934	 i:3 	 global-step:18683	 l-p:0.1203768327832222
epoch£º934	 i:4 	 global-step:18684	 l-p:0.1602398306131363
epoch£º934	 i:5 	 global-step:18685	 l-p:0.1614954024553299
epoch£º934	 i:6 	 global-step:18686	 l-p:0.14588205516338348
epoch£º934	 i:7 	 global-step:18687	 l-p:0.11816110461950302
epoch£º934	 i:8 	 global-step:18688	 l-p:0.13103893399238586
epoch£º934	 i:9 	 global-step:18689	 l-p:0.1348058134317398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:935
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1361, 4.9104, 4.6139],
        [5.1361, 5.1215, 5.1346],
        [5.1361, 5.1282, 5.1356],
        [5.1361, 4.9836, 5.0360]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:935, step:0 
model_pd.l_p.mean(): 0.17303742468357086 
model_pd.l_d.mean(): -20.32162857055664 
model_pd.lagr.mean(): -20.148591995239258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4732], device='cuda:0')), ('power', tensor([-21.1918], device='cuda:0'))])
epoch£º935	 i:0 	 global-step:18700	 l-p:0.17303742468357086
epoch£º935	 i:1 	 global-step:18701	 l-p:0.12662413716316223
epoch£º935	 i:2 	 global-step:18702	 l-p:0.1459108144044876
epoch£º935	 i:3 	 global-step:18703	 l-p:0.15373264253139496
epoch£º935	 i:4 	 global-step:18704	 l-p:0.11180070787668228
epoch£º935	 i:5 	 global-step:18705	 l-p:0.13017615675926208
epoch£º935	 i:6 	 global-step:18706	 l-p:0.1590384989976883
epoch£º935	 i:7 	 global-step:18707	 l-p:0.13683895766735077
epoch£º935	 i:8 	 global-step:18708	 l-p:0.17406436800956726
epoch£º935	 i:9 	 global-step:18709	 l-p:0.1310427188873291
====================================================================================================
====================================================================================================
====================================================================================================

epoch:936
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7711e-01, 7.1446e-01,
         1.0000e+00, 6.5686e-01, 1.0000e+00, 9.1938e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1337, 5.0222, 5.0794],
        [5.1337, 5.1003, 5.1276],
        [5.1337, 4.8644, 4.7608],
        [5.1337, 5.2681, 5.0328]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:936, step:0 
model_pd.l_p.mean(): 0.16427123546600342 
model_pd.l_d.mean(): -19.603778839111328 
model_pd.lagr.mean(): -19.43950843811035 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4871], device='cuda:0')), ('power', tensor([-20.4749], device='cuda:0'))])
epoch£º936	 i:0 	 global-step:18720	 l-p:0.16427123546600342
epoch£º936	 i:1 	 global-step:18721	 l-p:0.13394568860530853
epoch£º936	 i:2 	 global-step:18722	 l-p:0.1944168210029602
epoch£º936	 i:3 	 global-step:18723	 l-p:0.13678999245166779
epoch£º936	 i:4 	 global-step:18724	 l-p:0.13486045598983765
epoch£º936	 i:5 	 global-step:18725	 l-p:0.13378286361694336
epoch£º936	 i:6 	 global-step:18726	 l-p:0.11682096123695374
epoch£º936	 i:7 	 global-step:18727	 l-p:0.12197339534759521
epoch£º936	 i:8 	 global-step:18728	 l-p:0.12124179303646088
epoch£º936	 i:9 	 global-step:18729	 l-p:0.11360258609056473
====================================================================================================
====================================================================================================
====================================================================================================

epoch:937
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1617, 4.9079, 4.8475],
        [5.1617, 5.1794, 4.8903],
        [5.1617, 4.9222, 4.8928],
        [5.1617, 5.1613, 5.1617]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:937, step:0 
model_pd.l_p.mean(): 0.08326644450426102 
model_pd.l_d.mean(): -20.57221794128418 
model_pd.lagr.mean(): -20.488950729370117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4239], device='cuda:0')), ('power', tensor([-21.3959], device='cuda:0'))])
epoch£º937	 i:0 	 global-step:18740	 l-p:0.08326644450426102
epoch£º937	 i:1 	 global-step:18741	 l-p:0.11823488771915436
epoch£º937	 i:2 	 global-step:18742	 l-p:0.13491672277450562
epoch£º937	 i:3 	 global-step:18743	 l-p:0.1317884624004364
epoch£º937	 i:4 	 global-step:18744	 l-p:0.11642812192440033
epoch£º937	 i:5 	 global-step:18745	 l-p:0.12443404644727707
epoch£º937	 i:6 	 global-step:18746	 l-p:0.15114054083824158
epoch£º937	 i:7 	 global-step:18747	 l-p:0.20462673902511597
epoch£º937	 i:8 	 global-step:18748	 l-p:0.11972330510616302
epoch£º937	 i:9 	 global-step:18749	 l-p:0.15059517323970795
====================================================================================================
====================================================================================================
====================================================================================================

epoch:938
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1614, 5.1614, 5.1614],
        [5.1614, 5.1310, 5.1562],
        [5.1614, 4.8871, 4.7285],
        [5.1614, 5.0307, 5.0872]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:938, step:0 
model_pd.l_p.mean(): 0.12998847663402557 
model_pd.l_d.mean(): -19.27939224243164 
model_pd.lagr.mean(): -19.149404525756836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4788], device='cuda:0')), ('power', tensor([-20.1358], device='cuda:0'))])
epoch£º938	 i:0 	 global-step:18760	 l-p:0.12998847663402557
epoch£º938	 i:1 	 global-step:18761	 l-p:0.15886521339416504
epoch£º938	 i:2 	 global-step:18762	 l-p:0.16174206137657166
epoch£º938	 i:3 	 global-step:18763	 l-p:0.09387721866369247
epoch£º938	 i:4 	 global-step:18764	 l-p:0.15054596960544586
epoch£º938	 i:5 	 global-step:18765	 l-p:0.15874692797660828
epoch£º938	 i:6 	 global-step:18766	 l-p:0.11393430083990097
epoch£º938	 i:7 	 global-step:18767	 l-p:0.03701484203338623
epoch£º938	 i:8 	 global-step:18768	 l-p:0.11667867749929428
epoch£º938	 i:9 	 global-step:18769	 l-p:0.17207977175712585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:939
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4925,  0.3890,  1.0000,  0.3072,
          1.0000,  0.7897, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3101,  0.2099,  1.0000,  0.1421,
          1.0000,  0.6769, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6033,  0.5098,  1.0000,  0.4308,
          1.0000,  0.8450, 31.6228]], device='cuda:0')
 pt:tensor([[5.1741, 5.4307, 5.2628],
        [5.1741, 4.9705, 4.6646],
        [5.1741, 4.9030, 4.7702],
        [5.1741, 5.0832, 4.7637]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:939, step:0 
model_pd.l_p.mean(): 0.07389146834611893 
model_pd.l_d.mean(): -19.946645736694336 
model_pd.lagr.mean(): -19.87275505065918 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4834], device='cuda:0')), ('power', tensor([-20.8203], device='cuda:0'))])
epoch£º939	 i:0 	 global-step:18780	 l-p:0.07389146834611893
epoch£º939	 i:1 	 global-step:18781	 l-p:0.11019625514745712
epoch£º939	 i:2 	 global-step:18782	 l-p:0.12194058299064636
epoch£º939	 i:3 	 global-step:18783	 l-p:0.15735065937042236
epoch£º939	 i:4 	 global-step:18784	 l-p:0.07829396426677704
epoch£º939	 i:5 	 global-step:18785	 l-p:0.17764098942279816
epoch£º939	 i:6 	 global-step:18786	 l-p:0.11227992922067642
epoch£º939	 i:7 	 global-step:18787	 l-p:0.15771539509296417
epoch£º939	 i:8 	 global-step:18788	 l-p:0.16757339239120483
epoch£º939	 i:9 	 global-step:18789	 l-p:0.12634523212909698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:940
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9254e-01, 3.8898e-01,
         1.0000e+00, 3.0719e-01, 1.0000e+00, 7.8973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8254e-02, 3.9293e-02,
         1.0000e+00, 1.7494e-02, 1.0000e+00, 4.4522e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1680, 5.1669, 5.1680],
        [5.1680, 4.9631, 4.6568],
        [5.1680, 5.1534, 5.1665],
        [5.1680, 5.0920, 5.1413]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:940, step:0 
model_pd.l_p.mean(): 0.13205473124980927 
model_pd.l_d.mean(): -19.487524032592773 
model_pd.lagr.mean(): -19.35546875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5355], device='cuda:0')), ('power', tensor([-20.4065], device='cuda:0'))])
epoch£º940	 i:0 	 global-step:18800	 l-p:0.13205473124980927
epoch£º940	 i:1 	 global-step:18801	 l-p:0.08002234995365143
epoch£º940	 i:2 	 global-step:18802	 l-p:0.14593453705310822
epoch£º940	 i:3 	 global-step:18803	 l-p:0.11094646900892258
epoch£º940	 i:4 	 global-step:18804	 l-p:0.16654984652996063
epoch£º940	 i:5 	 global-step:18805	 l-p:0.10839647054672241
epoch£º940	 i:6 	 global-step:18806	 l-p:0.1600855588912964
epoch£º940	 i:7 	 global-step:18807	 l-p:0.1664268970489502
epoch£º940	 i:8 	 global-step:18808	 l-p:0.08214778453111649
epoch£º940	 i:9 	 global-step:18809	 l-p:0.16213497519493103
====================================================================================================
====================================================================================================
====================================================================================================

epoch:941
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1724, 5.0310, 4.7088],
        [5.1724, 5.1723, 5.1724],
        [5.1724, 4.9119, 4.8310],
        [5.1724, 4.9245, 4.6578]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:941, step:0 
model_pd.l_p.mean(): 0.14166691899299622 
model_pd.l_d.mean(): -19.388465881347656 
model_pd.lagr.mean(): -19.24679946899414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4950], device='cuda:0')), ('power', tensor([-20.2637], device='cuda:0'))])
epoch£º941	 i:0 	 global-step:18820	 l-p:0.14166691899299622
epoch£º941	 i:1 	 global-step:18821	 l-p:0.12940102815628052
epoch£º941	 i:2 	 global-step:18822	 l-p:0.14823119342327118
epoch£º941	 i:3 	 global-step:18823	 l-p:0.09327994287014008
epoch£º941	 i:4 	 global-step:18824	 l-p:0.14987966418266296
epoch£º941	 i:5 	 global-step:18825	 l-p:0.16224616765975952
epoch£º941	 i:6 	 global-step:18826	 l-p:0.10909813642501831
epoch£º941	 i:7 	 global-step:18827	 l-p:0.12770403921604156
epoch£º941	 i:8 	 global-step:18828	 l-p:0.08022591471672058
epoch£º941	 i:9 	 global-step:18829	 l-p:0.11595961451530457
====================================================================================================
====================================================================================================
====================================================================================================

epoch:942
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1755, 4.9797, 5.0063],
        [5.1755, 5.5820, 5.5090],
        [5.1755, 4.9550, 4.6591],
        [5.1755, 5.1702, 5.1752]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:942, step:0 
model_pd.l_p.mean(): 0.12095417082309723 
model_pd.l_d.mean(): -20.163179397583008 
model_pd.lagr.mean(): -20.042224884033203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4388], device='cuda:0')), ('power', tensor([-20.9947], device='cuda:0'))])
epoch£º942	 i:0 	 global-step:18840	 l-p:0.12095417082309723
epoch£º942	 i:1 	 global-step:18841	 l-p:0.15370133519172668
epoch£º942	 i:2 	 global-step:18842	 l-p:0.12251392751932144
epoch£º942	 i:3 	 global-step:18843	 l-p:0.17004118859767914
epoch£º942	 i:4 	 global-step:18844	 l-p:0.13077513873577118
epoch£º942	 i:5 	 global-step:18845	 l-p:0.11538520455360413
epoch£º942	 i:6 	 global-step:18846	 l-p:0.12518715858459473
epoch£º942	 i:7 	 global-step:18847	 l-p:0.1237085685133934
epoch£º942	 i:8 	 global-step:18848	 l-p:0.17733991146087646
epoch£º942	 i:9 	 global-step:18849	 l-p:0.0556098073720932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:943
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1750, 5.1750, 5.1750],
        [5.1750, 4.9142, 4.8330],
        [5.1750, 5.1749, 5.1750],
        [5.1750, 5.1749, 5.1750]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:943, step:0 
model_pd.l_p.mean(): 0.12103162705898285 
model_pd.l_d.mean(): -20.220285415649414 
model_pd.lagr.mean(): -20.099254608154297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4169], device='cuda:0')), ('power', tensor([-21.0301], device='cuda:0'))])
epoch£º943	 i:0 	 global-step:18860	 l-p:0.12103162705898285
epoch£º943	 i:1 	 global-step:18861	 l-p:0.13677072525024414
epoch£º943	 i:2 	 global-step:18862	 l-p:0.1334131360054016
epoch£º943	 i:3 	 global-step:18863	 l-p:0.0936204195022583
epoch£º943	 i:4 	 global-step:18864	 l-p:0.14154891669750214
epoch£º943	 i:5 	 global-step:18865	 l-p:0.08634714037179947
epoch£º943	 i:6 	 global-step:18866	 l-p:0.1547251045703888
epoch£º943	 i:7 	 global-step:18867	 l-p:0.12979696691036224
epoch£º943	 i:8 	 global-step:18868	 l-p:0.11657950282096863
epoch£º943	 i:9 	 global-step:18869	 l-p:0.16079159080982208
====================================================================================================
====================================================================================================
====================================================================================================

epoch:944
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4147,  0.3093,  1.0000,  0.2306,
          1.0000,  0.7457, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.6128,  0.5205,  1.0000,  0.4421,
          1.0000,  0.8494, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2428,  0.1514,  1.0000,  0.0945,
          1.0000,  0.6238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1755,  0.0983,  1.0000,  0.0550,
          1.0000,  0.5599, 31.6228]], device='cuda:0')
 pt:tensor([[5.1853, 4.9316, 4.6768],
        [5.1853, 5.1070, 4.7894],
        [5.1853, 4.9415, 4.9019],
        [5.1853, 5.0001, 5.0347]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:944, step:0 
model_pd.l_p.mean(): 0.1134808138012886 
model_pd.l_d.mean(): -20.814577102661133 
model_pd.lagr.mean(): -20.701095581054688 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3748], device='cuda:0')), ('power', tensor([-21.5920], device='cuda:0'))])
epoch£º944	 i:0 	 global-step:18880	 l-p:0.1134808138012886
epoch£º944	 i:1 	 global-step:18881	 l-p:0.10409145057201385
epoch£º944	 i:2 	 global-step:18882	 l-p:0.11940310150384903
epoch£º944	 i:3 	 global-step:18883	 l-p:0.14162014424800873
epoch£º944	 i:4 	 global-step:18884	 l-p:0.11513137817382812
epoch£º944	 i:5 	 global-step:18885	 l-p:0.11101249605417252
epoch£º944	 i:6 	 global-step:18886	 l-p:0.14282338321208954
epoch£º944	 i:7 	 global-step:18887	 l-p:0.1581101417541504
epoch£º944	 i:8 	 global-step:18888	 l-p:0.11719401925802231
epoch£º944	 i:9 	 global-step:18889	 l-p:0.1300954669713974
====================================================================================================
====================================================================================================
====================================================================================================

epoch:945
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4650e-03, 1.6638e-04,
         1.0000e+00, 1.8897e-05, 1.0000e+00, 1.1357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1818, 5.0469, 4.7244],
        [5.1818, 5.1776, 5.1816],
        [5.1818, 5.0858, 5.1406],
        [5.1818, 5.1818, 5.1818]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:945, step:0 
model_pd.l_p.mean(): 0.09809495508670807 
model_pd.l_d.mean(): -19.4797420501709 
model_pd.lagr.mean(): -19.38164710998535 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4492], device='cuda:0')), ('power', tensor([-20.3093], device='cuda:0'))])
epoch£º945	 i:0 	 global-step:18900	 l-p:0.09809495508670807
epoch£º945	 i:1 	 global-step:18901	 l-p:0.12337388098239899
epoch£º945	 i:2 	 global-step:18902	 l-p:0.12578369677066803
epoch£º945	 i:3 	 global-step:18903	 l-p:0.06286565959453583
epoch£º945	 i:4 	 global-step:18904	 l-p:0.11269059032201767
epoch£º945	 i:5 	 global-step:18905	 l-p:0.15032950043678284
epoch£º945	 i:6 	 global-step:18906	 l-p:0.1028289943933487
epoch£º945	 i:7 	 global-step:18907	 l-p:0.1382257491350174
epoch£º945	 i:8 	 global-step:18908	 l-p:0.1540655791759491
epoch£º945	 i:9 	 global-step:18909	 l-p:0.26646551489830017
====================================================================================================
====================================================================================================
====================================================================================================

epoch:946
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1526, 5.0110, 4.6861],
        [5.1526, 5.1368, 4.8345],
        [5.1526, 5.1525, 5.1526],
        [5.1526, 5.5953, 5.5469]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:946, step:0 
model_pd.l_p.mean(): 0.20235322415828705 
model_pd.l_d.mean(): -19.53331184387207 
model_pd.lagr.mean(): -19.33095932006836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5290], device='cuda:0')), ('power', tensor([-20.4465], device='cuda:0'))])
epoch£º946	 i:0 	 global-step:18920	 l-p:0.20235322415828705
epoch£º946	 i:1 	 global-step:18921	 l-p:0.12214794754981995
epoch£º946	 i:2 	 global-step:18922	 l-p:0.12283626943826675
epoch£º946	 i:3 	 global-step:18923	 l-p:0.0980953648686409
epoch£º946	 i:4 	 global-step:18924	 l-p:0.12759560346603394
epoch£º946	 i:5 	 global-step:18925	 l-p:0.12921270728111267
epoch£º946	 i:6 	 global-step:18926	 l-p:0.18991515040397644
epoch£º946	 i:7 	 global-step:18927	 l-p:0.0958884060382843
epoch£º946	 i:8 	 global-step:18928	 l-p:0.13912516832351685
epoch£º946	 i:9 	 global-step:18929	 l-p:0.11960462480783463
====================================================================================================
====================================================================================================
====================================================================================================

epoch:947
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1598, 5.1462, 5.1585],
        [5.1598, 5.1577, 5.1598],
        [5.1598, 5.1598, 5.1598],
        [5.1598, 5.1598, 5.1598]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:947, step:0 
model_pd.l_p.mean(): 0.1889776587486267 
model_pd.l_d.mean(): -20.67317008972168 
model_pd.lagr.mean(): -20.48419189453125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4154], device='cuda:0')), ('power', tensor([-21.4900], device='cuda:0'))])
epoch£º947	 i:0 	 global-step:18940	 l-p:0.1889776587486267
epoch£º947	 i:1 	 global-step:18941	 l-p:0.1522175520658493
epoch£º947	 i:2 	 global-step:18942	 l-p:0.12670572102069855
epoch£º947	 i:3 	 global-step:18943	 l-p:0.11978201568126678
epoch£º947	 i:4 	 global-step:18944	 l-p:0.09560619294643402
epoch£º947	 i:5 	 global-step:18945	 l-p:0.14612047374248505
epoch£º947	 i:6 	 global-step:18946	 l-p:0.1373143494129181
epoch£º947	 i:7 	 global-step:18947	 l-p:0.0933338925242424
epoch£º947	 i:8 	 global-step:18948	 l-p:0.13978800177574158
epoch£º947	 i:9 	 global-step:18949	 l-p:0.16266979277133942
====================================================================================================
====================================================================================================
====================================================================================================

epoch:948
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1507, 5.1772, 4.8903],
        [5.1507, 5.0759, 5.1250],
        [5.1507, 5.1506, 5.1507],
        [5.1507, 5.0153, 5.0719]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:948, step:0 
model_pd.l_p.mean(): 0.11605655401945114 
model_pd.l_d.mean(): -19.19068145751953 
model_pd.lagr.mean(): -19.07462501525879 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5776], device='cuda:0')), ('power', tensor([-20.1478], device='cuda:0'))])
epoch£º948	 i:0 	 global-step:18960	 l-p:0.11605655401945114
epoch£º948	 i:1 	 global-step:18961	 l-p:0.11170644313097
epoch£º948	 i:2 	 global-step:18962	 l-p:0.1172541007399559
epoch£º948	 i:3 	 global-step:18963	 l-p:0.14517690241336823
epoch£º948	 i:4 	 global-step:18964	 l-p:0.14452551305294037
epoch£º948	 i:5 	 global-step:18965	 l-p:0.16716304421424866
epoch£º948	 i:6 	 global-step:18966	 l-p:0.15805073082447052
epoch£º948	 i:7 	 global-step:18967	 l-p:0.06698057800531387
epoch£º948	 i:8 	 global-step:18968	 l-p:0.14114819467067719
epoch£º948	 i:9 	 global-step:18969	 l-p:0.19221532344818115
====================================================================================================
====================================================================================================
====================================================================================================

epoch:949
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1473, 5.1462, 5.1473],
        [5.1473, 5.0284, 5.0861],
        [5.1473, 5.0703, 5.1202],
        [5.1473, 5.0927, 5.1328]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:949, step:0 
model_pd.l_p.mean(): 0.12551051378250122 
model_pd.l_d.mean(): -20.618915557861328 
model_pd.lagr.mean(): -20.493404388427734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4050], device='cuda:0')), ('power', tensor([-21.4239], device='cuda:0'))])
epoch£º949	 i:0 	 global-step:18980	 l-p:0.12551051378250122
epoch£º949	 i:1 	 global-step:18981	 l-p:0.10034211724996567
epoch£º949	 i:2 	 global-step:18982	 l-p:0.15307559072971344
epoch£º949	 i:3 	 global-step:18983	 l-p:0.1484490931034088
epoch£º949	 i:4 	 global-step:18984	 l-p:0.2010132223367691
epoch£º949	 i:5 	 global-step:18985	 l-p:0.17848753929138184
epoch£º949	 i:6 	 global-step:18986	 l-p:0.12761828303337097
epoch£º949	 i:7 	 global-step:18987	 l-p:0.08412235230207443
epoch£º949	 i:8 	 global-step:18988	 l-p:0.15050175786018372
epoch£º949	 i:9 	 global-step:18989	 l-p:0.12340280413627625
====================================================================================================
====================================================================================================
====================================================================================================

epoch:950
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3675e-02, 6.7979e-03,
         1.0000e+00, 1.9520e-03, 1.0000e+00, 2.8714e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1547, 5.1467, 5.1541],
        [5.1547, 4.8770, 4.7179],
        [5.1547, 4.8869, 4.6497],
        [5.1547, 5.1222, 5.1488]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:950, step:0 
model_pd.l_p.mean(): 0.11618362367153168 
model_pd.l_d.mean(): -19.57379150390625 
model_pd.lagr.mean(): -19.45760726928711 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4999], device='cuda:0')), ('power', tensor([-20.4576], device='cuda:0'))])
epoch£º950	 i:0 	 global-step:19000	 l-p:0.11618362367153168
epoch£º950	 i:1 	 global-step:19001	 l-p:0.1950795203447342
epoch£º950	 i:2 	 global-step:19002	 l-p:0.15508480370044708
epoch£º950	 i:3 	 global-step:19003	 l-p:0.13736647367477417
epoch£º950	 i:4 	 global-step:19004	 l-p:0.1128334030508995
epoch£º950	 i:5 	 global-step:19005	 l-p:0.14952482283115387
epoch£º950	 i:6 	 global-step:19006	 l-p:0.16980835795402527
epoch£º950	 i:7 	 global-step:19007	 l-p:0.11340425908565521
epoch£º950	 i:8 	 global-step:19008	 l-p:0.13671936094760895
epoch£º950	 i:9 	 global-step:19009	 l-p:0.09234069287776947
====================================================================================================
====================================================================================================
====================================================================================================

epoch:951
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1408, 4.9644, 5.0074],
        [5.1408, 5.1408, 5.1408],
        [5.1408, 5.1408, 5.1408],
        [5.1408, 4.8615, 4.7024]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:951, step:0 
model_pd.l_p.mean(): 0.21938186883926392 
model_pd.l_d.mean(): -18.911602020263672 
model_pd.lagr.mean(): -18.69222068786621 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5331], device='cuda:0')), ('power', tensor([-19.8173], device='cuda:0'))])
epoch£º951	 i:0 	 global-step:19020	 l-p:0.21938186883926392
epoch£º951	 i:1 	 global-step:19021	 l-p:0.1389172375202179
epoch£º951	 i:2 	 global-step:19022	 l-p:0.13244356215000153
epoch£º951	 i:3 	 global-step:19023	 l-p:0.08952341973781586
epoch£º951	 i:4 	 global-step:19024	 l-p:0.12841443717479706
epoch£º951	 i:5 	 global-step:19025	 l-p:0.15985167026519775
epoch£º951	 i:6 	 global-step:19026	 l-p:0.16366815567016602
epoch£º951	 i:7 	 global-step:19027	 l-p:0.1388443559408188
epoch£º951	 i:8 	 global-step:19028	 l-p:0.11253800988197327
epoch£º951	 i:9 	 global-step:19029	 l-p:0.12643194198608398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:952
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1386, 5.1325, 5.1382],
        [5.1386, 4.9861, 4.6594],
        [5.1386, 5.0968, 5.1295],
        [5.1386, 5.1152, 5.1353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:952, step:0 
model_pd.l_p.mean(): 0.11888034641742706 
model_pd.l_d.mean(): -20.1868953704834 
model_pd.lagr.mean(): -20.06801414489746 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4623], device='cuda:0')), ('power', tensor([-21.0432], device='cuda:0'))])
epoch£º952	 i:0 	 global-step:19040	 l-p:0.11888034641742706
epoch£º952	 i:1 	 global-step:19041	 l-p:0.15246835350990295
epoch£º952	 i:2 	 global-step:19042	 l-p:0.11465741693973541
epoch£º952	 i:3 	 global-step:19043	 l-p:0.1616877019405365
epoch£º952	 i:4 	 global-step:19044	 l-p:0.1887066811323166
epoch£º952	 i:5 	 global-step:19045	 l-p:0.13946104049682617
epoch£º952	 i:6 	 global-step:19046	 l-p:0.11784925311803818
epoch£º952	 i:7 	 global-step:19047	 l-p:0.12720511853694916
epoch£º952	 i:8 	 global-step:19048	 l-p:0.13289017975330353
epoch£º952	 i:9 	 global-step:19049	 l-p:0.1922074854373932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:953
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1232, 4.8418, 4.6508],
        [5.1232, 5.1230, 5.1232],
        [5.1232, 5.1231, 5.1232],
        [5.1232, 5.1193, 5.1230]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:953, step:0 
model_pd.l_p.mean(): 0.10180841386318207 
model_pd.l_d.mean(): -20.2884578704834 
model_pd.lagr.mean(): -20.186649322509766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4792], device='cuda:0')), ('power', tensor([-21.1641], device='cuda:0'))])
epoch£º953	 i:0 	 global-step:19060	 l-p:0.10180841386318207
epoch£º953	 i:1 	 global-step:19061	 l-p:0.2302509993314743
epoch£º953	 i:2 	 global-step:19062	 l-p:0.17228813469409943
epoch£º953	 i:3 	 global-step:19063	 l-p:0.1805228441953659
epoch£º953	 i:4 	 global-step:19064	 l-p:0.15881074965000153
epoch£º953	 i:5 	 global-step:19065	 l-p:0.13389934599399567
epoch£º953	 i:6 	 global-step:19066	 l-p:0.09884921461343765
epoch£º953	 i:7 	 global-step:19067	 l-p:0.10927426815032959
epoch£º953	 i:8 	 global-step:19068	 l-p:0.21193844079971313
epoch£º953	 i:9 	 global-step:19069	 l-p:0.16767863929271698
====================================================================================================
====================================================================================================
====================================================================================================

epoch:954
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1073, 5.0391, 5.0857],
        [5.1073, 5.4765, 5.3793],
        [5.1073, 5.1043, 5.1072],
        [5.1073, 4.9175, 4.9540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:954, step:0 
model_pd.l_p.mean(): 0.14661258459091187 
model_pd.l_d.mean(): -19.683712005615234 
model_pd.lagr.mean(): -19.537099838256836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5625], device='cuda:0')), ('power', tensor([-20.6344], device='cuda:0'))])
epoch£º954	 i:0 	 global-step:19080	 l-p:0.14661258459091187
epoch£º954	 i:1 	 global-step:19081	 l-p:0.11058133840560913
epoch£º954	 i:2 	 global-step:19082	 l-p:0.31327250599861145
epoch£º954	 i:3 	 global-step:19083	 l-p:0.21483749151229858
epoch£º954	 i:4 	 global-step:19084	 l-p:0.09799553453922272
epoch£º954	 i:5 	 global-step:19085	 l-p:0.12334104627370834
epoch£º954	 i:6 	 global-step:19086	 l-p:0.25043985247612
epoch£º954	 i:7 	 global-step:19087	 l-p:0.10567652434110641
epoch£º954	 i:8 	 global-step:19088	 l-p:0.162599116563797
epoch£º954	 i:9 	 global-step:19089	 l-p:0.12967267632484436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:955
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1066, 5.1066, 5.1066],
        [5.1066, 5.4753, 5.3778],
        [5.1066, 5.0274, 5.0783],
        [5.1066, 5.0647, 5.0975]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:955, step:0 
model_pd.l_p.mean(): 0.09638651460409164 
model_pd.l_d.mean(): -20.036924362182617 
model_pd.lagr.mean(): -19.94053840637207 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5163], device='cuda:0')), ('power', tensor([-20.9463], device='cuda:0'))])
epoch£º955	 i:0 	 global-step:19100	 l-p:0.09638651460409164
epoch£º955	 i:1 	 global-step:19101	 l-p:0.11713612079620361
epoch£º955	 i:2 	 global-step:19102	 l-p:0.09444928914308548
epoch£º955	 i:3 	 global-step:19103	 l-p:-4.607685565948486
epoch£º955	 i:4 	 global-step:19104	 l-p:0.20416505634784698
epoch£º955	 i:5 	 global-step:19105	 l-p:0.13268570601940155
epoch£º955	 i:6 	 global-step:19106	 l-p:0.1863754838705063
epoch£º955	 i:7 	 global-step:19107	 l-p:0.1992160826921463
epoch£º955	 i:8 	 global-step:19108	 l-p:0.15161214768886566
epoch£º955	 i:9 	 global-step:19109	 l-p:0.5098337531089783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:956
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0758, 5.0380, 4.7259],
        [5.0758, 5.0752, 5.0758],
        [5.0758, 5.0747, 5.0758],
        [5.0758, 4.7887, 4.6296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:956, step:0 
model_pd.l_p.mean(): 0.1486215591430664 
model_pd.l_d.mean(): -20.830591201782227 
model_pd.lagr.mean(): -20.681968688964844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4180], device='cuda:0')), ('power', tensor([-21.6530], device='cuda:0'))])
epoch£º956	 i:0 	 global-step:19120	 l-p:0.1486215591430664
epoch£º956	 i:1 	 global-step:19121	 l-p:0.16049663722515106
epoch£º956	 i:2 	 global-step:19122	 l-p:0.12307870388031006
epoch£º956	 i:3 	 global-step:19123	 l-p:0.1832152158021927
epoch£º956	 i:4 	 global-step:19124	 l-p:0.12590743601322174
epoch£º956	 i:5 	 global-step:19125	 l-p:0.16220936179161072
epoch£º956	 i:6 	 global-step:19126	 l-p:0.5227904915809631
epoch£º956	 i:7 	 global-step:19127	 l-p:0.12671121954917908
epoch£º956	 i:8 	 global-step:19128	 l-p:-0.09599468111991882
epoch£º956	 i:9 	 global-step:19129	 l-p:0.2235323041677475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:957
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.0893, 5.0889, 5.0893],
        [5.0893, 4.9104, 4.9545],
        [5.0893, 5.0674, 5.0864],
        [5.0893, 5.0657, 5.0860]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:957, step:0 
model_pd.l_p.mean(): 0.16309349238872528 
model_pd.l_d.mean(): -20.563461303710938 
model_pd.lagr.mean(): -20.400367736816406 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4546], device='cuda:0')), ('power', tensor([-21.4188], device='cuda:0'))])
epoch£º957	 i:0 	 global-step:19140	 l-p:0.16309349238872528
epoch£º957	 i:1 	 global-step:19141	 l-p:0.13496583700180054
epoch£º957	 i:2 	 global-step:19142	 l-p:0.2055368423461914
epoch£º957	 i:3 	 global-step:19143	 l-p:0.11161771416664124
epoch£º957	 i:4 	 global-step:19144	 l-p:0.09619765728712082
epoch£º957	 i:5 	 global-step:19145	 l-p:0.1193627342581749
epoch£º957	 i:6 	 global-step:19146	 l-p:0.15331625938415527
epoch£º957	 i:7 	 global-step:19147	 l-p:0.20890681445598602
epoch£º957	 i:8 	 global-step:19148	 l-p:0.29587507247924805
epoch£º957	 i:9 	 global-step:19149	 l-p:0.19417983293533325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:958
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1155, 5.0920, 5.1121],
        [5.1155, 5.0442, 5.0921],
        [5.1155, 5.0363, 5.0872],
        [5.1155, 5.0084, 4.6808]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:958, step:0 
model_pd.l_p.mean(): 0.13879220187664032 
model_pd.l_d.mean(): -20.679927825927734 
model_pd.lagr.mean(): -20.541135787963867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4166], device='cuda:0')), ('power', tensor([-21.4981], device='cuda:0'))])
epoch£º958	 i:0 	 global-step:19160	 l-p:0.13879220187664032
epoch£º958	 i:1 	 global-step:19161	 l-p:0.23784011602401733
epoch£º958	 i:2 	 global-step:19162	 l-p:0.12277481704950333
epoch£º958	 i:3 	 global-step:19163	 l-p:0.17087163031101227
epoch£º958	 i:4 	 global-step:19164	 l-p:0.15798401832580566
epoch£º958	 i:5 	 global-step:19165	 l-p:0.10197334736585617
epoch£º958	 i:6 	 global-step:19166	 l-p:0.10899156332015991
epoch£º958	 i:7 	 global-step:19167	 l-p:0.16476207971572876
epoch£º958	 i:8 	 global-step:19168	 l-p:0.1271948665380478
epoch£º958	 i:9 	 global-step:19169	 l-p:0.13832667469978333
====================================================================================================
====================================================================================================
====================================================================================================

epoch:959
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1728, 4.9924, 4.6725],
        [5.1728, 5.1728, 5.1728],
        [5.1728, 4.9356, 4.6480],
        [5.1728, 5.1699, 5.1727]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:959, step:0 
model_pd.l_p.mean(): 0.0554828904569149 
model_pd.l_d.mean(): -20.40958023071289 
model_pd.lagr.mean(): -20.354097366333008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4370], device='cuda:0')), ('power', tensor([-21.2439], device='cuda:0'))])
epoch£º959	 i:0 	 global-step:19180	 l-p:0.0554828904569149
epoch£º959	 i:1 	 global-step:19181	 l-p:0.15038752555847168
epoch£º959	 i:2 	 global-step:19182	 l-p:0.16569827497005463
epoch£º959	 i:3 	 global-step:19183	 l-p:0.13681089878082275
epoch£º959	 i:4 	 global-step:19184	 l-p:0.12204968184232712
epoch£º959	 i:5 	 global-step:19185	 l-p:0.12580513954162598
epoch£º959	 i:6 	 global-step:19186	 l-p:0.13835187256336212
epoch£º959	 i:7 	 global-step:19187	 l-p:0.1387302279472351
epoch£º959	 i:8 	 global-step:19188	 l-p:0.11566172540187836
epoch£º959	 i:9 	 global-step:19189	 l-p:0.09958614408969879
====================================================================================================
====================================================================================================
====================================================================================================

epoch:960
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2021, 5.2020, 5.2021],
        [5.2021, 5.1985, 5.2019],
        [5.2021, 5.5047, 5.3623],
        [5.2021, 5.4933, 5.3439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:960, step:0 
model_pd.l_p.mean(): 0.10638550668954849 
model_pd.l_d.mean(): -19.423585891723633 
model_pd.lagr.mean(): -19.31719970703125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5011], device='cuda:0')), ('power', tensor([-20.3057], device='cuda:0'))])
epoch£º960	 i:0 	 global-step:19200	 l-p:0.10638550668954849
epoch£º960	 i:1 	 global-step:19201	 l-p:0.1640060842037201
epoch£º960	 i:2 	 global-step:19202	 l-p:0.1295836865901947
epoch£º960	 i:3 	 global-step:19203	 l-p:0.11958865821361542
epoch£º960	 i:4 	 global-step:19204	 l-p:0.13653917610645294
epoch£º960	 i:5 	 global-step:19205	 l-p:0.10617076605558395
epoch£º960	 i:6 	 global-step:19206	 l-p:0.11735211312770844
epoch£º960	 i:7 	 global-step:19207	 l-p:0.10495755821466446
epoch£º960	 i:8 	 global-step:19208	 l-p:0.12408805638551712
epoch£º960	 i:9 	 global-step:19209	 l-p:0.11646430939435959
====================================================================================================
====================================================================================================
====================================================================================================

epoch:961
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1840, 5.0389, 4.7139],
        [5.1840, 5.1103, 5.1589],
        [5.1840, 5.0177, 4.6953],
        [5.1840, 5.1791, 5.1838]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:961, step:0 
model_pd.l_p.mean(): 0.1274106204509735 
model_pd.l_d.mean(): -19.421955108642578 
model_pd.lagr.mean(): -19.294544219970703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5745], device='cuda:0')), ('power', tensor([-20.3802], device='cuda:0'))])
epoch£º961	 i:0 	 global-step:19220	 l-p:0.1274106204509735
epoch£º961	 i:1 	 global-step:19221	 l-p:0.12144831568002701
epoch£º961	 i:2 	 global-step:19222	 l-p:0.16759063303470612
epoch£º961	 i:3 	 global-step:19223	 l-p:0.13275279104709625
epoch£º961	 i:4 	 global-step:19224	 l-p:0.16603614389896393
epoch£º961	 i:5 	 global-step:19225	 l-p:0.12161698192358017
epoch£º961	 i:6 	 global-step:19226	 l-p:0.10510186851024628
epoch£º961	 i:7 	 global-step:19227	 l-p:0.13522744178771973
epoch£º961	 i:8 	 global-step:19228	 l-p:0.11888531595468521
epoch£º961	 i:9 	 global-step:19229	 l-p:0.12038005888462067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:962
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1614, 5.1126, 5.1495],
        [5.1614, 5.1614, 5.1614],
        [5.1614, 5.1600, 5.1613],
        [5.1614, 5.0604, 5.1166]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:962, step:0 
model_pd.l_p.mean(): 0.11263585090637207 
model_pd.l_d.mean(): -20.670997619628906 
model_pd.lagr.mean(): -20.558361053466797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4004], device='cuda:0')), ('power', tensor([-21.4722], device='cuda:0'))])
epoch£º962	 i:0 	 global-step:19240	 l-p:0.11263585090637207
epoch£º962	 i:1 	 global-step:19241	 l-p:0.09541454166173935
epoch£º962	 i:2 	 global-step:19242	 l-p:0.1278076171875
epoch£º962	 i:3 	 global-step:19243	 l-p:0.14431414008140564
epoch£º962	 i:4 	 global-step:19244	 l-p:0.10629920661449432
epoch£º962	 i:5 	 global-step:19245	 l-p:0.22795864939689636
epoch£º962	 i:6 	 global-step:19246	 l-p:0.10905240476131439
epoch£º962	 i:7 	 global-step:19247	 l-p:0.25983962416648865
epoch£º962	 i:8 	 global-step:19248	 l-p:0.1546899527311325
epoch£º962	 i:9 	 global-step:19249	 l-p:0.16111095249652863
====================================================================================================
====================================================================================================
====================================================================================================

epoch:963
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1173, 5.1148, 5.1172],
        [5.1173, 5.1157, 5.1172],
        [5.1173, 5.0617, 5.1024],
        [5.1173, 5.0733, 5.1074]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:963, step:0 
model_pd.l_p.mean(): 0.1207854375243187 
model_pd.l_d.mean(): -19.910091400146484 
model_pd.lagr.mean(): -19.789306640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4942], device='cuda:0')), ('power', tensor([-20.7943], device='cuda:0'))])
epoch£º963	 i:0 	 global-step:19260	 l-p:0.1207854375243187
epoch£º963	 i:1 	 global-step:19261	 l-p:0.13856762647628784
epoch£º963	 i:2 	 global-step:19262	 l-p:0.12966476380825043
epoch£º963	 i:3 	 global-step:19263	 l-p:0.08501759171485901
epoch£º963	 i:4 	 global-step:19264	 l-p:0.17937293648719788
epoch£º963	 i:5 	 global-step:19265	 l-p:0.23276056349277496
epoch£º963	 i:6 	 global-step:19266	 l-p:0.45680728554725647
epoch£º963	 i:7 	 global-step:19267	 l-p:0.15442578494548798
epoch£º963	 i:8 	 global-step:19268	 l-p:0.21858389675617218
epoch£º963	 i:9 	 global-step:19269	 l-p:0.11705636978149414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:964
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1046, 5.1030, 5.1046],
        [5.1046, 5.1043, 5.1046],
        [5.1046, 5.5310, 5.4710],
        [5.1046, 4.8895, 4.5728]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:964, step:0 
model_pd.l_p.mean(): 0.19482918083667755 
model_pd.l_d.mean(): -18.460607528686523 
model_pd.lagr.mean(): -18.265777587890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5964], device='cuda:0')), ('power', tensor([-19.4235], device='cuda:0'))])
epoch£º964	 i:0 	 global-step:19280	 l-p:0.19482918083667755
epoch£º964	 i:1 	 global-step:19281	 l-p:0.18622708320617676
epoch£º964	 i:2 	 global-step:19282	 l-p:0.15399520099163055
epoch£º964	 i:3 	 global-step:19283	 l-p:0.21439886093139648
epoch£º964	 i:4 	 global-step:19284	 l-p:0.1442342847585678
epoch£º964	 i:5 	 global-step:19285	 l-p:0.29618293046951294
epoch£º964	 i:6 	 global-step:19286	 l-p:0.13283947110176086
epoch£º964	 i:7 	 global-step:19287	 l-p:0.1486721634864807
epoch£º964	 i:8 	 global-step:19288	 l-p:0.10400319844484329
epoch£º964	 i:9 	 global-step:19289	 l-p:0.06821095943450928
====================================================================================================
====================================================================================================
====================================================================================================

epoch:965
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4687,  0.3641,  1.0000,  0.2828,
          1.0000,  0.7768, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2504,  0.1578,  1.0000,  0.0995,
          1.0000,  0.6303, 31.6228]], device='cuda:0')
 pt:tensor([[5.1187, 4.8808, 4.5813],
        [5.1187, 4.8453, 4.7502],
        [5.1187, 4.8827, 4.5815],
        [5.1187, 4.8620, 4.8132]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:965, step:0 
model_pd.l_p.mean(): 0.17652395367622375 
model_pd.l_d.mean(): -20.363399505615234 
model_pd.lagr.mean(): -20.18687629699707 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4514], device='cuda:0')), ('power', tensor([-21.2117], device='cuda:0'))])
epoch£º965	 i:0 	 global-step:19300	 l-p:0.17652395367622375
epoch£º965	 i:1 	 global-step:19301	 l-p:0.13706430792808533
epoch£º965	 i:2 	 global-step:19302	 l-p:0.13845454156398773
epoch£º965	 i:3 	 global-step:19303	 l-p:0.0944293737411499
epoch£º965	 i:4 	 global-step:19304	 l-p:0.13053864240646362
epoch£º965	 i:5 	 global-step:19305	 l-p:0.2206847071647644
epoch£º965	 i:6 	 global-step:19306	 l-p:0.2131458818912506
epoch£º965	 i:7 	 global-step:19307	 l-p:0.16521409153938293
epoch£º965	 i:8 	 global-step:19308	 l-p:0.12981952726840973
epoch£º965	 i:9 	 global-step:19309	 l-p:0.07671651989221573
====================================================================================================
====================================================================================================
====================================================================================================

epoch:966
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1432, 5.2473, 4.9938],
        [5.1432, 5.1427, 5.1432],
        [5.1432, 5.2173, 4.9495],
        [5.1432, 4.8932, 4.8544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:966, step:0 
model_pd.l_p.mean(): 0.21768184006214142 
model_pd.l_d.mean(): -19.3592472076416 
model_pd.lagr.mean(): -19.141565322875977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5461], device='cuda:0')), ('power', tensor([-20.2869], device='cuda:0'))])
epoch£º966	 i:0 	 global-step:19320	 l-p:0.21768184006214142
epoch£º966	 i:1 	 global-step:19321	 l-p:0.11307716369628906
epoch£º966	 i:2 	 global-step:19322	 l-p:0.07485649734735489
epoch£º966	 i:3 	 global-step:19323	 l-p:0.12527616322040558
epoch£º966	 i:4 	 global-step:19324	 l-p:0.13305442035198212
epoch£º966	 i:5 	 global-step:19325	 l-p:0.14670729637145996
epoch£º966	 i:6 	 global-step:19326	 l-p:0.14096000790596008
epoch£º966	 i:7 	 global-step:19327	 l-p:0.12066047638654709
epoch£º966	 i:8 	 global-step:19328	 l-p:0.14125142991542816
epoch£º966	 i:9 	 global-step:19329	 l-p:0.18361687660217285
====================================================================================================
====================================================================================================
====================================================================================================

epoch:967
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1441, 5.1440, 5.1441],
        [5.1441, 5.1441, 5.1441],
        [5.1441, 5.1441, 5.1441],
        [5.1441, 4.8716, 4.6329]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:967, step:0 
model_pd.l_p.mean(): 0.1639583706855774 
model_pd.l_d.mean(): -20.675933837890625 
model_pd.lagr.mean(): -20.51197624206543 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4367], device='cuda:0')), ('power', tensor([-21.5149], device='cuda:0'))])
epoch£º967	 i:0 	 global-step:19340	 l-p:0.1639583706855774
epoch£º967	 i:1 	 global-step:19341	 l-p:0.14230960607528687
epoch£º967	 i:2 	 global-step:19342	 l-p:0.16163772344589233
epoch£º967	 i:3 	 global-step:19343	 l-p:0.12318383157253265
epoch£º967	 i:4 	 global-step:19344	 l-p:0.11330652981996536
epoch£º967	 i:5 	 global-step:19345	 l-p:0.1326647698879242
epoch£º967	 i:6 	 global-step:19346	 l-p:0.1764570027589798
epoch£º967	 i:7 	 global-step:19347	 l-p:0.16810594499111176
epoch£º967	 i:8 	 global-step:19348	 l-p:0.1664620190858841
epoch£º967	 i:9 	 global-step:19349	 l-p:0.12783710658550262
====================================================================================================
====================================================================================================
====================================================================================================

epoch:968
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1357, 5.0659, 4.7449],
        [5.1357, 4.9788, 4.6495],
        [5.1357, 5.0490, 5.1023],
        [5.1357, 4.8634, 4.6195]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:968, step:0 
model_pd.l_p.mean(): 0.09063322097063065 
model_pd.l_d.mean(): -20.126506805419922 
model_pd.lagr.mean(): -20.035873413085938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5092], device='cuda:0')), ('power', tensor([-21.0303], device='cuda:0'))])
epoch£º968	 i:0 	 global-step:19360	 l-p:0.09063322097063065
epoch£º968	 i:1 	 global-step:19361	 l-p:0.18989695608615875
epoch£º968	 i:2 	 global-step:19362	 l-p:0.11228570342063904
epoch£º968	 i:3 	 global-step:19363	 l-p:0.15193681418895721
epoch£º968	 i:4 	 global-step:19364	 l-p:0.09509941935539246
epoch£º968	 i:5 	 global-step:19365	 l-p:0.1643141508102417
epoch£º968	 i:6 	 global-step:19366	 l-p:0.10588561743497849
epoch£º968	 i:7 	 global-step:19367	 l-p:0.14945818483829498
epoch£º968	 i:8 	 global-step:19368	 l-p:0.1995856761932373
epoch£º968	 i:9 	 global-step:19369	 l-p:0.1412932276725769
====================================================================================================
====================================================================================================
====================================================================================================

epoch:969
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1526, 4.8950, 4.6248],
        [5.1526, 5.2177, 4.9453],
        [5.1526, 4.9962, 4.6680],
        [5.1526, 5.0287, 5.0870]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:969, step:0 
model_pd.l_p.mean(): 0.1544525921344757 
model_pd.l_d.mean(): -20.782014846801758 
model_pd.lagr.mean(): -20.627561569213867 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3898], device='cuda:0')), ('power', tensor([-21.5744], device='cuda:0'))])
epoch£º969	 i:0 	 global-step:19380	 l-p:0.1544525921344757
epoch£º969	 i:1 	 global-step:19381	 l-p:0.13186194002628326
epoch£º969	 i:2 	 global-step:19382	 l-p:0.11181220412254333
epoch£º969	 i:3 	 global-step:19383	 l-p:0.1193288043141365
epoch£º969	 i:4 	 global-step:19384	 l-p:0.14150241017341614
epoch£º969	 i:5 	 global-step:19385	 l-p:0.19891686737537384
epoch£º969	 i:6 	 global-step:19386	 l-p:0.12374074012041092
epoch£º969	 i:7 	 global-step:19387	 l-p:0.13264693319797516
epoch£º969	 i:8 	 global-step:19388	 l-p:0.1924784779548645
epoch£º969	 i:9 	 global-step:19389	 l-p:0.09782016277313232
====================================================================================================
====================================================================================================
====================================================================================================

epoch:970
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1511, 5.1510, 5.1511],
        [5.1511, 4.9200, 4.9134],
        [5.1511, 4.9971, 5.0507],
        [5.1511, 4.8767, 4.7695]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:970, step:0 
model_pd.l_p.mean(): 0.11845490336418152 
model_pd.l_d.mean(): -20.49239158630371 
model_pd.lagr.mean(): -20.373937606811523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4389], device='cuda:0')), ('power', tensor([-21.3302], device='cuda:0'))])
epoch£º970	 i:0 	 global-step:19400	 l-p:0.11845490336418152
epoch£º970	 i:1 	 global-step:19401	 l-p:0.11454660445451736
epoch£º970	 i:2 	 global-step:19402	 l-p:0.1844346821308136
epoch£º970	 i:3 	 global-step:19403	 l-p:0.18762631714344025
epoch£º970	 i:4 	 global-step:19404	 l-p:0.1357838660478592
epoch£º970	 i:5 	 global-step:19405	 l-p:0.10842631757259369
epoch£º970	 i:6 	 global-step:19406	 l-p:0.1200149804353714
epoch£º970	 i:7 	 global-step:19407	 l-p:0.12636302411556244
epoch£º970	 i:8 	 global-step:19408	 l-p:0.13641811907291412
epoch£º970	 i:9 	 global-step:19409	 l-p:0.11119907349348068
====================================================================================================
====================================================================================================
====================================================================================================

epoch:971
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1684, 5.0748, 5.1297],
        [5.1684, 5.0793, 5.1331],
        [5.1684, 5.1676, 5.1684],
        [5.1684, 4.8911, 4.6806]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:971, step:0 
model_pd.l_p.mean(): 0.10261022299528122 
model_pd.l_d.mean(): -20.59183692932129 
model_pd.lagr.mean(): -20.489227294921875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4140], device='cuda:0')), ('power', tensor([-21.4057], device='cuda:0'))])
epoch£º971	 i:0 	 global-step:19420	 l-p:0.10261022299528122
epoch£º971	 i:1 	 global-step:19421	 l-p:0.1373707503080368
epoch£º971	 i:2 	 global-step:19422	 l-p:0.09148398041725159
epoch£º971	 i:3 	 global-step:19423	 l-p:0.14078567922115326
epoch£º971	 i:4 	 global-step:19424	 l-p:0.14680726826190948
epoch£º971	 i:5 	 global-step:19425	 l-p:0.16076552867889404
epoch£º971	 i:6 	 global-step:19426	 l-p:0.18333472311496735
epoch£º971	 i:7 	 global-step:19427	 l-p:0.10855036228895187
epoch£º971	 i:8 	 global-step:19428	 l-p:0.1506422907114029
epoch£º971	 i:9 	 global-step:19429	 l-p:0.13985902070999146
====================================================================================================
====================================================================================================
====================================================================================================

epoch:972
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1568, 5.0886, 5.1352],
        [5.1568, 5.1552, 5.1568],
        [5.1568, 4.9017, 4.8524],
        [5.1568, 5.1539, 5.1567]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:972, step:0 
model_pd.l_p.mean(): 0.15636734664440155 
model_pd.l_d.mean(): -19.805654525756836 
model_pd.lagr.mean(): -19.6492862701416 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5248], device='cuda:0')), ('power', tensor([-20.7195], device='cuda:0'))])
epoch£º972	 i:0 	 global-step:19440	 l-p:0.15636734664440155
epoch£º972	 i:1 	 global-step:19441	 l-p:0.11738260090351105
epoch£º972	 i:2 	 global-step:19442	 l-p:0.13991977274417877
epoch£º972	 i:3 	 global-step:19443	 l-p:0.1447274535894394
epoch£º972	 i:4 	 global-step:19444	 l-p:0.16019392013549805
epoch£º972	 i:5 	 global-step:19445	 l-p:0.14484061300754547
epoch£º972	 i:6 	 global-step:19446	 l-p:0.18936458230018616
epoch£º972	 i:7 	 global-step:19447	 l-p:0.08177058398723602
epoch£º972	 i:8 	 global-step:19448	 l-p:0.10858917981386185
epoch£º972	 i:9 	 global-step:19449	 l-p:0.10726410895586014
====================================================================================================
====================================================================================================
====================================================================================================

epoch:973
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1654, 5.1516, 5.1640],
        [5.1654, 4.9502, 4.9628],
        [5.1654, 5.0530, 5.1109],
        [5.1654, 5.1252, 4.8121]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:973, step:0 
model_pd.l_p.mean(): 0.17815075814723969 
model_pd.l_d.mean(): -19.830583572387695 
model_pd.lagr.mean(): -19.652433395385742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4442], device='cuda:0')), ('power', tensor([-20.6615], device='cuda:0'))])
epoch£º973	 i:0 	 global-step:19460	 l-p:0.17815075814723969
epoch£º973	 i:1 	 global-step:19461	 l-p:0.12672048807144165
epoch£º973	 i:2 	 global-step:19462	 l-p:0.06917183846235275
epoch£º973	 i:3 	 global-step:19463	 l-p:0.13373436033725739
epoch£º973	 i:4 	 global-step:19464	 l-p:0.16129113733768463
epoch£º973	 i:5 	 global-step:19465	 l-p:0.1225295439362526
epoch£º973	 i:6 	 global-step:19466	 l-p:0.15016497671604156
epoch£º973	 i:7 	 global-step:19467	 l-p:0.16741088032722473
epoch£º973	 i:8 	 global-step:19468	 l-p:0.11363013833761215
epoch£º973	 i:9 	 global-step:19469	 l-p:0.1174950823187828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:974
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1505, 4.8819, 4.7991],
        [5.1505, 5.0967, 5.1364],
        [5.1505, 4.9300, 4.9378],
        [5.1505, 5.1853, 4.8991]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:974, step:0 
model_pd.l_p.mean(): 0.14088906347751617 
model_pd.l_d.mean(): -19.08789825439453 
model_pd.lagr.mean(): -18.947010040283203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5066], device='cuda:0')), ('power', tensor([-19.9695], device='cuda:0'))])
epoch£º974	 i:0 	 global-step:19480	 l-p:0.14088906347751617
epoch£º974	 i:1 	 global-step:19481	 l-p:0.1210968941450119
epoch£º974	 i:2 	 global-step:19482	 l-p:0.1320839822292328
epoch£º974	 i:3 	 global-step:19483	 l-p:0.13833756744861603
epoch£º974	 i:4 	 global-step:19484	 l-p:0.22114361822605133
epoch£º974	 i:5 	 global-step:19485	 l-p:0.13268829882144928
epoch£º974	 i:6 	 global-step:19486	 l-p:0.13161274790763855
epoch£º974	 i:7 	 global-step:19487	 l-p:0.11265265941619873
epoch£º974	 i:8 	 global-step:19488	 l-p:0.13695214688777924
epoch£º974	 i:9 	 global-step:19489	 l-p:0.17324161529541016
====================================================================================================
====================================================================================================
====================================================================================================

epoch:975
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1372, 5.1372, 5.1372],
        [5.1372, 5.1372, 5.1372],
        [5.1372, 4.8531, 4.6584],
        [5.1372, 5.0033, 5.0614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:975, step:0 
model_pd.l_p.mean(): 0.10871972888708115 
model_pd.l_d.mean(): -19.38612174987793 
model_pd.lagr.mean(): -19.277402877807617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4823], device='cuda:0')), ('power', tensor([-20.2481], device='cuda:0'))])
epoch£º975	 i:0 	 global-step:19500	 l-p:0.10871972888708115
epoch£º975	 i:1 	 global-step:19501	 l-p:0.1948888748884201
epoch£º975	 i:2 	 global-step:19502	 l-p:0.15874360501766205
epoch£º975	 i:3 	 global-step:19503	 l-p:0.13312485814094543
epoch£º975	 i:4 	 global-step:19504	 l-p:0.10165075212717056
epoch£º975	 i:5 	 global-step:19505	 l-p:0.1492615044116974
epoch£º975	 i:6 	 global-step:19506	 l-p:0.10693033784627914
epoch£º975	 i:7 	 global-step:19507	 l-p:0.29581207036972046
epoch£º975	 i:8 	 global-step:19508	 l-p:0.11064447462558746
epoch£º975	 i:9 	 global-step:19509	 l-p:0.1468157023191452
====================================================================================================
====================================================================================================
====================================================================================================

epoch:976
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6529e-01, 1.7046e-01,
         1.0000e+00, 1.0953e-01, 1.0000e+00, 6.4255e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1159, 4.8486, 4.7789],
        [5.1159, 5.0895, 5.1119],
        [5.1159, 5.1481, 4.8605],
        [5.1159, 5.1160, 5.1159]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:976, step:0 
model_pd.l_p.mean(): 0.23750627040863037 
model_pd.l_d.mean(): -20.37867546081543 
model_pd.lagr.mean(): -20.14116859436035 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4733], device='cuda:0')), ('power', tensor([-21.2500], device='cuda:0'))])
epoch£º976	 i:0 	 global-step:19520	 l-p:0.23750627040863037
epoch£º976	 i:1 	 global-step:19521	 l-p:0.30579912662506104
epoch£º976	 i:2 	 global-step:19522	 l-p:0.15207979083061218
epoch£º976	 i:3 	 global-step:19523	 l-p:0.1534244418144226
epoch£º976	 i:4 	 global-step:19524	 l-p:0.16109833121299744
epoch£º976	 i:5 	 global-step:19525	 l-p:0.1273670196533203
epoch£º976	 i:6 	 global-step:19526	 l-p:0.11080842465162277
epoch£º976	 i:7 	 global-step:19527	 l-p:0.12810306251049042
epoch£º976	 i:8 	 global-step:19528	 l-p:0.12245047092437744
epoch£º976	 i:9 	 global-step:19529	 l-p:0.14486663043498993
====================================================================================================
====================================================================================================
====================================================================================================

epoch:977
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0561e-04, 6.2818e-05,
         1.0000e+00, 5.5925e-06, 1.0000e+00, 8.9027e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1174, 5.1174, 5.1174],
        [5.1174, 5.1174, 5.1174],
        [5.1174, 5.0203, 5.0765],
        [5.1174, 4.8588, 4.8104]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:977, step:0 
model_pd.l_p.mean(): 0.17074966430664062 
model_pd.l_d.mean(): -20.5064754486084 
model_pd.lagr.mean(): -20.335725784301758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4612], device='cuda:0')), ('power', tensor([-21.3676], device='cuda:0'))])
epoch£º977	 i:0 	 global-step:19540	 l-p:0.17074966430664062
epoch£º977	 i:1 	 global-step:19541	 l-p:0.1815207302570343
epoch£º977	 i:2 	 global-step:19542	 l-p:0.11582787334918976
epoch£º977	 i:3 	 global-step:19543	 l-p:0.12404496222734451
epoch£º977	 i:4 	 global-step:19544	 l-p:0.16914013028144836
epoch£º977	 i:5 	 global-step:19545	 l-p:0.0900007039308548
epoch£º977	 i:6 	 global-step:19546	 l-p:0.11430921405553818
epoch£º977	 i:7 	 global-step:19547	 l-p:0.2226058840751648
epoch£º977	 i:8 	 global-step:19548	 l-p:0.6121610999107361
epoch£º977	 i:9 	 global-step:19549	 l-p:0.211143359541893
====================================================================================================
====================================================================================================
====================================================================================================

epoch:978
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1338e-02, 4.1134e-02,
         1.0000e+00, 1.8525e-02, 1.0000e+00, 4.5035e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1083, 5.1076, 5.1083],
        [5.1083, 5.0258, 5.0781],
        [5.1083, 5.1083, 5.1083],
        [5.1083, 4.8196, 4.6455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:978, step:0 
model_pd.l_p.mean(): 0.1338949203491211 
model_pd.l_d.mean(): -20.15574836730957 
model_pd.lagr.mean(): -20.021854400634766 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4907], device='cuda:0')), ('power', tensor([-21.0408], device='cuda:0'))])
epoch£º978	 i:0 	 global-step:19560	 l-p:0.1338949203491211
epoch£º978	 i:1 	 global-step:19561	 l-p:0.16784580051898956
epoch£º978	 i:2 	 global-step:19562	 l-p:0.2610926330089569
epoch£º978	 i:3 	 global-step:19563	 l-p:0.12002592533826828
epoch£º978	 i:4 	 global-step:19564	 l-p:0.15042676031589508
epoch£º978	 i:5 	 global-step:19565	 l-p:0.13270916044712067
epoch£º978	 i:6 	 global-step:19566	 l-p:0.19667397439479828
epoch£º978	 i:7 	 global-step:19567	 l-p:0.28731033205986023
epoch£º978	 i:8 	 global-step:19568	 l-p:0.08733668923377991
epoch£º978	 i:9 	 global-step:19569	 l-p:0.12198871374130249
====================================================================================================
====================================================================================================
====================================================================================================

epoch:979
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3818,  0.2770,  1.0000,  0.2009,
          1.0000,  0.7255, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2894,  0.1914,  1.0000,  0.1266,
          1.0000,  0.6614, 31.6228]], device='cuda:0')
 pt:tensor([[5.1211, 5.5216, 5.4423],
        [5.1211, 4.8385, 4.6127],
        [5.1211, 4.8637, 4.5800],
        [5.1211, 4.8429, 4.7387]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:979, step:0 
model_pd.l_p.mean(): 0.17079094052314758 
model_pd.l_d.mean(): -19.676246643066406 
model_pd.lagr.mean(): -19.505455017089844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5458], device='cuda:0')), ('power', tensor([-20.6095], device='cuda:0'))])
epoch£º979	 i:0 	 global-step:19580	 l-p:0.17079094052314758
epoch£º979	 i:1 	 global-step:19581	 l-p:0.12080404162406921
epoch£º979	 i:2 	 global-step:19582	 l-p:0.18603484332561493
epoch£º979	 i:3 	 global-step:19583	 l-p:0.14864438772201538
epoch£º979	 i:4 	 global-step:19584	 l-p:0.11578036099672318
epoch£º979	 i:5 	 global-step:19585	 l-p:0.12384811788797379
epoch£º979	 i:6 	 global-step:19586	 l-p:0.12366973608732224
epoch£º979	 i:7 	 global-step:19587	 l-p:0.17282435297966003
epoch£º979	 i:8 	 global-step:19588	 l-p:0.16897325217723846
epoch£º979	 i:9 	 global-step:19589	 l-p:0.12410276383161545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:980
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3512e-02, 2.5340e-02,
         1.0000e+00, 1.0110e-02, 1.0000e+00, 3.9898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1507, 5.1507, 5.1507],
        [5.1507, 5.1477, 5.1506],
        [5.1507, 5.1045, 5.1400],
        [5.1507, 5.1504, 5.1507]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:980, step:0 
model_pd.l_p.mean(): 0.19146902859210968 
model_pd.l_d.mean(): -20.51371192932129 
model_pd.lagr.mean(): -20.322242736816406 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4328], device='cuda:0')), ('power', tensor([-21.3456], device='cuda:0'))])
epoch£º980	 i:0 	 global-step:19600	 l-p:0.19146902859210968
epoch£º980	 i:1 	 global-step:19601	 l-p:0.14873525500297546
epoch£º980	 i:2 	 global-step:19602	 l-p:0.14005360007286072
epoch£º980	 i:3 	 global-step:19603	 l-p:0.08310304582118988
epoch£º980	 i:4 	 global-step:19604	 l-p:0.10700754076242447
epoch£º980	 i:5 	 global-step:19605	 l-p:0.1964731514453888
epoch£º980	 i:6 	 global-step:19606	 l-p:0.11639928817749023
epoch£º980	 i:7 	 global-step:19607	 l-p:0.15078265964984894
epoch£º980	 i:8 	 global-step:19608	 l-p:0.12626396119594574
epoch£º980	 i:9 	 global-step:19609	 l-p:0.11125118285417557
====================================================================================================
====================================================================================================
====================================================================================================

epoch:981
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1664, 5.0421, 5.1005],
        [5.1664, 5.1664, 5.1664],
        [5.1664, 5.1642, 5.1663],
        [5.1664, 4.9144, 4.6337]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:981, step:0 
model_pd.l_p.mean(): 0.1402468979358673 
model_pd.l_d.mean(): -19.598447799682617 
model_pd.lagr.mean(): -19.458200454711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4847], device='cuda:0')), ('power', tensor([-20.4669], device='cuda:0'))])
epoch£º981	 i:0 	 global-step:19620	 l-p:0.1402468979358673
epoch£º981	 i:1 	 global-step:19621	 l-p:0.13156835734844208
epoch£º981	 i:2 	 global-step:19622	 l-p:0.13242974877357483
epoch£º981	 i:3 	 global-step:19623	 l-p:0.14050036668777466
epoch£º981	 i:4 	 global-step:19624	 l-p:0.1310531198978424
epoch£º981	 i:5 	 global-step:19625	 l-p:0.16352912783622742
epoch£º981	 i:6 	 global-step:19626	 l-p:0.15699777007102966
epoch£º981	 i:7 	 global-step:19627	 l-p:0.1659894585609436
epoch£º981	 i:8 	 global-step:19628	 l-p:0.09012781083583832
epoch£º981	 i:9 	 global-step:19629	 l-p:0.18222396075725555
====================================================================================================
====================================================================================================
====================================================================================================

epoch:982
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1371, 4.8842, 4.8456],
        [5.1371, 5.2855, 5.0531],
        [5.1371, 5.1371, 5.1371],
        [5.1371, 5.0494, 5.1032]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:982, step:0 
model_pd.l_p.mean(): 0.1478634476661682 
model_pd.l_d.mean(): -20.350811004638672 
model_pd.lagr.mean(): -20.20294761657715 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4691], device='cuda:0')), ('power', tensor([-21.2173], device='cuda:0'))])
epoch£º982	 i:0 	 global-step:19640	 l-p:0.1478634476661682
epoch£º982	 i:1 	 global-step:19641	 l-p:0.16068236529827118
epoch£º982	 i:2 	 global-step:19642	 l-p:0.17023120820522308
epoch£º982	 i:3 	 global-step:19643	 l-p:0.14955931901931763
epoch£º982	 i:4 	 global-step:19644	 l-p:0.14934974908828735
epoch£º982	 i:5 	 global-step:19645	 l-p:0.06731811165809631
epoch£º982	 i:6 	 global-step:19646	 l-p:0.1438174545764923
epoch£º982	 i:7 	 global-step:19647	 l-p:0.12309170514345169
epoch£º982	 i:8 	 global-step:19648	 l-p:0.16372397541999817
epoch£º982	 i:9 	 global-step:19649	 l-p:0.16491414606571198
====================================================================================================
====================================================================================================
====================================================================================================

epoch:983
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1449, 4.9730, 5.0208],
        [5.1449, 5.0107, 5.0690],
        [5.1449, 5.0573, 5.1110],
        [5.1449, 5.1419, 5.1448]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:983, step:0 
model_pd.l_p.mean(): 0.06880345195531845 
model_pd.l_d.mean(): -20.519487380981445 
model_pd.lagr.mean(): -20.45068359375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4507], device='cuda:0')), ('power', tensor([-21.3700], device='cuda:0'))])
epoch£º983	 i:0 	 global-step:19660	 l-p:0.06880345195531845
epoch£º983	 i:1 	 global-step:19661	 l-p:0.11847633123397827
epoch£º983	 i:2 	 global-step:19662	 l-p:0.14260831475257874
epoch£º983	 i:3 	 global-step:19663	 l-p:0.13126523792743683
epoch£º983	 i:4 	 global-step:19664	 l-p:0.1299935281276703
epoch£º983	 i:5 	 global-step:19665	 l-p:0.11747276782989502
epoch£º983	 i:6 	 global-step:19666	 l-p:0.14712336659431458
epoch£º983	 i:7 	 global-step:19667	 l-p:0.14443089067935944
epoch£º983	 i:8 	 global-step:19668	 l-p:0.2373393177986145
epoch£º983	 i:9 	 global-step:19669	 l-p:0.1475297510623932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:984
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1563, 4.9659, 4.6409],
        [5.1563, 5.1501, 5.1559],
        [5.1563, 5.1562, 5.1563],
        [5.1563, 5.1511, 5.1560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:984, step:0 
model_pd.l_p.mean(): 0.07794815301895142 
model_pd.l_d.mean(): -20.03878402709961 
model_pd.lagr.mean(): -19.96083641052246 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4972], device='cuda:0')), ('power', tensor([-20.9285], device='cuda:0'))])
epoch£º984	 i:0 	 global-step:19680	 l-p:0.07794815301895142
epoch£º984	 i:1 	 global-step:19681	 l-p:0.18309235572814941
epoch£º984	 i:2 	 global-step:19682	 l-p:0.11832063645124435
epoch£º984	 i:3 	 global-step:19683	 l-p:0.13814190030097961
epoch£º984	 i:4 	 global-step:19684	 l-p:0.12070133537054062
epoch£º984	 i:5 	 global-step:19685	 l-p:0.11529127508401871
epoch£º984	 i:6 	 global-step:19686	 l-p:0.11451704055070877
epoch£º984	 i:7 	 global-step:19687	 l-p:0.1808529496192932
epoch£º984	 i:8 	 global-step:19688	 l-p:0.20669494569301605
epoch£º984	 i:9 	 global-step:19689	 l-p:0.14335934817790985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:985
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1514e-01, 6.3952e-01,
         1.0000e+00, 5.7190e-01, 1.0000e+00, 8.9426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7692e-07, 1.8050e-09,
         1.0000e+00, 1.1765e-11, 1.0000e+00, 6.5181e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1411, 5.5232, 5.4306],
        [5.1411, 4.9689, 5.0168],
        [5.1411, 5.1745, 4.8864],
        [5.1411, 5.1411, 5.1411]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:985, step:0 
model_pd.l_p.mean(): 0.1900428682565689 
model_pd.l_d.mean(): -20.286361694335938 
model_pd.lagr.mean(): -20.0963191986084 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4664], device='cuda:0')), ('power', tensor([-21.1487], device='cuda:0'))])
epoch£º985	 i:0 	 global-step:19700	 l-p:0.1900428682565689
epoch£º985	 i:1 	 global-step:19701	 l-p:0.12848518788814545
epoch£º985	 i:2 	 global-step:19702	 l-p:0.14691151678562164
epoch£º985	 i:3 	 global-step:19703	 l-p:0.16115298867225647
epoch£º985	 i:4 	 global-step:19704	 l-p:0.16215331852436066
epoch£º985	 i:5 	 global-step:19705	 l-p:0.11398860067129135
epoch£º985	 i:6 	 global-step:19706	 l-p:0.1338898241519928
epoch£º985	 i:7 	 global-step:19707	 l-p:0.12946592271327972
epoch£º985	 i:8 	 global-step:19708	 l-p:0.1877918839454651
epoch£º985	 i:9 	 global-step:19709	 l-p:0.08887694031000137
====================================================================================================
====================================================================================================
====================================================================================================

epoch:986
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1490, 5.1490, 5.1490],
        [5.1490, 5.0560, 5.1112],
        [5.1490, 5.1445, 5.1488],
        [5.1490, 5.0179, 5.0765]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:986, step:0 
model_pd.l_p.mean(): 0.09716861695051193 
model_pd.l_d.mean(): -20.354642868041992 
model_pd.lagr.mean(): -20.257474899291992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4600], device='cuda:0')), ('power', tensor([-21.2117], device='cuda:0'))])
epoch£º986	 i:0 	 global-step:19720	 l-p:0.09716861695051193
epoch£º986	 i:1 	 global-step:19721	 l-p:0.16317768394947052
epoch£º986	 i:2 	 global-step:19722	 l-p:0.1188986524939537
epoch£º986	 i:3 	 global-step:19723	 l-p:0.09183445572853088
epoch£º986	 i:4 	 global-step:19724	 l-p:0.1569213569164276
epoch£º986	 i:5 	 global-step:19725	 l-p:0.2095029056072235
epoch£º986	 i:6 	 global-step:19726	 l-p:0.17819800972938538
epoch£º986	 i:7 	 global-step:19727	 l-p:0.13592082262039185
epoch£º986	 i:8 	 global-step:19728	 l-p:0.1027795597910881
epoch£º986	 i:9 	 global-step:19729	 l-p:0.1070316880941391
====================================================================================================
====================================================================================================
====================================================================================================

epoch:987
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1790, 5.1790, 5.1790],
        [5.1790, 5.1790, 5.1790],
        [5.1790, 5.1297, 5.1670],
        [5.1790, 5.0916, 5.1452]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:987, step:0 
model_pd.l_p.mean(): 0.11521384865045547 
model_pd.l_d.mean(): -19.223915100097656 
model_pd.lagr.mean(): -19.108701705932617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4835], device='cuda:0')), ('power', tensor([-20.0841], device='cuda:0'))])
epoch£º987	 i:0 	 global-step:19740	 l-p:0.11521384865045547
epoch£º987	 i:1 	 global-step:19741	 l-p:0.15307173132896423
epoch£º987	 i:2 	 global-step:19742	 l-p:0.03680999577045441
epoch£º987	 i:3 	 global-step:19743	 l-p:0.09289384633302689
epoch£º987	 i:4 	 global-step:19744	 l-p:0.1514529287815094
epoch£º987	 i:5 	 global-step:19745	 l-p:0.14232872426509857
epoch£º987	 i:6 	 global-step:19746	 l-p:0.1320561170578003
epoch£º987	 i:7 	 global-step:19747	 l-p:0.1702963262796402
epoch£º987	 i:8 	 global-step:19748	 l-p:0.15914589166641235
epoch£º987	 i:9 	 global-step:19749	 l-p:0.14400817453861237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:988
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1778, 4.8983, 4.6820],
        [5.1778, 4.8974, 4.7630],
        [5.1778, 5.1774, 5.1778],
        [5.1778, 5.1778, 5.1778]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:988, step:0 
model_pd.l_p.mean(): 0.16457267105579376 
model_pd.l_d.mean(): -20.583284378051758 
model_pd.lagr.mean(): -20.418712615966797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4092], device='cuda:0')), ('power', tensor([-21.3919], device='cuda:0'))])
epoch£º988	 i:0 	 global-step:19760	 l-p:0.16457267105579376
epoch£º988	 i:1 	 global-step:19761	 l-p:0.10905392467975616
epoch£º988	 i:2 	 global-step:19762	 l-p:0.13015340268611908
epoch£º988	 i:3 	 global-step:19763	 l-p:0.1460866779088974
epoch£º988	 i:4 	 global-step:19764	 l-p:0.16829179227352142
epoch£º988	 i:5 	 global-step:19765	 l-p:0.11596854031085968
epoch£º988	 i:6 	 global-step:19766	 l-p:0.09356415271759033
epoch£º988	 i:7 	 global-step:19767	 l-p:0.17607206106185913
epoch£º988	 i:8 	 global-step:19768	 l-p:0.15407812595367432
epoch£º988	 i:9 	 global-step:19769	 l-p:0.10799239575862885
====================================================================================================
====================================================================================================
====================================================================================================

epoch:989
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4579e-02, 3.5616e-03,
         1.0000e+00, 8.7008e-04, 1.0000e+00, 2.4429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1527, 4.9398, 4.9583],
        [5.1527, 5.1495, 5.1526],
        [5.1527, 5.0215, 5.0802],
        [5.1527, 5.1527, 5.1527]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:989, step:0 
model_pd.l_p.mean(): 0.13130871951580048 
model_pd.l_d.mean(): -20.607160568237305 
model_pd.lagr.mean(): -20.47585105895996 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4100], device='cuda:0')), ('power', tensor([-21.4171], device='cuda:0'))])
epoch£º989	 i:0 	 global-step:19780	 l-p:0.13130871951580048
epoch£º989	 i:1 	 global-step:19781	 l-p:0.1927664577960968
epoch£º989	 i:2 	 global-step:19782	 l-p:0.14403192698955536
epoch£º989	 i:3 	 global-step:19783	 l-p:0.07331740856170654
epoch£º989	 i:4 	 global-step:19784	 l-p:0.17031559348106384
epoch£º989	 i:5 	 global-step:19785	 l-p:0.16997399926185608
epoch£º989	 i:6 	 global-step:19786	 l-p:0.13006876409053802
epoch£º989	 i:7 	 global-step:19787	 l-p:0.16025108098983765
epoch£º989	 i:8 	 global-step:19788	 l-p:0.07569418102502823
epoch£º989	 i:9 	 global-step:19789	 l-p:0.13440096378326416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:990
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8776,  0.8402,  1.0000,  0.8044,
          1.0000,  0.9574, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2832,  0.1859,  1.0000,  0.1221,
          1.0000,  0.6567, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5465,  0.4468,  1.0000,  0.3653,
          1.0000,  0.8176, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8937,  0.8609,  1.0000,  0.8293,
          1.0000,  0.9632, 31.6228]], device='cuda:0')
 pt:tensor([[5.1744, 5.4603, 5.3054],
        [5.1744, 4.9012, 4.8051],
        [5.1744, 5.0068, 4.6773],
        [5.1744, 5.4859, 5.3468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:990, step:0 
model_pd.l_p.mean(): 0.10025857388973236 
model_pd.l_d.mean(): -19.020702362060547 
model_pd.lagr.mean(): -18.92044448852539 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5886], device='cuda:0')), ('power', tensor([-19.9860], device='cuda:0'))])
epoch£º990	 i:0 	 global-step:19800	 l-p:0.10025857388973236
epoch£º990	 i:1 	 global-step:19801	 l-p:0.13635501265525818
epoch£º990	 i:2 	 global-step:19802	 l-p:0.12069302052259445
epoch£º990	 i:3 	 global-step:19803	 l-p:0.1296754777431488
epoch£º990	 i:4 	 global-step:19804	 l-p:0.11098439991474152
epoch£º990	 i:5 	 global-step:19805	 l-p:0.15118353068828583
epoch£º990	 i:6 	 global-step:19806	 l-p:0.144227996468544
epoch£º990	 i:7 	 global-step:19807	 l-p:0.1405443400144577
epoch£º990	 i:8 	 global-step:19808	 l-p:0.14221473038196564
epoch£º990	 i:9 	 global-step:19809	 l-p:0.0695442408323288
====================================================================================================
====================================================================================================
====================================================================================================

epoch:991
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2025, 5.1520, 5.1899],
        [5.2025, 4.9227, 4.7763],
        [5.2025, 5.1218, 5.1732],
        [5.2025, 5.1984, 5.2023]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:991, step:0 
model_pd.l_p.mean(): 0.13157445192337036 
model_pd.l_d.mean(): -20.737642288208008 
model_pd.lagr.mean(): -20.606067657470703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3810], device='cuda:0')), ('power', tensor([-21.5200], device='cuda:0'))])
epoch£º991	 i:0 	 global-step:19820	 l-p:0.13157445192337036
epoch£º991	 i:1 	 global-step:19821	 l-p:0.04974091798067093
epoch£º991	 i:2 	 global-step:19822	 l-p:0.13302354514598846
epoch£º991	 i:3 	 global-step:19823	 l-p:0.15568824112415314
epoch£º991	 i:4 	 global-step:19824	 l-p:0.12727577984333038
epoch£º991	 i:5 	 global-step:19825	 l-p:0.11827173829078674
epoch£º991	 i:6 	 global-step:19826	 l-p:0.1408528834581375
epoch£º991	 i:7 	 global-step:19827	 l-p:0.16564859449863434
epoch£º991	 i:8 	 global-step:19828	 l-p:0.10608257353305817
epoch£º991	 i:9 	 global-step:19829	 l-p:0.10888355225324631
====================================================================================================
====================================================================================================
====================================================================================================

epoch:992
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3110e-02, 1.0632e-02,
         1.0000e+00, 3.4141e-03, 1.0000e+00, 3.2111e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1726e-01, 6.4204e-01,
         1.0000e+00, 5.7472e-01, 1.0000e+00, 8.9514e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1832, 5.1831, 5.1832],
        [5.1832, 5.1683, 5.1816],
        [5.1832, 5.2288, 4.9452],
        [5.1832, 5.0290, 4.6984]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:992, step:0 
model_pd.l_p.mean(): 0.11576421558856964 
model_pd.l_d.mean(): -20.503787994384766 
model_pd.lagr.mean(): -20.388023376464844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4180], device='cuda:0')), ('power', tensor([-21.3201], device='cuda:0'))])
epoch£º992	 i:0 	 global-step:19840	 l-p:0.11576421558856964
epoch£º992	 i:1 	 global-step:19841	 l-p:0.15302512049674988
epoch£º992	 i:2 	 global-step:19842	 l-p:0.13253170251846313
epoch£º992	 i:3 	 global-step:19843	 l-p:0.10739842802286148
epoch£º992	 i:4 	 global-step:19844	 l-p:0.16404159367084503
epoch£º992	 i:5 	 global-step:19845	 l-p:0.10380657017230988
epoch£º992	 i:6 	 global-step:19846	 l-p:0.16149264574050903
epoch£º992	 i:7 	 global-step:19847	 l-p:0.1294970065355301
epoch£º992	 i:8 	 global-step:19848	 l-p:0.10918151587247849
epoch£º992	 i:9 	 global-step:19849	 l-p:0.14037474989891052
====================================================================================================
====================================================================================================
====================================================================================================

epoch:993
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4931e-03, 1.7065e-04,
         1.0000e+00, 1.9504e-05, 1.0000e+00, 1.1429e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1637, 5.2655, 5.0078],
        [5.1637, 4.8785, 4.6851],
        [5.1637, 5.1637, 5.1637],
        [5.1637, 5.1526, 5.1628]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:993, step:0 
model_pd.l_p.mean(): 0.25169453024864197 
model_pd.l_d.mean(): -20.338504791259766 
model_pd.lagr.mean(): -20.086811065673828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4685], device='cuda:0')), ('power', tensor([-21.2040], device='cuda:0'))])
epoch£º993	 i:0 	 global-step:19860	 l-p:0.25169453024864197
epoch£º993	 i:1 	 global-step:19861	 l-p:0.1505364626646042
epoch£º993	 i:2 	 global-step:19862	 l-p:0.12192483991384506
epoch£º993	 i:3 	 global-step:19863	 l-p:0.15163485705852509
epoch£º993	 i:4 	 global-step:19864	 l-p:0.09567833691835403
epoch£º993	 i:5 	 global-step:19865	 l-p:0.09804944694042206
epoch£º993	 i:6 	 global-step:19866	 l-p:0.12189555913209915
epoch£º993	 i:7 	 global-step:19867	 l-p:0.13089820742607117
epoch£º993	 i:8 	 global-step:19868	 l-p:0.13755111396312714
epoch£º993	 i:9 	 global-step:19869	 l-p:0.1301753968000412
====================================================================================================
====================================================================================================
====================================================================================================

epoch:994
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1332, 5.1282, 5.1329],
        [5.1332, 5.1556, 4.8619],
        [5.1332, 4.8900, 4.5870],
        [5.1332, 5.1330, 5.1332]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:994, step:0 
model_pd.l_p.mean(): 0.13281653821468353 
model_pd.l_d.mean(): -20.207948684692383 
model_pd.lagr.mean(): -20.075132369995117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4747], device='cuda:0')), ('power', tensor([-21.0775], device='cuda:0'))])
epoch£º994	 i:0 	 global-step:19880	 l-p:0.13281653821468353
epoch£º994	 i:1 	 global-step:19881	 l-p:0.11214890331029892
epoch£º994	 i:2 	 global-step:19882	 l-p:0.11666528135538101
epoch£º994	 i:3 	 global-step:19883	 l-p:0.14620256423950195
epoch£º994	 i:4 	 global-step:19884	 l-p:0.12432774156332016
epoch£º994	 i:5 	 global-step:19885	 l-p:0.13164551556110382
epoch£º994	 i:6 	 global-step:19886	 l-p:0.5963085293769836
epoch£º994	 i:7 	 global-step:19887	 l-p:0.19991229474544525
epoch£º994	 i:8 	 global-step:19888	 l-p:0.1871735006570816
epoch£º994	 i:9 	 global-step:19889	 l-p:0.17444993555545807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:995
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1109, 4.9893, 5.0490],
        [5.1109, 4.9065, 4.5778],
        [5.1109, 4.9108, 4.9439],
        [5.1109, 5.1087, 5.1108]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:995, step:0 
model_pd.l_p.mean(): 0.17543049156665802 
model_pd.l_d.mean(): -20.56253433227539 
model_pd.lagr.mean(): -20.387104034423828 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4496], device='cuda:0')), ('power', tensor([-21.4127], device='cuda:0'))])
epoch£º995	 i:0 	 global-step:19900	 l-p:0.17543049156665802
epoch£º995	 i:1 	 global-step:19901	 l-p:0.11274095624685287
epoch£º995	 i:2 	 global-step:19902	 l-p:0.1276187151670456
epoch£º995	 i:3 	 global-step:19903	 l-p:0.26761531829833984
epoch£º995	 i:4 	 global-step:19904	 l-p:0.15635240077972412
epoch£º995	 i:5 	 global-step:19905	 l-p:0.29449018836021423
epoch£º995	 i:6 	 global-step:19906	 l-p:0.11942621320486069
epoch£º995	 i:7 	 global-step:19907	 l-p:0.08626964688301086
epoch£º995	 i:8 	 global-step:19908	 l-p:0.1528378427028656
epoch£º995	 i:9 	 global-step:19909	 l-p:0.12300591170787811
====================================================================================================
====================================================================================================
====================================================================================================

epoch:996
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1294, 4.9053, 4.9145],
        [5.1294, 5.4391, 5.2993],
        [5.1294, 4.8395, 4.6494],
        [5.1294, 5.1287, 5.1294]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:996, step:0 
model_pd.l_p.mean(): 0.11245783418416977 
model_pd.l_d.mean(): -18.519554138183594 
model_pd.lagr.mean(): -18.40709686279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5974], device='cuda:0')), ('power', tensor([-19.4845], device='cuda:0'))])
epoch£º996	 i:0 	 global-step:19920	 l-p:0.11245783418416977
epoch£º996	 i:1 	 global-step:19921	 l-p:0.24873168766498566
epoch£º996	 i:2 	 global-step:19922	 l-p:0.1389463245868683
epoch£º996	 i:3 	 global-step:19923	 l-p:0.14955870807170868
epoch£º996	 i:4 	 global-step:19924	 l-p:0.1739998608827591
epoch£º996	 i:5 	 global-step:19925	 l-p:0.14291757345199585
epoch£º996	 i:6 	 global-step:19926	 l-p:0.12867514789104462
epoch£º996	 i:7 	 global-step:19927	 l-p:0.13958650827407837
epoch£º996	 i:8 	 global-step:19928	 l-p:0.12783841788768768
epoch£º996	 i:9 	 global-step:19929	 l-p:0.11884256452322006
====================================================================================================
====================================================================================================
====================================================================================================

epoch:997
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1531, 5.0743, 5.1254],
        [5.1531, 5.1531, 5.1531],
        [5.1531, 4.8686, 4.7341],
        [5.1531, 5.2207, 4.9465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:997, step:0 
model_pd.l_p.mean(): 0.15146806836128235 
model_pd.l_d.mean(): -19.77956771850586 
model_pd.lagr.mean(): -19.62809944152832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5395], device='cuda:0')), ('power', tensor([-20.7082], device='cuda:0'))])
epoch£º997	 i:0 	 global-step:19940	 l-p:0.15146806836128235
epoch£º997	 i:1 	 global-step:19941	 l-p:0.13948796689510345
epoch£º997	 i:2 	 global-step:19942	 l-p:0.11499485373497009
epoch£º997	 i:3 	 global-step:19943	 l-p:0.10998807847499847
epoch£º997	 i:4 	 global-step:19944	 l-p:0.16888366639614105
epoch£º997	 i:5 	 global-step:19945	 l-p:0.18751750886440277
epoch£º997	 i:6 	 global-step:19946	 l-p:0.14123715460300446
epoch£º997	 i:7 	 global-step:19947	 l-p:0.13080106675624847
epoch£º997	 i:8 	 global-step:19948	 l-p:0.10927153378725052
epoch£º997	 i:9 	 global-step:19949	 l-p:0.1946048140525818
====================================================================================================
====================================================================================================
====================================================================================================

epoch:998
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1423, 4.9588, 4.6272],
        [5.1423, 5.1406, 5.1422],
        [5.1423, 5.1423, 5.1423],
        [5.1423, 5.1039, 5.1346]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:998, step:0 
model_pd.l_p.mean(): 0.1354992687702179 
model_pd.l_d.mean(): -20.863269805908203 
model_pd.lagr.mean(): -20.72776985168457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3782], device='cuda:0')), ('power', tensor([-21.6451], device='cuda:0'))])
epoch£º998	 i:0 	 global-step:19960	 l-p:0.1354992687702179
epoch£º998	 i:1 	 global-step:19961	 l-p:0.08158336579799652
epoch£º998	 i:2 	 global-step:19962	 l-p:0.15634658932685852
epoch£º998	 i:3 	 global-step:19963	 l-p:0.20906253159046173
epoch£º998	 i:4 	 global-step:19964	 l-p:0.12694254517555237
epoch£º998	 i:5 	 global-step:19965	 l-p:0.1465056985616684
epoch£º998	 i:6 	 global-step:19966	 l-p:0.1611102670431137
epoch£º998	 i:7 	 global-step:19967	 l-p:0.10372110456228256
epoch£º998	 i:8 	 global-step:19968	 l-p:0.164484903216362
epoch£º998	 i:9 	 global-step:19969	 l-p:0.14797677099704742
====================================================================================================
====================================================================================================
====================================================================================================

epoch:999
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8378e-01, 1.0449e-01,
         1.0000e+00, 5.9405e-02, 1.0000e+00, 5.6854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1554, 5.1552, 5.1554],
        [5.1554, 4.9538, 4.9837],
        [5.1554, 5.1525, 5.1553],
        [5.1554, 5.1508, 5.1552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:999, step:0 
model_pd.l_p.mean(): 0.15082861483097076 
model_pd.l_d.mean(): -17.910362243652344 
model_pd.lagr.mean(): -17.759532928466797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6039], device='cuda:0')), ('power', tensor([-18.8708], device='cuda:0'))])
epoch£º999	 i:0 	 global-step:19980	 l-p:0.15082861483097076
epoch£º999	 i:1 	 global-step:19981	 l-p:0.09945961087942123
epoch£º999	 i:2 	 global-step:19982	 l-p:0.1774451732635498
epoch£º999	 i:3 	 global-step:19983	 l-p:0.12572184205055237
epoch£º999	 i:4 	 global-step:19984	 l-p:0.12111469358205795
epoch£º999	 i:5 	 global-step:19985	 l-p:0.1464143693447113
epoch£º999	 i:6 	 global-step:19986	 l-p:0.1295175403356552
epoch£º999	 i:7 	 global-step:19987	 l-p:0.1265404224395752
epoch£º999	 i:8 	 global-step:19988	 l-p:0.1773381531238556
epoch£º999	 i:9 	 global-step:19989	 l-p:0.11446680873632431
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1000
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1646, 5.0765, 5.1305],
        [5.1646, 4.9327, 4.6242],
        [5.1646, 5.0712, 5.1266],
        [5.1646, 5.1956, 4.9050]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1000, step:0 
model_pd.l_p.mean(): 0.12293410301208496 
model_pd.l_d.mean(): -20.35785484313965 
model_pd.lagr.mean(): -20.234920501708984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4399], device='cuda:0')), ('power', tensor([-21.1941], device='cuda:0'))])
epoch£º1000	 i:0 	 global-step:20000	 l-p:0.12293410301208496
epoch£º1000	 i:1 	 global-step:20001	 l-p:0.12404660135507584
epoch£º1000	 i:2 	 global-step:20002	 l-p:0.1352112740278244
epoch£º1000	 i:3 	 global-step:20003	 l-p:0.16273199021816254
epoch£º1000	 i:4 	 global-step:20004	 l-p:0.1354575753211975
epoch£º1000	 i:5 	 global-step:20005	 l-p:0.1308884173631668
epoch£º1000	 i:6 	 global-step:20006	 l-p:0.13851675391197205
epoch£º1000	 i:7 	 global-step:20007	 l-p:0.0932314470410347
epoch£º1000	 i:8 	 global-step:20008	 l-p:0.2137971967458725
epoch£º1000	 i:9 	 global-step:20009	 l-p:0.1400051712989807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1001
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1506, 5.1494, 5.1506],
        [5.1506, 4.8961, 4.8584],
        [5.1506, 4.9853, 5.0370],
        [5.1506, 5.1506, 5.1506]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1001, step:0 
model_pd.l_p.mean(): 0.10513513535261154 
model_pd.l_d.mean(): -19.312410354614258 
model_pd.lagr.mean(): -19.207275390625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4808], device='cuda:0')), ('power', tensor([-20.1715], device='cuda:0'))])
epoch£º1001	 i:0 	 global-step:20020	 l-p:0.10513513535261154
epoch£º1001	 i:1 	 global-step:20021	 l-p:0.19203603267669678
epoch£º1001	 i:2 	 global-step:20022	 l-p:0.14155082404613495
epoch£º1001	 i:3 	 global-step:20023	 l-p:0.21327322721481323
epoch£º1001	 i:4 	 global-step:20024	 l-p:0.08801965415477753
epoch£º1001	 i:5 	 global-step:20025	 l-p:0.08459608256816864
epoch£º1001	 i:6 	 global-step:20026	 l-p:0.13062375783920288
epoch£º1001	 i:7 	 global-step:20027	 l-p:0.1719050109386444
epoch£º1001	 i:8 	 global-step:20028	 l-p:0.18940356373786926
epoch£º1001	 i:9 	 global-step:20029	 l-p:0.10738711804151535
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1002
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2980e-01, 6.5723e-02,
         1.0000e+00, 3.3277e-02, 1.0000e+00, 5.0633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1456, 5.1237, 5.1427],
        [5.1456, 4.9213, 4.9298],
        [5.1456, 5.5107, 5.4054],
        [5.1456, 5.0101, 5.0691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1002, step:0 
model_pd.l_p.mean(): 0.10494270920753479 
model_pd.l_d.mean(): -20.346416473388672 
model_pd.lagr.mean(): -20.241474151611328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4546], device='cuda:0')), ('power', tensor([-21.1977], device='cuda:0'))])
epoch£º1002	 i:0 	 global-step:20040	 l-p:0.10494270920753479
epoch£º1002	 i:1 	 global-step:20041	 l-p:0.21822383999824524
epoch£º1002	 i:2 	 global-step:20042	 l-p:0.13085608184337616
epoch£º1002	 i:3 	 global-step:20043	 l-p:0.11174021661281586
epoch£º1002	 i:4 	 global-step:20044	 l-p:0.10667617619037628
epoch£º1002	 i:5 	 global-step:20045	 l-p:0.2029530256986618
epoch£º1002	 i:6 	 global-step:20046	 l-p:0.14787399768829346
epoch£º1002	 i:7 	 global-step:20047	 l-p:0.13084469735622406
epoch£º1002	 i:8 	 global-step:20048	 l-p:0.14014016091823578
epoch£º1002	 i:9 	 global-step:20049	 l-p:0.1296456903219223
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1003
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1526, 4.9782, 4.6453],
        [5.1526, 5.1526, 5.1526],
        [5.1526, 5.1526, 5.1526],
        [5.1526, 5.1100, 5.1434]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1003, step:0 
model_pd.l_p.mean(): 0.12365169078111649 
model_pd.l_d.mean(): -19.590606689453125 
model_pd.lagr.mean(): -19.466955184936523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5507], device='cuda:0')), ('power', tensor([-20.5273], device='cuda:0'))])
epoch£º1003	 i:0 	 global-step:20060	 l-p:0.12365169078111649
epoch£º1003	 i:1 	 global-step:20061	 l-p:0.14478278160095215
epoch£º1003	 i:2 	 global-step:20062	 l-p:0.1584887057542801
epoch£º1003	 i:3 	 global-step:20063	 l-p:0.20261317491531372
epoch£º1003	 i:4 	 global-step:20064	 l-p:0.127982035279274
epoch£º1003	 i:5 	 global-step:20065	 l-p:0.14278939366340637
epoch£º1003	 i:6 	 global-step:20066	 l-p:0.11516376584768295
epoch£º1003	 i:7 	 global-step:20067	 l-p:0.10860715806484222
epoch£º1003	 i:8 	 global-step:20068	 l-p:0.13950835168361664
epoch£º1003	 i:9 	 global-step:20069	 l-p:0.15124329924583435
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1004
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1419, 4.8532, 4.6501],
        [5.1419, 5.1284, 4.8202],
        [5.1419, 4.8936, 4.8686],
        [5.1419, 5.1417, 5.1419]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1004, step:0 
model_pd.l_p.mean(): 0.1751071959733963 
model_pd.l_d.mean(): -20.14602279663086 
model_pd.lagr.mean(): -19.970914840698242 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5035], device='cuda:0')), ('power', tensor([-21.0442], device='cuda:0'))])
epoch£º1004	 i:0 	 global-step:20080	 l-p:0.1751071959733963
epoch£º1004	 i:1 	 global-step:20081	 l-p:0.17219755053520203
epoch£º1004	 i:2 	 global-step:20082	 l-p:0.13521148264408112
epoch£º1004	 i:3 	 global-step:20083	 l-p:0.13620921969413757
epoch£º1004	 i:4 	 global-step:20084	 l-p:0.1305614411830902
epoch£º1004	 i:5 	 global-step:20085	 l-p:0.12118473649024963
epoch£º1004	 i:6 	 global-step:20086	 l-p:0.10564760118722916
epoch£º1004	 i:7 	 global-step:20087	 l-p:0.13762058317661285
epoch£º1004	 i:8 	 global-step:20088	 l-p:0.16240713000297546
epoch£º1004	 i:9 	 global-step:20089	 l-p:0.22445398569107056
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1005
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1332, 5.0185, 5.0777],
        [5.1332, 4.8824, 4.8543],
        [5.1332, 5.1318, 5.1332],
        [5.1332, 4.9591, 5.0078]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1005, step:0 
model_pd.l_p.mean(): 0.12864449620246887 
model_pd.l_d.mean(): -20.358264923095703 
model_pd.lagr.mean(): -20.2296199798584 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4355], device='cuda:0')), ('power', tensor([-21.1900], device='cuda:0'))])
epoch£º1005	 i:0 	 global-step:20100	 l-p:0.12864449620246887
epoch£º1005	 i:1 	 global-step:20101	 l-p:0.11502023786306381
epoch£º1005	 i:2 	 global-step:20102	 l-p:0.14420531690120697
epoch£º1005	 i:3 	 global-step:20103	 l-p:0.18388782441616058
epoch£º1005	 i:4 	 global-step:20104	 l-p:0.09849868714809418
epoch£º1005	 i:5 	 global-step:20105	 l-p:0.2008904218673706
epoch£º1005	 i:6 	 global-step:20106	 l-p:0.18470929563045502
epoch£º1005	 i:7 	 global-step:20107	 l-p:0.1498701572418213
epoch£º1005	 i:8 	 global-step:20108	 l-p:0.10880658775568008
epoch£º1005	 i:9 	 global-step:20109	 l-p:0.21478793025016785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1006
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8889,  0.8547,  1.0000,  0.8218,
          1.0000,  0.9615, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5472,  0.4475,  1.0000,  0.3661,
          1.0000,  0.8179, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7922,  0.7330,  1.0000,  0.6782,
          1.0000,  0.9253, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7532,  0.6853,  1.0000,  0.6235,
          1.0000,  0.9099, 31.6228]], device='cuda:0')
 pt:tensor([[5.1320, 5.4181, 5.2631],
        [5.1320, 4.9532, 4.6189],
        [5.1320, 5.2697, 5.0297],
        [5.1320, 5.2122, 4.9435]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1006, step:0 
model_pd.l_p.mean(): 0.1611449420452118 
model_pd.l_d.mean(): -18.981496810913086 
model_pd.lagr.mean(): -18.82035255432129 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6075], device='cuda:0')), ('power', tensor([-19.9656], device='cuda:0'))])
epoch£º1006	 i:0 	 global-step:20120	 l-p:0.1611449420452118
epoch£º1006	 i:1 	 global-step:20121	 l-p:0.1060742437839508
epoch£º1006	 i:2 	 global-step:20122	 l-p:0.1655549257993698
epoch£º1006	 i:3 	 global-step:20123	 l-p:0.10614779591560364
epoch£º1006	 i:4 	 global-step:20124	 l-p:0.16570588946342468
epoch£º1006	 i:5 	 global-step:20125	 l-p:0.17145608365535736
epoch£º1006	 i:6 	 global-step:20126	 l-p:0.11985969543457031
epoch£º1006	 i:7 	 global-step:20127	 l-p:0.14725130796432495
epoch£º1006	 i:8 	 global-step:20128	 l-p:0.14848586916923523
epoch£º1006	 i:9 	 global-step:20129	 l-p:0.18817052245140076
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1007
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1435, 5.0874, 5.1285],
        [5.1435, 5.1428, 5.1434],
        [5.1435, 5.1434, 5.1435],
        [5.1435, 5.1213, 5.1405]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1007, step:0 
model_pd.l_p.mean(): 0.14466239511966705 
model_pd.l_d.mean(): -19.870576858520508 
model_pd.lagr.mean(): -19.725914001464844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5336], device='cuda:0')), ('power', tensor([-20.7948], device='cuda:0'))])
epoch£º1007	 i:0 	 global-step:20140	 l-p:0.14466239511966705
epoch£º1007	 i:1 	 global-step:20141	 l-p:0.2250385284423828
epoch£º1007	 i:2 	 global-step:20142	 l-p:0.12971872091293335
epoch£º1007	 i:3 	 global-step:20143	 l-p:0.11760467290878296
epoch£º1007	 i:4 	 global-step:20144	 l-p:0.14316849410533905
epoch£º1007	 i:5 	 global-step:20145	 l-p:0.11578422039747238
epoch£º1007	 i:6 	 global-step:20146	 l-p:0.16252979636192322
epoch£º1007	 i:7 	 global-step:20147	 l-p:0.11654748767614365
epoch£º1007	 i:8 	 global-step:20148	 l-p:0.13273635506629944
epoch£º1007	 i:9 	 global-step:20149	 l-p:0.14911600947380066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1008
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1487, 4.9495, 4.9820],
        [5.1487, 5.1143, 5.1423],
        [5.1487, 4.9517, 4.6233],
        [5.1487, 4.8950, 4.6027]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1008, step:0 
model_pd.l_p.mean(): 0.09044062346220016 
model_pd.l_d.mean(): -20.5087890625 
model_pd.lagr.mean(): -20.41834831237793 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4575], device='cuda:0')), ('power', tensor([-21.3661], device='cuda:0'))])
epoch£º1008	 i:0 	 global-step:20160	 l-p:0.09044062346220016
epoch£º1008	 i:1 	 global-step:20161	 l-p:0.17525774240493774
epoch£º1008	 i:2 	 global-step:20162	 l-p:0.10570164769887924
epoch£º1008	 i:3 	 global-step:20163	 l-p:0.17866681516170502
epoch£º1008	 i:4 	 global-step:20164	 l-p:0.15140922367572784
epoch£º1008	 i:5 	 global-step:20165	 l-p:0.1754264384508133
epoch£º1008	 i:6 	 global-step:20166	 l-p:0.10451696068048477
epoch£º1008	 i:7 	 global-step:20167	 l-p:0.14006634056568146
epoch£º1008	 i:8 	 global-step:20168	 l-p:0.13936756551265717
epoch£º1008	 i:9 	 global-step:20169	 l-p:0.1419782191514969
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1009
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3181e-03, 3.0678e-04,
         1.0000e+00, 4.0601e-05, 1.0000e+00, 1.3235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1640, 5.1610, 5.1638],
        [5.1640, 5.1639, 5.1640],
        [5.1640, 4.9730, 5.0109],
        [5.1640, 5.0421, 5.1014]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1009, step:0 
model_pd.l_p.mean(): 0.17637981474399567 
model_pd.l_d.mean(): -20.597061157226562 
model_pd.lagr.mean(): -20.42068099975586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4298], device='cuda:0')), ('power', tensor([-21.4273], device='cuda:0'))])
epoch£º1009	 i:0 	 global-step:20180	 l-p:0.17637981474399567
epoch£º1009	 i:1 	 global-step:20181	 l-p:0.11594872921705246
epoch£º1009	 i:2 	 global-step:20182	 l-p:0.15157504379749298
epoch£º1009	 i:3 	 global-step:20183	 l-p:0.10578060150146484
epoch£º1009	 i:4 	 global-step:20184	 l-p:0.17877104878425598
epoch£º1009	 i:5 	 global-step:20185	 l-p:0.11193297803401947
epoch£º1009	 i:6 	 global-step:20186	 l-p:0.13749073445796967
epoch£º1009	 i:7 	 global-step:20187	 l-p:0.12280282378196716
epoch£º1009	 i:8 	 global-step:20188	 l-p:0.08615198731422424
epoch£º1009	 i:9 	 global-step:20189	 l-p:0.1609131395816803
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1010
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1704, 5.0114, 4.6779],
        [5.1704, 5.1703, 5.1704],
        [5.1704, 5.1704, 5.1704],
        [5.1704, 5.1483, 5.1674]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1010, step:0 
model_pd.l_p.mean(): 0.12459471076726913 
model_pd.l_d.mean(): -19.97390365600586 
model_pd.lagr.mean(): -19.849308013916016 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4858], device='cuda:0')), ('power', tensor([-20.8506], device='cuda:0'))])
epoch£º1010	 i:0 	 global-step:20200	 l-p:0.12459471076726913
epoch£º1010	 i:1 	 global-step:20201	 l-p:0.15177032351493835
epoch£º1010	 i:2 	 global-step:20202	 l-p:0.0682215765118599
epoch£º1010	 i:3 	 global-step:20203	 l-p:0.1836344450712204
epoch£º1010	 i:4 	 global-step:20204	 l-p:0.18917682766914368
epoch£º1010	 i:5 	 global-step:20205	 l-p:0.12461618334054947
epoch£º1010	 i:6 	 global-step:20206	 l-p:0.1136816069483757
epoch£º1010	 i:7 	 global-step:20207	 l-p:0.11359260231256485
epoch£º1010	 i:8 	 global-step:20208	 l-p:0.097294881939888
epoch£º1010	 i:9 	 global-step:20209	 l-p:0.1850845068693161
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1011
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1630, 5.2316, 4.9572],
        [5.1630, 5.2617, 5.0016],
        [5.1630, 5.2517, 4.9866],
        [5.1630, 5.1625, 5.1630]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1011, step:0 
model_pd.l_p.mean(): 0.2182045727968216 
model_pd.l_d.mean(): -18.274927139282227 
model_pd.lagr.mean(): -18.05672264099121 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6145], device='cuda:0')), ('power', tensor([-19.2531], device='cuda:0'))])
epoch£º1011	 i:0 	 global-step:20220	 l-p:0.2182045727968216
epoch£º1011	 i:1 	 global-step:20221	 l-p:0.09207736700773239
epoch£º1011	 i:2 	 global-step:20222	 l-p:0.14553363621234894
epoch£º1011	 i:3 	 global-step:20223	 l-p:0.11704523116350174
epoch£º1011	 i:4 	 global-step:20224	 l-p:0.10397403687238693
epoch£º1011	 i:5 	 global-step:20225	 l-p:0.14721205830574036
epoch£º1011	 i:6 	 global-step:20226	 l-p:0.1335085779428482
epoch£º1011	 i:7 	 global-step:20227	 l-p:0.14095355570316315
epoch£º1011	 i:8 	 global-step:20228	 l-p:0.12431342899799347
epoch£º1011	 i:9 	 global-step:20229	 l-p:0.13975204527378082
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1012
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1632, 5.0069, 5.0615],
        [5.1632, 5.1585, 5.1630],
        [5.1632, 4.8786, 4.6610],
        [5.1632, 5.0460, 5.1051]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1012, step:0 
model_pd.l_p.mean(): 0.1312279850244522 
model_pd.l_d.mean(): -20.59309196472168 
model_pd.lagr.mean(): -20.461864471435547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4213], device='cuda:0')), ('power', tensor([-21.4145], device='cuda:0'))])
epoch£º1012	 i:0 	 global-step:20240	 l-p:0.1312279850244522
epoch£º1012	 i:1 	 global-step:20241	 l-p:0.14331172406673431
epoch£º1012	 i:2 	 global-step:20242	 l-p:0.1846856325864792
epoch£º1012	 i:3 	 global-step:20243	 l-p:0.18723461031913757
epoch£º1012	 i:4 	 global-step:20244	 l-p:0.12449177354574203
epoch£º1012	 i:5 	 global-step:20245	 l-p:0.11321718245744705
epoch£º1012	 i:6 	 global-step:20246	 l-p:0.13683682680130005
epoch£º1012	 i:7 	 global-step:20247	 l-p:0.14724907279014587
epoch£º1012	 i:8 	 global-step:20248	 l-p:0.13172639906406403
epoch£º1012	 i:9 	 global-step:20249	 l-p:0.08511592447757721
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1013
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1542, 5.0371, 4.7030],
        [5.1542, 5.1542, 5.1542],
        [5.1542, 4.8657, 4.7040],
        [5.1542, 4.8661, 4.6656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1013, step:0 
model_pd.l_p.mean(): 0.14827506244182587 
model_pd.l_d.mean(): -20.453832626342773 
model_pd.lagr.mean(): -20.305557250976562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4565], device='cuda:0')), ('power', tensor([-21.3091], device='cuda:0'))])
epoch£º1013	 i:0 	 global-step:20260	 l-p:0.14827506244182587
epoch£º1013	 i:1 	 global-step:20261	 l-p:0.13286948204040527
epoch£º1013	 i:2 	 global-step:20262	 l-p:0.12537723779678345
epoch£º1013	 i:3 	 global-step:20263	 l-p:0.2275777906179428
epoch£º1013	 i:4 	 global-step:20264	 l-p:0.10826137661933899
epoch£º1013	 i:5 	 global-step:20265	 l-p:0.11799244582653046
epoch£º1013	 i:6 	 global-step:20266	 l-p:0.10351825505495071
epoch£º1013	 i:7 	 global-step:20267	 l-p:0.15318189561367035
epoch£º1013	 i:8 	 global-step:20268	 l-p:0.16793940961360931
epoch£º1013	 i:9 	 global-step:20269	 l-p:0.11675314605236053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1014
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1488, 5.5803, 5.5188],
        [5.1488, 5.1484, 5.1488],
        [5.1488, 5.1488, 5.1488],
        [5.1488, 4.8596, 4.6655]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1014, step:0 
model_pd.l_p.mean(): 0.14263220131397247 
model_pd.l_d.mean(): -19.74655532836914 
model_pd.lagr.mean(): -19.603923797607422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4847], device='cuda:0')), ('power', tensor([-20.6178], device='cuda:0'))])
epoch£º1014	 i:0 	 global-step:20280	 l-p:0.14263220131397247
epoch£º1014	 i:1 	 global-step:20281	 l-p:0.20216423273086548
epoch£º1014	 i:2 	 global-step:20282	 l-p:0.11720773577690125
epoch£º1014	 i:3 	 global-step:20283	 l-p:0.09851916879415512
epoch£º1014	 i:4 	 global-step:20284	 l-p:0.127074733376503
epoch£º1014	 i:5 	 global-step:20285	 l-p:0.19998694956302643
epoch£º1014	 i:6 	 global-step:20286	 l-p:0.12351330369710922
epoch£º1014	 i:7 	 global-step:20287	 l-p:0.1997813880443573
epoch£º1014	 i:8 	 global-step:20288	 l-p:0.09798000752925873
epoch£º1014	 i:9 	 global-step:20289	 l-p:0.15843656659126282
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1015
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6447e-01, 4.6650e-01,
         1.0000e+00, 3.8554e-01, 1.0000e+00, 8.2644e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1324, 4.9707, 4.6338],
        [5.1324, 5.1263, 5.1321],
        [5.1324, 5.1304, 5.1323],
        [5.1324, 5.4967, 5.3909]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1015, step:0 
model_pd.l_p.mean(): 0.15447913110256195 
model_pd.l_d.mean(): -20.03751564025879 
model_pd.lagr.mean(): -19.88303565979004 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5131], device='cuda:0')), ('power', tensor([-20.9436], device='cuda:0'))])
epoch£º1015	 i:0 	 global-step:20300	 l-p:0.15447913110256195
epoch£º1015	 i:1 	 global-step:20301	 l-p:0.13901247084140778
epoch£º1015	 i:2 	 global-step:20302	 l-p:0.12161918729543686
epoch£º1015	 i:3 	 global-step:20303	 l-p:0.15660905838012695
epoch£º1015	 i:4 	 global-step:20304	 l-p:0.10289543122053146
epoch£º1015	 i:5 	 global-step:20305	 l-p:0.23301029205322266
epoch£º1015	 i:6 	 global-step:20306	 l-p:0.1623421013355255
epoch£º1015	 i:7 	 global-step:20307	 l-p:0.15219861268997192
epoch£º1015	 i:8 	 global-step:20308	 l-p:0.2972230017185211
epoch£º1015	 i:9 	 global-step:20309	 l-p:0.1230633482336998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1016
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1193, 5.1189, 5.1193],
        [5.1193, 5.1193, 5.1193],
        [5.1193, 5.0626, 5.1041],
        [5.1193, 5.1177, 5.1192]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1016, step:0 
model_pd.l_p.mean(): 0.14496348798274994 
model_pd.l_d.mean(): -18.937267303466797 
model_pd.lagr.mean(): -18.79230308532715 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4950], device='cuda:0')), ('power', tensor([-19.8040], device='cuda:0'))])
epoch£º1016	 i:0 	 global-step:20320	 l-p:0.14496348798274994
epoch£º1016	 i:1 	 global-step:20321	 l-p:0.25139814615249634
epoch£º1016	 i:2 	 global-step:20322	 l-p:0.10871969908475876
epoch£º1016	 i:3 	 global-step:20323	 l-p:0.14451423287391663
epoch£º1016	 i:4 	 global-step:20324	 l-p:0.10425542294979095
epoch£º1016	 i:5 	 global-step:20325	 l-p:0.10611898452043533
epoch£º1016	 i:6 	 global-step:20326	 l-p:0.2919573187828064
epoch£º1016	 i:7 	 global-step:20327	 l-p:0.1568891555070877
epoch£º1016	 i:8 	 global-step:20328	 l-p:0.2098260223865509
epoch£º1016	 i:9 	 global-step:20329	 l-p:0.10457537323236465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1017
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1238, 5.1238, 5.1238],
        [5.1238, 5.0311, 5.0867],
        [5.1238, 4.8869, 4.8823],
        [5.1238, 5.1238, 5.1238]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1017, step:0 
model_pd.l_p.mean(): 0.11615867167711258 
model_pd.l_d.mean(): -19.9873104095459 
model_pd.lagr.mean(): -19.871150970458984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5038], device='cuda:0')), ('power', tensor([-20.8828], device='cuda:0'))])
epoch£º1017	 i:0 	 global-step:20340	 l-p:0.11615867167711258
epoch£º1017	 i:1 	 global-step:20341	 l-p:0.13009023666381836
epoch£º1017	 i:2 	 global-step:20342	 l-p:0.12975738942623138
epoch£º1017	 i:3 	 global-step:20343	 l-p:0.331606924533844
epoch£º1017	 i:4 	 global-step:20344	 l-p:0.14708293974399567
epoch£º1017	 i:5 	 global-step:20345	 l-p:0.1174614280462265
epoch£º1017	 i:6 	 global-step:20346	 l-p:0.13004952669143677
epoch£º1017	 i:7 	 global-step:20347	 l-p:0.17421309649944305
epoch£º1017	 i:8 	 global-step:20348	 l-p:0.20554637908935547
epoch£º1017	 i:9 	 global-step:20349	 l-p:0.15986725687980652
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1018
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1185, 5.1138, 5.1183],
        [5.1185, 5.0354, 5.0882],
        [5.1185, 5.0827, 4.7659],
        [5.1185, 4.8381, 4.7438]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1018, step:0 
model_pd.l_p.mean(): 0.12131410092115402 
model_pd.l_d.mean(): -19.499189376831055 
model_pd.lagr.mean(): -19.37787437438965 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5096], device='cuda:0')), ('power', tensor([-20.3916], device='cuda:0'))])
epoch£º1018	 i:0 	 global-step:20360	 l-p:0.12131410092115402
epoch£º1018	 i:1 	 global-step:20361	 l-p:0.4153445065021515
epoch£º1018	 i:2 	 global-step:20362	 l-p:0.14398449659347534
epoch£º1018	 i:3 	 global-step:20363	 l-p:0.16919401288032532
epoch£º1018	 i:4 	 global-step:20364	 l-p:0.1338978409767151
epoch£º1018	 i:5 	 global-step:20365	 l-p:0.09667171537876129
epoch£º1018	 i:6 	 global-step:20366	 l-p:0.19116654992103577
epoch£º1018	 i:7 	 global-step:20367	 l-p:0.1376713663339615
epoch£º1018	 i:8 	 global-step:20368	 l-p:0.1240607425570488
epoch£º1018	 i:9 	 global-step:20369	 l-p:0.12810346484184265
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1019
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1253, 5.1144, 5.1244],
        [5.1253, 4.8558, 4.7914],
        [5.1253, 5.1423, 4.8451],
        [5.1253, 5.5198, 5.4338]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1019, step:0 
model_pd.l_p.mean(): 0.1395537108182907 
model_pd.l_d.mean(): -20.383251190185547 
model_pd.lagr.mean(): -20.243698120117188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4352], device='cuda:0')), ('power', tensor([-21.2151], device='cuda:0'))])
epoch£º1019	 i:0 	 global-step:20380	 l-p:0.1395537108182907
epoch£º1019	 i:1 	 global-step:20381	 l-p:0.19745078682899475
epoch£º1019	 i:2 	 global-step:20382	 l-p:0.19754576683044434
epoch£º1019	 i:3 	 global-step:20383	 l-p:0.13956515491008759
epoch£º1019	 i:4 	 global-step:20384	 l-p:0.13231535255908966
epoch£º1019	 i:5 	 global-step:20385	 l-p:0.2451038807630539
epoch£º1019	 i:6 	 global-step:20386	 l-p:0.12176942825317383
epoch£º1019	 i:7 	 global-step:20387	 l-p:0.17475652694702148
epoch£º1019	 i:8 	 global-step:20388	 l-p:0.08963427692651749
epoch£º1019	 i:9 	 global-step:20389	 l-p:0.11733552813529968
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1020
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1339, 4.8642, 4.5893],
        [5.1339, 5.0426, 5.0978],
        [5.1339, 4.8425, 4.6483],
        [5.1339, 4.9912, 4.6537]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1020, step:0 
model_pd.l_p.mean(): 0.1355872005224228 
model_pd.l_d.mean(): -20.31854820251465 
model_pd.lagr.mean(): -20.182960510253906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4658], device='cuda:0')), ('power', tensor([-21.1810], device='cuda:0'))])
epoch£º1020	 i:0 	 global-step:20400	 l-p:0.1355872005224228
epoch£º1020	 i:1 	 global-step:20401	 l-p:0.21762317419052124
epoch£º1020	 i:2 	 global-step:20402	 l-p:0.16680312156677246
epoch£º1020	 i:3 	 global-step:20403	 l-p:0.2466023564338684
epoch£º1020	 i:4 	 global-step:20404	 l-p:0.09994594007730484
epoch£º1020	 i:5 	 global-step:20405	 l-p:0.07167269289493561
epoch£º1020	 i:6 	 global-step:20406	 l-p:0.12521757185459137
epoch£º1020	 i:7 	 global-step:20407	 l-p:0.16641543805599213
epoch£º1020	 i:8 	 global-step:20408	 l-p:0.12619566917419434
epoch£º1020	 i:9 	 global-step:20409	 l-p:0.13049732148647308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1021
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1450, 4.8563, 4.7086],
        [5.1450, 5.1450, 5.1450],
        [5.1450, 4.8667, 4.6135],
        [5.1450, 4.8836, 4.8345]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1021, step:0 
model_pd.l_p.mean(): 0.12874092161655426 
model_pd.l_d.mean(): -20.578014373779297 
model_pd.lagr.mean(): -20.44927406311035 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4362], device='cuda:0')), ('power', tensor([-21.4145], device='cuda:0'))])
epoch£º1021	 i:0 	 global-step:20420	 l-p:0.12874092161655426
epoch£º1021	 i:1 	 global-step:20421	 l-p:0.12315724045038223
epoch£º1021	 i:2 	 global-step:20422	 l-p:0.10841628909111023
epoch£º1021	 i:3 	 global-step:20423	 l-p:0.19461338222026825
epoch£º1021	 i:4 	 global-step:20424	 l-p:0.12802566587924957
epoch£º1021	 i:5 	 global-step:20425	 l-p:0.13560791313648224
epoch£º1021	 i:6 	 global-step:20426	 l-p:0.20374102890491486
epoch£º1021	 i:7 	 global-step:20427	 l-p:0.21486830711364746
epoch£º1021	 i:8 	 global-step:20428	 l-p:0.10290317982435226
epoch£º1021	 i:9 	 global-step:20429	 l-p:0.14295344054698944
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1022
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6286e-03, 3.6277e-04,
         1.0000e+00, 5.0065e-05, 1.0000e+00, 1.3801e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8457e-01, 1.0508e-01,
         1.0000e+00, 5.9830e-02, 1.0000e+00, 5.6936e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1360, 5.1359, 5.1360],
        [5.1360, 4.8663, 4.5914],
        [5.1360, 4.9317, 4.9616],
        [5.1360, 5.1360, 5.1360]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1022, step:0 
model_pd.l_p.mean(): 0.14036236703395844 
model_pd.l_d.mean(): -20.526193618774414 
model_pd.lagr.mean(): -20.385831832885742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4333], device='cuda:0')), ('power', tensor([-21.3588], device='cuda:0'))])
epoch£º1022	 i:0 	 global-step:20440	 l-p:0.14036236703395844
epoch£º1022	 i:1 	 global-step:20441	 l-p:0.15347398817539215
epoch£º1022	 i:2 	 global-step:20442	 l-p:0.164035826921463
epoch£º1022	 i:3 	 global-step:20443	 l-p:0.13306958973407745
epoch£º1022	 i:4 	 global-step:20444	 l-p:0.11225701868534088
epoch£º1022	 i:5 	 global-step:20445	 l-p:0.10833094269037247
epoch£º1022	 i:6 	 global-step:20446	 l-p:0.18742406368255615
epoch£º1022	 i:7 	 global-step:20447	 l-p:0.14165174961090088
epoch£º1022	 i:8 	 global-step:20448	 l-p:0.25984886288642883
epoch£º1022	 i:9 	 global-step:20449	 l-p:0.11595138162374496
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1023
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1348, 5.1341, 5.1348],
        [5.1348, 4.8512, 4.6086],
        [5.1348, 5.1348, 5.1348],
        [5.1348, 4.9658, 5.0170]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1023, step:0 
model_pd.l_p.mean(): 0.14405126869678497 
model_pd.l_d.mean(): -20.89510726928711 
model_pd.lagr.mean(): -20.751056671142578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3709], device='cuda:0')), ('power', tensor([-21.6700], device='cuda:0'))])
epoch£º1023	 i:0 	 global-step:20460	 l-p:0.14405126869678497
epoch£º1023	 i:1 	 global-step:20461	 l-p:0.23232139647006989
epoch£º1023	 i:2 	 global-step:20462	 l-p:0.11837779730558395
epoch£º1023	 i:3 	 global-step:20463	 l-p:0.24808776378631592
epoch£º1023	 i:4 	 global-step:20464	 l-p:0.17293882369995117
epoch£º1023	 i:5 	 global-step:20465	 l-p:0.13084767758846283
epoch£º1023	 i:6 	 global-step:20466	 l-p:0.12166289240121841
epoch£º1023	 i:7 	 global-step:20467	 l-p:0.08502218127250671
epoch£º1023	 i:8 	 global-step:20468	 l-p:0.12162137031555176
epoch£º1023	 i:9 	 global-step:20469	 l-p:0.132216677069664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1024
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1396, 5.1396, 5.1396],
        [5.1396, 5.1084, 4.7932],
        [5.1396, 5.0831, 4.7601],
        [5.1396, 5.1396, 5.1396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1024, step:0 
model_pd.l_p.mean(): 0.1957775056362152 
model_pd.l_d.mean(): -19.560741424560547 
model_pd.lagr.mean(): -19.36496353149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5397], device='cuda:0')), ('power', tensor([-20.4855], device='cuda:0'))])
epoch£º1024	 i:0 	 global-step:20480	 l-p:0.1957775056362152
epoch£º1024	 i:1 	 global-step:20481	 l-p:0.13279403746128082
epoch£º1024	 i:2 	 global-step:20482	 l-p:0.11538340896368027
epoch£º1024	 i:3 	 global-step:20483	 l-p:0.22166936099529266
epoch£º1024	 i:4 	 global-step:20484	 l-p:0.12839308381080627
epoch£º1024	 i:5 	 global-step:20485	 l-p:0.09254389256238937
epoch£º1024	 i:6 	 global-step:20486	 l-p:0.17346222698688507
epoch£º1024	 i:7 	 global-step:20487	 l-p:0.11731728911399841
epoch£º1024	 i:8 	 global-step:20488	 l-p:0.12024740874767303
epoch£º1024	 i:9 	 global-step:20489	 l-p:0.16914470493793488
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1025
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1443, 5.1429, 5.1443],
        [5.1443, 5.1389, 5.1440],
        [5.1443, 5.1443, 5.1443],
        [5.1443, 5.1435, 5.1443]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1025, step:0 
model_pd.l_p.mean(): 0.18049079179763794 
model_pd.l_d.mean(): -20.57448959350586 
model_pd.lagr.mean(): -20.393999099731445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4136], device='cuda:0')), ('power', tensor([-21.3876], device='cuda:0'))])
epoch£º1025	 i:0 	 global-step:20500	 l-p:0.18049079179763794
epoch£º1025	 i:1 	 global-step:20501	 l-p:0.149482861161232
epoch£º1025	 i:2 	 global-step:20502	 l-p:0.133522167801857
epoch£º1025	 i:3 	 global-step:20503	 l-p:0.12819990515708923
epoch£º1025	 i:4 	 global-step:20504	 l-p:0.1650669276714325
epoch£º1025	 i:5 	 global-step:20505	 l-p:0.12085418403148651
epoch£º1025	 i:6 	 global-step:20506	 l-p:0.12506647408008575
epoch£º1025	 i:7 	 global-step:20507	 l-p:0.11959869414567947
epoch£º1025	 i:8 	 global-step:20508	 l-p:0.1665116399526596
epoch£º1025	 i:9 	 global-step:20509	 l-p:0.15522490441799164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1026
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.9596,  0.9464,  1.0000,  0.9335,
          1.0000,  0.9863, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3693,  0.2650,  1.0000,  0.1901,
          1.0000,  0.7175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7151,  0.6395,  1.0000,  0.5719,
          1.0000,  0.8943, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4406,  0.3353,  1.0000,  0.2551,
          1.0000,  0.7609, 31.6228]], device='cuda:0')
 pt:tensor([[5.1479, 5.5498, 5.4680],
        [5.1479, 4.8594, 4.6460],
        [5.1479, 5.1763, 4.8836],
        [5.1479, 4.8859, 4.6013]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1026, step:0 
model_pd.l_p.mean(): 0.07675398886203766 
model_pd.l_d.mean(): -19.782697677612305 
model_pd.lagr.mean(): -19.705944061279297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5355], device='cuda:0')), ('power', tensor([-20.7073], device='cuda:0'))])
epoch£º1026	 i:0 	 global-step:20520	 l-p:0.07675398886203766
epoch£º1026	 i:1 	 global-step:20521	 l-p:0.12943588197231293
epoch£º1026	 i:2 	 global-step:20522	 l-p:0.1612083911895752
epoch£º1026	 i:3 	 global-step:20523	 l-p:0.1919516772031784
epoch£º1026	 i:4 	 global-step:20524	 l-p:0.1561369150876999
epoch£º1026	 i:5 	 global-step:20525	 l-p:0.13946695625782013
epoch£º1026	 i:6 	 global-step:20526	 l-p:0.13749302923679352
epoch£º1026	 i:7 	 global-step:20527	 l-p:0.14749325811862946
epoch£º1026	 i:8 	 global-step:20528	 l-p:0.13879665732383728
epoch£º1026	 i:9 	 global-step:20529	 l-p:0.15109248459339142
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1027
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5086e-01, 1.5821e-01,
         1.0000e+00, 9.9781e-02, 1.0000e+00, 6.3068e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1510, 4.8605, 4.6843],
        [5.1510, 4.9863, 4.6504],
        [5.1510, 5.0607, 5.1156],
        [5.1510, 4.8893, 4.8394]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1027, step:0 
model_pd.l_p.mean(): 0.13503652811050415 
model_pd.l_d.mean(): -19.819107055664062 
model_pd.lagr.mean(): -19.684070587158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5427], device='cuda:0')), ('power', tensor([-20.7518], device='cuda:0'))])
epoch£º1027	 i:0 	 global-step:20540	 l-p:0.13503652811050415
epoch£º1027	 i:1 	 global-step:20541	 l-p:0.1347782462835312
epoch£º1027	 i:2 	 global-step:20542	 l-p:0.1504966765642166
epoch£º1027	 i:3 	 global-step:20543	 l-p:0.13817892968654633
epoch£º1027	 i:4 	 global-step:20544	 l-p:0.09700258076190948
epoch£º1027	 i:5 	 global-step:20545	 l-p:0.22883369028568268
epoch£º1027	 i:6 	 global-step:20546	 l-p:0.15739670395851135
epoch£º1027	 i:7 	 global-step:20547	 l-p:0.12393829971551895
epoch£º1027	 i:8 	 global-step:20548	 l-p:0.13150864839553833
epoch£º1027	 i:9 	 global-step:20549	 l-p:0.14319507777690887
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1028
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5898e-03, 1.2355e-03,
         1.0000e+00, 2.3163e-04, 1.0000e+00, 1.8748e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0124e-03, 1.0166e-04,
         1.0000e+00, 1.0208e-05, 1.0000e+00, 1.0041e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1443, 5.1437, 5.1443],
        [5.1443, 5.1443, 5.1444],
        [5.1443, 5.1398, 5.1441],
        [5.1443, 5.1224, 5.1414]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1028, step:0 
model_pd.l_p.mean(): 0.18003477156162262 
model_pd.l_d.mean(): -20.81313705444336 
model_pd.lagr.mean(): -20.633102416992188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3775], device='cuda:0')), ('power', tensor([-21.5933], device='cuda:0'))])
epoch£º1028	 i:0 	 global-step:20560	 l-p:0.18003477156162262
epoch£º1028	 i:1 	 global-step:20561	 l-p:0.1880079209804535
epoch£º1028	 i:2 	 global-step:20562	 l-p:0.11242420226335526
epoch£º1028	 i:3 	 global-step:20563	 l-p:0.14885461330413818
epoch£º1028	 i:4 	 global-step:20564	 l-p:0.20164988934993744
epoch£º1028	 i:5 	 global-step:20565	 l-p:0.15708453953266144
epoch£º1028	 i:6 	 global-step:20566	 l-p:0.10573523491621017
epoch£º1028	 i:7 	 global-step:20567	 l-p:0.12908132374286652
epoch£º1028	 i:8 	 global-step:20568	 l-p:0.10705211758613586
epoch£º1028	 i:9 	 global-step:20569	 l-p:0.11599614471197128
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1029
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1514, 4.8856, 4.6070],
        [5.1514, 5.1514, 5.1514],
        [5.1514, 5.1514, 5.1514],
        [5.1514, 4.9396, 4.9622]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1029, step:0 
model_pd.l_p.mean(): 0.1299794316291809 
model_pd.l_d.mean(): -20.67171287536621 
model_pd.lagr.mean(): -20.541732788085938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4018], device='cuda:0')), ('power', tensor([-21.4744], device='cuda:0'))])
epoch£º1029	 i:0 	 global-step:20580	 l-p:0.1299794316291809
epoch£º1029	 i:1 	 global-step:20581	 l-p:0.12837234139442444
epoch£º1029	 i:2 	 global-step:20582	 l-p:0.08745157718658447
epoch£º1029	 i:3 	 global-step:20583	 l-p:0.16576643288135529
epoch£º1029	 i:4 	 global-step:20584	 l-p:0.25305628776550293
epoch£º1029	 i:5 	 global-step:20585	 l-p:0.10774176567792892
epoch£º1029	 i:6 	 global-step:20586	 l-p:0.12426537275314331
epoch£º1029	 i:7 	 global-step:20587	 l-p:0.13213284313678741
epoch£º1029	 i:8 	 global-step:20588	 l-p:0.106167733669281
epoch£º1029	 i:9 	 global-step:20589	 l-p:0.21389277279376984
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1030
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1429, 5.1300, 5.1417],
        [5.1429, 5.1052, 5.1355],
        [5.1429, 5.2209, 4.9503],
        [5.1429, 4.9122, 4.9148]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1030, step:0 
model_pd.l_p.mean(): 0.157009556889534 
model_pd.l_d.mean(): -20.09419059753418 
model_pd.lagr.mean(): -19.93718147277832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4896], device='cuda:0')), ('power', tensor([-20.9770], device='cuda:0'))])
epoch£º1030	 i:0 	 global-step:20600	 l-p:0.157009556889534
epoch£º1030	 i:1 	 global-step:20601	 l-p:0.13037055730819702
epoch£º1030	 i:2 	 global-step:20602	 l-p:0.169869527220726
epoch£º1030	 i:3 	 global-step:20603	 l-p:0.10021824389696121
epoch£º1030	 i:4 	 global-step:20604	 l-p:0.1105678379535675
epoch£º1030	 i:5 	 global-step:20605	 l-p:0.1678035706281662
epoch£º1030	 i:6 	 global-step:20606	 l-p:0.12748147547245026
epoch£º1030	 i:7 	 global-step:20607	 l-p:0.2588297724723816
epoch£º1030	 i:8 	 global-step:20608	 l-p:0.08496693521738052
epoch£º1030	 i:9 	 global-step:20609	 l-p:0.16867198050022125
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1031
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1407, 5.3971, 5.2231],
        [5.1407, 4.9944, 4.6566],
        [5.1407, 4.8655, 4.7842],
        [5.1407, 5.1184, 5.1377]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1031, step:0 
model_pd.l_p.mean(): 0.1309298425912857 
model_pd.l_d.mean(): -19.769996643066406 
model_pd.lagr.mean(): -19.639066696166992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4983], device='cuda:0')), ('power', tensor([-20.6558], device='cuda:0'))])
epoch£º1031	 i:0 	 global-step:20620	 l-p:0.1309298425912857
epoch£º1031	 i:1 	 global-step:20621	 l-p:0.1664634644985199
epoch£º1031	 i:2 	 global-step:20622	 l-p:0.1449078768491745
epoch£º1031	 i:3 	 global-step:20623	 l-p:0.17974744737148285
epoch£º1031	 i:4 	 global-step:20624	 l-p:0.12519773840904236
epoch£º1031	 i:5 	 global-step:20625	 l-p:0.1317695826292038
epoch£º1031	 i:6 	 global-step:20626	 l-p:0.13268309831619263
epoch£º1031	 i:7 	 global-step:20627	 l-p:0.08907918632030487
epoch£º1031	 i:8 	 global-step:20628	 l-p:0.10451049357652664
epoch£º1031	 i:9 	 global-step:20629	 l-p:0.25038185715675354
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1032
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1464, 4.9053, 4.8935],
        [5.1464, 5.0693, 5.1200],
        [5.1464, 5.1460, 5.1464],
        [5.1464, 5.0029, 5.0612]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1032, step:0 
model_pd.l_p.mean(): 0.18557973206043243 
model_pd.l_d.mean(): -20.083295822143555 
model_pd.lagr.mean(): -19.897716522216797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4748], device='cuda:0')), ('power', tensor([-20.9506], device='cuda:0'))])
epoch£º1032	 i:0 	 global-step:20640	 l-p:0.18557973206043243
epoch£º1032	 i:1 	 global-step:20641	 l-p:0.13170751929283142
epoch£º1032	 i:2 	 global-step:20642	 l-p:0.17843161523342133
epoch£º1032	 i:3 	 global-step:20643	 l-p:0.14960093796253204
epoch£º1032	 i:4 	 global-step:20644	 l-p:0.1291719526052475
epoch£º1032	 i:5 	 global-step:20645	 l-p:0.1467117816209793
epoch£º1032	 i:6 	 global-step:20646	 l-p:0.10534457117319107
epoch£º1032	 i:7 	 global-step:20647	 l-p:0.17184577882289886
epoch£º1032	 i:8 	 global-step:20648	 l-p:0.08050661534070969
epoch£º1032	 i:9 	 global-step:20649	 l-p:0.13525359332561493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1033
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1600, 5.1600, 5.1600],
        [5.1600, 4.8708, 4.7089],
        [5.1600, 4.9409, 4.9556],
        [5.1600, 5.1588, 5.1600]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1033, step:0 
model_pd.l_p.mean(): 0.07837369292974472 
model_pd.l_d.mean(): -20.392141342163086 
model_pd.lagr.mean(): -20.31376838684082 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4605], device='cuda:0')), ('power', tensor([-21.2504], device='cuda:0'))])
epoch£º1033	 i:0 	 global-step:20660	 l-p:0.07837369292974472
epoch£º1033	 i:1 	 global-step:20661	 l-p:0.14027419686317444
epoch£º1033	 i:2 	 global-step:20662	 l-p:0.20636595785617828
epoch£º1033	 i:3 	 global-step:20663	 l-p:0.16193246841430664
epoch£º1033	 i:4 	 global-step:20664	 l-p:0.1315365433692932
epoch£º1033	 i:5 	 global-step:20665	 l-p:0.15297749638557434
epoch£º1033	 i:6 	 global-step:20666	 l-p:0.12975530326366425
epoch£º1033	 i:7 	 global-step:20667	 l-p:0.12376998364925385
epoch£º1033	 i:8 	 global-step:20668	 l-p:0.15519481897354126
epoch£º1033	 i:9 	 global-step:20669	 l-p:0.101791150867939
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1034
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8070e-02, 8.5309e-03,
         1.0000e+00, 2.5926e-03, 1.0000e+00, 3.0391e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1605, 4.9119, 4.6131],
        [5.1605, 5.1494, 5.1596],
        [5.1605, 5.1044, 5.1456],
        [5.1605, 5.1576, 5.1604]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1034, step:0 
model_pd.l_p.mean(): 0.22902348637580872 
model_pd.l_d.mean(): -20.74686050415039 
model_pd.lagr.mean(): -20.517837524414062 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3868], device='cuda:0')), ('power', tensor([-21.5354], device='cuda:0'))])
epoch£º1034	 i:0 	 global-step:20680	 l-p:0.22902348637580872
epoch£º1034	 i:1 	 global-step:20681	 l-p:0.13752852380275726
epoch£º1034	 i:2 	 global-step:20682	 l-p:0.11525119841098785
epoch£º1034	 i:3 	 global-step:20683	 l-p:0.13969239592552185
epoch£º1034	 i:4 	 global-step:20684	 l-p:0.20217427611351013
epoch£º1034	 i:5 	 global-step:20685	 l-p:0.18011531233787537
epoch£º1034	 i:6 	 global-step:20686	 l-p:0.06242798641324043
epoch£º1034	 i:7 	 global-step:20687	 l-p:0.10481975972652435
epoch£º1034	 i:8 	 global-step:20688	 l-p:0.08456160128116608
epoch£º1034	 i:9 	 global-step:20689	 l-p:0.11481421440839767
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1035
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0692e-02, 9.6095e-03,
         1.0000e+00, 3.0087e-03, 1.0000e+00, 3.1309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1690, 5.1559, 5.1678],
        [5.1690, 5.1601, 5.1684],
        [5.1690, 4.9032, 4.6286],
        [5.1690, 5.1503, 5.1668]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1035, step:0 
model_pd.l_p.mean(): 0.08649032562971115 
model_pd.l_d.mean(): -19.537099838256836 
model_pd.lagr.mean(): -19.45060920715332 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4844], device='cuda:0')), ('power', tensor([-20.4041], device='cuda:0'))])
epoch£º1035	 i:0 	 global-step:20700	 l-p:0.08649032562971115
epoch£º1035	 i:1 	 global-step:20701	 l-p:0.1499265879392624
epoch£º1035	 i:2 	 global-step:20702	 l-p:0.17374959588050842
epoch£º1035	 i:3 	 global-step:20703	 l-p:0.11648278683423996
epoch£º1035	 i:4 	 global-step:20704	 l-p:0.10006222128868103
epoch£º1035	 i:5 	 global-step:20705	 l-p:0.1393858641386032
epoch£º1035	 i:6 	 global-step:20706	 l-p:0.13981574773788452
epoch£º1035	 i:7 	 global-step:20707	 l-p:0.12567132711410522
epoch£º1035	 i:8 	 global-step:20708	 l-p:0.19697949290275574
epoch£º1035	 i:9 	 global-step:20709	 l-p:0.11649682372808456
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1036
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7700e-01, 9.6946e-01,
         1.0000e+00, 9.6197e-01, 1.0000e+00, 9.9227e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1668, 5.1429, 5.1634],
        [5.1668, 5.6029, 5.5431],
        [5.1668, 4.9736, 5.0107],
        [5.1668, 4.9911, 5.0386]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1036, step:0 
model_pd.l_p.mean(): 0.14349578320980072 
model_pd.l_d.mean(): -19.272916793823242 
model_pd.lagr.mean(): -19.12942123413086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5484], device='cuda:0')), ('power', tensor([-20.2013], device='cuda:0'))])
epoch£º1036	 i:0 	 global-step:20720	 l-p:0.14349578320980072
epoch£º1036	 i:1 	 global-step:20721	 l-p:0.15364575386047363
epoch£º1036	 i:2 	 global-step:20722	 l-p:0.16922146081924438
epoch£º1036	 i:3 	 global-step:20723	 l-p:0.10101960599422455
epoch£º1036	 i:4 	 global-step:20724	 l-p:0.11486272513866425
epoch£º1036	 i:5 	 global-step:20725	 l-p:0.08827772736549377
epoch£º1036	 i:6 	 global-step:20726	 l-p:0.1633279174566269
epoch£º1036	 i:7 	 global-step:20727	 l-p:0.15276919305324554
epoch£º1036	 i:8 	 global-step:20728	 l-p:0.13046306371688843
epoch£º1036	 i:9 	 global-step:20729	 l-p:0.15559549629688263
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1037
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6493e-01, 9.0445e-02,
         1.0000e+00, 4.9600e-02, 1.0000e+00, 5.4840e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1630, 5.1422, 5.1604],
        [5.1630, 4.9820, 5.0270],
        [5.1630, 4.8739, 4.7144],
        [5.1630, 5.1588, 5.1628]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1037, step:0 
model_pd.l_p.mean(): 0.12420011311769485 
model_pd.l_d.mean(): -18.828533172607422 
model_pd.lagr.mean(): -18.70433235168457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5491], device='cuda:0')), ('power', tensor([-19.7493], device='cuda:0'))])
epoch£º1037	 i:0 	 global-step:20740	 l-p:0.12420011311769485
epoch£º1037	 i:1 	 global-step:20741	 l-p:0.16501694917678833
epoch£º1037	 i:2 	 global-step:20742	 l-p:0.15598294138908386
epoch£º1037	 i:3 	 global-step:20743	 l-p:0.07086297869682312
epoch£º1037	 i:4 	 global-step:20744	 l-p:0.14834852516651154
epoch£º1037	 i:5 	 global-step:20745	 l-p:0.11897160857915878
epoch£º1037	 i:6 	 global-step:20746	 l-p:0.13881008327007294
epoch£º1037	 i:7 	 global-step:20747	 l-p:0.1733725666999817
epoch£º1037	 i:8 	 global-step:20748	 l-p:0.12763720750808716
epoch£º1037	 i:9 	 global-step:20749	 l-p:0.13185526430606842
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1038
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0536e-01, 5.1210e-01,
         1.0000e+00, 4.3320e-01, 1.0000e+00, 8.4594e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1718, 4.9189, 4.8847],
        [5.1718, 5.0621, 4.7283],
        [5.1718, 4.9238, 4.8985],
        [5.1718, 5.1718, 5.1718]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1038, step:0 
model_pd.l_p.mean(): 0.1391870230436325 
model_pd.l_d.mean(): -19.654155731201172 
model_pd.lagr.mean(): -19.514968872070312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4391], device='cuda:0')), ('power', tensor([-20.4764], device='cuda:0'))])
epoch£º1038	 i:0 	 global-step:20760	 l-p:0.1391870230436325
epoch£º1038	 i:1 	 global-step:20761	 l-p:0.09110074490308762
epoch£º1038	 i:2 	 global-step:20762	 l-p:0.12347283214330673
epoch£º1038	 i:3 	 global-step:20763	 l-p:0.19706784188747406
epoch£º1038	 i:4 	 global-step:20764	 l-p:0.08090981096029282
epoch£º1038	 i:5 	 global-step:20765	 l-p:0.17409048974514008
epoch£º1038	 i:6 	 global-step:20766	 l-p:0.15613561868667603
epoch£º1038	 i:7 	 global-step:20767	 l-p:0.1093059703707695
epoch£º1038	 i:8 	 global-step:20768	 l-p:0.16741858422756195
epoch£º1038	 i:9 	 global-step:20769	 l-p:0.11977925896644592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1039
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1665, 5.0754, 5.1304],
        [5.1665, 4.9995, 5.0509],
        [5.1665, 5.1117, 5.1521],
        [5.1665, 4.9718, 4.6422]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1039, step:0 
model_pd.l_p.mean(): 0.1463117152452469 
model_pd.l_d.mean(): -20.432239532470703 
model_pd.lagr.mean(): -20.28592872619629 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4628], device='cuda:0')), ('power', tensor([-21.2936], device='cuda:0'))])
epoch£º1039	 i:0 	 global-step:20780	 l-p:0.1463117152452469
epoch£º1039	 i:1 	 global-step:20781	 l-p:0.12042757123708725
epoch£º1039	 i:2 	 global-step:20782	 l-p:0.12028760462999344
epoch£º1039	 i:3 	 global-step:20783	 l-p:0.11549767106771469
epoch£º1039	 i:4 	 global-step:20784	 l-p:0.12977801263332367
epoch£º1039	 i:5 	 global-step:20785	 l-p:0.12666545808315277
epoch£º1039	 i:6 	 global-step:20786	 l-p:0.15484894812107086
epoch£º1039	 i:7 	 global-step:20787	 l-p:0.12288955599069595
epoch£º1039	 i:8 	 global-step:20788	 l-p:0.2811848819255829
epoch£º1039	 i:9 	 global-step:20789	 l-p:0.0981520265340805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1040
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5380e-05, 1.1615e-06,
         1.0000e+00, 3.8130e-08, 1.0000e+00, 3.2829e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1475, 5.1475, 5.1475],
        [5.1475, 5.1125, 5.1410],
        [5.1475, 4.8680, 4.6141],
        [5.1475, 5.1445, 5.1474]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1040, step:0 
model_pd.l_p.mean(): 0.09208100289106369 
model_pd.l_d.mean(): -19.95283317565918 
model_pd.lagr.mean(): -19.86075210571289 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5279], device='cuda:0')), ('power', tensor([-20.8727], device='cuda:0'))])
epoch£º1040	 i:0 	 global-step:20800	 l-p:0.09208100289106369
epoch£º1040	 i:1 	 global-step:20801	 l-p:0.13276831805706024
epoch£º1040	 i:2 	 global-step:20802	 l-p:0.15241298079490662
epoch£º1040	 i:3 	 global-step:20803	 l-p:0.14321403205394745
epoch£º1040	 i:4 	 global-step:20804	 l-p:0.11285769939422607
epoch£º1040	 i:5 	 global-step:20805	 l-p:0.18174441158771515
epoch£º1040	 i:6 	 global-step:20806	 l-p:0.1729225069284439
epoch£º1040	 i:7 	 global-step:20807	 l-p:0.1830383688211441
epoch£º1040	 i:8 	 global-step:20808	 l-p:0.13753247261047363
epoch£º1040	 i:9 	 global-step:20809	 l-p:0.148868590593338
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1041
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6820e-03, 2.0003e-04,
         1.0000e+00, 2.3788e-05, 1.0000e+00, 1.1892e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1473, 5.1472, 5.1473],
        [5.1473, 4.8900, 4.8515],
        [5.1473, 5.1429, 5.1471],
        [5.1473, 5.1233, 5.1439]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1041, step:0 
model_pd.l_p.mean(): 0.1185402050614357 
model_pd.l_d.mean(): -18.745969772338867 
model_pd.lagr.mean(): -18.627429962158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5557], device='cuda:0')), ('power', tensor([-19.6720], device='cuda:0'))])
epoch£º1041	 i:0 	 global-step:20820	 l-p:0.1185402050614357
epoch£º1041	 i:1 	 global-step:20821	 l-p:0.13378743827342987
epoch£º1041	 i:2 	 global-step:20822	 l-p:0.1123851090669632
epoch£º1041	 i:3 	 global-step:20823	 l-p:0.11934753507375717
epoch£º1041	 i:4 	 global-step:20824	 l-p:0.15612617135047913
epoch£º1041	 i:5 	 global-step:20825	 l-p:0.20814061164855957
epoch£º1041	 i:6 	 global-step:20826	 l-p:0.13117501139640808
epoch£º1041	 i:7 	 global-step:20827	 l-p:0.2051847279071808
epoch£º1041	 i:8 	 global-step:20828	 l-p:0.16172809898853302
epoch£º1041	 i:9 	 global-step:20829	 l-p:0.06849978119134903
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1042
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2260e-01, 4.2095e-01,
         1.0000e+00, 3.3907e-01, 1.0000e+00, 8.0548e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1589, 5.1588, 5.1589],
        [5.1589, 4.9846, 5.0332],
        [5.1589, 5.1587, 5.1589],
        [5.1589, 4.9587, 4.6296]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1042, step:0 
model_pd.l_p.mean(): 0.15009906888008118 
model_pd.l_d.mean(): -20.263460159301758 
model_pd.lagr.mean(): -20.113361358642578 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4569], device='cuda:0')), ('power', tensor([-21.1156], device='cuda:0'))])
epoch£º1042	 i:0 	 global-step:20840	 l-p:0.15009906888008118
epoch£º1042	 i:1 	 global-step:20841	 l-p:0.1205642893910408
epoch£º1042	 i:2 	 global-step:20842	 l-p:0.13343167304992676
epoch£º1042	 i:3 	 global-step:20843	 l-p:0.11754470318555832
epoch£º1042	 i:4 	 global-step:20844	 l-p:0.15147268772125244
epoch£º1042	 i:5 	 global-step:20845	 l-p:0.0971541702747345
epoch£º1042	 i:6 	 global-step:20846	 l-p:0.11410784721374512
epoch£º1042	 i:7 	 global-step:20847	 l-p:0.15741311013698578
epoch£º1042	 i:8 	 global-step:20848	 l-p:0.17820006608963013
epoch£º1042	 i:9 	 global-step:20849	 l-p:0.13456891477108002
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1043
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1767, 5.1767, 5.1767],
        [5.1767, 5.0892, 5.1432],
        [5.1767, 5.5132, 5.3875],
        [5.1767, 4.9435, 4.9404]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1043, step:0 
model_pd.l_p.mean(): 0.16874954104423523 
model_pd.l_d.mean(): -20.432453155517578 
model_pd.lagr.mean(): -20.263704299926758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4470], device='cuda:0')), ('power', tensor([-21.2775], device='cuda:0'))])
epoch£º1043	 i:0 	 global-step:20860	 l-p:0.16874954104423523
epoch£º1043	 i:1 	 global-step:20861	 l-p:0.1129847913980484
epoch£º1043	 i:2 	 global-step:20862	 l-p:0.16360263526439667
epoch£º1043	 i:3 	 global-step:20863	 l-p:0.13455131649971008
epoch£º1043	 i:4 	 global-step:20864	 l-p:0.13464578986167908
epoch£º1043	 i:5 	 global-step:20865	 l-p:0.11181017756462097
epoch£º1043	 i:6 	 global-step:20866	 l-p:0.06552520394325256
epoch£º1043	 i:7 	 global-step:20867	 l-p:0.15931376814842224
epoch£º1043	 i:8 	 global-step:20868	 l-p:0.11406736820936203
epoch£º1043	 i:9 	 global-step:20869	 l-p:0.14590325951576233
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1044
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1808, 5.1797, 5.1807],
        [5.1808, 5.1806, 5.1808],
        [5.1808, 5.1244, 5.1657],
        [5.1808, 5.0897, 4.7590]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1044, step:0 
model_pd.l_p.mean(): 0.1957189291715622 
model_pd.l_d.mean(): -20.14887237548828 
model_pd.lagr.mean(): -19.953153610229492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4578], device='cuda:0')), ('power', tensor([-20.9998], device='cuda:0'))])
epoch£º1044	 i:0 	 global-step:20880	 l-p:0.1957189291715622
epoch£º1044	 i:1 	 global-step:20881	 l-p:0.1454567164182663
epoch£º1044	 i:2 	 global-step:20882	 l-p:0.038748230785131454
epoch£º1044	 i:3 	 global-step:20883	 l-p:0.133992001414299
epoch£º1044	 i:4 	 global-step:20884	 l-p:0.12377335131168365
epoch£º1044	 i:5 	 global-step:20885	 l-p:0.13290506601333618
epoch£º1044	 i:6 	 global-step:20886	 l-p:0.17257758975028992
epoch£º1044	 i:7 	 global-step:20887	 l-p:0.11272424459457397
epoch£º1044	 i:8 	 global-step:20888	 l-p:0.10609834641218185
epoch£º1044	 i:9 	 global-step:20889	 l-p:0.1478431075811386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1045
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9026e-01, 8.5642e-01,
         1.0000e+00, 8.2387e-01, 1.0000e+00, 9.6199e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7778e-02, 4.5046e-02,
         1.0000e+00, 2.0753e-02, 1.0000e+00, 4.6070e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1768, 5.4766, 5.3277],
        [5.1768, 5.0850, 5.1402],
        [5.1768, 5.1708, 5.1765],
        [5.1768, 4.8933, 4.6651]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1045, step:0 
model_pd.l_p.mean(): 0.1505725085735321 
model_pd.l_d.mean(): -18.876691818237305 
model_pd.lagr.mean(): -18.726119995117188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5317], device='cuda:0')), ('power', tensor([-19.7803], device='cuda:0'))])
epoch£º1045	 i:0 	 global-step:20900	 l-p:0.1505725085735321
epoch£º1045	 i:1 	 global-step:20901	 l-p:0.16943566501140594
epoch£º1045	 i:2 	 global-step:20902	 l-p:0.11928945034742355
epoch£º1045	 i:3 	 global-step:20903	 l-p:0.19834908843040466
epoch£º1045	 i:4 	 global-step:20904	 l-p:0.11324671655893326
epoch£º1045	 i:5 	 global-step:20905	 l-p:0.1443091481924057
epoch£º1045	 i:6 	 global-step:20906	 l-p:0.08334854245185852
epoch£º1045	 i:7 	 global-step:20907	 l-p:0.1379803717136383
epoch£º1045	 i:8 	 global-step:20908	 l-p:0.09079449623823166
epoch£º1045	 i:9 	 global-step:20909	 l-p:0.1123683750629425
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1046
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9134e-01, 1.9314e-01,
         1.0000e+00, 1.2804e-01, 1.0000e+00, 6.6293e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1788, 4.8984, 4.7899],
        [5.1788, 5.1781, 5.1788],
        [5.1788, 5.0051, 4.6715],
        [5.1788, 5.1788, 5.1788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1046, step:0 
model_pd.l_p.mean(): 0.10049222409725189 
model_pd.l_d.mean(): -20.17995262145996 
model_pd.lagr.mean(): -20.07946014404297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4642], device='cuda:0')), ('power', tensor([-21.0381], device='cuda:0'))])
epoch£º1046	 i:0 	 global-step:20920	 l-p:0.10049222409725189
epoch£º1046	 i:1 	 global-step:20921	 l-p:0.11486872285604477
epoch£º1046	 i:2 	 global-step:20922	 l-p:0.08993636816740036
epoch£º1046	 i:3 	 global-step:20923	 l-p:0.15916745364665985
epoch£º1046	 i:4 	 global-step:20924	 l-p:0.1238398551940918
epoch£º1046	 i:5 	 global-step:20925	 l-p:0.11512483656406403
epoch£º1046	 i:6 	 global-step:20926	 l-p:0.13960719108581543
epoch£º1046	 i:7 	 global-step:20927	 l-p:0.1536117047071457
epoch£º1046	 i:8 	 global-step:20928	 l-p:0.2083798050880432
epoch£º1046	 i:9 	 global-step:20929	 l-p:0.12910673022270203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1047
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1689, 5.0201, 5.0772],
        [5.1689, 5.1421, 5.1648],
        [5.1689, 4.8796, 4.7199],
        [5.1689, 5.0993, 5.1469]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1047, step:0 
model_pd.l_p.mean(): 0.10822036862373352 
model_pd.l_d.mean(): -19.81675148010254 
model_pd.lagr.mean(): -19.70853042602539 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4641], device='cuda:0')), ('power', tensor([-20.6680], device='cuda:0'))])
epoch£º1047	 i:0 	 global-step:20940	 l-p:0.10822036862373352
epoch£º1047	 i:1 	 global-step:20941	 l-p:0.19978511333465576
epoch£º1047	 i:2 	 global-step:20942	 l-p:0.12979045510292053
epoch£º1047	 i:3 	 global-step:20943	 l-p:0.14153027534484863
epoch£º1047	 i:4 	 global-step:20944	 l-p:0.11246447265148163
epoch£º1047	 i:5 	 global-step:20945	 l-p:0.2019626498222351
epoch£º1047	 i:6 	 global-step:20946	 l-p:0.11643275618553162
epoch£º1047	 i:7 	 global-step:20947	 l-p:0.14454898238182068
epoch£º1047	 i:8 	 global-step:20948	 l-p:0.09861116856336594
epoch£º1047	 i:9 	 global-step:20949	 l-p:0.11210200935602188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1048
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9430e-01, 7.3560e-01,
         1.0000e+00, 6.8124e-01, 1.0000e+00, 9.2611e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1642, 5.3106, 5.0734],
        [5.1642, 4.9823, 5.0271],
        [5.1642, 5.1642, 5.1642],
        [5.1642, 4.8915, 4.6255]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1048, step:0 
model_pd.l_p.mean(): 0.08531463146209717 
model_pd.l_d.mean(): -19.72512435913086 
model_pd.lagr.mean(): -19.63981056213379 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5203], device='cuda:0')), ('power', tensor([-20.6329], device='cuda:0'))])
epoch£º1048	 i:0 	 global-step:20960	 l-p:0.08531463146209717
epoch£º1048	 i:1 	 global-step:20961	 l-p:0.0822591558098793
epoch£º1048	 i:2 	 global-step:20962	 l-p:0.14661583304405212
epoch£º1048	 i:3 	 global-step:20963	 l-p:0.13125215470790863
epoch£º1048	 i:4 	 global-step:20964	 l-p:0.1201174259185791
epoch£º1048	 i:5 	 global-step:20965	 l-p:0.23048703372478485
epoch£º1048	 i:6 	 global-step:20966	 l-p:0.11336654424667358
epoch£º1048	 i:7 	 global-step:20967	 l-p:0.1980385184288025
epoch£º1048	 i:8 	 global-step:20968	 l-p:0.12529003620147705
epoch£º1048	 i:9 	 global-step:20969	 l-p:0.1576424390077591
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1049
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1563, 5.0801, 5.1304],
        [5.1563, 4.9751, 4.6404],
        [5.1563, 5.5340, 5.4351],
        [5.1563, 4.9621, 4.9996]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1049, step:0 
model_pd.l_p.mean(): 0.12838229537010193 
model_pd.l_d.mean(): -19.321855545043945 
model_pd.lagr.mean(): -19.19347381591797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5201], device='cuda:0')), ('power', tensor([-20.2218], device='cuda:0'))])
epoch£º1049	 i:0 	 global-step:20980	 l-p:0.12838229537010193
epoch£º1049	 i:1 	 global-step:20981	 l-p:0.11692338436841965
epoch£º1049	 i:2 	 global-step:20982	 l-p:0.1283525973558426
epoch£º1049	 i:3 	 global-step:20983	 l-p:0.15729384124279022
epoch£º1049	 i:4 	 global-step:20984	 l-p:0.18326938152313232
epoch£º1049	 i:5 	 global-step:20985	 l-p:0.10771068185567856
epoch£º1049	 i:6 	 global-step:20986	 l-p:0.15020184218883514
epoch£º1049	 i:7 	 global-step:20987	 l-p:0.1551455855369568
epoch£º1049	 i:8 	 global-step:20988	 l-p:0.1329171508550644
epoch£º1049	 i:9 	 global-step:20989	 l-p:0.14144381880760193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1050
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1576, 4.8827, 4.8009],
        [5.1576, 5.1117, 5.1472],
        [5.1576, 5.1562, 5.1576],
        [5.1576, 5.1198, 5.1502]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1050, step:0 
model_pd.l_p.mean(): 0.132292702794075 
model_pd.l_d.mean(): -20.711843490600586 
model_pd.lagr.mean(): -20.579551696777344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4140], device='cuda:0')), ('power', tensor([-21.5279], device='cuda:0'))])
epoch£º1050	 i:0 	 global-step:21000	 l-p:0.132292702794075
epoch£º1050	 i:1 	 global-step:21001	 l-p:0.11067162454128265
epoch£º1050	 i:2 	 global-step:21002	 l-p:0.11316711455583572
epoch£º1050	 i:3 	 global-step:21003	 l-p:0.14714469015598297
epoch£º1050	 i:4 	 global-step:21004	 l-p:0.1487857550382614
epoch£º1050	 i:5 	 global-step:21005	 l-p:0.11525489389896393
epoch£º1050	 i:6 	 global-step:21006	 l-p:0.22487208247184753
epoch£º1050	 i:7 	 global-step:21007	 l-p:0.1567184180021286
epoch£º1050	 i:8 	 global-step:21008	 l-p:0.12063039839267731
epoch£º1050	 i:9 	 global-step:21009	 l-p:0.1763235181570053
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1051
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3191e-03, 1.6857e-03,
         1.0000e+00, 3.4156e-04, 1.0000e+00, 2.0262e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1453, 4.8525, 4.6763],
        [5.1453, 5.1442, 5.1453],
        [5.1453, 4.8526, 4.6613],
        [5.1453, 4.8565, 4.7233]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1051, step:0 
model_pd.l_p.mean(): 0.15623053908348083 
model_pd.l_d.mean(): -18.9094181060791 
model_pd.lagr.mean(): -18.75318717956543 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5560], device='cuda:0')), ('power', tensor([-19.8389], device='cuda:0'))])
epoch£º1051	 i:0 	 global-step:21020	 l-p:0.15623053908348083
epoch£º1051	 i:1 	 global-step:21021	 l-p:0.15658560395240784
epoch£º1051	 i:2 	 global-step:21022	 l-p:0.20608828961849213
epoch£º1051	 i:3 	 global-step:21023	 l-p:0.10650960355997086
epoch£º1051	 i:4 	 global-step:21024	 l-p:0.14788319170475006
epoch£º1051	 i:5 	 global-step:21025	 l-p:0.09063955396413803
epoch£º1051	 i:6 	 global-step:21026	 l-p:0.12628553807735443
epoch£º1051	 i:7 	 global-step:21027	 l-p:0.12368148565292358
epoch£º1051	 i:8 	 global-step:21028	 l-p:0.14408360421657562
epoch£º1051	 i:9 	 global-step:21029	 l-p:0.17762115597724915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1052
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1563, 5.1563, 5.1563],
        [5.1563, 5.1563, 5.1563],
        [5.1563, 5.0160, 5.0749],
        [5.1563, 4.9065, 4.8815]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1052, step:0 
model_pd.l_p.mean(): 0.09834443032741547 
model_pd.l_d.mean(): -20.511768341064453 
model_pd.lagr.mean(): -20.413423538208008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4291], device='cuda:0')), ('power', tensor([-21.3398], device='cuda:0'))])
epoch£º1052	 i:0 	 global-step:21040	 l-p:0.09834443032741547
epoch£º1052	 i:1 	 global-step:21041	 l-p:0.13977931439876556
epoch£º1052	 i:2 	 global-step:21042	 l-p:0.13906340301036835
epoch£º1052	 i:3 	 global-step:21043	 l-p:0.13362355530261993
epoch£º1052	 i:4 	 global-step:21044	 l-p:0.1902100294828415
epoch£º1052	 i:5 	 global-step:21045	 l-p:0.0982813686132431
epoch£º1052	 i:6 	 global-step:21046	 l-p:0.0822940245270729
epoch£º1052	 i:7 	 global-step:21047	 l-p:0.22197097539901733
epoch£º1052	 i:8 	 global-step:21048	 l-p:0.12990310788154602
epoch£º1052	 i:9 	 global-step:21049	 l-p:0.13188636302947998
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1053
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4903,  0.3866,  1.0000,  0.3049,
          1.0000,  0.7885, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8796,  0.8428,  1.0000,  0.8075,
          1.0000,  0.9581, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1845,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5693, 31.6228]], device='cuda:0')
 pt:tensor([[5.1742, 4.9480, 4.6315],
        [5.1742, 5.4553, 5.2947],
        [5.1742, 4.9127, 4.8625],
        [5.1742, 4.9705, 5.0000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1053, step:0 
model_pd.l_p.mean(): 0.1554604470729828 
model_pd.l_d.mean(): -20.371782302856445 
model_pd.lagr.mean(): -20.21632194519043 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4575], device='cuda:0')), ('power', tensor([-21.2265], device='cuda:0'))])
epoch£º1053	 i:0 	 global-step:21060	 l-p:0.1554604470729828
epoch£º1053	 i:1 	 global-step:21061	 l-p:0.11605232954025269
epoch£º1053	 i:2 	 global-step:21062	 l-p:0.1667841374874115
epoch£º1053	 i:3 	 global-step:21063	 l-p:0.132433220744133
epoch£º1053	 i:4 	 global-step:21064	 l-p:0.1048876941204071
epoch£º1053	 i:5 	 global-step:21065	 l-p:0.10932248830795288
epoch£º1053	 i:6 	 global-step:21066	 l-p:0.11345863342285156
epoch£º1053	 i:7 	 global-step:21067	 l-p:0.14354771375656128
epoch£º1053	 i:8 	 global-step:21068	 l-p:0.16311001777648926
epoch£º1053	 i:9 	 global-step:21069	 l-p:0.11899835616350174
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1054
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1812, 5.1782, 5.1811],
        [5.1812, 5.1813, 5.1813],
        [5.1812, 5.1809, 5.1812],
        [5.1812, 5.1770, 5.1810]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1054, step:0 
model_pd.l_p.mean(): 0.11911264061927795 
model_pd.l_d.mean(): -17.5572452545166 
model_pd.lagr.mean(): -17.438133239746094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6141], device='cuda:0')), ('power', tensor([-18.5216], device='cuda:0'))])
epoch£º1054	 i:0 	 global-step:21080	 l-p:0.11911264061927795
epoch£º1054	 i:1 	 global-step:21081	 l-p:0.11968490481376648
epoch£º1054	 i:2 	 global-step:21082	 l-p:0.13965541124343872
epoch£º1054	 i:3 	 global-step:21083	 l-p:0.11822523921728134
epoch£º1054	 i:4 	 global-step:21084	 l-p:0.1855359673500061
epoch£º1054	 i:5 	 global-step:21085	 l-p:0.08884528279304504
epoch£º1054	 i:6 	 global-step:21086	 l-p:0.1388891041278839
epoch£º1054	 i:7 	 global-step:21087	 l-p:0.13893239200115204
epoch£º1054	 i:8 	 global-step:21088	 l-p:0.16609051823616028
epoch£º1054	 i:9 	 global-step:21089	 l-p:0.0896402895450592
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1055
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8181e-01, 2.7699e-01,
         1.0000e+00, 2.0095e-01, 1.0000e+00, 7.2547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1843, 5.1783, 5.1839],
        [5.1843, 5.1369, 5.1732],
        [5.1843, 4.9007, 4.6722],
        [5.1843, 5.0159, 5.0667]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1055, step:0 
model_pd.l_p.mean(): 0.16882804036140442 
model_pd.l_d.mean(): -19.324560165405273 
model_pd.lagr.mean(): -19.155731201171875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5797], device='cuda:0')), ('power', tensor([-20.2863], device='cuda:0'))])
epoch£º1055	 i:0 	 global-step:21100	 l-p:0.16882804036140442
epoch£º1055	 i:1 	 global-step:21101	 l-p:0.05771901458501816
epoch£º1055	 i:2 	 global-step:21102	 l-p:0.1382683664560318
epoch£º1055	 i:3 	 global-step:21103	 l-p:0.15986956655979156
epoch£º1055	 i:4 	 global-step:21104	 l-p:0.14386537671089172
epoch£º1055	 i:5 	 global-step:21105	 l-p:0.13569508492946625
epoch£º1055	 i:6 	 global-step:21106	 l-p:0.1112261712551117
epoch£º1055	 i:7 	 global-step:21107	 l-p:0.07988084852695465
epoch£º1055	 i:8 	 global-step:21108	 l-p:0.15741130709648132
epoch£º1055	 i:9 	 global-step:21109	 l-p:0.13162170350551605
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1056
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1959, 5.1959, 5.1959],
        [5.1959, 5.1954, 5.1959],
        [5.1959, 5.1883, 5.1954],
        [5.1959, 5.1959, 5.1959]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1056, step:0 
model_pd.l_p.mean(): 0.17734234035015106 
model_pd.l_d.mean(): -20.831398010253906 
model_pd.lagr.mean(): -20.654056549072266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3648], device='cuda:0')), ('power', tensor([-21.5988], device='cuda:0'))])
epoch£º1056	 i:0 	 global-step:21120	 l-p:0.17734234035015106
epoch£º1056	 i:1 	 global-step:21121	 l-p:0.1225598081946373
epoch£º1056	 i:2 	 global-step:21122	 l-p:0.16077882051467896
epoch£º1056	 i:3 	 global-step:21123	 l-p:0.09569280594587326
epoch£º1056	 i:4 	 global-step:21124	 l-p:0.12127097696065903
epoch£º1056	 i:5 	 global-step:21125	 l-p:0.07683736830949783
epoch£º1056	 i:6 	 global-step:21126	 l-p:0.10622870922088623
epoch£º1056	 i:7 	 global-step:21127	 l-p:0.15931235253810883
epoch£º1056	 i:8 	 global-step:21128	 l-p:0.09374696761369705
epoch£º1056	 i:9 	 global-step:21129	 l-p:0.1561923772096634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1057
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1900, 5.1276, 5.1719],
        [5.1900, 5.2620, 4.9872],
        [5.1900, 5.1900, 5.1900],
        [5.1900, 5.0990, 5.1540]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1057, step:0 
model_pd.l_p.mean(): 0.16431021690368652 
model_pd.l_d.mean(): -20.345508575439453 
model_pd.lagr.mean(): -20.181198120117188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4430], device='cuda:0')), ('power', tensor([-21.1848], device='cuda:0'))])
epoch£º1057	 i:0 	 global-step:21140	 l-p:0.16431021690368652
epoch£º1057	 i:1 	 global-step:21141	 l-p:0.09208820015192032
epoch£º1057	 i:2 	 global-step:21142	 l-p:0.11378957331180573
epoch£º1057	 i:3 	 global-step:21143	 l-p:0.09924321621656418
epoch£º1057	 i:4 	 global-step:21144	 l-p:0.12967370450496674
epoch£º1057	 i:5 	 global-step:21145	 l-p:0.12755317986011505
epoch£º1057	 i:6 	 global-step:21146	 l-p:0.11321491003036499
epoch£º1057	 i:7 	 global-step:21147	 l-p:0.16096027195453644
epoch£º1057	 i:8 	 global-step:21148	 l-p:0.15076158940792084
epoch£º1057	 i:9 	 global-step:21149	 l-p:0.12885534763336182
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1058
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5632e-01, 1.6282e-01,
         1.0000e+00, 1.0343e-01, 1.0000e+00, 6.3523e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1188e-02, 2.9504e-02,
         1.0000e+00, 1.2228e-02, 1.0000e+00, 4.1445e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1879, 5.1691, 5.1856],
        [5.1879, 4.9544, 4.9512],
        [5.1879, 4.9236, 4.8655],
        [5.1879, 5.1314, 5.1728]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1058, step:0 
model_pd.l_p.mean(): 0.13392509520053864 
model_pd.l_d.mean(): -20.602230072021484 
model_pd.lagr.mean(): -20.468305587768555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4057], device='cuda:0')), ('power', tensor([-21.4076], device='cuda:0'))])
epoch£º1058	 i:0 	 global-step:21160	 l-p:0.13392509520053864
epoch£º1058	 i:1 	 global-step:21161	 l-p:0.1084594577550888
epoch£º1058	 i:2 	 global-step:21162	 l-p:0.11149872839450836
epoch£º1058	 i:3 	 global-step:21163	 l-p:0.1272217482328415
epoch£º1058	 i:4 	 global-step:21164	 l-p:0.1643812209367752
epoch£º1058	 i:5 	 global-step:21165	 l-p:0.17612536251544952
epoch£º1058	 i:6 	 global-step:21166	 l-p:0.1503135710954666
epoch£º1058	 i:7 	 global-step:21167	 l-p:0.06764714419841766
epoch£º1058	 i:8 	 global-step:21168	 l-p:0.12557286024093628
epoch£º1058	 i:9 	 global-step:21169	 l-p:0.18881940841674805
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1059
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5541e-02, 3.8784e-03,
         1.0000e+00, 9.6785e-04, 1.0000e+00, 2.4955e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1603, 5.1566, 5.1602],
        [5.1603, 4.9673, 5.0058],
        [5.1603, 5.1415, 5.1581],
        [5.1603, 5.1380, 5.1573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1059, step:0 
model_pd.l_p.mean(): 0.12380257248878479 
model_pd.l_d.mean(): -20.42426872253418 
model_pd.lagr.mean(): -20.300466537475586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4422], device='cuda:0')), ('power', tensor([-21.2642], device='cuda:0'))])
epoch£º1059	 i:0 	 global-step:21180	 l-p:0.12380257248878479
epoch£º1059	 i:1 	 global-step:21181	 l-p:0.11024802178144455
epoch£º1059	 i:2 	 global-step:21182	 l-p:0.09681452810764313
epoch£º1059	 i:3 	 global-step:21183	 l-p:0.1806914359331131
epoch£º1059	 i:4 	 global-step:21184	 l-p:0.26699209213256836
epoch£º1059	 i:5 	 global-step:21185	 l-p:0.14232243597507477
epoch£º1059	 i:6 	 global-step:21186	 l-p:0.14452868700027466
epoch£º1059	 i:7 	 global-step:21187	 l-p:0.1164376437664032
epoch£º1059	 i:8 	 global-step:21188	 l-p:0.13051755726337433
epoch£º1059	 i:9 	 global-step:21189	 l-p:0.12705199420452118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1060
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0572e-01, 3.0036e-01,
         1.0000e+00, 2.2235e-01, 1.0000e+00, 7.4030e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1415, 5.1384, 5.1414],
        [5.1415, 4.8591, 4.6043],
        [5.1415, 4.9376, 4.6057],
        [5.1415, 5.1410, 5.1415]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1060, step:0 
model_pd.l_p.mean(): 0.13336391746997833 
model_pd.l_d.mean(): -19.770166397094727 
model_pd.lagr.mean(): -19.636802673339844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4522], device='cuda:0')), ('power', tensor([-20.6082], device='cuda:0'))])
epoch£º1060	 i:0 	 global-step:21200	 l-p:0.13336391746997833
epoch£º1060	 i:1 	 global-step:21201	 l-p:0.17687132954597473
epoch£º1060	 i:2 	 global-step:21202	 l-p:0.128313809633255
epoch£º1060	 i:3 	 global-step:21203	 l-p:0.16453799605369568
epoch£º1060	 i:4 	 global-step:21204	 l-p:0.1142791137099266
epoch£º1060	 i:5 	 global-step:21205	 l-p:0.13670381903648376
epoch£º1060	 i:6 	 global-step:21206	 l-p:0.14149728417396545
epoch£º1060	 i:7 	 global-step:21207	 l-p:0.19043166935443878
epoch£º1060	 i:8 	 global-step:21208	 l-p:0.10833968222141266
epoch£º1060	 i:9 	 global-step:21209	 l-p:0.2461959570646286
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1061
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1326, 5.1326, 5.1326],
        [5.1326, 4.9369, 4.9750],
        [5.1326, 5.4921, 5.3815],
        [5.1326, 4.8548, 4.7734]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1061, step:0 
model_pd.l_p.mean(): 0.12765245139598846 
model_pd.l_d.mean(): -19.555429458618164 
model_pd.lagr.mean(): -19.427776336669922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5281], device='cuda:0')), ('power', tensor([-20.4681], device='cuda:0'))])
epoch£º1061	 i:0 	 global-step:21220	 l-p:0.12765245139598846
epoch£º1061	 i:1 	 global-step:21221	 l-p:0.15804548561573029
epoch£º1061	 i:2 	 global-step:21222	 l-p:0.20799651741981506
epoch£º1061	 i:3 	 global-step:21223	 l-p:0.20334522426128387
epoch£º1061	 i:4 	 global-step:21224	 l-p:0.09430788457393646
epoch£º1061	 i:5 	 global-step:21225	 l-p:0.13764935731887817
epoch£º1061	 i:6 	 global-step:21226	 l-p:0.13383333384990692
epoch£º1061	 i:7 	 global-step:21227	 l-p:0.12514543533325195
epoch£º1061	 i:8 	 global-step:21228	 l-p:0.18052451312541962
epoch£º1061	 i:9 	 global-step:21229	 l-p:0.16822229325771332
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1062
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.2564e-02, 2.4837e-02,
         1.0000e+00, 9.8600e-03, 1.0000e+00, 3.9699e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1404, 5.1399, 5.1404],
        [5.1404, 5.0943, 5.1299],
        [5.1404, 4.9627, 5.0110],
        [5.1404, 4.9958, 5.0546]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1062, step:0 
model_pd.l_p.mean(): 0.09869521856307983 
model_pd.l_d.mean(): -20.065744400024414 
model_pd.lagr.mean(): -19.96704864501953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4917], device='cuda:0')), ('power', tensor([-20.9502], device='cuda:0'))])
epoch£º1062	 i:0 	 global-step:21240	 l-p:0.09869521856307983
epoch£º1062	 i:1 	 global-step:21241	 l-p:0.13568006455898285
epoch£º1062	 i:2 	 global-step:21242	 l-p:0.13218002021312714
epoch£º1062	 i:3 	 global-step:21243	 l-p:0.20920144021511078
epoch£º1062	 i:4 	 global-step:21244	 l-p:0.11367199569940567
epoch£º1062	 i:5 	 global-step:21245	 l-p:0.11544675379991531
epoch£º1062	 i:6 	 global-step:21246	 l-p:0.1392371505498886
epoch£º1062	 i:7 	 global-step:21247	 l-p:0.12301306426525116
epoch£º1062	 i:8 	 global-step:21248	 l-p:0.12112613767385483
epoch£º1062	 i:9 	 global-step:21249	 l-p:0.3021896183490753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1063
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3626e-03, 7.1284e-04,
         1.0000e+00, 1.1648e-04, 1.0000e+00, 1.6340e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4275e-01, 1.5143e-01,
         1.0000e+00, 9.4465e-02, 1.0000e+00, 6.2381e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8872e-06, 1.0630e-07,
         1.0000e+00, 1.9195e-09, 1.0000e+00, 1.8057e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1403, 5.1400, 5.1403],
        [5.1403, 4.8812, 4.8428],
        [5.1403, 5.1403, 5.1403],
        [5.1403, 4.8588, 4.7639]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1063, step:0 
model_pd.l_p.mean(): 0.14051280915737152 
model_pd.l_d.mean(): -20.487966537475586 
model_pd.lagr.mean(): -20.347454071044922 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4292], device='cuda:0')), ('power', tensor([-21.3156], device='cuda:0'))])
epoch£º1063	 i:0 	 global-step:21260	 l-p:0.14051280915737152
epoch£º1063	 i:1 	 global-step:21261	 l-p:0.061235230416059494
epoch£º1063	 i:2 	 global-step:21262	 l-p:0.16639083623886108
epoch£º1063	 i:3 	 global-step:21263	 l-p:0.14539001882076263
epoch£º1063	 i:4 	 global-step:21264	 l-p:0.13001571595668793
epoch£º1063	 i:5 	 global-step:21265	 l-p:0.20444780588150024
epoch£º1063	 i:6 	 global-step:21266	 l-p:0.1640481948852539
epoch£º1063	 i:7 	 global-step:21267	 l-p:0.23166747391223907
epoch£º1063	 i:8 	 global-step:21268	 l-p:0.1179567277431488
epoch£º1063	 i:9 	 global-step:21269	 l-p:0.16463899612426758
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1064
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1361, 5.1359, 5.1362],
        [5.1361, 4.8417, 4.6767],
        [5.1361, 4.9132, 4.9276],
        [5.1361, 5.0584, 5.1095]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1064, step:0 
model_pd.l_p.mean(): 0.29509982466697693 
model_pd.l_d.mean(): -20.444711685180664 
model_pd.lagr.mean(): -20.149612426757812 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4320], device='cuda:0')), ('power', tensor([-21.2744], device='cuda:0'))])
epoch£º1064	 i:0 	 global-step:21280	 l-p:0.29509982466697693
epoch£º1064	 i:1 	 global-step:21281	 l-p:0.15952743589878082
epoch£º1064	 i:2 	 global-step:21282	 l-p:0.16513340175151825
epoch£º1064	 i:3 	 global-step:21283	 l-p:0.1301242709159851
epoch£º1064	 i:4 	 global-step:21284	 l-p:0.13370449841022491
epoch£º1064	 i:5 	 global-step:21285	 l-p:0.12288270890712738
epoch£º1064	 i:6 	 global-step:21286	 l-p:0.09368760138750076
epoch£º1064	 i:7 	 global-step:21287	 l-p:0.11733798682689667
epoch£º1064	 i:8 	 global-step:21288	 l-p:0.16138577461242676
epoch£º1064	 i:9 	 global-step:21289	 l-p:0.13050967454910278
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1065
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1427, 5.1427, 5.1427],
        [5.1427, 5.1427, 5.1427],
        [5.1427, 5.0170, 4.6781],
        [5.1427, 5.1420, 5.1427]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1065, step:0 
model_pd.l_p.mean(): 0.1353708952665329 
model_pd.l_d.mean(): -19.71108055114746 
model_pd.lagr.mean(): -19.57571029663086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4969], device='cuda:0')), ('power', tensor([-20.5943], device='cuda:0'))])
epoch£º1065	 i:0 	 global-step:21300	 l-p:0.1353708952665329
epoch£º1065	 i:1 	 global-step:21301	 l-p:0.24593989551067352
epoch£º1065	 i:2 	 global-step:21302	 l-p:0.1464134305715561
epoch£º1065	 i:3 	 global-step:21303	 l-p:0.14126820862293243
epoch£º1065	 i:4 	 global-step:21304	 l-p:0.22935999929904938
epoch£º1065	 i:5 	 global-step:21305	 l-p:0.11641188710927963
epoch£º1065	 i:6 	 global-step:21306	 l-p:0.1256498545408249
epoch£º1065	 i:7 	 global-step:21307	 l-p:0.061372365802526474
epoch£º1065	 i:8 	 global-step:21308	 l-p:0.12275613844394684
epoch£º1065	 i:9 	 global-step:21309	 l-p:0.1576133370399475
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1066
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3875e-01, 9.1917e-01,
         1.0000e+00, 9.0001e-01, 1.0000e+00, 9.7915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1416, 5.5039, 5.3947],
        [5.1416, 4.9058, 4.5887],
        [5.1416, 5.0269, 5.0867],
        [5.1416, 4.8924, 4.5859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1066, step:0 
model_pd.l_p.mean(): 0.2170993685722351 
model_pd.l_d.mean(): -20.364273071289062 
model_pd.lagr.mean(): -20.147172927856445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4464], device='cuda:0')), ('power', tensor([-21.2074], device='cuda:0'))])
epoch£º1066	 i:0 	 global-step:21320	 l-p:0.2170993685722351
epoch£º1066	 i:1 	 global-step:21321	 l-p:0.1157829612493515
epoch£º1066	 i:2 	 global-step:21322	 l-p:0.12574733793735504
epoch£º1066	 i:3 	 global-step:21323	 l-p:0.19634945690631866
epoch£º1066	 i:4 	 global-step:21324	 l-p:0.17120732367038727
epoch£º1066	 i:5 	 global-step:21325	 l-p:0.13417334854602814
epoch£º1066	 i:6 	 global-step:21326	 l-p:0.09628227353096008
epoch£º1066	 i:7 	 global-step:21327	 l-p:0.12820962071418762
epoch£º1066	 i:8 	 global-step:21328	 l-p:0.16058845818042755
epoch£º1066	 i:9 	 global-step:21329	 l-p:0.13515706360340118
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1067
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1321e-01, 8.8598e-01,
         1.0000e+00, 8.5957e-01, 1.0000e+00, 9.7019e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1399, 5.1399, 5.1399],
        [5.1399, 5.1386, 5.1398],
        [5.1399, 5.1346, 5.1396],
        [5.1399, 5.4609, 5.3253]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1067, step:0 
model_pd.l_p.mean(): 0.17837516963481903 
model_pd.l_d.mean(): -20.323055267333984 
model_pd.lagr.mean(): -20.14468002319336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4675], device='cuda:0')), ('power', tensor([-21.1873], device='cuda:0'))])
epoch£º1067	 i:0 	 global-step:21340	 l-p:0.17837516963481903
epoch£º1067	 i:1 	 global-step:21341	 l-p:0.13488353788852692
epoch£º1067	 i:2 	 global-step:21342	 l-p:0.17061655223369598
epoch£º1067	 i:3 	 global-step:21343	 l-p:0.08518879860639572
epoch£º1067	 i:4 	 global-step:21344	 l-p:0.14825816452503204
epoch£º1067	 i:5 	 global-step:21345	 l-p:0.16737164556980133
epoch£º1067	 i:6 	 global-step:21346	 l-p:0.16139265894889832
epoch£º1067	 i:7 	 global-step:21347	 l-p:0.16107341647148132
epoch£º1067	 i:8 	 global-step:21348	 l-p:0.11159656196832657
epoch£º1067	 i:9 	 global-step:21349	 l-p:0.16676680743694305
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1068
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1829e-06, 2.8316e-08,
         1.0000e+00, 3.6732e-10, 1.0000e+00, 1.2972e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7406e-03, 1.0279e-03,
         1.0000e+00, 1.8405e-04, 1.0000e+00, 1.7906e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1467, 5.1467, 5.1467],
        [5.1467, 5.1461, 5.1466],
        [5.1467, 5.1466, 5.1467],
        [5.1467, 5.1466, 5.1467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1068, step:0 
model_pd.l_p.mean(): 0.21027694642543793 
model_pd.l_d.mean(): -19.3729248046875 
model_pd.lagr.mean(): -19.162647247314453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4928], device='cuda:0')), ('power', tensor([-20.2456], device='cuda:0'))])
epoch£º1068	 i:0 	 global-step:21360	 l-p:0.21027694642543793
epoch£º1068	 i:1 	 global-step:21361	 l-p:0.17226949334144592
epoch£º1068	 i:2 	 global-step:21362	 l-p:0.1269296407699585
epoch£º1068	 i:3 	 global-step:21363	 l-p:0.13940981030464172
epoch£º1068	 i:4 	 global-step:21364	 l-p:0.07166197896003723
epoch£º1068	 i:5 	 global-step:21365	 l-p:0.16663016378879547
epoch£º1068	 i:6 	 global-step:21366	 l-p:0.11542943865060806
epoch£º1068	 i:7 	 global-step:21367	 l-p:0.14273285865783691
epoch£º1068	 i:8 	 global-step:21368	 l-p:0.17473159730434418
epoch£º1068	 i:9 	 global-step:21369	 l-p:0.10433953255414963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1069
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3206e-01, 1.4261e-01,
         1.0000e+00, 8.7634e-02, 1.0000e+00, 6.1452e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1654, 4.9157, 4.8913],
        [5.1654, 5.1518, 5.1641],
        [5.1654, 5.1445, 5.1627],
        [5.1654, 5.0267, 5.0859]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1069, step:0 
model_pd.l_p.mean(): 0.1114647164940834 
model_pd.l_d.mean(): -20.57291030883789 
model_pd.lagr.mean(): -20.461444854736328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4068], device='cuda:0')), ('power', tensor([-21.3789], device='cuda:0'))])
epoch£º1069	 i:0 	 global-step:21380	 l-p:0.1114647164940834
epoch£º1069	 i:1 	 global-step:21381	 l-p:0.09130372852087021
epoch£º1069	 i:2 	 global-step:21382	 l-p:0.16171512007713318
epoch£º1069	 i:3 	 global-step:21383	 l-p:0.14147937297821045
epoch£º1069	 i:4 	 global-step:21384	 l-p:0.1289878636598587
epoch£º1069	 i:5 	 global-step:21385	 l-p:0.14830361306667328
epoch£º1069	 i:6 	 global-step:21386	 l-p:0.1184777244925499
epoch£º1069	 i:7 	 global-step:21387	 l-p:0.12817969918251038
epoch£º1069	 i:8 	 global-step:21388	 l-p:0.1698644459247589
epoch£º1069	 i:9 	 global-step:21389	 l-p:0.14236190915107727
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1070
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5704e-02, 2.1274e-02,
         1.0000e+00, 8.1249e-03, 1.0000e+00, 3.8191e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1876, 5.3258, 5.0831],
        [5.1876, 5.1862, 5.1876],
        [5.1876, 5.1499, 5.1802],
        [5.1876, 5.2569, 4.9804]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1070, step:0 
model_pd.l_p.mean(): 0.0956505835056305 
model_pd.l_d.mean(): -19.334299087524414 
model_pd.lagr.mean(): -19.238649368286133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5496], device='cuda:0')), ('power', tensor([-20.2650], device='cuda:0'))])
epoch£º1070	 i:0 	 global-step:21400	 l-p:0.0956505835056305
epoch£º1070	 i:1 	 global-step:21401	 l-p:0.1416023075580597
epoch£º1070	 i:2 	 global-step:21402	 l-p:0.13903552293777466
epoch£º1070	 i:3 	 global-step:21403	 l-p:0.13598336279392242
epoch£º1070	 i:4 	 global-step:21404	 l-p:0.13454562425613403
epoch£º1070	 i:5 	 global-step:21405	 l-p:0.1273110806941986
epoch£º1070	 i:6 	 global-step:21406	 l-p:0.07390085607767105
epoch£º1070	 i:7 	 global-step:21407	 l-p:0.1569804549217224
epoch£º1070	 i:8 	 global-step:21408	 l-p:0.15505144000053406
epoch£º1070	 i:9 	 global-step:21409	 l-p:0.11149770766496658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1071
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4409e-01, 7.5538e-02,
         1.0000e+00, 3.9601e-02, 1.0000e+00, 5.2425e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2003, 5.2003, 5.2003],
        [5.2003, 5.1822, 5.1982],
        [5.2003, 5.0456, 5.1010],
        [5.2003, 4.9542, 4.9317]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1071, step:0 
model_pd.l_p.mean(): 0.10795129835605621 
model_pd.l_d.mean(): -19.868581771850586 
model_pd.lagr.mean(): -19.760629653930664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4754], device='cuda:0')), ('power', tensor([-20.7325], device='cuda:0'))])
epoch£º1071	 i:0 	 global-step:21420	 l-p:0.10795129835605621
epoch£º1071	 i:1 	 global-step:21421	 l-p:0.11042296886444092
epoch£º1071	 i:2 	 global-step:21422	 l-p:0.13909883797168732
epoch£º1071	 i:3 	 global-step:21423	 l-p:0.14491389691829681
epoch£º1071	 i:4 	 global-step:21424	 l-p:0.14842060208320618
epoch£º1071	 i:5 	 global-step:21425	 l-p:0.08338121324777603
epoch£º1071	 i:6 	 global-step:21426	 l-p:0.09426700323820114
epoch£º1071	 i:7 	 global-step:21427	 l-p:0.1219833567738533
epoch£º1071	 i:8 	 global-step:21428	 l-p:0.1692461371421814
epoch£º1071	 i:9 	 global-step:21429	 l-p:0.13846994936466217
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1072
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1986, 5.1986, 5.1986],
        [5.1986, 5.5891, 5.4967],
        [5.1986, 5.1978, 5.1986],
        [5.1986, 4.9115, 4.7071]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1072, step:0 
model_pd.l_p.mean(): 0.1457100510597229 
model_pd.l_d.mean(): -19.377901077270508 
model_pd.lagr.mean(): -19.23219108581543 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5453], device='cuda:0')), ('power', tensor([-20.3051], device='cuda:0'))])
epoch£º1072	 i:0 	 global-step:21440	 l-p:0.1457100510597229
epoch£º1072	 i:1 	 global-step:21441	 l-p:0.09400130808353424
epoch£º1072	 i:2 	 global-step:21442	 l-p:0.09910479933023453
epoch£º1072	 i:3 	 global-step:21443	 l-p:0.10384142398834229
epoch£º1072	 i:4 	 global-step:21444	 l-p:0.1189849004149437
epoch£º1072	 i:5 	 global-step:21445	 l-p:0.15748873353004456
epoch£º1072	 i:6 	 global-step:21446	 l-p:0.14649762213230133
epoch£º1072	 i:7 	 global-step:21447	 l-p:0.12102223932743073
epoch£º1072	 i:8 	 global-step:21448	 l-p:0.12075299769639969
epoch£º1072	 i:9 	 global-step:21449	 l-p:0.14375022053718567
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1073
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1986, 5.1767, 5.1957],
        [5.1986, 5.1918, 5.1982],
        [5.1986, 5.1259, 4.7981],
        [5.1986, 5.1827, 5.1969]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1073, step:0 
model_pd.l_p.mean(): 0.09204481542110443 
model_pd.l_d.mean(): -19.52058219909668 
model_pd.lagr.mean(): -19.428537368774414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5314], device='cuda:0')), ('power', tensor([-20.4360], device='cuda:0'))])
epoch£º1073	 i:0 	 global-step:21460	 l-p:0.09204481542110443
epoch£º1073	 i:1 	 global-step:21461	 l-p:0.10440023243427277
epoch£º1073	 i:2 	 global-step:21462	 l-p:0.122467041015625
epoch£º1073	 i:3 	 global-step:21463	 l-p:0.13048827648162842
epoch£º1073	 i:4 	 global-step:21464	 l-p:0.11070457100868225
epoch£º1073	 i:5 	 global-step:21465	 l-p:0.12020467221736908
epoch£º1073	 i:6 	 global-step:21466	 l-p:0.17138591408729553
epoch£º1073	 i:7 	 global-step:21467	 l-p:0.1442115753889084
epoch£º1073	 i:8 	 global-step:21468	 l-p:0.1336800754070282
epoch£º1073	 i:9 	 global-step:21469	 l-p:0.15104517340660095
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1074
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1109e-06, 8.8037e-08,
         1.0000e+00, 1.5165e-09, 1.0000e+00, 1.7225e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0776e-01, 2.0779e-01,
         1.0000e+00, 1.4029e-01, 1.0000e+00, 6.7516e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1820, 5.1820, 5.1820],
        [5.1820, 4.8952, 4.7627],
        [5.1820, 5.0892, 5.1448],
        [5.1820, 4.8947, 4.7588]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1074, step:0 
model_pd.l_p.mean(): 0.09591235220432281 
model_pd.l_d.mean(): -19.67291831970215 
model_pd.lagr.mean(): -19.57700538635254 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4936], device='cuda:0')), ('power', tensor([-20.5521], device='cuda:0'))])
epoch£º1074	 i:0 	 global-step:21480	 l-p:0.09591235220432281
epoch£º1074	 i:1 	 global-step:21481	 l-p:0.12305470556020737
epoch£º1074	 i:2 	 global-step:21482	 l-p:0.13354402780532837
epoch£º1074	 i:3 	 global-step:21483	 l-p:0.15072447061538696
epoch£º1074	 i:4 	 global-step:21484	 l-p:0.1633414477109909
epoch£º1074	 i:5 	 global-step:21485	 l-p:0.16115541756153107
epoch£º1074	 i:6 	 global-step:21486	 l-p:0.12128235399723053
epoch£º1074	 i:7 	 global-step:21487	 l-p:0.14934833347797394
epoch£º1074	 i:8 	 global-step:21488	 l-p:0.16460676491260529
epoch£º1074	 i:9 	 global-step:21489	 l-p:0.09333964437246323
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1075
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0221e-01, 4.7791e-02,
         1.0000e+00, 2.2345e-02, 1.0000e+00, 4.6756e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5896e-02, 3.9969e-03,
         1.0000e+00, 1.0050e-03, 1.0000e+00, 2.5144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1653, 5.0666, 5.1237],
        [5.1653, 5.1614, 5.1651],
        [5.1653, 5.1649, 5.1653],
        [5.1653, 4.9297, 4.9271]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1075, step:0 
model_pd.l_p.mean(): 0.09135700017213821 
model_pd.l_d.mean(): -20.641550064086914 
model_pd.lagr.mean(): -20.550193786621094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4024], device='cuda:0')), ('power', tensor([-21.4443], device='cuda:0'))])
epoch£º1075	 i:0 	 global-step:21500	 l-p:0.09135700017213821
epoch£º1075	 i:1 	 global-step:21501	 l-p:0.15794169902801514
epoch£º1075	 i:2 	 global-step:21502	 l-p:0.13308408856391907
epoch£º1075	 i:3 	 global-step:21503	 l-p:0.13062837719917297
epoch£º1075	 i:4 	 global-step:21504	 l-p:0.15092766284942627
epoch£º1075	 i:5 	 global-step:21505	 l-p:0.12794585525989532
epoch£º1075	 i:6 	 global-step:21506	 l-p:0.1291399449110031
epoch£º1075	 i:7 	 global-step:21507	 l-p:0.13798914849758148
epoch£º1075	 i:8 	 global-step:21508	 l-p:0.18108783662319183
epoch£º1075	 i:9 	 global-step:21509	 l-p:0.1254686564207077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1076
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3287e-02, 2.0052e-02,
         1.0000e+00, 7.5458e-03, 1.0000e+00, 3.7631e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1661, 5.1469, 5.1638],
        [5.1661, 5.4118, 5.2293],
        [5.1661, 5.4420, 5.2775],
        [5.1661, 5.1309, 5.1596]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1076, step:0 
model_pd.l_p.mean(): 0.10112762451171875 
model_pd.l_d.mean(): -20.204668045043945 
model_pd.lagr.mean(): -20.103540420532227 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4860], device='cuda:0')), ('power', tensor([-21.0859], device='cuda:0'))])
epoch£º1076	 i:0 	 global-step:21520	 l-p:0.10112762451171875
epoch£º1076	 i:1 	 global-step:21521	 l-p:0.15224364399909973
epoch£º1076	 i:2 	 global-step:21522	 l-p:0.16816703975200653
epoch£º1076	 i:3 	 global-step:21523	 l-p:0.13177762925624847
epoch£º1076	 i:4 	 global-step:21524	 l-p:0.13065087795257568
epoch£º1076	 i:5 	 global-step:21525	 l-p:0.10578134655952454
epoch£º1076	 i:6 	 global-step:21526	 l-p:0.1753070056438446
epoch£º1076	 i:7 	 global-step:21527	 l-p:0.07812508195638657
epoch£º1076	 i:8 	 global-step:21528	 l-p:0.1964377760887146
epoch£º1076	 i:9 	 global-step:21529	 l-p:0.13052871823310852
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1077
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8557e-01, 1.8806e-01,
         1.0000e+00, 1.2384e-01, 1.0000e+00, 6.5853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1664, 4.8848, 4.7846],
        [5.1664, 4.9009, 4.8463],
        [5.1664, 5.1613, 5.1662],
        [5.1664, 5.1658, 5.1664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1077, step:0 
model_pd.l_p.mean(): 0.16176550090312958 
model_pd.l_d.mean(): -19.897037506103516 
model_pd.lagr.mean(): -19.735271453857422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4266], device='cuda:0')), ('power', tensor([-20.7110], device='cuda:0'))])
epoch£º1077	 i:0 	 global-step:21540	 l-p:0.16176550090312958
epoch£º1077	 i:1 	 global-step:21541	 l-p:0.10253282636404037
epoch£º1077	 i:2 	 global-step:21542	 l-p:0.12572140991687775
epoch£º1077	 i:3 	 global-step:21543	 l-p:0.1838972121477127
epoch£º1077	 i:4 	 global-step:21544	 l-p:0.17281097173690796
epoch£º1077	 i:5 	 global-step:21545	 l-p:0.11713249236345291
epoch£º1077	 i:6 	 global-step:21546	 l-p:0.125797837972641
epoch£º1077	 i:7 	 global-step:21547	 l-p:0.15186643600463867
epoch£º1077	 i:8 	 global-step:21548	 l-p:0.09984954446554184
epoch£º1077	 i:9 	 global-step:21549	 l-p:0.12160782516002655
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1078
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1701, 4.8816, 4.7479],
        [5.1701, 4.9799, 5.0206],
        [5.1701, 4.9177, 4.8891],
        [5.1701, 5.1701, 5.1701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1078, step:0 
model_pd.l_p.mean(): 0.15340456366539001 
model_pd.l_d.mean(): -19.205564498901367 
model_pd.lagr.mean(): -19.052160263061523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5356], device='cuda:0')), ('power', tensor([-20.1194], device='cuda:0'))])
epoch£º1078	 i:0 	 global-step:21560	 l-p:0.15340456366539001
epoch£º1078	 i:1 	 global-step:21561	 l-p:0.13594725728034973
epoch£º1078	 i:2 	 global-step:21562	 l-p:0.12536095082759857
epoch£º1078	 i:3 	 global-step:21563	 l-p:0.1419336348772049
epoch£º1078	 i:4 	 global-step:21564	 l-p:0.12328159809112549
epoch£º1078	 i:5 	 global-step:21565	 l-p:0.09467671811580658
epoch£º1078	 i:6 	 global-step:21566	 l-p:0.11689472198486328
epoch£º1078	 i:7 	 global-step:21567	 l-p:0.1429443359375
epoch£º1078	 i:8 	 global-step:21568	 l-p:0.19292302429676056
epoch£º1078	 i:9 	 global-step:21569	 l-p:0.1393417865037918
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1079
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1690, 5.1296, 5.1610],
        [5.1690, 5.0458, 4.7077],
        [5.1690, 5.1643, 5.1688],
        [5.1690, 4.8769, 4.7115]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1079, step:0 
model_pd.l_p.mean(): 0.2380947470664978 
model_pd.l_d.mean(): -20.006359100341797 
model_pd.lagr.mean(): -19.768264770507812 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5005], device='cuda:0')), ('power', tensor([-20.8988], device='cuda:0'))])
epoch£º1079	 i:0 	 global-step:21580	 l-p:0.2380947470664978
epoch£º1079	 i:1 	 global-step:21581	 l-p:0.10896088182926178
epoch£º1079	 i:2 	 global-step:21582	 l-p:0.058538105338811874
epoch£º1079	 i:3 	 global-step:21583	 l-p:0.1335323303937912
epoch£º1079	 i:4 	 global-step:21584	 l-p:0.1156592145562172
epoch£º1079	 i:5 	 global-step:21585	 l-p:0.13752137124538422
epoch£º1079	 i:6 	 global-step:21586	 l-p:0.1310022473335266
epoch£º1079	 i:7 	 global-step:21587	 l-p:0.1316051036119461
epoch£º1079	 i:8 	 global-step:21588	 l-p:0.16100560128688812
epoch£º1079	 i:9 	 global-step:21589	 l-p:0.13584423065185547
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1080
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1740, 5.1457, 5.1695],
        [5.1740, 5.0737, 5.1311],
        [5.1740, 5.4361, 5.2631],
        [5.1740, 4.9687, 4.6393]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1080, step:0 
model_pd.l_p.mean(): 0.11293616145849228 
model_pd.l_d.mean(): -20.08427619934082 
model_pd.lagr.mean(): -19.97134017944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4845], device='cuda:0')), ('power', tensor([-20.9616], device='cuda:0'))])
epoch£º1080	 i:0 	 global-step:21600	 l-p:0.11293616145849228
epoch£º1080	 i:1 	 global-step:21601	 l-p:0.1138242855668068
epoch£º1080	 i:2 	 global-step:21602	 l-p:0.11070241779088974
epoch£º1080	 i:3 	 global-step:21603	 l-p:0.12897375226020813
epoch£º1080	 i:4 	 global-step:21604	 l-p:0.19350269436836243
epoch£º1080	 i:5 	 global-step:21605	 l-p:0.09291360527276993
epoch£º1080	 i:6 	 global-step:21606	 l-p:0.13299651443958282
epoch£º1080	 i:7 	 global-step:21607	 l-p:0.17080123722553253
epoch£º1080	 i:8 	 global-step:21608	 l-p:0.15693432092666626
epoch£º1080	 i:9 	 global-step:21609	 l-p:0.15213027596473694
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1081
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1668, 5.1667, 5.1668],
        [5.1668, 5.1668, 5.1668],
        [5.1668, 5.1653, 5.1667],
        [5.1668, 5.1582, 5.1662]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1081, step:0 
model_pd.l_p.mean(): 0.1124316155910492 
model_pd.l_d.mean(): -19.187681198120117 
model_pd.lagr.mean(): -19.07524871826172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5249], device='cuda:0')), ('power', tensor([-20.0902], device='cuda:0'))])
epoch£º1081	 i:0 	 global-step:21620	 l-p:0.1124316155910492
epoch£º1081	 i:1 	 global-step:21621	 l-p:0.20402823388576508
epoch£º1081	 i:2 	 global-step:21622	 l-p:0.17311930656433105
epoch£º1081	 i:3 	 global-step:21623	 l-p:0.1262853443622589
epoch£º1081	 i:4 	 global-step:21624	 l-p:0.19984354078769684
epoch£º1081	 i:5 	 global-step:21625	 l-p:0.1208394393324852
epoch£º1081	 i:6 	 global-step:21626	 l-p:0.10146278142929077
epoch£º1081	 i:7 	 global-step:21627	 l-p:0.08045009523630142
epoch£º1081	 i:8 	 global-step:21628	 l-p:0.10633623600006104
epoch£º1081	 i:9 	 global-step:21629	 l-p:0.14505259692668915
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1082
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1995e-01, 5.9154e-02,
         1.0000e+00, 2.9173e-02, 1.0000e+00, 4.9317e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5180e-01, 3.4668e-01,
         1.0000e+00, 2.6601e-01, 1.0000e+00, 7.6733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1674, 5.1674, 5.1674],
        [5.1674, 5.0440, 5.1041],
        [5.1674, 4.9098, 4.6147],
        [5.1674, 5.0011, 4.6625]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1082, step:0 
model_pd.l_p.mean(): 0.11068654805421829 
model_pd.l_d.mean(): -19.214353561401367 
model_pd.lagr.mean(): -19.103666305541992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5416], device='cuda:0')), ('power', tensor([-20.1346], device='cuda:0'))])
epoch£º1082	 i:0 	 global-step:21640	 l-p:0.11068654805421829
epoch£º1082	 i:1 	 global-step:21641	 l-p:0.0988868772983551
epoch£º1082	 i:2 	 global-step:21642	 l-p:0.13344664871692657
epoch£º1082	 i:3 	 global-step:21643	 l-p:0.13638095557689667
epoch£º1082	 i:4 	 global-step:21644	 l-p:0.1137719377875328
epoch£º1082	 i:5 	 global-step:21645	 l-p:0.17897994816303253
epoch£º1082	 i:6 	 global-step:21646	 l-p:0.23241856694221497
epoch£º1082	 i:7 	 global-step:21647	 l-p:0.13936899602413177
epoch£º1082	 i:8 	 global-step:21648	 l-p:0.1449652463197708
epoch£º1082	 i:9 	 global-step:21649	 l-p:0.106486476957798
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1083
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1540, 5.1510, 5.1539],
        [5.1540, 4.9002, 4.8718],
        [5.1540, 5.4395, 5.2808],
        [5.1540, 5.0088, 4.6682]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1083, step:0 
model_pd.l_p.mean(): 0.11526763439178467 
model_pd.l_d.mean(): -20.017072677612305 
model_pd.lagr.mean(): -19.901805877685547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5148], device='cuda:0')), ('power', tensor([-20.9246], device='cuda:0'))])
epoch£º1083	 i:0 	 global-step:21660	 l-p:0.11526763439178467
epoch£º1083	 i:1 	 global-step:21661	 l-p:0.19445061683654785
epoch£º1083	 i:2 	 global-step:21662	 l-p:0.14492709934711456
epoch£º1083	 i:3 	 global-step:21663	 l-p:0.12755125761032104
epoch£º1083	 i:4 	 global-step:21664	 l-p:0.09495766460895538
epoch£º1083	 i:5 	 global-step:21665	 l-p:0.11397254467010498
epoch£º1083	 i:6 	 global-step:21666	 l-p:0.2218228131532669
epoch£º1083	 i:7 	 global-step:21667	 l-p:0.15559734404087067
epoch£º1083	 i:8 	 global-step:21668	 l-p:0.09777979552745819
epoch£º1083	 i:9 	 global-step:21669	 l-p:0.18916593492031097
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1084
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1427, 5.4430, 5.2937],
        [5.1427, 5.1284, 5.1413],
        [5.1427, 4.8486, 4.6335],
        [5.1427, 4.9949, 4.6533]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1084, step:0 
model_pd.l_p.mean(): 0.12698142230510712 
model_pd.l_d.mean(): -20.06256103515625 
model_pd.lagr.mean(): -19.935579299926758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5149], device='cuda:0')), ('power', tensor([-20.9710], device='cuda:0'))])
epoch£º1084	 i:0 	 global-step:21680	 l-p:0.12698142230510712
epoch£º1084	 i:1 	 global-step:21681	 l-p:0.16837716102600098
epoch£º1084	 i:2 	 global-step:21682	 l-p:0.1251329630613327
epoch£º1084	 i:3 	 global-step:21683	 l-p:0.1507270187139511
epoch£º1084	 i:4 	 global-step:21684	 l-p:0.10089204460382462
epoch£º1084	 i:5 	 global-step:21685	 l-p:0.12075337767601013
epoch£º1084	 i:6 	 global-step:21686	 l-p:0.1287001520395279
epoch£º1084	 i:7 	 global-step:21687	 l-p:0.10871622711420059
epoch£º1084	 i:8 	 global-step:21688	 l-p:0.22423644363880157
epoch£º1084	 i:9 	 global-step:21689	 l-p:0.3328491449356079
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1085
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1281, 5.0084, 5.0689],
        [5.1281, 4.8490, 4.5752],
        [5.1281, 4.8878, 4.8836],
        [5.1281, 5.1253, 5.1280]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1085, step:0 
model_pd.l_p.mean(): 0.29210221767425537 
model_pd.l_d.mean(): -19.512954711914062 
model_pd.lagr.mean(): -19.22085189819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5003], device='cuda:0')), ('power', tensor([-20.3960], device='cuda:0'))])
epoch£º1085	 i:0 	 global-step:21700	 l-p:0.29210221767425537
epoch£º1085	 i:1 	 global-step:21701	 l-p:0.13652992248535156
epoch£º1085	 i:2 	 global-step:21702	 l-p:0.14685922861099243
epoch£º1085	 i:3 	 global-step:21703	 l-p:0.13192304968833923
epoch£º1085	 i:4 	 global-step:21704	 l-p:0.08571599423885345
epoch£º1085	 i:5 	 global-step:21705	 l-p:0.1934697926044464
epoch£º1085	 i:6 	 global-step:21706	 l-p:0.13351646065711975
epoch£º1085	 i:7 	 global-step:21707	 l-p:0.11139880865812302
epoch£º1085	 i:8 	 global-step:21708	 l-p:0.10758374631404877
epoch£º1085	 i:9 	 global-step:21709	 l-p:0.3141784071922302
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1086
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6532e-02, 4.4282e-02,
         1.0000e+00, 2.0314e-02, 1.0000e+00, 4.5873e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1205, 5.0288, 5.0846],
        [5.1205, 5.1944, 4.9197],
        [5.1205, 5.1205, 5.1205],
        [5.1205, 5.1204, 5.1205]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1086, step:0 
model_pd.l_p.mean(): 0.21126039326190948 
model_pd.l_d.mean(): -19.717424392700195 
model_pd.lagr.mean(): -19.50616455078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4322], device='cuda:0')), ('power', tensor([-20.5338], device='cuda:0'))])
epoch£º1086	 i:0 	 global-step:21720	 l-p:0.21126039326190948
epoch£º1086	 i:1 	 global-step:21721	 l-p:0.1339125633239746
epoch£º1086	 i:2 	 global-step:21722	 l-p:0.07053052634000778
epoch£º1086	 i:3 	 global-step:21723	 l-p:0.13553114235401154
epoch£º1086	 i:4 	 global-step:21724	 l-p:0.11256719380617142
epoch£º1086	 i:5 	 global-step:21725	 l-p:0.1739063858985901
epoch£º1086	 i:6 	 global-step:21726	 l-p:0.25465625524520874
epoch£º1086	 i:7 	 global-step:21727	 l-p:0.11188860982656479
epoch£º1086	 i:8 	 global-step:21728	 l-p:0.1441877782344818
epoch£º1086	 i:9 	 global-step:21729	 l-p:0.3242378234863281
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1087
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5590e-01, 4.5708e-01,
         1.0000e+00, 3.7583e-01, 1.0000e+00, 8.2224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1271, 5.1270, 5.1271],
        [5.1271, 5.0934, 5.1211],
        [5.1271, 5.1077, 5.1248],
        [5.1271, 4.9477, 4.6065]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1087, step:0 
model_pd.l_p.mean(): 0.18163429200649261 
model_pd.l_d.mean(): -19.407798767089844 
model_pd.lagr.mean(): -19.226163864135742 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5336], device='cuda:0')), ('power', tensor([-20.3234], device='cuda:0'))])
epoch£º1087	 i:0 	 global-step:21740	 l-p:0.18163429200649261
epoch£º1087	 i:1 	 global-step:21741	 l-p:0.13212591409683228
epoch£º1087	 i:2 	 global-step:21742	 l-p:0.1698632836341858
epoch£º1087	 i:3 	 global-step:21743	 l-p:0.14819805324077606
epoch£º1087	 i:4 	 global-step:21744	 l-p:0.12002124637365341
epoch£º1087	 i:5 	 global-step:21745	 l-p:0.28952932357788086
epoch£º1087	 i:6 	 global-step:21746	 l-p:0.11880403012037277
epoch£º1087	 i:7 	 global-step:21747	 l-p:0.1337205171585083
epoch£º1087	 i:8 	 global-step:21748	 l-p:0.10359418392181396
epoch£º1087	 i:9 	 global-step:21749	 l-p:0.17596293985843658
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1088
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1361, 5.1252, 5.1352],
        [5.1361, 5.1122, 4.7964],
        [5.1361, 5.3959, 5.2218],
        [5.1361, 5.0680, 4.7384]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1088, step:0 
model_pd.l_p.mean(): 0.14215031266212463 
model_pd.l_d.mean(): -19.97636604309082 
model_pd.lagr.mean(): -19.83421516418457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5081], device='cuda:0')), ('power', tensor([-20.8761], device='cuda:0'))])
epoch£º1088	 i:0 	 global-step:21760	 l-p:0.14215031266212463
epoch£º1088	 i:1 	 global-step:21761	 l-p:0.1331305056810379
epoch£º1088	 i:2 	 global-step:21762	 l-p:0.1782512068748474
epoch£º1088	 i:3 	 global-step:21763	 l-p:0.144225612282753
epoch£º1088	 i:4 	 global-step:21764	 l-p:0.07228532433509827
epoch£º1088	 i:5 	 global-step:21765	 l-p:0.1934632807970047
epoch£º1088	 i:6 	 global-step:21766	 l-p:0.10826124995946884
epoch£º1088	 i:7 	 global-step:21767	 l-p:0.27147090435028076
epoch£º1088	 i:8 	 global-step:21768	 l-p:0.13186617195606232
epoch£º1088	 i:9 	 global-step:21769	 l-p:0.14713867008686066
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1089
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0057e-01, 4.6772e-02,
         1.0000e+00, 2.1751e-02, 1.0000e+00, 4.6505e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1454, 5.0530, 5.1089],
        [5.1454, 5.0483, 5.1054],
        [5.1454, 4.8685, 4.5950],
        [5.1454, 5.1454, 5.1454]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1089, step:0 
model_pd.l_p.mean(): 0.1501167267560959 
model_pd.l_d.mean(): -20.542512893676758 
model_pd.lagr.mean(): -20.392396926879883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4414], device='cuda:0')), ('power', tensor([-21.3838], device='cuda:0'))])
epoch£º1089	 i:0 	 global-step:21780	 l-p:0.1501167267560959
epoch£º1089	 i:1 	 global-step:21781	 l-p:0.19428449869155884
epoch£º1089	 i:2 	 global-step:21782	 l-p:0.15578517317771912
epoch£º1089	 i:3 	 global-step:21783	 l-p:0.14224611222743988
epoch£º1089	 i:4 	 global-step:21784	 l-p:0.11755251884460449
epoch£º1089	 i:5 	 global-step:21785	 l-p:0.11811038851737976
epoch£º1089	 i:6 	 global-step:21786	 l-p:0.12458018213510513
epoch£º1089	 i:7 	 global-step:21787	 l-p:0.10177768766880035
epoch£º1089	 i:8 	 global-step:21788	 l-p:0.15727964043617249
epoch£º1089	 i:9 	 global-step:21789	 l-p:0.17898832261562347
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1090
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0432e-01, 2.9898e-01,
         1.0000e+00, 2.2108e-01, 1.0000e+00, 7.3945e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3942e-01, 6.6863e-01,
         1.0000e+00, 6.0462e-01, 1.0000e+00, 9.0427e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5719e-03, 2.0323e-03,
         1.0000e+00, 4.3151e-04, 1.0000e+00, 2.1232e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1600, 4.8679, 4.7234],
        [5.1600, 4.8769, 4.6227],
        [5.1600, 5.2192, 4.9373],
        [5.1600, 5.1586, 5.1600]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1090, step:0 
model_pd.l_p.mean(): 0.1952006071805954 
model_pd.l_d.mean(): -20.5165958404541 
model_pd.lagr.mean(): -20.321395874023438 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4295], device='cuda:0')), ('power', tensor([-21.3451], device='cuda:0'))])
epoch£º1090	 i:0 	 global-step:21800	 l-p:0.1952006071805954
epoch£º1090	 i:1 	 global-step:21801	 l-p:0.14831098914146423
epoch£º1090	 i:2 	 global-step:21802	 l-p:0.10576140880584717
epoch£º1090	 i:3 	 global-step:21803	 l-p:0.1191968321800232
epoch£º1090	 i:4 	 global-step:21804	 l-p:0.14166544377803802
epoch£º1090	 i:5 	 global-step:21805	 l-p:0.15283386409282684
epoch£º1090	 i:6 	 global-step:21806	 l-p:0.16426877677440643
epoch£º1090	 i:7 	 global-step:21807	 l-p:0.1272965967655182
epoch£º1090	 i:8 	 global-step:21808	 l-p:0.1195809543132782
epoch£º1090	 i:9 	 global-step:21809	 l-p:0.11881385743618011
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1091
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2256e-03, 4.7659e-04,
         1.0000e+00, 7.0418e-05, 1.0000e+00, 1.4775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1654, 5.0507, 4.7126],
        [5.1654, 4.8814, 4.6317],
        [5.1654, 4.9846, 4.6469],
        [5.1654, 5.1652, 5.1654]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1091, step:0 
model_pd.l_p.mean(): 0.15238305926322937 
model_pd.l_d.mean(): -20.149160385131836 
model_pd.lagr.mean(): -19.996776580810547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4336], device='cuda:0')), ('power', tensor([-20.9750], device='cuda:0'))])
epoch£º1091	 i:0 	 global-step:21820	 l-p:0.15238305926322937
epoch£º1091	 i:1 	 global-step:21821	 l-p:0.13976290822029114
epoch£º1091	 i:2 	 global-step:21822	 l-p:0.17855745553970337
epoch£º1091	 i:3 	 global-step:21823	 l-p:0.14203453063964844
epoch£º1091	 i:4 	 global-step:21824	 l-p:0.15543672442436218
epoch£º1091	 i:5 	 global-step:21825	 l-p:0.15458811819553375
epoch£º1091	 i:6 	 global-step:21826	 l-p:0.12390048801898956
epoch£º1091	 i:7 	 global-step:21827	 l-p:0.08934707194566727
epoch£º1091	 i:8 	 global-step:21828	 l-p:0.13765056431293488
epoch£º1091	 i:9 	 global-step:21829	 l-p:0.12336841970682144
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1092
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3509e-01, 1.4509e-01,
         1.0000e+00, 8.9548e-02, 1.0000e+00, 6.1718e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1614, 5.0128, 5.0710],
        [5.1614, 5.1425, 5.1592],
        [5.1614, 5.0991, 4.7715],
        [5.1614, 4.9076, 4.8794]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1092, step:0 
model_pd.l_p.mean(): 0.1293647587299347 
model_pd.l_d.mean(): -20.34848976135254 
model_pd.lagr.mean(): -20.219125747680664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4713], device='cuda:0')), ('power', tensor([-21.2172], device='cuda:0'))])
epoch£º1092	 i:0 	 global-step:21840	 l-p:0.1293647587299347
epoch£º1092	 i:1 	 global-step:21841	 l-p:0.11911600083112717
epoch£º1092	 i:2 	 global-step:21842	 l-p:0.1386813521385193
epoch£º1092	 i:3 	 global-step:21843	 l-p:0.131422758102417
epoch£º1092	 i:4 	 global-step:21844	 l-p:0.24636660516262054
epoch£º1092	 i:5 	 global-step:21845	 l-p:0.14059528708457947
epoch£º1092	 i:6 	 global-step:21846	 l-p:0.07858575880527496
epoch£º1092	 i:7 	 global-step:21847	 l-p:0.17018550634384155
epoch£º1092	 i:8 	 global-step:21848	 l-p:0.12654659152030945
epoch£º1092	 i:9 	 global-step:21849	 l-p:0.12559404969215393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1093
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4441e-04, 3.3914e-05,
         1.0000e+00, 2.5881e-06, 1.0000e+00, 7.6313e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7318e-03, 2.0796e-04,
         1.0000e+00, 2.4974e-05, 1.0000e+00, 1.2009e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1566, 5.0929, 4.7647],
        [5.1566, 5.1564, 5.1566],
        [5.1566, 5.1566, 5.1566],
        [5.1566, 5.1565, 5.1566]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1093, step:0 
model_pd.l_p.mean(): 0.17359788715839386 
model_pd.l_d.mean(): -20.326229095458984 
model_pd.lagr.mean(): -20.152631759643555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4592], device='cuda:0')), ('power', tensor([-21.1819], device='cuda:0'))])
epoch£º1093	 i:0 	 global-step:21860	 l-p:0.17359788715839386
epoch£º1093	 i:1 	 global-step:21861	 l-p:0.1161099374294281
epoch£º1093	 i:2 	 global-step:21862	 l-p:0.11017903685569763
epoch£º1093	 i:3 	 global-step:21863	 l-p:0.11170846968889236
epoch£º1093	 i:4 	 global-step:21864	 l-p:0.14552389085292816
epoch£º1093	 i:5 	 global-step:21865	 l-p:0.10467173159122467
epoch£º1093	 i:6 	 global-step:21866	 l-p:0.15943965315818787
epoch£º1093	 i:7 	 global-step:21867	 l-p:0.10348944365978241
epoch£º1093	 i:8 	 global-step:21868	 l-p:0.1992744356393814
epoch£º1093	 i:9 	 global-step:21869	 l-p:0.19912055134773254
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1094
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1563, 5.0747, 5.1272],
        [5.1563, 5.0514, 5.1100],
        [5.1563, 5.1563, 5.1563],
        [5.1563, 4.9057, 4.8839]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1094, step:0 
model_pd.l_p.mean(): 0.15652720630168915 
model_pd.l_d.mean(): -20.67818260192871 
model_pd.lagr.mean(): -20.521656036376953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4061], device='cuda:0')), ('power', tensor([-21.4855], device='cuda:0'))])
epoch£º1094	 i:0 	 global-step:21880	 l-p:0.15652720630168915
epoch£º1094	 i:1 	 global-step:21881	 l-p:0.12514713406562805
epoch£º1094	 i:2 	 global-step:21882	 l-p:0.10763227194547653
epoch£º1094	 i:3 	 global-step:21883	 l-p:0.13004599511623383
epoch£º1094	 i:4 	 global-step:21884	 l-p:0.16746482253074646
epoch£º1094	 i:5 	 global-step:21885	 l-p:0.12612740695476532
epoch£º1094	 i:6 	 global-step:21886	 l-p:0.10200367867946625
epoch£º1094	 i:7 	 global-step:21887	 l-p:0.17111918330192566
epoch£º1094	 i:8 	 global-step:21888	 l-p:0.1915965974330902
epoch£º1094	 i:9 	 global-step:21889	 l-p:0.15639714896678925
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1095
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1542, 5.1519, 5.1541],
        [5.1542, 4.9952, 5.0512],
        [5.1542, 4.8996, 4.8713],
        [5.1542, 5.1542, 5.1542]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1095, step:0 
model_pd.l_p.mean(): 0.13884221017360687 
model_pd.l_d.mean(): -20.212156295776367 
model_pd.lagr.mean(): -20.073314666748047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4678], device='cuda:0')), ('power', tensor([-21.0746], device='cuda:0'))])
epoch£º1095	 i:0 	 global-step:21900	 l-p:0.13884221017360687
epoch£º1095	 i:1 	 global-step:21901	 l-p:0.09709661453962326
epoch£º1095	 i:2 	 global-step:21902	 l-p:0.19166994094848633
epoch£º1095	 i:3 	 global-step:21903	 l-p:0.12548375129699707
epoch£º1095	 i:4 	 global-step:21904	 l-p:0.1500844955444336
epoch£º1095	 i:5 	 global-step:21905	 l-p:0.09825727343559265
epoch£º1095	 i:6 	 global-step:21906	 l-p:0.10760974884033203
epoch£º1095	 i:7 	 global-step:21907	 l-p:0.2012747973203659
epoch£º1095	 i:8 	 global-step:21908	 l-p:0.18130351603031158
epoch£º1095	 i:9 	 global-step:21909	 l-p:0.1336154192686081
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1096
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1545, 4.9037, 4.8819],
        [5.1545, 4.8997, 4.8711],
        [5.1545, 5.1544, 5.1545],
        [5.1545, 4.9370, 4.9576]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1096, step:0 
model_pd.l_p.mean(): 0.11294688284397125 
model_pd.l_d.mean(): -17.663393020629883 
model_pd.lagr.mean(): -17.550445556640625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6401], device='cuda:0')), ('power', tensor([-18.6566], device='cuda:0'))])
epoch£º1096	 i:0 	 global-step:21920	 l-p:0.11294688284397125
epoch£º1096	 i:1 	 global-step:21921	 l-p:0.13109377026557922
epoch£º1096	 i:2 	 global-step:21922	 l-p:0.20382216572761536
epoch£º1096	 i:3 	 global-step:21923	 l-p:0.09278244525194168
epoch£º1096	 i:4 	 global-step:21924	 l-p:0.17270323634147644
epoch£º1096	 i:5 	 global-step:21925	 l-p:0.12385598570108414
epoch£º1096	 i:6 	 global-step:21926	 l-p:0.12281060963869095
epoch£º1096	 i:7 	 global-step:21927	 l-p:0.14625872671604156
epoch£º1096	 i:8 	 global-step:21928	 l-p:0.16287319362163544
epoch£º1096	 i:9 	 global-step:21929	 l-p:0.15012811124324799
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1097
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1627, 5.1627, 5.1627],
        [5.1627, 5.1610, 5.1626],
        [5.1627, 5.1627, 5.1627],
        [5.1627, 5.2836, 5.0312]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1097, step:0 
model_pd.l_p.mean(): 0.10157886892557144 
model_pd.l_d.mean(): -19.433887481689453 
model_pd.lagr.mean(): -19.332307815551758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4799], device='cuda:0')), ('power', tensor([-20.2944], device='cuda:0'))])
epoch£º1097	 i:0 	 global-step:21940	 l-p:0.10157886892557144
epoch£º1097	 i:1 	 global-step:21941	 l-p:0.11905063688755035
epoch£º1097	 i:2 	 global-step:21942	 l-p:0.13276252150535583
epoch£º1097	 i:3 	 global-step:21943	 l-p:0.12617996335029602
epoch£º1097	 i:4 	 global-step:21944	 l-p:0.13150407373905182
epoch£º1097	 i:5 	 global-step:21945	 l-p:0.13107819855213165
epoch£º1097	 i:6 	 global-step:21946	 l-p:0.18800538778305054
epoch£º1097	 i:7 	 global-step:21947	 l-p:0.13283871114253998
epoch£º1097	 i:8 	 global-step:21948	 l-p:0.15694844722747803
epoch£º1097	 i:9 	 global-step:21949	 l-p:0.17844848334789276
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1098
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0217e-02, 9.4118e-03,
         1.0000e+00, 2.9315e-03, 1.0000e+00, 3.1147e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1599, 5.1551, 5.1597],
        [5.1599, 5.1470, 5.1587],
        [5.1599, 4.8651, 4.6994],
        [5.1599, 4.9391, 4.9560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1098, step:0 
model_pd.l_p.mean(): 0.18118521571159363 
model_pd.l_d.mean(): -20.677196502685547 
model_pd.lagr.mean(): -20.49601173400879 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4036], device='cuda:0')), ('power', tensor([-21.4818], device='cuda:0'))])
epoch£º1098	 i:0 	 global-step:21960	 l-p:0.18118521571159363
epoch£º1098	 i:1 	 global-step:21961	 l-p:0.19152075052261353
epoch£º1098	 i:2 	 global-step:21962	 l-p:0.17235656082630157
epoch£º1098	 i:3 	 global-step:21963	 l-p:0.16384704411029816
epoch£º1098	 i:4 	 global-step:21964	 l-p:0.16158057749271393
epoch£º1098	 i:5 	 global-step:21965	 l-p:0.11520734429359436
epoch£º1098	 i:6 	 global-step:21966	 l-p:0.07186439633369446
epoch£º1098	 i:7 	 global-step:21967	 l-p:0.11000650376081467
epoch£º1098	 i:8 	 global-step:21968	 l-p:0.11663803458213806
epoch£º1098	 i:9 	 global-step:21969	 l-p:0.09948323667049408
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1099
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0480e-03, 4.4193e-04,
         1.0000e+00, 6.4076e-05, 1.0000e+00, 1.4499e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6791e-02, 3.8427e-02,
         1.0000e+00, 1.7014e-02, 1.0000e+00, 4.4275e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1734, 5.1579, 4.8451],
        [5.1734, 5.1733, 5.1734],
        [5.1734, 5.0957, 5.1467],
        [5.1734, 5.1734, 5.1734]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1099, step:0 
model_pd.l_p.mean(): 0.13083432614803314 
model_pd.l_d.mean(): -20.410593032836914 
model_pd.lagr.mean(): -20.27975845336914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4501], device='cuda:0')), ('power', tensor([-21.2584], device='cuda:0'))])
epoch£º1099	 i:0 	 global-step:21980	 l-p:0.13083432614803314
epoch£º1099	 i:1 	 global-step:21981	 l-p:0.20643578469753265
epoch£º1099	 i:2 	 global-step:21982	 l-p:0.11615949124097824
epoch£º1099	 i:3 	 global-step:21983	 l-p:0.10854821652173996
epoch£º1099	 i:4 	 global-step:21984	 l-p:0.09306830167770386
epoch£º1099	 i:5 	 global-step:21985	 l-p:0.19821541011333466
epoch£º1099	 i:6 	 global-step:21986	 l-p:0.08410017192363739
epoch£º1099	 i:7 	 global-step:21987	 l-p:0.1793796867132187
epoch£º1099	 i:8 	 global-step:21988	 l-p:0.11715026944875717
epoch£º1099	 i:9 	 global-step:21989	 l-p:0.11448117345571518
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1100
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1790, 5.1790, 5.1790],
        [5.1790, 5.5143, 5.3856],
        [5.1790, 5.2625, 4.9914],
        [5.1790, 4.9229, 4.8887]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1100, step:0 
model_pd.l_p.mean(): 0.10986310988664627 
model_pd.l_d.mean(): -20.245405197143555 
model_pd.lagr.mean(): -20.135541915893555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4711], device='cuda:0')), ('power', tensor([-21.1119], device='cuda:0'))])
epoch£º1100	 i:0 	 global-step:22000	 l-p:0.10986310988664627
epoch£º1100	 i:1 	 global-step:22001	 l-p:0.1721518486738205
epoch£º1100	 i:2 	 global-step:22002	 l-p:0.10540947318077087
epoch£º1100	 i:3 	 global-step:22003	 l-p:0.1517760008573532
epoch£º1100	 i:4 	 global-step:22004	 l-p:0.1287652850151062
epoch£º1100	 i:5 	 global-step:22005	 l-p:0.10150004178285599
epoch£º1100	 i:6 	 global-step:22006	 l-p:0.10055440664291382
epoch£º1100	 i:7 	 global-step:22007	 l-p:0.17449045181274414
epoch£º1100	 i:8 	 global-step:22008	 l-p:0.14090050756931305
epoch£º1100	 i:9 	 global-step:22009	 l-p:0.13430756330490112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1101
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1835, 4.9320, 4.6305],
        [5.1835, 5.1566, 5.1794],
        [5.1835, 5.1784, 5.1833],
        [5.1835, 4.9539, 4.6350]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1101, step:0 
model_pd.l_p.mean(): 0.11686120927333832 
model_pd.l_d.mean(): -20.18590545654297 
model_pd.lagr.mean(): -20.06904411315918 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4775], device='cuda:0')), ('power', tensor([-21.0579], device='cuda:0'))])
epoch£º1101	 i:0 	 global-step:22020	 l-p:0.11686120927333832
epoch£º1101	 i:1 	 global-step:22021	 l-p:0.12290012836456299
epoch£º1101	 i:2 	 global-step:22022	 l-p:0.16001704335212708
epoch£º1101	 i:3 	 global-step:22023	 l-p:0.07942735403776169
epoch£º1101	 i:4 	 global-step:22024	 l-p:0.13873246312141418
epoch£º1101	 i:5 	 global-step:22025	 l-p:0.12251783907413483
epoch£º1101	 i:6 	 global-step:22026	 l-p:0.11589889973402023
epoch£º1101	 i:7 	 global-step:22027	 l-p:0.11253463476896286
epoch£º1101	 i:8 	 global-step:22028	 l-p:0.11583905667066574
epoch£º1101	 i:9 	 global-step:22029	 l-p:0.2207738757133484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1102
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5038e-01, 1.5781e-01,
         1.0000e+00, 9.9466e-02, 1.0000e+00, 6.3028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1874, 4.9240, 4.8743],
        [5.1874, 5.0066, 4.6701],
        [5.1874, 5.1788, 5.1868],
        [5.1874, 4.8954, 4.7308]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1102, step:0 
model_pd.l_p.mean(): 0.1292634904384613 
model_pd.l_d.mean(): -19.51380729675293 
model_pd.lagr.mean(): -19.384544372558594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4676], device='cuda:0')), ('power', tensor([-20.3630], device='cuda:0'))])
epoch£º1102	 i:0 	 global-step:22040	 l-p:0.1292634904384613
epoch£º1102	 i:1 	 global-step:22041	 l-p:0.15777689218521118
epoch£º1102	 i:2 	 global-step:22042	 l-p:0.15020762383937836
epoch£º1102	 i:3 	 global-step:22043	 l-p:0.1546015441417694
epoch£º1102	 i:4 	 global-step:22044	 l-p:0.14252330362796783
epoch£º1102	 i:5 	 global-step:22045	 l-p:0.10015130788087845
epoch£º1102	 i:6 	 global-step:22046	 l-p:0.1298912912607193
epoch£º1102	 i:7 	 global-step:22047	 l-p:0.11385682970285416
epoch£º1102	 i:8 	 global-step:22048	 l-p:0.16842374205589294
epoch£º1102	 i:9 	 global-step:22049	 l-p:0.07209513336420059
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1103
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4975e-01, 7.9520e-02,
         1.0000e+00, 4.2227e-02, 1.0000e+00, 5.3103e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9007e-01, 6.0981e-01,
         1.0000e+00, 5.3888e-01, 1.0000e+00, 8.8369e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1760, 5.1538, 5.1730],
        [5.1760, 5.0120, 5.0661],
        [5.1760, 5.1685, 4.8584],
        [5.1760, 5.1760, 5.1760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1103, step:0 
model_pd.l_p.mean(): 0.1399783045053482 
model_pd.l_d.mean(): -20.317398071289062 
model_pd.lagr.mean(): -20.177419662475586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4607], device='cuda:0')), ('power', tensor([-21.1745], device='cuda:0'))])
epoch£º1103	 i:0 	 global-step:22060	 l-p:0.1399783045053482
epoch£º1103	 i:1 	 global-step:22061	 l-p:0.14724430441856384
epoch£º1103	 i:2 	 global-step:22062	 l-p:0.197800874710083
epoch£º1103	 i:3 	 global-step:22063	 l-p:0.1366250216960907
epoch£º1103	 i:4 	 global-step:22064	 l-p:0.14623889327049255
epoch£º1103	 i:5 	 global-step:22065	 l-p:0.0575409010052681
epoch£º1103	 i:6 	 global-step:22066	 l-p:0.11523614823818207
epoch£º1103	 i:7 	 global-step:22067	 l-p:0.1312425136566162
epoch£º1103	 i:8 	 global-step:22068	 l-p:0.10891393572092056
epoch£º1103	 i:9 	 global-step:22069	 l-p:0.16699187457561493
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1104
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1757, 5.4661, 5.3093],
        [5.1757, 5.0850, 5.1403],
        [5.1757, 5.1757, 5.1757],
        [5.1757, 5.1713, 5.1755]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1104, step:0 
model_pd.l_p.mean(): 0.11699102818965912 
model_pd.l_d.mean(): -20.367084503173828 
model_pd.lagr.mean(): -20.250093460083008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4583], device='cuda:0')), ('power', tensor([-21.2226], device='cuda:0'))])
epoch£º1104	 i:0 	 global-step:22080	 l-p:0.11699102818965912
epoch£º1104	 i:1 	 global-step:22081	 l-p:0.1486731916666031
epoch£º1104	 i:2 	 global-step:22082	 l-p:0.0708896741271019
epoch£º1104	 i:3 	 global-step:22083	 l-p:0.13237948715686798
epoch£º1104	 i:4 	 global-step:22084	 l-p:0.12244892120361328
epoch£º1104	 i:5 	 global-step:22085	 l-p:0.14566795527935028
epoch£º1104	 i:6 	 global-step:22086	 l-p:0.16405721008777618
epoch£º1104	 i:7 	 global-step:22087	 l-p:0.11495906859636307
epoch£º1104	 i:8 	 global-step:22088	 l-p:0.20704513788223267
epoch£º1104	 i:9 	 global-step:22089	 l-p:0.12680836021900177
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1105
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2672e-01, 4.2538e-01,
         1.0000e+00, 3.4353e-01, 1.0000e+00, 8.0759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1973e-01, 5.2836e-01,
         1.0000e+00, 4.5047e-01, 1.0000e+00, 8.5258e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5921e-01, 8.6288e-02,
         1.0000e+00, 4.6767e-02, 1.0000e+00, 5.4198e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1725, 4.9718, 4.6381],
        [5.1725, 5.1647, 5.1719],
        [5.1725, 5.0725, 4.7361],
        [5.1725, 4.9963, 5.0455]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1105, step:0 
model_pd.l_p.mean(): 0.16180205345153809 
model_pd.l_d.mean(): -20.256555557250977 
model_pd.lagr.mean(): -20.09475326538086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4676], device='cuda:0')), ('power', tensor([-21.1196], device='cuda:0'))])
epoch£º1105	 i:0 	 global-step:22100	 l-p:0.16180205345153809
epoch£º1105	 i:1 	 global-step:22101	 l-p:0.1301441639661789
epoch£º1105	 i:2 	 global-step:22102	 l-p:0.13294558227062225
epoch£º1105	 i:3 	 global-step:22103	 l-p:0.17307880520820618
epoch£º1105	 i:4 	 global-step:22104	 l-p:0.15298894047737122
epoch£º1105	 i:5 	 global-step:22105	 l-p:0.16183774173259735
epoch£º1105	 i:6 	 global-step:22106	 l-p:0.09341197460889816
epoch£º1105	 i:7 	 global-step:22107	 l-p:0.11718416213989258
epoch£º1105	 i:8 	 global-step:22108	 l-p:0.0936175063252449
epoch£º1105	 i:9 	 global-step:22109	 l-p:0.1503710150718689
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1106
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1632, 5.2438, 4.9713],
        [5.1632, 5.1602, 5.1631],
        [5.1632, 5.1390, 5.1598],
        [5.1632, 5.1294, 5.1571]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1106, step:0 
model_pd.l_p.mean(): 0.13560889661312103 
model_pd.l_d.mean(): -20.13955307006836 
model_pd.lagr.mean(): -20.003944396972656 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4609], device='cuda:0')), ('power', tensor([-20.9935], device='cuda:0'))])
epoch£º1106	 i:0 	 global-step:22120	 l-p:0.13560889661312103
epoch£º1106	 i:1 	 global-step:22121	 l-p:0.10571590065956116
epoch£º1106	 i:2 	 global-step:22122	 l-p:0.1252875030040741
epoch£º1106	 i:3 	 global-step:22123	 l-p:0.14490501582622528
epoch£º1106	 i:4 	 global-step:22124	 l-p:0.11262993514537811
epoch£º1106	 i:5 	 global-step:22125	 l-p:0.21796070039272308
epoch£º1106	 i:6 	 global-step:22126	 l-p:0.13420070707798004
epoch£º1106	 i:7 	 global-step:22127	 l-p:0.1675141155719757
epoch£º1106	 i:8 	 global-step:22128	 l-p:0.1232616975903511
epoch£º1106	 i:9 	 global-step:22129	 l-p:0.1479884535074234
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1107
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0266e-01, 4.8071e-02,
         1.0000e+00, 2.2509e-02, 1.0000e+00, 4.6824e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1553, 5.2891, 5.0430],
        [5.1553, 5.1553, 5.1553],
        [5.1553, 5.0551, 5.1129],
        [5.1553, 5.1522, 5.1552]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1107, step:0 
model_pd.l_p.mean(): 0.10855928063392639 
model_pd.l_d.mean(): -20.340551376342773 
model_pd.lagr.mean(): -20.231992721557617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4711], device='cuda:0')), ('power', tensor([-21.2088], device='cuda:0'))])
epoch£º1107	 i:0 	 global-step:22140	 l-p:0.10855928063392639
epoch£º1107	 i:1 	 global-step:22141	 l-p:0.13510826230049133
epoch£º1107	 i:2 	 global-step:22142	 l-p:0.1686081439256668
epoch£º1107	 i:3 	 global-step:22143	 l-p:0.11994355171918869
epoch£º1107	 i:4 	 global-step:22144	 l-p:0.19078534841537476
epoch£º1107	 i:5 	 global-step:22145	 l-p:0.16128233075141907
epoch£º1107	 i:6 	 global-step:22146	 l-p:0.12649571895599365
epoch£º1107	 i:7 	 global-step:22147	 l-p:0.13078896701335907
epoch£º1107	 i:8 	 global-step:22148	 l-p:0.11430768668651581
epoch£º1107	 i:9 	 global-step:22149	 l-p:0.17131051421165466
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1108
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9071e-01, 2.8563e-01,
         1.0000e+00, 2.0881e-01, 1.0000e+00, 7.3106e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1575, 4.8676, 4.7454],
        [5.1575, 5.1575, 5.1575],
        [5.1575, 4.9178, 4.9132],
        [5.1575, 4.8680, 4.6275]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1108, step:0 
model_pd.l_p.mean(): 0.1387803852558136 
model_pd.l_d.mean(): -20.077159881591797 
model_pd.lagr.mean(): -19.938379287719727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4823], device='cuda:0')), ('power', tensor([-20.9521], device='cuda:0'))])
epoch£º1108	 i:0 	 global-step:22160	 l-p:0.1387803852558136
epoch£º1108	 i:1 	 global-step:22161	 l-p:0.19145150482654572
epoch£º1108	 i:2 	 global-step:22162	 l-p:0.17478685081005096
epoch£º1108	 i:3 	 global-step:22163	 l-p:0.11956195533275604
epoch£º1108	 i:4 	 global-step:22164	 l-p:0.1749906837940216
epoch£º1108	 i:5 	 global-step:22165	 l-p:0.12204461544752121
epoch£º1108	 i:6 	 global-step:22166	 l-p:0.09498853236436844
epoch£º1108	 i:7 	 global-step:22167	 l-p:0.09948298335075378
epoch£º1108	 i:8 	 global-step:22168	 l-p:0.16435202956199646
epoch£º1108	 i:9 	 global-step:22169	 l-p:0.13197538256645203
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1109
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8275e-03, 3.9983e-04,
         1.0000e+00, 5.6539e-05, 1.0000e+00, 1.4141e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1635, 5.2473, 4.9761],
        [5.1635, 5.1634, 5.1635],
        [5.1635, 4.9671, 5.0052],
        [5.1635, 4.9395, 4.9535]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1109, step:0 
model_pd.l_p.mean(): 0.15861843526363373 
model_pd.l_d.mean(): -20.17009925842285 
model_pd.lagr.mean(): -20.0114803314209 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4709], device='cuda:0')), ('power', tensor([-21.0349], device='cuda:0'))])
epoch£º1109	 i:0 	 global-step:22180	 l-p:0.15861843526363373
epoch£º1109	 i:1 	 global-step:22181	 l-p:0.19043870270252228
epoch£º1109	 i:2 	 global-step:22182	 l-p:0.13651227951049805
epoch£º1109	 i:3 	 global-step:22183	 l-p:0.14557255804538727
epoch£º1109	 i:4 	 global-step:22184	 l-p:0.15653711557388306
epoch£º1109	 i:5 	 global-step:22185	 l-p:0.1256384551525116
epoch£º1109	 i:6 	 global-step:22186	 l-p:0.10373146086931229
epoch£º1109	 i:7 	 global-step:22187	 l-p:0.08650243282318115
epoch£º1109	 i:8 	 global-step:22188	 l-p:0.13984447717666626
epoch£º1109	 i:9 	 global-step:22189	 l-p:0.1283015012741089
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1110
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1649,  0.0904,  1.0000,  0.0496,
          1.0000,  0.5484, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5591,  0.4606,  1.0000,  0.3795,
          1.0000,  0.8238, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7173,  0.6420,  1.0000,  0.5747,
          1.0000,  0.8951, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7511,  0.6828,  1.0000,  0.6206,
          1.0000,  0.9090, 31.6228]], device='cuda:0')
 pt:tensor([[5.1787, 4.9955, 5.0412],
        [5.1787, 5.0109, 4.6712],
        [5.1787, 5.2091, 4.9138],
        [5.1787, 5.2580, 4.9845]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1110, step:0 
model_pd.l_p.mean(): 0.17122958600521088 
model_pd.l_d.mean(): -20.053586959838867 
model_pd.lagr.mean(): -19.882356643676758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5043], device='cuda:0')), ('power', tensor([-20.9509], device='cuda:0'))])
epoch£º1110	 i:0 	 global-step:22200	 l-p:0.17122958600521088
epoch£º1110	 i:1 	 global-step:22201	 l-p:0.12017909437417984
epoch£º1110	 i:2 	 global-step:22202	 l-p:0.10690175741910934
epoch£º1110	 i:3 	 global-step:22203	 l-p:0.1734352707862854
epoch£º1110	 i:4 	 global-step:22204	 l-p:0.15620285272598267
epoch£º1110	 i:5 	 global-step:22205	 l-p:0.13447649776935577
epoch£º1110	 i:6 	 global-step:22206	 l-p:0.1266435831785202
epoch£º1110	 i:7 	 global-step:22207	 l-p:0.08689336478710175
epoch£º1110	 i:8 	 global-step:22208	 l-p:0.1172432228922844
epoch£º1110	 i:9 	 global-step:22209	 l-p:0.13229136168956757
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1111
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7294e-01, 5.8970e-01,
         1.0000e+00, 5.1676e-01, 1.0000e+00, 8.7631e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1989e-04, 5.9117e-06,
         1.0000e+00, 2.9150e-07, 1.0000e+00, 4.9309e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1869, 5.1582, 4.8405],
        [5.1869, 5.1480, 5.1791],
        [5.1869, 5.1869, 5.1869],
        [5.1869, 4.8941, 4.7281]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1111, step:0 
model_pd.l_p.mean(): 0.1322702020406723 
model_pd.l_d.mean(): -18.714929580688477 
model_pd.lagr.mean(): -18.582658767700195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5357], device='cuda:0')), ('power', tensor([-19.6198], device='cuda:0'))])
epoch£º1111	 i:0 	 global-step:22220	 l-p:0.1322702020406723
epoch£º1111	 i:1 	 global-step:22221	 l-p:0.09338879585266113
epoch£º1111	 i:2 	 global-step:22222	 l-p:0.14784349501132965
epoch£º1111	 i:3 	 global-step:22223	 l-p:0.15740102529525757
epoch£º1111	 i:4 	 global-step:22224	 l-p:0.13272510468959808
epoch£º1111	 i:5 	 global-step:22225	 l-p:0.18501971662044525
epoch£º1111	 i:6 	 global-step:22226	 l-p:0.128700390458107
epoch£º1111	 i:7 	 global-step:22227	 l-p:0.10781177133321762
epoch£º1111	 i:8 	 global-step:22228	 l-p:0.06041151285171509
epoch£º1111	 i:9 	 global-step:22229	 l-p:0.20082014799118042
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1112
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4713,  0.3668,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5018,  0.3987,  1.0000,  0.3168,
          1.0000,  0.7946, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2509,  0.1582,  1.0000,  0.0998,
          1.0000,  0.6307, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[5.1693, 4.9218, 4.6122],
        [5.1693, 4.9453, 4.6199],
        [5.1693, 4.9037, 4.8535],
        [5.1693, 4.9219, 4.6122]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1112, step:0 
model_pd.l_p.mean(): 0.13173769414424896 
model_pd.l_d.mean(): -19.687902450561523 
model_pd.lagr.mean(): -19.55616569519043 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4918], device='cuda:0')), ('power', tensor([-20.5654], device='cuda:0'))])
epoch£º1112	 i:0 	 global-step:22240	 l-p:0.13173769414424896
epoch£º1112	 i:1 	 global-step:22241	 l-p:0.1953713595867157
epoch£º1112	 i:2 	 global-step:22242	 l-p:0.07560956478118896
epoch£º1112	 i:3 	 global-step:22243	 l-p:0.18661965429782867
epoch£º1112	 i:4 	 global-step:22244	 l-p:0.1659892201423645
epoch£º1112	 i:5 	 global-step:22245	 l-p:0.11701247841119766
epoch£º1112	 i:6 	 global-step:22246	 l-p:0.09483732283115387
epoch£º1112	 i:7 	 global-step:22247	 l-p:0.13431905210018158
epoch£º1112	 i:8 	 global-step:22248	 l-p:0.131961852312088
epoch£º1112	 i:9 	 global-step:22249	 l-p:0.13110163807868958
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1113
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3185e-01, 1.4243e-01,
         1.0000e+00, 8.7500e-02, 1.0000e+00, 6.1433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1742, 5.1729, 5.1742],
        [5.1742, 5.1736, 5.1742],
        [5.1742, 5.1742, 5.1742],
        [5.1742, 4.9226, 4.8985]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1113, step:0 
model_pd.l_p.mean(): 0.16452965140342712 
model_pd.l_d.mean(): -19.265766143798828 
model_pd.lagr.mean(): -19.10123634338379 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5289], device='cuda:0')), ('power', tensor([-20.1739], device='cuda:0'))])
epoch£º1113	 i:0 	 global-step:22260	 l-p:0.16452965140342712
epoch£º1113	 i:1 	 global-step:22261	 l-p:0.164480060338974
epoch£º1113	 i:2 	 global-step:22262	 l-p:0.12599299848079681
epoch£º1113	 i:3 	 global-step:22263	 l-p:0.1318926066160202
epoch£º1113	 i:4 	 global-step:22264	 l-p:0.10777168720960617
epoch£º1113	 i:5 	 global-step:22265	 l-p:0.15634335577487946
epoch£º1113	 i:6 	 global-step:22266	 l-p:0.12769471108913422
epoch£º1113	 i:7 	 global-step:22267	 l-p:0.11144968867301941
epoch£º1113	 i:8 	 global-step:22268	 l-p:0.16292735934257507
epoch£º1113	 i:9 	 global-step:22269	 l-p:0.10895423591136932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1114
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1681, 5.0848, 5.1380],
        [5.1681, 4.8727, 4.7012],
        [5.1681, 4.8896, 4.6210],
        [5.1681, 5.1651, 5.1680]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1114, step:0 
model_pd.l_p.mean(): 0.13076339662075043 
model_pd.l_d.mean(): -18.35942268371582 
model_pd.lagr.mean(): -18.22865867614746 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5401], device='cuda:0')), ('power', tensor([-19.2622], device='cuda:0'))])
epoch£º1114	 i:0 	 global-step:22280	 l-p:0.13076339662075043
epoch£º1114	 i:1 	 global-step:22281	 l-p:0.15329009294509888
epoch£º1114	 i:2 	 global-step:22282	 l-p:0.1547452211380005
epoch£º1114	 i:3 	 global-step:22283	 l-p:0.1496657133102417
epoch£º1114	 i:4 	 global-step:22284	 l-p:0.12431158125400543
epoch£º1114	 i:5 	 global-step:22285	 l-p:0.15744994580745697
epoch£º1114	 i:6 	 global-step:22286	 l-p:0.17001311480998993
epoch£º1114	 i:7 	 global-step:22287	 l-p:0.13865742087364197
epoch£º1114	 i:8 	 global-step:22288	 l-p:0.10372146964073181
epoch£º1114	 i:9 	 global-step:22289	 l-p:0.09420893341302872
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1115
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1496e-02, 5.9771e-03,
         1.0000e+00, 1.6619e-03, 1.0000e+00, 2.7805e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1671, 4.9808, 4.6421],
        [5.1671, 5.1602, 5.1666],
        [5.1671, 5.2424, 4.9668],
        [5.1671, 5.1525, 5.1656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1115, step:0 
model_pd.l_p.mean(): 0.12125098705291748 
model_pd.l_d.mean(): -19.98427391052246 
model_pd.lagr.mean(): -19.86302375793457 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4930], device='cuda:0')), ('power', tensor([-20.8685], device='cuda:0'))])
epoch£º1115	 i:0 	 global-step:22300	 l-p:0.12125098705291748
epoch£º1115	 i:1 	 global-step:22301	 l-p:0.13879594206809998
epoch£º1115	 i:2 	 global-step:22302	 l-p:0.12970976531505585
epoch£º1115	 i:3 	 global-step:22303	 l-p:0.12532222270965576
epoch£º1115	 i:4 	 global-step:22304	 l-p:0.21928705275058746
epoch£º1115	 i:5 	 global-step:22305	 l-p:0.152835875749588
epoch£º1115	 i:6 	 global-step:22306	 l-p:0.1417572796344757
epoch£º1115	 i:7 	 global-step:22307	 l-p:0.11968962103128433
epoch£º1115	 i:8 	 global-step:22308	 l-p:0.12472514808177948
epoch£º1115	 i:9 	 global-step:22309	 l-p:0.10789581388235092
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1116
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6023e-01, 3.5533e-01,
         1.0000e+00, 2.7434e-01, 1.0000e+00, 7.7207e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1656, 4.9369, 4.9460],
        [5.1656, 4.9144, 4.8925],
        [5.1656, 4.9095, 4.6067],
        [5.1656, 5.1656, 5.1656]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1116, step:0 
model_pd.l_p.mean(): 0.13907429575920105 
model_pd.l_d.mean(): -20.348827362060547 
model_pd.lagr.mean(): -20.209753036499023 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4430], device='cuda:0')), ('power', tensor([-21.1881], device='cuda:0'))])
epoch£º1116	 i:0 	 global-step:22320	 l-p:0.13907429575920105
epoch£º1116	 i:1 	 global-step:22321	 l-p:0.17969466745853424
epoch£º1116	 i:2 	 global-step:22322	 l-p:0.12317144125699997
epoch£º1116	 i:3 	 global-step:22323	 l-p:0.17945201694965363
epoch£º1116	 i:4 	 global-step:22324	 l-p:0.15128417313098907
epoch£º1116	 i:5 	 global-step:22325	 l-p:0.1279243528842926
epoch£º1116	 i:6 	 global-step:22326	 l-p:0.11047938466072083
epoch£º1116	 i:7 	 global-step:22327	 l-p:0.10596885532140732
epoch£º1116	 i:8 	 global-step:22328	 l-p:0.13942526280879974
epoch£º1116	 i:9 	 global-step:22329	 l-p:0.14009612798690796
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1117
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5859e-02, 3.2113e-02,
         1.0000e+00, 1.3594e-02, 1.0000e+00, 4.2332e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1633, 5.0998, 5.1449],
        [5.1633, 5.1550, 5.1627],
        [5.1633, 4.9030, 4.8653],
        [5.1633, 5.1632, 5.1633]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1117, step:0 
model_pd.l_p.mean(): 0.13459454476833344 
model_pd.l_d.mean(): -20.550048828125 
model_pd.lagr.mean(): -20.415454864501953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4180], device='cuda:0')), ('power', tensor([-21.3672], device='cuda:0'))])
epoch£º1117	 i:0 	 global-step:22340	 l-p:0.13459454476833344
epoch£º1117	 i:1 	 global-step:22341	 l-p:0.1253625899553299
epoch£º1117	 i:2 	 global-step:22342	 l-p:0.21554076671600342
epoch£º1117	 i:3 	 global-step:22343	 l-p:0.05272751674056053
epoch£º1117	 i:4 	 global-step:22344	 l-p:0.17634910345077515
epoch£º1117	 i:5 	 global-step:22345	 l-p:0.1711234301328659
epoch£º1117	 i:6 	 global-step:22346	 l-p:0.10800712555646896
epoch£º1117	 i:7 	 global-step:22347	 l-p:0.10459237545728683
epoch£º1117	 i:8 	 global-step:22348	 l-p:0.18133844435214996
epoch£º1117	 i:9 	 global-step:22349	 l-p:0.12970054149627686
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1118
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1641, 5.1586, 5.1638],
        [5.1641, 4.9872, 5.0367],
        [5.1641, 5.0730, 5.1286],
        [5.1641, 5.1613, 5.1640]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1118, step:0 
model_pd.l_p.mean(): 0.11984622478485107 
model_pd.l_d.mean(): -19.109943389892578 
model_pd.lagr.mean(): -18.990097045898438 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5052], device='cuda:0')), ('power', tensor([-19.9906], device='cuda:0'))])
epoch£º1118	 i:0 	 global-step:22360	 l-p:0.11984622478485107
epoch£º1118	 i:1 	 global-step:22361	 l-p:0.1280694156885147
epoch£º1118	 i:2 	 global-step:22362	 l-p:0.1006079614162445
epoch£º1118	 i:3 	 global-step:22363	 l-p:0.12564478814601898
epoch£º1118	 i:4 	 global-step:22364	 l-p:0.146076500415802
epoch£º1118	 i:5 	 global-step:22365	 l-p:0.1729847639799118
epoch£º1118	 i:6 	 global-step:22366	 l-p:0.20141181349754333
epoch£º1118	 i:7 	 global-step:22367	 l-p:0.18524368107318878
epoch£º1118	 i:8 	 global-step:22368	 l-p:0.12358690053224564
epoch£º1118	 i:9 	 global-step:22369	 l-p:0.11094828695058823
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1119
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2249e-01, 1.3482e-01,
         1.0000e+00, 8.1691e-02, 1.0000e+00, 6.0595e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1594, 4.9143, 4.9027],
        [5.1594, 5.2833, 5.0317],
        [5.1594, 5.1541, 5.1591],
        [5.1594, 5.5171, 5.4024]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1119, step:0 
model_pd.l_p.mean(): 0.10814910382032394 
model_pd.l_d.mean(): -19.121957778930664 
model_pd.lagr.mean(): -19.013809204101562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5665], device='cuda:0')), ('power', tensor([-20.0663], device='cuda:0'))])
epoch£º1119	 i:0 	 global-step:22380	 l-p:0.10814910382032394
epoch£º1119	 i:1 	 global-step:22381	 l-p:0.1250297576189041
epoch£º1119	 i:2 	 global-step:22382	 l-p:0.16027586162090302
epoch£º1119	 i:3 	 global-step:22383	 l-p:0.16334019601345062
epoch£º1119	 i:4 	 global-step:22384	 l-p:0.15407535433769226
epoch£º1119	 i:5 	 global-step:22385	 l-p:0.11108846217393875
epoch£º1119	 i:6 	 global-step:22386	 l-p:0.11814483255147934
epoch£º1119	 i:7 	 global-step:22387	 l-p:0.14617584645748138
epoch£º1119	 i:8 	 global-step:22388	 l-p:0.1499370038509369
epoch£º1119	 i:9 	 global-step:22389	 l-p:0.1583675593137741
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1120
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6007e-01, 6.9365e-01,
         1.0000e+00, 6.3303e-01, 1.0000e+00, 9.1261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1715, 4.8863, 4.6339],
        [5.1715, 5.0452, 5.1058],
        [5.1715, 5.1213, 5.1593],
        [5.1715, 5.2607, 4.9916]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1120, step:0 
model_pd.l_p.mean(): 0.14522328972816467 
model_pd.l_d.mean(): -19.284263610839844 
model_pd.lagr.mean(): -19.139039993286133 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5467], device='cuda:0')), ('power', tensor([-20.2111], device='cuda:0'))])
epoch£º1120	 i:0 	 global-step:22400	 l-p:0.14522328972816467
epoch£º1120	 i:1 	 global-step:22401	 l-p:0.18148906528949738
epoch£º1120	 i:2 	 global-step:22402	 l-p:0.07594571262598038
epoch£º1120	 i:3 	 global-step:22403	 l-p:0.1212380975484848
epoch£º1120	 i:4 	 global-step:22404	 l-p:0.14936760067939758
epoch£º1120	 i:5 	 global-step:22405	 l-p:0.12607155740261078
epoch£º1120	 i:6 	 global-step:22406	 l-p:0.14167189598083496
epoch£º1120	 i:7 	 global-step:22407	 l-p:0.14013908803462982
epoch£º1120	 i:8 	 global-step:22408	 l-p:0.1321944296360016
epoch£º1120	 i:9 	 global-step:22409	 l-p:0.13262684643268585
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1121
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1868, 5.0648, 4.7252],
        [5.1868, 5.2108, 4.9124],
        [5.1868, 5.1759, 5.1859],
        [5.1868, 5.1865, 5.1868]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1121, step:0 
model_pd.l_p.mean(): 0.12161038815975189 
model_pd.l_d.mean(): -20.742877960205078 
model_pd.lagr.mean(): -20.621267318725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3949], device='cuda:0')), ('power', tensor([-21.5397], device='cuda:0'))])
epoch£º1121	 i:0 	 global-step:22420	 l-p:0.12161038815975189
epoch£º1121	 i:1 	 global-step:22421	 l-p:0.14947724342346191
epoch£º1121	 i:2 	 global-step:22422	 l-p:0.11283239722251892
epoch£º1121	 i:3 	 global-step:22423	 l-p:0.13724461197853088
epoch£º1121	 i:4 	 global-step:22424	 l-p:0.13006514310836792
epoch£º1121	 i:5 	 global-step:22425	 l-p:0.09920451790094376
epoch£º1121	 i:6 	 global-step:22426	 l-p:0.16955985128879547
epoch£º1121	 i:7 	 global-step:22427	 l-p:0.10334490239620209
epoch£º1121	 i:8 	 global-step:22428	 l-p:0.1255088895559311
epoch£º1121	 i:9 	 global-step:22429	 l-p:0.17921453714370728
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1122
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1732e-02, 1.9276e-02,
         1.0000e+00, 7.1823e-03, 1.0000e+00, 3.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1778, 5.1344, 5.1684],
        [5.1778, 5.1441, 5.1718],
        [5.1778, 5.0883, 5.1434],
        [5.1778, 5.4365, 5.2597]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1122, step:0 
model_pd.l_p.mean(): 0.15972298383712769 
model_pd.l_d.mean(): -19.85771942138672 
model_pd.lagr.mean(): -19.697996139526367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5264], device='cuda:0')), ('power', tensor([-20.7742], device='cuda:0'))])
epoch£º1122	 i:0 	 global-step:22440	 l-p:0.15972298383712769
epoch£º1122	 i:1 	 global-step:22441	 l-p:0.16162723302841187
epoch£º1122	 i:2 	 global-step:22442	 l-p:0.10617729276418686
epoch£º1122	 i:3 	 global-step:22443	 l-p:0.10702281445264816
epoch£º1122	 i:4 	 global-step:22444	 l-p:0.13028888404369354
epoch£º1122	 i:5 	 global-step:22445	 l-p:0.1400720328092575
epoch£º1122	 i:6 	 global-step:22446	 l-p:0.18523700535297394
epoch£º1122	 i:7 	 global-step:22447	 l-p:0.13289587199687958
epoch£º1122	 i:8 	 global-step:22448	 l-p:0.11514498293399811
epoch£º1122	 i:9 	 global-step:22449	 l-p:0.09545300155878067
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1123
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2290e-01, 4.2126e-01,
         1.0000e+00, 3.3938e-01, 1.0000e+00, 8.0563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1817, 4.8971, 4.6465],
        [5.1817, 4.9775, 4.6444],
        [5.1817, 5.0923, 5.1473],
        [5.1817, 5.1762, 5.1814]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1123, step:0 
model_pd.l_p.mean(): 0.11699526757001877 
model_pd.l_d.mean(): -20.212020874023438 
model_pd.lagr.mean(): -20.09502601623535 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4800], device='cuda:0')), ('power', tensor([-21.0871], device='cuda:0'))])
epoch£º1123	 i:0 	 global-step:22460	 l-p:0.11699526757001877
epoch£º1123	 i:1 	 global-step:22461	 l-p:0.10994315147399902
epoch£º1123	 i:2 	 global-step:22462	 l-p:0.16349700093269348
epoch£º1123	 i:3 	 global-step:22463	 l-p:0.11075523495674133
epoch£º1123	 i:4 	 global-step:22464	 l-p:0.1191016361117363
epoch£º1123	 i:5 	 global-step:22465	 l-p:0.15515007078647614
epoch£º1123	 i:6 	 global-step:22466	 l-p:0.10067606717348099
epoch£º1123	 i:7 	 global-step:22467	 l-p:0.148720383644104
epoch£º1123	 i:8 	 global-step:22468	 l-p:0.1765286773443222
epoch£º1123	 i:9 	 global-step:22469	 l-p:0.12771093845367432
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1124
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5301e-01, 4.5392e-01,
         1.0000e+00, 3.7258e-01, 1.0000e+00, 8.2081e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1793, 5.0037, 4.6638],
        [5.1793, 5.0598, 5.1201],
        [5.1793, 4.9089, 4.8479],
        [5.1793, 5.0742, 5.1329]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1124, step:0 
model_pd.l_p.mean(): 0.16062895953655243 
model_pd.l_d.mean(): -20.51519012451172 
model_pd.lagr.mean(): -20.35456085205078 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4214], device='cuda:0')), ('power', tensor([-21.3353], device='cuda:0'))])
epoch£º1124	 i:0 	 global-step:22480	 l-p:0.16062895953655243
epoch£º1124	 i:1 	 global-step:22481	 l-p:0.10721484571695328
epoch£º1124	 i:2 	 global-step:22482	 l-p:0.13018099963665009
epoch£º1124	 i:3 	 global-step:22483	 l-p:0.11456973850727081
epoch£º1124	 i:4 	 global-step:22484	 l-p:0.13691125810146332
epoch£º1124	 i:5 	 global-step:22485	 l-p:0.088474340736866
epoch£º1124	 i:6 	 global-step:22486	 l-p:0.1485535353422165
epoch£º1124	 i:7 	 global-step:22487	 l-p:0.12696941196918488
epoch£º1124	 i:8 	 global-step:22488	 l-p:0.1712920367717743
epoch£º1124	 i:9 	 global-step:22489	 l-p:0.18346057832241058
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1125
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3022e-01, 2.2824e-01,
         1.0000e+00, 1.5776e-01, 1.0000e+00, 6.9119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1685, 5.0745, 5.1309],
        [5.1685, 4.8723, 4.6756],
        [5.1685, 4.8726, 4.7077],
        [5.1685, 5.1594, 5.1678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1125, step:0 
model_pd.l_p.mean(): 0.1579209715127945 
model_pd.l_d.mean(): -20.726037979125977 
model_pd.lagr.mean(): -20.568117141723633 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4149], device='cuda:0')), ('power', tensor([-21.5434], device='cuda:0'))])
epoch£º1125	 i:0 	 global-step:22500	 l-p:0.1579209715127945
epoch£º1125	 i:1 	 global-step:22501	 l-p:0.15367688238620758
epoch£º1125	 i:2 	 global-step:22502	 l-p:0.12763682007789612
epoch£º1125	 i:3 	 global-step:22503	 l-p:0.11039745062589645
epoch£º1125	 i:4 	 global-step:22504	 l-p:0.13923552632331848
epoch£º1125	 i:5 	 global-step:22505	 l-p:0.1407262235879898
epoch£º1125	 i:6 	 global-step:22506	 l-p:0.1544928103685379
epoch£º1125	 i:7 	 global-step:22507	 l-p:0.08591601997613907
epoch£º1125	 i:8 	 global-step:22508	 l-p:0.14156825840473175
epoch£º1125	 i:9 	 global-step:22509	 l-p:0.15885308384895325
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1126
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0049e-01, 2.0126e-01,
         1.0000e+00, 1.3481e-01, 1.0000e+00, 6.6979e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1736, 5.1736, 5.1737],
        [5.1736, 5.0836, 5.1389],
        [5.1736, 5.0050, 5.0579],
        [5.1736, 4.8840, 4.7614]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1126, step:0 
model_pd.l_p.mean(): 0.1055835410952568 
model_pd.l_d.mean(): -20.57750129699707 
model_pd.lagr.mean(): -20.4719181060791 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4326], device='cuda:0')), ('power', tensor([-21.4103], device='cuda:0'))])
epoch£º1126	 i:0 	 global-step:22520	 l-p:0.1055835410952568
epoch£º1126	 i:1 	 global-step:22521	 l-p:0.2072509527206421
epoch£º1126	 i:2 	 global-step:22522	 l-p:0.1721140593290329
epoch£º1126	 i:3 	 global-step:22523	 l-p:0.10911642760038376
epoch£º1126	 i:4 	 global-step:22524	 l-p:0.11796008795499802
epoch£º1126	 i:5 	 global-step:22525	 l-p:0.12925541400909424
epoch£º1126	 i:6 	 global-step:22526	 l-p:0.1595829874277115
epoch£º1126	 i:7 	 global-step:22527	 l-p:0.11725368350744247
epoch£º1126	 i:8 	 global-step:22528	 l-p:0.1293438971042633
epoch£º1126	 i:9 	 global-step:22529	 l-p:0.09074723720550537
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1127
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1916e-01, 2.1811e-01,
         1.0000e+00, 1.4906e-01, 1.0000e+00, 6.8339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1844, 4.8914, 4.7419],
        [5.1844, 5.1839, 5.1844],
        [5.1844, 5.1842, 5.1844],
        [5.1844, 5.0004, 4.6621]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1127, step:0 
model_pd.l_p.mean(): 0.17599089443683624 
model_pd.l_d.mean(): -20.535579681396484 
model_pd.lagr.mean(): -20.359588623046875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4511], device='cuda:0')), ('power', tensor([-21.3867], device='cuda:0'))])
epoch£º1127	 i:0 	 global-step:22540	 l-p:0.17599089443683624
epoch£º1127	 i:1 	 global-step:22541	 l-p:0.07755587249994278
epoch£º1127	 i:2 	 global-step:22542	 l-p:0.13306352496147156
epoch£º1127	 i:3 	 global-step:22543	 l-p:0.11178595572710037
epoch£º1127	 i:4 	 global-step:22544	 l-p:0.11201226711273193
epoch£º1127	 i:5 	 global-step:22545	 l-p:0.1528155505657196
epoch£º1127	 i:6 	 global-step:22546	 l-p:0.1288726031780243
epoch£º1127	 i:7 	 global-step:22547	 l-p:0.14707067608833313
epoch£º1127	 i:8 	 global-step:22548	 l-p:0.11044580489397049
epoch£º1127	 i:9 	 global-step:22549	 l-p:0.17898643016815186
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1128
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1783, 5.1783, 5.1783],
        [5.1783, 5.1783, 5.1783],
        [5.1783, 5.1227, 5.1638],
        [5.1783, 4.8828, 4.7044]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1128, step:0 
model_pd.l_p.mean(): 0.18367095291614532 
model_pd.l_d.mean(): -18.654911041259766 
model_pd.lagr.mean(): -18.471240997314453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5534], device='cuda:0')), ('power', tensor([-19.5769], device='cuda:0'))])
epoch£º1128	 i:0 	 global-step:22560	 l-p:0.18367095291614532
epoch£º1128	 i:1 	 global-step:22561	 l-p:0.14774155616760254
epoch£º1128	 i:2 	 global-step:22562	 l-p:0.11833316832780838
epoch£º1128	 i:3 	 global-step:22563	 l-p:0.10960143804550171
epoch£º1128	 i:4 	 global-step:22564	 l-p:0.12284432351589203
epoch£º1128	 i:5 	 global-step:22565	 l-p:0.1594810038805008
epoch£º1128	 i:6 	 global-step:22566	 l-p:0.10024777799844742
epoch£º1128	 i:7 	 global-step:22567	 l-p:0.11924679577350616
epoch£º1128	 i:8 	 global-step:22568	 l-p:0.15970073640346527
epoch£º1128	 i:9 	 global-step:22569	 l-p:0.12911809980869293
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1129
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1708, 5.1708, 5.1709],
        [5.1708, 5.0782, 5.1342],
        [5.1708, 5.1584, 5.1697],
        [5.1708, 5.1547, 5.1691]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1129, step:0 
model_pd.l_p.mean(): 0.11478083580732346 
model_pd.l_d.mean(): -19.558523178100586 
model_pd.lagr.mean(): -19.443742752075195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4597], device='cuda:0')), ('power', tensor([-20.4003], device='cuda:0'))])
epoch£º1129	 i:0 	 global-step:22580	 l-p:0.11478083580732346
epoch£º1129	 i:1 	 global-step:22581	 l-p:0.18394990265369415
epoch£º1129	 i:2 	 global-step:22582	 l-p:0.11322110891342163
epoch£º1129	 i:3 	 global-step:22583	 l-p:0.12396952509880066
epoch£º1129	 i:4 	 global-step:22584	 l-p:0.13749901950359344
epoch£º1129	 i:5 	 global-step:22585	 l-p:0.1506899744272232
epoch£º1129	 i:6 	 global-step:22586	 l-p:0.18440872430801392
epoch£º1129	 i:7 	 global-step:22587	 l-p:0.19617433845996857
epoch£º1129	 i:8 	 global-step:22588	 l-p:0.08641079813241959
epoch£º1129	 i:9 	 global-step:22589	 l-p:0.08952350169420242
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1130
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6041e-01, 8.1836e-01,
         1.0000e+00, 7.7836e-01, 1.0000e+00, 9.5112e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1669, 5.4068, 5.2187],
        [5.1669, 5.1669, 5.1669],
        [5.1669, 5.4369, 5.2669],
        [5.1669, 4.9152, 4.8936]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1130, step:0 
model_pd.l_p.mean(): 0.1723194420337677 
model_pd.l_d.mean(): -19.066650390625 
model_pd.lagr.mean(): -18.894330978393555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5601], device='cuda:0')), ('power', tensor([-20.0032], device='cuda:0'))])
epoch£º1130	 i:0 	 global-step:22600	 l-p:0.1723194420337677
epoch£º1130	 i:1 	 global-step:22601	 l-p:0.14159494638442993
epoch£º1130	 i:2 	 global-step:22602	 l-p:0.11314420402050018
epoch£º1130	 i:3 	 global-step:22603	 l-p:0.11543384939432144
epoch£º1130	 i:4 	 global-step:22604	 l-p:0.16640149056911469
epoch£º1130	 i:5 	 global-step:22605	 l-p:0.1796119511127472
epoch£º1130	 i:6 	 global-step:22606	 l-p:0.11771955341100693
epoch£º1130	 i:7 	 global-step:22607	 l-p:0.09528056532144547
epoch£º1130	 i:8 	 global-step:22608	 l-p:0.12408101558685303
epoch£º1130	 i:9 	 global-step:22609	 l-p:0.14672169089317322
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1131
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3993e-01, 6.6924e-01,
         1.0000e+00, 6.0531e-01, 1.0000e+00, 9.0447e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1722, 5.1722, 5.1722],
        [5.1722, 5.1287, 5.1628],
        [5.1722, 5.2310, 4.9473],
        [5.1722, 5.0766, 5.1334]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1131, step:0 
model_pd.l_p.mean(): 0.1615307629108429 
model_pd.l_d.mean(): -19.818340301513672 
model_pd.lagr.mean(): -19.656808853149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4928], device='cuda:0')), ('power', tensor([-20.6993], device='cuda:0'))])
epoch£º1131	 i:0 	 global-step:22620	 l-p:0.1615307629108429
epoch£º1131	 i:1 	 global-step:22621	 l-p:0.14240311086177826
epoch£º1131	 i:2 	 global-step:22622	 l-p:0.13256952166557312
epoch£º1131	 i:3 	 global-step:22623	 l-p:0.10752537846565247
epoch£º1131	 i:4 	 global-step:22624	 l-p:0.12915198504924774
epoch£º1131	 i:5 	 global-step:22625	 l-p:0.18931438028812408
epoch£º1131	 i:6 	 global-step:22626	 l-p:0.10025840252637863
epoch£º1131	 i:7 	 global-step:22627	 l-p:0.09474721550941467
epoch£º1131	 i:8 	 global-step:22628	 l-p:0.18778502941131592
epoch£º1131	 i:9 	 global-step:22629	 l-p:0.12301792204380035
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1132
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.8696,  0.8300,  1.0000,  0.7922,
          1.0000,  0.9545, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2352,  0.1452,  1.0000,  0.0896,
          1.0000,  0.6173, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4518,  0.3467,  1.0000,  0.2660,
          1.0000,  0.7673, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2742,  0.1782,  1.0000,  0.1158,
          1.0000,  0.6497, 31.6228]], device='cuda:0')
 pt:tensor([[5.1730, 5.4290, 5.2502],
        [5.1730, 4.9176, 4.8891],
        [5.1730, 4.9114, 4.6140],
        [5.1730, 4.8931, 4.8090]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1132, step:0 
model_pd.l_p.mean(): 0.12918774783611298 
model_pd.l_d.mean(): -20.423656463623047 
model_pd.lagr.mean(): -20.29446792602539 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4423], device='cuda:0')), ('power', tensor([-21.2636], device='cuda:0'))])
epoch£º1132	 i:0 	 global-step:22640	 l-p:0.12918774783611298
epoch£º1132	 i:1 	 global-step:22641	 l-p:0.17392797768115997
epoch£º1132	 i:2 	 global-step:22642	 l-p:0.1591804027557373
epoch£º1132	 i:3 	 global-step:22643	 l-p:0.1666010320186615
epoch£º1132	 i:4 	 global-step:22644	 l-p:0.06914966553449631
epoch£º1132	 i:5 	 global-step:22645	 l-p:0.11447896808385849
epoch£º1132	 i:6 	 global-step:22646	 l-p:0.13057269155979156
epoch£º1132	 i:7 	 global-step:22647	 l-p:0.13676340878009796
epoch£º1132	 i:8 	 global-step:22648	 l-p:0.11399117112159729
epoch£º1132	 i:9 	 global-step:22649	 l-p:0.15673521161079407
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1133
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9462e-01, 1.1278e-01,
         1.0000e+00, 6.5359e-02, 1.0000e+00, 5.7951e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1815, 5.1815, 5.1815],
        [5.1815, 4.9576, 4.6315],
        [5.1815, 4.9637, 4.9841],
        [5.1815, 5.1323, 5.1698]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1133, step:0 
model_pd.l_p.mean(): 0.08250001817941666 
model_pd.l_d.mean(): -19.864133834838867 
model_pd.lagr.mean(): -19.781633377075195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4932], device='cuda:0')), ('power', tensor([-20.7464], device='cuda:0'))])
epoch£º1133	 i:0 	 global-step:22660	 l-p:0.08250001817941666
epoch£º1133	 i:1 	 global-step:22661	 l-p:0.13037852942943573
epoch£º1133	 i:2 	 global-step:22662	 l-p:0.14397354423999786
epoch£º1133	 i:3 	 global-step:22663	 l-p:0.1243891716003418
epoch£º1133	 i:4 	 global-step:22664	 l-p:0.15131787955760956
epoch£º1133	 i:5 	 global-step:22665	 l-p:0.12559211254119873
epoch£º1133	 i:6 	 global-step:22666	 l-p:0.1120021790266037
epoch£º1133	 i:7 	 global-step:22667	 l-p:0.1678539365530014
epoch£º1133	 i:8 	 global-step:22668	 l-p:0.1158679649233818
epoch£º1133	 i:9 	 global-step:22669	 l-p:0.16452987492084503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1134
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3037e-01, 1.4122e-01,
         1.0000e+00, 8.6569e-02, 1.0000e+00, 6.1302e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1895, 4.9389, 4.9166],
        [5.1895, 4.9617, 4.9710],
        [5.1895, 5.0985, 5.1540],
        [5.1895, 5.0109, 5.0593]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1134, step:0 
model_pd.l_p.mean(): 0.1598401516675949 
model_pd.l_d.mean(): -20.14531898498535 
model_pd.lagr.mean(): -19.9854793548584 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4854], device='cuda:0')), ('power', tensor([-21.0247], device='cuda:0'))])
epoch£º1134	 i:0 	 global-step:22680	 l-p:0.1598401516675949
epoch£º1134	 i:1 	 global-step:22681	 l-p:0.15141905844211578
epoch£º1134	 i:2 	 global-step:22682	 l-p:0.13653932511806488
epoch£º1134	 i:3 	 global-step:22683	 l-p:0.10621069371700287
epoch£º1134	 i:4 	 global-step:22684	 l-p:0.17011798918247223
epoch£º1134	 i:5 	 global-step:22685	 l-p:0.14328335225582123
epoch£º1134	 i:6 	 global-step:22686	 l-p:0.1070883572101593
epoch£º1134	 i:7 	 global-step:22687	 l-p:0.12013204395771027
epoch£º1134	 i:8 	 global-step:22688	 l-p:0.1220000609755516
epoch£º1134	 i:9 	 global-step:22689	 l-p:0.08990363776683807
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1135
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1473e-01, 3.0928e-01,
         1.0000e+00, 2.3065e-01, 1.0000e+00, 7.4574e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3115e-01, 2.2910e-01,
         1.0000e+00, 1.5850e-01, 1.0000e+00, 6.9184e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1893, 5.4661, 5.2995],
        [5.1893, 4.9103, 4.6446],
        [5.1893, 5.1377, 5.1765],
        [5.1893, 4.8947, 4.7282]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1135, step:0 
model_pd.l_p.mean(): 0.1383587270975113 
model_pd.l_d.mean(): -20.078094482421875 
model_pd.lagr.mean(): -19.939735412597656 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5011], device='cuda:0')), ('power', tensor([-20.9726], device='cuda:0'))])
epoch£º1135	 i:0 	 global-step:22700	 l-p:0.1383587270975113
epoch£º1135	 i:1 	 global-step:22701	 l-p:0.13042902946472168
epoch£º1135	 i:2 	 global-step:22702	 l-p:0.126563161611557
epoch£º1135	 i:3 	 global-step:22703	 l-p:0.12093545496463776
epoch£º1135	 i:4 	 global-step:22704	 l-p:0.14290212094783783
epoch£º1135	 i:5 	 global-step:22705	 l-p:0.06390181183815002
epoch£º1135	 i:6 	 global-step:22706	 l-p:0.13103234767913818
epoch£º1135	 i:7 	 global-step:22707	 l-p:0.11246783286333084
epoch£º1135	 i:8 	 global-step:22708	 l-p:0.17793284356594086
epoch£º1135	 i:9 	 global-step:22709	 l-p:0.14633435010910034
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1136
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6966e-02, 1.6945e-02,
         1.0000e+00, 6.1137e-03, 1.0000e+00, 3.6080e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1977, 5.1692, 5.1932],
        [5.1977, 5.1957, 5.1977],
        [5.1977, 5.0270, 5.0786],
        [5.1977, 4.9041, 4.7401]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1136, step:0 
model_pd.l_p.mean(): 0.10707488656044006 
model_pd.l_d.mean(): -20.6536808013916 
model_pd.lagr.mean(): -20.546606063842773 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3833], device='cuda:0')), ('power', tensor([-21.4368], device='cuda:0'))])
epoch£º1136	 i:0 	 global-step:22720	 l-p:0.10707488656044006
epoch£º1136	 i:1 	 global-step:22721	 l-p:0.09652280807495117
epoch£º1136	 i:2 	 global-step:22722	 l-p:0.1493125855922699
epoch£º1136	 i:3 	 global-step:22723	 l-p:0.13500958681106567
epoch£º1136	 i:4 	 global-step:22724	 l-p:0.1275455206632614
epoch£º1136	 i:5 	 global-step:22725	 l-p:0.09950259327888489
epoch£º1136	 i:6 	 global-step:22726	 l-p:0.14185158908367157
epoch£º1136	 i:7 	 global-step:22727	 l-p:0.131598562002182
epoch£º1136	 i:8 	 global-step:22728	 l-p:0.10938478261232376
epoch£º1136	 i:9 	 global-step:22729	 l-p:0.16888420283794403
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1137
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.2028, 5.2024, 5.2028],
        [5.2028, 5.2028, 5.2028],
        [5.2028, 5.1977, 5.2026],
        [5.2028, 4.9103, 4.7042]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1137, step:0 
model_pd.l_p.mean(): 0.09269138425588608 
model_pd.l_d.mean(): -20.145692825317383 
model_pd.lagr.mean(): -20.053001403808594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4663], device='cuda:0')), ('power', tensor([-21.0054], device='cuda:0'))])
epoch£º1137	 i:0 	 global-step:22740	 l-p:0.09269138425588608
epoch£º1137	 i:1 	 global-step:22741	 l-p:0.10417323559522629
epoch£º1137	 i:2 	 global-step:22742	 l-p:0.08670265227556229
epoch£º1137	 i:3 	 global-step:22743	 l-p:0.13662001490592957
epoch£º1137	 i:4 	 global-step:22744	 l-p:0.15449891984462738
epoch£º1137	 i:5 	 global-step:22745	 l-p:0.17228496074676514
epoch£º1137	 i:6 	 global-step:22746	 l-p:0.1504960060119629
epoch£º1137	 i:7 	 global-step:22747	 l-p:0.11619643121957779
epoch£º1137	 i:8 	 global-step:22748	 l-p:0.16793304681777954
epoch£º1137	 i:9 	 global-step:22749	 l-p:0.08782591670751572
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1138
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1918, 5.1876, 5.1916],
        [5.1918, 5.1919, 5.1919],
        [5.1918, 5.1369, 5.1776],
        [5.1918, 5.1356, 4.8081]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1138, step:0 
model_pd.l_p.mean(): 0.1129625141620636 
model_pd.l_d.mean(): -19.312211990356445 
model_pd.lagr.mean(): -19.199249267578125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4874], device='cuda:0')), ('power', tensor([-20.1781], device='cuda:0'))])
epoch£º1138	 i:0 	 global-step:22760	 l-p:0.1129625141620636
epoch£º1138	 i:1 	 global-step:22761	 l-p:0.14952421188354492
epoch£º1138	 i:2 	 global-step:22762	 l-p:0.14931422472000122
epoch£º1138	 i:3 	 global-step:22763	 l-p:0.12902675569057465
epoch£º1138	 i:4 	 global-step:22764	 l-p:0.18138647079467773
epoch£º1138	 i:5 	 global-step:22765	 l-p:0.08484626561403275
epoch£º1138	 i:6 	 global-step:22766	 l-p:0.11400791257619858
epoch£º1138	 i:7 	 global-step:22767	 l-p:0.1730758398771286
epoch£º1138	 i:8 	 global-step:22768	 l-p:0.13601216673851013
epoch£º1138	 i:9 	 global-step:22769	 l-p:0.10231191664934158
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1139
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5409e-01, 3.4902e-01,
         1.0000e+00, 2.6827e-01, 1.0000e+00, 7.6862e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1771, 5.1771, 5.1771],
        [5.1771, 5.1633, 5.1758],
        [5.1771, 4.9167, 4.6174],
        [5.1771, 5.1609, 5.1753]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1139, step:0 
model_pd.l_p.mean(): 0.13371658325195312 
model_pd.l_d.mean(): -20.017675399780273 
model_pd.lagr.mean(): -19.88395881652832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4875], device='cuda:0')), ('power', tensor([-20.8969], device='cuda:0'))])
epoch£º1139	 i:0 	 global-step:22780	 l-p:0.13371658325195312
epoch£º1139	 i:1 	 global-step:22781	 l-p:0.12033607065677643
epoch£º1139	 i:2 	 global-step:22782	 l-p:0.14524108171463013
epoch£º1139	 i:3 	 global-step:22783	 l-p:0.13422870635986328
epoch£º1139	 i:4 	 global-step:22784	 l-p:0.15168850123882294
epoch£º1139	 i:5 	 global-step:22785	 l-p:0.1637379676103592
epoch£º1139	 i:6 	 global-step:22786	 l-p:0.2647896707057953
epoch£º1139	 i:7 	 global-step:22787	 l-p:0.09013625979423523
epoch£º1139	 i:8 	 global-step:22788	 l-p:0.11331569403409958
epoch£º1139	 i:9 	 global-step:22789	 l-p:0.08120203763246536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1140
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1571, 5.1571, 5.1572],
        [5.1571, 5.1062, 5.1448],
        [5.1571, 5.1560, 5.1571],
        [5.1571, 5.1571, 5.1572]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1140, step:0 
model_pd.l_p.mean(): 0.1700955480337143 
model_pd.l_d.mean(): -20.785070419311523 
model_pd.lagr.mean(): -20.614974975585938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3872], device='cuda:0')), ('power', tensor([-21.5748], device='cuda:0'))])
epoch£º1140	 i:0 	 global-step:22800	 l-p:0.1700955480337143
epoch£º1140	 i:1 	 global-step:22801	 l-p:0.1426834911108017
epoch£º1140	 i:2 	 global-step:22802	 l-p:0.06529060751199722
epoch£º1140	 i:3 	 global-step:22803	 l-p:0.22841903567314148
epoch£º1140	 i:4 	 global-step:22804	 l-p:0.13076485693454742
epoch£º1140	 i:5 	 global-step:22805	 l-p:0.14238877594470978
epoch£º1140	 i:6 	 global-step:22806	 l-p:0.14252999424934387
epoch£º1140	 i:7 	 global-step:22807	 l-p:0.13040481507778168
epoch£º1140	 i:8 	 global-step:22808	 l-p:0.18724939227104187
epoch£º1140	 i:9 	 global-step:22809	 l-p:0.12851572036743164
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1141
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1484, 5.0164, 5.0775],
        [5.1484, 5.1477, 5.1484],
        [5.1484, 5.0307, 4.6882],
        [5.1484, 4.9176, 4.9272]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1141, step:0 
model_pd.l_p.mean(): 0.12315090000629425 
model_pd.l_d.mean(): -18.93486785888672 
model_pd.lagr.mean(): -18.811716079711914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6089], device='cuda:0')), ('power', tensor([-19.9196], device='cuda:0'))])
epoch£º1141	 i:0 	 global-step:22820	 l-p:0.12315090000629425
epoch£º1141	 i:1 	 global-step:22821	 l-p:0.21545512974262238
epoch£º1141	 i:2 	 global-step:22822	 l-p:0.14742489159107208
epoch£º1141	 i:3 	 global-step:22823	 l-p:0.11124026775360107
epoch£º1141	 i:4 	 global-step:22824	 l-p:0.12632709741592407
epoch£º1141	 i:5 	 global-step:22825	 l-p:0.18770244717597961
epoch£º1141	 i:6 	 global-step:22826	 l-p:0.20408236980438232
epoch£º1141	 i:7 	 global-step:22827	 l-p:0.0930149033665657
epoch£º1141	 i:8 	 global-step:22828	 l-p:0.14234521985054016
epoch£º1141	 i:9 	 global-step:22829	 l-p:0.11542604118585587
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1142
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1580, 5.1575, 5.1580],
        [5.1580, 5.5269, 5.4185],
        [5.1580, 5.1579, 5.1580],
        [5.1580, 5.0893, 4.7573]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1142, step:0 
model_pd.l_p.mean(): 0.1264476180076599 
model_pd.l_d.mean(): -19.63667106628418 
model_pd.lagr.mean(): -19.510223388671875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4528], device='cuda:0')), ('power', tensor([-20.4729], device='cuda:0'))])
epoch£º1142	 i:0 	 global-step:22840	 l-p:0.1264476180076599
epoch£º1142	 i:1 	 global-step:22841	 l-p:0.2520642876625061
epoch£º1142	 i:2 	 global-step:22842	 l-p:0.13646721839904785
epoch£º1142	 i:3 	 global-step:22843	 l-p:0.13366131484508514
epoch£º1142	 i:4 	 global-step:22844	 l-p:0.14913791418075562
epoch£º1142	 i:5 	 global-step:22845	 l-p:0.13064658641815186
epoch£º1142	 i:6 	 global-step:22846	 l-p:0.1327817738056183
epoch£º1142	 i:7 	 global-step:22847	 l-p:0.13254952430725098
epoch£º1142	 i:8 	 global-step:22848	 l-p:0.11349231749773026
epoch£º1142	 i:9 	 global-step:22849	 l-p:0.11256474256515503
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1143
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7756e-01, 8.4018e-01,
         1.0000e+00, 8.0439e-01, 1.0000e+00, 9.5740e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7552e-01, 9.8271e-02,
         1.0000e+00, 5.5021e-02, 1.0000e+00, 5.5989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1644, 5.4289, 5.2550],
        [5.1644, 5.1304, 5.1583],
        [5.1644, 4.9666, 5.0051],
        [5.1644, 5.1632, 5.1644]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1143, step:0 
model_pd.l_p.mean(): 0.10572918504476547 
model_pd.l_d.mean(): -19.974546432495117 
model_pd.lagr.mean(): -19.868816375732422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4313], device='cuda:0')), ('power', tensor([-20.7948], device='cuda:0'))])
epoch£º1143	 i:0 	 global-step:22860	 l-p:0.10572918504476547
epoch£º1143	 i:1 	 global-step:22861	 l-p:0.1316719651222229
epoch£º1143	 i:2 	 global-step:22862	 l-p:0.17863838374614716
epoch£º1143	 i:3 	 global-step:22863	 l-p:0.14259889721870422
epoch£º1143	 i:4 	 global-step:22864	 l-p:0.12420667707920074
epoch£º1143	 i:5 	 global-step:22865	 l-p:0.13145825266838074
epoch£º1143	 i:6 	 global-step:22866	 l-p:0.1048772782087326
epoch£º1143	 i:7 	 global-step:22867	 l-p:0.1309588998556137
epoch£º1143	 i:8 	 global-step:22868	 l-p:0.21835392713546753
epoch£º1143	 i:9 	 global-step:22869	 l-p:0.14737458527088165
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1144
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3514e-01, 2.3280e-01,
         1.0000e+00, 1.6170e-01, 1.0000e+00, 6.9461e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1618, 5.1618, 5.1619],
        [5.1618, 5.1618, 5.1618],
        [5.1618, 5.1616, 5.1618],
        [5.1618, 4.8633, 4.6912]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1144, step:0 
model_pd.l_p.mean(): 0.09715991467237473 
model_pd.l_d.mean(): -20.27253532409668 
model_pd.lagr.mean(): -20.17537498474121 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4586], device='cuda:0')), ('power', tensor([-21.1266], device='cuda:0'))])
epoch£º1144	 i:0 	 global-step:22880	 l-p:0.09715991467237473
epoch£º1144	 i:1 	 global-step:22881	 l-p:0.15388070046901703
epoch£º1144	 i:2 	 global-step:22882	 l-p:0.11481847614049911
epoch£º1144	 i:3 	 global-step:22883	 l-p:0.15323607623577118
epoch£º1144	 i:4 	 global-step:22884	 l-p:0.10941475629806519
epoch£º1144	 i:5 	 global-step:22885	 l-p:0.0945323184132576
epoch£º1144	 i:6 	 global-step:22886	 l-p:0.1812940537929535
epoch£º1144	 i:7 	 global-step:22887	 l-p:0.12208379805088043
epoch£º1144	 i:8 	 global-step:22888	 l-p:0.2506595253944397
epoch£º1144	 i:9 	 global-step:22889	 l-p:0.1365758627653122
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1145
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7576e-02, 8.3312e-03,
         1.0000e+00, 2.5170e-03, 1.0000e+00, 3.0212e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1628, 5.0723, 5.1279],
        [5.1628, 5.0201, 5.0802],
        [5.1628, 5.1564, 5.1625],
        [5.1628, 5.1518, 5.1619]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1145, step:0 
model_pd.l_p.mean(): 0.11743233352899551 
model_pd.l_d.mean(): -19.787267684936523 
model_pd.lagr.mean(): -19.669836044311523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5139], device='cuda:0')), ('power', tensor([-20.6896], device='cuda:0'))])
epoch£º1145	 i:0 	 global-step:22900	 l-p:0.11743233352899551
epoch£º1145	 i:1 	 global-step:22901	 l-p:0.1303568333387375
epoch£º1145	 i:2 	 global-step:22902	 l-p:0.12137624621391296
epoch£º1145	 i:3 	 global-step:22903	 l-p:0.17205344140529633
epoch£º1145	 i:4 	 global-step:22904	 l-p:0.13387376070022583
epoch£º1145	 i:5 	 global-step:22905	 l-p:0.14560815691947937
epoch£º1145	 i:6 	 global-step:22906	 l-p:0.06780104339122772
epoch£º1145	 i:7 	 global-step:22907	 l-p:0.17708277702331543
epoch£º1145	 i:8 	 global-step:22908	 l-p:0.15671363472938538
epoch£º1145	 i:9 	 global-step:22909	 l-p:0.18727755546569824
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1146
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1648, 5.1648, 5.1648],
        [5.1648, 5.5600, 5.4687],
        [5.1648, 4.8672, 4.7057],
        [5.1648, 4.8689, 4.7235]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1146, step:0 
model_pd.l_p.mean(): 0.1760571002960205 
model_pd.l_d.mean(): -19.721277236938477 
model_pd.lagr.mean(): -19.54521942138672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5498], device='cuda:0')), ('power', tensor([-20.6595], device='cuda:0'))])
epoch£º1146	 i:0 	 global-step:22920	 l-p:0.1760571002960205
epoch£º1146	 i:1 	 global-step:22921	 l-p:0.15682397782802582
epoch£º1146	 i:2 	 global-step:22922	 l-p:0.1081991195678711
epoch£º1146	 i:3 	 global-step:22923	 l-p:0.10965974628925323
epoch£º1146	 i:4 	 global-step:22924	 l-p:0.14893457293510437
epoch£º1146	 i:5 	 global-step:22925	 l-p:0.11034151166677475
epoch£º1146	 i:6 	 global-step:22926	 l-p:0.17021727561950684
epoch£º1146	 i:7 	 global-step:22927	 l-p:0.1472190022468567
epoch£º1146	 i:8 	 global-step:22928	 l-p:0.12708429992198944
epoch£º1146	 i:9 	 global-step:22929	 l-p:0.13181926310062408
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1147
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1695, 5.2933, 5.0405],
        [5.1695, 5.1689, 5.1695],
        [5.1695, 5.1684, 5.1694],
        [5.1695, 4.9804, 4.6398]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1147, step:0 
model_pd.l_p.mean(): 0.08753647655248642 
model_pd.l_d.mean(): -20.09049415588379 
model_pd.lagr.mean(): -20.002958297729492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4983], device='cuda:0')), ('power', tensor([-20.9823], device='cuda:0'))])
epoch£º1147	 i:0 	 global-step:22940	 l-p:0.08753647655248642
epoch£º1147	 i:1 	 global-step:22941	 l-p:0.14968429505825043
epoch£º1147	 i:2 	 global-step:22942	 l-p:0.13124458491802216
epoch£º1147	 i:3 	 global-step:22943	 l-p:0.14795072376728058
epoch£º1147	 i:4 	 global-step:22944	 l-p:0.13001975417137146
epoch£º1147	 i:5 	 global-step:22945	 l-p:0.15170657634735107
epoch£º1147	 i:6 	 global-step:22946	 l-p:0.142767995595932
epoch£º1147	 i:7 	 global-step:22947	 l-p:0.1359129697084427
epoch£º1147	 i:8 	 global-step:22948	 l-p:0.1253771334886551
epoch£º1147	 i:9 	 global-step:22949	 l-p:0.18056993186473846
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1148
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1673, 5.4196, 5.2382],
        [5.1673, 5.0850, 5.1380],
        [5.1673, 5.0501, 5.1107],
        [5.1673, 5.1629, 5.1671]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1148, step:0 
model_pd.l_p.mean(): 0.11940526962280273 
model_pd.l_d.mean(): -19.603105545043945 
model_pd.lagr.mean(): -19.483699798583984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5175], device='cuda:0')), ('power', tensor([-20.5057], device='cuda:0'))])
epoch£º1148	 i:0 	 global-step:22960	 l-p:0.11940526962280273
epoch£º1148	 i:1 	 global-step:22961	 l-p:0.16113533079624176
epoch£º1148	 i:2 	 global-step:22962	 l-p:0.15331432223320007
epoch£º1148	 i:3 	 global-step:22963	 l-p:0.14895662665367126
epoch£º1148	 i:4 	 global-step:22964	 l-p:0.13967174291610718
epoch£º1148	 i:5 	 global-step:22965	 l-p:0.13774919509887695
epoch£º1148	 i:6 	 global-step:22966	 l-p:0.13034707307815552
epoch£º1148	 i:7 	 global-step:22967	 l-p:0.11646713316440582
epoch£º1148	 i:8 	 global-step:22968	 l-p:0.13001561164855957
epoch£º1148	 i:9 	 global-step:22969	 l-p:0.17254890501499176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1149
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7870e-01, 4.8225e-01,
         1.0000e+00, 4.0188e-01, 1.0000e+00, 8.3333e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2712e-01, 6.3921e-02,
         1.0000e+00, 3.2140e-02, 1.0000e+00, 5.0282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1474e-01, 5.5756e-02,
         1.0000e+00, 2.7094e-02, 1.0000e+00, 4.8593e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1609, 5.0062, 4.6613],
        [5.1609, 4.9013, 4.5963],
        [5.1609, 5.0257, 5.0866],
        [5.1609, 5.0430, 5.1037]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1149, step:0 
model_pd.l_p.mean(): 0.1801944375038147 
model_pd.l_d.mean(): -19.71862030029297 
model_pd.lagr.mean(): -19.53842544555664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5264], device='cuda:0')), ('power', tensor([-20.6325], device='cuda:0'))])
epoch£º1149	 i:0 	 global-step:22980	 l-p:0.1801944375038147
epoch£º1149	 i:1 	 global-step:22981	 l-p:0.07184422016143799
epoch£º1149	 i:2 	 global-step:22982	 l-p:0.18604198098182678
epoch£º1149	 i:3 	 global-step:22983	 l-p:0.15313003957271576
epoch£º1149	 i:4 	 global-step:22984	 l-p:0.0994652509689331
epoch£º1149	 i:5 	 global-step:22985	 l-p:0.12290601432323456
epoch£º1149	 i:6 	 global-step:22986	 l-p:0.1380896121263504
epoch£º1149	 i:7 	 global-step:22987	 l-p:0.17397624254226685
epoch£º1149	 i:8 	 global-step:22988	 l-p:0.18918700516223907
epoch£º1149	 i:9 	 global-step:22989	 l-p:0.08156333863735199
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1150
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6457e-04, 3.5981e-05,
         1.0000e+00, 2.7867e-06, 1.0000e+00, 7.7449e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7948e-03, 5.9190e-04,
         1.0000e+00, 9.2323e-05, 1.0000e+00, 1.5598e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1702, 5.1702, 5.1702],
        [5.1702, 5.1698, 5.1702],
        [5.1702, 5.1699, 5.1702],
        [5.1702, 5.1697, 5.1701]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1150, step:0 
model_pd.l_p.mean(): 0.11496949940919876 
model_pd.l_d.mean(): -20.356014251708984 
model_pd.lagr.mean(): -20.241044998168945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4172], device='cuda:0')), ('power', tensor([-21.1687], device='cuda:0'))])
epoch£º1150	 i:0 	 global-step:23000	 l-p:0.11496949940919876
epoch£º1150	 i:1 	 global-step:23001	 l-p:0.1168946921825409
epoch£º1150	 i:2 	 global-step:23002	 l-p:0.09050869941711426
epoch£º1150	 i:3 	 global-step:23003	 l-p:0.14080116152763367
epoch£º1150	 i:4 	 global-step:23004	 l-p:0.21806147694587708
epoch£º1150	 i:5 	 global-step:23005	 l-p:0.1451263576745987
epoch£º1150	 i:6 	 global-step:23006	 l-p:0.10639369487762451
epoch£º1150	 i:7 	 global-step:23007	 l-p:0.1536780744791031
epoch£º1150	 i:8 	 global-step:23008	 l-p:0.1553526222705841
epoch£º1150	 i:9 	 global-step:23009	 l-p:0.14488400518894196
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1151
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0237e-03, 1.0317e-04,
         1.0000e+00, 1.0398e-05, 1.0000e+00, 1.0078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5131e-02, 4.3427e-02,
         1.0000e+00, 1.9824e-02, 1.0000e+00, 4.5650e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1714, 5.1714, 5.1714],
        [5.1714, 5.1714, 5.1714],
        [5.1714, 5.0813, 5.1368],
        [5.1714, 4.9014, 4.8465]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1151, step:0 
model_pd.l_p.mean(): 0.18551333248615265 
model_pd.l_d.mean(): -20.518993377685547 
model_pd.lagr.mean(): -20.333480834960938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4260], device='cuda:0')), ('power', tensor([-21.3439], device='cuda:0'))])
epoch£º1151	 i:0 	 global-step:23020	 l-p:0.18551333248615265
epoch£º1151	 i:1 	 global-step:23021	 l-p:0.10267316550016403
epoch£º1151	 i:2 	 global-step:23022	 l-p:0.12558016180992126
epoch£º1151	 i:3 	 global-step:23023	 l-p:0.14072111248970032
epoch£º1151	 i:4 	 global-step:23024	 l-p:0.12114249169826508
epoch£º1151	 i:5 	 global-step:23025	 l-p:0.15473195910453796
epoch£º1151	 i:6 	 global-step:23026	 l-p:0.10568899661302567
epoch£º1151	 i:7 	 global-step:23027	 l-p:0.12935106456279755
epoch£º1151	 i:8 	 global-step:23028	 l-p:0.12901276350021362
epoch£º1151	 i:9 	 global-step:23029	 l-p:0.1559382975101471
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1152
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7154e-01, 9.5316e-02,
         1.0000e+00, 5.2961e-02, 1.0000e+00, 5.5564e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1853, 5.1846, 5.1853],
        [5.1853, 4.8886, 4.7090],
        [5.1853, 5.1853, 5.1854],
        [5.1853, 4.9928, 5.0341]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1152, step:0 
model_pd.l_p.mean(): 0.14338241517543793 
model_pd.l_d.mean(): -20.73531723022461 
model_pd.lagr.mean(): -20.591934204101562 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3756], device='cuda:0')), ('power', tensor([-21.5121], device='cuda:0'))])
epoch£º1152	 i:0 	 global-step:23040	 l-p:0.14338241517543793
epoch£º1152	 i:1 	 global-step:23041	 l-p:0.09859561920166016
epoch£º1152	 i:2 	 global-step:23042	 l-p:0.13035035133361816
epoch£º1152	 i:3 	 global-step:23043	 l-p:0.05374133586883545
epoch£º1152	 i:4 	 global-step:23044	 l-p:0.16338297724723816
epoch£º1152	 i:5 	 global-step:23045	 l-p:0.13449852168560028
epoch£º1152	 i:6 	 global-step:23046	 l-p:0.113852858543396
epoch£º1152	 i:7 	 global-step:23047	 l-p:0.1429205685853958
epoch£º1152	 i:8 	 global-step:23048	 l-p:0.17034298181533813
epoch£º1152	 i:9 	 global-step:23049	 l-p:0.16952632367610931
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1153
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1892, 5.1881, 5.1892],
        [5.1892, 5.1361, 5.1759],
        [5.1892, 5.0655, 5.1262],
        [5.1892, 5.1749, 5.1878]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1153, step:0 
model_pd.l_p.mean(): 0.16343173384666443 
model_pd.l_d.mean(): -20.30617904663086 
model_pd.lagr.mean(): -20.14274787902832 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-21.1664], device='cuda:0'))])
epoch£º1153	 i:0 	 global-step:23060	 l-p:0.16343173384666443
epoch£º1153	 i:1 	 global-step:23061	 l-p:0.15089021623134613
epoch£º1153	 i:2 	 global-step:23062	 l-p:0.12005162984132767
epoch£º1153	 i:3 	 global-step:23063	 l-p:0.11883245408535004
epoch£º1153	 i:4 	 global-step:23064	 l-p:0.11978760361671448
epoch£º1153	 i:5 	 global-step:23065	 l-p:0.08050187677145004
epoch£º1153	 i:6 	 global-step:23066	 l-p:0.12800242006778717
epoch£º1153	 i:7 	 global-step:23067	 l-p:0.10758788883686066
epoch£º1153	 i:8 	 global-step:23068	 l-p:0.1592651754617691
epoch£º1153	 i:9 	 global-step:23069	 l-p:0.1740189641714096
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1154
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.7674e-11, 3.3141e-14,
         1.0000e+00, 1.4140e-17, 1.0000e+00, 4.2667e-04, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1790, 5.1391, 5.1710],
        [5.1790, 5.0500, 5.1109],
        [5.1790, 5.1790, 5.1790],
        [5.1790, 5.1450, 5.1729]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1154, step:0 
model_pd.l_p.mean(): 0.13762213289737701 
model_pd.l_d.mean(): -20.514633178710938 
model_pd.lagr.mean(): -20.377010345458984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4367], device='cuda:0')), ('power', tensor([-21.3506], device='cuda:0'))])
epoch£º1154	 i:0 	 global-step:23080	 l-p:0.13762213289737701
epoch£º1154	 i:1 	 global-step:23081	 l-p:0.1576327234506607
epoch£º1154	 i:2 	 global-step:23082	 l-p:0.12168069928884506
epoch£º1154	 i:3 	 global-step:23083	 l-p:0.13978798687458038
epoch£º1154	 i:4 	 global-step:23084	 l-p:0.17039285600185394
epoch£º1154	 i:5 	 global-step:23085	 l-p:0.13662570714950562
epoch£º1154	 i:6 	 global-step:23086	 l-p:0.06396766752004623
epoch£º1154	 i:7 	 global-step:23087	 l-p:0.15870806574821472
epoch£º1154	 i:8 	 global-step:23088	 l-p:0.13563574850559235
epoch£º1154	 i:9 	 global-step:23089	 l-p:0.13099165260791779
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1155
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9976e-05, 9.3117e-07,
         1.0000e+00, 2.8926e-08, 1.0000e+00, 3.1064e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0338e-01, 8.7330e-01,
         1.0000e+00, 8.4422e-01, 1.0000e+00, 9.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1786, 5.1737, 5.1783],
        [5.1786, 5.1786, 5.1786],
        [5.1786, 5.1786, 5.1786],
        [5.1786, 5.4876, 5.3402]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1155, step:0 
model_pd.l_p.mean(): 0.0944669097661972 
model_pd.l_d.mean(): -19.18804931640625 
model_pd.lagr.mean(): -19.093582153320312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5396], device='cuda:0')), ('power', tensor([-20.1058], device='cuda:0'))])
epoch£º1155	 i:0 	 global-step:23100	 l-p:0.0944669097661972
epoch£º1155	 i:1 	 global-step:23101	 l-p:0.12443696707487106
epoch£º1155	 i:2 	 global-step:23102	 l-p:0.09967999905347824
epoch£º1155	 i:3 	 global-step:23103	 l-p:0.21360965073108673
epoch£º1155	 i:4 	 global-step:23104	 l-p:0.17473597824573517
epoch£º1155	 i:5 	 global-step:23105	 l-p:0.1486404836177826
epoch£º1155	 i:6 	 global-step:23106	 l-p:0.14933282136917114
epoch£º1155	 i:7 	 global-step:23107	 l-p:0.11269110441207886
epoch£º1155	 i:8 	 global-step:23108	 l-p:0.1131766214966774
epoch£º1155	 i:9 	 global-step:23109	 l-p:0.1275692731142044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1156
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3388e-04, 4.3310e-05,
         1.0000e+00, 3.5135e-06, 1.0000e+00, 8.1124e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7026e-02, 2.1950e-02,
         1.0000e+00, 8.4486e-03, 1.0000e+00, 3.8491e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2871e-01, 3.2326e-01,
         1.0000e+00, 2.4375e-01, 1.0000e+00, 7.5403e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1747, 5.1292, 5.1646],
        [5.1747, 5.1747, 5.1747],
        [5.1747, 5.1347, 5.1666],
        [5.1747, 4.8982, 4.6185]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1156, step:0 
model_pd.l_p.mean(): 0.15952551364898682 
model_pd.l_d.mean(): -20.84009552001953 
model_pd.lagr.mean(): -20.680570602416992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3790], device='cuda:0')), ('power', tensor([-21.6224], device='cuda:0'))])
epoch£º1156	 i:0 	 global-step:23120	 l-p:0.15952551364898682
epoch£º1156	 i:1 	 global-step:23121	 l-p:0.13264717161655426
epoch£º1156	 i:2 	 global-step:23122	 l-p:0.10940086096525192
epoch£º1156	 i:3 	 global-step:23123	 l-p:0.19320911169052124
epoch£º1156	 i:4 	 global-step:23124	 l-p:0.17294010519981384
epoch£º1156	 i:5 	 global-step:23125	 l-p:0.14114351570606232
epoch£º1156	 i:6 	 global-step:23126	 l-p:0.15539993345737457
epoch£º1156	 i:7 	 global-step:23127	 l-p:0.1161784902215004
epoch£º1156	 i:8 	 global-step:23128	 l-p:0.09664756804704666
epoch£º1156	 i:9 	 global-step:23129	 l-p:0.12210145592689514
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1157
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7218e-04, 5.8882e-05,
         1.0000e+00, 5.1579e-06, 1.0000e+00, 8.7598e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3923e-01, 1.4851e-01,
         1.0000e+00, 9.2192e-02, 1.0000e+00, 6.2078e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1614, 5.1613, 5.1614],
        [5.1614, 4.9007, 4.8670],
        [5.1614, 5.1583, 5.1612],
        [5.1614, 5.0597, 5.1181]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1157, step:0 
model_pd.l_p.mean(): 0.12751419842243195 
model_pd.l_d.mean(): -20.068220138549805 
model_pd.lagr.mean(): -19.940706253051758 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4964], device='cuda:0')), ('power', tensor([-20.9577], device='cuda:0'))])
epoch£º1157	 i:0 	 global-step:23140	 l-p:0.12751419842243195
epoch£º1157	 i:1 	 global-step:23141	 l-p:0.17878371477127075
epoch£º1157	 i:2 	 global-step:23142	 l-p:0.14177097380161285
epoch£º1157	 i:3 	 global-step:23143	 l-p:0.17309948801994324
epoch£º1157	 i:4 	 global-step:23144	 l-p:0.12903223931789398
epoch£º1157	 i:5 	 global-step:23145	 l-p:0.12853756546974182
epoch£º1157	 i:6 	 global-step:23146	 l-p:0.19134585559368134
epoch£º1157	 i:7 	 global-step:23147	 l-p:0.08753345906734467
epoch£º1157	 i:8 	 global-step:23148	 l-p:0.12377950549125671
epoch£º1157	 i:9 	 global-step:23149	 l-p:0.11355527490377426
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1158
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.8104e-04, 2.7624e-05,
         1.0000e+00, 2.0027e-06, 1.0000e+00, 7.2498e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1741, 5.0569, 5.1175],
        [5.1741, 5.1741, 5.1741],
        [5.1741, 5.1741, 5.1741],
        [5.1741, 4.9013, 4.8403]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1158, step:0 
model_pd.l_p.mean(): 0.1525433212518692 
model_pd.l_d.mean(): -19.898242950439453 
model_pd.lagr.mean(): -19.745698928833008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4967], device='cuda:0')), ('power', tensor([-20.7847], device='cuda:0'))])
epoch£º1158	 i:0 	 global-step:23160	 l-p:0.1525433212518692
epoch£º1158	 i:1 	 global-step:23161	 l-p:0.12296735495328903
epoch£º1158	 i:2 	 global-step:23162	 l-p:0.1404915452003479
epoch£º1158	 i:3 	 global-step:23163	 l-p:0.1516326367855072
epoch£º1158	 i:4 	 global-step:23164	 l-p:0.11857007443904877
epoch£º1158	 i:5 	 global-step:23165	 l-p:0.13065741956233978
epoch£º1158	 i:6 	 global-step:23166	 l-p:0.13132990896701813
epoch£º1158	 i:7 	 global-step:23167	 l-p:0.18994437158107758
epoch£º1158	 i:8 	 global-step:23168	 l-p:0.11323142051696777
epoch£º1158	 i:9 	 global-step:23169	 l-p:0.13165973126888275
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1159
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1613, 4.8631, 4.6461],
        [5.1613, 5.1612, 5.1613],
        [5.1613, 5.1610, 5.1613],
        [5.1613, 5.1039, 5.1461]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1159, step:0 
model_pd.l_p.mean(): 0.19332091510295868 
model_pd.l_d.mean(): -20.389963150024414 
model_pd.lagr.mean(): -20.19664192199707 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4463], device='cuda:0')), ('power', tensor([-21.2334], device='cuda:0'))])
epoch£º1159	 i:0 	 global-step:23180	 l-p:0.19332091510295868
epoch£º1159	 i:1 	 global-step:23181	 l-p:0.1509600132703781
epoch£º1159	 i:2 	 global-step:23182	 l-p:0.12027992308139801
epoch£º1159	 i:3 	 global-step:23183	 l-p:0.12499800324440002
epoch£º1159	 i:4 	 global-step:23184	 l-p:0.13246048986911774
epoch£º1159	 i:5 	 global-step:23185	 l-p:0.10977355390787125
epoch£º1159	 i:6 	 global-step:23186	 l-p:0.1468982994556427
epoch£º1159	 i:7 	 global-step:23187	 l-p:0.14736363291740417
epoch£º1159	 i:8 	 global-step:23188	 l-p:0.14232683181762695
epoch£º1159	 i:9 	 global-step:23189	 l-p:0.16257400810718536
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1160
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9097e-02, 5.1045e-03,
         1.0000e+00, 1.3644e-03, 1.0000e+00, 2.6729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8705e-01, 3.8321e-01,
         1.0000e+00, 3.0150e-01, 1.0000e+00, 7.8679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1577, 4.8804, 4.5961],
        [5.1577, 5.1521, 5.1574],
        [5.1577, 4.9150, 4.5933],
        [5.1577, 5.1576, 5.1577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1160, step:0 
model_pd.l_p.mean(): 0.1297461986541748 
model_pd.l_d.mean(): -19.953060150146484 
model_pd.lagr.mean(): -19.823314666748047 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4544], device='cuda:0')), ('power', tensor([-20.7968], device='cuda:0'))])
epoch£º1160	 i:0 	 global-step:23200	 l-p:0.1297461986541748
epoch£º1160	 i:1 	 global-step:23201	 l-p:0.17469032108783722
epoch£º1160	 i:2 	 global-step:23202	 l-p:0.15359990298748016
epoch£º1160	 i:3 	 global-step:23203	 l-p:0.12454833835363388
epoch£º1160	 i:4 	 global-step:23204	 l-p:0.16163451969623566
epoch£º1160	 i:5 	 global-step:23205	 l-p:0.09546913206577301
epoch£º1160	 i:6 	 global-step:23206	 l-p:0.13032132387161255
epoch£º1160	 i:7 	 global-step:23207	 l-p:0.10712481290102005
epoch£º1160	 i:8 	 global-step:23208	 l-p:0.2447689026594162
epoch£º1160	 i:9 	 global-step:23209	 l-p:0.13830743730068207
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1161
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3784e-01, 4.3739e-01,
         1.0000e+00, 3.5571e-01, 1.0000e+00, 8.1324e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0344e-01, 4.8558e-02,
         1.0000e+00, 2.2794e-02, 1.0000e+00, 4.6942e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1529, 4.9530, 4.6117],
        [5.1529, 5.1529, 5.1529],
        [5.1529, 5.0506, 5.1092],
        [5.1529, 4.9287, 4.9464]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1161, step:0 
model_pd.l_p.mean(): 0.08687330037355423 
model_pd.l_d.mean(): -18.481657028198242 
model_pd.lagr.mean(): -18.39478302001953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6375], device='cuda:0')), ('power', tensor([-19.4876], device='cuda:0'))])
epoch£º1161	 i:0 	 global-step:23220	 l-p:0.08687330037355423
epoch£º1161	 i:1 	 global-step:23221	 l-p:0.1365908831357956
epoch£º1161	 i:2 	 global-step:23222	 l-p:0.11221655458211899
epoch£º1161	 i:3 	 global-step:23223	 l-p:0.13503317534923553
epoch£º1161	 i:4 	 global-step:23224	 l-p:0.10077562183141708
epoch£º1161	 i:5 	 global-step:23225	 l-p:0.21852844953536987
epoch£º1161	 i:6 	 global-step:23226	 l-p:0.18724662065505981
epoch£º1161	 i:7 	 global-step:23227	 l-p:0.18842244148254395
epoch£º1161	 i:8 	 global-step:23228	 l-p:0.11854224652051926
epoch£º1161	 i:9 	 global-step:23229	 l-p:0.1472765952348709
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1162
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6535,  0.5671,  1.0000,  0.4922,
          1.0000,  0.8678, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1313,  0.0668,  1.0000,  0.0339,
          1.0000,  0.5083, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2354,  0.1454,  1.0000,  0.0898,
          1.0000,  0.6175, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4474,  0.3422,  1.0000,  0.2617,
          1.0000,  0.7648, 31.6228]], device='cuda:0')
 pt:tensor([[5.1675, 5.1026, 4.7708],
        [5.1675, 5.0262, 5.0867],
        [5.1675, 4.9098, 4.8812],
        [5.1675, 4.8993, 4.6036]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1162, step:0 
model_pd.l_p.mean(): 0.21443840861320496 
model_pd.l_d.mean(): -20.056001663208008 
model_pd.lagr.mean(): -19.841564178466797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5186], device='cuda:0')), ('power', tensor([-20.9681], device='cuda:0'))])
epoch£º1162	 i:0 	 global-step:23240	 l-p:0.21443840861320496
epoch£º1162	 i:1 	 global-step:23241	 l-p:0.09423412382602692
epoch£º1162	 i:2 	 global-step:23242	 l-p:0.12430410832166672
epoch£º1162	 i:3 	 global-step:23243	 l-p:0.14890970289707184
epoch£º1162	 i:4 	 global-step:23244	 l-p:0.12159795314073563
epoch£º1162	 i:5 	 global-step:23245	 l-p:0.07962209731340408
epoch£º1162	 i:6 	 global-step:23246	 l-p:0.1905800998210907
epoch£º1162	 i:7 	 global-step:23247	 l-p:0.11520186811685562
epoch£º1162	 i:8 	 global-step:23248	 l-p:0.13479606807231903
epoch£º1162	 i:9 	 global-step:23249	 l-p:0.14163556694984436
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1163
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1860, 5.0406, 4.6966],
        [5.1860, 5.1860, 5.1860],
        [5.1860, 5.1860, 5.1860],
        [5.1860, 5.5131, 5.3766]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1163, step:0 
model_pd.l_p.mean(): 0.11374843120574951 
model_pd.l_d.mean(): -19.695932388305664 
model_pd.lagr.mean(): -19.582183837890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4856], device='cuda:0')), ('power', tensor([-20.5672], device='cuda:0'))])
epoch£º1163	 i:0 	 global-step:23260	 l-p:0.11374843120574951
epoch£º1163	 i:1 	 global-step:23261	 l-p:0.10143918544054031
epoch£º1163	 i:2 	 global-step:23262	 l-p:0.1866290271282196
epoch£º1163	 i:3 	 global-step:23263	 l-p:0.11183062195777893
epoch£º1163	 i:4 	 global-step:23264	 l-p:0.15922994911670685
epoch£º1163	 i:5 	 global-step:23265	 l-p:0.14448778331279755
epoch£º1163	 i:6 	 global-step:23266	 l-p:0.09655158966779709
epoch£º1163	 i:7 	 global-step:23267	 l-p:0.08011985570192337
epoch£º1163	 i:8 	 global-step:23268	 l-p:0.15921622514724731
epoch£º1163	 i:9 	 global-step:23269	 l-p:0.15879705548286438
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1164
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1270,  0.0638,  1.0000,  0.0321,
          1.0000,  0.5026, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1497,  0.0795,  1.0000,  0.0422,
          1.0000,  0.5310, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2501,  0.1576,  1.0000,  0.0993,
          1.0000,  0.6301, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.9439,  0.9259,  1.0000,  0.9083,
          1.0000,  0.9809, 31.6228]], device='cuda:0')
 pt:tensor([[5.1892, 5.0544, 5.1151],
        [5.1892, 5.0237, 5.0783],
        [5.1892, 4.9222, 4.8728],
        [5.1892, 5.5659, 5.4611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1164, step:0 
model_pd.l_p.mean(): 0.15446768701076508 
model_pd.l_d.mean(): -19.930017471313477 
model_pd.lagr.mean(): -19.775548934936523 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4972], device='cuda:0')), ('power', tensor([-20.8177], device='cuda:0'))])
epoch£º1164	 i:0 	 global-step:23280	 l-p:0.15446768701076508
epoch£º1164	 i:1 	 global-step:23281	 l-p:0.10528675466775894
epoch£º1164	 i:2 	 global-step:23282	 l-p:0.18836581707000732
epoch£º1164	 i:3 	 global-step:23283	 l-p:0.04279838129878044
epoch£º1164	 i:4 	 global-step:23284	 l-p:0.13144434988498688
epoch£º1164	 i:5 	 global-step:23285	 l-p:0.15627816319465637
epoch£º1164	 i:6 	 global-step:23286	 l-p:0.13068372011184692
epoch£º1164	 i:7 	 global-step:23287	 l-p:0.13820450007915497
epoch£º1164	 i:8 	 global-step:23288	 l-p:0.14884930849075317
epoch£º1164	 i:9 	 global-step:23289	 l-p:0.11470132321119308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1165
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1883, 4.9386, 4.6269],
        [5.1883, 5.0597, 4.7166],
        [5.1883, 4.9657, 4.9828],
        [5.1883, 5.1883, 5.1883]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1165, step:0 
model_pd.l_p.mean(): 0.1170739158987999 
model_pd.l_d.mean(): -19.798704147338867 
model_pd.lagr.mean(): -19.681631088256836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5143], device='cuda:0')), ('power', tensor([-20.7016], device='cuda:0'))])
epoch£º1165	 i:0 	 global-step:23300	 l-p:0.1170739158987999
epoch£º1165	 i:1 	 global-step:23301	 l-p:0.0760943740606308
epoch£º1165	 i:2 	 global-step:23302	 l-p:0.15077801048755646
epoch£º1165	 i:3 	 global-step:23303	 l-p:0.17411597073078156
epoch£º1165	 i:4 	 global-step:23304	 l-p:0.10484184324741364
epoch£º1165	 i:5 	 global-step:23305	 l-p:0.10584676265716553
epoch£º1165	 i:6 	 global-step:23306	 l-p:0.09007789194583893
epoch£º1165	 i:7 	 global-step:23307	 l-p:0.21790745854377747
epoch£º1165	 i:8 	 global-step:23308	 l-p:0.1928497701883316
epoch£º1165	 i:9 	 global-step:23309	 l-p:0.11290915310382843
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1166
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1952e-02, 1.0139e-02,
         1.0000e+00, 3.2173e-03, 1.0000e+00, 3.1732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1766, 5.1766, 5.1766],
        [5.1766, 5.0336, 5.0938],
        [5.1766, 5.1621, 5.1751],
        [5.1766, 5.0414, 5.1023]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1166, step:0 
model_pd.l_p.mean(): 0.17284496128559113 
model_pd.l_d.mean(): -20.3607120513916 
model_pd.lagr.mean(): -20.1878662109375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4501], device='cuda:0')), ('power', tensor([-21.2076], device='cuda:0'))])
epoch£º1166	 i:0 	 global-step:23320	 l-p:0.17284496128559113
epoch£º1166	 i:1 	 global-step:23321	 l-p:0.15524554252624512
epoch£º1166	 i:2 	 global-step:23322	 l-p:0.15070141851902008
epoch£º1166	 i:3 	 global-step:23323	 l-p:0.1082070991396904
epoch£º1166	 i:4 	 global-step:23324	 l-p:0.14121979475021362
epoch£º1166	 i:5 	 global-step:23325	 l-p:0.1076519563794136
epoch£º1166	 i:6 	 global-step:23326	 l-p:0.12202908843755722
epoch£º1166	 i:7 	 global-step:23327	 l-p:0.11852937191724777
epoch£º1166	 i:8 	 global-step:23328	 l-p:0.1463419497013092
epoch£º1166	 i:9 	 global-step:23329	 l-p:0.14142999053001404
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1167
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1660, 5.0598, 5.1192],
        [5.1660, 5.1435, 5.1630],
        [5.1660, 4.8971, 4.8481],
        [5.1660, 4.9406, 4.9565]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1167, step:0 
model_pd.l_p.mean(): 0.14445343613624573 
model_pd.l_d.mean(): -20.296314239501953 
model_pd.lagr.mean(): -20.1518611907959 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4968], device='cuda:0')), ('power', tensor([-21.1903], device='cuda:0'))])
epoch£º1167	 i:0 	 global-step:23340	 l-p:0.14445343613624573
epoch£º1167	 i:1 	 global-step:23341	 l-p:0.08273380994796753
epoch£º1167	 i:2 	 global-step:23342	 l-p:0.18325921893119812
epoch£º1167	 i:3 	 global-step:23343	 l-p:0.20786520838737488
epoch£º1167	 i:4 	 global-step:23344	 l-p:0.08738723397254944
epoch£º1167	 i:5 	 global-step:23345	 l-p:0.12720921635627747
epoch£º1167	 i:6 	 global-step:23346	 l-p:0.1564681977033615
epoch£º1167	 i:7 	 global-step:23347	 l-p:0.13149023056030273
epoch£º1167	 i:8 	 global-step:23348	 l-p:0.12698936462402344
epoch£º1167	 i:9 	 global-step:23349	 l-p:0.187723770737648
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1168
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7277e-02, 4.4662e-03,
         1.0000e+00, 1.1546e-03, 1.0000e+00, 2.5851e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9989e-02, 5.4247e-03,
         1.0000e+00, 1.4722e-03, 1.0000e+00, 2.7139e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8467e-01, 9.7961e-01,
         1.0000e+00, 9.7458e-01, 1.0000e+00, 9.9486e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1528, 5.1482, 5.1526],
        [5.1528, 5.1621, 4.8554],
        [5.1528, 5.1467, 5.1525],
        [5.1528, 5.5814, 5.5119]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1168, step:0 
model_pd.l_p.mean(): 0.17780303955078125 
model_pd.l_d.mean(): -17.2648983001709 
model_pd.lagr.mean(): -17.087095260620117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6636], device='cuda:0')), ('power', tensor([-18.2751], device='cuda:0'))])
epoch£º1168	 i:0 	 global-step:23360	 l-p:0.17780303955078125
epoch£º1168	 i:1 	 global-step:23361	 l-p:0.11927289515733719
epoch£º1168	 i:2 	 global-step:23362	 l-p:0.13168057799339294
epoch£º1168	 i:3 	 global-step:23363	 l-p:0.27613526582717896
epoch£º1168	 i:4 	 global-step:23364	 l-p:0.1272864192724228
epoch£º1168	 i:5 	 global-step:23365	 l-p:0.1592148393392563
epoch£º1168	 i:6 	 global-step:23366	 l-p:0.1168828010559082
epoch£º1168	 i:7 	 global-step:23367	 l-p:0.12844133377075195
epoch£º1168	 i:8 	 global-step:23368	 l-p:0.1305973082780838
epoch£º1168	 i:9 	 global-step:23369	 l-p:0.11251933127641678
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1169
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0624e-01, 5.0316e-02,
         1.0000e+00, 2.3831e-02, 1.0000e+00, 4.7362e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1546, 4.9999, 4.6529],
        [5.1546, 5.1522, 5.1545],
        [5.1546, 5.0818, 4.7472],
        [5.1546, 5.0481, 5.1077]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1169, step:0 
model_pd.l_p.mean(): 0.15148496627807617 
model_pd.l_d.mean(): -20.502914428710938 
model_pd.lagr.mean(): -20.351428985595703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4269], device='cuda:0')), ('power', tensor([-21.3284], device='cuda:0'))])
epoch£º1169	 i:0 	 global-step:23380	 l-p:0.15148496627807617
epoch£º1169	 i:1 	 global-step:23381	 l-p:0.14974971115589142
epoch£º1169	 i:2 	 global-step:23382	 l-p:0.14181460440158844
epoch£º1169	 i:3 	 global-step:23383	 l-p:0.20428189635276794
epoch£º1169	 i:4 	 global-step:23384	 l-p:0.13500380516052246
epoch£º1169	 i:5 	 global-step:23385	 l-p:0.11660775542259216
epoch£º1169	 i:6 	 global-step:23386	 l-p:0.10193141549825668
epoch£º1169	 i:7 	 global-step:23387	 l-p:0.16683891415596008
epoch£º1169	 i:8 	 global-step:23388	 l-p:0.17213857173919678
epoch£º1169	 i:9 	 global-step:23389	 l-p:0.11938831210136414
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1170
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1599, 5.1599, 5.1599],
        [5.1599, 4.8830, 4.8175],
        [5.1599, 4.9169, 4.9127],
        [5.1599, 4.9048, 4.5917]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1170, step:0 
model_pd.l_p.mean(): 0.14338098466396332 
model_pd.l_d.mean(): -19.90284538269043 
model_pd.lagr.mean(): -19.759464263916016 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5118], device='cuda:0')), ('power', tensor([-20.8051], device='cuda:0'))])
epoch£º1170	 i:0 	 global-step:23400	 l-p:0.14338098466396332
epoch£º1170	 i:1 	 global-step:23401	 l-p:0.12860675156116486
epoch£º1170	 i:2 	 global-step:23402	 l-p:0.12609434127807617
epoch£º1170	 i:3 	 global-step:23403	 l-p:0.09279758483171463
epoch£º1170	 i:4 	 global-step:23404	 l-p:0.12399531900882721
epoch£º1170	 i:5 	 global-step:23405	 l-p:0.17090119421482086
epoch£º1170	 i:6 	 global-step:23406	 l-p:0.18102824687957764
epoch£º1170	 i:7 	 global-step:23407	 l-p:0.20291873812675476
epoch£º1170	 i:8 	 global-step:23408	 l-p:0.15509067475795746
epoch£º1170	 i:9 	 global-step:23409	 l-p:0.09627870470285416
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1171
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9820e-01, 5.0403e-01,
         1.0000e+00, 4.2469e-01, 1.0000e+00, 8.4259e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1657, 5.1650, 5.1657],
        [5.1657, 5.0421, 4.6978],
        [5.1657, 5.0313, 4.6858],
        [5.1657, 5.0380, 5.0994]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1171, step:0 
model_pd.l_p.mean(): 0.13349071145057678 
model_pd.l_d.mean(): -20.257122039794922 
model_pd.lagr.mean(): -20.12363052368164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4497], device='cuda:0')), ('power', tensor([-21.1017], device='cuda:0'))])
epoch£º1171	 i:0 	 global-step:23420	 l-p:0.13349071145057678
epoch£º1171	 i:1 	 global-step:23421	 l-p:0.19369719922542572
epoch£º1171	 i:2 	 global-step:23422	 l-p:0.11161195486783981
epoch£º1171	 i:3 	 global-step:23423	 l-p:0.13659660518169403
epoch£º1171	 i:4 	 global-step:23424	 l-p:0.16758763790130615
epoch£º1171	 i:5 	 global-step:23425	 l-p:0.0863027349114418
epoch£º1171	 i:6 	 global-step:23426	 l-p:0.19815166294574738
epoch£º1171	 i:7 	 global-step:23427	 l-p:0.1201288029551506
epoch£º1171	 i:8 	 global-step:23428	 l-p:0.1178780347108841
epoch£º1171	 i:9 	 global-step:23429	 l-p:0.14541110396385193
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1172
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3873e-02, 3.3333e-03,
         1.0000e+00, 8.0093e-04, 1.0000e+00, 2.4028e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6142e-02, 4.0795e-03,
         1.0000e+00, 1.0310e-03, 1.0000e+00, 2.5273e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3264e-01, 6.7642e-02,
         1.0000e+00, 3.4496e-02, 1.0000e+00, 5.0998e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1626, 5.1595, 5.1625],
        [5.1626, 5.1594, 5.1624],
        [5.1626, 5.1585, 5.1624],
        [5.1626, 5.0190, 5.0795]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1172, step:0 
model_pd.l_p.mean(): 0.13109125196933746 
model_pd.l_d.mean(): -20.640392303466797 
model_pd.lagr.mean(): -20.509300231933594 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4137], device='cuda:0')), ('power', tensor([-21.4548], device='cuda:0'))])
epoch£º1172	 i:0 	 global-step:23440	 l-p:0.13109125196933746
epoch£º1172	 i:1 	 global-step:23441	 l-p:0.17860238254070282
epoch£º1172	 i:2 	 global-step:23442	 l-p:0.14623668789863586
epoch£º1172	 i:3 	 global-step:23443	 l-p:0.09024453163146973
epoch£º1172	 i:4 	 global-step:23444	 l-p:0.1460757851600647
epoch£º1172	 i:5 	 global-step:23445	 l-p:0.1457108110189438
epoch£º1172	 i:6 	 global-step:23446	 l-p:0.13231638073921204
epoch£º1172	 i:7 	 global-step:23447	 l-p:0.20974063873291016
epoch£º1172	 i:8 	 global-step:23448	 l-p:0.13589787483215332
epoch£º1172	 i:9 	 global-step:23449	 l-p:0.11480169743299484
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1173
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5110e-01, 2.4769e-01,
         1.0000e+00, 1.7474e-01, 1.0000e+00, 7.0547e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1603, 4.8589, 4.6646],
        [5.1603, 5.1603, 5.1603],
        [5.1603, 4.8765, 4.7926],
        [5.1603, 5.1603, 5.1603]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1173, step:0 
model_pd.l_p.mean(): 0.1316116452217102 
model_pd.l_d.mean(): -19.94647216796875 
model_pd.lagr.mean(): -19.814861297607422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4639], device='cuda:0')), ('power', tensor([-20.7999], device='cuda:0'))])
epoch£º1173	 i:0 	 global-step:23460	 l-p:0.1316116452217102
epoch£º1173	 i:1 	 global-step:23461	 l-p:0.1030515730381012
epoch£º1173	 i:2 	 global-step:23462	 l-p:0.15997706353664398
epoch£º1173	 i:3 	 global-step:23463	 l-p:0.1281747817993164
epoch£º1173	 i:4 	 global-step:23464	 l-p:0.1149807795882225
epoch£º1173	 i:5 	 global-step:23465	 l-p:0.11460918188095093
epoch£º1173	 i:6 	 global-step:23466	 l-p:0.15668094158172607
epoch£º1173	 i:7 	 global-step:23467	 l-p:0.14904403686523438
epoch£º1173	 i:8 	 global-step:23468	 l-p:0.19749943912029266
epoch£º1173	 i:9 	 global-step:23469	 l-p:0.16232287883758545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1174
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5364e-01, 8.2288e-02,
         1.0000e+00, 4.4073e-02, 1.0000e+00, 5.3559e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3929e-01, 6.6848e-01,
         1.0000e+00, 6.0445e-01, 1.0000e+00, 9.0421e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1678, 4.9961, 5.0494],
        [5.1678, 5.0380, 5.0993],
        [5.1678, 4.8765, 4.6237],
        [5.1678, 5.2195, 4.9306]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1174, step:0 
model_pd.l_p.mean(): 0.16028256714344025 
model_pd.l_d.mean(): -19.526090621948242 
model_pd.lagr.mean(): -19.365808486938477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5083], device='cuda:0')), ('power', tensor([-20.4177], device='cuda:0'))])
epoch£º1174	 i:0 	 global-step:23480	 l-p:0.16028256714344025
epoch£º1174	 i:1 	 global-step:23481	 l-p:0.18160124123096466
epoch£º1174	 i:2 	 global-step:23482	 l-p:0.12891018390655518
epoch£º1174	 i:3 	 global-step:23483	 l-p:0.13252854347229004
epoch£º1174	 i:4 	 global-step:23484	 l-p:0.15598490834236145
epoch£º1174	 i:5 	 global-step:23485	 l-p:0.1188279539346695
epoch£º1174	 i:6 	 global-step:23486	 l-p:0.11285673081874847
epoch£º1174	 i:7 	 global-step:23487	 l-p:0.10746244341135025
epoch£º1174	 i:8 	 global-step:23488	 l-p:0.1327430158853531
epoch£º1174	 i:9 	 global-step:23489	 l-p:0.1563519686460495
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1175
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4450e-01, 9.2669e-01,
         1.0000e+00, 9.0922e-01, 1.0000e+00, 9.8115e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3580e-03, 3.1386e-04,
         1.0000e+00, 4.1775e-05, 1.0000e+00, 1.3310e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1705, 5.5402, 5.4309],
        [5.1705, 5.0061, 5.0619],
        [5.1705, 4.8724, 4.7231],
        [5.1705, 5.1704, 5.1705]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1175, step:0 
model_pd.l_p.mean(): 0.11727544665336609 
model_pd.l_d.mean(): -18.871273040771484 
model_pd.lagr.mean(): -18.753997802734375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5899], device='cuda:0')), ('power', tensor([-19.8352], device='cuda:0'))])
epoch£º1175	 i:0 	 global-step:23500	 l-p:0.11727544665336609
epoch£º1175	 i:1 	 global-step:23501	 l-p:0.21038758754730225
epoch£º1175	 i:2 	 global-step:23502	 l-p:0.1316346377134323
epoch£º1175	 i:3 	 global-step:23503	 l-p:0.12775938212871552
epoch£º1175	 i:4 	 global-step:23504	 l-p:0.08229225128889084
epoch£º1175	 i:5 	 global-step:23505	 l-p:0.11036452651023865
epoch£º1175	 i:6 	 global-step:23506	 l-p:0.1277756541967392
epoch£º1175	 i:7 	 global-step:23507	 l-p:0.15295672416687012
epoch£º1175	 i:8 	 global-step:23508	 l-p:0.1942034810781479
epoch£º1175	 i:9 	 global-step:23509	 l-p:0.1350196897983551
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1176
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3545e-01, 1.4539e-01,
         1.0000e+00, 8.9776e-02, 1.0000e+00, 6.1749e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3261e-01, 1.4306e-01,
         1.0000e+00, 8.7982e-02, 1.0000e+00, 6.1501e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1690, 4.9106, 4.8820],
        [5.1690, 4.8684, 4.6882],
        [5.1690, 5.1690, 5.1690],
        [5.1690, 4.9128, 4.8881]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1176, step:0 
model_pd.l_p.mean(): 0.12325583398342133 
model_pd.l_d.mean(): -19.79539680480957 
model_pd.lagr.mean(): -19.67214012145996 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4351], device='cuda:0')), ('power', tensor([-20.6162], device='cuda:0'))])
epoch£º1176	 i:0 	 global-step:23520	 l-p:0.12325583398342133
epoch£º1176	 i:1 	 global-step:23521	 l-p:0.12253355979919434
epoch£º1176	 i:2 	 global-step:23522	 l-p:0.18119306862354279
epoch£º1176	 i:3 	 global-step:23523	 l-p:0.11040565371513367
epoch£º1176	 i:4 	 global-step:23524	 l-p:0.09138461947441101
epoch£º1176	 i:5 	 global-step:23525	 l-p:0.12169855833053589
epoch£º1176	 i:6 	 global-step:23526	 l-p:0.179641455411911
epoch£º1176	 i:7 	 global-step:23527	 l-p:0.14391502737998962
epoch£º1176	 i:8 	 global-step:23528	 l-p:0.1595049798488617
epoch£º1176	 i:9 	 global-step:23529	 l-p:0.14729173481464386
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1177
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2872e-02, 3.0166e-03,
         1.0000e+00, 7.0696e-04, 1.0000e+00, 2.3436e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5321e-01, 6.8531e-01,
         1.0000e+00, 6.2353e-01, 1.0000e+00, 9.0985e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1744, 5.1449, 5.1696],
        [5.1744, 5.1169, 5.1591],
        [5.1744, 5.1718, 5.1743],
        [5.1744, 5.2479, 4.9689]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1177, step:0 
model_pd.l_p.mean(): 0.18546220660209656 
model_pd.l_d.mean(): -20.227798461914062 
model_pd.lagr.mean(): -20.042335510253906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4672], device='cuda:0')), ('power', tensor([-21.0900], device='cuda:0'))])
epoch£º1177	 i:0 	 global-step:23540	 l-p:0.18546220660209656
epoch£º1177	 i:1 	 global-step:23541	 l-p:0.08707235008478165
epoch£º1177	 i:2 	 global-step:23542	 l-p:0.14928512275218964
epoch£º1177	 i:3 	 global-step:23543	 l-p:0.13595308363437653
epoch£º1177	 i:4 	 global-step:23544	 l-p:0.10661718249320984
epoch£º1177	 i:5 	 global-step:23545	 l-p:0.1238032728433609
epoch£º1177	 i:6 	 global-step:23546	 l-p:0.14125631749629974
epoch£º1177	 i:7 	 global-step:23547	 l-p:0.17070946097373962
epoch£º1177	 i:8 	 global-step:23548	 l-p:0.13709577918052673
epoch£º1177	 i:9 	 global-step:23549	 l-p:0.13670873641967773
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1178
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5576e-02, 1.6280e-02,
         1.0000e+00, 5.8152e-03, 1.0000e+00, 3.5720e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1737, 5.1737, 5.1737],
        [5.1737, 5.1464, 5.1695],
        [5.1737, 4.8775, 4.7402],
        [5.1737, 5.1737, 5.1737]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1178, step:0 
model_pd.l_p.mean(): 0.1903761625289917 
model_pd.l_d.mean(): -20.773883819580078 
model_pd.lagr.mean(): -20.583507537841797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3899], device='cuda:0')), ('power', tensor([-21.5662], device='cuda:0'))])
epoch£º1178	 i:0 	 global-step:23560	 l-p:0.1903761625289917
epoch£º1178	 i:1 	 global-step:23561	 l-p:0.10084439814090729
epoch£º1178	 i:2 	 global-step:23562	 l-p:0.11940698325634003
epoch£º1178	 i:3 	 global-step:23563	 l-p:0.14826448261737823
epoch£º1178	 i:4 	 global-step:23564	 l-p:0.1103925034403801
epoch£º1178	 i:5 	 global-step:23565	 l-p:0.11097551137208939
epoch£º1178	 i:6 	 global-step:23566	 l-p:0.16743533313274384
epoch£º1178	 i:7 	 global-step:23567	 l-p:0.11945626884698868
epoch£º1178	 i:8 	 global-step:23568	 l-p:0.1613849401473999
epoch£º1178	 i:9 	 global-step:23569	 l-p:0.13812243938446045
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1179
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7705e-02, 1.2643e-02,
         1.0000e+00, 4.2396e-03, 1.0000e+00, 3.3532e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5065e-01, 5.6381e-01,
         1.0000e+00, 4.8856e-01, 1.0000e+00, 8.6653e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1869e-02, 1.9344e-02,
         1.0000e+00, 7.2140e-03, 1.0000e+00, 3.7294e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1808, 5.0022, 5.0523],
        [5.1808, 5.1612, 5.1784],
        [5.1808, 5.1135, 4.7806],
        [5.1808, 5.1466, 5.1746]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1179, step:0 
model_pd.l_p.mean(): 0.13521522283554077 
model_pd.l_d.mean(): -20.530731201171875 
model_pd.lagr.mean(): -20.39551544189453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4234], device='cuda:0')), ('power', tensor([-21.3531], device='cuda:0'))])
epoch£º1179	 i:0 	 global-step:23580	 l-p:0.13521522283554077
epoch£º1179	 i:1 	 global-step:23581	 l-p:0.11185609549283981
epoch£º1179	 i:2 	 global-step:23582	 l-p:0.15237024426460266
epoch£º1179	 i:3 	 global-step:23583	 l-p:0.12310836464166641
epoch£º1179	 i:4 	 global-step:23584	 l-p:0.12678088247776031
epoch£º1179	 i:5 	 global-step:23585	 l-p:0.12955324351787567
epoch£º1179	 i:6 	 global-step:23586	 l-p:0.1449083685874939
epoch£º1179	 i:7 	 global-step:23587	 l-p:0.18070189654827118
epoch£º1179	 i:8 	 global-step:23588	 l-p:0.12775744497776031
epoch£º1179	 i:9 	 global-step:23589	 l-p:0.1127825528383255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1180
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0939e-02, 2.9366e-02,
         1.0000e+00, 1.2157e-02, 1.0000e+00, 4.1396e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3585e-02, 3.6546e-02,
         1.0000e+00, 1.5979e-02, 1.0000e+00, 4.3723e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1837, 5.1747, 5.1831],
        [5.1837, 5.1262, 5.1684],
        [5.1837, 5.1093, 5.1594],
        [5.1837, 5.1640, 5.1813]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1180, step:0 
model_pd.l_p.mean(): 0.1403696984052658 
model_pd.l_d.mean(): -19.57378387451172 
model_pd.lagr.mean(): -19.433414459228516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4672], device='cuda:0')), ('power', tensor([-20.4237], device='cuda:0'))])
epoch£º1180	 i:0 	 global-step:23600	 l-p:0.1403696984052658
epoch£º1180	 i:1 	 global-step:23601	 l-p:0.12379167228937149
epoch£º1180	 i:2 	 global-step:23602	 l-p:0.13192245364189148
epoch£º1180	 i:3 	 global-step:23603	 l-p:0.14812442660331726
epoch£º1180	 i:4 	 global-step:23604	 l-p:0.13470959663391113
epoch£º1180	 i:5 	 global-step:23605	 l-p:0.1538291722536087
epoch£º1180	 i:6 	 global-step:23606	 l-p:0.0894940048456192
epoch£º1180	 i:7 	 global-step:23607	 l-p:0.11608975380659103
epoch£º1180	 i:8 	 global-step:23608	 l-p:0.15189464390277863
epoch£º1180	 i:9 	 global-step:23609	 l-p:0.19843901693820953
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1181
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0085e-01, 8.7004e-01,
         1.0000e+00, 8.4028e-01, 1.0000e+00, 9.6579e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0760e-02, 1.4027e-02,
         1.0000e+00, 4.8274e-03, 1.0000e+00, 3.4415e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6955e-01, 8.2997e-01,
         1.0000e+00, 7.9219e-01, 1.0000e+00, 9.5448e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1651, 5.4623, 5.3068],
        [5.1651, 5.1564, 5.1645],
        [5.1651, 5.1426, 5.1621],
        [5.1651, 5.4128, 5.2273]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1181, step:0 
model_pd.l_p.mean(): 0.12470792233943939 
model_pd.l_d.mean(): -19.862442016601562 
model_pd.lagr.mean(): -19.737733840942383 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5322], device='cuda:0')), ('power', tensor([-20.7851], device='cuda:0'))])
epoch£º1181	 i:0 	 global-step:23620	 l-p:0.12470792233943939
epoch£º1181	 i:1 	 global-step:23621	 l-p:0.18641039729118347
epoch£º1181	 i:2 	 global-step:23622	 l-p:0.12574483454227448
epoch£º1181	 i:3 	 global-step:23623	 l-p:0.19463081657886505
epoch£º1181	 i:4 	 global-step:23624	 l-p:0.16672375798225403
epoch£º1181	 i:5 	 global-step:23625	 l-p:0.0933680310845375
epoch£º1181	 i:6 	 global-step:23626	 l-p:0.11617143452167511
epoch£º1181	 i:7 	 global-step:23627	 l-p:0.08532863110303879
epoch£º1181	 i:8 	 global-step:23628	 l-p:0.17806583642959595
epoch£º1181	 i:9 	 global-step:23629	 l-p:0.15237779915332794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1182
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8102e-01, 1.0240e-01,
         1.0000e+00, 5.7925e-02, 1.0000e+00, 5.6568e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1592, 5.0281, 4.6818],
        [5.1592, 4.9797, 5.0302],
        [5.1592, 4.9530, 4.9875],
        [5.1592, 5.1592, 5.1592]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1182, step:0 
model_pd.l_p.mean(): 0.12714603543281555 
model_pd.l_d.mean(): -20.471254348754883 
model_pd.lagr.mean(): -20.34410858154297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4286], device='cuda:0')), ('power', tensor([-21.2980], device='cuda:0'))])
epoch£º1182	 i:0 	 global-step:23640	 l-p:0.12714603543281555
epoch£º1182	 i:1 	 global-step:23641	 l-p:0.1494772881269455
epoch£º1182	 i:2 	 global-step:23642	 l-p:0.19806699454784393
epoch£º1182	 i:3 	 global-step:23643	 l-p:0.12770627439022064
epoch£º1182	 i:4 	 global-step:23644	 l-p:0.10749869048595428
epoch£º1182	 i:5 	 global-step:23645	 l-p:0.21705889701843262
epoch£º1182	 i:6 	 global-step:23646	 l-p:0.08017458021640778
epoch£º1182	 i:7 	 global-step:23647	 l-p:0.1673821210861206
epoch£º1182	 i:8 	 global-step:23648	 l-p:0.13002635538578033
epoch£º1182	 i:9 	 global-step:23649	 l-p:0.16916751861572266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1183
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1024e-01, 7.5535e-01,
         1.0000e+00, 7.0418e-01, 1.0000e+00, 9.3226e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0003e-01, 2.9475e-01,
         1.0000e+00, 2.1718e-01, 1.0000e+00, 7.3682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1546, 5.4099, 5.2291],
        [5.1546, 5.3068, 5.0675],
        [5.1546, 5.1546, 5.1546],
        [5.1546, 4.8605, 4.6072]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1183, step:0 
model_pd.l_p.mean(): 0.10156749188899994 
model_pd.l_d.mean(): -20.094337463378906 
model_pd.lagr.mean(): -19.992769241333008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5083], device='cuda:0')), ('power', tensor([-20.9966], device='cuda:0'))])
epoch£º1183	 i:0 	 global-step:23660	 l-p:0.10156749188899994
epoch£º1183	 i:1 	 global-step:23661	 l-p:0.14125993847846985
epoch£º1183	 i:2 	 global-step:23662	 l-p:0.09239138662815094
epoch£º1183	 i:3 	 global-step:23663	 l-p:0.1462743580341339
epoch£º1183	 i:4 	 global-step:23664	 l-p:0.1986195147037506
epoch£º1183	 i:5 	 global-step:23665	 l-p:0.21528621017932892
epoch£º1183	 i:6 	 global-step:23666	 l-p:0.14043235778808594
epoch£º1183	 i:7 	 global-step:23667	 l-p:0.13565419614315033
epoch£º1183	 i:8 	 global-step:23668	 l-p:0.11808902770280838
epoch£º1183	 i:9 	 global-step:23669	 l-p:0.1600942313671112
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1184
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3190e-01, 6.5958e-01,
         1.0000e+00, 5.9441e-01, 1.0000e+00, 9.0119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1688e-01, 1.3031e-01,
         1.0000e+00, 7.8290e-02, 1.0000e+00, 6.0082e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1601, 5.0908, 4.7564],
        [5.1601, 5.1517, 5.1596],
        [5.1601, 5.1978, 4.9024],
        [5.1601, 4.9165, 4.9124]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1184, step:0 
model_pd.l_p.mean(): 0.17124441266059875 
model_pd.l_d.mean(): -20.472797393798828 
model_pd.lagr.mean(): -20.30155372619629 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4398], device='cuda:0')), ('power', tensor([-21.3111], device='cuda:0'))])
epoch£º1184	 i:0 	 global-step:23680	 l-p:0.17124441266059875
epoch£º1184	 i:1 	 global-step:23681	 l-p:0.16943491995334625
epoch£º1184	 i:2 	 global-step:23682	 l-p:0.17657676339149475
epoch£º1184	 i:3 	 global-step:23683	 l-p:0.11656520515680313
epoch£º1184	 i:4 	 global-step:23684	 l-p:0.1021038144826889
epoch£º1184	 i:5 	 global-step:23685	 l-p:0.14862854778766632
epoch£º1184	 i:6 	 global-step:23686	 l-p:0.13912400603294373
epoch£º1184	 i:7 	 global-step:23687	 l-p:0.07453048974275589
epoch£º1184	 i:8 	 global-step:23688	 l-p:0.19637498259544373
epoch£º1184	 i:9 	 global-step:23689	 l-p:0.11396777629852295
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1185
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0862e-01, 2.0856e-01,
         1.0000e+00, 1.4094e-01, 1.0000e+00, 6.7578e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0050e-01, 1.1735e-01,
         1.0000e+00, 6.8681e-02, 1.0000e+00, 5.8529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0389e-01, 1.2000e-01,
         1.0000e+00, 7.0632e-02, 1.0000e+00, 5.8857e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1927e-01, 5.8710e-02,
         1.0000e+00, 2.8899e-02, 1.0000e+00, 4.9224e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1749, 4.8786, 4.7434],
        [5.1749, 4.9480, 4.9629],
        [5.1749, 4.9446, 4.9556],
        [5.1749, 5.0500, 5.1114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1185, step:0 
model_pd.l_p.mean(): 0.15287040174007416 
model_pd.l_d.mean(): -20.344724655151367 
model_pd.lagr.mean(): -20.19185447692871 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4485], device='cuda:0')), ('power', tensor([-21.1897], device='cuda:0'))])
epoch£º1185	 i:0 	 global-step:23700	 l-p:0.15287040174007416
epoch£º1185	 i:1 	 global-step:23701	 l-p:0.12648828327655792
epoch£º1185	 i:2 	 global-step:23702	 l-p:0.15810248255729675
epoch£º1185	 i:3 	 global-step:23703	 l-p:0.12133157253265381
epoch£º1185	 i:4 	 global-step:23704	 l-p:0.12368565052747726
epoch£º1185	 i:5 	 global-step:23705	 l-p:0.18555551767349243
epoch£º1185	 i:6 	 global-step:23706	 l-p:0.13406702876091003
epoch£º1185	 i:7 	 global-step:23707	 l-p:0.13911674916744232
epoch£º1185	 i:8 	 global-step:23708	 l-p:0.10868257284164429
epoch£º1185	 i:9 	 global-step:23709	 l-p:0.10806523263454437
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1186
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1822, 5.1383, 5.1727],
        [5.1822, 5.1295, 5.1691],
        [5.1822, 4.9108, 4.8560],
        [5.1822, 5.1383, 4.8118]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1186, step:0 
model_pd.l_p.mean(): 0.08736569434404373 
model_pd.l_d.mean(): -20.32921600341797 
model_pd.lagr.mean(): -20.241849899291992 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4539], device='cuda:0')), ('power', tensor([-21.1794], device='cuda:0'))])
epoch£º1186	 i:0 	 global-step:23720	 l-p:0.08736569434404373
epoch£º1186	 i:1 	 global-step:23721	 l-p:0.11964334547519684
epoch£º1186	 i:2 	 global-step:23722	 l-p:0.1361350417137146
epoch£º1186	 i:3 	 global-step:23723	 l-p:0.14731745421886444
epoch£º1186	 i:4 	 global-step:23724	 l-p:0.13652893900871277
epoch£º1186	 i:5 	 global-step:23725	 l-p:0.18633349239826202
epoch£º1186	 i:6 	 global-step:23726	 l-p:0.11031253635883331
epoch£º1186	 i:7 	 global-step:23727	 l-p:0.13748334348201752
epoch£º1186	 i:8 	 global-step:23728	 l-p:0.11769517511129379
epoch£º1186	 i:9 	 global-step:23729	 l-p:0.15627071261405945
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1187
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1857, 5.0319, 5.0904],
        [5.1857, 5.3098, 5.0553],
        [5.1857, 5.1031, 5.1563],
        [5.1857, 5.1843, 5.1857]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1187, step:0 
model_pd.l_p.mean(): 0.17839482426643372 
model_pd.l_d.mean(): -20.319557189941406 
model_pd.lagr.mean(): -20.141162872314453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4522], device='cuda:0')), ('power', tensor([-21.1678], device='cuda:0'))])
epoch£º1187	 i:0 	 global-step:23740	 l-p:0.17839482426643372
epoch£º1187	 i:1 	 global-step:23741	 l-p:0.14642071723937988
epoch£º1187	 i:2 	 global-step:23742	 l-p:0.11524047702550888
epoch£º1187	 i:3 	 global-step:23743	 l-p:0.15206439793109894
epoch£º1187	 i:4 	 global-step:23744	 l-p:0.12159083783626556
epoch£º1187	 i:5 	 global-step:23745	 l-p:0.14404554665088654
epoch£º1187	 i:6 	 global-step:23746	 l-p:0.09524907916784286
epoch£º1187	 i:7 	 global-step:23747	 l-p:0.15498895943164825
epoch£º1187	 i:8 	 global-step:23748	 l-p:0.0689244419336319
epoch£º1187	 i:9 	 global-step:23749	 l-p:0.15607787668704987
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1188
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7200e-02, 4.4691e-02,
         1.0000e+00, 2.0548e-02, 1.0000e+00, 4.5979e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0993e-04, 5.2659e-06,
         1.0000e+00, 2.5226e-07, 1.0000e+00, 4.7904e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.7314e-01, 9.6434e-01,
         1.0000e+00, 9.5563e-01, 1.0000e+00, 9.9096e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1582e-02, 2.4319e-02,
         1.0000e+00, 9.6035e-03, 1.0000e+00, 3.9490e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1883, 5.0948, 5.1513],
        [5.1883, 5.1883, 5.1883],
        [5.1883, 5.6093, 5.5329],
        [5.1883, 5.1426, 5.1781]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1188, step:0 
model_pd.l_p.mean(): 0.12469292432069778 
model_pd.l_d.mean(): -20.2728328704834 
model_pd.lagr.mean(): -20.14813995361328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3990], device='cuda:0')), ('power', tensor([-21.0652], device='cuda:0'))])
epoch£º1188	 i:0 	 global-step:23760	 l-p:0.12469292432069778
epoch£º1188	 i:1 	 global-step:23761	 l-p:0.16258984804153442
epoch£º1188	 i:2 	 global-step:23762	 l-p:0.11208314448595047
epoch£º1188	 i:3 	 global-step:23763	 l-p:0.12238030880689621
epoch£º1188	 i:4 	 global-step:23764	 l-p:0.10994972288608551
epoch£º1188	 i:5 	 global-step:23765	 l-p:0.18761466443538666
epoch£º1188	 i:6 	 global-step:23766	 l-p:0.0629916787147522
epoch£º1188	 i:7 	 global-step:23767	 l-p:0.18179185688495636
epoch£º1188	 i:8 	 global-step:23768	 l-p:0.13587266206741333
epoch£º1188	 i:9 	 global-step:23769	 l-p:0.12034532427787781
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1189
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3134e-01, 6.6760e-02,
         1.0000e+00, 3.3935e-02, 1.0000e+00, 5.0831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1928, 5.0514, 5.1119],
        [5.1928, 4.9693, 4.9866],
        [5.1928, 5.1928, 5.1928],
        [5.1928, 5.1909, 5.1928]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1189, step:0 
model_pd.l_p.mean(): 0.11238986998796463 
model_pd.l_d.mean(): -20.83181381225586 
model_pd.lagr.mean(): -20.719423294067383 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3624], device='cuda:0')), ('power', tensor([-21.5967], device='cuda:0'))])
epoch£º1189	 i:0 	 global-step:23780	 l-p:0.11238986998796463
epoch£º1189	 i:1 	 global-step:23781	 l-p:0.19769375026226044
epoch£º1189	 i:2 	 global-step:23782	 l-p:0.1684061735868454
epoch£º1189	 i:3 	 global-step:23783	 l-p:0.05439595505595207
epoch£º1189	 i:4 	 global-step:23784	 l-p:0.1392669528722763
epoch£º1189	 i:5 	 global-step:23785	 l-p:0.12753866612911224
epoch£º1189	 i:6 	 global-step:23786	 l-p:0.19203561544418335
epoch£º1189	 i:7 	 global-step:23787	 l-p:0.09162961691617966
epoch£º1189	 i:8 	 global-step:23788	 l-p:0.12137938290834427
epoch£º1189	 i:9 	 global-step:23789	 l-p:0.10121400654315948
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1190
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7346e-02, 1.2483e-02,
         1.0000e+00, 4.1725e-03, 1.0000e+00, 3.3426e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1931, 5.1931, 5.1931],
        [5.1931, 5.1739, 5.1908],
        [5.1931, 5.1931, 5.1931],
        [5.1931, 5.1768, 5.1913]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1190, step:0 
model_pd.l_p.mean(): 0.08583018183708191 
model_pd.l_d.mean(): -19.53955078125 
model_pd.lagr.mean(): -19.453720092773438 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4920], device='cuda:0')), ('power', tensor([-20.4145], device='cuda:0'))])
epoch£º1190	 i:0 	 global-step:23800	 l-p:0.08583018183708191
epoch£º1190	 i:1 	 global-step:23801	 l-p:0.16502048075199127
epoch£º1190	 i:2 	 global-step:23802	 l-p:0.07199188321828842
epoch£º1190	 i:3 	 global-step:23803	 l-p:0.1615421622991562
epoch£º1190	 i:4 	 global-step:23804	 l-p:0.1572062224149704
epoch£º1190	 i:5 	 global-step:23805	 l-p:0.19295106828212738
epoch£º1190	 i:6 	 global-step:23806	 l-p:0.13348077237606049
epoch£º1190	 i:7 	 global-step:23807	 l-p:0.128886416554451
epoch£º1190	 i:8 	 global-step:23808	 l-p:0.08875741064548492
epoch£º1190	 i:9 	 global-step:23809	 l-p:0.11971582472324371
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1191
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8652e-03, 2.2959e-04,
         1.0000e+00, 2.8261e-05, 1.0000e+00, 1.2309e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9951e-01, 1.1658e-01,
         1.0000e+00, 6.8120e-02, 1.0000e+00, 5.8433e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1952, 5.5577, 5.4424],
        [5.1952, 5.1952, 5.1953],
        [5.1952, 5.1514, 5.1858],
        [5.1952, 4.9702, 4.9857]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1191, step:0 
model_pd.l_p.mean(): 0.0732707753777504 
model_pd.l_d.mean(): -19.113271713256836 
model_pd.lagr.mean(): -19.040000915527344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5696], device='cuda:0')), ('power', tensor([-20.0606], device='cuda:0'))])
epoch£º1191	 i:0 	 global-step:23820	 l-p:0.0732707753777504
epoch£º1191	 i:1 	 global-step:23821	 l-p:0.12347368150949478
epoch£º1191	 i:2 	 global-step:23822	 l-p:0.10612792521715164
epoch£º1191	 i:3 	 global-step:23823	 l-p:0.15018031001091003
epoch£º1191	 i:4 	 global-step:23824	 l-p:0.08931837230920792
epoch£º1191	 i:5 	 global-step:23825	 l-p:0.155242919921875
epoch£º1191	 i:6 	 global-step:23826	 l-p:0.15091365575790405
epoch£º1191	 i:7 	 global-step:23827	 l-p:0.1267891228199005
epoch£º1191	 i:8 	 global-step:23828	 l-p:0.14081458747386932
epoch£º1191	 i:9 	 global-step:23829	 l-p:0.19029328227043152
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1192
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6834e-02, 3.8452e-02,
         1.0000e+00, 1.7027e-02, 1.0000e+00, 4.4282e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6179e-02, 4.4066e-02,
         1.0000e+00, 2.0190e-02, 1.0000e+00, 4.5817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8257e-02, 4.8072e-03,
         1.0000e+00, 1.2658e-03, 1.0000e+00, 2.6331e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1928, 5.1139, 5.1657],
        [5.1928, 5.1008, 5.1569],
        [5.1928, 5.0306, 5.0869],
        [5.1928, 5.1877, 5.1926]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1192, step:0 
model_pd.l_p.mean(): 0.1421242505311966 
model_pd.l_d.mean(): -19.602256774902344 
model_pd.lagr.mean(): -19.460132598876953 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5162], device='cuda:0')), ('power', tensor([-20.5035], device='cuda:0'))])
epoch£º1192	 i:0 	 global-step:23840	 l-p:0.1421242505311966
epoch£º1192	 i:1 	 global-step:23841	 l-p:0.18544234335422516
epoch£º1192	 i:2 	 global-step:23842	 l-p:0.13482443988323212
epoch£º1192	 i:3 	 global-step:23843	 l-p:0.08626512438058853
epoch£º1192	 i:4 	 global-step:23844	 l-p:0.1329963207244873
epoch£º1192	 i:5 	 global-step:23845	 l-p:0.1539689004421234
epoch£º1192	 i:6 	 global-step:23846	 l-p:0.13252809643745422
epoch£º1192	 i:7 	 global-step:23847	 l-p:0.10803522914648056
epoch£º1192	 i:8 	 global-step:23848	 l-p:0.14604060351848602
epoch£º1192	 i:9 	 global-step:23849	 l-p:0.09317892044782639
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1193
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1887, 4.8968, 4.6490],
        [5.1887, 5.1325, 5.1740],
        [5.1887, 5.1839, 5.1885],
        [5.1887, 5.1659, 5.1856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1193, step:0 
model_pd.l_p.mean(): 0.1407320201396942 
model_pd.l_d.mean(): -20.00648307800293 
model_pd.lagr.mean(): -19.865751266479492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5149], device='cuda:0')), ('power', tensor([-20.9138], device='cuda:0'))])
epoch£º1193	 i:0 	 global-step:23860	 l-p:0.1407320201396942
epoch£º1193	 i:1 	 global-step:23861	 l-p:0.0996827483177185
epoch£º1193	 i:2 	 global-step:23862	 l-p:0.10861755162477493
epoch£º1193	 i:3 	 global-step:23863	 l-p:0.14561688899993896
epoch£º1193	 i:4 	 global-step:23864	 l-p:0.14201320707798004
epoch£º1193	 i:5 	 global-step:23865	 l-p:0.14822445809841156
epoch£º1193	 i:6 	 global-step:23866	 l-p:0.13174748420715332
epoch£º1193	 i:7 	 global-step:23867	 l-p:0.15215975046157837
epoch£º1193	 i:8 	 global-step:23868	 l-p:0.12712129950523376
epoch£º1193	 i:9 	 global-step:23869	 l-p:0.13234743475914001
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1194
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7145e-01, 3.6693e-01,
         1.0000e+00, 2.8558e-01, 1.0000e+00, 7.7830e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2355e-03, 1.6631e-03,
         1.0000e+00, 3.3585e-04, 1.0000e+00, 2.0194e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8226e-01, 4.8620e-01,
         1.0000e+00, 4.0600e-01, 1.0000e+00, 8.3503e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1849, 4.9317, 4.6182],
        [5.1849, 5.1838, 5.1849],
        [5.1849, 5.0341, 4.6876],
        [5.1849, 4.8862, 4.6695]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1194, step:0 
model_pd.l_p.mean(): 0.1197030320763588 
model_pd.l_d.mean(): -20.337779998779297 
model_pd.lagr.mean(): -20.218076705932617 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4468], device='cuda:0')), ('power', tensor([-21.1809], device='cuda:0'))])
epoch£º1194	 i:0 	 global-step:23880	 l-p:0.1197030320763588
epoch£º1194	 i:1 	 global-step:23881	 l-p:0.13610027730464935
epoch£º1194	 i:2 	 global-step:23882	 l-p:0.12980569899082184
epoch£º1194	 i:3 	 global-step:23883	 l-p:0.1600906103849411
epoch£º1194	 i:4 	 global-step:23884	 l-p:0.14111293852329254
epoch£º1194	 i:5 	 global-step:23885	 l-p:0.16181865334510803
epoch£º1194	 i:6 	 global-step:23886	 l-p:0.10934318602085114
epoch£º1194	 i:7 	 global-step:23887	 l-p:0.12311166524887085
epoch£º1194	 i:8 	 global-step:23888	 l-p:0.14958275854587555
epoch£º1194	 i:9 	 global-step:23889	 l-p:0.17016083002090454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1195
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9919e-03, 8.5314e-04,
         1.0000e+00, 1.4581e-04, 1.0000e+00, 1.7091e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2697e-01, 6.3817e-02,
         1.0000e+00, 3.2075e-02, 1.0000e+00, 5.0261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5838e-01, 1.6457e-01,
         1.0000e+00, 1.0482e-01, 1.0000e+00, 6.3692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1640, 5.1588, 5.1637],
        [5.1640, 5.1636, 5.1640],
        [5.1640, 5.0277, 5.0892],
        [5.1640, 4.8878, 4.8269]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1195, step:0 
model_pd.l_p.mean(): 0.14116021990776062 
model_pd.l_d.mean(): -20.346586227416992 
model_pd.lagr.mean(): -20.205425262451172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4521], device='cuda:0')), ('power', tensor([-21.1953], device='cuda:0'))])
epoch£º1195	 i:0 	 global-step:23900	 l-p:0.14116021990776062
epoch£º1195	 i:1 	 global-step:23901	 l-p:0.11842203885316849
epoch£º1195	 i:2 	 global-step:23902	 l-p:0.17049632966518402
epoch£º1195	 i:3 	 global-step:23903	 l-p:0.16813020408153534
epoch£º1195	 i:4 	 global-step:23904	 l-p:0.12416274845600128
epoch£º1195	 i:5 	 global-step:23905	 l-p:0.08771488070487976
epoch£º1195	 i:6 	 global-step:23906	 l-p:0.1348625272512436
epoch£º1195	 i:7 	 global-step:23907	 l-p:0.13921509683132172
epoch£º1195	 i:8 	 global-step:23908	 l-p:0.15603378415107727
epoch£º1195	 i:9 	 global-step:23909	 l-p:0.17873737215995789
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1196
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9514e-02, 4.0042e-02,
         1.0000e+00, 1.7912e-02, 1.0000e+00, 4.4733e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9392e-02, 1.8122e-02,
         1.0000e+00, 6.6490e-03, 1.0000e+00, 3.6690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5479e-01, 6.8723e-01,
         1.0000e+00, 6.2572e-01, 1.0000e+00, 9.1049e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1668, 5.0837, 5.1373],
        [5.1668, 5.1352, 5.1615],
        [5.1668, 5.3822, 5.1772],
        [5.1668, 5.2382, 4.9575]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1196, step:0 
model_pd.l_p.mean(): 0.13475148379802704 
model_pd.l_d.mean(): -20.138742446899414 
model_pd.lagr.mean(): -20.003990173339844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4851], device='cuda:0')), ('power', tensor([-21.0178], device='cuda:0'))])
epoch£º1196	 i:0 	 global-step:23920	 l-p:0.13475148379802704
epoch£º1196	 i:1 	 global-step:23921	 l-p:0.14655855298042297
epoch£º1196	 i:2 	 global-step:23922	 l-p:0.18755464255809784
epoch£º1196	 i:3 	 global-step:23923	 l-p:0.14959155023097992
epoch£º1196	 i:4 	 global-step:23924	 l-p:0.101372629404068
epoch£º1196	 i:5 	 global-step:23925	 l-p:0.14412522315979004
epoch£º1196	 i:6 	 global-step:23926	 l-p:0.13990764319896698
epoch£º1196	 i:7 	 global-step:23927	 l-p:0.12287404388189316
epoch£º1196	 i:8 	 global-step:23928	 l-p:0.13502845168113708
epoch£º1196	 i:9 	 global-step:23929	 l-p:0.13678626716136932
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1197
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7425e-01, 1.7818e-01,
         1.0000e+00, 1.1577e-01, 1.0000e+00, 6.4970e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1374e-01, 8.8667e-01,
         1.0000e+00, 8.6041e-01, 1.0000e+00, 9.7038e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1691, 4.8843, 4.8000],
        [5.1691, 5.0845, 5.1385],
        [5.1691, 5.4864, 5.3428],
        [5.1691, 4.9563, 4.6170]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1197, step:0 
model_pd.l_p.mean(): 0.1477377712726593 
model_pd.l_d.mean(): -19.990581512451172 
model_pd.lagr.mean(): -19.842844009399414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5084], device='cuda:0')), ('power', tensor([-20.8909], device='cuda:0'))])
epoch£º1197	 i:0 	 global-step:23940	 l-p:0.1477377712726593
epoch£º1197	 i:1 	 global-step:23941	 l-p:0.13997387886047363
epoch£º1197	 i:2 	 global-step:23942	 l-p:0.11617307364940643
epoch£º1197	 i:3 	 global-step:23943	 l-p:0.1434914618730545
epoch£º1197	 i:4 	 global-step:23944	 l-p:0.1496635526418686
epoch£º1197	 i:5 	 global-step:23945	 l-p:0.20794616639614105
epoch£º1197	 i:6 	 global-step:23946	 l-p:0.1384633630514145
epoch£º1197	 i:7 	 global-step:23947	 l-p:0.16247084736824036
epoch£º1197	 i:8 	 global-step:23948	 l-p:0.12811779975891113
epoch£º1197	 i:9 	 global-step:23949	 l-p:0.07315723598003387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1198
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1607e-07, 8.8969e-09,
         1.0000e+00, 8.6406e-11, 1.0000e+00, 9.7120e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1702, 5.1427, 4.8209],
        [5.1702, 5.1702, 5.1702],
        [5.1702, 5.1658, 5.1700],
        [5.1702, 5.1532, 5.1684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1198, step:0 
model_pd.l_p.mean(): 0.1062566414475441 
model_pd.l_d.mean(): -19.88983726501465 
model_pd.lagr.mean(): -19.783580780029297 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4960], device='cuda:0')), ('power', tensor([-20.7755], device='cuda:0'))])
epoch£º1198	 i:0 	 global-step:23960	 l-p:0.1062566414475441
epoch£º1198	 i:1 	 global-step:23961	 l-p:0.128524512052536
epoch£º1198	 i:2 	 global-step:23962	 l-p:0.09556123614311218
epoch£º1198	 i:3 	 global-step:23963	 l-p:0.16653800010681152
epoch£º1198	 i:4 	 global-step:23964	 l-p:0.14211127161979675
epoch£º1198	 i:5 	 global-step:23965	 l-p:0.12027625739574432
epoch£º1198	 i:6 	 global-step:23966	 l-p:0.13626939058303833
epoch£º1198	 i:7 	 global-step:23967	 l-p:0.14108708500862122
epoch£º1198	 i:8 	 global-step:23968	 l-p:0.21369177103042603
epoch£º1198	 i:9 	 global-step:23969	 l-p:0.14558091759681702
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1199
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0518e-03, 1.0696e-04,
         1.0000e+00, 1.0878e-05, 1.0000e+00, 1.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1715, 5.1700, 5.1714],
        [5.1715, 5.1715, 5.1715],
        [5.1715, 5.1706, 5.1715],
        [5.1715, 5.1715, 5.1715]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1199, step:0 
model_pd.l_p.mean(): 0.10633155703544617 
model_pd.l_d.mean(): -19.119861602783203 
model_pd.lagr.mean(): -19.013530731201172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5527], device='cuda:0')), ('power', tensor([-20.0498], device='cuda:0'))])
epoch£º1199	 i:0 	 global-step:23980	 l-p:0.10633155703544617
epoch£º1199	 i:1 	 global-step:23981	 l-p:0.1880435347557068
epoch£º1199	 i:2 	 global-step:23982	 l-p:0.09286165237426758
epoch£º1199	 i:3 	 global-step:23983	 l-p:0.14291079342365265
epoch£º1199	 i:4 	 global-step:23984	 l-p:0.13159656524658203
epoch£º1199	 i:5 	 global-step:23985	 l-p:0.15019628405570984
epoch£º1199	 i:6 	 global-step:23986	 l-p:0.09243328124284744
epoch£º1199	 i:7 	 global-step:23987	 l-p:0.18610888719558716
epoch£º1199	 i:8 	 global-step:23988	 l-p:0.13378404080867767
epoch£º1199	 i:9 	 global-step:23989	 l-p:0.1621025800704956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1200
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0523e-01, 1.2105e-01,
         1.0000e+00, 7.1404e-02, 1.0000e+00, 5.8985e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1057e-01, 1.2527e-01,
         1.0000e+00, 7.4530e-02, 1.0000e+00, 5.9493e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1712, 5.0904, 5.1431],
        [5.1712, 5.1711, 5.1712],
        [5.1712, 4.9385, 4.9483],
        [5.1712, 4.9333, 4.9367]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1200, step:0 
model_pd.l_p.mean(): 0.0497305653989315 
model_pd.l_d.mean(): -20.134593963623047 
model_pd.lagr.mean(): -20.084863662719727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4662], device='cuda:0')), ('power', tensor([-20.9940], device='cuda:0'))])
epoch£º1200	 i:0 	 global-step:24000	 l-p:0.0497305653989315
epoch£º1200	 i:1 	 global-step:24001	 l-p:0.11998103559017181
epoch£º1200	 i:2 	 global-step:24002	 l-p:0.11203525960445404
epoch£º1200	 i:3 	 global-step:24003	 l-p:0.13527339696884155
epoch£º1200	 i:4 	 global-step:24004	 l-p:0.1681230515241623
epoch£º1200	 i:5 	 global-step:24005	 l-p:0.1299060434103012
epoch£º1200	 i:6 	 global-step:24006	 l-p:0.16654902696609497
epoch£º1200	 i:7 	 global-step:24007	 l-p:0.21795737743377686
epoch£º1200	 i:8 	 global-step:24008	 l-p:0.11312492936849594
epoch£º1200	 i:9 	 global-step:24009	 l-p:0.18228763341903687
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1201
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1702, 5.1702, 5.1702],
        [5.1702, 5.1455, 5.1667],
        [5.1702, 4.8751, 4.6268],
        [5.1702, 4.8699, 4.6468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1201, step:0 
model_pd.l_p.mean(): 0.13722656667232513 
model_pd.l_d.mean(): -18.874618530273438 
model_pd.lagr.mean(): -18.73739242553711 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5383], device='cuda:0')), ('power', tensor([-19.7851], device='cuda:0'))])
epoch£º1201	 i:0 	 global-step:24020	 l-p:0.13722656667232513
epoch£º1201	 i:1 	 global-step:24021	 l-p:0.13933926820755005
epoch£º1201	 i:2 	 global-step:24022	 l-p:0.1863018423318863
epoch£º1201	 i:3 	 global-step:24023	 l-p:0.10692528635263443
epoch£º1201	 i:4 	 global-step:24024	 l-p:0.13226142525672913
epoch£º1201	 i:5 	 global-step:24025	 l-p:0.12210129201412201
epoch£º1201	 i:6 	 global-step:24026	 l-p:0.13399755954742432
epoch£º1201	 i:7 	 global-step:24027	 l-p:0.1442759931087494
epoch£º1201	 i:8 	 global-step:24028	 l-p:0.1864377111196518
epoch£º1201	 i:9 	 global-step:24029	 l-p:0.11226443201303482
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1202
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1810e-04, 5.2651e-05,
         1.0000e+00, 4.4850e-06, 1.0000e+00, 8.5183e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8889e-01, 8.5467e-01,
         1.0000e+00, 8.2177e-01, 1.0000e+00, 9.6150e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1680, 5.0833, 5.1374],
        [5.1680, 4.8791, 4.7830],
        [5.1680, 5.1680, 5.1680],
        [5.1680, 5.4450, 5.2764]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1202, step:0 
model_pd.l_p.mean(): 0.08735708147287369 
model_pd.l_d.mean(): -19.666536331176758 
model_pd.lagr.mean(): -19.579179763793945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5121], device='cuda:0')), ('power', tensor([-20.5647], device='cuda:0'))])
epoch£º1202	 i:0 	 global-step:24040	 l-p:0.08735708147287369
epoch£º1202	 i:1 	 global-step:24041	 l-p:0.14839954674243927
epoch£º1202	 i:2 	 global-step:24042	 l-p:0.14224399626255035
epoch£º1202	 i:3 	 global-step:24043	 l-p:0.14296354353427887
epoch£º1202	 i:4 	 global-step:24044	 l-p:0.09372163563966751
epoch£º1202	 i:5 	 global-step:24045	 l-p:0.10213397443294525
epoch£º1202	 i:6 	 global-step:24046	 l-p:0.1499830037355423
epoch£º1202	 i:7 	 global-step:24047	 l-p:0.27044185996055603
epoch£º1202	 i:8 	 global-step:24048	 l-p:0.15153276920318604
epoch£º1202	 i:9 	 global-step:24049	 l-p:0.13953246176242828
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1203
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2209e-02, 1.4696e-02,
         1.0000e+00, 5.1170e-03, 1.0000e+00, 3.4818e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7124e-01, 3.6671e-01,
         1.0000e+00, 2.8537e-01, 1.0000e+00, 7.7818e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1610, 5.1370, 5.1577],
        [5.1610, 5.1607, 5.1610],
        [5.1610, 4.9789, 4.6316],
        [5.1610, 4.9030, 4.5883]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1203, step:0 
model_pd.l_p.mean(): 0.22556763887405396 
model_pd.l_d.mean(): -20.27410125732422 
model_pd.lagr.mean(): -20.048534393310547 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4932], device='cuda:0')), ('power', tensor([-21.1640], device='cuda:0'))])
epoch£º1203	 i:0 	 global-step:24060	 l-p:0.22556763887405396
epoch£º1203	 i:1 	 global-step:24061	 l-p:0.15656079351902008
epoch£º1203	 i:2 	 global-step:24062	 l-p:0.11404614895582199
epoch£º1203	 i:3 	 global-step:24063	 l-p:0.17094475030899048
epoch£º1203	 i:4 	 global-step:24064	 l-p:0.12191116809844971
epoch£º1203	 i:5 	 global-step:24065	 l-p:0.15347881615161896
epoch£º1203	 i:6 	 global-step:24066	 l-p:0.10588817298412323
epoch£º1203	 i:7 	 global-step:24067	 l-p:0.13560418784618378
epoch£º1203	 i:8 	 global-step:24068	 l-p:0.13124942779541016
epoch£º1203	 i:9 	 global-step:24069	 l-p:0.12661173939704895
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1204
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4795e-02, 7.2304e-03,
         1.0000e+00, 2.1084e-03, 1.0000e+00, 2.9160e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7554e-02, 1.7229e-02,
         1.0000e+00, 6.2419e-03, 1.0000e+00, 3.6230e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8938e-01, 1.9141e-01,
         1.0000e+00, 1.2661e-01, 1.0000e+00, 6.6144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4560e-01, 7.6598e-02,
         1.0000e+00, 4.0297e-02, 1.0000e+00, 5.2608e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1595, 5.1503, 5.1588],
        [5.1595, 5.1298, 5.1547],
        [5.1595, 4.8667, 4.7599],
        [5.1595, 4.9972, 5.0546]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1204, step:0 
model_pd.l_p.mean(): 0.16590389609336853 
model_pd.l_d.mean(): -20.564922332763672 
model_pd.lagr.mean(): -20.399019241333008 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4280], device='cuda:0')), ('power', tensor([-21.3927], device='cuda:0'))])
epoch£º1204	 i:0 	 global-step:24080	 l-p:0.16590389609336853
epoch£º1204	 i:1 	 global-step:24081	 l-p:0.1413232982158661
epoch£º1204	 i:2 	 global-step:24082	 l-p:0.12853042781352997
epoch£º1204	 i:3 	 global-step:24083	 l-p:0.12368077039718628
epoch£º1204	 i:4 	 global-step:24084	 l-p:0.12949490547180176
epoch£º1204	 i:5 	 global-step:24085	 l-p:0.2632206976413727
epoch£º1204	 i:6 	 global-step:24086	 l-p:0.08156739175319672
epoch£º1204	 i:7 	 global-step:24087	 l-p:0.14373347163200378
epoch£º1204	 i:8 	 global-step:24088	 l-p:0.11266059428453445
epoch£º1204	 i:9 	 global-step:24089	 l-p:0.1602211743593216
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1205
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6869e-01, 3.6407e-01,
         1.0000e+00, 2.8280e-01, 1.0000e+00, 7.7678e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7761e-02, 2.7625e-02,
         1.0000e+00, 1.1262e-02, 1.0000e+00, 4.0769e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8573e-01, 7.2504e-01,
         1.0000e+00, 6.6904e-01, 1.0000e+00, 9.2276e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0176e-01, 3.9872e-01,
         1.0000e+00, 3.1683e-01, 1.0000e+00, 7.9463e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1593, 4.8991, 4.5859],
        [5.1593, 5.1055, 5.1458],
        [5.1593, 5.2737, 5.0139],
        [5.1593, 4.9240, 4.5927]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1205, step:0 
model_pd.l_p.mean(): 0.11621449887752533 
model_pd.l_d.mean(): -20.381629943847656 
model_pd.lagr.mean(): -20.26541519165039 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4671], device='cuda:0')), ('power', tensor([-21.2465], device='cuda:0'))])
epoch£º1205	 i:0 	 global-step:24100	 l-p:0.11621449887752533
epoch£º1205	 i:1 	 global-step:24101	 l-p:0.14988870918750763
epoch£º1205	 i:2 	 global-step:24102	 l-p:0.1270245909690857
epoch£º1205	 i:3 	 global-step:24103	 l-p:0.21948300302028656
epoch£º1205	 i:4 	 global-step:24104	 l-p:0.10300612449645996
epoch£º1205	 i:5 	 global-step:24105	 l-p:0.14587906002998352
epoch£º1205	 i:6 	 global-step:24106	 l-p:0.16427665948867798
epoch£º1205	 i:7 	 global-step:24107	 l-p:0.12963491678237915
epoch£º1205	 i:8 	 global-step:24108	 l-p:0.19295017421245575
epoch£º1205	 i:9 	 global-step:24109	 l-p:0.11586223542690277
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1206
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1467e-04, 4.1245e-05,
         1.0000e+00, 3.3053e-06, 1.0000e+00, 8.0139e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5394e-01, 2.5037e-01,
         1.0000e+00, 1.7710e-01, 1.0000e+00, 7.0736e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6004e-02, 2.6675e-02,
         1.0000e+00, 1.0780e-02, 1.0000e+00, 4.0413e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1556, 5.1556, 5.1556],
        [5.1556, 4.8513, 4.6525],
        [5.1556, 5.1040, 5.1431],
        [5.1556, 4.8765, 4.8114]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1206, step:0 
model_pd.l_p.mean(): 0.16886785626411438 
model_pd.l_d.mean(): -20.599260330200195 
model_pd.lagr.mean(): -20.43039321899414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4178], device='cuda:0')), ('power', tensor([-21.4171], device='cuda:0'))])
epoch£º1206	 i:0 	 global-step:24120	 l-p:0.16886785626411438
epoch£º1206	 i:1 	 global-step:24121	 l-p:0.1267893761396408
epoch£º1206	 i:2 	 global-step:24122	 l-p:0.07327135652303696
epoch£º1206	 i:3 	 global-step:24123	 l-p:0.14135326445102692
epoch£º1206	 i:4 	 global-step:24124	 l-p:0.16221080720424652
epoch£º1206	 i:5 	 global-step:24125	 l-p:0.1833462119102478
epoch£º1206	 i:6 	 global-step:24126	 l-p:0.17441833019256592
epoch£º1206	 i:7 	 global-step:24127	 l-p:0.09240001440048218
epoch£º1206	 i:8 	 global-step:24128	 l-p:0.1268090456724167
epoch£º1206	 i:9 	 global-step:24129	 l-p:0.21174484491348267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1207
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8120e-03, 1.8201e-03,
         1.0000e+00, 3.7594e-04, 1.0000e+00, 2.0655e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1598, 4.9775, 5.0274],
        [5.1598, 4.8785, 4.5923],
        [5.1598, 5.1585, 5.1597],
        [5.1598, 4.8567, 4.6917]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1207, step:0 
model_pd.l_p.mean(): 0.17102991044521332 
model_pd.l_d.mean(): -20.043954849243164 
model_pd.lagr.mean(): -19.8729248046875 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5069], device='cuda:0')), ('power', tensor([-20.9438], device='cuda:0'))])
epoch£º1207	 i:0 	 global-step:24140	 l-p:0.17102991044521332
epoch£º1207	 i:1 	 global-step:24141	 l-p:0.13282924890518188
epoch£º1207	 i:2 	 global-step:24142	 l-p:0.11822415888309479
epoch£º1207	 i:3 	 global-step:24143	 l-p:0.16937413811683655
epoch£º1207	 i:4 	 global-step:24144	 l-p:0.17251481115818024
epoch£º1207	 i:5 	 global-step:24145	 l-p:0.12843060493469238
epoch£º1207	 i:6 	 global-step:24146	 l-p:0.14632205665111542
epoch£º1207	 i:7 	 global-step:24147	 l-p:0.15058259665966034
epoch£º1207	 i:8 	 global-step:24148	 l-p:0.12428604811429977
epoch£º1207	 i:9 	 global-step:24149	 l-p:0.11863763630390167
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1208
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1886e-04, 2.1784e-05,
         1.0000e+00, 1.4882e-06, 1.0000e+00, 6.8318e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1658, 4.9851, 4.6379],
        [5.1658, 5.0325, 5.0942],
        [5.1658, 5.1658, 5.1658],
        [5.1658, 5.0139, 5.0736]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1208, step:0 
model_pd.l_p.mean(): 0.12918733060359955 
model_pd.l_d.mean(): -19.92926597595215 
model_pd.lagr.mean(): -19.800079345703125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5122], device='cuda:0')), ('power', tensor([-20.8324], device='cuda:0'))])
epoch£º1208	 i:0 	 global-step:24160	 l-p:0.12918733060359955
epoch£º1208	 i:1 	 global-step:24161	 l-p:0.2046712189912796
epoch£º1208	 i:2 	 global-step:24162	 l-p:0.17577432096004486
epoch£º1208	 i:3 	 global-step:24163	 l-p:0.12374701350927353
epoch£º1208	 i:4 	 global-step:24164	 l-p:0.13584592938423157
epoch£º1208	 i:5 	 global-step:24165	 l-p:0.1843957155942917
epoch£º1208	 i:6 	 global-step:24166	 l-p:0.13808928430080414
epoch£º1208	 i:7 	 global-step:24167	 l-p:0.08461323380470276
epoch£º1208	 i:8 	 global-step:24168	 l-p:0.13191787898540497
epoch£º1208	 i:9 	 global-step:24169	 l-p:0.10872180759906769
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1209
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9454e-02, 9.0960e-03,
         1.0000e+00, 2.8091e-03, 1.0000e+00, 3.0882e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3764e-08, 6.8321e-11,
         1.0000e+00, 1.9642e-13, 1.0000e+00, 2.8750e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1650, 5.0696, 5.1269],
        [5.1650, 5.1524, 5.1639],
        [5.1650, 5.1650, 5.1650],
        [5.1650, 5.4399, 5.2699]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1209, step:0 
model_pd.l_p.mean(): 0.1650875061750412 
model_pd.l_d.mean(): -19.504358291625977 
model_pd.lagr.mean(): -19.339271545410156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4734], device='cuda:0')), ('power', tensor([-20.3593], device='cuda:0'))])
epoch£º1209	 i:0 	 global-step:24180	 l-p:0.1650875061750412
epoch£º1209	 i:1 	 global-step:24181	 l-p:0.17118461430072784
epoch£º1209	 i:2 	 global-step:24182	 l-p:0.16100715100765228
epoch£º1209	 i:3 	 global-step:24183	 l-p:0.1043497622013092
epoch£º1209	 i:4 	 global-step:24184	 l-p:0.1398790031671524
epoch£º1209	 i:5 	 global-step:24185	 l-p:0.11465619504451752
epoch£º1209	 i:6 	 global-step:24186	 l-p:0.1164352297782898
epoch£º1209	 i:7 	 global-step:24187	 l-p:0.21130353212356567
epoch£º1209	 i:8 	 global-step:24188	 l-p:0.12119408696889877
epoch£º1209	 i:9 	 global-step:24189	 l-p:0.12373944371938705
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1210
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0331e-02, 2.2500e-03,
         1.0000e+00, 4.9005e-04, 1.0000e+00, 2.1780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8723e-02, 4.9717e-03,
         1.0000e+00, 1.3202e-03, 1.0000e+00, 2.6554e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1611, 5.1594, 5.1611],
        [5.1611, 5.3738, 5.1672],
        [5.1611, 5.1557, 5.1608],
        [5.1611, 5.1086, 5.1482]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1210, step:0 
model_pd.l_p.mean(): 0.15271447598934174 
model_pd.l_d.mean(): -19.900068283081055 
model_pd.lagr.mean(): -19.74735450744629 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5276], device='cuda:0')), ('power', tensor([-20.8186], device='cuda:0'))])
epoch£º1210	 i:0 	 global-step:24200	 l-p:0.15271447598934174
epoch£º1210	 i:1 	 global-step:24201	 l-p:0.126161590218544
epoch£º1210	 i:2 	 global-step:24202	 l-p:0.12281915545463562
epoch£º1210	 i:3 	 global-step:24203	 l-p:0.17091402411460876
epoch£º1210	 i:4 	 global-step:24204	 l-p:0.10235095769166946
epoch£º1210	 i:5 	 global-step:24205	 l-p:0.10226310789585114
epoch£º1210	 i:6 	 global-step:24206	 l-p:0.14083801209926605
epoch£º1210	 i:7 	 global-step:24207	 l-p:0.16177710890769958
epoch£º1210	 i:8 	 global-step:24208	 l-p:0.25230640172958374
epoch£º1210	 i:9 	 global-step:24209	 l-p:0.1208258867263794
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1211
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0959e-06, 4.5121e-08,
         1.0000e+00, 6.5762e-10, 1.0000e+00, 1.4575e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1778e-02, 1.0066e-02,
         1.0000e+00, 3.1883e-03, 1.0000e+00, 3.1675e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1580, 5.1580, 5.1580],
        [5.1580, 5.1435, 5.1566],
        [5.1580, 5.1580, 5.1580],
        [5.1580, 5.1580, 5.1580]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1211, step:0 
model_pd.l_p.mean(): 0.1930719017982483 
model_pd.l_d.mean(): -20.42856216430664 
model_pd.lagr.mean(): -20.235490798950195 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4593], device='cuda:0')), ('power', tensor([-21.2863], device='cuda:0'))])
epoch£º1211	 i:0 	 global-step:24220	 l-p:0.1930719017982483
epoch£º1211	 i:1 	 global-step:24221	 l-p:0.11829517036676407
epoch£º1211	 i:2 	 global-step:24222	 l-p:0.15123829245567322
epoch£º1211	 i:3 	 global-step:24223	 l-p:0.11880873143672943
epoch£º1211	 i:4 	 global-step:24224	 l-p:0.23853611946105957
epoch£º1211	 i:5 	 global-step:24225	 l-p:0.13691987097263336
epoch£º1211	 i:6 	 global-step:24226	 l-p:0.1276148408651352
epoch£º1211	 i:7 	 global-step:24227	 l-p:0.07586710900068283
epoch£º1211	 i:8 	 global-step:24228	 l-p:0.15697714686393738
epoch£º1211	 i:9 	 global-step:24229	 l-p:0.13978688418865204
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1212
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0259e-02, 5.5229e-03,
         1.0000e+00, 1.5056e-03, 1.0000e+00, 2.7261e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0856e-02, 2.4039e-03,
         1.0000e+00, 5.3229e-04, 1.0000e+00, 2.2143e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5322e-01, 8.1989e-02,
         1.0000e+00, 4.3872e-02, 1.0000e+00, 5.3510e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1585, 5.1522, 5.1581],
        [5.1585, 5.1566, 5.1584],
        [5.1585, 4.9860, 5.0401],
        [5.1585, 5.1584, 5.1585]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1212, step:0 
model_pd.l_p.mean(): 0.18652305006980896 
model_pd.l_d.mean(): -19.14914321899414 
model_pd.lagr.mean(): -18.96261978149414 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5552], device='cuda:0')), ('power', tensor([-20.0823], device='cuda:0'))])
epoch£º1212	 i:0 	 global-step:24240	 l-p:0.18652305006980896
epoch£º1212	 i:1 	 global-step:24241	 l-p:0.0933418944478035
epoch£º1212	 i:2 	 global-step:24242	 l-p:0.13912537693977356
epoch£º1212	 i:3 	 global-step:24243	 l-p:0.083826944231987
epoch£º1212	 i:4 	 global-step:24244	 l-p:0.1599041223526001
epoch£º1212	 i:5 	 global-step:24245	 l-p:0.21445468068122864
epoch£º1212	 i:6 	 global-step:24246	 l-p:0.1955500990152359
epoch£º1212	 i:7 	 global-step:24247	 l-p:0.12376996129751205
epoch£º1212	 i:8 	 global-step:24248	 l-p:0.10966315120458603
epoch£º1212	 i:9 	 global-step:24249	 l-p:0.13924965262413025
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1213
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8086e-03, 3.9626e-04,
         1.0000e+00, 5.5908e-05, 1.0000e+00, 1.4109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3567e-03, 3.1361e-04,
         1.0000e+00, 4.1734e-05, 1.0000e+00, 1.3308e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1067e-02, 4.0972e-02,
         1.0000e+00, 1.8433e-02, 1.0000e+00, 4.4990e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2287e-01, 6.1086e-02,
         1.0000e+00, 3.0369e-02, 1.0000e+00, 4.9715e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1642, 5.1640, 5.1642],
        [5.1642, 5.1641, 5.1642],
        [5.1642, 5.0787, 5.1331],
        [5.1642, 5.0335, 5.0953]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1213, step:0 
model_pd.l_p.mean(): 0.1535353809595108 
model_pd.l_d.mean(): -20.23634910583496 
model_pd.lagr.mean(): -20.082813262939453 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4714], device='cuda:0')), ('power', tensor([-21.1029], device='cuda:0'))])
epoch£º1213	 i:0 	 global-step:24260	 l-p:0.1535353809595108
epoch£º1213	 i:1 	 global-step:24261	 l-p:0.13735315203666687
epoch£º1213	 i:2 	 global-step:24262	 l-p:0.1159605011343956
epoch£º1213	 i:3 	 global-step:24263	 l-p:0.1001572385430336
epoch£º1213	 i:4 	 global-step:24264	 l-p:0.1014292985200882
epoch£º1213	 i:5 	 global-step:24265	 l-p:0.13035105168819427
epoch£º1213	 i:6 	 global-step:24266	 l-p:0.15058085322380066
epoch£º1213	 i:7 	 global-step:24267	 l-p:0.13222025334835052
epoch£º1213	 i:8 	 global-step:24268	 l-p:0.19148263335227966
epoch£º1213	 i:9 	 global-step:24269	 l-p:0.2064013034105301
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1214
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3938e-01, 7.2267e-02,
         1.0000e+00, 3.7469e-02, 1.0000e+00, 5.1848e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2493e-01, 4.2345e-01,
         1.0000e+00, 3.4159e-01, 1.0000e+00, 8.0668e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6051e-02, 3.7990e-02,
         1.0000e+00, 1.6772e-02, 1.0000e+00, 4.4149e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1661, 5.0125, 5.0718],
        [5.1661, 4.9520, 4.6121],
        [5.1661, 5.1014, 5.1473],
        [5.1661, 5.0877, 5.1396]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1214, step:0 
model_pd.l_p.mean(): 0.13794635236263275 
model_pd.l_d.mean(): -20.310161590576172 
model_pd.lagr.mean(): -20.17221450805664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4565], device='cuda:0')), ('power', tensor([-21.1627], device='cuda:0'))])
epoch£º1214	 i:0 	 global-step:24280	 l-p:0.13794635236263275
epoch£º1214	 i:1 	 global-step:24281	 l-p:0.12032487243413925
epoch£º1214	 i:2 	 global-step:24282	 l-p:0.1782018393278122
epoch£º1214	 i:3 	 global-step:24283	 l-p:0.1422034353017807
epoch£º1214	 i:4 	 global-step:24284	 l-p:0.14933334290981293
epoch£º1214	 i:5 	 global-step:24285	 l-p:0.12888294458389282
epoch£º1214	 i:6 	 global-step:24286	 l-p:0.11403132975101471
epoch£º1214	 i:7 	 global-step:24287	 l-p:0.1608200967311859
epoch£º1214	 i:8 	 global-step:24288	 l-p:0.11561034619808197
epoch£º1214	 i:9 	 global-step:24289	 l-p:0.1705719381570816
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1215
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1491e-01, 1.2873e-01,
         1.0000e+00, 7.7109e-02, 1.0000e+00, 5.9899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8972e-04, 6.0940e-05,
         1.0000e+00, 5.3842e-06, 1.0000e+00, 8.8354e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1664, 4.9658, 4.6219],
        [5.1664, 4.9849, 4.6377],
        [5.1664, 4.9238, 4.9222],
        [5.1664, 5.1664, 5.1664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1215, step:0 
model_pd.l_p.mean(): 0.13104993104934692 
model_pd.l_d.mean(): -20.50752067565918 
model_pd.lagr.mean(): -20.3764705657959 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4394], device='cuda:0')), ('power', tensor([-21.3461], device='cuda:0'))])
epoch£º1215	 i:0 	 global-step:24300	 l-p:0.13104993104934692
epoch£º1215	 i:1 	 global-step:24301	 l-p:0.14854440093040466
epoch£º1215	 i:2 	 global-step:24302	 l-p:0.14198091626167297
epoch£º1215	 i:3 	 global-step:24303	 l-p:0.15776492655277252
epoch£º1215	 i:4 	 global-step:24304	 l-p:0.16606469452381134
epoch£º1215	 i:5 	 global-step:24305	 l-p:0.11205225437879562
epoch£º1215	 i:6 	 global-step:24306	 l-p:0.14924441277980804
epoch£º1215	 i:7 	 global-step:24307	 l-p:0.165674090385437
epoch£º1215	 i:8 	 global-step:24308	 l-p:0.10408910363912582
epoch£º1215	 i:9 	 global-step:24309	 l-p:0.13577742874622345
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1216
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5852e-01, 4.5996e-01,
         1.0000e+00, 3.7879e-01, 1.0000e+00, 8.2353e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6889e-01, 5.8498e-01,
         1.0000e+00, 5.1159e-01, 1.0000e+00, 8.7455e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.8141e-02, 4.5269e-02,
         1.0000e+00, 2.0881e-02, 1.0000e+00, 4.6126e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.5959e-03, 7.6413e-04,
         1.0000e+00, 1.2705e-04, 1.0000e+00, 1.6626e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1684, 4.9875, 4.6403],
        [5.1684, 5.1189, 4.7895],
        [5.1684, 5.0730, 5.1303],
        [5.1684, 5.1681, 5.1684]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1216, step:0 
model_pd.l_p.mean(): 0.12183322012424469 
model_pd.l_d.mean(): -19.307390213012695 
model_pd.lagr.mean(): -19.185556411743164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4757], device='cuda:0')), ('power', tensor([-20.1612], device='cuda:0'))])
epoch£º1216	 i:0 	 global-step:24320	 l-p:0.12183322012424469
epoch£º1216	 i:1 	 global-step:24321	 l-p:0.14406581223011017
epoch£º1216	 i:2 	 global-step:24322	 l-p:0.14708440005779266
epoch£º1216	 i:3 	 global-step:24323	 l-p:0.1329835057258606
epoch£º1216	 i:4 	 global-step:24324	 l-p:0.11029233783483505
epoch£º1216	 i:5 	 global-step:24325	 l-p:0.08521195501089096
epoch£º1216	 i:6 	 global-step:24326	 l-p:0.12365224212408066
epoch£º1216	 i:7 	 global-step:24327	 l-p:0.1490066647529602
epoch£º1216	 i:8 	 global-step:24328	 l-p:0.23876650631427765
epoch£º1216	 i:9 	 global-step:24329	 l-p:0.16628539562225342
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1217
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4074e-02, 3.3981e-03,
         1.0000e+00, 8.2043e-04, 1.0000e+00, 2.4144e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8281e-01, 4.8682e-01,
         1.0000e+00, 4.0664e-01, 1.0000e+00, 8.3530e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6537e-01, 9.0771e-02,
         1.0000e+00, 4.9824e-02, 1.0000e+00, 5.4889e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1657, 5.1626, 5.1656],
        [5.1657, 4.8669, 4.7315],
        [5.1657, 5.0102, 4.6614],
        [5.1657, 4.9778, 5.0246]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1217, step:0 
model_pd.l_p.mean(): 0.15417009592056274 
model_pd.l_d.mean(): -19.335126876831055 
model_pd.lagr.mean(): -19.18095588684082 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5393], device='cuda:0')), ('power', tensor([-20.2553], device='cuda:0'))])
epoch£º1217	 i:0 	 global-step:24340	 l-p:0.15417009592056274
epoch£º1217	 i:1 	 global-step:24341	 l-p:0.13035468757152557
epoch£º1217	 i:2 	 global-step:24342	 l-p:0.14150285720825195
epoch£º1217	 i:3 	 global-step:24343	 l-p:0.178078755736351
epoch£º1217	 i:4 	 global-step:24344	 l-p:0.11165450513362885
epoch£º1217	 i:5 	 global-step:24345	 l-p:0.20793169736862183
epoch£º1217	 i:6 	 global-step:24346	 l-p:0.14915333688259125
epoch£º1217	 i:7 	 global-step:24347	 l-p:0.12883834540843964
epoch£º1217	 i:8 	 global-step:24348	 l-p:0.054543931037187576
epoch£º1217	 i:9 	 global-step:24349	 l-p:0.1538487672805786
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1218
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3929e-01, 4.3897e-01,
         1.0000e+00, 3.5730e-01, 1.0000e+00, 8.1397e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3998e-01, 2.3728e-01,
         1.0000e+00, 1.6561e-01, 1.0000e+00, 6.9794e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0821e-03, 1.1109e-04,
         1.0000e+00, 1.1405e-05, 1.0000e+00, 1.0266e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1351e-01, 5.4963e-02,
         1.0000e+00, 2.6612e-02, 1.0000e+00, 4.8419e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1697, 4.9696, 4.6259],
        [5.1697, 4.8666, 4.6865],
        [5.1697, 5.1696, 5.1697],
        [5.1697, 5.0523, 5.1135]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1218, step:0 
model_pd.l_p.mean(): 0.16570903360843658 
model_pd.l_d.mean(): -19.74047088623047 
model_pd.lagr.mean(): -19.57476234436035 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4991], device='cuda:0')), ('power', tensor([-20.6266], device='cuda:0'))])
epoch£º1218	 i:0 	 global-step:24360	 l-p:0.16570903360843658
epoch£º1218	 i:1 	 global-step:24361	 l-p:0.16012097895145416
epoch£º1218	 i:2 	 global-step:24362	 l-p:0.17107956111431122
epoch£º1218	 i:3 	 global-step:24363	 l-p:0.1417468637228012
epoch£º1218	 i:4 	 global-step:24364	 l-p:0.1730002611875534
epoch£º1218	 i:5 	 global-step:24365	 l-p:0.07273748517036438
epoch£º1218	 i:6 	 global-step:24366	 l-p:0.14953158795833588
epoch£º1218	 i:7 	 global-step:24367	 l-p:0.10188703238964081
epoch£º1218	 i:8 	 global-step:24368	 l-p:0.13035422563552856
epoch£º1218	 i:9 	 global-step:24369	 l-p:0.1283414661884308
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1219
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6431e-02, 2.1645e-02,
         1.0000e+00, 8.3024e-03, 1.0000e+00, 3.8357e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1723, 4.8701, 4.6619],
        [5.1723, 5.0875, 5.1416],
        [5.1723, 5.1326, 5.1643],
        [5.1723, 5.1723, 5.1723]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1219, step:0 
model_pd.l_p.mean(): 0.1314382404088974 
model_pd.l_d.mean(): -19.184999465942383 
model_pd.lagr.mean(): -19.05356216430664 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5056], device='cuda:0')), ('power', tensor([-20.0674], device='cuda:0'))])
epoch£º1219	 i:0 	 global-step:24380	 l-p:0.1314382404088974
epoch£º1219	 i:1 	 global-step:24381	 l-p:0.13222183287143707
epoch£º1219	 i:2 	 global-step:24382	 l-p:0.06925969570875168
epoch£º1219	 i:3 	 global-step:24383	 l-p:0.17236419022083282
epoch£º1219	 i:4 	 global-step:24384	 l-p:0.11155940592288971
epoch£º1219	 i:5 	 global-step:24385	 l-p:0.17529064416885376
epoch£º1219	 i:6 	 global-step:24386	 l-p:0.16504940390586853
epoch£º1219	 i:7 	 global-step:24387	 l-p:0.12090974301099777
epoch£º1219	 i:8 	 global-step:24388	 l-p:0.20358146727085114
epoch£º1219	 i:9 	 global-step:24389	 l-p:0.10998914390802383
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1220
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.4964e-01, 8.0472e-01,
         1.0000e+00, 7.6218e-01, 1.0000e+00, 9.4713e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3448e-01, 5.4520e-01,
         1.0000e+00, 4.6848e-01, 1.0000e+00, 8.5929e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1728, 5.1727, 5.1728],
        [5.1728, 5.3889, 5.1839],
        [5.1728, 5.0798, 4.7388],
        [5.1728, 4.9939, 4.6468]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1220, step:0 
model_pd.l_p.mean(): 0.1758342683315277 
model_pd.l_d.mean(): -20.588586807250977 
model_pd.lagr.mean(): -20.412752151489258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4258], device='cuda:0')), ('power', tensor([-21.4146], device='cuda:0'))])
epoch£º1220	 i:0 	 global-step:24400	 l-p:0.1758342683315277
epoch£º1220	 i:1 	 global-step:24401	 l-p:0.13665048778057098
epoch£º1220	 i:2 	 global-step:24402	 l-p:0.1278878003358841
epoch£º1220	 i:3 	 global-step:24403	 l-p:0.12244987487792969
epoch£º1220	 i:4 	 global-step:24404	 l-p:0.1854977011680603
epoch£º1220	 i:5 	 global-step:24405	 l-p:0.12079620361328125
epoch£º1220	 i:6 	 global-step:24406	 l-p:0.07414000481367111
epoch£º1220	 i:7 	 global-step:24407	 l-p:0.14172133803367615
epoch£º1220	 i:8 	 global-step:24408	 l-p:0.1324695348739624
epoch£º1220	 i:9 	 global-step:24409	 l-p:0.17467088997364044
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1221
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.2728e-03, 1.6732e-03,
         1.0000e+00, 3.3839e-04, 1.0000e+00, 2.0225e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8471e-03, 2.2663e-04,
         1.0000e+00, 2.7807e-05, 1.0000e+00, 1.2270e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8435e-01, 6.0308e-01,
         1.0000e+00, 5.3145e-01, 1.0000e+00, 8.8124e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5587e-01, 2.5218e-01,
         1.0000e+00, 1.7871e-01, 1.0000e+00, 7.0865e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1705, 5.1694, 5.1705],
        [5.1705, 5.1705, 5.1705],
        [5.1705, 5.1420, 4.8195],
        [5.1705, 4.8676, 4.6663]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1221, step:0 
model_pd.l_p.mean(): 0.10961072891950607 
model_pd.l_d.mean(): -20.259674072265625 
model_pd.lagr.mean(): -20.150062561035156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4839], device='cuda:0')), ('power', tensor([-21.1397], device='cuda:0'))])
epoch£º1221	 i:0 	 global-step:24420	 l-p:0.10961072891950607
epoch£º1221	 i:1 	 global-step:24421	 l-p:0.10936011373996735
epoch£º1221	 i:2 	 global-step:24422	 l-p:0.15322881937026978
epoch£º1221	 i:3 	 global-step:24423	 l-p:0.17389339208602905
epoch£º1221	 i:4 	 global-step:24424	 l-p:0.163703054189682
epoch£º1221	 i:5 	 global-step:24425	 l-p:0.13385190069675446
epoch£º1221	 i:6 	 global-step:24426	 l-p:0.15278659760951996
epoch£º1221	 i:7 	 global-step:24427	 l-p:0.17574013769626617
epoch£º1221	 i:8 	 global-step:24428	 l-p:0.1091892197728157
epoch£º1221	 i:9 	 global-step:24429	 l-p:0.12359701842069626
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1222
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4248e-06, 1.1944e-07,
         1.0000e+00, 2.2204e-09, 1.0000e+00, 1.8590e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9884e-02, 2.8785e-02,
         1.0000e+00, 1.1857e-02, 1.0000e+00, 4.1190e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1678, 5.1678, 5.1678],
        [5.1678, 5.1113, 5.1531],
        [5.1678, 5.1508, 5.1660],
        [5.1678, 5.1673, 5.1678]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1222, step:0 
model_pd.l_p.mean(): 0.13448554277420044 
model_pd.l_d.mean(): -20.632911682128906 
model_pd.lagr.mean(): -20.49842643737793 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4112], device='cuda:0')), ('power', tensor([-21.4446], device='cuda:0'))])
epoch£º1222	 i:0 	 global-step:24440	 l-p:0.13448554277420044
epoch£º1222	 i:1 	 global-step:24441	 l-p:0.13218694925308228
epoch£º1222	 i:2 	 global-step:24442	 l-p:0.13862180709838867
epoch£º1222	 i:3 	 global-step:24443	 l-p:0.10754425078630447
epoch£º1222	 i:4 	 global-step:24444	 l-p:0.11467614769935608
epoch£º1222	 i:5 	 global-step:24445	 l-p:0.1487514227628708
epoch£º1222	 i:6 	 global-step:24446	 l-p:0.13941733539104462
epoch£º1222	 i:7 	 global-step:24447	 l-p:0.16258713603019714
epoch£º1222	 i:8 	 global-step:24448	 l-p:0.23184221982955933
epoch£º1222	 i:9 	 global-step:24449	 l-p:0.1296520233154297
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1223
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7844e-02, 3.9050e-02,
         1.0000e+00, 1.7359e-02, 1.0000e+00, 4.4453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9545e-01, 1.1342e-01,
         1.0000e+00, 6.5824e-02, 1.0000e+00, 5.8033e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1603, 5.1092, 5.1480],
        [5.1603, 5.1463, 5.1590],
        [5.1603, 5.0793, 5.1321],
        [5.1603, 4.9369, 4.9577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1223, step:0 
model_pd.l_p.mean(): 0.11726608872413635 
model_pd.l_d.mean(): -19.29126739501953 
model_pd.lagr.mean(): -19.174001693725586 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5031], device='cuda:0')), ('power', tensor([-20.1731], device='cuda:0'))])
epoch£º1223	 i:0 	 global-step:24460	 l-p:0.11726608872413635
epoch£º1223	 i:1 	 global-step:24461	 l-p:0.19029517471790314
epoch£º1223	 i:2 	 global-step:24462	 l-p:0.13906008005142212
epoch£º1223	 i:3 	 global-step:24463	 l-p:0.1471499353647232
epoch£º1223	 i:4 	 global-step:24464	 l-p:0.19832004606723785
epoch£º1223	 i:5 	 global-step:24465	 l-p:0.16329853236675262
epoch£º1223	 i:6 	 global-step:24466	 l-p:0.12035993486642838
epoch£º1223	 i:7 	 global-step:24467	 l-p:0.10778121650218964
epoch£º1223	 i:8 	 global-step:24468	 l-p:0.13783977925777435
epoch£º1223	 i:9 	 global-step:24469	 l-p:0.1308896541595459
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1224
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4390e-01, 9.2591e-01,
         1.0000e+00, 9.0826e-01, 1.0000e+00, 9.8094e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6933e-01, 2.6498e-01,
         1.0000e+00, 1.9012e-01, 1.0000e+00, 7.1747e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1586, 5.0766, 5.1298],
        [5.1586, 5.1586, 5.1586],
        [5.1586, 5.5193, 5.4032],
        [5.1586, 4.8555, 4.6369]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1224, step:0 
model_pd.l_p.mean(): 0.2086653709411621 
model_pd.l_d.mean(): -19.02686882019043 
model_pd.lagr.mean(): -18.81820297241211 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5230], device='cuda:0')), ('power', tensor([-19.9244], device='cuda:0'))])
epoch£º1224	 i:0 	 global-step:24480	 l-p:0.2086653709411621
epoch£º1224	 i:1 	 global-step:24481	 l-p:0.16660884022712708
epoch£º1224	 i:2 	 global-step:24482	 l-p:0.1322806179523468
epoch£º1224	 i:3 	 global-step:24483	 l-p:0.10725344717502594
epoch£º1224	 i:4 	 global-step:24484	 l-p:0.20105212926864624
epoch£º1224	 i:5 	 global-step:24485	 l-p:0.12967915832996368
epoch£º1224	 i:6 	 global-step:24486	 l-p:0.13499224185943604
epoch£º1224	 i:7 	 global-step:24487	 l-p:0.14772187173366547
epoch£º1224	 i:8 	 global-step:24488	 l-p:0.09315736591815948
epoch£º1224	 i:9 	 global-step:24489	 l-p:0.13363125920295715
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1225
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1054e-02, 1.4162e-02,
         1.0000e+00, 4.8856e-03, 1.0000e+00, 3.4497e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6609e-02, 1.2156e-02,
         1.0000e+00, 4.0362e-03, 1.0000e+00, 3.3204e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1585, 5.1356, 5.1555],
        [5.1585, 5.1583, 5.1585],
        [5.1585, 5.1398, 5.1564],
        [5.1585, 5.1586, 5.1585]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1225, step:0 
model_pd.l_p.mean(): 0.10962876677513123 
model_pd.l_d.mean(): -20.30369758605957 
model_pd.lagr.mean(): -20.194068908691406 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4404], device='cuda:0')), ('power', tensor([-21.1395], device='cuda:0'))])
epoch£º1225	 i:0 	 global-step:24500	 l-p:0.10962876677513123
epoch£º1225	 i:1 	 global-step:24501	 l-p:0.2216978818178177
epoch£º1225	 i:2 	 global-step:24502	 l-p:0.13580410182476044
epoch£º1225	 i:3 	 global-step:24503	 l-p:0.12523917853832245
epoch£º1225	 i:4 	 global-step:24504	 l-p:0.1223992332816124
epoch£º1225	 i:5 	 global-step:24505	 l-p:0.1291694939136505
epoch£º1225	 i:6 	 global-step:24506	 l-p:0.1680506318807602
epoch£º1225	 i:7 	 global-step:24507	 l-p:0.19650745391845703
epoch£º1225	 i:8 	 global-step:24508	 l-p:0.11268316209316254
epoch£º1225	 i:9 	 global-step:24509	 l-p:0.15042300522327423
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1226
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4739e-01, 3.4218e-01,
         1.0000e+00, 2.6170e-01, 1.0000e+00, 7.6483e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6610e-07, 9.1306e-10,
         1.0000e+00, 5.0191e-12, 1.0000e+00, 5.4970e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1550, 5.0213, 5.0833],
        [5.1550, 4.8801, 4.5817],
        [5.1550, 5.1550, 5.1550],
        [5.1550, 5.1550, 5.1550]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1226, step:0 
model_pd.l_p.mean(): 0.12522125244140625 
model_pd.l_d.mean(): -18.97869300842285 
model_pd.lagr.mean(): -18.853471755981445 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5719], device='cuda:0')), ('power', tensor([-19.9259], device='cuda:0'))])
epoch£º1226	 i:0 	 global-step:24520	 l-p:0.12522125244140625
epoch£º1226	 i:1 	 global-step:24521	 l-p:0.16650137305259705
epoch£º1226	 i:2 	 global-step:24522	 l-p:0.11665015667676926
epoch£º1226	 i:3 	 global-step:24523	 l-p:0.17721107602119446
epoch£º1226	 i:4 	 global-step:24524	 l-p:0.19914935529232025
epoch£º1226	 i:5 	 global-step:24525	 l-p:0.16444595158100128
epoch£º1226	 i:6 	 global-step:24526	 l-p:0.11682140827178955
epoch£º1226	 i:7 	 global-step:24527	 l-p:0.09728891402482986
epoch£º1226	 i:8 	 global-step:24528	 l-p:0.1590651124715805
epoch£º1226	 i:9 	 global-step:24529	 l-p:0.15440836548805237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1227
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1555, 5.1499, 5.1552],
        [5.1555, 5.1543, 5.1555],
        [5.1555, 5.0637, 5.1202],
        [5.1555, 5.1555, 5.1555]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1227, step:0 
model_pd.l_p.mean(): 0.0994533896446228 
model_pd.l_d.mean(): -20.030057907104492 
model_pd.lagr.mean(): -19.930604934692383 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5020], device='cuda:0')), ('power', tensor([-20.9245], device='cuda:0'))])
epoch£º1227	 i:0 	 global-step:24540	 l-p:0.0994533896446228
epoch£º1227	 i:1 	 global-step:24541	 l-p:0.1828436553478241
epoch£º1227	 i:2 	 global-step:24542	 l-p:0.12206900119781494
epoch£º1227	 i:3 	 global-step:24543	 l-p:0.14492405951023102
epoch£º1227	 i:4 	 global-step:24544	 l-p:0.09429221600294113
epoch£º1227	 i:5 	 global-step:24545	 l-p:0.14102689921855927
epoch£º1227	 i:6 	 global-step:24546	 l-p:0.16628707945346832
epoch£º1227	 i:7 	 global-step:24547	 l-p:0.20290899276733398
epoch£º1227	 i:8 	 global-step:24548	 l-p:0.14055819809436798
epoch£º1227	 i:9 	 global-step:24549	 l-p:0.17012172937393188
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1228
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7604e-01, 4.7930e-01,
         1.0000e+00, 3.9880e-01, 1.0000e+00, 8.3206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7561e-02, 8.3252e-03,
         1.0000e+00, 2.5147e-03, 1.0000e+00, 3.0206e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4130e-02, 3.4161e-03,
         1.0000e+00, 8.2588e-04, 1.0000e+00, 2.4176e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5843e-01, 4.5986e-01,
         1.0000e+00, 3.7869e-01, 1.0000e+00, 8.2348e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1592, 4.9944, 4.6449],
        [5.1592, 5.1480, 5.1583],
        [5.1592, 5.1560, 5.1591],
        [5.1592, 4.9758, 4.6277]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1228, step:0 
model_pd.l_p.mean(): 0.1296553611755371 
model_pd.l_d.mean(): -20.347993850708008 
model_pd.lagr.mean(): -20.218338012695312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4493], device='cuda:0')), ('power', tensor([-21.1939], device='cuda:0'))])
epoch£º1228	 i:0 	 global-step:24560	 l-p:0.1296553611755371
epoch£º1228	 i:1 	 global-step:24561	 l-p:0.14293748140335083
epoch£º1228	 i:2 	 global-step:24562	 l-p:0.14875461161136627
epoch£º1228	 i:3 	 global-step:24563	 l-p:0.12379302084445953
epoch£º1228	 i:4 	 global-step:24564	 l-p:0.1304815113544464
epoch£º1228	 i:5 	 global-step:24565	 l-p:0.16894462704658508
epoch£º1228	 i:6 	 global-step:24566	 l-p:0.09496451169252396
epoch£º1228	 i:7 	 global-step:24567	 l-p:0.21707694232463837
epoch£º1228	 i:8 	 global-step:24568	 l-p:0.1317255049943924
epoch£º1228	 i:9 	 global-step:24569	 l-p:0.14843273162841797
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1229
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.6732e-02, 2.7067e-02,
         1.0000e+00, 1.0979e-02, 1.0000e+00, 4.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3960e-01, 2.3693e-01,
         1.0000e+00, 1.6530e-01, 1.0000e+00, 6.9768e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1659, 5.1134, 5.1530],
        [5.1659, 5.1580, 5.1654],
        [5.1659, 4.8621, 4.6825],
        [5.1659, 4.8668, 4.7313]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1229, step:0 
model_pd.l_p.mean(): 0.0925610139966011 
model_pd.l_d.mean(): -19.231002807617188 
model_pd.lagr.mean(): -19.13844108581543 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4913], device='cuda:0')), ('power', tensor([-20.0995], device='cuda:0'))])
epoch£º1229	 i:0 	 global-step:24580	 l-p:0.0925610139966011
epoch£º1229	 i:1 	 global-step:24581	 l-p:0.1973240077495575
epoch£º1229	 i:2 	 global-step:24582	 l-p:0.12524481117725372
epoch£º1229	 i:3 	 global-step:24583	 l-p:0.1381252408027649
epoch£º1229	 i:4 	 global-step:24584	 l-p:0.1872521936893463
epoch£º1229	 i:5 	 global-step:24585	 l-p:0.1160261407494545
epoch£º1229	 i:6 	 global-step:24586	 l-p:0.1300439089536667
epoch£º1229	 i:7 	 global-step:24587	 l-p:0.15651234984397888
epoch£º1229	 i:8 	 global-step:24588	 l-p:0.1344364732503891
epoch£º1229	 i:9 	 global-step:24589	 l-p:0.12747107446193695
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1230
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4289e-02, 7.0340e-03,
         1.0000e+00, 2.0371e-03, 1.0000e+00, 2.8960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.7647e-03, 1.0336e-03,
         1.0000e+00, 1.8533e-04, 1.0000e+00, 1.7930e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0334e-01, 5.0982e-01,
         1.0000e+00, 4.3080e-01, 1.0000e+00, 8.4500e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1711, 5.1152, 5.1566],
        [5.1711, 5.1623, 5.1704],
        [5.1711, 5.1705, 5.1711],
        [5.1711, 5.0396, 4.6921]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1230, step:0 
model_pd.l_p.mean(): 0.12497708946466446 
model_pd.l_d.mean(): -20.758920669555664 
model_pd.lagr.mean(): -20.633943557739258 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3950], device='cuda:0')), ('power', tensor([-21.5562], device='cuda:0'))])
epoch£º1230	 i:0 	 global-step:24600	 l-p:0.12497708946466446
epoch£º1230	 i:1 	 global-step:24601	 l-p:0.14301414787769318
epoch£º1230	 i:2 	 global-step:24602	 l-p:0.19730278849601746
epoch£º1230	 i:3 	 global-step:24603	 l-p:0.12323395162820816
epoch£º1230	 i:4 	 global-step:24604	 l-p:0.12244246155023575
epoch£º1230	 i:5 	 global-step:24605	 l-p:0.10180965065956116
epoch£º1230	 i:6 	 global-step:24606	 l-p:0.13295838236808777
epoch£º1230	 i:7 	 global-step:24607	 l-p:0.186381533741951
epoch£º1230	 i:8 	 global-step:24608	 l-p:0.11734393984079361
epoch£º1230	 i:9 	 global-step:24609	 l-p:0.1447039544582367
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1231
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7885e-01, 3.7462e-01,
         1.0000e+00, 2.9308e-01, 1.0000e+00, 7.8235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1003e-03, 2.6898e-04,
         1.0000e+00, 3.4446e-05, 1.0000e+00, 1.2806e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9895e-04, 1.1614e-05,
         1.0000e+00, 6.7803e-07, 1.0000e+00, 5.8378e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9985e-01, 5.0589e-01,
         1.0000e+00, 4.2664e-01, 1.0000e+00, 8.4336e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1704, 4.9184, 4.5989],
        [5.1704, 5.1703, 5.1704],
        [5.1704, 5.1704, 5.1704],
        [5.1704, 5.0346, 4.6866]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1231, step:0 
model_pd.l_p.mean(): 0.09110603481531143 
model_pd.l_d.mean(): -20.48691177368164 
model_pd.lagr.mean(): -20.39580535888672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4332], device='cuda:0')), ('power', tensor([-21.3187], device='cuda:0'))])
epoch£º1231	 i:0 	 global-step:24620	 l-p:0.09110603481531143
epoch£º1231	 i:1 	 global-step:24621	 l-p:0.13582059741020203
epoch£º1231	 i:2 	 global-step:24622	 l-p:0.200347900390625
epoch£º1231	 i:3 	 global-step:24623	 l-p:0.149875670671463
epoch£º1231	 i:4 	 global-step:24624	 l-p:0.25294557213783264
epoch£º1231	 i:5 	 global-step:24625	 l-p:0.14749263226985931
epoch£º1231	 i:6 	 global-step:24626	 l-p:0.12183048576116562
epoch£º1231	 i:7 	 global-step:24627	 l-p:0.11672735959291458
epoch£º1231	 i:8 	 global-step:24628	 l-p:0.12633158266544342
epoch£º1231	 i:9 	 global-step:24629	 l-p:0.06776897609233856
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1232
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.6073e-01, 3.5585e-01,
         1.0000e+00, 2.7484e-01, 1.0000e+00, 7.7235e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4065e-02, 1.1043e-02,
         1.0000e+00, 3.5797e-03, 1.0000e+00, 3.2417e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2412e-01, 3.1865e-01,
         1.0000e+00, 2.3941e-01, 1.0000e+00, 7.5133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1627, 4.8969, 4.5888],
        [5.1627, 5.1463, 5.1609],
        [5.1627, 4.8767, 4.5984],
        [5.1627, 4.8881, 4.8334]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1232, step:0 
model_pd.l_p.mean(): 0.1722421944141388 
model_pd.l_d.mean(): -20.62771224975586 
model_pd.lagr.mean(): -20.455469131469727 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4244], device='cuda:0')), ('power', tensor([-21.4530], device='cuda:0'))])
epoch£º1232	 i:0 	 global-step:24640	 l-p:0.1722421944141388
epoch£º1232	 i:1 	 global-step:24641	 l-p:0.06982821971178055
epoch£º1232	 i:2 	 global-step:24642	 l-p:0.13122960925102234
epoch£º1232	 i:3 	 global-step:24643	 l-p:0.15152400732040405
epoch£º1232	 i:4 	 global-step:24644	 l-p:0.14892855286598206
epoch£º1232	 i:5 	 global-step:24645	 l-p:0.13666430115699768
epoch£º1232	 i:6 	 global-step:24646	 l-p:0.23737065494060516
epoch£º1232	 i:7 	 global-step:24647	 l-p:0.13935548067092896
epoch£º1232	 i:8 	 global-step:24648	 l-p:0.14119482040405273
epoch£º1232	 i:9 	 global-step:24649	 l-p:0.12374874204397202
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1233
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4752e-02, 7.2135e-03,
         1.0000e+00, 2.1023e-03, 1.0000e+00, 2.9143e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1567, 4.8699, 4.7859],
        [5.1567, 5.0325, 4.6850],
        [5.1567, 5.1567, 5.1567],
        [5.1567, 5.1475, 5.1560]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1233, step:0 
model_pd.l_p.mean(): 0.10831318795681 
model_pd.l_d.mean(): -20.445959091186523 
model_pd.lagr.mean(): -20.337646484375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4310], device='cuda:0')), ('power', tensor([-21.2747], device='cuda:0'))])
epoch£º1233	 i:0 	 global-step:24660	 l-p:0.10831318795681
epoch£º1233	 i:1 	 global-step:24661	 l-p:0.1171548068523407
epoch£º1233	 i:2 	 global-step:24662	 l-p:0.09626857191324234
epoch£º1233	 i:3 	 global-step:24663	 l-p:0.12814411520957947
epoch£º1233	 i:4 	 global-step:24664	 l-p:0.18966971337795258
epoch£º1233	 i:5 	 global-step:24665	 l-p:0.2418665587902069
epoch£º1233	 i:6 	 global-step:24666	 l-p:0.24236933887004852
epoch£º1233	 i:7 	 global-step:24667	 l-p:0.11653478443622589
epoch£º1233	 i:8 	 global-step:24668	 l-p:0.15502658486366272
epoch£º1233	 i:9 	 global-step:24669	 l-p:0.10172431915998459
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1234
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6972e-04, 9.3967e-06,
         1.0000e+00, 5.2026e-07, 1.0000e+00, 5.5366e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1516, 5.1500, 5.1516],
        [5.1516, 5.1516, 5.1516],
        [5.1516, 5.1516, 5.1516],
        [5.1516, 4.8789, 4.8301]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1234, step:0 
model_pd.l_p.mean(): 0.126564621925354 
model_pd.l_d.mean(): -20.602930068969727 
model_pd.lagr.mean(): -20.47636604309082 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4024], device='cuda:0')), ('power', tensor([-21.4049], device='cuda:0'))])
epoch£º1234	 i:0 	 global-step:24680	 l-p:0.126564621925354
epoch£º1234	 i:1 	 global-step:24681	 l-p:0.11538602411746979
epoch£º1234	 i:2 	 global-step:24682	 l-p:0.11936631798744202
epoch£º1234	 i:3 	 global-step:24683	 l-p:0.15581777691841125
epoch£º1234	 i:4 	 global-step:24684	 l-p:0.10853730887174606
epoch£º1234	 i:5 	 global-step:24685	 l-p:0.2471514791250229
epoch£º1234	 i:6 	 global-step:24686	 l-p:0.3112296164035797
epoch£º1234	 i:7 	 global-step:24687	 l-p:0.10923141241073608
epoch£º1234	 i:8 	 global-step:24688	 l-p:0.10445449501276016
epoch£º1234	 i:9 	 global-step:24689	 l-p:0.12092744559049606
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1235
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2834e-02, 1.9825e-02,
         1.0000e+00, 7.4392e-03, 1.0000e+00, 3.7524e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3715e-02, 6.8136e-03,
         1.0000e+00, 1.9576e-03, 1.0000e+00, 2.8731e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1473, 5.1032, 5.1378],
        [5.1473, 5.1442, 5.1472],
        [5.1473, 5.1116, 5.1407],
        [5.1473, 5.1388, 5.1467]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1235, step:0 
model_pd.l_p.mean(): 0.13871125876903534 
model_pd.l_d.mean(): -20.54767608642578 
model_pd.lagr.mean(): -20.408964157104492 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4159], device='cuda:0')), ('power', tensor([-21.3627], device='cuda:0'))])
epoch£º1235	 i:0 	 global-step:24700	 l-p:0.13871125876903534
epoch£º1235	 i:1 	 global-step:24701	 l-p:0.18374472856521606
epoch£º1235	 i:2 	 global-step:24702	 l-p:0.20623858273029327
epoch£º1235	 i:3 	 global-step:24703	 l-p:0.248056098818779
epoch£º1235	 i:4 	 global-step:24704	 l-p:0.10789977759122849
epoch£º1235	 i:5 	 global-step:24705	 l-p:0.11830171942710876
epoch£º1235	 i:6 	 global-step:24706	 l-p:0.09166163951158524
epoch£º1235	 i:7 	 global-step:24707	 l-p:0.09503473341464996
epoch£º1235	 i:8 	 global-step:24708	 l-p:0.131388321518898
epoch£º1235	 i:9 	 global-step:24709	 l-p:0.22180131077766418
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1236
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5417e-01, 1.6100e-01,
         1.0000e+00, 1.0199e-01, 1.0000e+00, 6.3344e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5279e-01, 8.1680e-02,
         1.0000e+00, 4.3666e-02, 1.0000e+00, 5.3460e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4816e-01, 7.8402e-02,
         1.0000e+00, 4.1487e-02, 1.0000e+00, 5.2915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5303e-04, 2.4951e-05,
         1.0000e+00, 1.7634e-06, 1.0000e+00, 7.0676e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1453, 4.8693, 4.8149],
        [5.1453, 4.9726, 5.0273],
        [5.1453, 4.9788, 5.0355],
        [5.1453, 5.1453, 5.1453]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1236, step:0 
model_pd.l_p.mean(): 0.08954406529664993 
model_pd.l_d.mean(): -20.155488967895508 
model_pd.lagr.mean(): -20.06594467163086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4961], device='cuda:0')), ('power', tensor([-21.0462], device='cuda:0'))])
epoch£º1236	 i:0 	 global-step:24720	 l-p:0.08954406529664993
epoch£º1236	 i:1 	 global-step:24721	 l-p:0.13446936011314392
epoch£º1236	 i:2 	 global-step:24722	 l-p:0.19680482149124146
epoch£º1236	 i:3 	 global-step:24723	 l-p:0.1225915402173996
epoch£º1236	 i:4 	 global-step:24724	 l-p:0.16729404032230377
epoch£º1236	 i:5 	 global-step:24725	 l-p:0.26275473833084106
epoch£º1236	 i:6 	 global-step:24726	 l-p:0.13891197741031647
epoch£º1236	 i:7 	 global-step:24727	 l-p:0.13568711280822754
epoch£º1236	 i:8 	 global-step:24728	 l-p:0.19268947839736938
epoch£º1236	 i:9 	 global-step:24729	 l-p:0.10506460815668106
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1237
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0890e-07, 2.0881e-09,
         1.0000e+00, 1.4116e-11, 1.0000e+00, 6.7599e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1550e-02, 2.4302e-02,
         1.0000e+00, 9.5951e-03, 1.0000e+00, 3.9483e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1494, 5.1494, 5.1494],
        [5.1494, 5.1494, 5.1494],
        [5.1494, 5.1489, 5.1494],
        [5.1494, 5.1032, 5.1391]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1237, step:0 
model_pd.l_p.mean(): 0.12138677388429642 
model_pd.l_d.mean(): -19.882204055786133 
model_pd.lagr.mean(): -19.76081657409668 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4598], device='cuda:0')), ('power', tensor([-20.7303], device='cuda:0'))])
epoch£º1237	 i:0 	 global-step:24740	 l-p:0.12138677388429642
epoch£º1237	 i:1 	 global-step:24741	 l-p:0.2305092215538025
epoch£º1237	 i:2 	 global-step:24742	 l-p:0.13421081006526947
epoch£º1237	 i:3 	 global-step:24743	 l-p:0.10136482864618301
epoch£º1237	 i:4 	 global-step:24744	 l-p:0.14291267096996307
epoch£º1237	 i:5 	 global-step:24745	 l-p:0.16357742249965668
epoch£º1237	 i:6 	 global-step:24746	 l-p:0.14635847508907318
epoch£º1237	 i:7 	 global-step:24747	 l-p:0.09434016793966293
epoch£º1237	 i:8 	 global-step:24748	 l-p:0.10331007838249207
epoch£º1237	 i:9 	 global-step:24749	 l-p:0.2713765799999237
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1238
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4052e-01, 2.3778e-01,
         1.0000e+00, 1.6605e-01, 1.0000e+00, 6.9831e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5008e-01, 1.5755e-01,
         1.0000e+00, 9.9262e-02, 1.0000e+00, 6.3002e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6070e-02, 3.2232e-02,
         1.0000e+00, 1.3657e-02, 1.0000e+00, 4.2371e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1717e-02, 2.4390e-02,
         1.0000e+00, 9.6384e-03, 1.0000e+00, 3.9519e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1516, 4.8459, 4.6649],
        [5.1516, 4.8788, 4.8302],
        [5.1516, 5.0866, 5.1328],
        [5.1516, 5.1053, 5.1413]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1238, step:0 
model_pd.l_p.mean(): 0.1289866864681244 
model_pd.l_d.mean(): -20.939159393310547 
model_pd.lagr.mean(): -20.81017303466797 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3588], device='cuda:0')), ('power', tensor([-21.7023], device='cuda:0'))])
epoch£º1238	 i:0 	 global-step:24760	 l-p:0.1289866864681244
epoch£º1238	 i:1 	 global-step:24761	 l-p:0.1064678430557251
epoch£º1238	 i:2 	 global-step:24762	 l-p:0.14978286623954773
epoch£º1238	 i:3 	 global-step:24763	 l-p:0.15144042670726776
epoch£º1238	 i:4 	 global-step:24764	 l-p:0.15327812731266022
epoch£º1238	 i:5 	 global-step:24765	 l-p:0.12864437699317932
epoch£º1238	 i:6 	 global-step:24766	 l-p:0.27816417813301086
epoch£º1238	 i:7 	 global-step:24767	 l-p:0.13801895081996918
epoch£º1238	 i:8 	 global-step:24768	 l-p:0.13705353438854218
epoch£º1238	 i:9 	 global-step:24769	 l-p:0.1157657578587532
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1239
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0388e-02, 9.4829e-03,
         1.0000e+00, 2.9592e-03, 1.0000e+00, 3.1206e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1570, 5.1565, 5.1570],
        [5.1570, 5.1491, 5.1565],
        [5.1570, 5.2630, 4.9985],
        [5.1570, 5.1436, 5.1558]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1239, step:0 
model_pd.l_p.mean(): 0.11852171272039413 
model_pd.l_d.mean(): -20.802661895751953 
model_pd.lagr.mean(): -20.684139251708984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3890], device='cuda:0')), ('power', tensor([-21.5945], device='cuda:0'))])
epoch£º1239	 i:0 	 global-step:24780	 l-p:0.11852171272039413
epoch£º1239	 i:1 	 global-step:24781	 l-p:0.13734374940395355
epoch£º1239	 i:2 	 global-step:24782	 l-p:0.1235056072473526
epoch£º1239	 i:3 	 global-step:24783	 l-p:0.12906254827976227
epoch£º1239	 i:4 	 global-step:24784	 l-p:0.10743718594312668
epoch£º1239	 i:5 	 global-step:24785	 l-p:0.310684472322464
epoch£º1239	 i:6 	 global-step:24786	 l-p:0.1178620383143425
epoch£º1239	 i:7 	 global-step:24787	 l-p:0.18131515383720398
epoch£º1239	 i:8 	 global-step:24788	 l-p:0.12356331199407578
epoch£º1239	 i:9 	 global-step:24789	 l-p:0.12713056802749634
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1240
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9375e-01, 8.6090e-01,
         1.0000e+00, 8.2926e-01, 1.0000e+00, 9.6325e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1654e-01, 5.6923e-02,
         1.0000e+00, 2.7804e-02, 1.0000e+00, 4.8845e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.1828e-01, 4.1631e-01,
         1.0000e+00, 3.3440e-01, 1.0000e+00, 8.0326e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1548, 5.5386, 5.4374],
        [5.1548, 5.4335, 5.2656],
        [5.1548, 5.0326, 5.0945],
        [5.1548, 4.9314, 4.5925]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1240, step:0 
model_pd.l_p.mean(): 0.12111832946538925 
model_pd.l_d.mean(): -19.972322463989258 
model_pd.lagr.mean(): -19.85120391845703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4968], device='cuda:0')), ('power', tensor([-20.8603], device='cuda:0'))])
epoch£º1240	 i:0 	 global-step:24800	 l-p:0.12111832946538925
epoch£º1240	 i:1 	 global-step:24801	 l-p:0.21302102506160736
epoch£º1240	 i:2 	 global-step:24802	 l-p:0.17742083966732025
epoch£º1240	 i:3 	 global-step:24803	 l-p:0.13145017623901367
epoch£º1240	 i:4 	 global-step:24804	 l-p:0.13694524765014648
epoch£º1240	 i:5 	 global-step:24805	 l-p:0.11487841606140137
epoch£º1240	 i:6 	 global-step:24806	 l-p:0.12924550473690033
epoch£º1240	 i:7 	 global-step:24807	 l-p:0.16072119772434235
epoch£º1240	 i:8 	 global-step:24808	 l-p:0.15417517721652985
epoch£º1240	 i:9 	 global-step:24809	 l-p:0.139070525765419
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1241
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2137e-01, 6.0092e-02,
         1.0000e+00, 2.9753e-02, 1.0000e+00, 4.9511e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8216e-01, 1.8507e-01,
         1.0000e+00, 1.2138e-01, 1.0000e+00, 6.5589e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1567, 5.0277, 5.0898],
        [5.1567, 4.8658, 4.7698],
        [5.1567, 4.9469, 4.9802],
        [5.1567, 5.0881, 5.1359]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1241, step:0 
model_pd.l_p.mean(): 0.13685546815395355 
model_pd.l_d.mean(): -18.451759338378906 
model_pd.lagr.mean(): -18.314903259277344 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5987], device='cuda:0')), ('power', tensor([-19.4169], device='cuda:0'))])
epoch£º1241	 i:0 	 global-step:24820	 l-p:0.13685546815395355
epoch£º1241	 i:1 	 global-step:24821	 l-p:0.16097010672092438
epoch£º1241	 i:2 	 global-step:24822	 l-p:0.2414444088935852
epoch£º1241	 i:3 	 global-step:24823	 l-p:0.1282840371131897
epoch£º1241	 i:4 	 global-step:24824	 l-p:0.11558515578508377
epoch£º1241	 i:5 	 global-step:24825	 l-p:0.1842658519744873
epoch£º1241	 i:6 	 global-step:24826	 l-p:0.12325307726860046
epoch£º1241	 i:7 	 global-step:24827	 l-p:0.09300051629543304
epoch£º1241	 i:8 	 global-step:24828	 l-p:0.10314010083675385
epoch£º1241	 i:9 	 global-step:24829	 l-p:0.1800108551979065
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1242
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1283e-01, 5.2054e-01,
         1.0000e+00, 4.4215e-01, 1.0000e+00, 8.4940e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3037e-04, 6.6106e-06,
         1.0000e+00, 3.3520e-07, 1.0000e+00, 5.0706e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6706e-02, 4.2705e-03,
         1.0000e+00, 1.0917e-03, 1.0000e+00, 2.5563e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1579, 5.0341, 4.6864],
        [5.1579, 5.1478, 4.8315],
        [5.1579, 5.1579, 5.1579],
        [5.1579, 5.1535, 5.1577]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1242, step:0 
model_pd.l_p.mean(): 0.17585687339305878 
model_pd.l_d.mean(): -19.98999786376953 
model_pd.lagr.mean(): -19.81414031982422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5088], device='cuda:0')), ('power', tensor([-20.8908], device='cuda:0'))])
epoch£º1242	 i:0 	 global-step:24840	 l-p:0.17585687339305878
epoch£º1242	 i:1 	 global-step:24841	 l-p:0.11166729778051376
epoch£º1242	 i:2 	 global-step:24842	 l-p:0.14523278176784515
epoch£º1242	 i:3 	 global-step:24843	 l-p:0.09278765320777893
epoch£º1242	 i:4 	 global-step:24844	 l-p:0.1407259851694107
epoch£º1242	 i:5 	 global-step:24845	 l-p:0.1751091182231903
epoch£º1242	 i:6 	 global-step:24846	 l-p:0.2003738284111023
epoch£º1242	 i:7 	 global-step:24847	 l-p:0.1529226452112198
epoch£º1242	 i:8 	 global-step:24848	 l-p:0.11494992673397064
epoch£º1242	 i:9 	 global-step:24849	 l-p:0.14525210857391357
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1243
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2290e-01, 6.1104e-02,
         1.0000e+00, 3.0380e-02, 1.0000e+00, 4.9718e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1625, 5.1621, 5.1625],
        [5.1625, 5.0805, 5.1337],
        [5.1625, 5.0314, 5.0934],
        [5.1625, 4.8664, 4.7491]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1243, step:0 
model_pd.l_p.mean(): 0.17081305384635925 
model_pd.l_d.mean(): -20.1861572265625 
model_pd.lagr.mean(): -20.015344619750977 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4750], device='cuda:0')), ('power', tensor([-21.0556], device='cuda:0'))])
epoch£º1243	 i:0 	 global-step:24860	 l-p:0.17081305384635925
epoch£º1243	 i:1 	 global-step:24861	 l-p:0.13880114257335663
epoch£º1243	 i:2 	 global-step:24862	 l-p:0.12568385899066925
epoch£º1243	 i:3 	 global-step:24863	 l-p:0.12241214513778687
epoch£º1243	 i:4 	 global-step:24864	 l-p:0.16750648617744446
epoch£º1243	 i:5 	 global-step:24865	 l-p:0.12602433562278748
epoch£º1243	 i:6 	 global-step:24866	 l-p:0.13410671055316925
epoch£º1243	 i:7 	 global-step:24867	 l-p:0.15273387730121613
epoch£º1243	 i:8 	 global-step:24868	 l-p:0.13068881630897522
epoch£º1243	 i:9 	 global-step:24869	 l-p:0.161732479929924
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1244
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.1198e-02, 3.5161e-02,
         1.0000e+00, 1.5226e-02, 1.0000e+00, 4.3303e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3808e-01, 7.1367e-02,
         1.0000e+00, 3.6887e-02, 1.0000e+00, 5.1686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0474e-01, 1.2067e-01,
         1.0000e+00, 7.1122e-02, 1.0000e+00, 5.8939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1671, 5.0953, 5.1445],
        [5.1671, 5.0149, 5.0747],
        [5.1671, 4.9339, 4.9444],
        [5.1671, 5.1671, 5.1671]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1244, step:0 
model_pd.l_p.mean(): 0.17519943416118622 
model_pd.l_d.mean(): -20.14961051940918 
model_pd.lagr.mean(): -19.974411010742188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4282], device='cuda:0')), ('power', tensor([-20.9699], device='cuda:0'))])
epoch£º1244	 i:0 	 global-step:24880	 l-p:0.17519943416118622
epoch£º1244	 i:1 	 global-step:24881	 l-p:0.13674871623516083
epoch£º1244	 i:2 	 global-step:24882	 l-p:0.2313595861196518
epoch£º1244	 i:3 	 global-step:24883	 l-p:0.13357946276664734
epoch£º1244	 i:4 	 global-step:24884	 l-p:0.1207113042473793
epoch£º1244	 i:5 	 global-step:24885	 l-p:0.11493605375289917
epoch£º1244	 i:6 	 global-step:24886	 l-p:0.13604415953159332
epoch£º1244	 i:7 	 global-step:24887	 l-p:0.14200371503829956
epoch£º1244	 i:8 	 global-step:24888	 l-p:0.07502105832099915
epoch£º1244	 i:9 	 global-step:24889	 l-p:0.1390068233013153
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1245
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.6075,  0.5145,  1.0000,  0.4357,
          1.0000,  0.8469, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.5837,  0.4878,  1.0000,  0.4077,
          1.0000,  0.8357, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.7394,  0.6686,  1.0000,  0.6046,
          1.0000,  0.9043, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4715,  0.3669,  1.0000,  0.2856,
          1.0000,  0.7783, 31.6228]], device='cuda:0')
 pt:tensor([[5.1727, 5.0458, 4.6985],
        [5.1727, 5.0185, 4.6694],
        [5.1727, 5.2211, 4.9290],
        [5.1727, 4.9152, 4.6000]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1245, step:0 
model_pd.l_p.mean(): 0.16054467856884003 
model_pd.l_d.mean(): -20.42306137084961 
model_pd.lagr.mean(): -20.262516021728516 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4597], device='cuda:0')), ('power', tensor([-21.2811], device='cuda:0'))])
epoch£º1245	 i:0 	 global-step:24900	 l-p:0.16054467856884003
epoch£º1245	 i:1 	 global-step:24901	 l-p:0.21930748224258423
epoch£º1245	 i:2 	 global-step:24902	 l-p:0.11366119235754013
epoch£º1245	 i:3 	 global-step:24903	 l-p:0.06004779785871506
epoch£º1245	 i:4 	 global-step:24904	 l-p:0.1621246337890625
epoch£º1245	 i:5 	 global-step:24905	 l-p:0.16335558891296387
epoch£º1245	 i:6 	 global-step:24906	 l-p:0.1280832290649414
epoch£º1245	 i:7 	 global-step:24907	 l-p:0.10173821449279785
epoch£º1245	 i:8 	 global-step:24908	 l-p:0.15702132880687714
epoch£º1245	 i:9 	 global-step:24909	 l-p:0.12348691374063492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1246
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7129e-01, 3.6677e-01,
         1.0000e+00, 2.8542e-01, 1.0000e+00, 7.7821e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6790e-04, 4.7029e-05,
         1.0000e+00, 3.8945e-06, 1.0000e+00, 8.2812e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7637e-06, 2.1310e-08,
         1.0000e+00, 2.5747e-10, 1.0000e+00, 1.2082e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1713, 4.9135, 4.5983],
        [5.1713, 5.1713, 5.1713],
        [5.1713, 5.1271, 5.1618],
        [5.1713, 5.1713, 5.1713]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1246, step:0 
model_pd.l_p.mean(): 0.20986244082450867 
model_pd.l_d.mean(): -19.977005004882812 
model_pd.lagr.mean(): -19.76714324951172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5278], device='cuda:0')), ('power', tensor([-20.8972], device='cuda:0'))])
epoch£º1246	 i:0 	 global-step:24920	 l-p:0.20986244082450867
epoch£º1246	 i:1 	 global-step:24921	 l-p:0.11507434397935867
epoch£º1246	 i:2 	 global-step:24922	 l-p:0.14388255774974823
epoch£º1246	 i:3 	 global-step:24923	 l-p:0.1339608132839203
epoch£º1246	 i:4 	 global-step:24924	 l-p:0.1318100094795227
epoch£º1246	 i:5 	 global-step:24925	 l-p:0.11465014517307281
epoch£º1246	 i:6 	 global-step:24926	 l-p:0.11727266013622284
epoch£º1246	 i:7 	 global-step:24927	 l-p:0.11749258637428284
epoch£º1246	 i:8 	 global-step:24928	 l-p:0.15099453926086426
epoch£º1246	 i:9 	 global-step:24929	 l-p:0.17318323254585266
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1247
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0058e-07, 1.1742e-09,
         1.0000e+00, 6.8731e-12, 1.0000e+00, 5.8537e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1655, 4.8636, 4.7136],
        [5.1655, 5.1655, 5.1655],
        [5.1655, 4.9908, 5.0441],
        [5.1655, 5.2737, 5.0101]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1247, step:0 
model_pd.l_p.mean(): 0.14845800399780273 
model_pd.l_d.mean(): -19.906131744384766 
model_pd.lagr.mean(): -19.757673263549805 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4847], device='cuda:0')), ('power', tensor([-20.7804], device='cuda:0'))])
epoch£º1247	 i:0 	 global-step:24940	 l-p:0.14845800399780273
epoch£º1247	 i:1 	 global-step:24941	 l-p:0.11856059730052948
epoch£º1247	 i:2 	 global-step:24942	 l-p:0.10731855034828186
epoch£º1247	 i:3 	 global-step:24943	 l-p:0.14837323129177094
epoch£º1247	 i:4 	 global-step:24944	 l-p:0.15671709179878235
epoch£º1247	 i:5 	 global-step:24945	 l-p:0.12128620594739914
epoch£º1247	 i:6 	 global-step:24946	 l-p:0.16745847463607788
epoch£º1247	 i:7 	 global-step:24947	 l-p:0.13786424696445465
epoch£º1247	 i:8 	 global-step:24948	 l-p:0.14342562854290009
epoch£º1247	 i:9 	 global-step:24949	 l-p:0.17466232180595398
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1248
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5394e-02, 4.3587e-02,
         1.0000e+00, 1.9916e-02, 1.0000e+00, 4.5692e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6565e-05, 4.2225e-07,
         1.0000e+00, 1.0764e-08, 1.0000e+00, 2.5491e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7298e-01, 1.7708e-01,
         1.0000e+00, 1.1487e-01, 1.0000e+00, 6.4870e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3114e-01, 2.2909e-01,
         1.0000e+00, 1.5849e-01, 1.0000e+00, 6.9183e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1657, 5.0739, 5.1303],
        [5.1657, 5.1657, 5.1657],
        [5.1657, 4.8798, 4.7974],
        [5.1657, 4.8619, 4.6939]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1248, step:0 
model_pd.l_p.mean(): 0.1339005082845688 
model_pd.l_d.mean(): -20.866384506225586 
model_pd.lagr.mean(): -20.732484817504883 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.3788], device='cuda:0')), ('power', tensor([-21.6489], device='cuda:0'))])
epoch£º1248	 i:0 	 global-step:24960	 l-p:0.1339005082845688
epoch£º1248	 i:1 	 global-step:24961	 l-p:0.1692771315574646
epoch£º1248	 i:2 	 global-step:24962	 l-p:0.12755708396434784
epoch£º1248	 i:3 	 global-step:24963	 l-p:0.132822185754776
epoch£º1248	 i:4 	 global-step:24964	 l-p:0.20739510655403137
epoch£º1248	 i:5 	 global-step:24965	 l-p:0.16926589608192444
epoch£º1248	 i:6 	 global-step:24966	 l-p:0.13228534162044525
epoch£º1248	 i:7 	 global-step:24967	 l-p:0.1546749770641327
epoch£º1248	 i:8 	 global-step:24968	 l-p:0.07762697339057922
epoch£º1248	 i:9 	 global-step:24969	 l-p:0.11956453323364258
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1249
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.3557e-07, 7.8701e-09,
         1.0000e+00, 7.4126e-11, 1.0000e+00, 9.4188e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1664, 4.8634, 4.6459],
        [5.1664, 5.1664, 5.1664],
        [5.1664, 4.8847, 4.5981],
        [5.1664, 5.1664, 5.1664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1249, step:0 
model_pd.l_p.mean(): 0.1747424602508545 
model_pd.l_d.mean(): -20.15886116027832 
model_pd.lagr.mean(): -19.984119415283203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4838], device='cuda:0')), ('power', tensor([-21.0370], device='cuda:0'))])
epoch£º1249	 i:0 	 global-step:24980	 l-p:0.1747424602508545
epoch£º1249	 i:1 	 global-step:24981	 l-p:0.08794322609901428
epoch£º1249	 i:2 	 global-step:24982	 l-p:0.17943067848682404
epoch£º1249	 i:3 	 global-step:24983	 l-p:0.16177253425121307
epoch£º1249	 i:4 	 global-step:24984	 l-p:0.10069540143013
epoch£º1249	 i:5 	 global-step:24985	 l-p:0.1732098013162613
epoch£º1249	 i:6 	 global-step:24986	 l-p:0.12268217653036118
epoch£º1249	 i:7 	 global-step:24987	 l-p:0.16364900767803192
epoch£º1249	 i:8 	 global-step:24988	 l-p:0.10897378623485565
epoch£º1249	 i:9 	 global-step:24989	 l-p:0.1447848081588745
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1250
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1828,  0.1038,  1.0000,  0.0589,
          1.0000,  0.5675, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2052,  0.1211,  1.0000,  0.0714,
          1.0000,  0.5899, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1353,  0.0695,  1.0000,  0.0357,
          1.0000,  0.5134, 31.6228]], device='cuda:0')
 pt:tensor([[5.1683, 4.9590, 4.9920],
        [5.1683, 4.9346, 4.9446],
        [5.1683, 5.0130, 5.0722],
        [5.1683, 5.0199, 5.0804]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1250, step:0 
model_pd.l_p.mean(): 0.1329367607831955 
model_pd.l_d.mean(): -20.23050880432129 
model_pd.lagr.mean(): -20.097572326660156 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4502], device='cuda:0')), ('power', tensor([-21.0751], device='cuda:0'))])
epoch£º1250	 i:0 	 global-step:25000	 l-p:0.1329367607831955
epoch£º1250	 i:1 	 global-step:25001	 l-p:0.1712767630815506
epoch£º1250	 i:2 	 global-step:25002	 l-p:0.1356842815876007
epoch£º1250	 i:3 	 global-step:25003	 l-p:0.1278686821460724
epoch£º1250	 i:4 	 global-step:25004	 l-p:0.1264576017856598
epoch£º1250	 i:5 	 global-step:25005	 l-p:0.22085779905319214
epoch£º1250	 i:6 	 global-step:25006	 l-p:0.07501935213804245
epoch£º1250	 i:7 	 global-step:25007	 l-p:0.12208936363458633
epoch£º1250	 i:8 	 global-step:25008	 l-p:0.11053744703531265
epoch£º1250	 i:9 	 global-step:25009	 l-p:0.19730521738529205
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1251
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.3578e-03, 1.4311e-03,
         1.0000e+00, 2.7834e-04, 1.0000e+00, 1.9450e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8713e-05, 8.7922e-07,
         1.0000e+00, 2.6923e-08, 1.0000e+00, 3.0621e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8281e-01, 1.0375e-01,
         1.0000e+00, 5.8885e-02, 1.0000e+00, 5.6754e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1661, 4.8802, 4.7975],
        [5.1661, 5.1652, 5.1661],
        [5.1661, 5.1661, 5.1661],
        [5.1661, 4.9566, 4.9898]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1251, step:0 
model_pd.l_p.mean(): 0.09445790946483612 
model_pd.l_d.mean(): -19.931257247924805 
model_pd.lagr.mean(): -19.83679962158203 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4958], device='cuda:0')), ('power', tensor([-20.8175], device='cuda:0'))])
epoch£º1251	 i:0 	 global-step:25020	 l-p:0.09445790946483612
epoch£º1251	 i:1 	 global-step:25021	 l-p:0.1325429528951645
epoch£º1251	 i:2 	 global-step:25022	 l-p:0.18681471049785614
epoch£º1251	 i:3 	 global-step:25023	 l-p:0.13174985349178314
epoch£º1251	 i:4 	 global-step:25024	 l-p:0.17638283967971802
epoch£º1251	 i:5 	 global-step:25025	 l-p:0.11356739699840546
epoch£º1251	 i:6 	 global-step:25026	 l-p:0.1127854511141777
epoch£º1251	 i:7 	 global-step:25027	 l-p:0.22666415572166443
epoch£º1251	 i:8 	 global-step:25028	 l-p:0.12097091227769852
epoch£º1251	 i:9 	 global-step:25029	 l-p:0.12466482073068619
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1252
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3533e-01, 6.9480e-02,
         1.0000e+00, 3.5672e-02, 1.0000e+00, 5.1341e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5912e-01, 4.6062e-01,
         1.0000e+00, 3.7947e-01, 1.0000e+00, 8.2383e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0156e-03, 1.0208e-04,
         1.0000e+00, 1.0261e-05, 1.0000e+00, 1.0052e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.4058e-01, 3.3525e-01,
         1.0000e+00, 2.5510e-01, 1.0000e+00, 7.6093e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1671, 5.0186, 5.0792],
        [5.1671, 4.9850, 4.6368],
        [5.1671, 5.1671, 5.1671],
        [5.1671, 4.8893, 4.5963]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1252, step:0 
model_pd.l_p.mean(): 0.15863558650016785 
model_pd.l_d.mean(): -19.93840217590332 
model_pd.lagr.mean(): -19.779766082763672 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5068], device='cuda:0')), ('power', tensor([-20.8362], device='cuda:0'))])
epoch£º1252	 i:0 	 global-step:25040	 l-p:0.15863558650016785
epoch£º1252	 i:1 	 global-step:25041	 l-p:0.20450694859027863
epoch£º1252	 i:2 	 global-step:25042	 l-p:0.1382918506860733
epoch£º1252	 i:3 	 global-step:25043	 l-p:0.12336131930351257
epoch£º1252	 i:4 	 global-step:25044	 l-p:0.11039762943983078
epoch£º1252	 i:5 	 global-step:25045	 l-p:0.14679624140262604
epoch£º1252	 i:6 	 global-step:25046	 l-p:0.09945397078990936
epoch£º1252	 i:7 	 global-step:25047	 l-p:0.1649436205625534
epoch£º1252	 i:8 	 global-step:25048	 l-p:0.14238306879997253
epoch£º1252	 i:9 	 global-step:25049	 l-p:0.12185933440923691
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1253
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6999e-05, 1.2329e-06,
         1.0000e+00, 4.1083e-08, 1.0000e+00, 3.3322e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0876e-05, 5.7480e-07,
         1.0000e+00, 1.5827e-08, 1.0000e+00, 2.7535e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9571e-05, 5.2743e-07,
         1.0000e+00, 1.4214e-08, 1.0000e+00, 2.6949e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1690, 5.1690, 5.1690],
        [5.1690, 5.1684, 5.1689],
        [5.1690, 5.1690, 5.1690],
        [5.1690, 5.1690, 5.1690]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1253, step:0 
model_pd.l_p.mean(): 0.12481921166181564 
model_pd.l_d.mean(): -20.054597854614258 
model_pd.lagr.mean(): -19.929779052734375 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4298], device='cuda:0')), ('power', tensor([-20.8748], device='cuda:0'))])
epoch£º1253	 i:0 	 global-step:25060	 l-p:0.12481921166181564
epoch£º1253	 i:1 	 global-step:25061	 l-p:0.1345176249742508
epoch£º1253	 i:2 	 global-step:25062	 l-p:0.15083102881908417
epoch£º1253	 i:3 	 global-step:25063	 l-p:0.16337423026561737
epoch£º1253	 i:4 	 global-step:25064	 l-p:0.16138030588626862
epoch£º1253	 i:5 	 global-step:25065	 l-p:0.13326895236968994
epoch£º1253	 i:6 	 global-step:25066	 l-p:0.09780975431203842
epoch£º1253	 i:7 	 global-step:25067	 l-p:0.09177747368812561
epoch£º1253	 i:8 	 global-step:25068	 l-p:0.11327574402093887
epoch£º1253	 i:9 	 global-step:25069	 l-p:0.25129371881484985
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1254
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7676e-01, 8.3915e-01,
         1.0000e+00, 8.0316e-01, 1.0000e+00, 9.5711e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5477e-01, 8.3097e-02,
         1.0000e+00, 4.4615e-02, 1.0000e+00, 5.3690e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4046e-02, 3.3891e-03,
         1.0000e+00, 8.1772e-04, 1.0000e+00, 2.4128e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2922e-01, 2.2733e-01,
         1.0000e+00, 1.5697e-01, 1.0000e+00, 6.9050e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1650, 5.4198, 5.2371],
        [5.1650, 4.9902, 5.0435],
        [5.1650, 5.1619, 5.1649],
        [5.1650, 4.8612, 4.6959]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1254, step:0 
model_pd.l_p.mean(): 0.12301596999168396 
model_pd.l_d.mean(): -19.149044036865234 
model_pd.lagr.mean(): -19.02602767944336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5404], device='cuda:0')), ('power', tensor([-20.0669], device='cuda:0'))])
epoch£º1254	 i:0 	 global-step:25080	 l-p:0.12301596999168396
epoch£º1254	 i:1 	 global-step:25081	 l-p:0.10700185596942902
epoch£º1254	 i:2 	 global-step:25082	 l-p:0.11122526973485947
epoch£º1254	 i:3 	 global-step:25083	 l-p:0.11532440036535263
epoch£º1254	 i:4 	 global-step:25084	 l-p:0.14016078412532806
epoch£º1254	 i:5 	 global-step:25085	 l-p:0.12233235687017441
epoch£º1254	 i:6 	 global-step:25086	 l-p:0.2552790641784668
epoch£º1254	 i:7 	 global-step:25087	 l-p:0.12149026244878769
epoch£º1254	 i:8 	 global-step:25088	 l-p:0.2113315463066101
epoch£º1254	 i:9 	 global-step:25089	 l-p:0.1309182196855545
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1255
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6078e-01, 8.7427e-02,
         1.0000e+00, 4.7540e-02, 1.0000e+00, 5.4377e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2474e-01, 6.2329e-02,
         1.0000e+00, 3.1143e-02, 1.0000e+00, 4.9966e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3780e-04, 2.3526e-05,
         1.0000e+00, 1.6385e-06, 1.0000e+00, 6.9645e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1624, 4.9797, 5.0297],
        [5.1624, 4.8622, 4.7266],
        [5.1624, 5.0286, 5.0906],
        [5.1624, 5.1624, 5.1624]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1255, step:0 
model_pd.l_p.mean(): 0.11436744779348373 
model_pd.l_d.mean(): -19.063570022583008 
model_pd.lagr.mean(): -18.949203491210938 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5651], device='cuda:0')), ('power', tensor([-20.0054], device='cuda:0'))])
epoch£º1255	 i:0 	 global-step:25100	 l-p:0.11436744779348373
epoch£º1255	 i:1 	 global-step:25101	 l-p:0.18112079799175262
epoch£º1255	 i:2 	 global-step:25102	 l-p:0.11938922852277756
epoch£º1255	 i:3 	 global-step:25103	 l-p:0.17471201717853546
epoch£º1255	 i:4 	 global-step:25104	 l-p:0.11671274900436401
epoch£º1255	 i:5 	 global-step:25105	 l-p:0.20214927196502686
epoch£º1255	 i:6 	 global-step:25106	 l-p:0.1605963110923767
epoch£º1255	 i:7 	 global-step:25107	 l-p:0.11838703602552414
epoch£º1255	 i:8 	 global-step:25108	 l-p:0.14304499328136444
epoch£º1255	 i:9 	 global-step:25109	 l-p:0.10595554113388062
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1256
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0317e-01, 4.8389e-02,
         1.0000e+00, 2.2695e-02, 1.0000e+00, 4.6902e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6139e-01, 1.6713e-01,
         1.0000e+00, 1.0686e-01, 1.0000e+00, 6.3939e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8835e-01, 8.5398e-01,
         1.0000e+00, 8.2094e-01, 1.0000e+00, 9.6131e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4131e-02, 6.9733e-03,
         1.0000e+00, 2.0151e-03, 1.0000e+00, 2.8898e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1646, 5.0617, 5.1209],
        [5.1646, 4.8850, 4.8197],
        [5.1646, 5.4375, 5.2657],
        [5.1646, 5.1559, 5.1640]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1256, step:0 
model_pd.l_p.mean(): 0.0859561637043953 
model_pd.l_d.mean(): -20.393198013305664 
model_pd.lagr.mean(): -20.307241439819336 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4413], device='cuda:0')), ('power', tensor([-21.2316], device='cuda:0'))])
epoch£º1256	 i:0 	 global-step:25120	 l-p:0.0859561637043953
epoch£º1256	 i:1 	 global-step:25121	 l-p:0.1477728933095932
epoch£º1256	 i:2 	 global-step:25122	 l-p:0.16578027606010437
epoch£º1256	 i:3 	 global-step:25123	 l-p:0.17672765254974365
epoch£º1256	 i:4 	 global-step:25124	 l-p:0.17118459939956665
epoch£º1256	 i:5 	 global-step:25125	 l-p:0.15154941380023956
epoch£º1256	 i:6 	 global-step:25126	 l-p:0.10168249160051346
epoch£º1256	 i:7 	 global-step:25127	 l-p:0.131813645362854
epoch£º1256	 i:8 	 global-step:25128	 l-p:0.16157594323158264
epoch£º1256	 i:9 	 global-step:25129	 l-p:0.14002084732055664
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1257
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.9634e-01, 1.9757e-01,
         1.0000e+00, 1.3172e-01, 1.0000e+00, 6.6670e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1456e-01, 5.2250e-01,
         1.0000e+00, 4.4423e-01, 1.0000e+00, 8.5020e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8043e-04, 1.0195e-05,
         1.0000e+00, 5.7611e-07, 1.0000e+00, 5.6507e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1628, 4.8663, 4.7490],
        [5.1628, 5.0416, 4.6942],
        [5.1628, 5.1624, 5.1628],
        [5.1628, 5.1628, 5.1628]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1257, step:0 
model_pd.l_p.mean(): 0.11559350043535233 
model_pd.l_d.mean(): -20.265033721923828 
model_pd.lagr.mean(): -20.14944076538086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4717], device='cuda:0')), ('power', tensor([-21.1326], device='cuda:0'))])
epoch£º1257	 i:0 	 global-step:25140	 l-p:0.11559350043535233
epoch£º1257	 i:1 	 global-step:25141	 l-p:0.213795468211174
epoch£º1257	 i:2 	 global-step:25142	 l-p:0.11801204085350037
epoch£º1257	 i:3 	 global-step:25143	 l-p:0.22390171885490417
epoch£º1257	 i:4 	 global-step:25144	 l-p:0.10181644558906555
epoch£º1257	 i:5 	 global-step:25145	 l-p:0.11132173240184784
epoch£º1257	 i:6 	 global-step:25146	 l-p:0.14745141565799713
epoch£º1257	 i:7 	 global-step:25147	 l-p:0.10584743320941925
epoch£º1257	 i:8 	 global-step:25148	 l-p:0.14952512085437775
epoch£º1257	 i:9 	 global-step:25149	 l-p:0.1547701358795166
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1258
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8385e-03, 8.1837e-04,
         1.0000e+00, 1.3842e-04, 1.0000e+00, 1.6914e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9249e-05, 1.8052e-06,
         1.0000e+00, 6.6169e-08, 1.0000e+00, 3.6655e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3524e-01, 1.4521e-01,
         1.0000e+00, 8.9642e-02, 1.0000e+00, 6.1731e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.1827e-01, 3.1281e-01,
         1.0000e+00, 2.3394e-01, 1.0000e+00, 7.4786e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1615, 5.1611, 5.1615],
        [5.1615, 5.1615, 5.1615],
        [5.1615, 4.9001, 4.8721],
        [5.1615, 4.8718, 4.5986]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1258, step:0 
model_pd.l_p.mean(): 0.13511939346790314 
model_pd.l_d.mean(): -20.670663833618164 
model_pd.lagr.mean(): -20.535545349121094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4160], device='cuda:0')), ('power', tensor([-21.4881], device='cuda:0'))])
epoch£º1258	 i:0 	 global-step:25160	 l-p:0.13511939346790314
epoch£º1258	 i:1 	 global-step:25161	 l-p:0.1837960034608841
epoch£º1258	 i:2 	 global-step:25162	 l-p:0.19702880084514618
epoch£º1258	 i:3 	 global-step:25163	 l-p:0.1271802932024002
epoch£º1258	 i:4 	 global-step:25164	 l-p:0.1211402639746666
epoch£º1258	 i:5 	 global-step:25165	 l-p:0.22651880979537964
epoch£º1258	 i:6 	 global-step:25166	 l-p:0.06498542428016663
epoch£º1258	 i:7 	 global-step:25167	 l-p:0.12196220457553864
epoch£º1258	 i:8 	 global-step:25168	 l-p:0.14425495266914368
epoch£º1258	 i:9 	 global-step:25169	 l-p:0.1224297285079956
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1259
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6895e-02, 4.3354e-03,
         1.0000e+00, 1.1125e-03, 1.0000e+00, 2.5660e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.6163e-01, 1.6733e-01,
         1.0000e+00, 1.0702e-01, 1.0000e+00, 6.3958e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.5982e-01, 4.6138e-01,
         1.0000e+00, 3.8025e-01, 1.0000e+00, 8.2417e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1646, 5.1602, 5.1644],
        [5.1646, 4.8849, 4.8192],
        [5.1646, 5.1604, 5.1644],
        [5.1646, 4.9826, 4.6339]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1259, step:0 
model_pd.l_p.mean(): 0.14348375797271729 
model_pd.l_d.mean(): -19.846172332763672 
model_pd.lagr.mean(): -19.702688217163086 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5143], device='cuda:0')), ('power', tensor([-20.7499], device='cuda:0'))])
epoch£º1259	 i:0 	 global-step:25180	 l-p:0.14348375797271729
epoch£º1259	 i:1 	 global-step:25181	 l-p:0.16310229897499084
epoch£º1259	 i:2 	 global-step:25182	 l-p:0.13407744467258453
epoch£º1259	 i:3 	 global-step:25183	 l-p:0.12062332034111023
epoch£º1259	 i:4 	 global-step:25184	 l-p:0.07778340578079224
epoch£º1259	 i:5 	 global-step:25185	 l-p:0.17418429255485535
epoch£º1259	 i:6 	 global-step:25186	 l-p:0.13053041696548462
epoch£º1259	 i:7 	 global-step:25187	 l-p:0.13756884634494781
epoch£º1259	 i:8 	 global-step:25188	 l-p:0.1971082240343094
epoch£º1259	 i:9 	 global-step:25189	 l-p:0.1445402204990387
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1260
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6841e-02, 4.3167e-03,
         1.0000e+00, 1.1065e-03, 1.0000e+00, 2.5632e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.4203e-01, 1.5084e-01,
         1.0000e+00, 9.4000e-02, 1.0000e+00, 6.2320e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7813e-04, 2.7343e-05,
         1.0000e+00, 1.9773e-06, 1.0000e+00, 7.2312e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1673, 5.1628, 5.1671],
        [5.1673, 4.9009, 4.8634],
        [5.1673, 5.1673, 5.1673],
        [5.1673, 5.0803, 5.1353]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1260, step:0 
model_pd.l_p.mean(): 0.16367942094802856 
model_pd.l_d.mean(): -20.335247039794922 
model_pd.lagr.mean(): -20.171567916870117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4589], device='cuda:0')), ('power', tensor([-21.1908], device='cuda:0'))])
epoch£º1260	 i:0 	 global-step:25200	 l-p:0.16367942094802856
epoch£º1260	 i:1 	 global-step:25201	 l-p:0.1897602379322052
epoch£º1260	 i:2 	 global-step:25202	 l-p:0.1592535525560379
epoch£º1260	 i:3 	 global-step:25203	 l-p:0.11737105995416641
epoch£º1260	 i:4 	 global-step:25204	 l-p:0.16523827612400055
epoch£º1260	 i:5 	 global-step:25205	 l-p:0.1287468820810318
epoch£º1260	 i:6 	 global-step:25206	 l-p:0.1014166995882988
epoch£º1260	 i:7 	 global-step:25207	 l-p:0.18696144223213196
epoch£º1260	 i:8 	 global-step:25208	 l-p:0.06272503733634949
epoch£º1260	 i:9 	 global-step:25209	 l-p:0.13402113318443298
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1261
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.3685e-05, 1.0879e-06,
         1.0000e+00, 3.5134e-08, 1.0000e+00, 3.2296e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.8582e-03, 4.0563e-04,
         1.0000e+00, 5.7565e-05, 1.0000e+00, 1.4192e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4776e-02, 1.1351e-02,
         1.0000e+00, 3.7050e-03, 1.0000e+00, 3.2641e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7716e-02, 4.6182e-03,
         1.0000e+00, 1.2039e-03, 1.0000e+00, 2.6069e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1716, 5.1716, 5.1716],
        [5.1716, 5.1715, 5.1716],
        [5.1716, 5.1546, 5.1697],
        [5.1716, 5.1667, 5.1714]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1261, step:0 
model_pd.l_p.mean(): 0.17002618312835693 
model_pd.l_d.mean(): -20.654756546020508 
model_pd.lagr.mean(): -20.484729766845703 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4137], device='cuda:0')), ('power', tensor([-21.4695], device='cuda:0'))])
epoch£º1261	 i:0 	 global-step:25220	 l-p:0.17002618312835693
epoch£º1261	 i:1 	 global-step:25221	 l-p:0.15969428420066833
epoch£º1261	 i:2 	 global-step:25222	 l-p:0.21351489424705505
epoch£º1261	 i:3 	 global-step:25223	 l-p:0.11539693921804428
epoch£º1261	 i:4 	 global-step:25224	 l-p:0.10329608619213104
epoch£º1261	 i:5 	 global-step:25225	 l-p:0.0951651856303215
epoch£º1261	 i:6 	 global-step:25226	 l-p:0.11426814645528793
epoch£º1261	 i:7 	 global-step:25227	 l-p:0.12541037797927856
epoch£º1261	 i:8 	 global-step:25228	 l-p:0.17976312339305878
epoch£º1261	 i:9 	 global-step:25229	 l-p:0.11406054347753525
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1262
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1203e-01, 6.3581e-01,
         1.0000e+00, 5.6775e-01, 1.0000e+00, 8.9296e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.2834e-02, 1.4987e-02,
         1.0000e+00, 5.2439e-03, 1.0000e+00, 3.4989e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1742, 5.1833, 4.8743],
        [5.1742, 5.1494, 5.1707],
        [5.1742, 5.1815, 4.8718],
        [5.1742, 5.0529, 4.7061]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1262, step:0 
model_pd.l_p.mean(): 0.18035507202148438 
model_pd.l_d.mean(): -19.47112274169922 
model_pd.lagr.mean(): -19.290767669677734 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4904], device='cuda:0')), ('power', tensor([-20.3431], device='cuda:0'))])
epoch£º1262	 i:0 	 global-step:25240	 l-p:0.18035507202148438
epoch£º1262	 i:1 	 global-step:25241	 l-p:0.16240113973617554
epoch£º1262	 i:2 	 global-step:25242	 l-p:0.16115149855613708
epoch£º1262	 i:3 	 global-step:25243	 l-p:0.17900054156780243
epoch£º1262	 i:4 	 global-step:25244	 l-p:0.11590167880058289
epoch£º1262	 i:5 	 global-step:25245	 l-p:0.11480822414159775
epoch£º1262	 i:6 	 global-step:25246	 l-p:0.14783570170402527
epoch£º1262	 i:7 	 global-step:25247	 l-p:0.07980983704328537
epoch£º1262	 i:8 	 global-step:25248	 l-p:0.10126706212759018
epoch£º1262	 i:9 	 global-step:25249	 l-p:0.1444341242313385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1263
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3872e-02, 2.5532e-02,
         1.0000e+00, 1.0206e-02, 1.0000e+00, 3.9973e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.9291e-02, 4.5978e-02,
         1.0000e+00, 2.1290e-02, 1.0000e+00, 4.6306e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4661e-01, 7.7305e-02,
         1.0000e+00, 4.0762e-02, 1.0000e+00, 5.2729e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1745, 5.3030, 5.0495],
        [5.1745, 5.1256, 5.1631],
        [5.1745, 5.0773, 5.1351],
        [5.1745, 5.0107, 5.0677]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1263, step:0 
model_pd.l_p.mean(): 0.08026815205812454 
model_pd.l_d.mean(): -18.81903648376465 
model_pd.lagr.mean(): -18.738767623901367 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5324], device='cuda:0')), ('power', tensor([-19.7224], device='cuda:0'))])
epoch£º1263	 i:0 	 global-step:25260	 l-p:0.08026815205812454
epoch£º1263	 i:1 	 global-step:25261	 l-p:0.13990825414657593
epoch£º1263	 i:2 	 global-step:25262	 l-p:0.20780880749225616
epoch£º1263	 i:3 	 global-step:25263	 l-p:0.11688367277383804
epoch£º1263	 i:4 	 global-step:25264	 l-p:0.11399289965629578
epoch£º1263	 i:5 	 global-step:25265	 l-p:0.1961727887392044
epoch£º1263	 i:6 	 global-step:25266	 l-p:0.138260155916214
epoch£º1263	 i:7 	 global-step:25267	 l-p:0.10485438257455826
epoch£º1263	 i:8 	 global-step:25268	 l-p:0.12984029948711395
epoch£º1263	 i:9 	 global-step:25269	 l-p:0.15673896670341492
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1264
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7327e-01, 2.6876e-01,
         1.0000e+00, 1.9351e-01, 1.0000e+00, 7.2001e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9196e-01, 1.1074e-01,
         1.0000e+00, 6.3880e-02, 1.0000e+00, 5.7686e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8051e-08, 2.7783e-10,
         1.0000e+00, 1.1343e-12, 1.0000e+00, 4.0827e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1751, 4.8735, 4.6499],
        [5.1751, 4.9556, 4.9798],
        [5.1751, 5.1751, 5.1751],
        [5.1751, 5.1751, 5.1751]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1264, step:0 
model_pd.l_p.mean(): 0.10443389415740967 
model_pd.l_d.mean(): -19.042123794555664 
model_pd.lagr.mean(): -18.93769073486328 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5209], device='cuda:0')), ('power', tensor([-19.9377], device='cuda:0'))])
epoch£º1264	 i:0 	 global-step:25280	 l-p:0.10443389415740967
epoch£º1264	 i:1 	 global-step:25281	 l-p:0.2136850655078888
epoch£º1264	 i:2 	 global-step:25282	 l-p:0.14319099485874176
epoch£º1264	 i:3 	 global-step:25283	 l-p:0.16246497631072998
epoch£º1264	 i:4 	 global-step:25284	 l-p:0.12431272119283676
epoch£º1264	 i:5 	 global-step:25285	 l-p:0.1037164032459259
epoch£º1264	 i:6 	 global-step:25286	 l-p:0.19156081974506378
epoch£º1264	 i:7 	 global-step:25287	 l-p:0.10199642181396484
epoch£º1264	 i:8 	 global-step:25288	 l-p:0.13300861418247223
epoch£º1264	 i:9 	 global-step:25289	 l-p:0.10750754177570343
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1265
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8408e-02, 4.8605e-03,
         1.0000e+00, 1.2834e-03, 1.0000e+00, 2.6404e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.8003e-02, 2.7757e-02,
         1.0000e+00, 1.1329e-02, 1.0000e+00, 4.0817e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1723, 5.1670, 5.1720],
        [5.1723, 4.9601, 4.9909],
        [5.1723, 5.1180, 5.1586],
        [5.1723, 5.1723, 5.1723]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1265, step:0 
model_pd.l_p.mean(): 0.107322558760643 
model_pd.l_d.mean(): -19.24891471862793 
model_pd.lagr.mean(): -19.141592025756836 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5285], device='cuda:0')), ('power', tensor([-20.1563], device='cuda:0'))])
epoch£º1265	 i:0 	 global-step:25300	 l-p:0.107322558760643
epoch£º1265	 i:1 	 global-step:25301	 l-p:0.13855521380901337
epoch£º1265	 i:2 	 global-step:25302	 l-p:0.1436823159456253
epoch£º1265	 i:3 	 global-step:25303	 l-p:0.1567779928445816
epoch£º1265	 i:4 	 global-step:25304	 l-p:0.12541942298412323
epoch£º1265	 i:5 	 global-step:25305	 l-p:0.11661281436681747
epoch£º1265	 i:6 	 global-step:25306	 l-p:0.21270030736923218
epoch£º1265	 i:7 	 global-step:25307	 l-p:0.19422942399978638
epoch£º1265	 i:8 	 global-step:25308	 l-p:0.11811169981956482
epoch£º1265	 i:9 	 global-step:25309	 l-p:0.10154030472040176
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1266
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.1995,  0.1166,  1.0000,  0.0681,
          1.0000,  0.5843, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1403,  0.0729,  1.0000,  0.0379,
          1.0000,  0.5196, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.1846,  0.1051,  1.0000,  0.0598,
          1.0000,  0.5694, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2321,  0.1426,  1.0000,  0.0876,
          1.0000,  0.6145, 31.6228]], device='cuda:0')
 pt:tensor([[5.1670, 4.9389, 4.9553],
        [5.1670, 5.0115, 5.0708],
        [5.1670, 4.9553, 4.9869],
        [5.1670, 4.9084, 4.8846]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1266, step:0 
model_pd.l_p.mean(): 0.204181507229805 
model_pd.l_d.mean(): -19.705583572387695 
model_pd.lagr.mean(): -19.501401901245117 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4745], device='cuda:0')), ('power', tensor([-20.5655], device='cuda:0'))])
epoch£º1266	 i:0 	 global-step:25320	 l-p:0.204181507229805
epoch£º1266	 i:1 	 global-step:25321	 l-p:0.15805360674858093
epoch£º1266	 i:2 	 global-step:25322	 l-p:0.1311027705669403
epoch£º1266	 i:3 	 global-step:25323	 l-p:0.13883797824382782
epoch£º1266	 i:4 	 global-step:25324	 l-p:0.10680438578128815
epoch£º1266	 i:5 	 global-step:25325	 l-p:0.19360633194446564
epoch£º1266	 i:6 	 global-step:25326	 l-p:0.10395684093236923
epoch£º1266	 i:7 	 global-step:25327	 l-p:0.08965533971786499
epoch£º1266	 i:8 	 global-step:25328	 l-p:0.13018950819969177
epoch£º1266	 i:9 	 global-step:25329	 l-p:0.16345080733299255
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1267
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0595e-02, 5.6452e-03,
         1.0000e+00, 1.5474e-03, 1.0000e+00, 2.7411e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.6019e-06, 1.4947e-07,
         1.0000e+00, 2.9390e-09, 1.0000e+00, 1.9663e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1664, 4.8915, 4.8369],
        [5.1664, 5.1659, 5.1664],
        [5.1664, 5.1600, 5.1661],
        [5.1664, 5.1664, 5.1664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1267, step:0 
model_pd.l_p.mean(): 0.13599893450737 
model_pd.l_d.mean(): -20.64077377319336 
model_pd.lagr.mean(): -20.50477409362793 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4233], device='cuda:0')), ('power', tensor([-21.4651], device='cuda:0'))])
epoch£º1267	 i:0 	 global-step:25340	 l-p:0.13599893450737
epoch£º1267	 i:1 	 global-step:25341	 l-p:0.1230136975646019
epoch£º1267	 i:2 	 global-step:25342	 l-p:0.1253519356250763
epoch£º1267	 i:3 	 global-step:25343	 l-p:0.10024968534708023
epoch£º1267	 i:4 	 global-step:25344	 l-p:0.16783061623573303
epoch£º1267	 i:5 	 global-step:25345	 l-p:0.17864970862865448
epoch£º1267	 i:6 	 global-step:25346	 l-p:0.15573112666606903
epoch£º1267	 i:7 	 global-step:25347	 l-p:0.13192515075206757
epoch£º1267	 i:8 	 global-step:25348	 l-p:0.18137788772583008
epoch£º1267	 i:9 	 global-step:25349	 l-p:0.1328909695148468
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1268
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0324e-02, 2.2481e-03,
         1.0000e+00, 4.8953e-04, 1.0000e+00, 2.1775e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7425e-01, 9.7324e-02,
         1.0000e+00, 5.4360e-02, 1.0000e+00, 5.5854e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.9614e-07, 8.6398e-09,
         1.0000e+00, 8.3297e-11, 1.0000e+00, 9.6411e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3019e-01, 1.4108e-01,
         1.0000e+00, 8.6461e-02, 1.0000e+00, 6.1286e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1641, 5.1623, 5.1640],
        [5.1641, 4.9644, 5.0050],
        [5.1641, 5.1641, 5.1641],
        [5.1641, 4.9068, 4.8856]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1268, step:0 
model_pd.l_p.mean(): 0.13371199369430542 
model_pd.l_d.mean(): -19.39861488342285 
model_pd.lagr.mean(): -19.264902114868164 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4700], device='cuda:0')), ('power', tensor([-20.2481], device='cuda:0'))])
epoch£º1268	 i:0 	 global-step:25360	 l-p:0.13371199369430542
epoch£º1268	 i:1 	 global-step:25361	 l-p:0.12659114599227905
epoch£º1268	 i:2 	 global-step:25362	 l-p:0.15475371479988098
epoch£º1268	 i:3 	 global-step:25363	 l-p:0.16893601417541504
epoch£º1268	 i:4 	 global-step:25364	 l-p:0.12981949746608734
epoch£º1268	 i:5 	 global-step:25365	 l-p:0.12766186892986298
epoch£º1268	 i:6 	 global-step:25366	 l-p:0.19984221458435059
epoch£º1268	 i:7 	 global-step:25367	 l-p:0.09038020670413971
epoch£º1268	 i:8 	 global-step:25368	 l-p:0.13279235363006592
epoch£º1268	 i:9 	 global-step:25369	 l-p:0.1660405695438385
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1269
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2880e-02, 6.4955e-03,
         1.0000e+00, 1.8440e-03, 1.0000e+00, 2.8389e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7702e-05, 4.6133e-07,
         1.0000e+00, 1.2023e-08, 1.0000e+00, 2.6062e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6570e-03, 1.9607e-04,
         1.0000e+00, 2.3201e-05, 1.0000e+00, 1.1833e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1664, 4.9857, 5.0366],
        [5.1664, 5.1585, 5.1659],
        [5.1664, 5.1664, 5.1664],
        [5.1664, 5.1664, 5.1664]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1269, step:0 
model_pd.l_p.mean(): 0.0696844756603241 
model_pd.l_d.mean(): -20.12535285949707 
model_pd.lagr.mean(): -20.055667877197266 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4684], device='cuda:0')), ('power', tensor([-20.9868], device='cuda:0'))])
epoch£º1269	 i:0 	 global-step:25380	 l-p:0.0696844756603241
epoch£º1269	 i:1 	 global-step:25381	 l-p:0.17048659920692444
epoch£º1269	 i:2 	 global-step:25382	 l-p:0.12796196341514587
epoch£º1269	 i:3 	 global-step:25383	 l-p:0.17603832483291626
epoch£º1269	 i:4 	 global-step:25384	 l-p:0.11658371984958649
epoch£º1269	 i:5 	 global-step:25385	 l-p:0.16502004861831665
epoch£º1269	 i:6 	 global-step:25386	 l-p:0.15729188919067383
epoch£º1269	 i:7 	 global-step:25387	 l-p:0.1434188038110733
epoch£º1269	 i:8 	 global-step:25388	 l-p:0.12833905220031738
epoch£º1269	 i:9 	 global-step:25389	 l-p:0.1528811752796173
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1270
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8147e-01, 7.1981e-01,
         1.0000e+00, 6.6301e-01, 1.0000e+00, 9.2109e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.5558e-03, 1.7499e-03,
         1.0000e+00, 3.5790e-04, 1.0000e+00, 2.0453e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6120e-01, 2.5723e-01,
         1.0000e+00, 1.8319e-01, 1.0000e+00, 7.1217e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9512e-01, 2.8994e-01,
         1.0000e+00, 2.1275e-01, 1.0000e+00, 7.3380e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1757, 5.2862, 5.0234],
        [5.1757, 5.1745, 5.1757],
        [5.1757, 4.8724, 4.6638],
        [5.1757, 4.8793, 4.6301]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1270, step:0 
model_pd.l_p.mean(): 0.1297023594379425 
model_pd.l_d.mean(): -20.332683563232422 
model_pd.lagr.mean(): -20.20298194885254 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4521], device='cuda:0')), ('power', tensor([-21.1811], device='cuda:0'))])
epoch£º1270	 i:0 	 global-step:25400	 l-p:0.1297023594379425
epoch£º1270	 i:1 	 global-step:25401	 l-p:0.13808007538318634
epoch£º1270	 i:2 	 global-step:25402	 l-p:0.047928888350725174
epoch£º1270	 i:3 	 global-step:25403	 l-p:0.18785083293914795
epoch£º1270	 i:4 	 global-step:25404	 l-p:0.1611059606075287
epoch£º1270	 i:5 	 global-step:25405	 l-p:0.1465597152709961
epoch£º1270	 i:6 	 global-step:25406	 l-p:0.1494074910879135
epoch£º1270	 i:7 	 global-step:25407	 l-p:0.14124463498592377
epoch£º1270	 i:8 	 global-step:25408	 l-p:0.14451798796653748
epoch£º1270	 i:9 	 global-step:25409	 l-p:0.12392188608646393
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1271
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.0089e-01, 6.2259e-01,
         1.0000e+00, 5.5304e-01, 1.0000e+00, 8.8828e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4718e-01, 4.4754e-01,
         1.0000e+00, 3.6605e-01, 1.0000e+00, 8.1792e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.7150e-02, 2.7294e-02,
         1.0000e+00, 1.1094e-02, 1.0000e+00, 4.0646e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0864e-01, 2.0858e-01,
         1.0000e+00, 1.4096e-01, 1.0000e+00, 6.7580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1823, 5.1777, 4.8633],
        [5.1823, 4.9906, 4.6449],
        [5.1823, 5.1292, 5.1691],
        [5.1823, 4.8837, 4.7479]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1271, step:0 
model_pd.l_p.mean(): 0.13151580095291138 
model_pd.l_d.mean(): -19.27521514892578 
model_pd.lagr.mean(): -19.143699645996094 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5105], device='cuda:0')), ('power', tensor([-20.1643], device='cuda:0'))])
epoch£º1271	 i:0 	 global-step:25420	 l-p:0.13151580095291138
epoch£º1271	 i:1 	 global-step:25421	 l-p:0.15653479099273682
epoch£º1271	 i:2 	 global-step:25422	 l-p:0.14934322237968445
epoch£º1271	 i:3 	 global-step:25423	 l-p:0.12019093334674835
epoch£º1271	 i:4 	 global-step:25424	 l-p:0.1374620646238327
epoch£º1271	 i:5 	 global-step:25425	 l-p:0.14794188737869263
epoch£º1271	 i:6 	 global-step:25426	 l-p:0.1873181164264679
epoch£º1271	 i:7 	 global-step:25427	 l-p:0.12134161591529846
epoch£º1271	 i:8 	 global-step:25428	 l-p:0.12391707301139832
epoch£º1271	 i:9 	 global-step:25429	 l-p:0.08865935355424881
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1272
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.5393,  0.4390,  1.0000,  0.3573,
          1.0000,  0.8140, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3907,  0.2856,  1.0000,  0.2088,
          1.0000,  0.7311, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.4429,  0.3376,  1.0000,  0.2574,
          1.0000,  0.7623, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.8496,  0.8047,  1.0000,  0.7622,
          1.0000,  0.9471, 31.6228]], device='cuda:0')
 pt:tensor([[5.1797, 4.9797, 4.6354],
        [5.1797, 4.8825, 4.6383],
        [5.1797, 4.9046, 4.6098],
        [5.1797, 5.3961, 5.1905]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1272, step:0 
model_pd.l_p.mean(): 0.13519248366355896 
model_pd.l_d.mean(): -19.90789222717285 
model_pd.lagr.mean(): -19.7726993560791 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5263], device='cuda:0')), ('power', tensor([-20.8253], device='cuda:0'))])
epoch£º1272	 i:0 	 global-step:25440	 l-p:0.13519248366355896
epoch£º1272	 i:1 	 global-step:25441	 l-p:0.13111257553100586
epoch£º1272	 i:2 	 global-step:25442	 l-p:0.12176450341939926
epoch£º1272	 i:3 	 global-step:25443	 l-p:0.13360005617141724
epoch£º1272	 i:4 	 global-step:25444	 l-p:0.10008563101291656
epoch£º1272	 i:5 	 global-step:25445	 l-p:0.1426861584186554
epoch£º1272	 i:6 	 global-step:25446	 l-p:0.1444980800151825
epoch£º1272	 i:7 	 global-step:25447	 l-p:0.13975390791893005
epoch£º1272	 i:8 	 global-step:25448	 l-p:0.1325162649154663
epoch£º1272	 i:9 	 global-step:25449	 l-p:0.17680633068084717
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1273
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5907e-01, 2.5522e-01,
         1.0000e+00, 1.8140e-01, 1.0000e+00, 7.1077e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.9335e-02, 2.8484e-02,
         1.0000e+00, 1.1702e-02, 1.0000e+00, 4.1082e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.7213e-03, 7.9205e-04,
         1.0000e+00, 1.3287e-04, 1.0000e+00, 1.6776e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7410e-02, 4.5121e-03,
         1.0000e+00, 1.1694e-03, 1.0000e+00, 2.5918e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1842, 4.8816, 4.6757],
        [5.1842, 5.1283, 5.1698],
        [5.1842, 5.1838, 5.1842],
        [5.1842, 5.1795, 5.1840]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1273, step:0 
model_pd.l_p.mean(): 0.15622195601463318 
model_pd.l_d.mean(): -19.72416877746582 
model_pd.lagr.mean(): -19.567947387695312 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5368], device='cuda:0')), ('power', tensor([-20.6490], device='cuda:0'))])
epoch£º1273	 i:0 	 global-step:25460	 l-p:0.15622195601463318
epoch£º1273	 i:1 	 global-step:25461	 l-p:0.19125168025493622
epoch£º1273	 i:2 	 global-step:25462	 l-p:0.08436639606952667
epoch£º1273	 i:3 	 global-step:25463	 l-p:0.14683911204338074
epoch£º1273	 i:4 	 global-step:25464	 l-p:0.11340987682342529
epoch£º1273	 i:5 	 global-step:25465	 l-p:0.099393330514431
epoch£º1273	 i:6 	 global-step:25466	 l-p:0.17596033215522766
epoch£º1273	 i:7 	 global-step:25467	 l-p:0.12546736001968384
epoch£º1273	 i:8 	 global-step:25468	 l-p:0.10833954811096191
epoch£º1273	 i:9 	 global-step:25469	 l-p:0.14320681989192963
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1274
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7052e-04, 9.4560e-06,
         1.0000e+00, 5.2436e-07, 1.0000e+00, 5.5453e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.5922e-01, 8.6297e-02,
         1.0000e+00, 4.6773e-02, 1.0000e+00, 5.4200e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8523e-01, 1.0559e-01,
         1.0000e+00, 6.0188e-02, 1.0000e+00, 5.7004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1823e-02, 2.6934e-03,
         1.0000e+00, 6.1359e-04, 1.0000e+00, 2.2781e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1852, 5.1853, 5.1853],
        [5.1852, 5.0051, 5.0557],
        [5.1852, 4.9736, 5.0042],
        [5.1852, 5.1830, 5.1852]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1274, step:0 
model_pd.l_p.mean(): 0.14875419437885284 
model_pd.l_d.mean(): -19.89466094970703 
model_pd.lagr.mean(): -19.745906829833984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4612], device='cuda:0')), ('power', tensor([-20.7443], device='cuda:0'))])
epoch£º1274	 i:0 	 global-step:25480	 l-p:0.14875419437885284
epoch£º1274	 i:1 	 global-step:25481	 l-p:0.12704187631607056
epoch£º1274	 i:2 	 global-step:25482	 l-p:0.14338141679763794
epoch£º1274	 i:3 	 global-step:25483	 l-p:0.10850220173597336
epoch£º1274	 i:4 	 global-step:25484	 l-p:0.17429709434509277
epoch£º1274	 i:5 	 global-step:25485	 l-p:0.12543612718582153
epoch£º1274	 i:6 	 global-step:25486	 l-p:0.12013561278581619
epoch£º1274	 i:7 	 global-step:25487	 l-p:0.08002737164497375
epoch£º1274	 i:8 	 global-step:25488	 l-p:0.1980876922607422
epoch£º1274	 i:9 	 global-step:25489	 l-p:0.13905565440654755
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1275
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2135e-01, 6.0082e-02,
         1.0000e+00, 2.9746e-02, 1.0000e+00, 4.9509e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1218e-02, 2.5112e-03,
         1.0000e+00, 5.6215e-04, 1.0000e+00, 2.2386e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5448e-03, 1.2242e-03,
         1.0000e+00, 2.2899e-04, 1.0000e+00, 1.8705e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.3563e-01, 9.1510e-01,
         1.0000e+00, 8.9503e-01, 1.0000e+00, 9.7807e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1764, 5.0476, 5.1096],
        [5.1764, 5.1744, 5.1764],
        [5.1764, 5.1757, 5.1764],
        [5.1764, 5.5283, 5.4054]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1275, step:0 
model_pd.l_p.mean(): 0.09553583711385727 
model_pd.l_d.mean(): -20.0814208984375 
model_pd.lagr.mean(): -19.985885620117188 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4891], device='cuda:0')), ('power', tensor([-20.9635], device='cuda:0'))])
epoch£º1275	 i:0 	 global-step:25500	 l-p:0.09553583711385727
epoch£º1275	 i:1 	 global-step:25501	 l-p:0.14655975997447968
epoch£º1275	 i:2 	 global-step:25502	 l-p:0.10151393711566925
epoch£º1275	 i:3 	 global-step:25503	 l-p:0.13655459880828857
epoch£º1275	 i:4 	 global-step:25504	 l-p:0.12058522552251816
epoch£º1275	 i:5 	 global-step:25505	 l-p:0.13654306530952454
epoch£º1275	 i:6 	 global-step:25506	 l-p:0.1694411039352417
epoch£º1275	 i:7 	 global-step:25507	 l-p:0.15520763397216797
epoch£º1275	 i:8 	 global-step:25508	 l-p:0.13055787980556488
epoch£º1275	 i:9 	 global-step:25509	 l-p:0.19902876019477844
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1276
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.3998e-03, 9.4733e-04,
         1.0000e+00, 1.6620e-04, 1.0000e+00, 1.7544e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.8371e-01, 4.8782e-01,
         1.0000e+00, 4.0769e-01, 1.0000e+00, 8.3573e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1004e-01, 2.0984e-01,
         1.0000e+00, 1.4202e-01, 1.0000e+00, 6.7682e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1723, 5.1718, 5.1723],
        [5.1723, 4.9751, 4.6292],
        [5.1723, 5.0169, 4.6671],
        [5.1723, 4.8722, 4.7344]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1276, step:0 
model_pd.l_p.mean(): 0.1205054223537445 
model_pd.l_d.mean(): -20.35577964782715 
model_pd.lagr.mean(): -20.235273361206055 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4688], device='cuda:0')), ('power', tensor([-21.2220], device='cuda:0'))])
epoch£º1276	 i:0 	 global-step:25520	 l-p:0.1205054223537445
epoch£º1276	 i:1 	 global-step:25521	 l-p:0.1420128345489502
epoch£º1276	 i:2 	 global-step:25522	 l-p:0.14756925404071808
epoch£º1276	 i:3 	 global-step:25523	 l-p:0.17506954073905945
epoch£º1276	 i:4 	 global-step:25524	 l-p:0.13085687160491943
epoch£º1276	 i:5 	 global-step:25525	 l-p:0.12380079925060272
epoch£º1276	 i:6 	 global-step:25526	 l-p:0.12138976156711578
epoch£º1276	 i:7 	 global-step:25527	 l-p:0.11756455898284912
epoch£º1276	 i:8 	 global-step:25528	 l-p:0.12554702162742615
epoch£º1276	 i:9 	 global-step:25529	 l-p:0.19099368155002594
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1277
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.9030e-01, 3.8662e-01,
         1.0000e+00, 3.0486e-01, 1.0000e+00, 7.8853e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5907e-03, 2.0377e-03,
         1.0000e+00, 4.3293e-04, 1.0000e+00, 2.1246e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.7961e-01, 8.4279e-01,
         1.0000e+00, 8.0751e-01, 1.0000e+00, 9.5814e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.6165e-03, 9.9836e-04,
         1.0000e+00, 1.7746e-04, 1.0000e+00, 1.7775e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1732, 4.9287, 4.6021],
        [5.1732, 5.1717, 5.1731],
        [5.1732, 5.4344, 5.2551],
        [5.1732, 5.1727, 5.1732]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1277, step:0 
model_pd.l_p.mean(): 0.18396398425102234 
model_pd.l_d.mean(): -20.236621856689453 
model_pd.lagr.mean(): -20.052658081054688 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5074], device='cuda:0')), ('power', tensor([-21.1406], device='cuda:0'))])
epoch£º1277	 i:0 	 global-step:25540	 l-p:0.18396398425102234
epoch£º1277	 i:1 	 global-step:25541	 l-p:0.11885229498147964
epoch£º1277	 i:2 	 global-step:25542	 l-p:0.14411771297454834
epoch£º1277	 i:3 	 global-step:25543	 l-p:0.11999063193798065
epoch£º1277	 i:4 	 global-step:25544	 l-p:0.15208445489406586
epoch£º1277	 i:5 	 global-step:25545	 l-p:0.11105848103761673
epoch£º1277	 i:6 	 global-step:25546	 l-p:0.12397067993879318
epoch£º1277	 i:7 	 global-step:25547	 l-p:0.11105521768331528
epoch£º1277	 i:8 	 global-step:25548	 l-p:0.12607067823410034
epoch£º1277	 i:9 	 global-step:25549	 l-p:0.19404758512973785
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1278
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4009e-04, 9.2093e-05,
         1.0000e+00, 9.0216e-06, 1.0000e+00, 9.7962e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.1076e-01, 6.3430e-01,
         1.0000e+00, 5.6607e-01, 1.0000e+00, 8.9243e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.3359e-01, 5.4418e-01,
         1.0000e+00, 4.6739e-01, 1.0000e+00, 8.5888e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2205e-02, 1.0246e-02,
         1.0000e+00, 3.2598e-03, 1.0000e+00, 3.1816e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1768, 5.1768, 5.1768],
        [5.1768, 5.1842, 4.8743],
        [5.1768, 5.0814, 4.7389],
        [5.1768, 5.1619, 5.1753]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1278, step:0 
model_pd.l_p.mean(): 0.1423121988773346 
model_pd.l_d.mean(): -20.3978271484375 
model_pd.lagr.mean(): -20.25551414489746 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4371], device='cuda:0')), ('power', tensor([-21.2319], device='cuda:0'))])
epoch£º1278	 i:0 	 global-step:25560	 l-p:0.1423121988773346
epoch£º1278	 i:1 	 global-step:25561	 l-p:0.16056224703788757
epoch£º1278	 i:2 	 global-step:25562	 l-p:0.11355172842741013
epoch£º1278	 i:3 	 global-step:25563	 l-p:0.16648666560649872
epoch£º1278	 i:4 	 global-step:25564	 l-p:0.13298487663269043
epoch£º1278	 i:5 	 global-step:25565	 l-p:0.15899889171123505
epoch£º1278	 i:6 	 global-step:25566	 l-p:0.11991821974515915
epoch£º1278	 i:7 	 global-step:25567	 l-p:0.09424115717411041
epoch£º1278	 i:8 	 global-step:25568	 l-p:0.15078423917293549
epoch£º1278	 i:9 	 global-step:25569	 l-p:0.13290223479270935
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1279
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4651e-01, 4.4682e-01,
         1.0000e+00, 3.6531e-01, 1.0000e+00, 8.1759e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7806e-03, 2.1582e-04,
         1.0000e+00, 2.6159e-05, 1.0000e+00, 1.2121e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1788, 5.0849, 5.1419],
        [5.1788, 5.1786, 5.1788],
        [5.1788, 4.9854, 4.6394],
        [5.1788, 5.1788, 5.1788]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1279, step:0 
model_pd.l_p.mean(): 0.1347409337759018 
model_pd.l_d.mean(): -17.93573760986328 
model_pd.lagr.mean(): -17.800996780395508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.6612], device='cuda:0')), ('power', tensor([-18.9560], device='cuda:0'))])
epoch£º1279	 i:0 	 global-step:25580	 l-p:0.1347409337759018
epoch£º1279	 i:1 	 global-step:25581	 l-p:0.1306237429380417
epoch£º1279	 i:2 	 global-step:25582	 l-p:0.15203094482421875
epoch£º1279	 i:3 	 global-step:25583	 l-p:0.12028767168521881
epoch£º1279	 i:4 	 global-step:25584	 l-p:0.17366279661655426
epoch£º1279	 i:5 	 global-step:25585	 l-p:0.10692527890205383
epoch£º1279	 i:6 	 global-step:25586	 l-p:0.15616475045681
epoch£º1279	 i:7 	 global-step:25587	 l-p:0.1297740489244461
epoch£º1279	 i:8 	 global-step:25588	 l-p:0.14175620675086975
epoch£º1279	 i:9 	 global-step:25589	 l-p:0.12791553139686584
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1280
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.6828e-01, 2.6398e-01,
         1.0000e+00, 1.8922e-01, 1.0000e+00, 7.1679e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4032e-01, 7.2916e-02,
         1.0000e+00, 3.7891e-02, 1.0000e+00, 5.1964e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3388e-02, 3.1790e-03,
         1.0000e+00, 7.5485e-04, 1.0000e+00, 2.3745e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.0166e-02, 2.2024e-03,
         1.0000e+00, 4.7711e-04, 1.0000e+00, 2.1663e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1761, 4.8733, 4.6556],
        [5.1761, 5.0207, 5.0799],
        [5.1761, 5.1732, 5.1760],
        [5.1761, 5.1744, 5.1760]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1280, step:0 
model_pd.l_p.mean(): 0.14643679559230804 
model_pd.l_d.mean(): -19.60837173461914 
model_pd.lagr.mean(): -19.46193504333496 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4998], device='cuda:0')), ('power', tensor([-20.4927], device='cuda:0'))])
epoch£º1280	 i:0 	 global-step:25600	 l-p:0.14643679559230804
epoch£º1280	 i:1 	 global-step:25601	 l-p:0.14260978996753693
epoch£º1280	 i:2 	 global-step:25602	 l-p:0.13354729115962982
epoch£º1280	 i:3 	 global-step:25603	 l-p:0.14169542491436005
epoch£º1280	 i:4 	 global-step:25604	 l-p:0.12984395027160645
epoch£º1280	 i:5 	 global-step:25605	 l-p:0.12125474959611893
epoch£º1280	 i:6 	 global-step:25606	 l-p:0.15529179573059082
epoch£º1280	 i:7 	 global-step:25607	 l-p:0.1286781132221222
epoch£º1280	 i:8 	 global-step:25608	 l-p:0.14905568957328796
epoch£º1280	 i:9 	 global-step:25609	 l-p:0.1395077258348465
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1281
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.9563e-02, 1.3481e-02,
         1.0000e+00, 4.5935e-03, 1.0000e+00, 3.4074e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5400e-01, 1.6086e-01,
         1.0000e+00, 1.0187e-01, 1.0000e+00, 6.3330e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2735e-01, 6.4070e-02,
         1.0000e+00, 3.2234e-02, 1.0000e+00, 5.0311e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1728, 5.1513, 5.1700],
        [5.1728, 4.8980, 4.8433],
        [5.1728, 5.0353, 5.0971],
        [5.1728, 4.8868, 4.8040]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1281, step:0 
model_pd.l_p.mean(): 0.08952436596155167 
model_pd.l_d.mean(): -20.41695785522461 
model_pd.lagr.mean(): -20.32743263244629 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4344], device='cuda:0')), ('power', tensor([-21.2487], device='cuda:0'))])
epoch£º1281	 i:0 	 global-step:25620	 l-p:0.08952436596155167
epoch£º1281	 i:1 	 global-step:25621	 l-p:0.17594291269779205
epoch£º1281	 i:2 	 global-step:25622	 l-p:0.14264759421348572
epoch£º1281	 i:3 	 global-step:25623	 l-p:0.1607106328010559
epoch£º1281	 i:4 	 global-step:25624	 l-p:0.1677558422088623
epoch£º1281	 i:5 	 global-step:25625	 l-p:0.12740859389305115
epoch£º1281	 i:6 	 global-step:25626	 l-p:0.17135117948055267
epoch£º1281	 i:7 	 global-step:25627	 l-p:0.12353921681642532
epoch£º1281	 i:8 	 global-step:25628	 l-p:0.09266804158687592
epoch£º1281	 i:9 	 global-step:25629	 l-p:0.13244366645812988
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1282
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3912e-03, 3.1975e-04,
         1.0000e+00, 4.2758e-05, 1.0000e+00, 1.3372e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1984e-02, 2.7424e-03,
         1.0000e+00, 6.2758e-04, 1.0000e+00, 2.2884e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1758e-01, 1.3087e-01,
         1.0000e+00, 7.8713e-02, 1.0000e+00, 6.0146e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0045e-01, 5.0656e-01,
         1.0000e+00, 4.2736e-01, 1.0000e+00, 8.4364e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1790, 5.1789, 5.1790],
        [5.1790, 5.1767, 5.1789],
        [5.1790, 4.9334, 4.9284],
        [5.1790, 5.0439, 4.6954]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1282, step:0 
model_pd.l_p.mean(): 0.10548467189073563 
model_pd.l_d.mean(): -20.227657318115234 
model_pd.lagr.mean(): -20.122173309326172 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4695], device='cuda:0')), ('power', tensor([-21.0922], device='cuda:0'))])
epoch£º1282	 i:0 	 global-step:25640	 l-p:0.10548467189073563
epoch£º1282	 i:1 	 global-step:25641	 l-p:0.08011645823717117
epoch£º1282	 i:2 	 global-step:25642	 l-p:0.15053251385688782
epoch£º1282	 i:3 	 global-step:25643	 l-p:0.1598336100578308
epoch£º1282	 i:4 	 global-step:25644	 l-p:0.11023379117250443
epoch£º1282	 i:5 	 global-step:25645	 l-p:0.1357877552509308
epoch£º1282	 i:6 	 global-step:25646	 l-p:0.09874249249696732
epoch£º1282	 i:7 	 global-step:25647	 l-p:0.19020415842533112
epoch£º1282	 i:8 	 global-step:25648	 l-p:0.2075258195400238
epoch£º1282	 i:9 	 global-step:25649	 l-p:0.14009471237659454
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1283
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1014e-01, 2.0993e-01,
         1.0000e+00, 1.4210e-01, 1.0000e+00, 6.7689e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5725e-03, 1.2311e-03,
         1.0000e+00, 2.3061e-04, 1.0000e+00, 1.8732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.2735e-04, 1.3876e-05,
         1.0000e+00, 8.4688e-07, 1.0000e+00, 6.1033e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1763, 4.8765, 4.7385],
        [5.1763, 5.1756, 5.1763],
        [5.1763, 5.1740, 5.1763],
        [5.1763, 5.1763, 5.1763]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1283, step:0 
model_pd.l_p.mean(): 0.08051300793886185 
model_pd.l_d.mean(): -20.07121467590332 
model_pd.lagr.mean(): -19.99070167541504 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4748], device='cuda:0')), ('power', tensor([-20.9383], device='cuda:0'))])
epoch£º1283	 i:0 	 global-step:25660	 l-p:0.08051300793886185
epoch£º1283	 i:1 	 global-step:25661	 l-p:0.13781990110874176
epoch£º1283	 i:2 	 global-step:25662	 l-p:0.16358637809753418
epoch£º1283	 i:3 	 global-step:25663	 l-p:0.14647184312343597
epoch£º1283	 i:4 	 global-step:25664	 l-p:0.18378011882305145
epoch£º1283	 i:5 	 global-step:25665	 l-p:0.0837559849023819
epoch£º1283	 i:6 	 global-step:25666	 l-p:0.12281784415245056
epoch£º1283	 i:7 	 global-step:25667	 l-p:0.15360040962696075
epoch£º1283	 i:8 	 global-step:25668	 l-p:0.1507391333580017
epoch£º1283	 i:9 	 global-step:25669	 l-p:0.1498778909444809
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1284
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5110e-01, 6.8275e-01,
         1.0000e+00, 6.2062e-01, 1.0000e+00, 9.0900e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3831e-02, 3.3199e-03,
         1.0000e+00, 7.9690e-04, 1.0000e+00, 2.4004e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.0608e-02, 4.0696e-02,
         1.0000e+00, 1.8279e-02, 1.0000e+00, 4.4915e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6065e-03, 1.8815e-04,
         1.0000e+00, 2.2036e-05, 1.0000e+00, 1.1712e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1809, 5.2473, 4.9628],
        [5.1809, 5.1779, 5.1808],
        [5.1809, 5.0960, 5.1502],
        [5.1809, 5.1809, 5.1809]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1284, step:0 
model_pd.l_p.mean(): 0.1282465010881424 
model_pd.l_d.mean(): -20.65515899658203 
model_pd.lagr.mean(): -20.526912689208984 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4161], device='cuda:0')), ('power', tensor([-21.4724], device='cuda:0'))])
epoch£º1284	 i:0 	 global-step:25680	 l-p:0.1282465010881424
epoch£º1284	 i:1 	 global-step:25681	 l-p:0.1512986570596695
epoch£º1284	 i:2 	 global-step:25682	 l-p:0.12518344819545746
epoch£º1284	 i:3 	 global-step:25683	 l-p:0.12529835104942322
epoch£º1284	 i:4 	 global-step:25684	 l-p:0.14828746020793915
epoch£º1284	 i:5 	 global-step:25685	 l-p:0.1927998661994934
epoch£º1284	 i:6 	 global-step:25686	 l-p:0.1352236270904541
epoch£º1284	 i:7 	 global-step:25687	 l-p:0.1409541815519333
epoch£º1284	 i:8 	 global-step:25688	 l-p:0.09475389868021011
epoch£º1284	 i:9 	 global-step:25689	 l-p:0.1298249214887619
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1285
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2452e-01, 4.2301e-01,
         1.0000e+00, 3.4114e-01, 1.0000e+00, 8.0647e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.8986e-02, 5.0649e-03,
         1.0000e+00, 1.3512e-03, 1.0000e+00, 2.6677e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.5015e-01, 1.5761e-01,
         1.0000e+00, 9.9309e-02, 1.0000e+00, 6.3008e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.1244e-01, 5.2010e-01,
         1.0000e+00, 4.4168e-01, 1.0000e+00, 8.4922e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1769, 4.9619, 4.6211],
        [5.1769, 5.1713, 5.1766],
        [5.1769, 4.9049, 4.8558],
        [5.1769, 5.0554, 4.7083]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1285, step:0 
model_pd.l_p.mean(): 0.20907439291477203 
model_pd.l_d.mean(): -20.520029067993164 
model_pd.lagr.mean(): -20.310955047607422 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4410], device='cuda:0')), ('power', tensor([-21.3605], device='cuda:0'))])
epoch£º1285	 i:0 	 global-step:25700	 l-p:0.20907439291477203
epoch£º1285	 i:1 	 global-step:25701	 l-p:0.1619464010000229
epoch£º1285	 i:2 	 global-step:25702	 l-p:0.11205211281776428
epoch£º1285	 i:3 	 global-step:25703	 l-p:0.09821578860282898
epoch£º1285	 i:4 	 global-step:25704	 l-p:0.12505929172039032
epoch£º1285	 i:5 	 global-step:25705	 l-p:0.13430339097976685
epoch£º1285	 i:6 	 global-step:25706	 l-p:0.10277176648378372
epoch£º1285	 i:7 	 global-step:25707	 l-p:0.16442188620567322
epoch£º1285	 i:8 	 global-step:25708	 l-p:0.14101430773735046
epoch£º1285	 i:9 	 global-step:25709	 l-p:0.13515301048755646
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1286
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7310e-01, 1.7718e-01,
         1.0000e+00, 1.1495e-01, 1.0000e+00, 6.4879e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0110e-02, 2.3547e-02,
         1.0000e+00, 9.2238e-03, 1.0000e+00, 3.9173e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1603e-01, 8.8964e-01,
         1.0000e+00, 8.6401e-01, 1.0000e+00, 9.7119e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9350e-01, 7.3462e-01,
         1.0000e+00, 6.8010e-01, 1.0000e+00, 9.2580e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1730, 4.8869, 4.8041],
        [5.1730, 5.1287, 5.1634],
        [5.1730, 5.4918, 5.3478],
        [5.1730, 5.3002, 5.0457]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1286, step:0 
model_pd.l_p.mean(): 0.12092042714357376 
model_pd.l_d.mean(): -19.41398811340332 
model_pd.lagr.mean(): -19.293067932128906 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5144], device='cuda:0')), ('power', tensor([-20.3098], device='cuda:0'))])
epoch£º1286	 i:0 	 global-step:25720	 l-p:0.12092042714357376
epoch£º1286	 i:1 	 global-step:25721	 l-p:0.1296466737985611
epoch£º1286	 i:2 	 global-step:25722	 l-p:0.11964240670204163
epoch£º1286	 i:3 	 global-step:25723	 l-p:0.193806990981102
epoch£º1286	 i:4 	 global-step:25724	 l-p:0.16093845665454865
epoch£º1286	 i:5 	 global-step:25725	 l-p:0.18623530864715576
epoch£º1286	 i:6 	 global-step:25726	 l-p:0.14391084015369415
epoch£º1286	 i:7 	 global-step:25727	 l-p:0.11837510019540787
epoch£º1286	 i:8 	 global-step:25728	 l-p:0.12316657602787018
epoch£º1286	 i:9 	 global-step:25729	 l-p:0.09674569964408875
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1287
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5352e-01, 5.6713e-01,
         1.0000e+00, 4.9215e-01, 1.0000e+00, 8.6780e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2931e-01, 2.2741e-01,
         1.0000e+00, 1.5704e-01, 1.0000e+00, 6.9056e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4320e-03, 1.6141e-04,
         1.0000e+00, 1.8194e-05, 1.0000e+00, 1.1272e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.2657e-05, 3.0318e-06,
         1.0000e+00, 1.2651e-07, 1.0000e+00, 4.1728e-02, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1746, 5.1037, 4.7669],
        [5.1746, 4.8709, 4.7053],
        [5.1746, 5.1745, 5.1746],
        [5.1746, 5.1746, 5.1746]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1287, step:0 
model_pd.l_p.mean(): 0.12576140463352203 
model_pd.l_d.mean(): -20.272289276123047 
model_pd.lagr.mean(): -20.146528244018555 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4926], device='cuda:0')), ('power', tensor([-21.1615], device='cuda:0'))])
epoch£º1287	 i:0 	 global-step:25740	 l-p:0.12576140463352203
epoch£º1287	 i:1 	 global-step:25741	 l-p:0.11666608601808548
epoch£º1287	 i:2 	 global-step:25742	 l-p:0.1513533741235733
epoch£º1287	 i:3 	 global-step:25743	 l-p:0.14297831058502197
epoch£º1287	 i:4 	 global-step:25744	 l-p:0.1345517784357071
epoch£º1287	 i:5 	 global-step:25745	 l-p:0.12279417365789413
epoch£º1287	 i:6 	 global-step:25746	 l-p:0.11208365857601166
epoch£º1287	 i:7 	 global-step:25747	 l-p:0.12263394892215729
epoch£º1287	 i:8 	 global-step:25748	 l-p:0.18399029970169067
epoch£º1287	 i:9 	 global-step:25749	 l-p:0.1835751235485077
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1288
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.3073e-03, 3.0489e-04,
         1.0000e+00, 4.0288e-05, 1.0000e+00, 1.3214e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.7411e-01, 1.7806e-01,
         1.0000e+00, 1.1567e-01, 1.0000e+00, 6.4960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.0748e-01, 5.1449e-01,
         1.0000e+00, 4.3573e-01, 1.0000e+00, 8.4692e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1704, 5.1704, 5.1704],
        [5.1704, 5.1703, 5.1704],
        [5.1704, 4.8835, 4.7992],
        [5.1704, 5.0415, 4.6929]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1288, step:0 
model_pd.l_p.mean(): 0.17343254387378693 
model_pd.l_d.mean(): -19.791000366210938 
model_pd.lagr.mean(): -19.61756706237793 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4801], device='cuda:0')), ('power', tensor([-20.6584], device='cuda:0'))])
epoch£º1288	 i:0 	 global-step:25760	 l-p:0.17343254387378693
epoch£º1288	 i:1 	 global-step:25761	 l-p:0.15947610139846802
epoch£º1288	 i:2 	 global-step:25762	 l-p:0.13238829374313354
epoch£º1288	 i:3 	 global-step:25763	 l-p:0.19239211082458496
epoch£º1288	 i:4 	 global-step:25764	 l-p:0.12030867487192154
epoch£º1288	 i:5 	 global-step:25765	 l-p:0.08100586384534836
epoch£º1288	 i:6 	 global-step:25766	 l-p:0.13209345936775208
epoch£º1288	 i:7 	 global-step:25767	 l-p:0.1302592009305954
epoch£º1288	 i:8 	 global-step:25768	 l-p:0.10452994704246521
epoch£º1288	 i:9 	 global-step:25769	 l-p:0.17938464879989624
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1289
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.9219e-01, 7.3301e-01,
         1.0000e+00, 6.7825e-01, 1.0000e+00, 9.2529e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.6918e-02, 4.4519e-02,
         1.0000e+00, 2.0449e-02, 1.0000e+00, 4.5934e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5385e-08, 3.1845e-10,
         1.0000e+00, 1.3453e-12, 1.0000e+00, 4.2244e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9926e-02, 2.3451e-02,
         1.0000e+00, 9.1769e-03, 1.0000e+00, 3.9133e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1716, 5.2963, 5.0405],
        [5.1716, 5.0775, 5.1346],
        [5.1716, 5.1716, 5.1716],
        [5.1716, 5.1275, 5.1621]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1289, step:0 
model_pd.l_p.mean(): 0.10697710514068604 
model_pd.l_d.mean(): -20.1595458984375 
model_pd.lagr.mean(): -20.052568435668945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4343], device='cuda:0')), ('power', tensor([-20.9863], device='cuda:0'))])
epoch£º1289	 i:0 	 global-step:25780	 l-p:0.10697710514068604
epoch£º1289	 i:1 	 global-step:25781	 l-p:0.10609691590070724
epoch£º1289	 i:2 	 global-step:25782	 l-p:0.11638958752155304
epoch£º1289	 i:3 	 global-step:25783	 l-p:0.22329097986221313
epoch£º1289	 i:4 	 global-step:25784	 l-p:0.1303478628396988
epoch£º1289	 i:5 	 global-step:25785	 l-p:0.13141128420829773
epoch£º1289	 i:6 	 global-step:25786	 l-p:0.1227077916264534
epoch£º1289	 i:7 	 global-step:25787	 l-p:0.2108454406261444
epoch£º1289	 i:8 	 global-step:25788	 l-p:0.137411966919899
epoch£º1289	 i:9 	 global-step:25789	 l-p:0.11781478673219681
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1290
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1480e-04, 5.5793e-06,
         1.0000e+00, 2.7116e-07, 1.0000e+00, 4.8601e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.9798e-01, 1.1539e-01,
         1.0000e+00, 6.7250e-02, 1.0000e+00, 5.8283e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.7706e-01, 9.9426e-02,
         1.0000e+00, 5.5831e-02, 1.0000e+00, 5.6153e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.4638e-02, 4.3127e-02,
         1.0000e+00, 1.9654e-02, 1.0000e+00, 4.5571e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1705, 5.1705, 5.1705],
        [5.1705, 4.9439, 4.9619],
        [5.1705, 4.9675, 5.0057],
        [5.1705, 5.0797, 5.1359]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1290, step:0 
model_pd.l_p.mean(): 0.10330989956855774 
model_pd.l_d.mean(): -19.48712921142578 
model_pd.lagr.mean(): -19.383819580078125 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5230], device='cuda:0')), ('power', tensor([-20.3933], device='cuda:0'))])
epoch£º1290	 i:0 	 global-step:25800	 l-p:0.10330989956855774
epoch£º1290	 i:1 	 global-step:25801	 l-p:0.22920851409435272
epoch£º1290	 i:2 	 global-step:25802	 l-p:0.14905206859111786
epoch£º1290	 i:3 	 global-step:25803	 l-p:0.12438830733299255
epoch£º1290	 i:4 	 global-step:25804	 l-p:0.2277660071849823
epoch£º1290	 i:5 	 global-step:25805	 l-p:0.13514061272144318
epoch£º1290	 i:6 	 global-step:25806	 l-p:0.08154485374689102
epoch£º1290	 i:7 	 global-step:25807	 l-p:0.09583347290754318
epoch£º1290	 i:8 	 global-step:25808	 l-p:0.09557431191205978
epoch£º1290	 i:9 	 global-step:25809	 l-p:0.16641338169574738
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1291
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.0078e-01, 1.1757e-01,
         1.0000e+00, 6.8844e-02, 1.0000e+00, 5.8556e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 2.1062e-01, 1.2532e-01,
         1.0000e+00, 7.4561e-02, 1.0000e+00, 5.9498e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1849e-01, 2.1750e-01,
         1.0000e+00, 1.4853e-01, 1.0000e+00, 6.8291e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.3842e-03, 1.5426e-04,
         1.0000e+00, 1.7192e-05, 1.0000e+00, 1.1145e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1688, 4.9390, 4.9541],
        [5.1688, 4.9291, 4.9328],
        [5.1688, 4.8661, 4.7159],
        [5.1688, 5.1687, 5.1688]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1291, step:0 
model_pd.l_p.mean(): 0.129654198884964 
model_pd.l_d.mean(): -20.731544494628906 
model_pd.lagr.mean(): -20.601890563964844 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4008], device='cuda:0')), ('power', tensor([-21.5344], device='cuda:0'))])
epoch£º1291	 i:0 	 global-step:25820	 l-p:0.129654198884964
epoch£º1291	 i:1 	 global-step:25821	 l-p:0.1367722898721695
epoch£º1291	 i:2 	 global-step:25822	 l-p:0.12664443254470825
epoch£º1291	 i:3 	 global-step:25823	 l-p:0.23517249524593353
epoch£º1291	 i:4 	 global-step:25824	 l-p:0.1357792615890503
epoch£º1291	 i:5 	 global-step:25825	 l-p:0.1753673553466797
epoch£º1291	 i:6 	 global-step:25826	 l-p:0.1404164880514145
epoch£º1291	 i:7 	 global-step:25827	 l-p:0.0695120096206665
epoch£º1291	 i:8 	 global-step:25828	 l-p:0.12763503193855286
epoch£º1291	 i:9 	 global-step:25829	 l-p:0.14525851607322693
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1292
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.8255e-03, 8.1545e-04,
         1.0000e+00, 1.3780e-04, 1.0000e+00, 1.6899e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.5956e-01, 9.4644e-01,
         1.0000e+00, 9.3351e-01, 1.0000e+00, 9.8633e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.6920e-03, 1.7871e-03,
         1.0000e+00, 3.6745e-04, 1.0000e+00, 2.0561e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 8.8488e-02, 3.9432e-02,
         1.0000e+00, 1.7572e-02, 1.0000e+00, 4.4562e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1687, 5.1683, 5.1687],
        [5.1687, 5.5558, 5.4556],
        [5.1687, 5.1675, 5.1687],
        [5.1687, 5.0865, 5.1399]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1292, step:0 
model_pd.l_p.mean(): 0.12056300044059753 
model_pd.l_d.mean(): -18.810482025146484 
model_pd.lagr.mean(): -18.689918518066406 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5306], device='cuda:0')), ('power', tensor([-19.7118], device='cuda:0'))])
epoch£º1292	 i:0 	 global-step:25840	 l-p:0.12056300044059753
epoch£º1292	 i:1 	 global-step:25841	 l-p:0.19528627395629883
epoch£º1292	 i:2 	 global-step:25842	 l-p:0.13998009264469147
epoch£º1292	 i:3 	 global-step:25843	 l-p:0.12579677999019623
epoch£º1292	 i:4 	 global-step:25844	 l-p:0.11380142718553543
epoch£º1292	 i:5 	 global-step:25845	 l-p:0.14113931357860565
epoch£º1292	 i:6 	 global-step:25846	 l-p:0.17013239860534668
epoch£º1292	 i:7 	 global-step:25847	 l-p:0.13548648357391357
epoch£º1292	 i:8 	 global-step:25848	 l-p:0.13875648379325867
epoch£º1292	 i:9 	 global-step:25849	 l-p:0.1278800219297409
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1293
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.6497e-02, 4.1997e-03,
         1.0000e+00, 1.0691e-03, 1.0000e+00, 2.5457e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1612e-01, 2.1535e-01,
         1.0000e+00, 1.4670e-01, 1.0000e+00, 6.8122e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.4586e-01, 7.6782e-02,
         1.0000e+00, 4.0418e-02, 1.0000e+00, 5.2640e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.1434e-01, 5.5493e-02,
         1.0000e+00, 2.6934e-02, 1.0000e+00, 4.8536e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1705, 5.1663, 5.1704],
        [5.1705, 4.8685, 4.7217],
        [5.1705, 5.0073, 5.0648],
        [5.1705, 5.0515, 5.1131]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1293, step:0 
model_pd.l_p.mean(): 0.0991353765130043 
model_pd.l_d.mean(): -20.176361083984375 
model_pd.lagr.mean(): -20.077226638793945 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4775], device='cuda:0')), ('power', tensor([-21.0482], device='cuda:0'))])
epoch£º1293	 i:0 	 global-step:25860	 l-p:0.0991353765130043
epoch£º1293	 i:1 	 global-step:25861	 l-p:0.12021490186452866
epoch£º1293	 i:2 	 global-step:25862	 l-p:0.24473828077316284
epoch£º1293	 i:3 	 global-step:25863	 l-p:0.15107284486293793
epoch£º1293	 i:4 	 global-step:25864	 l-p:0.15801334381103516
epoch£º1293	 i:5 	 global-step:25865	 l-p:0.10642051696777344
epoch£º1293	 i:6 	 global-step:25866	 l-p:0.126114159822464
epoch£º1293	 i:7 	 global-step:25867	 l-p:0.08125568926334381
epoch£º1293	 i:8 	 global-step:25868	 l-p:0.1980140656232834
epoch£º1293	 i:9 	 global-step:25869	 l-p:0.1322159618139267
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1294
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5557e-03, 1.4826e-03,
         1.0000e+00, 2.9093e-04, 1.0000e+00, 1.9623e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.4818e-02, 2.6037e-02,
         1.0000e+00, 1.0459e-02, 1.0000e+00, 4.0170e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2747e-01, 2.2571e-01,
         1.0000e+00, 1.5558e-01, 1.0000e+00, 6.8927e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.2865e-03, 9.2091e-04,
         1.0000e+00, 1.6043e-04, 1.0000e+00, 1.7420e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1670, 5.1661, 5.1670],
        [5.1670, 5.1168, 5.1551],
        [5.1670, 4.8626, 4.6995],
        [5.1670, 5.1666, 5.1670]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1294, step:0 
model_pd.l_p.mean(): 0.09312450885772705 
model_pd.l_d.mean(): -19.185073852539062 
model_pd.lagr.mean(): -19.091949462890625 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5341], device='cuda:0')), ('power', tensor([-20.0970], device='cuda:0'))])
epoch£º1294	 i:0 	 global-step:25880	 l-p:0.09312450885772705
epoch£º1294	 i:1 	 global-step:25881	 l-p:0.15668098628520966
epoch£º1294	 i:2 	 global-step:25882	 l-p:0.11347807943820953
epoch£º1294	 i:3 	 global-step:25883	 l-p:0.23665758967399597
epoch£º1294	 i:4 	 global-step:25884	 l-p:0.12576648592948914
epoch£º1294	 i:5 	 global-step:25885	 l-p:0.14278651773929596
epoch£º1294	 i:6 	 global-step:25886	 l-p:0.1537172496318817
epoch£º1294	 i:7 	 global-step:25887	 l-p:0.12355329841375351
epoch£º1294	 i:8 	 global-step:25888	 l-p:0.1489245444536209
epoch£º1294	 i:9 	 global-step:25889	 l-p:0.12971358001232147
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1295
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 1.2103e-02, 2.7789e-03,
         1.0000e+00, 6.3802e-04, 1.0000e+00, 2.2960e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.1170e-02, 9.8095e-03,
         1.0000e+00, 3.0872e-03, 1.0000e+00, 3.1471e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 6.5639e-02, 2.6478e-02,
         1.0000e+00, 1.0681e-02, 1.0000e+00, 4.0339e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.5706e-01, 6.8999e-01,
         1.0000e+00, 6.2886e-01, 1.0000e+00, 9.1140e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1664, 5.1641, 5.1663],
        [5.1664, 5.1524, 5.1651],
        [5.1664, 5.1151, 5.1540],
        [5.1664, 5.2370, 4.9544]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1295, step:0 
model_pd.l_p.mean(): 0.16626249253749847 
model_pd.l_d.mean(): -20.284862518310547 
model_pd.lagr.mean(): -20.118600845336914 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4604], device='cuda:0')), ('power', tensor([-21.1411], device='cuda:0'))])
epoch£º1295	 i:0 	 global-step:25900	 l-p:0.16626249253749847
epoch£º1295	 i:1 	 global-step:25901	 l-p:0.0735917016863823
epoch£º1295	 i:2 	 global-step:25902	 l-p:0.20270799100399017
epoch£º1295	 i:3 	 global-step:25903	 l-p:0.12634927034378052
epoch£º1295	 i:4 	 global-step:25904	 l-p:0.12540891766548157
epoch£º1295	 i:5 	 global-step:25905	 l-p:0.10768960416316986
epoch£º1295	 i:6 	 global-step:25906	 l-p:0.16483956575393677
epoch£º1295	 i:7 	 global-step:25907	 l-p:0.16628246009349823
epoch£º1295	 i:8 	 global-step:25908	 l-p:0.15032188594341278
epoch£º1295	 i:9 	 global-step:25909	 l-p:0.13854441046714783
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1296
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 9.1964e-02, 4.1511e-02,
         1.0000e+00, 1.8737e-02, 1.0000e+00, 4.5138e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 7.8792e-02, 3.3779e-02,
         1.0000e+00, 1.4481e-02, 1.0000e+00, 4.2871e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.0820e-08, 9.6631e-11,
         1.0000e+00, 3.0297e-13, 1.0000e+00, 3.1353e-03, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.7843e-02, 1.2705e-02,
         1.0000e+00, 4.2656e-03, 1.0000e+00, 3.3573e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1677, 5.0806, 5.1357],
        [5.1677, 5.0990, 5.1469],
        [5.1677, 5.1677, 5.1677],
        [5.1677, 5.1478, 5.1653]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1296, step:0 
model_pd.l_p.mean(): 0.14524570107460022 
model_pd.l_d.mean(): -19.256790161132812 
model_pd.lagr.mean(): -19.111543655395508 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5204], device='cuda:0')), ('power', tensor([-20.1559], device='cuda:0'))])
epoch£º1296	 i:0 	 global-step:25920	 l-p:0.14524570107460022
epoch£º1296	 i:1 	 global-step:25921	 l-p:0.10700925439596176
epoch£º1296	 i:2 	 global-step:25922	 l-p:0.15524694323539734
epoch£º1296	 i:3 	 global-step:25923	 l-p:0.118785560131073
epoch£º1296	 i:4 	 global-step:25924	 l-p:0.17281866073608398
epoch£º1296	 i:5 	 global-step:25925	 l-p:0.1574888825416565
epoch£º1296	 i:6 	 global-step:25926	 l-p:0.18038880825042725
epoch£º1296	 i:7 	 global-step:25927	 l-p:0.13975436985492706
epoch£º1296	 i:8 	 global-step:25928	 l-p:0.12214412540197372
epoch£º1296	 i:9 	 global-step:25929	 l-p:0.11190309375524521
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1297
****************************************
forward func Hx:tensor([[10.5409, 10.5409, 10.5409,  1.0000,  0.4712,  0.3667,  1.0000,  0.2854,
          1.0000,  0.7782, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2304,  0.1412,  1.0000,  0.0866,
          1.0000,  0.6130, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.2326,  0.1431,  1.0000,  0.0880,
          1.0000,  0.6150, 31.6228],
        [10.5409, 10.5409, 10.5409,  1.0000,  0.3396,  0.2369,  1.0000,  0.1653,
          1.0000,  0.6977, 31.6228]], device='cuda:0')
 pt:tensor([[5.1725, 4.9132, 4.5972],
        [5.1725, 4.9151, 4.8936],
        [5.1725, 4.9132, 4.8887],
        [5.1725, 4.8675, 4.6874]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1297, step:0 
model_pd.l_p.mean(): 0.12180144339799881 
model_pd.l_d.mean(): -18.716087341308594 
model_pd.lagr.mean(): -18.59428596496582 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.5371], device='cuda:0')), ('power', tensor([-19.6223], device='cuda:0'))])
epoch£º1297	 i:0 	 global-step:25940	 l-p:0.12180144339799881
epoch£º1297	 i:1 	 global-step:25941	 l-p:0.12082022428512573
epoch£º1297	 i:2 	 global-step:25942	 l-p:0.20003598928451538
epoch£º1297	 i:3 	 global-step:25943	 l-p:0.14721840620040894
epoch£º1297	 i:4 	 global-step:25944	 l-p:0.1973496377468109
epoch£º1297	 i:5 	 global-step:25945	 l-p:0.18471147119998932
epoch£º1297	 i:6 	 global-step:25946	 l-p:0.0974663719534874
epoch£º1297	 i:7 	 global-step:25947	 l-p:0.08518455922603607
epoch£º1297	 i:8 	 global-step:25948	 l-p:0.14707043766975403
epoch£º1297	 i:9 	 global-step:25949	 l-p:0.1027277261018753
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1298
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.4818e-03, 5.2771e-04,
         1.0000e+00, 7.9983e-05, 1.0000e+00, 1.5157e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.3315e-01, 3.2773e-01,
         1.0000e+00, 2.4796e-01, 1.0000e+00, 7.5662e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.4390e-01, 4.4398e-01,
         1.0000e+00, 3.6241e-01, 1.0000e+00, 8.1628e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 4.0162e-01, 2.9632e-01,
         1.0000e+00, 2.1862e-01, 1.0000e+00, 7.3780e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1707, 5.1705, 5.1707],
        [5.1707, 4.8881, 4.6009],
        [5.1707, 4.9724, 4.6259],
        [5.1707, 4.8747, 4.6182]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1298, step:0 
model_pd.l_p.mean(): 0.10704585909843445 
model_pd.l_d.mean(): -20.30986213684082 
model_pd.lagr.mean(): -20.202816009521484 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4753], device='cuda:0')), ('power', tensor([-21.1820], device='cuda:0'))])
epoch£º1298	 i:0 	 global-step:25960	 l-p:0.10704585909843445
epoch£º1298	 i:1 	 global-step:25961	 l-p:0.17618140578269958
epoch£º1298	 i:2 	 global-step:25962	 l-p:0.20485995709896088
epoch£º1298	 i:3 	 global-step:25963	 l-p:0.08118660002946854
epoch£º1298	 i:4 	 global-step:25964	 l-p:0.10705918818712234
epoch£º1298	 i:5 	 global-step:25965	 l-p:0.12296189367771149
epoch£º1298	 i:6 	 global-step:25966	 l-p:0.19134047627449036
epoch£º1298	 i:7 	 global-step:25967	 l-p:0.13970281183719635
epoch£º1298	 i:8 	 global-step:25968	 l-p:0.2041414976119995
epoch£º1298	 i:9 	 global-step:25969	 l-p:0.08215884119272232
====================================================================================================
====================================================================================================
====================================================================================================

epoch:1299
****************************************
forward func Hx:tensor([[1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.9540e-03, 1.0791e-03,
         1.0000e+00, 1.9559e-04, 1.0000e+00, 1.8125e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.5388e-01, 2.5031e-01,
         1.0000e+00, 1.7705e-01, 1.0000e+00, 7.0732e-01, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 3.2674e-04, 2.2505e-05,
         1.0000e+00, 1.5500e-06, 1.0000e+00, 6.8876e-02, 3.1623e+01],
        [1.0541e+01, 1.0541e+01, 1.0541e+01, 1.0000e+00, 5.0169e-02, 1.8503e-02,
         1.0000e+00, 6.8243e-03, 1.0000e+00, 3.6882e-01, 3.1623e+01]],
       device='cuda:0')
 pt:tensor([[5.1667, 5.1661, 5.1667],
        [5.1667, 4.8609, 4.6615],
        [5.1667, 5.1667, 5.1667],
        [5.1667, 5.1340, 5.1611]], device='cuda:0', grad_fn=<SliceBackward0>)

training epoch:1299, step:0 
model_pd.l_p.mean(): 0.12342584133148193 
model_pd.l_d.mean(): -20.764625549316406 
model_pd.lagr.mean(): -20.641199111938477 
model_pd.lambdas: dict_items([('pout', tensor([1.0167], device='cuda:0')), ('power', tensor([0.9816], device='cuda:0'))]) 
model_pd.vars: dict_items([('pout', tensor([0.4015], device='cuda:0')), ('power', tensor([-21.5688], device='cuda:0'))])
epoch£º1299	 i:0 	 global-step:25980	 l-p:0.12342584133148193
epoch£º1299	 i:1 	 global-step:25981	 l-p:0.12316712737083435
epoch£º1299	 i:2 	 global-step:25982	 l-p:0.21335798501968384
epoch£º1299	 i:3 	 global-step:25983	 l-p:0.1490599811077118
epoch£º1299	 i:4 	 global-step:25984	 l-p:0.12658466398715973
epoch£º1299	 i:5 	 global-step:25985	 l-p:0.12133798003196716
epoch£º1299	 i:6 	 global-step:25986	 l-p:0.12074755132198334
epoch£º1299	 i:7 	 global-step:25987	 l-p:0.15664975345134735
epoch£º1299	 i:8 	 global-step:25988	 l-p:0.14141082763671875
epoch£º1299	 i:9 	 global-step:25989	 l-p:0.1551298350095749
